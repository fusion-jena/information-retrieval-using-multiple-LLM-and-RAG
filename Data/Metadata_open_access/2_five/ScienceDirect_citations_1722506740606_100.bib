@article{CHAWLA2024102548,
title = {MobileNet-GRU fusion for optimizing diagnosis of yellow vein mosaic virus},
journal = {Ecological Informatics},
volume = {81},
pages = {102548},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102548},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000906},
author = {Tisha Chawla and Shubh Mittal and Hiteshwar Kumar Azad},
keywords = {Yellow vein mosaic virus, MobileNet, Gated recurrent unit, Recurrent neural networks, Transfer learning, Deep learning},
abstract = {Yellow vein mosaic virus (YVMV) is a destructive plant virus that commonly affects crops, particularly okra, in India. The virus is transmitted by whiteflies and poses significant challenges to agricultural productivity. Infection with YVMV leads to distinct yellow vein patterns on leaves, stunted growth, reduced yield, and ultimately economic losses for farmers. Timely and accurate detection of YVMV is crucial for effective disease management. In this article, we present a novel method that employs advanced deep-learning models to identify YVMV-infected okra plants. The study leverages a dataset of over 2000 okra plant leaves that implements transfer learning models, including MobileNet, EfficientNet, InceptionV3, VGG19, InceptionResNetV2, and ResNet50 and recurrent neural networks (RNN) variants, including Long short-term memory (LSTM), Bidirectional long short-term memory (BiLSTM) and Gated recurrent unit (GRU). Additionally, three hybrid models, combining MobileNet with LSTM, BiLSTM, and GRU, are incorporated to capitalize on the characteristics of both MobileNet and RNNs through superior feature extraction and detection of temporal dependencies. The results demonstrate that the MobileNet model combined with all three RNNs achieves exceptional accuracy, surpassing 99.27%. Notably, the MobileNet model integrated with GRU exhibits the most optimized performance with the least loss and greatest accuracy, facilitating improved disease management strategies and aiding in the yield of crops by reducing the impact of YVMV.}
}
@article{GOMEZVARGAS2023102036,
title = {Re-identification of fish individuals of undulate skate via deep learning within a few-shot context},
journal = {Ecological Informatics},
volume = {75},
pages = {102036},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102036},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000651},
author = {Nuria Gómez-Vargas and Alexandre Alonso-Fernández and Rafael Blanquero and Luis T. Antelo},
keywords = {Deep learning, Few-shot learning, Photo-identification, Siamese networks},
abstract = {Individual re-identification is critical to track population changes in order to assess status, being particularly relevant in species with conservation concerns and difficult access like marine organisms. For this, we propose photo-identification via deep learning as a non-invasive technique to discriminate between individuals of the undulate skate (Raja undulata). Nevertheless, accruing enough training samples might be difficult to achieve in the case of underwater fish images. We develop a novel methodology based on a siamese neural network that incorporates statistical fundamentals as motivation to overcome the few-shot context. Our work provides a hands-on experience and highlights on pitfalls when trying to apply photo-identification in a limited scenario, concerning both data quantity and quality, yet providing remarkable results over the test set including recaptures, where the model is capable of correctly identifying the 70% of the individuals. The findings of this study can be of strong impact for the research teams becoming familiar with deep learning approaches, as it can be easily extended to re-identify individuals of other marine species of interest from a conservation or exploitation point of view.}
}
@article{WHITE2023102363,
title = {One size fits all? Adaptation of trained CNNs to new marine acoustic environments},
journal = {Ecological Informatics},
volume = {78},
pages = {102363},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102363},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003928},
author = {Ellen L. White and Holger Klinck and Jonathan M. Bull and Paul R. White and Denise Risch},
keywords = {Bioacoustics, Deep learning, Domain adaptation, Marine acoustics, Marine mammal detection, Soundscapes},
abstract = {Convolutional neural networks (CNNs) have the potential to enable a revolution in bioacoustics, allowing robust detection and classification of marine sound sources. As global Passive Acoustic Monitoring (PAM) datasets continue to expand it is critical we improve our confidence in the performance of models across different marine environments, if we are to exploit the full ecological value of information within the data. This work demonstrates the transferability of developed CNN models to new acoustic environments by using a pre-trained model developed for one location (West of Scotland, UK) and deploying it in a distinctly different soundscape (Gulf of Mexico, USA). In this work transfer learning is used to fine-tune an existing open-source ‘small-scale’ CNN, which detects odontocete tonal and broadband call types and vessel noise (operating between 0 and 48 kHz). The CNN is fine-tuned on training sets of differing sizes, from the unseen site, to understand the adaptability of a network to new marine acoustic environments. Fine-tuning with a small sample of site-specific data significantly improves the performance of the CNN in the new environment, across all classes. We demonstrate an improved performance in area-under-curve (AUC) score of 0.30, across four classes by fine-training with only 50 spectrograms per class, with a 5% improvement in accuracy between 50 frames and 500 frames. This work shows that only a small amount of site-specific data is needed to retrain a CNN, enabling researchers to harness the power of existing pre-trained models for their own datasets. The marine bioacoustic domain will benefit from a larger pool of global data for training large deep learning models, but we illustrate in this work that domain adaptation can be improved with limited site-specific exemplars.}
}
@article{GHAFFARI2024102573,
title = {On the role of audio frontends in bird species recognition},
journal = {Ecological Informatics},
volume = {81},
pages = {102573},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102573},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001158},
author = {Houtan Ghaffari and Paul Devos},
keywords = {Bioacoustics, Audio frontend, Bird sound recognition, Deep learning},
abstract = {Automatic acoustic monitoring of bird populations and their diversity is in demand for conservation planning. This requirement and recent advances in deep learning have inspired sophisticated species recognizers. However, there are still open challenges in creating reliable monitoring systems of natural habitats. One of many open questions is whether predominantly used audio features like mel-filterbanks are appropriate for such analysis since their design follows human's perception of the sound, making them susceptible to discarding fine details from other animals' vocalization. Although research shows that different audio features work better for particular tasks and datasets, it is hard to attribute all advantages to input features since the experimental setups vary. A general solution is to design a learnable audio frontend to extract task-relevant features from raw waveform since it contains all the information in other audio features. The current paper thoroughly analyzes the role of such frontends in bird species recognition, which helped to evaluate the adequacy of traditional time-frequency representations (static frontends) in capturing the relevant information from bird vocalization. In particular, this work shows that the main performance gain in learnable audio frontends comes from the normalization and compression operations rather than the data-driven frequency selectivity and functional form of filters. We observed no significant discrepancy between the frequency bands of the learned and static frontends for bird vocalization. Although the performance of learnable frontends was much higher, we will show that adequate normalization and compression enhance the accuracy of traditional frontends by more than 16% to achieve comparable results for bird species recognition. Ablation studies of the frontends under different configurations and detailed analysis of noise robustness provide evidence for the conclusions, validate the use of mel-filterbanks and similar features in prior works, and provide guidelines for designing future species recognizers. The code is available at https://github.com/houtan-ghaffari/bird-frontends.}
}
@article{MENG2024102677,
title = {Association between multilevel landscape characteristics and rural sustainability: A case study of the water-net region in the Yangtze River Delta, China},
journal = {Ecological Informatics},
volume = {82},
pages = {102677},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102677},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400219X},
author = {Chengyu Meng and Yimei Chen and Jiexin Yang and Xinyi Su and Wei Guo and Kaili Zhang},
keywords = {Typo-morphology, Deep learning, Spatial heterogeneity, Automatic classification, Regression analysis},
abstract = {In the Yangtze River Delta in China, known for its intricate water network, achieving harmonious development between humans and nature in rural areas is imperative. However, the identification of the water-net landscape characteristics and the relationship between rural sustainability and these landscape characteristics remain unclear. The aim of this study was to bridge this gap by proposing a novel framework for investigating the relationship between landscape characteristics and rural sustainability from a typo-morphological perspective. Specifically, through regression analysis, the influence of multilevel spatial characteristics of rural landscape on sustainability was selected as the research focus. First, multilevel metrics were introduced to delineate the rural landscape characteristics, including single and multiple landscape elements and landscape types, using deep learning methods to achieve automatic classification. Subsequently, by employing an improved entropy method, we comprehensively quantified rural sustainability indicators from the economic, social, and ecological dimensions. Finally, the ordinary least squares (OLS) model and two spatial variation coefficient models, namely, geographically weighted regression (GWR) and multiscale geographically weighted regression (MGWR), were used to quantitatively analyze the relationship between the landscape characteristics and rural sustainability. Significant regression model performances were obtained with adjusted R2 values of 0.33, 0.35, and 0.4 at each landscape characteristic level. The adjusted R2 values for the GWR and MGWR, which incorporated all of the landscape characteristics metrics, were 0.84 and 0.88, respectively. The results demonstrate that rural sustainability highly depends on the proposed multilevel characteristics and exhibits spatial heterogeneity. The findings of this study improve our understanding of the typo-morphological characteristics of the landscape and provide important planning and decision-making references for sustainable development in rural areas.}
}
@article{DUAN2024102637,
title = {SIAlex: Species identification and monitoring based on bird sound features},
journal = {Ecological Informatics},
volume = {81},
pages = {102637},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102637},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001791},
author = {Lin Duan and Lidong Yang and Yong Guo},
keywords = {Lightweight, Cascading activation function, Bird sound recognition, Structural re-parameterization, Nonlinear performance},
abstract = {The combination of deep learning and bird sound recognition is widely employed in bird species conservation monitoring. A complex network structure is not conducive for deploying bird sound recognition devices, resulting in problems such as long inference time and low efficiency. Using AlexNet as the backbone model, we explore the potential of shallow and straightforward models without complex connection techniques or attention mechanisms, named SIAlex, to recognise and classify 20 bird sound datasets, which are simultaneously validated on a 10 class UrbanSound8k dataset. Using the structural re-parameterization method, the number of model layers is reduced, computational efficiency is improved, and the inference time is significantly reduced, achieving a decoupling of training and inference time in the structure. To increase the nonlinearity of the model, a cascaded approach is utilised to increase the number of activation functions, thereby significantly improving the generalisation performance of the model. Simultaneously, in the classifier section, convolutional layer replaces the original fully connected layer, thereby reducing the inference time and increasing the feature extraction ability of the model, improving accuracy, and effectively recognising bird speech. The experimental data show that the SIAlex network on the Birdsdata dataset improves the accuracy to 93.66%, and the inference time for a piece of data is only 2.466 ms. The accuracy of the UrbanSound8k dataset reaches 96.04%, and the inference time for a piece of data is 3.031 ms. A large number of experimental comparisons have shown that the method proposed in this paper achieves good results in reducing the inference time of the model, bringing breakthroughs in the application of shallow, simple models.}
}
@article{BOHNETT2023102214,
title = {Comparison of two individual identification algorithms for snow leopards (Panthera uncia) after automated detection},
journal = {Ecological Informatics},
volume = {77},
pages = {102214},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102214},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002431},
author = {Eve Bohnett and Jason Holmberg and Sorosh Poya Faryabi and Li An and Bilal Ahmad and Wajid Rashid and Stephane Ostrowski},
keywords = {Background subtraction, Deep learning, Hotspotter, Individual identification, PIE v2, Snow leopards},
abstract = {Photo-identification of individual snow leopards (Panthera uncia) is the primary data source for density estimation via capture-recapture statistical methods. To identify individual snow leopards in camera trap imagery, it is necessary to match individuals from a large number of images from multiple cameras and historical catalogues, which is both time-consuming and costly. The camouflaged snow leopards also make it difficult for machine learning to classify photos, as they blend in so well with the surrounding mountain environment, rendering applicable software solutions unavailable for the species. To potentially make snow leopard individual identification available via an artificial intelligence (AI) software interface, we first trained and evaluated image classification techniques for a convolutional neural network, pose invariant embeddings (PIE) (a triplet loss network), and compared the accuracy of PIE to that of the HotSpotter algorithm (a SIFT-based algorithm). Data were acquired from a curated library of free-ranging snow leopards taken in Afghanistan between 2012 and 2019 and from captive animals in zoos in Finland, Sweden, Germany, and the United States. We discovered several flaws in the initial PIE model, such as a small amount of background matching, that was addressed, albeit likely not fixed, using background subtraction (BGS) and left-right mirroring (LR) techniques which demonstrated reasonable accuracy (Rank 1: 74% Rank-5: 92%) comparable to the Hotspotter results (Rank 1: 74% Rank 2: 84%)The PIE BGS LR model, in conjunction with Hotspotter, yielded the following results: Rank-1: 85%, Rank-5: 95%, Rank-20: 99%. In general, our findings indicate that PIE BGS LR, in conjunction with HotSpotter, can classify snow leopards more accurately than using either algorithm alone.}
}
@article{XIE2022101893,
title = {Multi-view features fusion for birdsong classification},
journal = {Ecological Informatics},
volume = {72},
pages = {101893},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101893},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003430},
author = {Shanshan Xie and Jing Lu and Jiang Liu and Yan Zhang and Danjv Lv and Xu Chen and Youjie Zhao},
keywords = {Birdsong recognition, Deep features, Handcrafted features, mRMR, Feature selection},
abstract = {As important members of the ecosystem, birds are good monitors of the ecological environment. Bird recognition, especially birdsong recognition, has attracted more and more attention in the field of artificial intelligence. At present, traditional machine learning and deep learning are widely used in birdsong recognition. Deep learning can not only classify and recognize the spectrums of birdsong, but also be used as a feature extractor. Machine learning is often used to classify and recognize the extracted birdsong handcrafted feature parameters. As the data samples of the classifier, the feature of birdsong directly determines the performance of the classifier. Multi-view features from different methods of feature extraction can obtain more perfect information of birdsong. Therefore, aiming at enriching the representational capacity of single feature and getting a better way to combine features, this paper proposes a birdsong classification model based multi-view features, which combines the deep features extracted by convolutional neural network (CNN) and handcrafted features. Firstly, four kinds of handcrafted features are extracted. Those are wavelet transform (WT) spectrum, Hilbert-Huang transform (HHT) spectrum, short-time Fourier transform (STFT) spectrum and Mel-frequency cepstral coefficients (MFCC). Then CNN is used to extract the deep features from WT, HHT and STFT spectrum, and the minimal-redundancy-maximal-relevance (mRMR) to select optimal features. Finally, three classification models (random forest, support vector machine and multi-layer perceptron) are built with the deep features and handcrafted features, and the probability of classification results of the two types of features are fused as the new features to recognize birdsong. Taking sixteen species of birds as research objects, the experimental results show that the three classifiers obtain the accuracy of 95.49%, 96.25% and 96.16% respectively for the features of the proposed method, which are better than the seven single features and three fused features involved in the experiment. This proposed method effectively combines the deep features and handcrafted features from the perspectives of signal. The fused features can more comprehensively express the information of the bird audio itself, and have higher classification accuracy and lower dimension, which can effectively improve the performance of bird audio classification.}
}
@article{JAMALI2022101904,
title = {3DUNetGSFormer: A deep learning pipeline for complex wetland mapping using generative adversarial networks and Swin transformer},
journal = {Ecological Informatics},
volume = {72},
pages = {101904},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101904},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003545},
author = {Ali Jamali and Masoud Mahdianpari and Brian Brisco and Dehua Mao and Bahram Salehi and Fariba Mohammadimanesh},
keywords = {Generative adversarial network, Convolutional neural networks, Wetland mapping, Vision transformers, Deep learning, Swin transformer},
abstract = {Many ecosystems, particularly wetlands, are significantly degraded or lost as a result of climate change and anthropogenic activities. Simultaneously, developments in machine learning, particularly deep learning methods, have greatly improved wetland mapping, which is a critical step in ecosystem monitoring. Yet, present deep and very deep models necessitate a greater number of training data, which are costly, logistically challenging, and time-consuming to acquire. Thus, we explore and address the potential and possible limitations caused by the availability of limited ground-truth data for large-scale wetland mapping. To overcome this persistent problem for remote sensing data classification using deep learning models, we propose 3D UNet Generative Adversarial Network Swin Transformer (3DUNetGSFormer) to adaptively synthesize wetland training data based on each class's data availability. Both real and synthesized training data are then imported to a novel deep learning architecture consisting of cutting-edge Convolutional Neural Networks and vision transformers for wetland mapping. Results demonstrated that the developed wetland classifier obtained a high level of kappa coefficient, average accuracy, and overall accuracy of 96.99%, 97.13%, and 97.39%, respectively, for the data in three pilot sites in and around Grand Falls-Windsor, Avalon, and Gros Morne National Park located in Canada. The results show that the proposed methodology opens a new window for future high-quality wetland data generation and classification. The developed codes are available at https://github.com/aj1365/3DUNetGSFormer.}
}
@article{ZANG2022101892,
title = {Ages of giant panda can be accurately predicted using facial images and machine learning},
journal = {Ecological Informatics},
volume = {72},
pages = {101892},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101892},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003429},
author = {Hang-Xing Zang and Han Su and Yu Qi and Lin Feng and Rong Hou and Mengnan He and Peng Liu and Ping Xu and Yanglina Yu and Peng Chen},
keywords = {Biodiversity, Giant panda, Machine learning, Age estimation, Ordinal regression},
abstract = {To forecast giant panda (Ailuropoda melanoleuca) population dynamics in the wild, it is crucial to comprehend their age distribution. Traditional methods for estimating the age of panda are costly, time-consuming, and inaccurate. Additionally, these methods only forecast an age group rather than a real age, and lack a uniform standard. However, advances in deep learning and computer vision have given rise to fresh approaches to this problem. Classification models can be improved by using ordinal regression, which uses ordinal correlations across ages to reduce the non-stationary nature of aging tasks. In this study, we collected 8002 images from 272 pandas in various environments, whose ages ranged from 0 to 38. We applied a five-fold subject-exclusive (SE) protocol to train seven Convolutional Neural Networks (CNN) based on ordinal regression. Experiments were conducted on the Panda Age Dataset (PAD Full) and the Lite Panda Age Dataset (PAD Lite). The results were very encouraging and achieved a Mean Absolute Error (MAE) of 2.51 and 2.41, respectively. Our findings demonstrate that this new tool can noninvasively predict the age of giant pandas in captivity and the wild. Continued development of computer vision technology will drive progress in ecology and conservation.}
}
@article{CANOVI2024102733,
title = {Trajectory-based fish event classification through pre-training with diffusion models},
journal = {Ecological Informatics},
volume = {82},
pages = {102733},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102733},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002759},
author = {Noemi Canovi and Benjamin A. Ellis and Tonje K. Sørdalen and Vaneeda Allken and Kim T. Halvorsen and Ketil Malde and Cigdem Beyan},
keywords = {Fish behavior, Underwater videos, Event recognition, Trajectory, Generative models, Autoencoder, Diffusion model, Corkwing wrasse},
abstract = {This study contributes to advancing the field of automatic fish event recognition in natural underwater videos, addressing the current gap in studying fish interaction and competition, including predator-prey relationships and mating behaviors. We used the corkwing wrasse (Symphodus melops) as a model, a marine species of commercial importance that reproduces in sea-weed nests built and cared for by a single male. These nests attract a wide range of visitors and are the focal point for behavior such as spawning, chasing, and maintenance. We propose a deep learning methodology to analyze the movement trajectories of the nesting male and classify the associated events observed in their natural habitat. Our approach leverages unsupervised pre-training based on diffusion models, leading to improved feature learning. Additionally, we introduce a dataset comprising 16,937 trajectories across 12 event classes, making it the largest in terms of event class diversity. Our results demonstrate the superior performance of our method compared to several deep architectures. The code for the proposed method and the trajectories can be found at https://github.com/NoeCanovi/Fish_Behaviors_Generative_Models.}
}
@article{ELRAWY2024102652,
title = {Assessing and segmenting salt-affected soils using in-situ EC measurements, remote sensing, and a modified deep learning MU-NET convolutional neural network},
journal = {Ecological Informatics},
volume = {81},
pages = {102652},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102652},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001948},
author = {Mustafa El-Rawy and Sally Y. Sayed and Mohamed A.E. AbdelRahman and Atef Makhloof and Nassir Al-Arifi and Mahmoud Khaled Abd-Ellah},
keywords = {Soil salinity, Artificial neural networks, Deep learning, Remote sensing, Salinity indices},
abstract = {A study revealed that the Siwa Oasis faces high soil salinity, which negatively impacts agricultural areas and crop productivity, despite its significant economic and agricultural importance. The current research proposes an approach to detect and segment salinity and vegetation areas at Siwa Oasis, Egypt, by combining remote sensing and building a deep learning neural network model-based U-NET algorithm to detect salinity change areas, anticipate further degradation, and predict soil quality indicators. To locate changes among the available images, standard image improvement, classification, and change detection methods have been used. We applied a deep learning modified U-Net (MU-NET) algorithm to segment and produce salinity maps. The MU-NET architecture is a two-level nested U-structure merged with a residual U-block (RUb), which consists of an encoder and a decoder. We applied RUb, which consists of several layers and skip connections. Different combinations of the salinity and vegetation indices were added to the original image to improve segmentation precision. The model was validated and trained using actual data samples collected over a 10-year period from the Landsat 8 satellite, which can monitor and analyse present land cover changes. The dataset consisted of 91 OLI and TIRS spectral images. Each one consists of eleven bands with a spatial resolution of 30 m for bands 1 to 7 and 9. A field survey was used as the main source of data for comparing the proposed model's outputs to assess the error rate. The study region is experiencing an increase in soil salinity in all directions, particularly with regard to the spatial distribution of saline soils, not just the quantitative increase in salt-affected soils. These findings supported the acceleration of soil salinization and vegetation death. The proposed model achieved the highest performance results among the other models and literature and was based on applying method 12 using 13 image layers, with the highest accuracies of 91.27% and 90.83% for salinity and vegetation, respectively.}
}
@article{GONCALVES2024102628,
title = {Revealing forest structural "fingerprints": An integration of LiDAR and deep learning uncovers topographical influences on Central Amazon forests},
journal = {Ecological Informatics},
volume = {81},
pages = {102628},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102628},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001705},
author = {Nathan Borges Gonçalves and Diogo Martins Rosa and Dalton Freitas {do Valle} and Marielle N. Smith and Ricardo Dalagnol and Danilo Roberti Alves {de Almeida} and Bruce W. Nelson and Scott C. Stark},
keywords = {Structural "fingerprints", LiDAR, Deep learning, Amazon Forest, Terrain variation},
abstract = {Amazon forests are characterized by rich structural diversity. However, the influence of factors such as topography, soil attributes, and external disturbances on structural variability is not always well characterized, and traditional structural metrics may be inadequate to capture this type of complexity. While LiDAR offers expanded structural insights, traditional parameters used in LiDAR analysis, such as mean or maximum canopy height, are not always well directly linked to environmental variables like topography. Emerging approaches merge LiDAR with machine learning to uncover deeper structural complexities. However, work to date may fail to fully utilize the potential of fine-scale LiDAR information. Here we introduce a novel approach, leveraging 2D point cloud images derived from a profiling canopy LiDAR (PCL). The technique targets intricate details within LiDAR point clouds by using deep learning algorithms. With a dataset from the Central Amazon comprising 18 multitemporal transects of 450 m in length, our objective was to detect structural "fingerprints" of varied topographical types along a hillslope, comprising: Riparian, White-sand, and Plateau, and to detect any gradient of structural shifts based on terrain variations here represented by the height above the nearest drainage (HAND). The dataset was trained and tested using a leave-one-group-out approach (LOGO) in which, for each iteration, a complete 450 m multitemporal transect was excluded from training and tested after each iteration. The fast.ai platform and a ResNet-34 architecture, coupled with transfer learning, were used to perform a classification to distinguish between three topographical types. Furthermore, a hybrid model combining a Convolutional Autoencoder, and Partial Least Square (PLS) regression was designed to detect forest structural gradient correlations with HAND variation. Cross-validation achieved a promising high weighted F1 score of 0.83 to classify forests based on the topographical types. Additionally, a combined Convolutional Autoencoder and PLS regression revealed a strong correlation (R2 = 0.76) between actual and predicted HAND. Innovatively combining deep learning with ground-based PCL LiDAR, our study revealed unique Amazon Forest structures connected to topographic variation. Our findings underscore the transformative potential of such integrative approaches for investigating forest dynamics and promise a powerful new tool for understanding climate-related forest structure change.}
}
@article{ZHOU2024102680,
title = {Real-time underwater object detection technology for complex underwater environments based on deep learning},
journal = {Ecological Informatics},
volume = {82},
pages = {102680},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102680},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400222X},
author = {Hui Zhou and Meiwei Kong and Hexiang Yuan and Yanyan Pan and Xinru Wang and Rong Chen and Weiheng Lu and Ruizhi Wang and Qunhui Yang},
keywords = {Underwater object detection, You only look once, Cross stage multi-branch, Large kernel spatial pyramid},
abstract = {Underwater object detection technology is crucial in many marine-related fields, including marine environmental monitoring, marine resource development, and marine ecological protection. However, this technology faces great challenges due to the poor quality of underwater optical images and the varying sizes of underwater objects. Therefore, we proposed an underwater optical detection network (UODN) based on the you only look once version 8 (YOLOv8) framework, which addresses these issues through the cross stage multi-branch (CSMB) module and large kernel spatial pyramid (LKSP) module. The aim of the CSMB module is to extract more features from underwater optical images to address the issue of poor image quality, while the LKSP module is designed to enhance the ability of the network to detect underwater objects of various scales. Furthermore, CSMBDarknet built by CSMB and LKSP can be used as the backbone of other underwater object detection algorithms for underwater feature extraction. Extensive experimental results on the underwater robot professional contest 2020 dataset revealed that the average precision (AP) of UODN increased by 1.0%, the AP50 of UODN increased by 1.1%, and the AP75 of UODN increased by 2.1% compared with those of the original YOLOv8s. Furthermore, UODN outperforms 12 state-of-the-art models on multiple underwater optical datasets, paving the way for future real-time and high-precision underwater object detection.}
}
@article{HU2024102640,
title = {Capturing urban green view with mobile crowd sensing},
journal = {Ecological Informatics},
volume = {81},
pages = {102640},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102640},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001821},
author = {Yingqiang Hu and Yue Wu and Zhuzi Tantian and Guodong Sun},
keywords = {Urban green spaces, Green view index, Deep learning, Mobile crowd sensing},
abstract = {Urban green spaces are beneficial to ecosystems and the health of people. The Green View Index (GVI) is an essential metric for assessing urban green spaces from a human perspective. However, measuring GVI at an urban scale requires extensive collection and processing of sensing data, posing challenges in terms of high resource consumption, difficulty in implementation, and lack of user participation. Mobile crowd Sensing (MCS) is an emerging large-scale, low-cost solution for sensing data collection. To address the aforementioned issues, this study proposes an MCS system called GreenCam to measure the GVI with smartphone sensors. GreenCam guides users to capture photos of urban green spaces from human perspective. The system employs a Transformer-based model, which is trained on a customized dataset of 1200 carefully-labeled urban green images, to extract the greenery from the captured photos and calculate GVI. With widespread participation from urban users, the photos captured by users with GreenCam can cover various streets and areas of the city, enabling the measurement of GVI at an urban scale. Additionally, these photos reflect people's preferences towards specific urban landscapes, and analyzing the distribution and characteristics of popular landscapes contributes to the enhancement of urban ecosystems and landscapes.}
}
@article{DEMELOLIMA2024102543,
title = {A lightweight and enhanced model for detecting the Neotropical brown stink bug, Euschistus heros (Hemiptera: Pentatomidae) based on YOLOv8 for soybean fields},
journal = {Ecological Informatics},
volume = {80},
pages = {102543},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102543},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000852},
author = {Bruno Pinheiro {de Melo Lima} and Lurdineide {de Araújo Barbosa Borges} and Edson Hirose and Díbio Leandro Borges},
keywords = {Heteroptera, Insect pest detection, Improved YOLO model, Image-based detection and counting, Deep learning, Soybean field images},
abstract = {Insect pest detection and monitoring are vital in an agricultural crop to help prevent losses and be more precise and sustainable regarding the consequent actions to be taken. Deep learning (DL) approaches have attracted attention, showing triumphant performance in many image-based applications. In the adult stage, this research considers detecting a vital insect pest in soybean crops, the Neotropical brown stink bug (Euschistus heros), from field images acquired by drones and cellphones. We develop and test an improved YOLO-model convolutional neural network (CNN) with fewer parameters than other state-of-the-art models and demonstrate its superior generalization and average precision on public image datasets and the new field data provided here. Considering the proposal's precision and time of response, the possibility of deploying this technology for automatic monitoring and pest management in the near future is promising. We provide open code and data for all the experiments performed.}
}
@article{KWON2024102588,
title = {Estimation of aquatic ecosystem health using deep neural network with nonlinear data mapping},
journal = {Ecological Informatics},
volume = {81},
pages = {102588},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102588},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001304},
author = {Yong Sung Kwon and Hyeongsik Kang and JongCheol Pyo},
keywords = {Deep learning, Aquatic ecosystem health index, Autoencoder, Convolutional neural network},
abstract = {Estimation of aquatic ecosystem health indices can assist in reducing the burden of time-consuming, labor-intensive, and cost-effective fieldwork for the sustainable evaluation of freshwater ecosystem status. In this study, we developed a deep neural network to estimate the trophic diatom index (TDI), benthic macroinvertebrate index (BMI), and fish assessment index (FAI) using water quality and hydraulic and hydrological data. A convolutional neural network (CNN) model was built to estimate health indices. In addition, an autoencoder was adopted to produce manifold features that were used as inputs for the CNN model. Conventional machine learning models, including artificial neural networks, support vector machines, random forests, and extreme gradient boosting, have been developed to estimate the TDI, BMI, and FAI. The results showed that the CNN with an autoencoder exhibited the best performance, with validation accuracies of Nash Sutcliffe Efficiency (NSE) and root mean squared error (RMSE) values of 0.592 and 17.249 for TDI, 0.669 and 12.282 for BMI, and 0.638 and 13.897 for FAI, respectively. The autoencoder enhanced the nonlinear feature learning of the time series and static input data, which contributed to improving the CNN feature extraction for accurate estimation of aquatic ecosystem health indices compared to other data-driven approaches. Therefore, deep learning techniques can be used to investigate aquatic ecosystem health by successfully reflecting the quantitative and qualitative features of health indices.}
}
@article{COMESANACEBRAL2024102612,
title = {Wildfire response of forest species from multispectral LiDAR data. A deep learning approach with synthetic data},
journal = {Ecological Informatics},
volume = {81},
pages = {102612},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102612},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001547},
author = {Lino Comesaña-Cebral and Joaquín Martínez-Sánchez and Gabriel Suárez-Fernández and Pedro Arias},
keywords = {Multispectral LiDAR, Deep learning, Fire response, Synthetic data, Wildfire},
abstract = {Forests play a crucial role as the lungs and life-support system of our planet, harbouring 80% of the Earth's biodiversity. However, we are witnessing an average loss of 480 ha of forest every hour because of destructive wildfires spreading across the globe. To effectively mitigate the threat of wildfires, it is crucial to devise precise and dependable approaches for forecasting fire dynamics and formulating efficient fire management strategies, such as the utilisation of fuel models. The objective of this study was to enhance forest fuel classification that considers only structural information, such as the Prometheus model, by integrating data on the fire responses of various tree species and other vegetation elements, such as ground litter and shrubs. This distinction can be achieved using multispectral (MS) Light Detection and Ranging (LiDAR) data in mixed forests. The methodology involves a novel approach in semantic classifications of forests by generating synthetic data with semantic labels regarding fire responses and reflectance information at different spectral bands, as a real MS scanner device would detect. Forests, which are highly intricate environments, present challenges in accurately classifying point clouds. To address this complexity, a deep learning (DL) model for semantic classification was trained on synthetic point clouds in different formats to achieve the best performance when leveraging MS data. Forest plots in the study region were scanned using different Terrestrial Laser Scanning sensors at wavelengths of 905 and 1550 nm. Subsequently, an interpolation process was applied to generate the MS point clouds of each plot, and the trained DL model was applied to classify them. These classifications surpassed the average thresholds of 90% and 75% for accuracy and intersection over union, respectively, resulting in a more precise categorisation of fuel models based on the distinct responses of forest elements to fire. The results of this study reveal the potential of MS LiDAR data and DL classification models for improving fuel model retrieval in forest ecosystems and enhancing wildfire management efforts.}
}
@article{CLARK2023102065,
title = {The effect of soundscape composition on bird vocalization classification in a citizen science biodiversity monitoring project},
journal = {Ecological Informatics},
volume = {75},
pages = {102065},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102065},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000948},
author = {Matthew L. Clark and Leonardo Salas and Shrishail Baligar and Colin A. Quinn and Rose L. Snyder and David Leland and Wendy Schackwitz and Scott J. Goetz and Shawn Newsam},
keywords = {Convolutional neural networks, CNN, Ecoacoustics, Avian diversity, Bird species classification, Mixture of experts (MoE), Citizen science, Automated recording units, ARU, Soundscapes to landscapes, BirdNET, Soundscape components},
abstract = {There is a need for monitoring biodiversity at multiple spatial and temporal scales to aid conservation efforts. Autonomous recording units (ARUs) can provide cost-effective, long-term and systematic species monitoring data for sound-producing wildlife, including birds, amphibians, insects and mammals over large areas. Modern deep learning can efficiently automate the detection of species occurrences in these sound data with high accuracy. Further, citizen science can be leveraged to scale up the deployment of ARUs and collect reference vocalizations needed for training and validating deep learning models. In this study we develop a convolutional neural network (CNN) acoustic classification pipeline for detecting 54 bird species in Sonoma County, California USA, with sound and reference vocalization data collected by citizen scientists within the Soundscapes to Landscapes project (www.soundscapes2landscapes.org). We trained three ImageNet-based CNN architectures (MobileNetv2, ResNet50v2, ResNet100v2), which function as a Mixture of Experts (MoE), to evaluate the usefulness of several methods to enhance model accuracy. Specifically, we: 1) quantify accuracy with fully-labeled 1-min soundscapes for an assessment of real-world conditions; 2) assess the effect on precision and recall of additional pre-training with an external sound archive (xeno-canto) prior to fine-tuning with vocalization data from our study domain; and, 3) assess how detections and errors are influenced by the presence of coincident biotic and non-biotic sounds (i.e., soundscape components). In evaluating accuracy with soundscape data (n = 37 species) across CNN probability thresholds and models, we found acoustic pre-training followed by fine-tuning improved average precision by 10.3% relative to no pre-training, although there was a small average 0.8% reduction in recall. In selecting an optimal CNN architecture for each species based on maximum F(β = 0.5), we found our MoE approach had total precision of 84.5% and average species precision of 85.1%. Our data exhibit multiple issues arising from applying citizen science and acoustic monitoring at the county scale, including deployment of ARUs with relatively low fidelity and recordings with background noise and overlapping vocalizations. In particular, human noise was significantly associated with more incorrect species detections (false positives, decreased precision), while physical interference (e.g., recorder hit by a branch) and geophony (e.g., wind) was associated with the classifier missing detections (false negatives, decreased recall). Our process surmounted these obstacles, and our final predictions allowed us to demonstrate how deep learning applied to acoustic data from low-cost ARUs paired with citizen science can provide valuable bird diversity data for monitoring and conservation efforts.}
}
@article{CHEN2024102693,
title = {Weight-based ensemble method for crop pest identification},
journal = {Ecological Informatics},
volume = {82},
pages = {102693},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102693},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002358},
author = {Miao Chen and Jianji Wang and Yanan Chen and Minghui Guo and Nanning Zheng},
keywords = {Crop pest identification, Deep learning, Ensemble method, Convex optimization},
abstract = {Crop pests cause significant losses to agricultural production. Pests can be detected and controlled over time using accurate and effective methods, thereby reducing potential losses. However, there are challenges in realistic agricultural scenarios, such as diverse pest species and complicated environments, which render manual recognition and conventional machine learning methods insufficient. To address this issue, deep learning methods that can automatically extract features have recently been widely used for pest identification. However, accurately recognizing images that resemble complex real-world scenarios remains a challenging task for a single deep learning model. The ensemble method, which combines multiple basic models, provides a solution for improving recognition performance. In this study, we proposed two weight-based ensemble methods, VecEnsemble and MatEnsemble, constructed from vector- and matrix-based weights, respectively. The weights that combine basic models significantly influence the performance of the ensemble methods. Therefore, to effectively combine the basic models, we formulated the weight design problem as a quadratic convex optimization problem whose solution has a closed-form expression and can be computed efficiently. Our method achieved the highest accuracy of 77.39% on the large-scale complex-scene IP102 dataset, which was competitive with those of other state-of-the-art methods. Furthermore, we conducted comprehensive ablation experiments to compare our proposed methods with voting-based approaches and illustrate the scenarios in which they are applicable. These results highlight the practical significance of our method for agricultural production and provide a foundation for further research on crop pest identification. The source code is available at https://github.com/shiguangqianmo/WBEnsemble.}
}
@article{WANG2024102721,
title = {Forecasting ecological water demand of an arid oasis under a drying climate scenario based on deep learning methods},
journal = {Ecological Informatics},
volume = {82},
pages = {102721},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102721},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002632},
author = {Xu-dong Wang and Hao-jie Xu and Yan-xia Pan and Xue-mei Yang},
keywords = {Environmental flow, Artificial oasis, Model simulation, Meteorological drought, Scenario analysis},
abstract = {Ecological water diversion projects (EWDP) are an effective management tool for restoring oasis ecosystems in arid regions. Given the potential for drier climatic conditions in arid regions in the future, it is essential to develop water diversion strategies that can adapt to climate change and reduce the risk of oasis ecosystem degradation. Here, this study used a Bayesian optimization-based long- and short-term memory (BO-LSTM) model to determine the optimal amount of water diversion needed to maintain healthy growth of oasis vegetation under future climate change scenarios in the Qingtu Oasis, which is a typical downstream oasis of inland rivers restored by EWDP in China. The results showed that the BO-LSTM model effectively captured the response of oasis vegetation to changes in water inundation areas and drought stress with low computational cost and high accuracy. The study revealed that regional vegetation became more vulnerable than previously thought when extreme drought and a drying trend were taken into account. It was found that if the amount of water entering the oasis ranges from 10 to 15 million m3, there will be a decline in the growth of oasis vegetation as indicated by the normalized difference vegetation index (NDVI). Even if current levels of water diversion (20 million m3) are maintained, oasis vegetation may still face growth decline due to meteorological drought. The optimal amount of water diversion was determined to be 25 million m3, resulting in a 21.7% increase in NDVI regardless of drought events. This study represents an innovative approach as it couples EWDP, climate change, and oasis vegetation dynamics based on deep learning models, which unveils divergent responses of oasis vegetation to climate change and EWDP and verifies a non-linear relationship between water diversion amounts and ecological benefits generated.}
}
@article{YANG2024102705,
title = {Adaptive image processing embedding to make the ecological tasks of deep learning more robust on camera traps images},
journal = {Ecological Informatics},
volume = {82},
pages = {102705},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102705},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002474},
author = {Zihe Yang and Ye Tian and Junguo Zhang},
keywords = {Adaptive image processing, Camera traps, Deep learning, Ecological tasks},
abstract = {Camera traps serve as a valuable tool for wildlife monitoring, generating a vast collection of images for ecologists to conduct ecological investigations, such as species identification and population estimation. However, the sheer volume of images poses a challenge, and the integration of deep learning into automated ecological investigation tasks remains complex, particularly when dealing with low-quality images in long-term monitoring programs. Existing approaches often struggle to strike a balance between image enhancement and deep learning for ecological tasks, thereby overlooking crucial information contained within low-quality images. This research introduces a pioneering adaptive image processing module (AIP) that seamlessly incorporates image processing into camera trap ecological tasks, elevating the performance of wildlife monitoring activities. Specifically, a differentiable image processing (DIP) module is presented to enhance low-quality images, with its parameters predicted by a Non-local based parameter predictor (NLPP). Additionally, an end-to-end approach based on hybrid data containing both original and synthetic data is proposed, encompassing adaptive image processing methods and downstream tasks for camera traps, adaptable to various scenarios. This approach effectively reduces the manual labor and time required for professional image processing. When applied to real-world camera trap images and synthetic image datasets, our method achieves an accuracy of 92.26% and 86.65% in classifying wildlife, respectively, demonstrating its robustness. By outperforming alternative methods under harsh conditions, the application of the adaptive image processing module instills greater confidence in deep learning applications within complex environments.}
}
@article{WEI2024102445,
title = {YOLO_MRC: A fast and lightweight model for real-time detection and individual counting of Tephritidae pests},
journal = {Ecological Informatics},
volume = {79},
pages = {102445},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102445},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004740},
author = {Min Wei and Wei Zhan},
keywords = {Real-time detection, Individual counting, YOLOv8, Lightweight, Attention mechanism, },
abstract = {Tephritidae pests severely affect the quality and safety of various melons, fruits and vegetable crops. However, many agricultural managers lack an adequate understanding of the level of pest occurrence, resulting in the misuse of pesticides, which ultimately leads to environmental pollution and economic loss. Therefore, real-time detection and counting of Tephritidae pests are important for timely pest spotting and control. This work helps quickly determine the distribution and abundance of pests in the current environment, thus providing data on pest conditions for agricultural management to optimize pesticide use. Nevertheless, the fast speed, high accuracy, and lightweight performance of real-time detection and counting are difficult to balance. To address this problem, based on the YOLOv8n model, this paper takes Bactrocera cucurbitae pests as the detection target and proposes a fast and lightweight real-time detection and individual counting model for Tephritidae pests, named YOLO_MRC. This paper introduces three key improvements: (1) Constructing a new module called Multicat into the neck network increases the focus on the detection target by incorporating an attention mechanism; (2) Reducing the original three detection heads to two and then adjusting their sizes to decrease the number of parameters in the network model; (3) Devising a novel module, C2flite, to enhance the deep feature extraction capability of the backbone network. According to the above points, this paper conducts ablation experiments to compare the performances of different models. Experiments showed that the Multicat module significantly offsets the large increase in GFLOPs and processing time caused by reducing the detection head and can further reduce the number of parameters and improve the accuracy when combined with the C2flite module. On our Bactrocera cucurbitae pest dataset, the mAP@0.5 of the YOLO_MRC model reached 99.3%. Simultaneously, as the number of parameters decreases by 63.68%, GFLOPs is reduced by 19.75%, and the processing time is shortened by 5%. To ensure the validity of the model, YOLO_MRC is compared with four excellent detection models by using manual counting results as the benchmark. YOLO_MRC achieves an average pest counting accuracy of 94%, demonstrating superior performance in terms of model size and processing time. To further explore the performance of YOLO_MRC in multiclass insect detection tasks, we choose the public dataset Pest_24_640 for comparison with four state-of-the-art models. YOLO_MRC achieves a 3.6 ms processing time and 70.4% accuracy with only a 2.4 MB model size, which demonstrates the potential of YOLO_MRC in multiclass pest detection.}
}
@article{HERDY2024102417,
title = {Utilization of deep learning tools to map and monitor biological soil crusts},
journal = {Ecological Informatics},
volume = {79},
pages = {102417},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102417},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004466},
author = {Stefan Herdy and Emilio Rodríguez-Caballero and Thomas Pock and Bettina Weber},
keywords = {Semantic segmentation, Joint energy-based modelling, Deep learning, Biological soil crust, Domain adaption, Neural network, Long-term monitoring},
abstract = {Biological soil crusts (biocrusts) form a layer of only one to few centimeters depth on the soil surface and occur mostly in hot and cold deserts. Biocrusts have a major impact on different processes in these ecosystems, like carbon and nitrogen cycling, biodiversity preservation, erosion protection and soil dust emission reduction, but also react highly sensitive upon climate alterations and land use intensification. Therefore, monitoring tools are required to keep track of the changes of these specialized communities in an altering environment. In the current study, we applied a semantic image segmentation approach, using neural networks. One main problem to be solved was, that the training data and target data, on which the model is applied, are often recorded with different camera devices. This leads to different statistical properties of the image data, like different scale, resolution, brightness etc., which could significantly affect the model's performance. To solve this problem, we propose a new domain adaption method using a joint energy-based approach. To test a semantic segmentation approach in general, we utilized biocrust imagery taken in Utah (United States of America) and two sub datasets from the National Park Gesäuse (Austria). Here, we achieved highly reliable results with an overall classification accuracy of 85.9% for the USA data and 88.6% and 91.4%, respectively, for the two sub datasets of the National Park Gesäuse. To test our joint energy-based domain adaption approach, we used the two sub datasets from the National Park Gesäuse, which were recorded with different camera devices. With this newly established approach, we improved the accuracy of our segmentation on the unlabeled sub dataset from 70.4% to 75.3%. The results suggest that joint energy-based modelling is a well-suited domain adaption method for semantic segmentation that could be applied to face various deep learning and image-based biomonitoring challenges.}
}
@article{PELE2024102463,
title = {A neural network encoder-decoder for time series prediction: Application on 137Cs particulate concentrations in nuclearized rivers},
journal = {Ecological Informatics},
volume = {80},
pages = {102463},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102463},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000050},
author = {Kathleen Pelé and Valérie Nicoulaud-Gouin and Hugo Lepage},
keywords = {Deep learning, Suspended particulate matter, Radioactivity, Micropollutant, Data-driven model},
abstract = {Monitoring the impact of human activities on the environment is a major challenge as many pollutants can be found in the different ecosystems. This is the case of the caesium-137 that has been present in the environment for many decades as a result of atmospheric tests, accidents such as Chernobyl and release from nuclear industries. With the recent advance in data-driven models, this study evaluate the relevance of a deep learning tool for reconstructing caesium-137 chronics particulate concentration in rivers. An encoder-decoder neural network, “Hierarchical Attention-Based Recurrent Highway Networks”(HRHN), is proposed notably for its ability to extract the most relevant temporal and spatial information from the databases. Three monitoring stations were studied, one on the Rhône River and two on the Loire River, all of them downstream nuclear industries in these catchments affected by the global fallout and the accident of Chernobyl. The objective is to predict the future concentration from a set of variables providing past information on water discharge, washout flux and industrial radioactive releases. Once optimised, the model generates first results in agreement with the real concentration curves by correctly following the main trends, with a NSE of 0.89, 0.53 and 0.35 respectively for the Rhone station and the two stations on the Loire. The main reason of inaccuracies is due to the quantity of data available. The originality of this model is its capacity to make predictions on different catchment areas. In fact the training was conducted on the Rhône station as the range of the concentration was higher (from 265.4 to 2700.0 Bq/kg) and the testing on the two Loire station. Another encoder-decoder model DA-RNN (Dual-Stage Attention-Based Recurrent Neural Network) was also evaluate in order to compare the performance of an alternative architecture, without convolution layer. The conclusion is that HRHN remains more powerful in the predictions on the 3 systems. With these first interesting results for HRHN, further investigations should be taken into account for other pollutants than caesium-137 to better understand the robustness of the model.}
}
@article{LI2023102215,
title = {Tree trunk detection in urban scenes using a multiscale attention-based deep learning method},
journal = {Ecological Informatics},
volume = {77},
pages = {102215},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102215},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002443},
author = {Rao Li and GuoDong Sun and Sheng Wang and TianZhuzi Tan and Fu Xu},
keywords = {Salient object detection, Tree trunk detection, Deep learning, Attention mechanism, Dataset construction},
abstract = {Precise identification of tree trunks contributes to the understanding of urban green dynamics. Previous attempts to develop tree trunk detection methods have faced limitations in respect of precision and generalization due to the use of hand-engineered features and the constraint of single-species detection. In this study, we construct a new tree trunk dataset considering the object’s strong diversity and propose a deep model to detect and segment the salient tree trunks or even branches in urban scenes. Comprehensive experiments are performed to evaluate our model. The presented method exhibits exceptional performance, evidenced by its outstanding scores across seven evaluation metrics, indicating its capability to segment tree trunks of different species, even if they exhibit significant variations in appearance. Specifically, our model demonstrates outstanding accuracy in detecting trunks with intricate furcations, as well as effectively identifying trunks that are partially occluded.}
}
@article{CEIAHASSE2023102272,
title = {Forecasting the abundance of disease vectors with deep learning},
journal = {Ecological Informatics},
volume = {78},
pages = {102272},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102272},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003011},
author = {Ana Ceia-Hasse and Carla A. Sousa and Bruna R. Gouveia and César Capinha},
keywords = {Machine learning, Mosquito, Dengue, Forecast, Time series classification},
abstract = {Arboviral diseases such as dengue, Zika, chikungunya or yellow fever are a worldwide concern. The abundance of vector species plays a key role in the emergence of outbreaks of these diseases, so forecasting these numbers is fundamental in preventive risk assessment. Here we describe and demonstrate a novel approach that uses state-of-the-art deep learning algorithms to forecast disease vector abundances. Unlike classical statistical and machine learning methods, deep learning models use time series data directly as predictors and identify the features that are most relevant from a predictive perspective. We demonstrate for the first time the application of this approach to predict short-term temporal trends in the number of Aedes aegypti mosquito eggs across Madeira Island for the period 2013 to 2019. Specifically, we apply the deep learning models to predict whether, in the following week, the number of Ae. aegypti eggs will remain unchanged, or whether it will increase or decrease, considering different percentages of change. We obtained high predictive performance for all years considered (mean AUC = 0.92 ± 0.05 SD). Our approach performed better than classical machine learning methods. We also found that the preceding numbers of eggs is a highly informative predictor of future trends. Linking our approach to disease transmission or importation models will contribute to operational, early warning systems of arboviral disease risk.}
}
@article{MCEWEN2023102280,
title = {Automatic noise reduction of extremely sparse vocalisations for bioacoustic monitoring},
journal = {Ecological Informatics},
volume = {77},
pages = {102280},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102280},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003096},
author = {Ben McEwen and Kaspar Soltero and Stefanie Gutschmidt and Andrew Bainbridge-Smith and James Atlas and Richard Green},
keywords = {Audio enhancement, Bioacoustics, Noise reduction, Perceptual quality, Signal processing},
abstract = {Environmental noise and data sparsity present major challenges within the field of bioacoustics. The presence of noise degrades the analysis of audio and field recordings commonly containing large quantities of data with sparse vocalisation features. This work explores noise reduction (audio enhancement) techniques in the context of extremely sparse vocalisations (< 1% occurrence rates) of invasive mammalian and marsupial species, and the clear implications for other bioacoustics applications which face similar challenges. This work compares relevant noise reduction techniques and recommends a spectral subtraction approach. Spectral subtraction achieved a 42.1 dB improvement in signal to noise power (SnNR) and a 2.7 dB improvement in noise variance (SR). We also demonstrate reliable noise reduction at bandwidths up to 250 kHz and efficiency improvements compared to alternative methods. We explore the benefits of deep audio enhancement approaches demonstrating comparable noise reduction with improvements in transient noise reduction but also key limitations such as bandwidth, efficiency and data generation in bioacoustics applications. We identify how the contributions of this work can be applied within the broader context of bioacoustics. All data and code is publicly available at https://github.com/BenMcEwen1/Sparse-Noise-Reduction}
}
@article{JEANTET2023102256,
title = {Improving deep learning acoustic classifiers with contextual information for wildlife monitoring},
journal = {Ecological Informatics},
volume = {77},
pages = {102256},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102256},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002856},
author = {Lorène Jeantet and Emmanuel Dufourq},
keywords = {Bioacoustics, Deep learning, Convolutional neural networks, Passive acoustic monitoring, Species identification, Birds, Hainan gibbons},
abstract = {Bioacoustics, the exploration of animal vocalizations and natural soundscapes, has emerged as a valuable tool for studying species within their habitats, particularly those that are challenging to observe. This approach has broadened the horizons of biodiversity assessment and ecological research. However, monitoring wildlife with acoustic recorders produces large volumes of data that can be labor-intensive to analyze. Deep learning has recently transformed many computational disciplines by enabling the automated processing of large and complex datasets and has gained attention within the bioacoustics community. Despite the revolutionary impact of deep learning on acoustic detection and classification, attaining both high detection accuracy and low false positive rates in bioacoustics remains a significant challenge. An intriguing yet unexplored avenue for enhancing deep learning in bioacoustics involves the utilization of contextual information, such as time and location, to discern animal vocalizations within acoustic recordings. As a first case study, a multi-branch Convolutional Neural Network (CNN) was developed to classify 22 different bird songs using spectrograms as a first input, and spatial metadata as a secondary input. A comparison was made to a baseline model with only spectrogram input. A geographical prior neural network was trained, separately, to estimate the probability of a species occurring at a given location. The output of this network was combined with the baseline CNN. As a second case study, temporal data and spectrograms were used as input to a multi-branch CNN for the detection of Hainan gibbon (Nomascus hainanus) calls, the world’s rarest primate. Our findings demonstrate that adding metadata to the bird song classifier significantly improves classification performance, with the highest improvement achieved using the geographical prior model (F1-score of 87.78% compared to 61.02% for the baseline model). The multi-branch CNNs also proved efficient (F1-scores of 76.87% and 78.77%) and simpler to use than the geographical prior. In the second case study, our findings revealed a decrease in false positives by 63% (94% of the calls were detected) when the metadata was used by the multi-branch CNN, and an increase of 19% in gibbon detection. This study has uncovered an exciting new avenue for improving classifier performance in bioacoustics. The methodology described in this study can assist ecologists, wildlife management teams, and researchers in reducing the amount of time spent analyzing large acoustic datasets obtained from passive acoustic monitoring studies. Our approach can be adapted and applied to other calling species, and thus tailored to other use cases.}
}
@article{AKINOSHO2022101609,
title = {A scalable deep learning system for monitoring and forecasting pollutant concentration levels on UK highways},
journal = {Ecological Informatics},
volume = {69},
pages = {101609},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101609},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122000589},
author = {Taofeek D. Akinosho and Lukumon O. Oyedele and Muhammad Bilal and Ari Y. Barrera-Animas and Abdul-Quayyum Gbadamosi and Oladimeji A. Olawale},
keywords = {Urban air pollution, Air quality prediction, Highway, Deep learning, Big data, Internet of things},
abstract = {The construction of intercity highways by the government has resulted in a progressive increase in vehicle emissions and pollution from noise, dust, and vibrations despite its recognition of the air pollution menace. Efforts that have targeted roadside pollution still do not accurately monitor deadly pollutants such as nitrogen oxides and particulate matter. Reports on regional highways across the country are based on a limited number of fixed monitoring stations that are sometimes located far from the highway. These periodic and coarse-grained measurements cause inefficient highway air quality reporting, leading to inaccurate air quality forecasts. This paper, therefore, proposes and validates a scalable deep learning framework for efficiently capturing fine-grained highway data and forecasting future concentration levels. Highways in four different UK regions - Newport, Lewisham, Southwark, and Chepstow were used as case studies to develop a REVIS system and validate the proposed framework. REVIS examined the framework's ability to capture granular pollution data, scale up its storage facility to rapid data growth and translate high-level user queries to structured query language (SQL) required for exploratory data analysis. Finally, the framework's suitability for predictive analytics was tested using fastai's library for tabular data, and automated hyperparameter tuning was implemented using bayesian optimisation. The results of our experiments demonstrate the suitability of the proposed framework in building end-to-end systems for extensive monitoring and forecasting of pollutant concentration levels on highways. The study serves as a background for future related research looking to improve the overall performance of roadside and highway air quality forecasting models.}
}
@article{MA2024102651,
title = {UAV equipped with infrared imaging for Cervidae monitoring: Improving detection accuracy by eliminating background information interference},
journal = {Ecological Informatics},
volume = {81},
pages = {102651},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102651},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001936},
author = {Guangkai Ma and Wenjiao Li and Heng Bao and Nathan James Roberts and Yang Li and Weihua Zhang and Kun Yang and Guangshun Jiang},
keywords = {YOLOv7, ViT, Wild Cervidae monitoring},
abstract = {Wild Cervidae(deer and their relatives) play a crucial role in maintaining ecological balance and are integral components of ecosystems. However, factors such as environmental changes and poaching behaviors have resulted in habitat degradation for Cervidae. The protection of wild Cervidae has become urgent, and Cervidae monitoring is one of the key means to ensure the effectiveness of wild Cervidae protection. Object detection algorithms based on deep learning offer promising potential for automatically detecting and identifying animals. However, when those algorithms are used for inference in unseen background environments, there will be a significant decrease in accuracy, especially in the situation that a certain type of Cervidae images are collected from single scene for algorithm training. In this paper, a two-stage localization and classification pipeline for Cervidae monitoring is proposed. The pipeline effectively reduces background interference in Cervidae monitoring and enhances monitoring accuracy. In the first stage, the YOLOv7 network is designed to automatically locate Cervidae in UAV infrared images, while implementing improved bounding box regression through the α-IoU loss function enables the network to locate Cervidae more accurately. Then, Cevdidae objects are extracted to eliminate the background information. In the second stage, a classification network named CA-Hybrid, based on Convolutional Neural Networks(CNN) and Vision Transformer(ViT), as well as Channel Attention Mechanism(CAM) enhances the expression of key features, is constructed to accurately identify Cervidae categories. Experimental results indicate that this method achieves an Average Precision (AP) of 95.9% for Cervidae location and a top-1 accuracy of 77.73% for Cervidae identification. This research contributes to a more comprehensive and accurate monitoring of wild Cervidae, and provides valuable references for subsequent UAV-based wildlife monitoring.}
}
@article{MORITAKE2024102462,
title = {Sub-alpine shrub classification using UAV images: Performance of human observers vs DL classifiers},
journal = {Ecological Informatics},
volume = {80},
pages = {102462},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102462},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000049},
author = {Koma Moritake and Mariano Cabezas and Tran Thi Cam Nhung and Maximo Larry {Lopez Caceres} and Yago Diez},
keywords = {Sub-alpine vegetation, Vegetation change monitoring, Deep learning, Observer study, ConvNeXt, Swin},
abstract = {In recent years, the automatic analysis of natural environment images acquired with unmanned aerial vehicles (UAV) has rapidly gained popularity. UAVs are specially important in mountainous forests where access is difficult and large areas need to be surveyed. In Zao mountains in northeastern Japan, regenerated fir saplings are competing with sub-alpine vegetation shrubs after a severe fir tree mortality caused by bark beetle infestation. A detailed survey of vegetation distribution is key to improve our understanding of species succession and the influence of climate change in that process. To that end, we evaluated the suitability of deep-learning-based automatic image classification of UAV images in order to map sub-alpine vegetation succession in large areas and the potential of fir regeneration. In order to assess the contribution of this technology in this research field, we first conducted an observer study to assess the difficulty for humans of the task of classifying vegetation from images. Afterwards, we compared the observers' accuracy to four state-of-the art deep learning networks for automatic image classification. The best observer accuracy of 55% demonstrates the limitations of species classification using only images. Furthermore, a detailed analysis of the sources of error showed that even though humans could differentiate between deciduous and evergreen species with an accuracy of 96%, identifying the correct species within each group proved much more challenging. In contrast, deep learning networks achieved accuracy values in the range of 70–80% for species classification, clearly demonstrating capabilities beyond human experts. Our experiments also indicated that the performance of these networks was significantly influenced by the similarity between the datasets used to fine-tune them and evaluate them. This fact highlights the importance of building publicly available images databases to further improve the results. Nevertheless, the results presented in this paper show that the analysis of UAV-acquired with deep learning networks can usher in a new type of large-scale study, spanning tenths or even hundreds of hectares with high spatial resolution (of a few cms per pixel), providing the ability to assess challenging vegetation dynamics problems that go beyond the ability of conventional fieldwork methodologies.}
}
@article{AREPALLI2024102405,
title = {Water contamination analysis in IoT enabled aquaculture using deep learning based AODEGRU},
journal = {Ecological Informatics},
volume = {79},
pages = {102405},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102405},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300434X},
author = {Peda Gopi Arepalli and K. Jairam Naik},
keywords = {Water quality, Water contamination, Water contamination index (WCI), Gated recurrent unit (GRU), Internet of things (IoT)},
abstract = {Water contamination presents a significant challenge in aquaculture, impacting the sustainability of ecosystems and the health of aquatic organisms. Precisely assessing water contamination levels is crucial for effective monitoring and safeguarding aquatic life within the aquaculture industry. Traditional methods for evaluating water contamination are characterized by their costliness, time-consuming nature, and susceptibility to errors. Integrating computer technologies such as Artificial Intelligence (AI), the Internet of Things (IoT), and Data Analytics offers promising potential in addressing this issue. Nevertheless, current deep learning solutions have limitations related to data variability, interpretability, and performance. To address these limitations, this study proposes a comprehensive framework that incorporates IoT-based data collection and data segregation techniques to enhance the accuracy of water contamination classification in aquaculture. Real-time data collected through IoT devices, encompassing parameters like temperature, pH levels, dissolved oxygen, nitrate concentration, and other water quality indicators, enables a holistic evaluation of water quality. By considering predefined acceptable ranges for aquatic life, this framework calculates a water contamination index, facilitating the classification of data into categories such as contaminated and non-contaminated. To ensure robust classification, the study introduces an innovative attention-based model known as the Ordinary Differential Equation Gated Recurrent Unit (AODEGRU). This attention mechanism directs the model's focus towards salient features associated with water contamination, while the AODEGRU architecture captures temporal patterns within the data. Experimental results underscore the effectiveness of the proposed model. It demonstrates its superiority with high performance, achieving an accuracy rate of approximately 98.69% on a publicly available dataset and an impressive 99.89% accuracy on a real-time dataset, clearly outperforming existing methodologies.}
}
@article{MATA2024102708,
title = {Drone imagery and deep learning for mapping the density of wild Pacific oysters to manage their expansion into protected areas},
journal = {Ecological Informatics},
volume = {82},
pages = {102708},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102708},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002504},
author = {Aser Mata and David Moffat and Sílvia Almeida and Marko Radeta and William Jay and Nigel Mortimer and Katie Awty-Carroll and Oliver R. Thomas and Vanda Brotas and Steve Groom},
keywords = {Pacific oysters, Invasive species, Convolutional neural networks, Deep learning, Drone, Remote sensing, Ecological management},
abstract = {The recent expansion of wild Pacific oysters already had negative repercussions on sites in Europe and has raised further concerns over their potential harmful impact on the balance of biomes within protected areas. Monitoring their colonisation, especially at early stages, has become an urgent ecological issue. Current efforts to monitor wild Pacific oysters rely on “walk-over” surveys that are highly laborious and often limited to specific areas of easy access. Remotely Piloted Aircraft Systems (RPAS), commonly known as drones, can provide an effective tool for surveying complex terrains and detect Pacific oysters. This study provides a novel workflow for automated detection, counting and mapping of individual Pacific oysters to estimate their density per square meter by using Convolutional Neural Networks (CNNs) applied to drone imagery. Drone photos were collected at low tides and altitudes of approximately 10 m across a variety of cases of rocky shore and mudflats scenarios. Using object detection, we compared how different Convolutional Neural Networks (CNNs) architectures including YOLOv5s, YOLOv5m, TPH-YOLOv5 and FR-CNN performed in the detection of Pacific oysters over the surveyed areas. We report the precision of our model at 88% with a difference in performance of 1% across the two sites. The workflow presented in this work proposes the use of grid maps to visualize the density of Pacific oysters per square meter towards ecological management and the creation of time series to identify trends.}
}
@article{JAMSHIDI2024102595,
title = {Predicting oil palm yield using a comprehensive agronomy dataset and 17 machine learning and deep learning models},
journal = {Ecological Informatics},
volume = {81},
pages = {102595},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102595},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001377},
author = {Ehsan Jolous Jamshidi and Yusri Yusup and Chee Wooi Hooy and Mohamad Anuar Kamaruddin and Hasnuri {Mat Hassan} and Syahidah Akmal Muhammad and Helmi Zulhaidi {Mohd Shafri} and Kek Hoe Then and Mohd Shahkhirat Norizan and Choon Chek Tan},
keywords = {Oil palm, Yield prediction, Machine learning, Deep learning, Agriculture},
abstract = {The rising global demand for oil palm emphasizes the importance of accurate oil palm yield predictions. This predictive capability is critical for effective crop management, supply chain optimization, and sustainable farming practices. However, the oil palm sector faces challenges in yield projection, stressing a noteworthy gap in the application and evaluation of modern machine learning and deep learning technologies. Our study addressed this gap by systematically evaluating 17 machine and deep learning models in predicting oil palm yield, incorporating various agronomic parameters, e.g., soil composition, climatic conditions, plant age, and farming techniques. This holistic approach enhances the application of machine and deep learning in agriculture. Using the feature selection technique and a maximum depth of 32 and 1000 estimators, the Extra Trees Regressor exhibited positive performance, i.e., MSE = 860.36 and an R2 = 0.65, and stands out among the 17 models evaluated. Our findings also showed that incorporating a comprehensive agronomic dataset is critical to accurate yield prediction. Hence, this model and approach have the potential to be a robust decision-making tool for agronomists and farmers in the oil palm industry, setting the stage for future innovations in sustainable agriculture practices.}
}
@article{ZBINDEN2024102623,
title = {On the selection and effectiveness of pseudo-absences for species distribution modeling with deep learning},
journal = {Ecological Informatics},
volume = {81},
pages = {102623},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102623},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001651},
author = {Robin Zbinden and Nina {van Tiel} and Benjamin Kellenberger and Lloyd Hughes and Devis Tuia},
keywords = {Species distribution modeling, Neural networks, Presence-only data, Pseudo-absences, Deep learning},
abstract = {Species distribution modeling is a highly versatile tool for understanding the intricate relationship between environmental conditions and species occurrences. However, the available data often lacks information on confirmed species absence and is limited to opportunistically sampled, presence-only observations. To overcome this limitation, a common approach is to employ pseudo-absences, which are specific geographic locations designated as negative samples. While pseudo-absences are well-established for single-species distribution models, their application in the context of multi-species neural networks remains underexplored. Notably, the significant class imbalance between species presences and pseudo-absences is often left unaddressed. Moreover, the existence of different types of pseudo-absences (e.g., random and target-group background points) adds complexity to the selection process. Determining the optimal combination of pseudo-absences types is difficult and depends on the characteristics of the data, particularly considering that certain types of pseudo-absences can be used to mitigate geographic biases. In this paper, we demonstrate that these challenges can be effectively tackled by integrating pseudo-absences in the training of multi-species neural networks through modifications to the loss function. This adjustment involves assigning different weights to the distinct terms of the loss function, thereby addressing both the class imbalance and the choice of pseudo-absence types. Additionally, we propose a strategy to set these loss weights using spatial block cross-validation with presence-only data. We evaluate our approach using a benchmark dataset containing independent presence-absence data from six different regions and report improved results when compared to competing approaches.}
}
@article{JANSIRANI2024102663,
title = {A novel automated approach for fish biomass estimation in turbid environments through deep learning, object detection, and regression},
journal = {Ecological Informatics},
volume = {81},
pages = {102663},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102663},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400205X},
author = {S.V. {Jansi Rani} and Iacovos Ioannou and R. Swetha and R.M. {Dhivya Lakshmi} and Vasos Vassiliou},
keywords = {Fish biomass, Object detection, Regression, YOLOv8, mYOLOv8},
abstract = {Estimating fish biomass is crucial in the fisheries sector, where traditional methods often harm fish through manual sampling and anesthetics. A non-invasive approach is introduced using underwater films to estimate fish biomass in turbid conditions. This study presents the “Aquatic WeightNet” dataset, targeting the Genetically Improved Formed Tilapia (GIFT) Tilapia species, and addresses the challenge of unclear images with preprocessing techniques like dehazing and Contrast Limited Adaptive Histogram Equalization (CLAHE). YOLOv8, a leading object detection model modified to accommodate the custom Aquatic WeightNet dataset's varied image sizes with five detection heads, P2 to P6, is employed, achieving a recall of 0.997 and a mean Average Precision (mAP) of 0.899 within the 50–95% Intersection over Union (IoU) range. Fish biomass estimation assesses depth, length, and width using regression models for calculation. A three-phase grid search identifies the most effective models, with the Extra Trees Regressor outperforming depth estimation with mean absolute error (MAE) of 0.63 and coefficient of determination (R2) of 0.87 and the Random Forest Regressor for length and width (MAE of 0.01 and R2 of 0.99). For biomass estimation, the Extra Trees Regressor again performs well (MAE of 0.004 and R2 of 0.99), which is critical for determining optimal feed quantities to enhance aquaculture efficiency. This study emphasizes a non-invasive method to estimate fish biomass, optimizing the effectiveness and ecological sustainability of fish farming in murky waters through advanced detection algorithms and robust regression models.}
}
@article{NGUYEN2024102744,
title = {Improving pollen-bearing honey bee detection from videos captured at hive entrance by combining deep learning and handling imbalance techniques},
journal = {Ecological Informatics},
pages = {102744},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102744},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002863},
author = {Dinh-Tu Nguyen and Thi-Nhung Le and Thi-Huong Phung and Duc-Manh Nguyen and Hong-Quan Nguyen and Hong-Thai Pham and Thi-Thu-Hong Phan and  Vu-Hai and Thi-Lan Le},
keywords = {Pollen foraging behavior, Pollen-bearing honey bee detection},
abstract = {The number of pollen-bearing honey bees serves as a vital indicator for assessing colony balance and health. Despite its significance, prevailing detection techniques still rely heavily on manual observation and annotation, leading to time-consuming processes that cannot sustain long-term, continuous monitoring efforts. To facilitate automatic beehive monitoring, this study introduces an efficient method for pollen-bearing bee detection. Initially, we furnish a comprehensive dataset, dubbed VnPollenBee, meticulously annotated for pollen-bearing honey bee detection and classification. The dataset comprises 60,826 annotated boxes that delineate both pollen-bearing and non-pollen-bearing bees in 2051 images captured at the entrances of beehives under various environmental conditions. To the best of our knowledge, this represents the first dedicated dataset for pollen-bearing bee detection. The VnPollenBee dataset is publicly accessible to the research community at https://comvis-hust.github.io/datasets/pollenbee.html. Subsequently, we propose the incorporation of diverse techniques into two baseline models, namely YOLOv5 and Faster RCNN, to effectively address the imbalance that arises during the detection of pollen-bearing bees due to their number being typically much lower than the total number of bees present at hive entrances. The experimental results demonstrate that our proposed method outperforms the baseline models on the VnPollenBee dataset, yielding Precision, Recall, and F1 score of 99%, 93%, and 95%, respectively. Specifically, the improvements obtained are 3% and 2% in Recall and F1 score when using YOLOv5, and 3%, 2%, and 2% in Precision, Recall, and F1 score when using Faster RCNN. These findings confirm the potential of our approach to facilitate bee foraging behavior analysis and automated bee monitoring.}
}
@article{KLADNY2024102474,
title = {Enhanced prediction of vegetation responses to extreme drought using deep learning and Earth observation data},
journal = {Ecological Informatics},
volume = {80},
pages = {102474},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102474},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000165},
author = {Klaus-Rudolf Kladny and Marco Milanta and Oto Mraz and Koen Hufkens and Benjamin D. Stocker},
keywords = {Drought impact forecasting, Sentinel-2, EarthNet2021, ConvLSTM, NDVI},
abstract = {The advent of abundant Earth observation data enables the development of novel predictive methods for forecasting climate impacts on the state and health of terrestrial ecosystems. Here, we predict the spatial and temporal variations of land surface reflectance and vegetation greenness, measuring the density of green vegetation and active foliage area, conditioned on current and past weather and the local topography. We train two alternative recurrent deep learning models that combine Long Short-Term Memory cells with convolutional layers (ConvLSTM) for forecasting the spatially resolved deviation of surface reflectance across a heterogeneous landscape from a specified initial state. Using data from diverse ecosystems and land cover types across Europe and following a standardized model evaluation framework (EarthNet2021 Challenge), our results indicate increased performance in predicting surface greenness during extreme drought events of the models presented here, compared to currently published benchmarks. This demonstrates how deep learning methods for optical Earth observation time series enable an early-warning of vegetation responses to the impacts of climatic extreme events, such as the drought-related loss of green foliage.}
}
@article{DING2024102664,
title = {Algal blooms forecasting with hybrid deep learning models from satellite data in the Zhoushan fishery},
journal = {Ecological Informatics},
volume = {82},
pages = {102664},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102664},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002061},
author = {Wenxiang Ding and Changlin Li},
keywords = {Hybrid deep learning model, Algal bloom, Chlorophyll, Forecast, Zhoushan fishery},
abstract = {Algal blooms are increasingly frequent in coastal areas, posing a significant threat to coastal ecosystems. The Zhoushan fishery, one of the most affected regions along the Chinese coast, faces severe challenges from algal blooms. In this study, Convolutional Neural Network (CNN), Long Short-term Memory (LSTM) and hybrid CNN-LSTM deep learning models were constructed to forecast chlorophyll (Chl) concentrations and algal blooms from satellite data. The hybrid CNN-LSTM model outperformed the individual models, achieving the highest determination coefficient and the lowest root mean square error for Chl concentration forecasts. It also excelled in predicting algal blooms, with the highest probability of detection and Heidke skill score, effectively capturing the trends in algal bloom development. In areas with high Chl concentration, the Chl parameter significantly influences model forecasts, while meridional wind and current are the main influence factors in the regions with medium and low Chl concentration. The powerful algal bloom forecast provided by the hybrid CNN-LSTM model offers valuable support for the efficient management and sustainable development of the Zhoushan fishery.}
}
@article{ZHANG2024102394,
title = {Evaluation of digital soil mapping projection in soil organic carbon change modeling},
journal = {Ecological Informatics},
volume = {79},
pages = {102394},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102394},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004235},
author = {Tao Zhang and Lai-Ming Huang and Ren-Min Yang},
keywords = {Soil carbon change, Digital soil mapping, Model projections, Environmental change, Temporal transferability},
abstract = {There is increasing interest in the application of digital soil mapping (DSM) projections to infer changes in soil carbon across both space and time. This approach relies on the assumption that the spatially modeled soil carbon-environment relationship can be transferred over time. However, this assumption is rarely tested due to a lack of temporally independent validation data. This paper assesses this assumption by developing models of topsoil organic carbon stocks (SOCS) with a deep learning algorithm and data covering mainland China pertaining to the 1980s and 2010s. The temporal prediction performance of models capturing a specific period was assessed by evaluating their performance in the prediction of data during another period. The results revealed that the prediction accuracy of temporal modeling decreased, as indicated by the coefficient of determination, and was lower than that of spatial modeling. The lower prediction accuracy obtained with the DSM-projection approach may result from differences in the magnitudes of influential variables across periods. We found that different levels of environmental similarity and model projection sensitivity to dynamic variables may cause discrepancies in forecast and hindcast accuracy. Our results demonstrate that the prediction error in temporal modeling is related to the degree of environmental similarity between periods. Our findings generally support the implementation of the DSM-projection approach in soil carbon change modeling. However, caution should be exercised, as there exists much uncertainty regarding the projection of spatial models over time.}
}
@article{DWIVEDI2024102451,
title = {EMViT-Net: A novel transformer-based network utilizing CNN and multilayer perceptron for the classification of environmental microorganisms using microscopic images},
journal = {Ecological Informatics},
volume = {79},
pages = {102451},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102451},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004806},
author = {Karnika Dwivedi and Malay Kishore Dutta and Jay Prakash Pandey},
keywords = {Environmental microorganisms classification, Microscopic images, Computer-aided system, Deep learning, Vision transformer},
abstract = {Environmental microbes are certainly present in our surroundings since they are essential to the growth and survival of human advancement. The detailed analysis of environmental microorganisms (EMs) is very important to recognize, understand and make use of microbes as well and prevent damage. Extracting the discriminatory features from a limited-size dataset is very challenging for a deep learning model and a pure transformer-based network cannot achieve good classification results on a limited-size dataset due to the lack of muti-scale features. In this study, a novel vision transformer-based deep neural network is proposed by integrating the transformer with CNN for the classification of EM using microscopic images. The proposed network EMViT-Net has three main modules: a transformer module, a CNN module and a multilayer perceptron module. The transformer model extracted multiscale features to generate more discriminatory information from the images. A new separable convolutional parameter-sharing attention (SCPSA) block is integrated with the CNN module in the core of EMViT-Net, which makes the model robust to capture the local and global features, and simultaneously reduces the computational complexity of the model. The data augmentation is performed to introduce the variability in the dataset and counter the problem of overfitting and data imbalance. After extensive experiments and detailed analysis, it has been determined that the proposed model EMViT-Net outperforms the other existing methods and achieves state-of-the-art results with an accuracy of 71.17% which proves the effectiveness of the model for the classification of environmental microbes.}
}
@article{BAKANA2024102541,
title = {WildARe-YOLO: A lightweight and efficient wild animal recognition model},
journal = {Ecological Informatics},
volume = {80},
pages = {102541},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102541},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000839},
author = {Sibusiso Reuben Bakana and Yongfei Zhang and Bhekisipho Twala},
keywords = {Wild animal recognition, Deep learning, Lightweight, Efficient, Loss function},
abstract = {For the protection of endangered species and successful wildlife population monitoring, wild animal recognition is essential. While deep learning models like YOLOv5 have shown promise in real-time object recognition, their practical applicability may be constrained by their high processing requirements. In this paper, we suggest a faster and lighter version of YOLOv5s for wild animal recognition. To lower computational costs for model parameters and floating-point operations (FLOPs) for the backbone, our suggested model includes Mobile Bottleneck Block modules and an improved StemBlock. We also use Focal-EIoU as a loss function to gauge the accuracy of the predicted bounding boxes during inference and employ a BiFPN-based neck. We tested our technique on three datasets, including Wild Animal Facing Extinction, Fishmarket, and MS COCO 2017. Additionally, our technique is compared with state-of-the-art deep learning models, and from the baseline model we recorded a 17.65% increase in FPS, 28.55% model parameters reduction, and 50.92% in FLOPs reduction. Furthermore, our model has a faster model loading time, which is critical for deployment in remote areas. This enables real-time species recognition on basic hardware, aiding conservation efforts through rapid analysis. The model advances deep learning in ecology by balancing efficiency with performance.}
}
@article{DELPLANQUE2024102679,
title = {Will artificial intelligence revolutionize aerial surveys? A first large-scale semi-automated survey of African wildlife using oblique imagery and deep learning},
journal = {Ecological Informatics},
volume = {82},
pages = {102679},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102679},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002218},
author = {Alexandre Delplanque and Julie Linchant and Xavier Vincke and Richard Lamprey and Jérôme Théau and Cédric Vermeulen and Samuel Foucher and Amara Ouattara and Roger Kouadio and Philippe Lejeune},
keywords = {Wildlife population estimation, Aerial surveys, Deep learning, Biodiversity monitoring, Conservation technology, African savanna},
abstract = {Large African mammal populations are traditionally estimated using the systematic reconnaissance flights (SRF) with rear-seat observers (RSOs). The oblique-camera-count (OCC) approach, utilizing digital cameras on aircraft sides, proved to provide more reliable population estimates but incurs high manual processing costs. Addressing the urgent need for efficiency, the research explores whether a semi-automated deep learning (SADL) model coupled with OCC improves wildlife population estimates compared to the SRF-RSO method. The study area was the Comoé National Park, in Ivory Coast, spanning 11,488 km2 of savannas and open forests. It was surveyed following both SRF-RSO standards and OCC method. Key species included the elephant, western hartebeest, roan antelope, buffalo, kob, waterbuck and warthog. The deep learning model HerdNet, priorly pre-trained on images from Uganda, was incorporated in the SADL pipeline to process the 190,686 images. It involved three human verification steps to ensure quality of detections and to avoid overestimating counts. The entire pipeline aims to balance efficiency and human effort in wildlife population estimation. RSO and SADL-OCC approaches were compared using the Jolly II analysis and a verification of 200 random RSO observations. Jolly II analysis revealed SADL-OCC estimates significantly higher for small-sized species (kob, warthog) and comparable for other key species. Counting differences were mainly attributed to vegetation obstruction, RSO observations not found in the images, and suspected RSO counting errors. Human effort in the SADL-OCC approach totaled 111 h, representing a significant time savings compared to a fully manual interpretation. Introducing the SADL approach for aerial surveys in Comoé National Park enabled us to address the OCC's time-intensive image interpretation. Achieving a significant reduction in human workload, our method provided population estimates comparable to or better than SRF-RSO counts. Vegetation obstruction was a key factor explaining differences, highlighting the OCC method's limitation in vegetated areas. Method comparisons emphasized SADL-OCC's advantages in spotting isolated, small and static animals, reducing count variance between sample units. Despite limitations, the SADL-OCC approach offers transformative potential, suggesting a shift towards DL-assisted aerial surveys for increased efficiency and affordability, especially using microlight aircraft and drones in future wildlife monitoring initiatives.}
}
@article{ORIOL2024102606,
title = {Automatic identification of Collembola with deep learning techniques},
journal = {Ecological Informatics},
volume = {81},
pages = {102606},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102606},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001481},
author = {Théo Oriol and Jérôme Pasquet and Jérôme Cortet},
keywords = {Deep learning, Object detection, Collembola, Soil quality, Bioindication},
abstract = {Collembola are very abundant organisms in soils (several thousand individuals per square meter) and are considered to be good indicators of soil quality. These indicators are mainly based on the number of individuals observed (abundance per square meter of soil), but also the singularity and number of species present (species richness). A limitation that comes with the usage of collembola as an indicator is the complexity of the identification of the species under a microscope, how time-consuming it is, and the morphological similarity between some species. Deep learning approaches have been very successful in the resolution of image-based problems. Still, no work yet exists that uses deep learning in the recognition of collembola on a microscope slide. This could be a valuable tool for experts seeking to use Collembola as a metric on a larger scale. In this work, we explore and evaluate the performance of state-of-the-art deep learning techniques over the identification of Collembola on a new manually annotated dataset.}
}
@article{DUFOURQ2022101688,
title = {Passive acoustic monitoring of animal populations with transfer learning},
journal = {Ecological Informatics},
volume = {70},
pages = {101688},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101688},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122001388},
author = {Emmanuel Dufourq and Carly Batist and Ruben Foquet and Ian Durbach},
keywords = {Transfer learning, Convolutional neural networks, Deep learning, Vocalisation classification, Bioacoustics},
abstract = {Progress in deep learning, more specifically in using convolutional neural networks (CNNs) for the creation of classification models, has been tremendous in recent years. Within bioacoustics research, there has been a large number of recent studies that use CNNs. Designing CNN architectures from scratch is non-trivial and requires knowledge of machine learning. Furthermore, hyper-parameter tuning associated with CNNs is extremely time consuming and requires expensive hardware. In this paper we assess whether it is possible to build good bioacoustic classifiers by adapting and re-using existing CNNs pre-trained on the ImageNet dataset – instead of designing them from scratch, a strategy known as transfer learning that has proved highly successful in other domains. This study is a first attempt to conduct a large-scale investigation on how transfer learning can be used for passive acoustic monitoring (PAM), to simplify the implementation of CNNs and the design decisions when creating them, and to remove time consuming hyper-parameter tuning phases. We compare 12 modern CNN architectures across 4 passive acoustic datasets that target calls of the Hainan gibbon Nomascus hainanus, the critically endangered black-and-white ruffed lemur Varecia variegata, the vulnerable Thyolo alethe Chamaetylas choloensis, and the Pin-tailed whydah Vidua macroura. We focus our work on data scarcity issues by training PAM binary classification models very small datasets, with as few as 25 verified examples. Our findings reveal that transfer learning can result in up to 82% F1 score while keeping CNN implementation details to a minimum, thus rendering this approach accessible, easier to design, and speeding up further vocalisation annotations to create PAM robust models.}
}
@article{GRIJALVA2024102540,
title = {Detecting and counting sorghum aphid alates using smart computer vision models},
journal = {Ecological Informatics},
volume = {80},
pages = {102540},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102540},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000827},
author = {Ivan Grijalva and H. Braden Adams and Nicholas Clark and Brian McCornack},
keywords = {sorghum, sorghum aphid, alates, automation, detection},
abstract = {Sorghum aphid [Melanaphis sorghi (Theobald)] is considered an economic pest causing significant yield losses in susceptible sorghum in the southern U.S. Infestations start with the migration of alates (i.e., winged adults) to sorghum and establishing aphid colonies. In favorable conditions, sorghum aphid can exponentially reproduce via asexual reproduction. A suggested strategy is to monitor alates to determine initial infestations and take preventive strategies, which can result in more efficient pest monitoring and management. To reduce the time of monitoring and better understand of alate establishment under field conditions, we propose using computer vision models, specifically deep learning, to detect and count alates using field-collected images. During pest monitoring, we captured 2527 images and assessed the performance of five models within the YOLOv5 architecture family using two different image sizes, including input resolutions of 640 × 640, and 1280 × 1280 pixels. We trained models to detect and count individual alates, which ranged between 1 and 100 alates/leaf. Among models, the YOLOv5l Pytorch detection model had the best overall performance at 1280 × 1280 input pixel resolution. The YOLOv5l model is a candidate model for quantifying alates on sorghum leaves using deep learning with a precision of 83.80%, 85.60% recall, and 89% mAP@0.5 with a lower mean percent error of misdetection. To enable the use of our best deep learning model by the research community, we developed a web-based application that is freely available to the public. Using this application, users can upload images to detect and count alates with a low error of misdetection.}
}
@article{KAHL2021101236,
title = {BirdNET: A deep learning solution for avian diversity monitoring},
journal = {Ecological Informatics},
volume = {61},
pages = {101236},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101236},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000273},
author = {Stefan Kahl and Connor M. Wood and Maximilian Eibl and Holger Klinck},
keywords = {Bioacoustics, Deep learning, Convolutional neural networks, Bird sound recognition, Avian diversity, Passive acoustic monitoring, Conservation},
abstract = {Variation in avian diversity in space and time is commonly used as a metric to assess environmental changes. Conventionally, such data were collected by expert observers, but passively collected acoustic data is rapidly emerging as an alternative survey technique. However, efficiently extracting accurate species richness data from large audio datasets has proven challenging. Recent advances in deep artificial neural networks (DNNs) have transformed the field of machine learning, frequently outperforming traditional signal processing techniques in the domain of acoustic event detection and classification. We developed a DNN, called BirdNET, capable of identifying 984 North American and European bird species by sound. Our task-specific model architecture was derived from the family of residual networks (ResNets), consisted of 157 layers with more than 27 million parameters, and was trained using extensive data pre-processing, augmentation, and mixup. We tested the model against three independent datasets: (a) 22,960 single-species recordings; (b) 286 h of fully annotated soundscape data collected by an array of autonomous recording units in a design analogous to what researchers might use to measure avian diversity in a field setting; and (c) 33,670 h of soundscape data from a single high-quality omnidirectional microphone deployed near four eBird hotspots frequented by expert birders. We found that domain-specific data augmentation is key to build models that are robust against high ambient noise levels and can cope with overlapping vocalizations. Task-specific model designs and training regimes for audio event recognition perform on-par with very complex architectures used in other domains (e.g., object detection in images). We also found that high temporal resolution of input spectrograms (short FFT window length) improves the classification performance for bird sounds. In summary, BirdNET achieved a mean average precision of 0.791 for single-species recordings, a F0.5 score of 0.414 for annotated soundscapes, and an average correlation of 0.251 with hotspot observation across 121 species and 4 years of audio data. By enabling the efficient extraction of the vocalizations of many hundreds of bird species from potentially vast amounts of audio data, BirdNET and similar tools have the potential to add tremendous value to existing and future passively collected audio datasets and may transform the field of avian ecology and conservation.}
}
@article{CAPINHA2021101252,
title = {Deep learning for supervised classification of temporal data in ecology},
journal = {Ecological Informatics},
volume = {61},
pages = {101252},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101252},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000431},
author = {César Capinha and Ana Ceia-Hasse and Andrew M. Kramer and Christiaan Meijer},
keywords = {Deep learning, Ecological prediction, Scalability, Sequential data, Temporal ecology, Time series},
abstract = {Temporal data is ubiquitous in ecology and ecologists often face the challenge of accurately differentiating these data into predefined classes, such as biological entities or ecological states. The usual approach consists of transforming the time series into user-defined features and then using these features as predictors in conventional statistical or machine learning models. Here we suggest the use of deep learning models as an alternative to this approach. Recent deep learning techniques can perform the classification directly from the time series, eliminating subjective and resource-consuming data transformation steps, and potentially improving classification results. We describe some of the deep learning architectures relevant for time series classification and show how these architectures and their hyper-parameters can be tested and used for the classification problems at hand. We illustrate the approach using three case studies from distinct ecological subdisciplines: i) insect species identification from wingbeat spectrograms; ii) species distribution modelling from climate time series and iii) the classification of phenological phases from continuous meteorological data. The deep learning approach delivered ecologically sensible and accurate classifications demonstrating its potential for wide applicability across subfields of ecology.}
}
@article{CHAKRABARTY2024102718,
title = {An interpretable fusion model integrating lightweight CNN and transformer architectures for rice leaf disease identification},
journal = {Ecological Informatics},
volume = {82},
pages = {102718},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102718},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002607},
author = {Amitabha Chakrabarty and Sarder Tanvir Ahmed and Md. Fahim Ul Islam and Syed Mahfuzul Aziz and Siti Sarah Maidin},
keywords = {Plant disease detection, BEiT model, Attention mapping, Deep learning, Process innovation, Food productivity},
abstract = {Swift identification of leaf diseases is crucial for sustainable rice farming, a staple grain consumed globally. The high costs and inefficiencies of manual identification underline the requirement of prompt disease detection. Traditional approaches for identifying leaf diseases in crops, particularly rice, are laborious, and and often ineffective. Given the significant impact of leaf diseases (such as Rice Blast, Brown Spot, and Rice Turgor) on rice quality and yield, computer-assisted detection can be an effective method of ensuring the long-term sustainability of rice production. This study utilizes advanced artificial intelligence (AI) as the optimized bidirectional encoder representations from the transformers for images(BEiT) model along with pre-trained CNNs (Convolutional Neural Networks), to build a comprehensive study for detecting rice leaf diseases. We train and validate two extensive datasets, featuring healthy and various types of unhealthy plant and rice leaf images respectively.Our optimized model demonstrates high accuracy, outperforming other deep learning and transformer-based models such as ViT, Xception, InceptionV3, DenseNet169, VGG16, and ResNet50. The proposed model achieves a precision of 0.97, a recall of 0.96, and an F1-score of 0.97.The explainability of our proposed model is achieved through the use of segmentation techniques in conjunction with the Local Interpretable Model-agnostic Explanations (LIME) method.}
}
@article{LAVNER2024102528,
title = {The bioacoustic soundscape of a pandemic: Continuous annual monitoring using a deep learning system in Agmon Hula Lake Park},
journal = {Ecological Informatics},
volume = {80},
pages = {102528},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102528},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000700},
author = {Yizhar Lavner and Ronen Melamed and Moshe Bashan and Yoni Vortman},
keywords = {Long-term bird monitoring, BirdNET, Deep learning, Bioacoustics, Passive acoustic monitoring, Avian influenza},
abstract = {Continuous bioacoustic monitoring is an emerging opportunity as well as a challenge, allowing detection of cryptic species' activity while producing high computational demands. In this paper, we present an automated framework that allows the monitoring of a large number of bird species by their vocalizations over extended periods. The framework relies on the BirdNET-Analyzer deep learning model. We applied the framework to >80 species; 20 species with the highest recall scores were selected for further analysis. We used the framework to analyze acoustic signals recorded continuously for over two years using autonomous recorders at various locations in Agmon Hula Lake Park, Israel. During this period there was an acute outbreak of avian influenza in the area. We analyzed differences in acoustic occupancy for various species between two consecutive years (November 2020 to October 2022). We examined between-year population trends for 17 species, both migratory and resident, and found a significant decline in vocal activity between the two years for 10 species. We assume that this decline is related to the avian influenza outbreak, suggesting that the impact of the pandemic may be more widespread and affected a greater number of local species than was previously realized. This further highlights the power and effectiveness of bioacoustic monitoring in detecting cryptic but dramatic dynamics.}
}
@article{CHABOT2022101547,
title = {Using Web images to train a deep neural network to detect sparsely distributed wildlife in large volumes of remotely sensed imagery: A case study of polar bears on sea ice},
journal = {Ecological Informatics},
volume = {68},
pages = {101547},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101547},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121003381},
author = {Dominique Chabot and Seth Stapleton and Charles M. Francis},
keywords = {Artificial intelligence, Big data, Computer vision, Conservation, Machine learning, Marine mammals},
abstract = {Remote sensing can be a valuable alternative or complement to traditional techniques for monitoring wildlife populations, but often entails operational bottlenecks at the image analysis stage. For example, photographic aerial surveys have several advantages over surveys employing airborne observers or other more intrusive monitoring techniques, but produce onerous amounts of imagery for manual analysis when conducted across vast areas, such as the Arctic. Deep learning algorithms, chiefly convolutional neural networks (CNNs), have shown promise for automatically detecting wildlife in large and/or complex image sets. But for sparsely distributed species, such as polar bears (Ursus maritimus), there may not be sufficient known instances of the animals in an image set to train a CNN. We investigated the feasibility of instead providing ‘synthesized’ training data to a CNN to detect polar bears throughout large volumes of aerial imagery from a survey of the Baffin Bay subpopulation. We harvested 534 miscellaneous images of polar bears from the Web that we edited to more closely resemble 21 known images of bears from the aerial survey that were solely used for validation. We combined the Web images of polar bears with 6292 random background images from the aerial survey to train a CNN (ResNet-50), which subsequently correctly classified 20/21 (95%) bear images from the survey and 1172/1179 (99.4%) random background validation images. Given that even a small background misclassification rate could produce multitudinous false positives over many thousands of photos, we describe a potential workflow to efficiently screen out erroneous detections. We also discuss potential avenues to improve CNN accuracy, and the broader applicability of our approach to other image-based wildlife monitoring scenarios. Our results demonstrate the feasibility of using miscellaneously sourced images of animals to train deep neural networks for specific wildlife detection tasks.}
}
@article{XU2024102460,
title = {A novel and dynamic land use/cover change research framework based on an improved PLUS model and a fuzzy multiobjective programming model},
journal = {Ecological Informatics},
volume = {80},
pages = {102460},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102460},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000025},
author = {Xianghui Xu and Weijiang Kong and Ligang Wang and Tengji Wang and Pingping Luo and Jianjun Cui},
keywords = {Land use/cover change, Uncertainty, Multiobjective planning, Multiscenario forecasting},
abstract = {Spatial reconstruction and scenario simulation of historical processes and future trends of land use/cover change (LUCC) can help to reveal the historical background of land conversion and the spatial distribution of future land. Moreover, there is a close relationship between the spatiotemporal dynamics of land use/cover and changes in different ecosystem services (ESs). Using this relationship to simulate future land use scenarios is important. In this study, an LUCC dynamic analysis framework (LSTM-PLUS-FMOP) was constructed based on a deep learning time series forecasting model (LSTM), a parallelized urban land use simulation (PLUS) model and a fuzzy multiobjective programming (FMOP) model. The PLUS model was used to analyze the driving mechanism of land expansion and explore the land conversion pattern. In addition, three land conversion scenarios were established: natural land expansion (NLE), economic development priority (EDP) and regional sustainable development (RSD). The FMOP model and the relationship between LUCC and ES were used to perform a spatial simulation of land conversion. The uncertainty parameters in the model were treated by intuitionistic fuzzy numbers (IFSs). This study applied the constructed framework to the Yellow River Basin of Shaanxi Province (YRB-SX). The results showed that (1) from 2000 to 2020, the cropland area of the YRB-SX continuously decreased by 12.67 × 104 ha, while the built-up area continuously increased by 28.25 × 104 ha. The net reduction in woodland and grassland area was 13.90 × 104 ha. (2) The relative error range of land prediction using the LSTM model was 0.0003– 0.0042. This model had better accuracy than the Markov chain prediction model. (3) The cropland area decreased by 0.26% (NLE), 0.85% (EDP) and 1.68% (RSD) under the three scenarios. The built-up area increased by 25.01%, 32.76% and 14.72%, respectively. The RSD scenario followed the principles of ecological protection and spatial constraints, which mitigated the degradation of the ecosystem to some extent. This coupled simulation framework will help to obtain land allocation schemes that meet the requirements of ecological protection and provide solutions for rational land management.}
}
@article{CIAMPI2023102384,
title = {A deep learning-based pipeline for whitefly pest abundance estimation on chromotropic sticky traps},
journal = {Ecological Informatics},
volume = {78},
pages = {102384},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102384},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004132},
author = {Luca Ciampi and Valeria Zeni and Luca Incrocci and Angelo Canale and Giovanni Benelli and Fabrizio Falchi and Giuseppe Amato and Stefano Chessa},
keywords = {Smart agriculture, Smart farming, Integrated pest management, Computer vision, Object counting, Visual counting},
abstract = {Integrated Pest Management (IPM) is an essential approach used in smart agriculture to manage pest populations and sustainably optimize crop production. One of the cornerstones underlying IPM solutions is pest monitoring, a practice often performed by farm owners by using chromotropic sticky traps placed on insect hot spots to gauge pest population densities. In this paper, we propose a modular model-agnostic deep learning-based counting pipeline for estimating the number of insects present in pictures of chromotropic sticky traps, thus reducing the need for manual trap inspections and minimizing human effort. Additionally, our solution generates a set of raw positions of the counted insects and confidence scores expressing their reliability, allowing practitioners to filter out unreliable predictions. We train and assess our technique by exploiting PST - Pest Sticky Traps, a new collection of dot-annotated images we created on purpose and we publicly release, suitable for counting whiteflies. Experimental evaluation shows that our proposed counting strategy can be a valuable Artificial Intelligence-based tool to help farm owners to control pest outbreaks and prevent crop damages effectively. Specifically, our solution achieves an average counting error of approximately 9% compared to human capabilities requiring a matter of seconds, a large improvement respecting the time-intensive process of manual human inspections, which often take hours or even days.}
}
@article{WEINSTEIN2020101061,
title = {Cross-site learning in deep learning RGB tree crown detection},
journal = {Ecological Informatics},
volume = {56},
pages = {101061},
year = {2020},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101061},
url = {https://www.sciencedirect.com/science/article/pii/S157495412030011X},
author = {Ben G. Weinstein and Sergio Marconi and Stephanie A. Bohlman and Alina Zare and Ethan P. White},
keywords = {Tree crown detection, RGB deep learning, Object detection, Airborne LiDAR},
abstract = {Tree crown detection is a fundamental task in remote sensing for forestry and ecosystem ecology. While many individual tree segmentation algorithms have been proposed, the development and testing of these algorithms is typically site specific, with few methods evaluated against data from multiple forest types simultaneously. This makes it difficult to determine the generalization of proposed approaches, and limits tree detection at broad scales. Using data from the National Ecological Observatory Network, we extend a recently developed deep learning approach to include data from a range of forest types to determine whether information from one forest can be used for tree detection in other forests, and explore the potential for building a universal tree detection algorithm. We find that the deep learning approach works well for overstory tree detection across forest conditions. Performance was best in open oak woodlands and worst in alpine forests. When models were fit to one forest type and used to predict another, performance generally decreased, with better performance when forests were more similar in structure. However, when models were pretrained on data from other sites and then fine-tuned using a relatively small amount of hand-labeled data from the evaluation site, they performed similarly to local site models. Most importantly, a model fit to data from all sites performed as well or better than individual models trained for each local site.}
}
@article{MORALES2022101909,
title = {Method for passive acoustic monitoring of bird communities using UMAP and a deep neural network},
journal = {Ecological Informatics},
volume = {72},
pages = {101909},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101909},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003594},
author = {Gabriel Morales and Víctor Vargas and Diego Espejo and Víctor Poblete and Jorge A. Tomasevic and Felipe Otondo and Juan G. Navedo},
keywords = {Passive acoustic monitoring, Bird community, Deep learning, Soundscape, Phenology},
abstract = {An effective practice for monitoring bird communities is the recognition and identification of their acoustic signals, whether simple, complex, fixed or variable. A method for the passive monitoring of diversity, activity and acoustic phenology of structural species of a bird community in an annual cycle is presented. The method includes the semi-automatic elaboration of a dataset of 22 vocal and instrumental forms of 16 species. To analyze bioacoustic richness, the UMAP algorithm was run on two parallel feature extraction channels. A convolutional neural network was trained using STFT-Mel spectrograms to perform the task of automatic identification of bird species. The predictive performance was evaluated by obtaining a minimum average precision of 0.79, a maximum equal to 1.0 and a mAP equal to 0.97. The model was applied to a huge set of passive recordings made in a network of urban wetlands for one year. The acoustic activity results were synchronized with climatological temperature data and sunlight hours. The results confirm that the proposed method allows for monitoring a taxonomically diverse group of birds that nourish the annual soundscape of an ecosystem, as well as detecting the presence of cryptic species that often go unnoticed.}
}
@article{NOLASCO2023102258,
title = {Learning to detect an animal sound from five examples},
journal = {Ecological Informatics},
volume = {77},
pages = {102258},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102258},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300287X},
author = {Ines Nolasco and Shubhr Singh and Veronica Morfi and Vincent Lostanlen and Ariana Strandburg-Peshkin and Ester Vidaña-Vila and Lisa Gill and Hanna Pamuła and Helen Whitehead and Ivan Kiskin and Frants H. Jensen and Joe Morford and Michael G. Emmerson and Elisabetta Versace and Emily Grout and Haohe Liu and Burooj Ghani and Dan Stowell},
keywords = {Bioacoustics, Deep learning, Event detection, Few-shot learning},
abstract = {Automatic detection and classification of animal sounds has many applications in biodiversity monitoring and animal behavior. In the past twenty years, the volume of digitised wildlife sound available has massively increased, and automatic classification through deep learning now shows strong results. However, bioacoustics is not a single task but a vast range of small-scale tasks (such as individual ID, call type, emotional indication) with wide variety in data characteristics, and most bioacoustic tasks do not come with strongly-labelled training data. The standard paradigm of supervised learning, focussed on a single large-scale dataset and/or a generic pre-trained algorithm, is insufficient. In this work we recast bioacoustic sound event detection within the AI framework of few-shot learning. We adapt this framework to sound event detection, such that a system can be given the annotated start/end times of as few as 5 events, and can then detect events in long-duration audio—even when the sound category was not known at the time of algorithm training. We introduce a collection of open datasets designed to strongly test a system's ability to perform few-shot sound event detections, and we present the results of a public contest to address the task. Our analysis shows that prototypical networks are a very common used strategy and they perform well when enhanced with adaptations for general characteristics of animal sounds. However, systems with high time resolution capabilities perform the best in this challenge. We demonstrate that widely-varying sound event durations are an important factor in performance, as well as non-stationarity, i.e. gradual changes in conditions throughout the duration of a recording. For fine-grained bioacoustic recognition tasks without massive annotated training data, our analysis demonstrate that few-shot sound event detection is a powerful new method, strongly outperforming traditional signal-processing detection methods in the fully automated scenario.}
}
@article{CONCEPCION2023102344,
title = {BivalveNet: A hybrid deep neural network for common cockle (Cerastoderma edule) geographical traceability based on shell image analysis},
journal = {Ecological Informatics},
volume = {78},
pages = {102344},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102344},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003734},
author = {Ronnie Concepcion and Marielet Guillermo and Susanne E. Tanner and Vanessa Fonseca and Bernardo Duarte},
keywords = {Traceability, Morphometric analysis, Deep learning, Bivalves, Low-cost},
abstract = {Bivalve traceability is a major concern. It is of utmost importance to develop tools that allow providing important information to the consumer, not only on the origin of the product but also on its sustainability and safety, due to the harvest restrictions imposed by regulatory entities. This study evaluated the application of computer vision machine learning technologies for efficiently discriminating cockle harvesting origin based on shell geometric and morphometric analysis, improving the traceability methodologies in these organisms, and highlighting the potential of these low-cost techniques as a reliable traceability tool. Thirty Cerastoderma edule samples were collected along the five locations in Atlantic West and South Portuguese coast with individual images processed using lazysnapping segmentation, spectro-textural-morphological phenotype extraction, and feature selection through hybrid Principal Component Analysis and Neighborhood Component Analysis which resulted in R, a*, b*, entropy, and diameter. Three approaches of traceability models were developed and tested: pre-trained networks (EfficientNet-Bo, ResNet101, MobileNetV2, InceptionV3) with numerical inputs (Approach 1), image-based pre-trained networks (Approach 2), and hybrid deep neural networks of Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Bidirectional LSTM (BiLSTM) (Approach 3). Based on the test results, Approach 3 with GRU-LSTM-BiLSTM sequence exhibited the highest accuracy (96.91%) and sensitivity (96%) among the other thirteen machine learning models, hence, named as BivalveNet. Comparing the attained accuracy from the BivalveNet to other mollusc traceability studies, it was observed that an efficiency close to the attained using standard destructive, time-consuming, and expensive techniques, making BivalveNet a highly advantageous approach for common cockle geographical traceability studies, available for application to other bivalve species.}
}
@article{BACHECHI2024102568,
title = {HypeAIR: A novel framework for real-time low-cost sensor calibration for air quality monitoring in smart cities},
journal = {Ecological Informatics},
volume = {81},
pages = {102568},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102568},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001109},
author = {Chiara Bachechi and Federica Rollo and Laura Po},
keywords = {Real-time, Sensor calibration, Air quality monitoring, Smart cities, Air pollution monitoring, Low cost sensors, Time series, Framework, Air quality, LSTM, Random forest},
abstract = {While less reliable than authorized air quality stations, low-cost sensors help monitor air quality in areas overlooked by traditional devices. A calibration process in the same environment as the sensor is crucial to enhance their accuracy. Furthermore, low-cost sensors deteriorate over time, necessitating repeated calibration for sustained performance. HypeAIR is a novel open-source framework for the management of sensor calibration in real-time. It incorporates two calibration methodologies: a combination of machine learning models (Voting Regressor and Support Vector Regression) and the Long Short-Term Memory deep learning model. To evaluate the framework, three extensive experiments were conducted over a 2-year period in the city of Modena, Italy, to monitor NO, NO2, and O3 gases. Both calibration methodologies outperform the manufacturer calibration and our baseline (i.e., a variation of the Random Forest algorithm) and maintain efficiency over time. The availability of the source code facilitates customization for monitoring additional pollutants, while shared air quality datasets ensure reproducibility.}
}
@article{VILLON2024102499,
title = {Toward an artificial intelligence-assisted counting of sharks on baited video},
journal = {Ecological Informatics},
volume = {80},
pages = {102499},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102499},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000414},
author = {Sébastien Villon and Corina Iovan and Morgan Mangeas and Laurent Vigliola},
keywords = {Deep learning, Neural network, Coral reef, Marine ecology, Shark conservation},
abstract = {Given the global biodiversity crisis, there is an urgent need for new tools to monitor populations of endangered marine megafauna, like sharks. To this end, Baited Remote Underwater Video Stations (BRUVS) stand as the most effective tools for estimating shark abundance, measured using the MaxN metric. However, a bottleneck exists in manually computing MaxN from extensive BRUVS video data. Although artificial intelligence methods are capable of solving this problem, their effectiveness is tested using AI metrics such as the F-measure, rather than ecologically informative metrics employed by ecologists, such as MaxN. In this study, we present both an automated and a semi-automated deep learning approach designed to produce the MaxN abundance metric for three distinct reef shark species: the grey reef shark (Carcharhinus amblyrhynchos), the blacktip reef shark (C. melanopterus), and the whitetip reef shark (Triaenodon obesus). Our approach was applied to one-hour baited underwater videos recorded in New Caledonia (South Pacific). Our fully automated model achieved F-measures of 0.85, 0.43, and 0.72 for the respective three species. It also generated MaxN abundance values that showed a high correlation with manually derived data for C. amblyrhynchos (R = 0.88). For the two other species, correlations were significant but weak (R = 0.35–0.44). Our semi-automated method significantly enhanced F-measures to 0.97, 0.86, and 0.82, resulting in high-quality MaxN abundance estimations while drastically reducing the video processing time. To our knowledge, we are the first to estimate MaxN with a deep-learning approach. In our discussion, we explore the implications of this novel tool and underscore its potential to produce innovative metrics for estimating fish abundance in videos, thereby addressing current limitations and paving the way for comprehensive ecological assessments.}
}
@article{LEVY2024102737,
title = {Improving deep learning based bluespotted ribbontail ray (Taeniura Lymma) recognition},
journal = {Ecological Informatics},
volume = {82},
pages = {102737},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102737},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002796},
author = {Avivit Levy and Adi Barash and Chen Zaguri and Ariel Hadad and Polina Polsky},
keywords = {Computer vision, Citizen science, Ecological study, Marine animal recognition, Pose handling},
abstract = {This paper presents the novel task of bluespotted ribbontail (BR) ray (Taeniura lymma) recognition using deep learning based computer vision methods to enable the identification of specific individuals of this species. Mapping the specific individuals in relation to location and time will allow marine researchers to understand their movement patterns, habitat choice, life span, size of the population and more – data which could allow monitoring and establishing a tailor-made conservation plan for this species. Our work is pioneer on this recognition problem. We give a detailed description of the three basic steps of detection, feature extraction and recognition in this vision problem and perform experiments to explore the system configuration and what improves the performance. A feature extraction enhancement as well as a crucial effect of a split into different main poses are demonstrated. Though the precision results achieved in this paper are still moderate and should be further improved, they are nevertheless promising and reasonable for practical use if the six best matches are chosen. For this scenario, almost 85% precision for upper-pose model, and almost 80% precision for left- and right-pose models, are achieved demonstrating the feasibility of the pipeline suggested as well as opportunities for improvement.}
}
@article{XIE2024102661,
title = {Forecasting China's agricultural carbon emissions: A comparative study based on deep learning models},
journal = {Ecological Informatics},
volume = {82},
pages = {102661},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102661},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002036},
author = {Tiantian Xie and Zetao Huang and Tao Tan and Yong Chen},
keywords = {Forecast, Agricultural carbon emissions, Deep learning, Long short-term memory neural network, Tree-structured Parzen estimator Bayesian optimization},
abstract = {Given the critical urgency to combat the escalating climate crisis and the continuous rise in agricultural carbon emissions (ACE) in China, accurately forecasting their future trends is crucial. This research employs the emission factor method to assess ACE throughout mainland China from 1993 to 2021. To refine our forecasting approach, both statistical and neural network methodologies were utilized to pinpoint key factors influencing ACE. We crafted forecasting models incorporating both deep learning techniques and traditional methods. Notably, the Tree-structured Parzen Estimator Bayesian Optimization (TPEBO) algorithm was applied to optimize Long Short-Term Memory (LSTM) neural networks, culminating in the creation of a superior integrated TPEBO-LSTM model that demonstrated strong performance across various datasets. The forecasting outcomes suggest that ACE in 24 provinces are expected to reach their zenith before 2030, primarily driven by farm operations, as well as livestock and poultry manure management. The result provides a significant forecasting tool for assessing agricultural carbon emissions in different regions, offering insights crucial for targeted mitigation strategies.}
}
@article{VEGA2024102535,
title = {Convolutional neural networks for hydrothermal vents substratum classification: An introspective study},
journal = {Ecological Informatics},
volume = {80},
pages = {102535},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102535},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000773},
author = {Pedro Juan Soto Vega and Panagiotis Papadakis and Marjolaine Matabos and Loïc {Van Audenhaege} and Annah Ramiere and Jozée Sarrazin and Gilson Alexandre Ostwald Pedro {da Costa}},
keywords = {Image classification, Deep learning, Hydrothermal vents, Uncertainty analysis},
abstract = {The increasing availability of seabed images has created new opportunities and challenges for monitoring and better understanding the spatial distribution of fauna and substrata. To date, however, deep-sea substratum classification relies mostly on visual interpretation, which is costly, time-consuming, and prone to human bias or error. Motivated by the success of convolutional neural networks in learning semantically rich representations directly from images, this work investigates the application of state-of-the-art network architectures, originally employed in the classification of non-seabed images, for the task of hydrothermal vent substrata image classification. In assessing their potential, we conduct a study on the generalization, complementarity and human interpretability aspects of those architectures. Specifically, we independently trained deep learning models with the selected architectures using images obtained from three distinct sites within the Lucky-Strike vent field and assessed the models' performances on-site as well as off-site. To investigate complementarity, we evaluated a classification decision committee (CDC) built as an ensemble of networks in which individual predictions were fused through a majority voting scheme. The experimental results demonstrated the suitability of the deep learning models for deep-sea substratum classification, attaining accuracies reaching up to 80% in terms of F1-score. Finally, by further investigating the classification uncertainty computed from the set of individual predictions of the CDC, we describe a semiautomatic framework for human annotation, which prescribes visual inspection of only the images with high uncertainty. Overall, the results demonstrated that high accuracy values of over 90% F1-score can be obtained with the framework, with a small amount of human intervention.}
}
@article{JAMES2024102580,
title = {Monitoring vegetation patterns and their drivers to infer resilience: Automated detection of vegetation and megaherbivores from drone imagery using deep learning},
journal = {Ecological Informatics},
volume = {81},
pages = {102580},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102580},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001225},
author = {Rebecca K. James and Freek Daniels and Aneesh Chauhan and Pramaditya Wicaksono and Muhammad Hafizt and Setiawan Djody Harahap and Marjolijn J.A. Christianen},
keywords = {Semantic segmentation, Object detection, Pytorch, Seagrass, Drone imagery, Turtle monitoring, Conservation},
abstract = {Ecological pattern theory has highlighted spatial vegetation patterns that can be used as indicators of ecosystem resilience. Combining this spatial pattern theory with aerial imagery from drones and automated image processing with deep learning methods, we show how monitoring of natural ecosystems can be enhanced through quantifying vegetation spatial patterns. We demonstrate this approach in a tropical seagrass ecosystem with a high abundance of turtles that generate vegetation patches when grazing. Past field observations suggest that patch size and density reflect the seagrass meadow resilience, but understanding the natural variation in vegetation patchiness is crucial. Employing the deep learning methods of semantic segmentation and object detection, we quantify vegetation patchiness metrics and turtle distribution across 12 ha of seagrass meadow in the years 2012 and 2022. The resulting output facilitates spatial and temporal comparisons, revealing areas of low resilience. In 2012, turtle grazing across the entire site yielded vegetation patch sizes averaging 2 ± 0.2 m2 (95% confidence interval). Reduced patch sizes of 0.24 ± 0.05 m2 and 0.67 ± 0.6 m2 at the reef edge and beach slope respectively, in conjunction with a reduced patch density, indicated lower resilience at the seagrass meadow edges. Analysis of the 2022 dataset indicates a general decrease in patch size over time, suggesting declining resilience. A retraining experiment of the semantic segmentation model was conducted where the initial model was retrained on the 2022 dataset and demonstrated the adaptability of the deep learning methods. Despite using different equipment, the model achieved high accuracy with only 5–10 additional training images. By providing the tools to conduct these analyses, we aim to stimulate the uptake of deep learning for enhancing the data obtained from aerial imagery to improve the monitoring and conservation of natural ecosystems.}
}
@article{LIN2024102507,
title = {A model for forest type identification and forest regeneration monitoring based on deep learning and hyperspectral imagery},
journal = {Ecological Informatics},
volume = {80},
pages = {102507},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102507},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000499},
author = {Feng-Cheng Lin and Yi-Shiang Shiu and Pei-Jung Wang and Uen-Hao Wang and Jhe-Syuan Lai and Yung-Chung Chuang},
keywords = {Remote sensing, Deep learning, VGG19, ResNet50, Hyperspectral images},
abstract = {Traditional ground-based forest survey methods involve high labor costs, and their inefficiency makes comprehensive forest resource surveys challenging. With the development of new sensors and vehicles in recent years, more diverse and novel remote sensing detection and survey techniques have emerged. This study aims to use hyperspectral imagery to classify forest types containing representative tree species. To verify the feasibility of the proposed methods, we used hyperspectral imagery from the Taiwan Forestry Experiment Institute's Liugui Research Forest in southern Taiwan, which has an area of 9882 ha and an altitude of 250–2600 m. Hyperspectral imagery offers several advantages compared to traditional multispectral imagery; it captures a broad spectrum of contiguous, narrow spectral bands, providing highly detailed spectral information, enabling differentiation of tree components that appear similar in multispectral imagery. Eight identifiable forest types were selected for the models considered, and three different deep learning algorithms, VGG19, ResNet50 and a proposed combination (VGG19 + ResNet50), were used to screen the best algorithms. Data formats and pre-processing methods that can effectively improve computational performance were explored. The research results found that: (1) Band filtering is a necessary means to improve calculation performance; (2) Flattening the original convolution kernel with cubic characteristics can significantly reduce the time required for calculation. In terms of simulation results, VGG19 + ResNet50 was identified as the best model. Its overall classification accuracy can generally reach 93% to 100%. According to the calculation process set in this study, the time required for model training can be shortened to less than 30 min. The results of this research will help process more detailed and complex information in forest resource management and more accurately quantify forest ecology and woodland conditions.}
}
@article{JOELIANTO2024102495,
title = {Convolutional neural network-based real-time mosquito genus identification using wingbeat frequency: A binary and multiclass classification approach},
journal = {Ecological Informatics},
volume = {80},
pages = {102495},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102495},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000372},
author = {Endra Joelianto and Miranti Indar Mandasari and Daniel Beltsazar Marpaung and Naufal Dzaki Hafizhan and Teddy Heryono and Maria Ekawati Prasetyo and  Dani and Susy Tjahjani and Tjandra Anggraeni and Intan Ahmad},
keywords = {Dengue infection, , Mosquito vector, Wingbeat frequency, Deep learning, Sustainability monitoring},
abstract = {Global rises in dengue hemorrhagic fever, especially in Asia and Latin America, underscore the necessity for enhanced public health interventions. Aedes spp. mosquitoes are the primary vectors; however, species such as Culex quinquefasciatus pose significant health risks by transmitting diseases such as filariasis, impacting millions of people worldwide. This study introduces a real-time convolutional neural network-based mosquito classification system using wingbeat frequency for identifying various mosquito species, with emphasis on Aedes sp. We proposed and assessed two models: a binary classification and a multiclass system. The binary system exhibited an outstanding accuracy of 91.76% in distinguishing between Aedes aegypti and Culex quinquefasciatus. The multiclass system accurately identified female and male Aedes aegypti and Culex quinquefasciatus with a precision of 87.16%. This innovative approach serves as a potential tool for dengue infection control and a versatile instrument for combating various mosquito-borne illnesses, enhancing vector surveillance for comprehensive disease management.}
}
@article{ZHENG2024102689,
title = {A video object segmentation-based fish individual recognition method for underwater complex environments},
journal = {Ecological Informatics},
volume = {82},
pages = {102689},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102689},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002310},
author = {Tao Zheng and Junfeng Wu and Han Kong and Haiyan Zhao and Boyu Qu and Liang Liu and Hong Yu and Chunyu Zhou},
keywords = {Individual fish recognition, Video object segmentation, Underwater complex environments, Deep learning, Intelligent aquaculture},
abstract = {Currently, aquaculture methods tend to combine scale and intelligence, which saves manpower and improves the survival rate of seafood at the same time. High-precision and high-efficiency fish individual recognition can provide key technical support for fish disease detection, feeding habits, body condition, etc. In the realm of intelligent aquaculture, it provides robust data support for precision fish farming. However, the current research methods for individual fish recognition struggle to maintain the network model's focus on the fish body in real marine underwater complex environments (e.g., environmental background interference such as coral reefs, overlap between fish bodies, light noise, etc.), leading to unsatisfactory recognition results. To this end, this paper proposes a method for fish individual recognition in underwater complex environments based on video object segmentation, which consists of three parts, including a fish individual segmentation detection module, a fish individual recognition module, and an all-in-one visualization module. The work adopts a combination of deep learning methods and video object segmentation algorithms to solve the problem of low attention and poor detection accuracy of fish individuals in real underwater complex environments, which effectively improves the accuracy and efficiency of fish individual recognition, and analyzes and discusses the comparison of recognition effects using different weights. The results of the simulation experiments show that the key metric Rank1 value of the method achieves more than 96% accuracy on the public datasets DlouFish, WideFish, and the Fish-seg dataset produced in this paper, and improves over the state-of-the-art methods for fish individual recognition by 2.23%, 1.33%, and 1.25%, respectively.}
}
@article{WANG2024102409,
title = {Vegetation coverage precisely extracting and driving factors analysis in drylands},
journal = {Ecological Informatics},
volume = {79},
pages = {102409},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102409},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004387},
author = {Haolin Wang and Dongwei Gui and Qi Liu and Xinlong Feng and Jia Qu and Jianping Zhao and Guangyan Wang and Guanghui Wei},
keywords = {Image segmentation, Fractional vegetation coverage, Arid region, Ecological restoration, Deep learning},
abstract = {Fractional Vegetation Coverage (FVC) is an essential indicator that captures variations in vegetation and documents the impacts of climate change and human activity for environmental assessment. However, conventional methods encounter challenges in accurately extracting fine-scale FVC in drylands due to the vegetation distribution being very heterogeneous in space with patches and inter-patches. Using the lower Tarim River Basin as a typical study case, we investigated three deep convolutional neural networks—Unet, Pspnet, and Deeplabv3 + —to generate high-precision FVC in drylands with high-resolution (0.8 m) remote sensing images. Among these models, the Unet model performed better, with an accuracy of 93.38%, while the accuracy of Pspnet and Deeplabv3+ was 88.14% and 88.91%, respectively. Comparison with the FVC derived from normalized difference vegetation index (NDVI), and land use/land cover data from ESRI and ESA indicated that the FVC map produced by Unet was more consistent with on-site field observations. Delving into drivers influencing dryland FVC, we found that groundwater depth plays a pivotal role compared to topographical and climatic variables. Specifically, when the groundwater depth exceeds −3 m, the probability of occurring high FVC is reduced to 50%. This study innovatively extracted the FVC of drylands with high vegetation spatial heterogeneity, which better solves the insufficient accuracy of the existing dataset, serves as a valuable reference for monitoring vegetation change, and facilitates more precise quantification of carbon storage.}
}
@article{KIM2024102576,
title = {Application of the domain adaptation method using a phenological classification framework for the land-cover classification of North Korea},
journal = {Ecological Informatics},
volume = {81},
pages = {102576},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102576},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001183},
author = {Joon Kim and Hyun-Woo Jo and Whijin Kim and Yujeong Jeong and Eunbeen Park and Sujong Lee and Moonil Kim and Woo-Kyun Lee},
keywords = {Land-cover mapping, Phenological classification, Domain adaptation, North Korea, Deep learning},
abstract = {Efforts to achieve carbon neutrality are global, encompassing a wide range of factors. For the estimation of greenhouse gas emissions from the agriculture, forestry, and other land use (AFOLU) sector, the Intergovernmental Panel on Climate Change has proposed an advanced method that requires Approach 3, the highest level of suggested method, of activity data. Accordingly, we propose a phenological classification framework (PCF) that can perform land-cover classification by training the climatic repeatability of the annual cycle using a U-Net deep learning model. Additionally, the domain adaptation (DA) method can be applied to classify areas with insufficient data. We applied these methods to classify North Korea (i.e., using South Korean data), with an accuracy of 81.31%; overall this effort culminated in the simultaneous classification of the Korean Peninsula. Domain distribution comparison showed that the results for the two regions were similar. The PCF and DA methods proposed in this study allow for annual production of a land-cover map and change matrix, regardless of the presence or absence of data. The application of these methods is expected to provide a scientific basis for policy decisions that can facilitate the global attainment of carbon neutrality.}
}
@article{LIU2024102401,
title = {YWnet: A convolutional block attention-based fusion deep learning method for complex underwater small target detection},
journal = {Ecological Informatics},
volume = {79},
pages = {102401},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102401},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004302},
author = {Pingzhu Liu and Wenbin Qian and Yinglong Wang},
keywords = {Underwater target detection, Deep learning, Feature fusion, Attention mechanism},
abstract = {Underwater target detection holds a noteworthy role in the field of marine exploration. However, it is difficult to extract useful feature information from blurred images with complex backgrounds, resulting in suboptimal and unsatisfactory target detection in conventional models. Among them, YOLOv5 leverages the advantages of fast detection performs better in detecting underwater samples. Nevertheless, YOLOv5 still faces difficulties including missed and incorrect detections due to the underwater environment's small scale of objects, dense distribution of organisms, and occlusion. To address these challenges, we propose a novel YoLoWaternet (YWnet) model that builds upon the YOLOv5 framework for complex underwater species detection with three main innovations: 1) A convolutional block attention module (CBAM) is introduced to enhance feature extraction for blurry images in the initial stages of the network and a new feature fusion network called the CRFPN is created to transfer important information and detect underwater targets. 2) A novel feature extraction module is presented, namely, the skip residual C3 module (SRC3), by effectively merging information from various scales to minimize the loss of original data during transmission. 3) Regression and classification algorithms are separated using the decoupled head to improve the effectiveness of detection and the EIoU loss function is employed to accelerate the convergence speed. Finally, the experimental results demonstrate that YWnet achieves remarkable accuracies of 73.2% mAp and 39.3% mAp50–95 on the underwater dataset, surpassing YOLOv5 by 2.3% and 2.4%, respectively. Furthermore, the proposed fusion model outperforms nine state-of-the-art baseline models on the undersea dataset and has generalization capabilities in other datasets.}
}
@article{LEKUNBERRI2022101495,
title = {Identification and measurement of tropical tuna species in purse seiner catches using computer vision and deep learning},
journal = {Ecological Informatics},
volume = {67},
pages = {101495},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101495},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121002867},
author = {Xabier Lekunberri and Jon Ruiz and Iñaki Quincoces and Fadi Dornaika and Ignacio Arganda-Carreras and Jose A. Fernandes},
keywords = {Computer vision, Deep learning, Fisheries, Electronic monitoring, Tropical tuna species identification},
abstract = {Fishery monitoring programs are essential for effective management of marine resources, as they provide scientists and managers with the necessary data for both the preparation of scientific advice and fisheries control and surveillance. The monitoring is generally done by human observers, both in port and onboard, with a high cost involved. Consequently, some Regional Fisheries Management Organizations (RFMO) are opting for electronic monitoring (EM) as an alternative or complement to human observers in certain fisheries. This is the case of the tropical tuna purse seine fishery operating in the Indian and Atlantic oceans, which started an EM program on a voluntary basis in 2017. However, even when the monitoring is conducted though EM, the image analysis is a tedious task manually performed by experts. In this paper, we propose a cost-effective methodology for the automatic processing of the images already being collected by cameras onboard tropical tuna purse seiners. Firstly, the images are preprocessed to homogenize them across all vessels and facilitate subsequent steps. Secondly, the fish are individually segmented using a deep neural network (Mask R-CNN). Then, all segments are passed through other deep neural network (ResNet50V2) to classify them by species and estimate their size distribution. For the classification of fish, we achieved an accuracy for all species of over 70%, i.e., about 3 out of 4 individuals are correctly classified to their corresponding species. The size distribution estimates are aligned with official port measurements but calculated using a larger number of individuals. Finally, we also propose improvements to the current image capture systems which can facilitate the work of the proposed automation methodology.}
}
@article{VABO2021101322,
title = {Automatic interpretation of salmon scales using deep learning},
journal = {Ecological Informatics},
volume = {63},
pages = {101322},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101322},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121001138},
author = {Rune Vabø and Endre Moen and Szymon Smoliński and Åse Husebø and Nils Olav Handegard and Ketil Malde},
keywords = {Fish scales, Deep learning, EfficientNet, Transfer learning, Age reading, Maturity staging},
abstract = {For several fish species, age and other important biological information is manually inferred from visual scrutinization of scales, and reliable automatic methods are not widely available. Here, we apply Convolutional Neural Networks (CNN) with transfer learning on a novel dataset of 9056 images of Atlantic salmon scales for four different prediction tasks. We predicted fish origin (wild/farmed), spawning history (previous spawner/non-spawner), river age, and sea age. We obtained high prediction accuracy for fish origin (96.70%), spawning history (96.40%), and sea age (86.99%), but lower accuracy for river age (63.20%). Against six human expert readers with an additional dataset of 150 scales, the CNN showed the second-highest percentage agreement for sea age (94.00%, range 87.25±97.30%), but the lowest agreement for river age (66.00%, range 66.00– 84.68%). Estimates of river age by expert readers exhibited higher variance and lower levels of agreement compared to sea age and may indicate why this task is also more difficult for the CNN. Automatic interpretation of scales may provide a cost- and time-efficient method of predicting fish age and life-history traits.}
}
@article{NAZIR2024102453,
title = {Object classification and visualization with edge artificial intelligence for a customized camera trap platform},
journal = {Ecological Informatics},
volume = {79},
pages = {102453},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102453},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300482X},
author = {Sajid Nazir and Mohammad Kaleem},
keywords = {Data science, Computer vision, Deep learning, Model generalization, Fine tuning, Explainable AI, Hyperparameter tuning, Vision transformers},
abstract = {The camera traps have revolutionized the image and video capture in ecology and are often used to monitor and record animal presence. With miniaturization of low power electronic devices, better battery technologies, and software advancements, it has become possible to use the edge devices, such as Raspberry Pi as camera traps that can not only capture images and videos, but can also enable sophisticated image processing, and off-site communications. These developments can help to provide near real-time insights and reduce the manual processing of images. The on-board image classification and visualization is facilitated by the advancements in the Deep Neural Networks (DNN), transfer learning approaches, and software libraries. This paper provides an investigation of image classification with transfer learning approaches using pre-trained DNN models, and visualizations with Explainable Artificial Intelligence (XAI) techniques on Raspberry Pi Zero (RPi-Z) edge device. The MobileNetV2 model was used for image classification on the Florida-Part1 dataset obtaining the results for precision, recall, and F1-score as 0.95, 0.96, and 0.95 respectively. We also compared the model performance of MobileNetV2, EfficientNetV2B0, and MobileViT models for classification on the Extinction dataset with the best results for precision, recall, and F1-score as 0.97, 0.96, and 0.96 respectively, obtained with the EfficientNetV2B0 model. Two XAI techniques, Gradient-weighted-Class Activation Mapping (Grad-CAM) and Occlusion Sensitivity were used for visualization through heatmaps, to highlight the relative importance of the image areas contributing to the DNN model's prediction, that can also help to understand the model's performance and bias. The results provide practical use case scenarios for utilizing the transfer learning approaches, model optimization and deployment to edge devices, and model visualizations in ecological research.}
}
@article{SIGURDARDOTTIR2023102046,
title = {Otolith age determination with a simple computer vision based few-shot learning method},
journal = {Ecological Informatics},
volume = {76},
pages = {102046},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102046},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000754},
author = {Andrea Rakel Sigurðardóttir and Þór Sverrisson and Aðalbjörg Jónsdóttir and María Gudjónsdóttir and Bjarki Þór Elvarsson and Hafsteinn Einarsson},
keywords = {Otoliths, Fish age estimation, Few-shot learning, Deep-learning, Image analysis},
abstract = {In this study, we propose a computer vision-based few-shot learning method for otolith age determination in European plaice, Atlantic cod, Greenland halibut, and haddock. Our method outperforms prior state-of-the-art approaches, and is based on a vision encoder from CLIP as a feature extractor, which is used to train shallow models. The method is computationally efficient, as it does not require fine-tuning of deep networks, and is also data efficient, as it performs better than fine-tuning on the same data. Our results suggest that in some cases, our method can achieve the same performance as state-of-the-art finetuning approaches with up to three times less training data.}
}
@article{KUMAR2024102510,
title = {Bird species recognition using transfer learning with a hybrid hyperparameter optimization scheme (HHOS)},
journal = {Ecological Informatics},
volume = {80},
pages = {102510},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102510},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000529},
author = {Samparthi V.S. Kumar and Hari Kishan Kondaveeti},
keywords = {Automatic bird species recognition, Convolutional neural network, Deep learning, Hyperparameter tuning, Transfer learning},
abstract = {The use of automatic bird species recognition methods reduces the burden on scientists, ornithologists, and bird watchers as these methods help identify birds with minimal human effort and intervention. The current study employs a transfer learning approach combined with a Hybrid hyperparameter Optimization Scheme (HHOS) to enhance the efficiency and accuracy of automatic bird species recognition. First, the weights of selected pre-trained deep learning models are downloaded from ImageNet, and a few new trainable layers are added at the top. Thereafter, the selected models are trained using HHOS, which strategically integrates both manual and random searches to achieve favorable results. The manual search relies on domain knowledge and experience to identify the best hyperparameter settings, thereby making the search space smaller and more focused. Random search tests various combinations of the hyperparameters identified in manual search and trains the selected models to achieve the maximum possible accuracy through multiple iterations. Experimental analysis revealed that the Fine-tuned EfficientNetB0 model exhibited superior performance, achieving an accuracy of 99.12%. In contrast, the performance of the ResNet18 model was disappointing with an accuracy of 93.24%, while other models outperformed it.}
}
@article{ESTOPINAN2024102627,
title = {Mapping global orchid assemblages with deep learning provides novel conservation insights},
journal = {Ecological Informatics},
volume = {81},
pages = {102627},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102627},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001699},
author = {Joaquim Estopinan and Maximilien Servajean and Pierre Bonnet and Alexis Joly and François Munoz},
keywords = {Spatial indicator, Species assemblage, Deep learning, Species distribution modelling, IUCN status, Orchids},
abstract = {Although increasing threats on biodiversity are now widely recognised, there are no accurate global maps showing whether and where species assemblages are at risk. We hereby assess and map at kilometre resolution the conservation status of the iconic orchid family, and discuss the insights conveyed at multiple scales. We introduce a new Deep Species Distribution Model trained on 1 M occurrences of 14 K orchid species to predict their assemblages at global scale and at kilometre resolution. We propose two main indicators of the conservation status of the assemblages: (i) the proportion of threatened species, and (ii) the status of the most threatened species in the assemblage. We show and analyze the variation of these indicators at World scale and in relation to currently protected areas in Sumatra island. Global and interactive maps available online show the indicators of conservation status of orchid assemblages, with sharp spatial variations at all scales. The highest level of threat is found at Madagascar and the neighbouring islands. In Sumatra, we found good correspondence of protected areas with our indicators, but supplementing current IUCN assessments with status predictions results in alarming levels of species threat across the island. Recent advances in deep learning enable reliable mapping of the conservation status of species assemblages on a global scale. As an umbrella taxon, orchid family provides a reference for identifying vulnerable ecosystems worldwide, and prioritising conservation actions both at international and local levels.}
}
@article{KHANMOHAMMADI2024102711,
title = {Using AutoML and generative AI to predict the type of wildfire propagation in Canadian conifer forests},
journal = {Ecological Informatics},
volume = {82},
pages = {102711},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102711},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400253X},
author = {Sadegh Khanmohammadi and Miguel G. Cruz and Daniel D.B. Perrakis and Martin E. Alexander and Mehrdad Arashpour},
keywords = {Wildfire behaviour, Crown fire, Generative adversarial networks, Random Forest, TabPFN, XGBoost, AutoGluon},
abstract = {Accurate prediction of wildfire behaviour is critical for safe and effective fire management and suppression. This study focused on developing and evaluating machine learning (ML) models based on a dataset collected of outdoor experimental fires in Canadian conifer forests. Binary classification models (surface fire vs. crown fire) and multi-class fire type models (surface, passive crown, active crown) were created using ensemble tree methods (Random Forest, XGBoost) and automated ML (AutoGluon, TabPFN). Generative adversarial networks (GAN) were used to generate synthetic data to overcome imbalances in the type of fire distribution. The results show automated ML methods applied to the binary problem to perform best of all tested methods, with an overall accuracy of 91%. TabPFN had the highest accuracy for the multi-class problem, with the use of GAN improving model fit. Results show overfitting issues with some of the ML models, highlighting the need of independent evaluation when ML models are developed. The TabPFN models have potential for application in supporting fire management, namely in fuel management and identifying situations with high fire spread and intensity potential that can impact firefighter safety.}
}
@article{DENG2024102546,
title = {Weed database development: An updated survey of public weed datasets and cross-season weed detection adaptation},
journal = {Ecological Informatics},
volume = {81},
pages = {102546},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102546},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000888},
author = {Boyang Deng and Yuzhen Lu and Jiajun Xu},
keywords = {Deep learning, Domain adaptation, Machine vision, Precision agriculture, Robustness, Weed detection},
abstract = {Weeds are a major threat to crop production. Automated innovations for reducing herbicides and labor needed for weeding have become a high priority for sustainable weed management. The current state-of-the-art weeding systems still cannot reliably recognize weeds in changing field conditions for precision weed control. Enhancing weed recognition accentuates the critical need to develop dedicated, labeled weed databases and whereby train advanced AI (artificial intelligence) models while ensuring the robustness of models across diverse field conditions. This study presents an up-to-date survey on publicly available image datasets for weed recognition. Among 36 datasets identified, limitations exist in terms of data variations and distribution shifts, and few of the datasets are suitable for examining the robustness of weed recognition models. A new two-season, eight-class weed dataset is described in this study, comprising two sub-datasets of images collected in the seasons of 2021 and 2022, respectively. Three state-of-the-art deep learning object detectors, i.e., YOLOX, YOLOv8, and DINO, were benchmarked and evaluated for their in-season and cross-season weed detection performance on the dataset. All three models attained in-season detection accuracies of 92% and higher in terms of mAP@50. However substantial accuracy drops of up to 14.5% were observed between in-season and cross-season testing, especially for YOLOX and YOLOv8. Unsupervised domain adaptation based on an implicit instance-invariant network (I3Net) was utilized for improved generalization of the YOLO models. The I3Net-based models resulted in accuracy improvements of 1.4% and 3.3% for YOLOX and YOLOv8, respectively, compared to modeling without domain adaptation, in the cross-season testing. Both the two-season detection dataset11https://doi.org/10.5281/zenodo.10762138 and software programs22https://github.com/vicdxxx/CrossSeasonWeedDetection for weed detection modeling in this study are made publicly available.}
}
@article{KANG2024102482,
title = {A deep learning-based biomonitoring system for detecting water pollution using Caenorhabditis elegans swimming behaviors},
journal = {Ecological Informatics},
volume = {80},
pages = {102482},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102482},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000244},
author = {Seung-Ho Kang and In-Seon Jeong and Hyeong-Seok Lim},
keywords = {, Biomonitoring system, Water pollution, Water pollutant, Long short-term memory model, Branch length similarity entropy},
abstract = {Caenorhabditis elegans is a representative organism whose DNA structure has been fully elucidated. It has been used as a model organism for various analyses, including genetic functional analysis, individual behavioral analysis, and group behavioral analysis. Recently, it has also been studied as an important bioindicator of water pollution. In previous studies, traditional machine learning methods, such as the Hidden Markov Model (HMM), were used to determine water pollution and identify pollutants based on the differences in the swimming behavior of C. elegans before and after exposure to chemicals. However, these traditional machine learning models have low accuracy and a relatively high false-negative rate. This study proposes a method for detecting water pollution and identifying the types of pollutants using the Long Short-Term Memory (LSTM) model, a deep learning model suitable for time-series data analysis. The swimming activities of C. elegans in each of the image frames are characterized by the Branch Length Similarity (BLS) entropy profile. These BLS entropy profiles are converted into input vectors through additional preprocessing using two clustering methods. We conduct experiments using formaldehyde and benzene at 0.1 mg/L each, with observation time intervals varying from 30 to 180 s. The performance of the proposed method is compared with that of the previously proposed HMM approach and variants of LSTM models, such as Gated Recurrent Unit (GRU) and Bidirectional LSTM (BiLSTM).}
}
@article{HASAN2023102361,
title = {Image patch-based deep learning approach for crop and weed recognition},
journal = {Ecological Informatics},
volume = {78},
pages = {102361},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102361},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003904},
author = {A S M Mahmudul Hasan and Dean Diepeveen and Hamid Laga and Michael G.K. Jones and Ferdous Sohel},
keywords = {Weed classification, Deep learning, Patch-based technique, Digital agriculture},
abstract = {Accurate classification of weed species in crop plants plays a crucial role in precision agriculture by enabling targeted treatment. Recent studies show that artificial intelligence deep learning (DL) models achieve promising solutions. However, several challenging issues, such as lack of adequate training data, inter-class similarity between weed species and intra-class dissimilarity between the images of the same weed species at different growth stages or for other reasons (e.g., variations in lighting conditions, image capturing mechanism, agricultural field environments) limit their performance. In this research, we propose an image based weed classification pipeline where a patch of the image is considered at a time to improve the performance. We first enhance the images using generative adversarial networks. The enhanced images are divided into overlapping patches, a subset of which are used for training the DL models. For selecting the most informative patches, we use the variance of Laplacian and the mean frequency of Fast Fourier Transforms. At test time, the model's outputs are fused using a weighted majority voting technique to infer the class label of an image. The proposed pipeline was evaluated using 10 state-of-the-art DL models on four publicly available crop weed datasets: DeepWeeds, Cotton weed, Corn weed, and Cotton Tomato weed. Our pipeline achieved significant performance improvements on all four datasets. DenseNet201 achieved the top performance with F1 scores of 98.49%, 99.83% and 100% on Deepweeds, Corn weed and Cotton Tomato weed datasets, respectively. The highest F1 score on the Cotton weed dataset was 98.96%, obtained by InceptionResNetV2. Moreover, the proposed pipeline addressed the issues of intra-class dissimilarity and inter-class similarity in the DeepWeeds dataset and more accurately classified the minority weed classes in the Cotton weed dataset. This performance indicates that the proposed pipeline can be used in farming applications.}
}
@article{SONG2024102466,
title = {Benchmarking wild bird detection in complex forest scenes},
journal = {Ecological Informatics},
volume = {80},
pages = {102466},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102466},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000086},
author = {Qi Song and Yu Guan and Xi Guo and Xinhui Guo and Yufeng Chen and Hongfang Wang and Jianping Ge and Tianming Wang and Lei Bao},
keywords = {Object detection, Bird detection, Deep learning, Camera trap},
abstract = {Camera traps are widely used for wildlife monitoring and making informed conservation and land-management decisions, but the resulting ‘big data’ are laborious to process. Deep learning-based methods have been adopted for wildlife detection in camera traps. However, these methods detect large mammals in uncomplicated scenes, where powerful deep-learning models work effectively. Few studies have been conducted to develop artificial intelligence for recognizing wild birds that live in complicated field scenes with protective colors and small sizes. Here we used a dataset of 9717 images from 15 bird species based on camera traps to test 8 object detection algorithms (Faster RCNN, Cascade RCNN, RetinaNet, FCOS, RepPoints, ATSS, Deformable-DETR, and Sparse RCNN) and assess their performance. We also explored the effect of different backbones on model accuracy. Among them, the Cascade RCNN model performs best, with a mAP of 0.693 in model capabilities. Models perform differently in certain species, and backbones significantly affect the accuracy of the model. Cascade RCNN utilizing the Swin-T backbone is the best-performing combination, with a mAP of 0.704. This study could help researchers identify birds efficiently and inspires research on wildlife recognition in complex ecological settings.}
}
@article{CHEN2024102660,
title = {Detecting sun glint in UAV RGB images at different times using a deep learning algorithm},
journal = {Ecological Informatics},
volume = {81},
pages = {102660},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102660},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002024},
author = {Jiahao Chen and Yi Xiao and Yahui Guo and Mingwei Li and Xiran Li and Xuan Zhang and Fanghua Hao and Xiao Pu and Yongshuo Fu},
keywords = {Unmanned aerial vehicle, Water quality monitoring, Sun glint, Convolutional neural network, Res_AUNet network, WSGD dataset},
abstract = {Unmanned aerial vehicle (UAV) remote sensing has played a crucial role in water quality monitoring. However, the presence of water sun glint, resulting from specular reflection on the water surface, poses an inevitable challenge in UAV-acquired images. These excessively bright pixels disrupt the original images' spectral and textural characteristics, significantly diminishing their usability. This disruption has repercussions on subsequent tasks, such as target object classification and water quality parameter inversion. Precise detection of sun glint is a prerequisite for removing them, but current methods suffer from missed and false detections. In this study, we collected images by UAV to construct a specialized dataset for water surface sun glint, namely water sun glint detection (WSGD) dataset, laying the groundwork for further research endeavors. We proposed the Res_AUNet network by enhancing the UNet convolutional neural network. The Convolutional Block Attention Module was integrated into the encoding-decoding skip connections of the network, we also refined the convolutional blocks to better capture the distinctive semantic features associated with water sun glint. To mitigate overfitting, the residual structures were incorporated and the number of convolutional kernels within each block was reduced. The Res_AUNet network was trained and evaluated using the WSGD dataset, achieving metrics with an Accuracy of 98.02%, an F1-score of 83.67%, and an IOU of 74.73%. These results underscore the precision of our proposed method for water sun glint detection in UAV water images, offering valuable insights for effectively eliminating water sun glint and determining the optimal timing for UAV water image acquisition.}
}
@article{BHAGABATI2024102398,
title = {An automated approach for human-animal conflict minimisation in Assam and protection of wildlife around the Kaziranga National Park using YOLO and SENet Attention Framework},
journal = {Ecological Informatics},
volume = {79},
pages = {102398},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102398},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004272},
author = {Bijuphukan Bhagabati and Kandarpa Kumar Sarma and Kanak Chandra Bora},
keywords = {Computer vision, Object detection, Animal detection, Human-animal conflict, Kaziranga, Deep learning, Yolo},
abstract = {Human-animal conflict in Assam, India's north-eastern state, is rising continuously. Because it occurs year-round, it damages agricultural productivity and kills people and animals, including elephants. When a herd of wild elephants emerges from a deep forest and enters human-inhabited territory around the Kaziranga National Park (KNP) in Assam, an alert must be sounded for the neighbourhood residents and forest workers to prevent conflicts. Another concern is that many wild animals die near the KNP while crossing the national highway NH-37 which traverses the area. During floods, animals flee to the highlands for food and shelter. An automated animal identification and warning system near the KNP may reduce human-animal confrontations. This paper reports the design of a system that attempts to address the above concerns. Artificial Intelligence (AI)-based strategies are utilized to recognize wild animals from live video sequences, provide warnings to avoid encounters, and protect humans and animals. Deep learning models and YoloV5 with the SENet attention layer are used to recognize wild animals in real-time. This model is trained using a public and customized dataset of animal species. Cameras attached to the cloud-based AI system take photographs from several KNP locations to confirm the model. The model's 96% accuracy in animal photographs and videos taken day and night and in feed from contemporaneous location has shown its utility. The model also improves reliability by 1–13% over previous methods.}
}
@article{AZEDOU2023102333,
title = {Enhancing Land Cover/Land Use (LCLU) classification through a comparative analysis of hyperparameters optimization approaches for deep neural network (DNN)},
journal = {Ecological Informatics},
volume = {78},
pages = {102333},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102333},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300362X},
author = {Ali Azedou and Aouatif Amine and Isaya Kisekka and Said Lahssini and Youness Bouziani and Said Moukrim},
keywords = {Deep learning, Optimization algorithms, Image processing, Remote sensing, Land-use and land-cover classification, Google Earth Engine},
abstract = {Sustainable natural resources management relies on effective and timely assessment of conservation and land management practices. Using satellite imagery for Earth observation has become essential for monitoring land cover/land use (LCLU) changes and identifying critical areas for conserving biodiversity. Remote Sensing (RS) datasets are often quite large and require tremendous computing power to process. The emergence of cloud-based computing techniques presents a powerful avenue to overcome computing limitations by allowing machine-learning algorithms to process and analyze large RS datasets on the cloud. Our study aimed to classify LCLU for the Talassemtane National Park (TNP) using a Deep Neural Network (DNN) model incorporating five spectral indices to differentiate six land use classes using Sentinel-2 satellite imagery. Optimization of the DNN model was conducted using a comparative analysis of three optimization algorithms: Random Search, Hyperband, and Bayesian optimization. Results indicated that the spectral indices improved classification between classes with similar reflectance. The Hyperband method had the best performance, improving the classification accuracy by 12.5% and achieving an overall accuracy of 94.5% with a kappa coefficient of 93.4%. The dropout regularization method prevented overfitting and mitigated over-activation of hidden nodes. Our initial results show that machine learning (ML) applications can be effective tools for improving natural resources management.}
}
@article{NOMAN2023102047,
title = {Improving accuracy and efficiency in seagrass detection using state-of-the-art AI techniques},
journal = {Ecological Informatics},
volume = {76},
pages = {102047},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102047},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000766},
author = {Md Kislu Noman and Syed Mohammed Shamsul Islam and Jumana Abu-Khalaf and Seyed Mohammad Jafar Jalali and Paul Lavery},
keywords = {Deep learning, EfficientDet, Faster R-CNN, , NASNet, Seagrass, YOLOv5},
abstract = {Seagrasses provide a wide range of ecosystem services in coastal marine environments. Despite their ecological and economic importance, these species are declining because of human impact. This decline has driven the need for monitoring and mapping to estimate the overall health and dynamics of seagrasses in coastal environments, often based on underwater images. However, seagrass detection from underwater digital images is not a trivial task; it requires taxonomic expertise and is time-consuming and expensive. Recently automatic approaches based on deep learning have revolutionised object detection performance in many computer vision applications, and there has been interest in applying this to automated seagrass detection from imagery. Deep learning–based techniques reduce the need for hardcore feature extraction by domain experts which is required in machine learning-based techniques. This study presents a YOLOv5-based one-stage detector and an EfficientDetD7–based two-stage detector for detecting seagrass, in this case, Halophila ovalis, one of the most widely distributed seagrass species. The EfficientDet-D7–based seagrass detector achieves the highest mAP of 0.484 on the ECUHO-2 dataset and mAP of 0.354 on the ECUHO-1 dataset, which are about 7% and 5% better than the state-of-the-art Halophila ovalis detection performance on those datasets, respectively. The proposed YOLOv5-based detector achieves an average inference time of 0.077 s and 0.043 s respectively which are much lower than the state-of-the-art approach on the same datasets.}
}
@article{OLIVARESPINTO2024102653,
title = {Using honey bee flight activity data and a deep learning model as a toxicovigilance tool},
journal = {Ecological Informatics},
volume = {81},
pages = {102653},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102653},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400195X},
author = {Ulises Olivares-Pinto and Cédric Alaux and Yves {Le Conte} and Didier Crauser and Alberto Prado},
keywords = {Honeybee flight activity, Toxicovigilance, Neurotoxic pesticides, Recurrent neural network},
abstract = {Automatic monitoring devices placed at the entrances of honey bee hives have facilitated the detection of various sublethal effects related to pesticide exposure, such as homing failure and reduced flight activity. These devices have further demonstrated that different neurotoxic pesticide molecules produce similar sublethal impacts on flight activity. The detection of these effects was conducted a posteriori, following the recording of flight activity data. This study introduces a method using an artificial intelligence model, specifically a recurrent neural network, to detect the sublethal effects of pesticides in real-time based on honey bee flight activity. This model was trained on a flight activity dataset comprising 42,092 flight records from 1107 control and 1689 pesticide-exposed bees. The model was able to classify honey bees as healthy or pesticide-exposed based on the number of flights and minutes spent foraging per day. The model was the least accurate (68.46%) when only five days of records per bee were used for training. However, the highest classification accuracy of 99%, a Cohen Kappa of 0.9766, a precision of 0.99, a recall of 0.99, and an F1-score of 0.99 was achieved with the model trained on 25 days of activity data, signifying near-perfect classification ability. These results underscore the highly predictive performance of AI models for toxicovigilance and highlight the potential of our approach for real-time and cost-effective monitoring of risks due to exposure to neurotoxic pesticide in honey bee populations.}
}
@article{KORSCHENS2024102516,
title = {Determining the community composition of herbaceous species from images using convolutional neural networks},
journal = {Ecological Informatics},
volume = {80},
pages = {102516},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102516},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400058X},
author = {Matthias Körschens and Solveig Franziska Bucher and Paul Bodesheim and Josephine Ulrich and Joachim Denzler and Christine Römermann},
keywords = {Plant biodiversity, Plant cover, Deep learning, Convolutional neural networks, Semantic segmentation, Artificial intelligence},
abstract = {Global change has a detrimental impact on the environment and changes biodiversity patterns, which can be observed, among others, via analyzing changes in the composition of plant communities. Typically, vegetation relevées are done manually, which is time-consuming, laborious, and subjective. Applying an automatic system for such an analysis that can also identify co-occurring species would be beneficial as it is fast, effortless to use, and consistent. Here, we introduce such a system based on Convolutional Neural Networks for automatically predicting the species-wise plant cover. The system is trained on freely available image data of herbaceous plant species from web sources and plant cover estimates done by experts. With a novel extension of our original approach, the system can even be applied directly to vegetation images without requiring such cover estimates. Our extended approach, not utilizing dedicated training data, performs similarly to humans concerning the relative species abundances in the vegetation relevées. When trained on dedicated training annotations, it reflects the original estimates more closely than (independent) human experts, who manually analyzed the same sites. Our method is, with little adaptation, usable in novel domains and could be used to analyze plant community dynamics and responses of different plant species to environmental changes.}
}
@article{NAPPA2024102723,
title = {Probabilistic Bayesian Neural Networks for olive phenology prediction in precision agriculture},
journal = {Ecological Informatics},
volume = {82},
pages = {102723},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102723},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002656},
author = {A. Nappa and M. Quartulli and I. Azpiroz and S. Marchi and D. Guidotti and M. Staiano and R. Siciliano},
keywords = {Olive phenology, Modeling neural networks, Bayesian inference, Monte Carlo Dropout, Precision agriculture},
abstract = {Plant phenology is the study of cyclical events in a plant life cycle such as leaf bud burst, flowering, and fruiting. In this article the problem of olive phenology prediction is addressed through the use of Deep Learning. Although Neural Networks have already been used in this area, to the best of our knowledge, this is the first implementation of Probabilistic Bayesian Neural Networks for olive phenology prediction. This architecture gives particular emphasis to estimating the model uncertainty, both aleatoric and epistemic. The Bayesian Inference method, more precisely the Variational Inference one, is compared with the Monte Carlo Dropout technique, which is known to be a less computationally intensive approximation of Variational Inference. For validation purposes, models performance is compared to the state-of-the-art results.}
}
@article{JAMEI2024102455,
title = {Quantitative improvement of streamflow forecasting accuracy in the Atlantic zones of Canada based on hydro-meteorological signals: A multi-level advanced intelligent expert framework},
journal = {Ecological Informatics},
volume = {80},
pages = {102455},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102455},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004843},
author = {Mozhdeh Jamei and Mehdi Jamei and Mumtaz Ali and Masoud Karbasi and Aitazaz A. Farooque and Anurag Malik and Saad Javed Cheema and Travis J. Esau and Zaher Mundher Yaseen},
keywords = {Streamflow forecasting, Hydro-meteorological drivers, Multivariate variational mode decomposition, CNN-BiGRU, Boruta-CART, Multi-temporal},
abstract = {Developing reliable streamflow forecasting models is critical for hydrological tasks such as improving water resource management, analyzing river patterns, and flood forecasting. In this research, for the first time, an emerging multi-level TOPSIS (technique for order preference by similarity to the ideal solution) -based hybridization comprised of the Boruta classification and regression tree (Boruta-CART) feature selection, multivariate variational mode decomposition (MVMD), and a hybrid Convolutional Neural Network (CNN) Bidirectional Gated Recurrent Unit (CNN-BiGRU) deep learning was adopted to multi-temporal (one and three days ahead) forecast the daily streamflow in the Rivers of Prince Edward Island, Canada. For this aim, in the first step, the Boruta-CART feature selection technique determines the most effective lagged components among all the antecedent two-day information (i.e., t-1 and t-2) of hydro-meteorological features (from 2015 to 2020), including the water level, mean air temperature, heat degree days, total precipitation, dew point temperature, and relative humidity in the Bear and Winter Rivers of Prince Edward Island, Canada. Afterwards, a multivariate variational mode decomposition (MVMD) decomposes the input time series to decrease the complexity and non-linearity of the non-stationary ones before feeding the deep learning (DL) models. Here, the CNN-GRU was employed as the primary DL model, along with the kernel extreme machine method (KELM), random variational function link (RVFL), and hybrid CNN bidirectional recurrent neural network (CNN-BiRNN) as the comparative models. A TOPSIS scheme applying several performance measures like the correlation coefficient (R), root mean square error (RMSE), and reliability was designed for the robustness assessment of the hybrid (MVM-CNN-BiGRU, MVM-CNN-BiRNN, MVM-RVFL, and MVM-KELM) and standalone models. The computational outcomes revealed that in the Bear River, the MVM-CNN-BiGRU, owing to its best forecasting performance (one day ahead: TOPSIS score 1, R = 0.960, RMSE = 0.098, and reliability = 65.082; three days ahead: TOPSIS score = 0.999, R = 0.924, and RMSE = 0.33) outperformed the other hybrid models, followed by the MVM-CNN-BiRNN, MVM-RVFL, and MVM-KELM, respectively. Moreover, in the Winter River, the MVM-CNN-BiGRU in terms of (one-day ahead: TOPSIS score = 0.890, R = 0.955, RMSE = 0.274, and reliability = 34.004; three-days ahead: TOPSIS score = 0.686, R = 0.924, and RMSE = 0.330) was superior to the other models. The provided expert system could be vital in the local flood decision-making process, in the absence of streamflow information as input modeling, during the flood seasons to reduce flood damage in residential areas.}
}
@article{DUBUS2024102642,
title = {From citizen science to AI models: Advancing cetacean vocalization automatic detection through multi-annotator campaigns},
journal = {Ecological Informatics},
volume = {81},
pages = {102642},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102642},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001845},
author = {Gabriel Dubus and Dorian Cazau and Maëlle Torterotot and Anatole Gros-Martial and Paul {Nguyen Hong Duc} and Olivier Adam},
keywords = {Marine bioacoustics, Passive acoustic monitoring, Citizen science, Multi-annotation, Deep learning for automatic detection, Convolutional neural networks, Soft labeling},
abstract = {Continuous underwater Passive Acoustic Monitoring (PAM) has emerged as a strong tool for cetacean research. To handle the vast volume of collected data, it is essential to employ automated detection and classification methods. The recent advancement of deep learning, involving model training and testing, requires a large amount of labeled data. These labels are derived through the manual annotation of audio files often reliant on human experts. Based on an annotation campaign focusing on blue whale calls in the Indian Ocean involving 19 novice annotators and one expert in bioacoustics, this study explores the integration of novice annotators in marine bioacoustics research, through citizen science programs, which could drastically increase the size of labeled datasets and enhance the performance of detection and classification models. The analysis reveals distinctive annotation profiles influenced by the complexity of vocalizations and the annotators' strategies, ranging from conservative to permissive. To address the challenges of annotation discrepancies, Convolutional Neural Networks (CNNs) are trained on annotations from both novices and the expert. The results show variations in model performance. Our work highlights the importance of annotation guidelines encouraging a more conservative approach to improve overall annotation quality. In an effort to optimize the potential of multi-annotation and mitigate the presence of noisy labels, two annotation aggregation methods (majority voting and soft labeling) are proposed and tested. The results demonstrate that both methods, particularly when a sufficient number of annotators are involved, significantly improve model performance and reduce variability: the standard deviation of the area under PR and ROC curves fall under 0.02 for both vocalizations with 13 aggregated annotators, while it was at 0.17 and 0.21 for the Blue Whale Dcalls and 0.05 and 0.04 for the SEIO PBW vocalizations with all annotators separately. Moreover, these aggregation methods enable the training of models using non-expert annotations that achieve performance of models trained with expert annotations. These findings suggest that crowdsourced annotations from novice annotators can be a viable alternative to expert annotations.}
}
@article{ALI2024102618,
title = {An ensemble of deep learning architectures for accurate plant disease classification},
journal = {Ecological Informatics},
volume = {81},
pages = {102618},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102618},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001602},
author = {Ali Hussein Ali and Ayman Youssef and Mahmoud Abdelal and Muhammad Adil Raja},
keywords = {Plant leaf disease, Agriculture, Ensemble deep learning models},
abstract = {A substantial fraction of agricultural produce loss can be attributed to plant diseases. Agricultural yield loss can have far-reaching consequences for a country's economy and contribute to global food insecurity. Early detection of plant diseases can be instrumental in maintaining global health and welfare. A pathologist's visual evaluation is typically used to make an early diagnosis of plant diseases. This technique involves experts or farmers examining plants with the naked eye and classifying the disease depending on their previous experience. This conventional approach includes drawbacks like low accuracy and the need for human expertise. This motivates researchers to investigate automated systems for the early diagnosis of plant diseases. To achieve this goal an ensemble of different deep learning architectures (DenseNet201, efficientNetB0, inceptionresnetV2, efficientNetB3) is introduced to increase the classification accuracy of plant leaf diseases. In this work, a novel image-processing technique is proposed to increase the efficiency of deep-learning models. Also, a data balancing technique is used to solve the problem of the imbalanced dataset. Five different deep-learning models are trained and tested using the largest plant disease dataset; PlantVillage. Ten different ensembles (chosen randomly) of the deep learning models are tested and compared to find the ensemble with the highest accuracy. The proposed ensemble model was able to achieve 99.89% accuracy on the New PlantVillage dataset. PlantVillage is a challenging dataset with 38 classes. Achieving high accuracies on such a dataset proves the ability of the system to generalize on unseen data or real-world scenarios. A comparison with the state-of-the-art is made with other available models from the literature. A section about this is added to show the superior performance of the proposed ensemble model in terms of accuracy and F1-score.}
}
@article{KUMAR2024102699,
title = {Improving learning-based birdsong classification by utilizing combined audio augmentation strategies},
journal = {Ecological Informatics},
volume = {82},
pages = {102699},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102699},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002413},
author = {Arunodhayan Sampath Kumar and Tobias Schlosser and Stefan Kahl and Danny Kowerko},
keywords = {Audio classification, Augmentation strategies, Birdsong soundscapes, Computer vision and pattern recognition, Convolutional neural networks, Vision transformers},
abstract = {In ecology, changes in environmental conditions are often closely linked to shifts in species diversity. This relationship can be investigated by analyzing avian vocalizations, which are robust indicators of trends in biodiversity. Within this contribution, we explored various data augmentation techniques and deep learning strategies for the classification of birdsong within natural soundscapes. For this purpose, we employed three fundamental deep neural network architectures, such as vision transformers, to classify 397 different bird species. To improve both the accuracy and generalizability of our models, we incorporated up to 19 well-established data augmentation techniques commonly used in audio classification. This included an iterative selection process where only augmentations that enhanced classification performance were selected. The primary augmentation technique involved the integration of various noise samples and non-bird audio elements, which significantly improved model performance as assessed on the BirdCLEF 2021 data set. Individual augmentations achieved F1-scores from 48.0 % (vertical flip) to 72.6 % (primary background noise soundscapes). Through the strategic combination of key techniques – namely simulated pink noise, interspecies sound mixing, and loudness normalization – we achieved a top F1-score of 73.7%. Depending on the selected classification model, this corresponds to an improvement by 4.81 % to 10.5 %. Improvements and deteriorations of all applied augmentation techniques appeared to be robust across our three evaluated models. Therefore, our approach highlights the potential of sophisticated audio augmentations in refining the accuracy and robustness of birdsong classification models.}
}
@article{CARDOSO2024102602,
title = {Can citizen science and social media images support the detection of new invasion sites? A deep learning test case with Cortaderia selloana},
journal = {Ecological Informatics},
volume = {81},
pages = {102602},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102602},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001444},
author = {Ana Sofia Cardoso and Eva Malta-Pinto and Siham Tabik and Tom August and Helen E. Roy and Ricardo Correia and Joana R. Vicente and Ana Sofia Vaz},
keywords = {Artificial intelligence, Convolutional neural networks, Computer vision, Pampas grass},
abstract = {Deep learning has advanced the content analysis of digital data, unlocking opportunities for detecting, mapping, and monitoring invasive species. Here, we tested the ability of open source classification and object detection models (i.e., convolutional neural networks: CNNs) to identify and map the invasive plant Cortaderia selloana (pampas grass) in mainland Portugal. CNNs were trained over citizen science images and then applied to social media content (from Flickr, Twitter, Instagram, and Facebook), allowing to classify or detect the species in over 77% of situations. Images where the species was identified were mapped, using their georeferenced coordinates and time stamp, showing previously unreported occurrences of C. selloana, and a tendency for the species expansion from 2019 to 2021. Our study shows great potential from deep learning, citizen science and social media data for the detection, mapping, and monitoring of invasive plants, and, by extension, for supporting follow-up management options.}
}
@article{ZHANG2024102517,
title = {Automatic bioacoustics noise reduction method based on a deep feature loss network},
journal = {Ecological Informatics},
volume = {80},
pages = {102517},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102517},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000591},
author = {Chengyun Zhang and Kaiying He and Xinghui Gao and Yingying Guo},
keywords = {Bioacoustics, Noise reduction, Deep learning, Deep feature loss network},
abstract = {Acoustic sensors that collect acoustic data over extended periods and broad ranges are widely used in bioacoustics monitoring. However, in open environments, acoustic data collected using acoustic sensors can be subject to interference from various real-world noises, thereby influencing the subsequent analysis and processing of bioacoustic data. Existing bioacoustic noise reduction methods are limited in their application because of their low efficiency, unsuitability for non-stationary noise, generally unimproved signal-to-noise ratio (SNR) efficacy, and considerable amounts of residual noise. These limitations hinder the effective processing of recorded signals for which extraneous noise overlaps with bird vocalizations. In this study, we propose a bioacoustic noise reduction method based on a deep feature loss network for bird sounds. The method has a rapid denoising speed and can more effectively remove background noise from field recording signals without distorting the bird acoustic spectrum. The denoising effects of the proposed method were compared with those of a speech enhancement generative adversarial network, web real-time communications denoising, and other noise reduction methods. The denoising ability of these methods for different noises was evaluated using spectrograms and objective evaluation measures such as the SNR and perceptual evaluation of speech quality (PESQ). The experimental results revealed that our proposed noise reduction method can obtain higher SNRs and PESQ scores than other noise reduction methods, with the SNR increasing by up to 35.83 dB following denoising.}
}
@article{ZHANG2024102399,
title = {Fully automatic system for fish biomass estimation based on deep neural network},
journal = {Ecological Informatics},
volume = {79},
pages = {102399},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102399},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004284},
author = {Tianye Zhang and Yuqiao Yang and Yueyue Liu and Chenglei Liu and Ran Zhao and Daoliang Li and Chen Shi},
keywords = {Aquaculture, Mass estimation, Neural network, Sustainable production},
abstract = {The approach for estimating biomass in non-contact, free-swimming fish has encountered difficulties such as fish body occlusion, bending, non-orthogonal angles, and low efficiency. To address these issues, this study had combined fish posture recognition (using deep learning technology) with biomass estimation (utilizing stereo vision technology) for the first time, and developed a fast, precise, and fully automatic fish biomass estimation system. The improved single-stage target detection algorithm significantly improved the direct detection and extraction of high-quality images of moving fish in real-time, eliminating the need for manual processing of images that may have imperfect posture. Fish body length and height were measured in the real world using binocular stereo vision technology. Finally, the fish body weight can be estimated by considering the relationship among their body length, height, and weight. The experiment confirmed that the system had successfully avoided influencing factors that could affect fully automatic estimation. The results demonstrated a strong linear relationship between the estimated and measured fish body weights, with a mean relative error (MRE) of 2.87%. There were no significant differences between the estimated and measured weights (p = 0.94). The MRE of the multi-factor model was much lower than that of the single-factor model (length-weight of 8.86% and height-weight of 7.41%). The results indicate that the system developed is a highly effective approach to fully automated biomass estimation. This can be used to guide actual production and further study of the mechanism of fish growth.}
}
@article{ZOU2024102562,
title = {Bacterial community characterization by deep learning aided image analysis in soil chips},
journal = {Ecological Informatics},
volume = {81},
pages = {102562},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102562},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001043},
author = {Hanbang Zou and Alexandros Sopasakis and François Maillard and Erik Karlsson and Julia Duljas and Simon Silwer and Pelle Ohlsson and Edith C. Hammer},
keywords = {Soil bacterial cell counting, Segmentation, Microfluidics, Microbial image recognition, Morphological biodiversity, Bacterial traits},
abstract = {Soil microbes play an important role in governing global processes such as carbon cycling, but it is challenging to study them embedded in their natural environment and at the single cell level due to the opaque nature of the soil. Nonetheless, progress has been achieved in recent years towards visualizing microbial activities and organo-mineral interaction at the pore scale, especially thanks to the development of microfluidic ‘soil chips’ creating transparent soil model habitats. Image-based analyses come with new challenges as manual counting of bacteria in thousands of digital images taken from the soil chips is excessively time-consuming, while simple thresholding cannot be applied due to the background of soil minerals and debris. Here, we adopt the well-developed deep learning algorithm Mask-RCNN to quantitatively analyze the bacterial communities in soil samples from different locations in the world. This work demonstrates analysis of bacterial abundance from three contrasting locations (Greenland, Sweden and Kenya) using deep learning in microfluidic soil chips in order to characterize population and community dynamics. We additionally quantified cell- and colony morphology including cell size, shape and the cell aggregation level via calculation of the distance to the nearest neighbor. This approach allows for the first time an automated visual investigation of soil bacterial communities, and a crude biodiversity measure based on phenotypic cell morphology, which could become a valuable complement to molecular studies.}
}
@article{BJERGE2023102278,
title = {Hierarchical classification of insects with multitask learning and anomaly detection},
journal = {Ecological Informatics},
volume = {77},
pages = {102278},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102278},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003072},
author = {Kim Bjerge and Quentin Geissmann and Jamie Alison and Hjalte M.R. Mann and Toke T. Høye and Mads Dyrmann and Henrik Karstoft},
keywords = {nomaly detection, Computer vision, Deep learning, Hierarchical classification, Insects, Taxonomy},
abstract = {Cameras and computer vision are revolutionising the study of insects, creating new research opportunities within agriculture, epidemiology, evolution, ecology and monitoring of biodiversity. However, the diversity of insects and close resemblances of many species are a major challenge for image-based species-level classification. Here, we present an algorithm to hierarchically classify insects from images, leveraging a simple taxonomy to (1) classify specimens across multiple taxonomic ranks simultaneously, and (2) identify the lowest rank at which a reliable classification can be reached. Specifically, we propose multitask learning, a loss function incorporating class dependency at each taxonomic rank, and anomaly detection based on outlier analysis to quantify the uncertainty. First, we compile a dataset of 41,731 images of insects, combining images from time-lapse monitoring of floral scenes with images from the Global Biodiversity Information Facility (GBIF). Second, we adapt state-of-the-art convolutional neural networks, ResNet and EfficientNet, for the hierarchical classification of insects belonging to three orders, five families and nine species. Third, we assess model generalization for 11 species unseen by the trained models. Here, anomaly detection is used to predict the higher rank of the species which were not present in the training set. We found that incorporating a simple taxonomy into our model increased the accuracy at higher taxonomic ranks. As expected, our algorithm correctly classified new insect species at higher taxonomic ranks, while classification was uncertain at lower taxonomic ranks. Anomaly detection can effectively flag novel taxa that are visually distinct from species in the training data. However, five novel taxa were consistently mistaken for visually similar species in the training data. Above all, we have demonstrated a practical approach to hierarchical classification based on species taxonomy and uncertainty during automated in situ monitoring of live insects. Our method is simple and versatile, forming a valuable step towards high-level classification of species not found in training data.}
}
@article{AZADNIA2024102683,
title = {Medicinal and poisonous plants classification from visual characteristics of leaves using computer vision and deep neural networks},
journal = {Ecological Informatics},
volume = {82},
pages = {102683},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102683},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002255},
author = {Rahim Azadnia and Faramarz Noei-Khodabadi and Azad Moloudzadeh and Ahmad Jahanbakhshi and Mahmoud Omid},
keywords = {Medicinal plants, Image classification, Machine learning, Deep learning, Data augmentation, Fast AutoAugment},
abstract = {Poisonous plants are the third largest category of poisons known globally, which pose a risk of poisoning and death to humans. Currently, the identification of medicinal and poisonous plants is done by humans using experimental methods, which are not accurate and are associated with many errors, and also the use of laboratory methods requires experts and this method is very costly and time-consuming. Therefore, distinguishing between medicinal and poisonous plants is very important using emerging, non-destructive, fast and accurate methods such as computer vision and artificial intelligence. In this study, we propose a robust and generalized model using spatial attention (SA) and channel attention (CA) modules for the classification of different plants. A dataset containing 900 confirmed images of three plant classes (oregano, poisonous and weed) was used. The attention mechanisms enhance efficiency of deep learning (DL) networks by allowing them to precisely focus on all relevant input elements. In order to enhance the performance of the proposed model, the CA was implemented based on four pooling operations including global average pooling-based CA (GAP-CA), mixed pooling-based CA (Mixed-CA), gated pooling-based CA (Gated-CA), and tree pooling-based CA (Tree-CA) operations. The results showed that the DL model based on Tree-CA had promising performance and outperformed other state-of-the-art models, achieving the values of 99.63%, 99.38%, 99.52%, 99.74%, and 99.42%, for accuracy, precision, recall, specificity, and F1-score, respectively. The findings support our proposed attention model's success in identifying medicinal plants from similar poisonous plants. Recent advancements in computer-based technologies and artificial intelligence enable automatic detection of medicinal and poisonous plants, revolutionizing traditional identification methods.}
}
@article{YANG2024102477,
title = {Deep learning-based air pollution analysis on carbon monoxide in Taiwan},
journal = {Ecological Informatics},
volume = {80},
pages = {102477},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102477},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000190},
author = {Cheng-Hong Yang and Po-Hung Chen and Chih-Hsien Wu and Cheng-San Yang and Li-Yeh Chuang},
keywords = {Air pollution, Carbon Monoxide (CO), Deep learning, Seasonal gated recurrent unit (SGRU)},
abstract = {Global air pollution poses a threat to humanity. Specifically, CO directly affects cardiovascular and other organ tissues and leads to numerous chronic diseases and major public health problems. The effective implementation of a deep learning model for predicting variations in CO levels would enable the early formulation of policies for controlling air pollution. In this study, a seasonal gated recurrent unit (SGRU) model, which is a deep learning time-series prediction model, was developed to predict the levels of CO in Taiwan. Atmospheric CO measurements from 2005 to 2021 were collected from the Environmental Protection Administration of Taiwan and preprocessed using the Kalman filter to achieve accurate forecasting. The performance of the proposed SGRU model was compared with that of the autoregressive integrated moving average (ARIMA), seasonal ARIMA, exponential smoothing (ETS), Holt–Winters ETS, support vector regression, and seasonal long short-term memory models in terms of mean absolute percentage error (MAPE) and root mean square error. The SGRU model achieved the lowest MAPE value of 0.94, which demonstrated its superior performance. The construction of an accurate air pollution prediction model can assist government entities in formulating health and social care strategies and in planning future air pollution control measures.}
}
@article{RIECHMANN2022101657,
title = {Motion vectors and deep neural networks for video camera traps},
journal = {Ecological Informatics},
volume = {69},
pages = {101657},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101657},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122001066},
author = {Miklas Riechmann and Ross Gardiner and Kai Waddington and Ryan Rueger and Frederic Fol Leymarie and Stefan Rueger},
keywords = {AI, Computer vision, Machine learning, Motion vectors, Video camera trap, Video pipeline},
abstract = {Commercial camera traps are usually triggered by a Passive Infra-Red (PIR) motion sensor necessitating a delay between triggering and the image being captured. This often seriously limits the ability to record images of small and fast moving animals. It also results in many “empty” images, e.g., owing to moving foliage against a background of different temperature. In this paper we detail a new triggering mechanism based solely on the camera sensor. This is intended for use by citizen scientists and for deployment on an affordable, compact, low-power Raspberry Pi computer (RPi). Our system introduces a video frame filtering pipeline consisting of movement and image-based processing. This makes use of Machine Learning (ML) feasible on a live camera stream on an RPi. We describe our free and open-source software implementation of the system; introduce a suitable ecology efficiency measure that mediates between specificity and recall; provide ground-truth for a video clip collection from camera traps; and evaluate the effectiveness of our system thoroughly. Overall, our video camera trap turns out to be robust and effective.}
}
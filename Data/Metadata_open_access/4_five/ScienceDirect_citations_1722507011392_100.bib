@article{VIZCARRA2021101268,
title = {The Peruvian Amazon forestry dataset: A leaf image classification corpus},
journal = {Ecological Informatics},
volume = {62},
pages = {101268},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101268},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000595},
author = {Gerson Vizcarra and Danitza Bermejo and Antoni Mauricio and Ricardo {Zarate Gomez} and Erwin Dianderas},
keywords = {Leaves dataset, Peruvian Amazon, Deep learning, Visual interpretation, Interpretation},
abstract = {Forest census allows getting precise data for logging planning and elaboration of the forest management plan. Species identification blunders carry inadequate forest management plans and high risks inside forest concessions. Hence, an identification protocol prevents the exploitation of non-commercial or endangered timber species. The current Peruvian legislation allows the incorporation of non-technical experts, called “materos”, during the identification. Materos use common names given by the folklore and traditions of their communities instead of formal ones, which generally lead to misclassifications. In the real world, logging companies hire materos instead of botanists due to cost/time limitations. Given such a motivation, we explore an end-to-end software solution to automatize the species identification. This paper introduces the Peruvian Amazon Forestry Dataset, which includes 59,441 leaves samples from ten of the most profitable and endangered timber-tree species. The proposal contemplates a background removal algorithm to feed a pre-trained CNN by the ImageNet dataset. We evaluate the quantitative (accuracy metric) and qualitative (visual interpretation) impacts of each stage by ablation experiments. The results show a 96.64% training accuracy and 96.52% testing accuracy on the VGG-19 model. Furthermore, the visual interpretation of the model evidences that leaf venations have the highest correlation in the plant recognition task.}
}
@article{KATH2024102710,
title = {Leveraging transfer learning and active learning for data annotation in passive acoustic monitoring of wildlife},
journal = {Ecological Informatics},
volume = {82},
pages = {102710},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102710},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002528},
author = {Hannes Kath and Patricia P. Serafini and Ivan B. Campos and Thiago S. Gouvêa and Daniel Sonntag},
keywords = {Active learning, Transfer learning, Passive acoustic monitoring, BirdNet},
abstract = {Passive Acoustic Monitoring (PAM) has emerged as a pivotal technology for wildlife monitoring, generating vast amounts of acoustic data. However, the successful application of machine learning methods for sound event detection in PAM datasets heavily relies on the availability of annotated data, which can be laborious to acquire. In this study, we investigate the effectiveness of transfer learning and active learning techniques to address the data annotation challenge in PAM. Transfer learning allows us to use pre-trained models from related tasks or datasets to bootstrap the learning process for sound event detection. Furthermore, active learning promises strategic selection of the most informative samples for annotation, effectively reducing the annotation cost and improving model performance. We evaluate an approach that combines transfer learning and active learning to efficiently exploit existing annotated data and optimize the annotation process for PAM datasets. Our transfer learning observations show that embeddings produced by BirdNet, a model trained on high signal-to-noise recordings of bird vocalisations, can be effectively used for predicting anurans in PAM data: a linear classifier constructed using these embeddings outperforms the benchmark by 21.7%. Our results indicate that active learning is superior to random sampling, although no clear winner emerges among the strategies employed. The proposed method holds promise for facilitating broader adoption of machine learning techniques in PAM and advancing our understanding of biodiversity dynamics through acoustic data analysis.}
}
@article{ZHANG2024102556,
title = {A reliable unmanned aerial vehicle multi-target tracking system with global motion compensation for monitoring Procapra przewalskii},
journal = {Ecological Informatics},
volume = {81},
pages = {102556},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102556},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000980},
author = {Guoqing Zhang and Yongxiang Zhao and Ping Fu and Wei Luo and Quanqin Shao and Tongzuo Zhang and Zhongde Yu},
keywords = { monitoring, UAV MOT, GMC, Deep SORT},
abstract = {Procapra przewalskii, which inhabits plateau areas, faces the constant threat of poaching and unpredictable risks that impede its survival. The implementation of a comprehensive, real-time monitoring and tracking system for Procapra przewalskii using artificial intelligence and unmanned aerial vehicle (UAV) technology is crucial to safeguard its existence. Therefore, a UAV multi-object-tracking (MOT) system with global motion compensation (GMC) was proposed in this study. YOLOv7 and Deep SORT were employed for object detection and tracking, respectively. Furthermore, the Kalman filter (KF) in Deep SORT is optimized to enhance the accuracy of object-tracking. Moreover, a novel appearance feature-extraction network (FEN) is introduced to enable more effective multi-scale feature (MSF) extraction. In addition, a GMC module was proposed to align neighboring frames through feature matching. This facilitates the correction of the position of the target in the subsequent frame, mitigating the impact of UAV camera motion on tracking. The results demonstrated the remarkable tracking accuracy of the system. Compared with the Deep SORT model, the proposed system exhibited an increase of 6.4% in MOTA, 2.7% in MOTP, and 7.9% in IDF1. Through a comprehensive evaluation and analysis of real-world tracking scenarios, the system proposed in this study exhibits reliability in complex scenes and holds the potential to significantly enhance the protection of Procapra przewalskii from threats.}
}
@article{ZHANG2023102242,
title = {Research on the identification of land types and tree species in the Engebei ecological demonstration area based on GF-1 remote sensing},
journal = {Ecological Informatics},
volume = {77},
pages = {102242},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102242},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002716},
author = {Jie Zhang and Yanyan Zhang and Tiantian Zhou and Yi Sun and Zhichao Yang and Shulin Zheng},
keywords = {Multi-scale segmentation, Tree species identification, Nearest neighbor classification, Random forest classification, Object-oriented classification, Change detection},
abstract = {Identifying forest types is crucial in satellite remote sensing monitoring. Research focusing on the identification of forest tree species using high-resolution remote sensing images is on the rise. Traditional pixel-based classification methods have not been successful in fully utilizing the rich spatial and texture feature information present in the image. Further, these methods are plagued by “classification noise”, which impedes the precise extraction of information regarding forest tree species. The object-oriented classification method, on the other hand, offers an extraction methodology rooted in image segmentation object features. It efficiently leverages the spectral, texture, and spatial geometry information of high-resolution remote sensing images. In this study, an initial application of the object-oriented, multi-level information extraction was conducted on ground objects within the Engebei ecological demonstration area. This was performed utilizing Gaofen (GF)-1 remote sensing images. Following this, a multi-scale segmentation algorithm was utilized to establish different segmentation scales relative to the features of the different objects. The optimal segmentation scale, shape factor, and compactness factor were determined to be 222/0.5/0.5, 167/0.5/0.6, and 81/0.4/0.6, respectively, for the three levels. Subsequently, training samples were used to select optimal features during the classification process. The nearest neighbor classification and random forest classification were performed respectively. The results indicate an overall classification accuracy of 88.58% and 87.26% for the nearest neighbor classification of remote sensing images in 2020 and 2021 respectively. Meanwhile, the overall classification accuracy for the random forest classification was 92.95% and 92.02% respectively. The Kappa coefficients for the nearest neighbor classification were 0.86 and 0.85, whereas those for the random forest classification were 0.92 and 0.90. These outcomes underscore the utility of the object-oriented classification method in enhancing the accuracy of forest type identification.}
}
@article{CROCKER2024102698,
title = {Synthetic data for reef modelling},
journal = {Ecological Informatics},
volume = {82},
pages = {102698},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102698},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002401},
author = {Rose Crocker and Barbara J. Robson and Chinenye Ani and Ken Anthony and Takuya Iwanaga},
keywords = {Synthetic data, Reef modelling, Data pipeline, Model testing and validation, Decision support tools, Machine learning, Neural networks, Synthetic data vault},
abstract = {Synthetic data mimics the statistical properties of real-world datasets while removing reference to sensitive or confidential information in the original dataset (Quintana, 2020). Synthetic data is also useful for general model testing and development, with many methods available for generating data from machine learning models (Raghunathan, 2021). Although not widely used in the context of ecological and environmental modelling, synthetic data can support and accelerate model testing and analyses where rightsholders are sensitive to data disclosure for study areas, or data collection is expensive. In the context of reef modelling, synthetic data can be used to support model analyses that can be published without referring to specific sites, reefs, or study areas. This is desirable in the context of decision support for restoration of the Great Barrier Reef. The Reef has many stakeholders and release of early modelling results for intervention scenarios for specific areas would be premature until management or intervention strategy options have been discussed with stakeholders and/or rightsholders. Synthetic data allows a path to publish model and method demonstrations to share knowledge with the reef decision support community without prematurely suggesting policy recommendations for reefs which are sensitive to rightsholders or stakeholders. We showcase a synthetic data pipeline developed for the reef decision-support system ADRIA (Adaptive Dynamic Reef Intervention Algorithms), using methods from the Python package Synthetic Data Vault (Patki et al., 2016) and others. The synthetic data models are developed to emulate the statistics of case-study reefs for publishing decision-support tool demonstrations, testing and method validation without revealing sensitive reef site information. This pipeline includes developing models for tabular (benthic/compositional reef data), spatial-temporal (wave and heat stress data) and spatial network data (coral larval connectivity). Conditional sampling methods which connect spatial relationships across datasets are used to develop synthetic reef data packages which mimic the statistical properties of the original dataset. The utility of the synthetic data is demonstrated on a sample reef data package, and methods used for anonymizing the data are detailed. The results are discussed in the context of formalizing synthetic data for reef modelling. All synthetic data code is available at ADRIA-synthetic-data/README.md at v0.1.0 · open-AIMS/ADRIA-synthetic-data (github.com), DOI: https://doi.org/10.5281/zenodo.10158323.}
}
@article{DUAN2024102637,
title = {SIAlex: Species identification and monitoring based on bird sound features},
journal = {Ecological Informatics},
volume = {81},
pages = {102637},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102637},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001791},
author = {Lin Duan and Lidong Yang and Yong Guo},
keywords = {Lightweight, Cascading activation function, Bird sound recognition, Structural re-parameterization, Nonlinear performance},
abstract = {The combination of deep learning and bird sound recognition is widely employed in bird species conservation monitoring. A complex network structure is not conducive for deploying bird sound recognition devices, resulting in problems such as long inference time and low efficiency. Using AlexNet as the backbone model, we explore the potential of shallow and straightforward models without complex connection techniques or attention mechanisms, named SIAlex, to recognise and classify 20 bird sound datasets, which are simultaneously validated on a 10 class UrbanSound8k dataset. Using the structural re-parameterization method, the number of model layers is reduced, computational efficiency is improved, and the inference time is significantly reduced, achieving a decoupling of training and inference time in the structure. To increase the nonlinearity of the model, a cascaded approach is utilised to increase the number of activation functions, thereby significantly improving the generalisation performance of the model. Simultaneously, in the classifier section, convolutional layer replaces the original fully connected layer, thereby reducing the inference time and increasing the feature extraction ability of the model, improving accuracy, and effectively recognising bird speech. The experimental data show that the SIAlex network on the Birdsdata dataset improves the accuracy to 93.66%, and the inference time for a piece of data is only 2.466 ms. The accuracy of the UrbanSound8k dataset reaches 96.04%, and the inference time for a piece of data is 3.031 ms. A large number of experimental comparisons have shown that the method proposed in this paper achieves good results in reducing the inference time of the model, bringing breakthroughs in the application of shallow, simple models.}
}
@article{MCEWEN2024102734,
title = {Active few-shot learning for rare bioacoustic feature annotation},
journal = {Ecological Informatics},
volume = {82},
pages = {102734},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102734},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002760},
author = {Ben McEwen and Kaspar Soltero and Stefanie Gutschmidt and Andrew Bainbridge-Smith and James Atlas and Richard Green},
keywords = {Active learning, Audio classification, Bioacoustics, Few-shot learning, Machine learning},
abstract = {The collection and annotation of bioacoustic data present several challenges to researchers. Bioacoustic monitoring of rare (sparse) or cryptic species generally encounter two main issues. The cost of collecting and processing field data and a lack of labelled datasets for the target species. The detection of invasive species incursions and probability of absence testing is especially challenging due to these species having population densities at or close to zero. We present a methodology specifically designed to aid in the analysis of rare acoustic events within long-term field recordings. This approach combines a wavelet-based segmentation method that automatically extracts transient features from within-field recordings. A few-shot active learning recommender system in a human-in-the-loop process prioritises the annotation of low-certainty samples. This process combines the accuracy of human classification and the speed of computational tools to greatly reduce the presence of non-target features in field recordings. We evaluate this approach using an invasive species identification case study. This methodology achieves a test accuracy of 98.4% as well as 81.2% test accuracy using 2-shot, 2-way prototypical learning without fine-tuning, demonstrating high performance at varying data availability contexts. Active learning using low-certainty samples achieves >90% test accuracy using only 20 training samples compared to 80 samples without active learning. This approach allows users to train custom audio classification models for any application with rare features. The model can be easily exported for use in the field making real-time bioacoustic monitoring of less-vocal species a possibility. All code and data are available at https://github.com/Listening-Lab/Annotator.}
}
@article{CANOVI2024102733,
title = {Trajectory-based fish event classification through pre-training with diffusion models},
journal = {Ecological Informatics},
volume = {82},
pages = {102733},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102733},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002759},
author = {Noemi Canovi and Benjamin A. Ellis and Tonje K. Sørdalen and Vaneeda Allken and Kim T. Halvorsen and Ketil Malde and Cigdem Beyan},
keywords = {Fish behavior, Underwater videos, Event recognition, Trajectory, Generative models, Autoencoder, Diffusion model, Corkwing wrasse},
abstract = {This study contributes to advancing the field of automatic fish event recognition in natural underwater videos, addressing the current gap in studying fish interaction and competition, including predator-prey relationships and mating behaviors. We used the corkwing wrasse (Symphodus melops) as a model, a marine species of commercial importance that reproduces in sea-weed nests built and cared for by a single male. These nests attract a wide range of visitors and are the focal point for behavior such as spawning, chasing, and maintenance. We propose a deep learning methodology to analyze the movement trajectories of the nesting male and classify the associated events observed in their natural habitat. Our approach leverages unsupervised pre-training based on diffusion models, leading to improved feature learning. Additionally, we introduce a dataset comprising 16,937 trajectories across 12 event classes, making it the largest in terms of event class diversity. Our results demonstrate the superior performance of our method compared to several deep architectures. The code for the proposed method and the trajectories can be found at https://github.com/NoeCanovi/Fish_Behaviors_Generative_Models.}
}
@article{ELRAWY2024102652,
title = {Assessing and segmenting salt-affected soils using in-situ EC measurements, remote sensing, and a modified deep learning MU-NET convolutional neural network},
journal = {Ecological Informatics},
volume = {81},
pages = {102652},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102652},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001948},
author = {Mustafa El-Rawy and Sally Y. Sayed and Mohamed A.E. AbdelRahman and Atef Makhloof and Nassir Al-Arifi and Mahmoud Khaled Abd-Ellah},
keywords = {Soil salinity, Artificial neural networks, Deep learning, Remote sensing, Salinity indices},
abstract = {A study revealed that the Siwa Oasis faces high soil salinity, which negatively impacts agricultural areas and crop productivity, despite its significant economic and agricultural importance. The current research proposes an approach to detect and segment salinity and vegetation areas at Siwa Oasis, Egypt, by combining remote sensing and building a deep learning neural network model-based U-NET algorithm to detect salinity change areas, anticipate further degradation, and predict soil quality indicators. To locate changes among the available images, standard image improvement, classification, and change detection methods have been used. We applied a deep learning modified U-Net (MU-NET) algorithm to segment and produce salinity maps. The MU-NET architecture is a two-level nested U-structure merged with a residual U-block (RUb), which consists of an encoder and a decoder. We applied RUb, which consists of several layers and skip connections. Different combinations of the salinity and vegetation indices were added to the original image to improve segmentation precision. The model was validated and trained using actual data samples collected over a 10-year period from the Landsat 8 satellite, which can monitor and analyse present land cover changes. The dataset consisted of 91 OLI and TIRS spectral images. Each one consists of eleven bands with a spatial resolution of 30 m for bands 1 to 7 and 9. A field survey was used as the main source of data for comparing the proposed model's outputs to assess the error rate. The study region is experiencing an increase in soil salinity in all directions, particularly with regard to the spatial distribution of saline soils, not just the quantitative increase in salt-affected soils. These findings supported the acceleration of soil salinization and vegetation death. The proposed model achieved the highest performance results among the other models and literature and was based on applying method 12 using 13 image layers, with the highest accuracies of 91.27% and 90.83% for salinity and vegetation, respectively.}
}
@article{KALFAS2023102037,
title = {Towards automatic insect monitoring on witloof chicory fields using sticky plate image analysis},
journal = {Ecological Informatics},
volume = {75},
pages = {102037},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102037},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000663},
author = {Ioannis Kalfas and Bart {De Ketelaere} and Klaartje Bunkens and Wouter Saeys},
keywords = {Insect recognition, Convolutional neural networks, Pest management, Automatic monitoring},
abstract = {Context
Sticky trap catches of agricultural pests can be employed for early hotspot detection, identification, and estimation of pest presence in greenhouses or in the field. However, manual procedures to produce and analyze catch results require substantial time and effort. As a result, much research has gone into creating efficient techniques for remotely monitoring possible infestations. A considerable number of these studies use Artificial Intelligence (AI) to analyze the acquired data and focus on performance metrics for various model architectures. Less emphasis, however, was devoted to the testing of the trained models to investigate how well they would perform under practical, in-field conditions.
Objective
In this study, we showcase an automatic and reliable computational method for monitoring insects in witloof chicory fields, while shifting the focus to the challenges of compiling and using a realistic insect image dataset that contains insects with common taxonomy levels.
Methods
To achieve this, we collected, imaged, and annotated 731 sticky plates - containing 74,616 bounding boxes - to train a YOLOv5 object detection model, concentrating on two pest insects (chicory leaf-miners and wooly aphids) and their two predatory counterparts (ichneumon wasps and grass flies). To better understand the object detection model's actual field performance, it was validated in a practical manner by splitting our image data on the sticky plate level.
Results and conclusions
According to experimental findings, the average mAP score for all dataset classes was 0.76. For both pest species and their corresponding predators, high mAP values of 0.73 and 0.86 were obtained. Additionally, the model accurately forecasted the presence of pests when presented with unseen sticky plate images from the test set.
Significance
The findings of this research clarify the feasibility of AI-powered pest monitoring in the field for real-world applications and provide opportunities for implementing pest monitoring in witloof chicory fields with minimal human intervention.}
}
@article{ZHOU2024102680,
title = {Real-time underwater object detection technology for complex underwater environments based on deep learning},
journal = {Ecological Informatics},
volume = {82},
pages = {102680},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102680},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400222X},
author = {Hui Zhou and Meiwei Kong and Hexiang Yuan and Yanyan Pan and Xinru Wang and Rong Chen and Weiheng Lu and Ruizhi Wang and Qunhui Yang},
keywords = {Underwater object detection, You only look once, Cross stage multi-branch, Large kernel spatial pyramid},
abstract = {Underwater object detection technology is crucial in many marine-related fields, including marine environmental monitoring, marine resource development, and marine ecological protection. However, this technology faces great challenges due to the poor quality of underwater optical images and the varying sizes of underwater objects. Therefore, we proposed an underwater optical detection network (UODN) based on the you only look once version 8 (YOLOv8) framework, which addresses these issues through the cross stage multi-branch (CSMB) module and large kernel spatial pyramid (LKSP) module. The aim of the CSMB module is to extract more features from underwater optical images to address the issue of poor image quality, while the LKSP module is designed to enhance the ability of the network to detect underwater objects of various scales. Furthermore, CSMBDarknet built by CSMB and LKSP can be used as the backbone of other underwater object detection algorithms for underwater feature extraction. Extensive experimental results on the underwater robot professional contest 2020 dataset revealed that the average precision (AP) of UODN increased by 1.0%, the AP50 of UODN increased by 1.1%, and the AP75 of UODN increased by 2.1% compared with those of the original YOLOv8s. Furthermore, UODN outperforms 12 state-of-the-art models on multiple underwater optical datasets, paving the way for future real-time and high-precision underwater object detection.}
}
@article{DEMELOLIMA2024102543,
title = {A lightweight and enhanced model for detecting the Neotropical brown stink bug, Euschistus heros (Hemiptera: Pentatomidae) based on YOLOv8 for soybean fields},
journal = {Ecological Informatics},
volume = {80},
pages = {102543},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102543},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000852},
author = {Bruno Pinheiro {de Melo Lima} and Lurdineide {de Araújo Barbosa Borges} and Edson Hirose and Díbio Leandro Borges},
keywords = {Heteroptera, Insect pest detection, Improved YOLO model, Image-based detection and counting, Deep learning, Soybean field images},
abstract = {Insect pest detection and monitoring are vital in an agricultural crop to help prevent losses and be more precise and sustainable regarding the consequent actions to be taken. Deep learning (DL) approaches have attracted attention, showing triumphant performance in many image-based applications. In the adult stage, this research considers detecting a vital insect pest in soybean crops, the Neotropical brown stink bug (Euschistus heros), from field images acquired by drones and cellphones. We develop and test an improved YOLO-model convolutional neural network (CNN) with fewer parameters than other state-of-the-art models and demonstrate its superior generalization and average precision on public image datasets and the new field data provided here. Considering the proposal's precision and time of response, the possibility of deploying this technology for automatic monitoring and pest management in the near future is promising. We provide open code and data for all the experiments performed.}
}
@article{KWON2024102588,
title = {Estimation of aquatic ecosystem health using deep neural network with nonlinear data mapping},
journal = {Ecological Informatics},
volume = {81},
pages = {102588},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102588},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001304},
author = {Yong Sung Kwon and Hyeongsik Kang and JongCheol Pyo},
keywords = {Deep learning, Aquatic ecosystem health index, Autoencoder, Convolutional neural network},
abstract = {Estimation of aquatic ecosystem health indices can assist in reducing the burden of time-consuming, labor-intensive, and cost-effective fieldwork for the sustainable evaluation of freshwater ecosystem status. In this study, we developed a deep neural network to estimate the trophic diatom index (TDI), benthic macroinvertebrate index (BMI), and fish assessment index (FAI) using water quality and hydraulic and hydrological data. A convolutional neural network (CNN) model was built to estimate health indices. In addition, an autoencoder was adopted to produce manifold features that were used as inputs for the CNN model. Conventional machine learning models, including artificial neural networks, support vector machines, random forests, and extreme gradient boosting, have been developed to estimate the TDI, BMI, and FAI. The results showed that the CNN with an autoencoder exhibited the best performance, with validation accuracies of Nash Sutcliffe Efficiency (NSE) and root mean squared error (RMSE) values of 0.592 and 17.249 for TDI, 0.669 and 12.282 for BMI, and 0.638 and 13.897 for FAI, respectively. The autoencoder enhanced the nonlinear feature learning of the time series and static input data, which contributed to improving the CNN feature extraction for accurate estimation of aquatic ecosystem health indices compared to other data-driven approaches. Therefore, deep learning techniques can be used to investigate aquatic ecosystem health by successfully reflecting the quantitative and qualitative features of health indices.}
}
@article{HU2024102640,
title = {Capturing urban green view with mobile crowd sensing},
journal = {Ecological Informatics},
volume = {81},
pages = {102640},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102640},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001821},
author = {Yingqiang Hu and Yue Wu and Zhuzi Tantian and Guodong Sun},
keywords = {Urban green spaces, Green view index, Deep learning, Mobile crowd sensing},
abstract = {Urban green spaces are beneficial to ecosystems and the health of people. The Green View Index (GVI) is an essential metric for assessing urban green spaces from a human perspective. However, measuring GVI at an urban scale requires extensive collection and processing of sensing data, posing challenges in terms of high resource consumption, difficulty in implementation, and lack of user participation. Mobile crowd Sensing (MCS) is an emerging large-scale, low-cost solution for sensing data collection. To address the aforementioned issues, this study proposes an MCS system called GreenCam to measure the GVI with smartphone sensors. GreenCam guides users to capture photos of urban green spaces from human perspective. The system employs a Transformer-based model, which is trained on a customized dataset of 1200 carefully-labeled urban green images, to extract the greenery from the captured photos and calculate GVI. With widespread participation from urban users, the photos captured by users with GreenCam can cover various streets and areas of the city, enabling the measurement of GVI at an urban scale. Additionally, these photos reflect people's preferences towards specific urban landscapes, and analyzing the distribution and characteristics of popular landscapes contributes to the enhancement of urban ecosystems and landscapes.}
}
@article{LAKDARI2024102457,
title = {Mel-frequency cepstral coefficients outperform embeddings from pre-trained convolutional neural networks under noisy conditions for discrimination tasks of individual gibbons},
journal = {Ecological Informatics},
volume = {80},
pages = {102457},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102457},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004867},
author = {Mohamed Walid Lakdari and Abdul Hamid Ahmad and Sarab Sethi and Gabriel A. Bohn and Dena J. Clink},
keywords = {Vocal individuality, Sound feature extraction, Mel-frequency cepstral coefficients, Convolutional neural networks, Acoustic indices, },
abstract = {Passive acoustic monitoring – an approach that utilizes autonomous acoustic recording units – allows for non-invasive monitoring of individuals, assuming that it is possible to acoustically distinguish individuals. However, identifying effective analytical approaches for individual identification remains a challenge. Our study investigates how the use of different feature representations impacts our ability to distinguish between individual female Northern grey gibbons (Hylobates funereus). We broadcast pre-recorded calls from twelve gibbon females and re-recorded the calls at varying distances (directly under the tree to ∼400 m away) using autonomous recording units. We evaluated the effectiveness of using different automated feature extraction approaches to classify gibbon calls: Mel-frequency cepstral coefficients (MFCCs), embeddings from three pre-trained neural networks (BirdNET, VGGish, and Wav2Vec2), and four commonly used acoustic indices. We used a supervised classification approach (random forest) to classify calls to the respective female and compared two unsupervised clustering approaches (affinity propagation clustering and hierarchical density-based spatial clustering) to evaluate which features were most effective for distinguishing female calls without using class labels. We used MFCCs as a baseline as previous work has shown they can be used to distinguish high-quality calls of individual gibbon females. Human annotators could only identify calls in spectrograms from recordings <350 m from the playback speaker with signal-to-noise ratio ∼ 0 dB, so our results focus on these recordings. Using supervised classification, our results confirmed the efficiency of MFCCs and the use of embeddings from one neural network (BirdNET) for effective acoustic classification of gibbon individuals at closer recording distances (signal-to-noise ratio > 10 dB), while the remaining features did not perform well. Contrary to our expectations, we found that MFCCs outperformed all other features for the unsupervised clustering tasks at closer distances and none of the features performed well at farther distances. The ability to acoustically discriminate animals under noisy conditions and from low signal-to-noise ratio calls has important implications for monitoring populations of endangered animals, such as gibbons. Focusing only on high signal-to-noise ratio calls for individual discrimination may not be possible for rare sounds, and future work should focus on developing effective approaches of feature extraction that can perform well across noisy, real-world conditions with a limited number of training samples.}
}
@article{VALERIO2024102502,
title = {GEE_xtract: High-quality remote sensing data preparation and extraction for multiple spatio-temporal ecological scaling},
journal = {Ecological Informatics},
volume = {80},
pages = {102502},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102502},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400044X},
author = {Francesco Valerio and Sérgio Godinho and Ana T. Marques and Tiago Crispim-Mendes and Ricardo Pita and João Paulo Silva},
keywords = {Multiple spatiotemporal scales, Time series, Sentinel, Landsat, MODIS, Google earth engine},
abstract = {Environmental sensing via Earth Observation Satellites (EOS) is critically important for understanding Earth’ biosphere. The last decade witnessed a “Klondike Gold Rush” era for ecological research given a growing multidisciplinary interest in EOS. Presently, the combination of repositories of remotely sensed big data, with cloud infrastructures granting exceptional analytical power, may now mark the emergence of a new paradigm in understanding spatio-temporal dynamics of ecological systems, by allowing appropriate scaling of environmental data to ecological phenomena at an unprecedented level. However, while some efforts have been made to combine remotely sensed data with (near) ground ecological observations, virtually no study has focused on multiple spatial and temporal scales over long time series, and on integrating different EOS sensors. Furthermore, there is still a lack of applications offering flexible approaches to deal with the scaling limits of multiple sensors, while ensuring high-quality data extraction at high resolution. We present GEE_xtract, an original EOS-based (Sentinel-2, Landsat, and MODIS) code operational within Google Earth Engine (GEE) to allow for straightforward preparation and extraction of remote sensing data matching the multiple spatio-temporal scales at which ecological processes occur. The GEE_xtract code consists of three main customisable operations: (1) time series imageries filtering and calibration; (2) calculation of comparable metrics across EOS sensors; (3) scaling of spatio-temporal remote sensing time series data from ground-based data. We illustrate the value of GEE_xtract with a complex case concerning the seasonal distribution of a threatened elusive bird and highlight its broad application to a myriad of ecological phenomena. Being user-friendly designed and implemented in a widely used cloud platform (GEE), we believe our approach provides a major contribution to effectively extracting high-quality data that can be quickly computed for metrics time series, converted at any scale, and extracted from ground information. Additionally, the framework was prepared to facilitate comparative research initiatives and data-fusion approaches in ecological research.}
}
@article{YANG2024102527,
title = {A systematic study on transfer learning: Automatically identifying empty camera trap images using deep convolutional neural networks},
journal = {Ecological Informatics},
volume = {80},
pages = {102527},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102527},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000694},
author = {Deng-Qi Yang and De-Yao Meng and Hao-Xuan Li and Meng-Tao Li and Han-Lin Jiang and Kun Tan and Zhi-Pang Huang and Na Li and Rong-Hai Wu and Xiao-Wei Li and Ben-Hui Chen and Mei Zhang and Guo-Peng Ren and Wen Xiao},
keywords = {Transfer learning, Camera trap images, ResNext-101, Updating layer selection, Image recognition},
abstract = {Transfer learning is extensively utilized for automatically recognizing and filtering out empty camera trap images that lack animal presence. Current research that uses transfer learning for identifying empty images typically solely updates the fully connected layer of models, and they usually select a pre-trained source model only based on its relevance to the target task. However, they do not consider the optimization of update layer selection, nor do they investigate the effect of sample size and class number of source domain data set used to construct the source model on the performance of the transfer model. Both of these are issues worth exploring. We answered these two issues using three different datasets and the ResNext-101 model. Our experimental results showed that when using 20,000 training samples to transfer the model from the ImageNet dataset to the Snapshot Serengeti dataset, our proposed optimal update layers improved the accuracy of the transfer model from 92.9% to 95.5% (z = −7.087, p < 0.001, N = 8118) compared to the existing method of updating only the fully connected layer. A similar improvement was observed when transferring the model from ImageNet to the Lasha Mountain dataset. Additionally, our results indicated that when using 20,000 training samples to update the pre-trained model and increasing the sample size of the binary-class training dataset used to build the source model from 100,000 to 1 million, the accuracy of the transfer model improved from 90.4% to 93.5% (z = −3.869, p < 0.001, N = 8948). Similar results were obtained when constructing the source domain dataset using ten classifications. Based on these results, we drew the following conclusions: (1) using our proposed optimal update layers instead of the commonly used method of updating only the fully connected layers can significantly improve the model's performance. (2) The optimal update layers varied when the model transferred from different source domain datasets to the same target dataset. (3) The number of classes in the source domain dataset did not significantly impact the transfer model performance. However, the sample size of the source domain dataset positively correlated with the transfer model performance, and there might be a threshold effect.}
}
@article{SZABO2024102624,
title = {Aquatic vegetation mapping with UAS-cameras considering phenotypes},
journal = {Ecological Informatics},
volume = {81},
pages = {102624},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102624},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001663},
author = {Loránd Szabó and László Bertalan and Gergely Szabó and István Grigorszky and Imre Somlyai and György Dévai and Sándor Alex Nagy and Imre J. Holb and Szilárd Szabó},
keywords = {Image classification, Spectral index, Texture index, DSM, Data fusion, UAS, Recursive feature elimination, Aquatic vegetation},
abstract = {Aquatic vegetation species at the genus level in an oxbow lake were identified in Hungary based on a multispectral Uncrewed Aerial System (UAS) survey within an elongated oxbow lake area of the Tisza River under continental climate. Seven and 13 classes were discriminated using three different classification methods (Support Vector Machine [SVM], Random Forest [RF], and Multivariate Adaptive Regression Splines [MARS]) using different input data in ten combinations: original spectral bands, spectral indices, Digital Surface Model (DSM), and Haralick texture indices. We achieved a high (97.1%) overall accuracies (OAs) by applying the SVM classifier, but the RF performed only <1% worse, as it was represented in the first places of the classification rank before the MARS. The highest classification accuracies (>84% OA) were obtained using the most important variables derived by the Recursive Feature Elimination (RFE) method. The best classification required DSM as an input variable. The poorest classification performance belonged to the model that used only texture indices or spectral indices. On the class level, Stratiotes aloides exhibit the lowest degree of separability compared to the other classes. Accordingly, we recommend using supplementary input data for the classifications besides the original spectral bands, for example, DSM, spectral, and texture indices, as these variables significantly improve the classification accuracies in the proper combinations of the input variables.}
}
@article{YANG2024102705,
title = {Adaptive image processing embedding to make the ecological tasks of deep learning more robust on camera traps images},
journal = {Ecological Informatics},
volume = {82},
pages = {102705},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102705},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002474},
author = {Zihe Yang and Ye Tian and Junguo Zhang},
keywords = {Adaptive image processing, Camera traps, Deep learning, Ecological tasks},
abstract = {Camera traps serve as a valuable tool for wildlife monitoring, generating a vast collection of images for ecologists to conduct ecological investigations, such as species identification and population estimation. However, the sheer volume of images poses a challenge, and the integration of deep learning into automated ecological investigation tasks remains complex, particularly when dealing with low-quality images in long-term monitoring programs. Existing approaches often struggle to strike a balance between image enhancement and deep learning for ecological tasks, thereby overlooking crucial information contained within low-quality images. This research introduces a pioneering adaptive image processing module (AIP) that seamlessly incorporates image processing into camera trap ecological tasks, elevating the performance of wildlife monitoring activities. Specifically, a differentiable image processing (DIP) module is presented to enhance low-quality images, with its parameters predicted by a Non-local based parameter predictor (NLPP). Additionally, an end-to-end approach based on hybrid data containing both original and synthetic data is proposed, encompassing adaptive image processing methods and downstream tasks for camera traps, adaptable to various scenarios. This approach effectively reduces the manual labor and time required for professional image processing. When applied to real-world camera trap images and synthetic image datasets, our method achieves an accuracy of 92.26% and 86.65% in classifying wildlife, respectively, demonstrating its robustness. By outperforming alternative methods under harsh conditions, the application of the adaptive image processing module instills greater confidence in deep learning applications within complex environments.}
}
@article{ARAUJO2024102388,
title = {Membership inference attack for beluga whales discrimination},
journal = {Ecological Informatics},
volume = {79},
pages = {102388},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102388},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300417X},
author = {Voncarlos M. Araújo and Sébastien Gambs and Robert Michaud and Hadrien Lautraite and Léo Schneider and Clément Chion},
keywords = {Membership inference attack, Animal ecology, -identification, Discrimination, Open-set problem},
abstract = {To efficiently monitor the growth and evolution of a particular wildlife population, one of the fundamental challenges to address in animal ecology is the re-identification of individuals that have been previously encountered but also the discrimination between known and unknown individuals (the so-called “open-set problem”), which is the first step to realize before re-identification. In particular, in this work, we are interested in the discrimination within digital photos of beluga whales, which are known to be among the most challenging marine species to discriminate due to their lack of distinctive features. To tackle this problem, we propose a novel approach based on the use of Membership Inference Attacks (MIAs), which are normally used to assess the privacy risks associated with releasing a particular machine learning model. More precisely, we demonstrate that the problem of discriminating between known and unknown individuals can be solved efficiently using state-of-the-art approaches for MIAs. Extensive experiments on three benchmark datasets related to whales, two different neural network architectures, and three MIA clearly demonstrate the performance of the approach. In addition, we have also designed a novel MIA strategy that we coined as ensemble MIA, which combines the outputs of different MIAs to increase the discrimination accuracy while diminishing the false positive rate. Overall, one of the main objectives of our work is to demonstrate that the study of privacy attacks can also be harnessed positively, assisting in the resolution of practical issues encountered in the field of animal ecology.}
}
@article{WEI2024102445,
title = {YOLO_MRC: A fast and lightweight model for real-time detection and individual counting of Tephritidae pests},
journal = {Ecological Informatics},
volume = {79},
pages = {102445},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102445},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004740},
author = {Min Wei and Wei Zhan},
keywords = {Real-time detection, Individual counting, YOLOv8, Lightweight, Attention mechanism, },
abstract = {Tephritidae pests severely affect the quality and safety of various melons, fruits and vegetable crops. However, many agricultural managers lack an adequate understanding of the level of pest occurrence, resulting in the misuse of pesticides, which ultimately leads to environmental pollution and economic loss. Therefore, real-time detection and counting of Tephritidae pests are important for timely pest spotting and control. This work helps quickly determine the distribution and abundance of pests in the current environment, thus providing data on pest conditions for agricultural management to optimize pesticide use. Nevertheless, the fast speed, high accuracy, and lightweight performance of real-time detection and counting are difficult to balance. To address this problem, based on the YOLOv8n model, this paper takes Bactrocera cucurbitae pests as the detection target and proposes a fast and lightweight real-time detection and individual counting model for Tephritidae pests, named YOLO_MRC. This paper introduces three key improvements: (1) Constructing a new module called Multicat into the neck network increases the focus on the detection target by incorporating an attention mechanism; (2) Reducing the original three detection heads to two and then adjusting their sizes to decrease the number of parameters in the network model; (3) Devising a novel module, C2flite, to enhance the deep feature extraction capability of the backbone network. According to the above points, this paper conducts ablation experiments to compare the performances of different models. Experiments showed that the Multicat module significantly offsets the large increase in GFLOPs and processing time caused by reducing the detection head and can further reduce the number of parameters and improve the accuracy when combined with the C2flite module. On our Bactrocera cucurbitae pest dataset, the mAP@0.5 of the YOLO_MRC model reached 99.3%. Simultaneously, as the number of parameters decreases by 63.68%, GFLOPs is reduced by 19.75%, and the processing time is shortened by 5%. To ensure the validity of the model, YOLO_MRC is compared with four excellent detection models by using manual counting results as the benchmark. YOLO_MRC achieves an average pest counting accuracy of 94%, demonstrating superior performance in terms of model size and processing time. To further explore the performance of YOLO_MRC in multiclass insect detection tasks, we choose the public dataset Pest_24_640 for comparison with four state-of-the-art models. YOLO_MRC achieves a 3.6 ms processing time and 70.4% accuracy with only a 2.4 MB model size, which demonstrates the potential of YOLO_MRC in multiclass pest detection.}
}
@article{HERDY2024102417,
title = {Utilization of deep learning tools to map and monitor biological soil crusts},
journal = {Ecological Informatics},
volume = {79},
pages = {102417},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102417},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004466},
author = {Stefan Herdy and Emilio Rodríguez-Caballero and Thomas Pock and Bettina Weber},
keywords = {Semantic segmentation, Joint energy-based modelling, Deep learning, Biological soil crust, Domain adaption, Neural network, Long-term monitoring},
abstract = {Biological soil crusts (biocrusts) form a layer of only one to few centimeters depth on the soil surface and occur mostly in hot and cold deserts. Biocrusts have a major impact on different processes in these ecosystems, like carbon and nitrogen cycling, biodiversity preservation, erosion protection and soil dust emission reduction, but also react highly sensitive upon climate alterations and land use intensification. Therefore, monitoring tools are required to keep track of the changes of these specialized communities in an altering environment. In the current study, we applied a semantic image segmentation approach, using neural networks. One main problem to be solved was, that the training data and target data, on which the model is applied, are often recorded with different camera devices. This leads to different statistical properties of the image data, like different scale, resolution, brightness etc., which could significantly affect the model's performance. To solve this problem, we propose a new domain adaption method using a joint energy-based approach. To test a semantic segmentation approach in general, we utilized biocrust imagery taken in Utah (United States of America) and two sub datasets from the National Park Gesäuse (Austria). Here, we achieved highly reliable results with an overall classification accuracy of 85.9% for the USA data and 88.6% and 91.4%, respectively, for the two sub datasets of the National Park Gesäuse. To test our joint energy-based domain adaption approach, we used the two sub datasets from the National Park Gesäuse, which were recorded with different camera devices. With this newly established approach, we improved the accuracy of our segmentation on the unlabeled sub dataset from 70.4% to 75.3%. The results suggest that joint energy-based modelling is a well-suited domain adaption method for semantic segmentation that could be applied to face various deep learning and image-based biomonitoring challenges.}
}
@article{LEORNA2022101876,
title = {Human vs. machine: Detecting wildlife in camera trap images},
journal = {Ecological Informatics},
volume = {72},
pages = {101876},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101876},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003260},
author = {Scott Leorna and Todd Brinkman},
keywords = {Confusion matrix, Deep learning, Image analysis, Software, Trail camera},
abstract = {As the capacity to collect and store large amounts of data expands, identifying and evaluating strategies to efficiently convert raw data into meaningful information is increasingly necessary. Across disciplines, this data processing task has become a significant challenge, delaying progress and actionable insights. In ecology, the growing use of camera traps (i.e., remotely triggered cameras) to collect information on wildlife has led to an enormous volume of raw data (i.e., images) in need of review and annotation. To expedite camera trap image processing, many have turned to the field of artificial intelligence (AI) and use machine learning models to automate tasks such as detecting and classifying wildlife in images. To contribute understanding of the utility of AI tools for processing wildlife camera trap images, we evaluated the performance of a state-of-the-art computer vision model developed by Microsoft AI for Earth named MegaDetector using data from an ongoing camera trap study in Arctic Alaska, USA. Compared to image labels determined by manual human review, we found MegaDetector reliably determined the presence or absence of wildlife in images generated by motion detection camera settings (≥94.6% accuracy), however, performance was substantially poorer for images collected with time-lapse camera settings (≤61.6% accuracy). By examining time-lapse images where MegaDetector failed to detect wildlife, we gained practical insights into animal size and distance detection limits and discuss how those may impact the performance of MegaDetector in other systems. We anticipate our findings will stimulate critical thinking about the tradeoffs of using automated AI tools or manual human review to process camera trap images and help to inform effective implementation of study designs.}
}
@article{LI2023102215,
title = {Tree trunk detection in urban scenes using a multiscale attention-based deep learning method},
journal = {Ecological Informatics},
volume = {77},
pages = {102215},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102215},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002443},
author = {Rao Li and GuoDong Sun and Sheng Wang and TianZhuzi Tan and Fu Xu},
keywords = {Salient object detection, Tree trunk detection, Deep learning, Attention mechanism, Dataset construction},
abstract = {Precise identification of tree trunks contributes to the understanding of urban green dynamics. Previous attempts to develop tree trunk detection methods have faced limitations in respect of precision and generalization due to the use of hand-engineered features and the constraint of single-species detection. In this study, we construct a new tree trunk dataset considering the object’s strong diversity and propose a deep model to detect and segment the salient tree trunks or even branches in urban scenes. Comprehensive experiments are performed to evaluate our model. The presented method exhibits exceptional performance, evidenced by its outstanding scores across seven evaluation metrics, indicating its capability to segment tree trunks of different species, even if they exhibit significant variations in appearance. Specifically, our model demonstrates outstanding accuracy in detecting trunks with intricate furcations, as well as effectively identifying trunks that are partially occluded.}
}
@article{CEIAHASSE2023102272,
title = {Forecasting the abundance of disease vectors with deep learning},
journal = {Ecological Informatics},
volume = {78},
pages = {102272},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102272},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003011},
author = {Ana Ceia-Hasse and Carla A. Sousa and Bruna R. Gouveia and César Capinha},
keywords = {Machine learning, Mosquito, Dengue, Forecast, Time series classification},
abstract = {Arboviral diseases such as dengue, Zika, chikungunya or yellow fever are a worldwide concern. The abundance of vector species plays a key role in the emergence of outbreaks of these diseases, so forecasting these numbers is fundamental in preventive risk assessment. Here we describe and demonstrate a novel approach that uses state-of-the-art deep learning algorithms to forecast disease vector abundances. Unlike classical statistical and machine learning methods, deep learning models use time series data directly as predictors and identify the features that are most relevant from a predictive perspective. We demonstrate for the first time the application of this approach to predict short-term temporal trends in the number of Aedes aegypti mosquito eggs across Madeira Island for the period 2013 to 2019. Specifically, we apply the deep learning models to predict whether, in the following week, the number of Ae. aegypti eggs will remain unchanged, or whether it will increase or decrease, considering different percentages of change. We obtained high predictive performance for all years considered (mean AUC = 0.92 ± 0.05 SD). Our approach performed better than classical machine learning methods. We also found that the preceding numbers of eggs is a highly informative predictor of future trends. Linking our approach to disease transmission or importation models will contribute to operational, early warning systems of arboviral disease risk.}
}
@article{PIECHAUD2022101786,
title = {Fast and accurate mapping of fine scale abundance of a VME in the deep sea with computer vision},
journal = {Ecological Informatics},
volume = {71},
pages = {101786},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101786},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002369},
author = {Nils Piechaud and Kerry L. Howell},
keywords = {Benthic ecology, Computer vision, Xenophyophores, Quantitative ecology, Mapping, Automated image analysis, Marine conservation},
abstract = {With growing anthropogenic pressure on deep-sea ecosystems, large quantities of data are needed to understand their ecology, monitor changes over time and inform conservation managers. Current methods of image analysis are too slow to meet these requirements. Recently, computer vision has become more accessible to biologists, and could help address this challenge. In this study we demonstrate a method by which non-specialists can train a YOLOV4 Convolutional Neural Network (CNN) able to count and measure a single class of objects. We apply CV to the extraction of quantitative data on the density and population size structure of the xenophyophore Syringammina fragilissima, from more than 58,000 images taken by an AUV 1200 m deep in the North-East Atlantic. The workflow developed used open-source tools, cloud-base hardware, and only required a level of experience with CV commonly found among ecologists. The CNN performed well, achieving a recall of 0.84 and precision of 0.91. Individual counts per image and size measurements resulting from model predictions were highly correlated (0.96 and 0.92, respectively) with manually collected data. The analysis could be completed in less than 10 days thus bringing novel insights into the population size structure and fine scale distribution of this Vulnerable Marine Ecosystem. It showed S. fragilissima distribution is patchy. The average density is 2.5 ind.m−2 but can vary from up to 45 ind.m−2 only a few tens of meter away from areas where it is almost absent. The average size is 5.5 cm and the largest individuals (>15 cm) tend to be in areas of low density. This study demonstrates how researchers could take advantage of CV to quickly and efficiently generate large quantitative datasets data on benthic ecosystems extent and distribution. This, coupled with the large sampling capacity of AUVs could bypass the bottleneck of image analysis and greatly facilitate future deep-ocean exploration and monitoring. It also illustrates the future potential of these new technologies to meet the goals set by the UN Ocean Decade.}
}
@article{JEANTET2023102256,
title = {Improving deep learning acoustic classifiers with contextual information for wildlife monitoring},
journal = {Ecological Informatics},
volume = {77},
pages = {102256},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102256},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002856},
author = {Lorène Jeantet and Emmanuel Dufourq},
keywords = {Bioacoustics, Deep learning, Convolutional neural networks, Passive acoustic monitoring, Species identification, Birds, Hainan gibbons},
abstract = {Bioacoustics, the exploration of animal vocalizations and natural soundscapes, has emerged as a valuable tool for studying species within their habitats, particularly those that are challenging to observe. This approach has broadened the horizons of biodiversity assessment and ecological research. However, monitoring wildlife with acoustic recorders produces large volumes of data that can be labor-intensive to analyze. Deep learning has recently transformed many computational disciplines by enabling the automated processing of large and complex datasets and has gained attention within the bioacoustics community. Despite the revolutionary impact of deep learning on acoustic detection and classification, attaining both high detection accuracy and low false positive rates in bioacoustics remains a significant challenge. An intriguing yet unexplored avenue for enhancing deep learning in bioacoustics involves the utilization of contextual information, such as time and location, to discern animal vocalizations within acoustic recordings. As a first case study, a multi-branch Convolutional Neural Network (CNN) was developed to classify 22 different bird songs using spectrograms as a first input, and spatial metadata as a secondary input. A comparison was made to a baseline model with only spectrogram input. A geographical prior neural network was trained, separately, to estimate the probability of a species occurring at a given location. The output of this network was combined with the baseline CNN. As a second case study, temporal data and spectrograms were used as input to a multi-branch CNN for the detection of Hainan gibbon (Nomascus hainanus) calls, the world’s rarest primate. Our findings demonstrate that adding metadata to the bird song classifier significantly improves classification performance, with the highest improvement achieved using the geographical prior model (F1-score of 87.78% compared to 61.02% for the baseline model). The multi-branch CNNs also proved efficient (F1-scores of 76.87% and 78.77%) and simpler to use than the geographical prior. In the second case study, our findings revealed a decrease in false positives by 63% (94% of the calls were detected) when the metadata was used by the multi-branch CNN, and an increase of 19% in gibbon detection. This study has uncovered an exciting new avenue for improving classifier performance in bioacoustics. The methodology described in this study can assist ecologists, wildlife management teams, and researchers in reducing the amount of time spent analyzing large acoustic datasets obtained from passive acoustic monitoring studies. Our approach can be adapted and applied to other calling species, and thus tailored to other use cases.}
}
@article{MA2024102651,
title = {UAV equipped with infrared imaging for Cervidae monitoring: Improving detection accuracy by eliminating background information interference},
journal = {Ecological Informatics},
volume = {81},
pages = {102651},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102651},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001936},
author = {Guangkai Ma and Wenjiao Li and Heng Bao and Nathan James Roberts and Yang Li and Weihua Zhang and Kun Yang and Guangshun Jiang},
keywords = {YOLOv7, ViT, Wild Cervidae monitoring},
abstract = {Wild Cervidae(deer and their relatives) play a crucial role in maintaining ecological balance and are integral components of ecosystems. However, factors such as environmental changes and poaching behaviors have resulted in habitat degradation for Cervidae. The protection of wild Cervidae has become urgent, and Cervidae monitoring is one of the key means to ensure the effectiveness of wild Cervidae protection. Object detection algorithms based on deep learning offer promising potential for automatically detecting and identifying animals. However, when those algorithms are used for inference in unseen background environments, there will be a significant decrease in accuracy, especially in the situation that a certain type of Cervidae images are collected from single scene for algorithm training. In this paper, a two-stage localization and classification pipeline for Cervidae monitoring is proposed. The pipeline effectively reduces background interference in Cervidae monitoring and enhances monitoring accuracy. In the first stage, the YOLOv7 network is designed to automatically locate Cervidae in UAV infrared images, while implementing improved bounding box regression through the α-IoU loss function enables the network to locate Cervidae more accurately. Then, Cevdidae objects are extracted to eliminate the background information. In the second stage, a classification network named CA-Hybrid, based on Convolutional Neural Networks(CNN) and Vision Transformer(ViT), as well as Channel Attention Mechanism(CAM) enhances the expression of key features, is constructed to accurately identify Cervidae categories. Experimental results indicate that this method achieves an Average Precision (AP) of 95.9% for Cervidae location and a top-1 accuracy of 77.73% for Cervidae identification. This research contributes to a more comprehensive and accurate monitoring of wild Cervidae, and provides valuable references for subsequent UAV-based wildlife monitoring.}
}
@article{MICHAELSEN2022101721,
title = {Uncertainty and ignored information in the analysis of bat ultrasound: Bayesian approximation to the rescue},
journal = {Ecological Informatics},
volume = {70},
pages = {101721},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101721},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122001716},
author = {Tore Christian Michaelsen and Jens Rydell and Rasmus Bååth and Knut Helge Jensen},
keywords = {Bayesian statistics, Bat ultrasound analysis, Probability theory, GNU R},
abstract = {Bat ultrasound analysis has been around for several decades and it is one of the most important tools in studies of bat ecology. Discrimination between species is based on intra-specific features of echolocation calls. Identification of species and genera in audio files can be attempted either manually or through software which performs a fully automated discrimination between species. However, significant overlap in various features (e.g. frequencies of calls) exists between species and even genera. Species ID is therefore often not an absolute conclusion, but rather an opinion or best guess, as opposed to DNA tests or measurements on external characters of captured bats. To make things even worse, the probability of actually observing a bat of a given species in space and time is ignored when performing bat ultrasound analysis. This study introduces Bayesian approximation through a new method we have named Alternative Bayesian Bat Analysis (ABBA). We show, through a simple proof-of-concept example, the importance of adding information about the local composition of the bat community, hence making informed decisions regarding which species is most likely present in audio files. The superior performance of ABBA is also shown through an example using R code. Here, we use simulated data for three Pipistrellus spp., a genus with significant overlap in frequencies, but the code can easily be adapted to other bat species and genera worldwide. ABBA outperformed the non-Bayesian approach for all three species. The rare species in the simulated data set was super-inflated when using the non-Bayesian method. Further the results show, contrarily to common belief, that the frequency dominated by a given species in a data set, depends on the composition of the bat fauna and not just means and SDs reported in the literature. ABBA allows researchers to include all observations in statistical modeling, rather than excluding observations, an approach which can affect the reliability of studies. This study also, to a great extent, explains the poor performance of software attempting automated bat ID. Implementing Bayesian algorithms, and thereby allowing users to interact with the software, should significantly improve their performance.}
}
@article{PUSHPA2024102611,
title = {On the importance of integrating convolution features for Indian medicinal plant species classification using hierarchical machine learning approach},
journal = {Ecological Informatics},
volume = {81},
pages = {102611},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102611},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001535},
author = {B.R. Pushpa and N. Shobha Rani and M. Chandrajith and N. Manohar and Smitha Sunil Kumaran Nair},
keywords = {Medicinal plant classification, Fusion features, Convolution features, Smartphone images, Inter-class similarities},
abstract = {This work proposes a novel hierarchical classification framework designed to categorize hundred Indian medicinal plant species. The innovation lies in introducing a comprehensive feature representation by integrating convolutional features with geometric, texture, shape, and multispectral features for classification tasks. In this study, a two-level hierarchical plant classification model is proposed to address the challenges of inter-class similarity and intra-class variations. The first level classifies 100 medicinal plant species into 11 groups based on visual similarities among the plants. At level two, the specific plant species containing in each group are predicted using Random Forest classifier. The evaluation is performed at two levels to analyze the effectiveness of the proposed model. The performance analysis compares the effectiveness of individual feature types against the composite feature model. Performance is also evaluated based on specific groups that demonstrate high similarity between classes and intra-class variations among the plant species separately. Furthermore, the generality of the model is tested using two self-created datasets-RTL80 and RTP40, requiring more than 300 man-hours to collect. Experimental results demonstrate a promising accuracy of 94.54% on GSL100 leaf dataset and 75.46% on RTL80 and RTP40 real-time datasets reflecting the superiority of the proposed hierarchical model over state-of-the-art methods.}
}
@article{GALAZGARCIA2024102559,
title = {Mapping invasive iceplant extent in southern coastal California using high-resolution aerial imagery},
journal = {Ecological Informatics},
volume = {81},
pages = {102559},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102559},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001018},
author = {Carmen {Galaz García} and Julien Brun and Benjamin S. Halpern},
keywords = {Invasive species, Iceplant, , Remote sensing, NAIP, Machine learning, Random forest, Microsoft Planetary Computer, Texture},
abstract = {Invasive species threaten natural ecosystems globally, displacing native species and causing biodiversity loss. In coastal areas with Mediterranean climate around the world, iceplant (Carpobrotus edulis) has become highly invasive, forming large monospecific zones that compete for resources with native plant species, including threatened or endangered species. Despite the widespread impact of iceplant across coastal areas with a Mediterranean climate, there is no precise information on where it is and how much it has spread. This study focuses on mapping and quantifying iceplant extent along the coast of Santa Barbara County in California, USA, by leveraging machine learning methods to identify iceplant in images from the 2020 National Agriculture Imagery Program (NAIP) archive at 0.6 m/pixel resolution, creating the most extensive assessment to date of this invasive species. Results include a map showing iceplant locations in 2020 with overall accuracy of 87.11% ± 2.45% (95% confidence interval). The estimated iceplant coverage in our region of study is 2.2 ± 0.42 km2 (95% confidence interval). Additionally, this study's use of open data and reproducible data analysis and validation workflow opens the door for the methods presented to be adapted and applied across California and all other Mediterranean climatic regions. In addition, the developed approach will accelerate monitoring over time to comprehend the spread and mitigation of iceplant invasions.}
}
@article{MATA2024102708,
title = {Drone imagery and deep learning for mapping the density of wild Pacific oysters to manage their expansion into protected areas},
journal = {Ecological Informatics},
volume = {82},
pages = {102708},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102708},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002504},
author = {Aser Mata and David Moffat and Sílvia Almeida and Marko Radeta and William Jay and Nigel Mortimer and Katie Awty-Carroll and Oliver R. Thomas and Vanda Brotas and Steve Groom},
keywords = {Pacific oysters, Invasive species, Convolutional neural networks, Deep learning, Drone, Remote sensing, Ecological management},
abstract = {The recent expansion of wild Pacific oysters already had negative repercussions on sites in Europe and has raised further concerns over their potential harmful impact on the balance of biomes within protected areas. Monitoring their colonisation, especially at early stages, has become an urgent ecological issue. Current efforts to monitor wild Pacific oysters rely on “walk-over” surveys that are highly laborious and often limited to specific areas of easy access. Remotely Piloted Aircraft Systems (RPAS), commonly known as drones, can provide an effective tool for surveying complex terrains and detect Pacific oysters. This study provides a novel workflow for automated detection, counting and mapping of individual Pacific oysters to estimate their density per square meter by using Convolutional Neural Networks (CNNs) applied to drone imagery. Drone photos were collected at low tides and altitudes of approximately 10 m across a variety of cases of rocky shore and mudflats scenarios. Using object detection, we compared how different Convolutional Neural Networks (CNNs) architectures including YOLOv5s, YOLOv5m, TPH-YOLOv5 and FR-CNN performed in the detection of Pacific oysters over the surveyed areas. We report the precision of our model at 88% with a difference in performance of 1% across the two sites. The workflow presented in this work proposes the use of grid maps to visualize the density of Pacific oysters per square meter towards ecological management and the creation of time series to identify trends.}
}
@article{JANSIRANI2024102663,
title = {A novel automated approach for fish biomass estimation in turbid environments through deep learning, object detection, and regression},
journal = {Ecological Informatics},
volume = {81},
pages = {102663},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102663},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400205X},
author = {S.V. {Jansi Rani} and Iacovos Ioannou and R. Swetha and R.M. {Dhivya Lakshmi} and Vasos Vassiliou},
keywords = {Fish biomass, Object detection, Regression, YOLOv8, mYOLOv8},
abstract = {Estimating fish biomass is crucial in the fisheries sector, where traditional methods often harm fish through manual sampling and anesthetics. A non-invasive approach is introduced using underwater films to estimate fish biomass in turbid conditions. This study presents the “Aquatic WeightNet” dataset, targeting the Genetically Improved Formed Tilapia (GIFT) Tilapia species, and addresses the challenge of unclear images with preprocessing techniques like dehazing and Contrast Limited Adaptive Histogram Equalization (CLAHE). YOLOv8, a leading object detection model modified to accommodate the custom Aquatic WeightNet dataset's varied image sizes with five detection heads, P2 to P6, is employed, achieving a recall of 0.997 and a mean Average Precision (mAP) of 0.899 within the 50–95% Intersection over Union (IoU) range. Fish biomass estimation assesses depth, length, and width using regression models for calculation. A three-phase grid search identifies the most effective models, with the Extra Trees Regressor outperforming depth estimation with mean absolute error (MAE) of 0.63 and coefficient of determination (R2) of 0.87 and the Random Forest Regressor for length and width (MAE of 0.01 and R2 of 0.99). For biomass estimation, the Extra Trees Regressor again performs well (MAE of 0.004 and R2 of 0.99), which is critical for determining optimal feed quantities to enhance aquaculture efficiency. This study emphasizes a non-invasive method to estimate fish biomass, optimizing the effectiveness and ecological sustainability of fish farming in murky waters through advanced detection algorithms and robust regression models.}
}
@article{NGUYEN2024102744,
title = {Improving pollen-bearing honey bee detection from videos captured at hive entrance by combining deep learning and handling imbalance techniques},
journal = {Ecological Informatics},
pages = {102744},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102744},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002863},
author = {Dinh-Tu Nguyen and Thi-Nhung Le and Thi-Huong Phung and Duc-Manh Nguyen and Hong-Quan Nguyen and Hong-Thai Pham and Thi-Thu-Hong Phan and  Vu-Hai and Thi-Lan Le},
keywords = {Pollen foraging behavior, Pollen-bearing honey bee detection},
abstract = {The number of pollen-bearing honey bees serves as a vital indicator for assessing colony balance and health. Despite its significance, prevailing detection techniques still rely heavily on manual observation and annotation, leading to time-consuming processes that cannot sustain long-term, continuous monitoring efforts. To facilitate automatic beehive monitoring, this study introduces an efficient method for pollen-bearing bee detection. Initially, we furnish a comprehensive dataset, dubbed VnPollenBee, meticulously annotated for pollen-bearing honey bee detection and classification. The dataset comprises 60,826 annotated boxes that delineate both pollen-bearing and non-pollen-bearing bees in 2051 images captured at the entrances of beehives under various environmental conditions. To the best of our knowledge, this represents the first dedicated dataset for pollen-bearing bee detection. The VnPollenBee dataset is publicly accessible to the research community at https://comvis-hust.github.io/datasets/pollenbee.html. Subsequently, we propose the incorporation of diverse techniques into two baseline models, namely YOLOv5 and Faster RCNN, to effectively address the imbalance that arises during the detection of pollen-bearing bees due to their number being typically much lower than the total number of bees present at hive entrances. The experimental results demonstrate that our proposed method outperforms the baseline models on the VnPollenBee dataset, yielding Precision, Recall, and F1 score of 99%, 93%, and 95%, respectively. Specifically, the improvements obtained are 3% and 2% in Recall and F1 score when using YOLOv5, and 3%, 2%, and 2% in Precision, Recall, and F1 score when using Faster RCNN. These findings confirm the potential of our approach to facilitate bee foraging behavior analysis and automated bee monitoring.}
}
@article{LI2024102704,
title = {Multi-species identification and number counting of fish passing through fishway at hydropower stations with LigTraNet},
journal = {Ecological Informatics},
volume = {82},
pages = {102704},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102704},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002462},
author = {Jianyuan Li and Chunna Liu and Luhai Wang and Yi Liu and Rui Li and Xiaochun Lu and Jia Lu and Jian Shen},
keywords = {LigTraNet, Fishway monitoring, Artificial intelligence, Lightweight model, Computer vision, Revolutionizes},
abstract = {Fishway monitoring can verify the effectiveness of the fishway, optimise the operation mode, and achieve scientific management of fishway operations. Traditional fishway monitoring approaches, hindered by their inefficiency and substantial disruption of fish, are ill-suited for long-term surveillance; thus, employing video monitoring coupled with object detection technology presents an alternative or complementary solution. However, challenges such as the constrained computational capacity of onsite equipment in fishways, complexities involved in model deployment, and sluggish pace of detection are significant hurdles. In this study, by utilising the YOLOv8n model as a benchmark, we engineered a cross-stage partial module with a single convolution (C1) module to replace the existing C2f module with the aim of enhancing performance. We replaced the conventional 2D convolutions in the bottleneck configuration with depthwise separable convolutions and integrated the SimAM module to extract the detailed characteristics of the fish species. By amalgamating LigObNet detection with the DeepSORT algorithm, we established LigTraNet, which is designed to enable precise tracking, identification, and counting of individual fish. The results showed that LigObNet exhibited the lowest complexity and fastest detection speed for underwater fish among similar object recognition and detection models. Compared with the benchmark YOLOv8n model, there were reductions of 8.9% in the network layers, 40.5% in the parameter count, 39.3% in the memory footprint, and 35.8% in the giga floating-point operations and a 38.1% improvement in the inference speed. LigTraNet achieved a total count accuracy rate of 91.8%, demonstrating superior species quantification capabilities over other models with minimal resource usage and rapid inference capabilities, thus offering enhanced practicality for deployment on devices in real-world engineering contexts. This represents a departure from traditional manual monitoring methods for assessing fishway effectiveness, revolutionising aquatic ecological monitoring tools and methodologies and fostering the collaborative advancement of water resource project operations and ecological conservation.}
}
@article{GUO2024102607,
title = {A novel space–spectrum array tile probability random-forest model enhances LULC mapping accuracy on Google Earth Engine: An experiment in Ordos, China},
journal = {Ecological Informatics},
volume = {81},
pages = {102607},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102607},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001493},
author = {Fuchen Guo and Liangxin Fan and Chengkang Zhang and Sha Xue},
keywords = {Arid zone, LULC, Tile, Multiple probabilistic classification, Space–spectrum array, Random forest, Google earth engine},
abstract = {The rapid renewal of land use and land cover (LULC) maps using remote-sensing technologies constitutes a sine qua non for judicious land resource management at both regional and national scales. Existing research conducted on the Google Earth Engine (GEE) platform has overwhelmingly focused on pixel-based LULC classification techniques, often neglecting the role of spatial context via neighbouring valuable pixel information. Remarkably, little attention has been paid to the amalgamation of 3 × 3 neighbouring pixels into a three-dimensional space–spectrum array that can emulate the functionalities of object-based image analysis. In this study, we developed a novel integrated model consisting of a space–spectrum array (SSA) model based on 3 × 3 neighbouring pixels, a tile model based on random forest, and a multiple probabilistic classification model (SSA-TPRF) on the GEE platform to generate a LULC map with high overall accuracy (OA) for Ordos in 2020. Three bimonthly median value images were synthesised and feature collections, including spectral bands and vegetation indices, were constructed. Five experimental groups (EXP1–EXP5) were used to assess the different model combinations. Subsequent validation procedures employed abundant reference samples and compared the results with those of the three extant LULC mapping products. The results showed that EXP2, which was grounded in the tile-based model, yielded an OA of 87.53%, surpassing that of EXP1 (84.99%), which employed a traditional overall model. Furthermore, EXP3, which integrated the multiple probabilistic classification model with the traditional overall model, exhibited an OA of 85.19%, exceeding that of EXP1. A comparison of the five experimental groups using the four regional spatial subtlety features revealed that the EXP5, employing the SSA-TPRF model, successfully decreased the salt and pepper noise. The OA of six tile sizes ranging from 10 km to 100 km were compared, and the highest OA (88.35%) was achieved at a tile size of 25 km. The resultant LULC map in Ordos, derived from the SSA-TPRF model, showed superior OA compared with the extant LULC products. This study thus contributes to a versatile and scalable model within the GEE framework, offering avenues for facile adaptation and recurrent application across disparate geographical locations and temporal settings. The adaptability of this model is particularly advantageous for developing nations and regions typified by diverse landscapes, thereby catalysing the iterative updating of LULC maps through advanced remote-sensing paradigms.}
}
@article{YOU2023102200,
title = {Segmentation of individual mangrove trees using UAV-based LiDAR data},
journal = {Ecological Informatics},
volume = {77},
pages = {102200},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102200},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002297},
author = {Haotian You and Yao Liu and Peng Lei and Zhigang Qin and Qixu You},
keywords = {LiDAR data, CHM, Segmentation algorithm, Stand density, Spatial resolution},
abstract = {Accurate assessment of structural parameters is essential to effectively monitor the mangrove resources. However, the extraction results of mangrove structural parameters are closely related to the segmentation results of individual trees. Although the results of individual tree segmentation are influenced by many factors, the specific factors affecting the segmentation results of individual mangrove trees, such as data source, image resolution, segmentation algorithm, and stand density, have not yet been elucidated. Therefore, in this study, canopy height models (CHMs) with different spatial resolutions were derived from unmanned aerial vehicle (UAV)-based light detection and ranging (LiDAR) data. Moreover, the watershed algorithm (WA), regional growth (RG), and improved K-nearest neighbour (KNN) and bird's eye view (BEV) faster region-based convolutional neural network (R-CNN) algorithms were used to segment the individual mangrove trees based on CHMs and LiDAR data at three sites with varying stand densities. Finally, different segmentation algorithms, image resolutions, and forest densities were comparatively assessed to determine their influence on the segmentation results of individual trees. Segmentation accuracy of the improved KNN algorithm was the highest among the CHM-based algorithms, such as the WA, RG, and improved KNN algorithms, with an optimal F of 0.893 and minimum F of 0.628. R-CNN algorithm based on LiDAR data had an optimal F value of 0.931 and minimum F value of 0.612. Based on the segmentation results, the overall accuracy ranking of the different segmentation algorithms was BEV Faster R-CNN > improved KNN > RG > WA. The ranking of the segmentation results for sites with different stand densities was low-density (LD) > medium-density (MD) > high-density (HD). For LD and MD sites, the BEV Faster R-CNN algorithm had the highest F values (0.931 and 0.712, respectively). For the HD site, all algorithms performed poorly, and the F values of all algorithms, except the RG algorithm, were higher than 0.6. Based on the segmentation results of different spatial resolutions, CHM result with 0.1 m was the best, being better than the CHM results with 0.25 and 0.5 m. Our results demonstrated that all segmentation algorithms, spatial resolutions, and stand densities affected the segmentation results for individual mangrove trees. Although the segmentation results of the deep learning algorithm were better than those of the other algorithms, the segmentation results at the HD site were limited. Therefore, further research is necessary to improve the accuracy of the segmentation results for individual mangrove trees at HD sites.}
}
@article{CLARFELD2023102257,
title = {Evaluating a tandem human-machine approach to labelling of wildlife in remote camera monitoring},
journal = {Ecological Informatics},
volume = {77},
pages = {102257},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102257},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002868},
author = {Laurence A. Clarfeld and Alexej P.K. Sirén and Brendan M. Mulhall and Tammy L. Wilson and Elena Bernier and John Farrell and Gus Lunde and Nicole Hardy and Katherina D. Gieder and Robert Abrams and Sue Staats and Scott McLellan and Therese M. Donovan},
keywords = {Artificial intelligence, Camera trap, Data labeling, Machine learning, Trail camera, Wildlife monitoring, Bounding box},
abstract = {Remote cameras (“trail cameras”) are a popular tool for non-invasive, continuous wildlife monitoring, and as they become more prevalent in wildlife research, machine learning (ML) is increasingly used to automate or accelerate the labor-intensive process of labelling (i.e., tagging) photos. Human-machine hybrid tagging approaches have been shown to greatly increase tagging efficiency (i.e., time to tag a single image). However, those potential increases hinge on the extent to which an ML model makes correct vs. incorrect predictions. We performed an experiment using a ML model that produces bounding boxes around animals, people, and vehicles in remote camera imagery (MegaDetector) to consider the impact of a ML model's performance on its ability to accelerate human labeling. Six participants tagged trail camera images collected from 12 sites in Vermont and Maine, USA (January–September 2022) using three tagging methods (one with ML bounding box assistance and two without assistance). We used a generalized linear mixed model to examine the influence of ML model performance and tagging method on tagging efficiency. We found that ML bounding boxes offer significant improvement in tagging efficiency when labelling data compared to unassisted tagging. Additionally, the time taken to label with bounding boxes was not statistically different from an unassisted tagging approach. However, we found that gains in efficiency are contingent on the ML algorithm's performance and that incorrect ML predictions, particularly the 4.2% false positive and 3.6% false negative predictions, can slow the tagging process compared to a non-hybrid approach. These findings indicate that although practitioners usually forgo the production of bounding boxes when selecting a data labelling process due to the increased effort, ML bounding box-assisted tagging can offer an efficient method for labeling. More broadly, ML-assisted data labelling offers an opportunity to accelerate the analysis of trail camera imagery, but an assessment of the ML model's performance can illuminate whether the hybrid-tagging approach is ultimately a help or hinderance.}
}
@article{KURU2023102285,
title = {Intelligent airborne monitoring of irregularly shaped man-made marine objects using statistical Machine Learning techniques},
journal = {Ecological Informatics},
volume = {78},
pages = {102285},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102285},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300314X},
author = {Kaya Kuru and Stuart Clough and Darren Ansell and John McCarthy and Stephanie McGovern},
keywords = {Marine ecosystems, Marine man-made objects, Statistical ML, HSV colour space, Object detection, ROC curve, Aerial surveys},
abstract = {The marine economy has historically been highly diversified and prolific due to the fact that the Earth's oceans comprise two-thirds of its total surface area. As technology advances, leading enterprises and ecological organisations are building and mobilising new devices supported by cutting-edge marine mechatronics solutions to explore and harness this challenging environment. Automated tracking of these types of industries and the marine life around them can help us figure out what's causing the current changes in species numbers, predict what could happen in the future, and create the right policies to help reduce the environmental impact and make the planet more sustainable. The objective of this study is to create a new platform for the automated detection of irregularly shaped man-made marine objects (ISMMMOs) in large datasets derived from marine aerial survey imagery. In this context, a novel nonparametric methodology, which harbours several hybrid statistical Machine Learning (ML) methods, was developed to automatically segment ISMMMOs on the sea surface in large surveys. This methodology was validated on a wide range of marine domains, providing robust empirical proof of concept. This approach enables the detection of ISMMMOs automatically, without any prior training, with accuracy (ACC), Matthews correlation coefficient (MCC), negative predictive value (NPV), positive predictive value (PPV), specificity (Sp) and sensitivity(Se) over 0.95. The outlined methodology can be utilised for a variety of purposes, but it's especially useful for researchers and policymakers who want to keep an eye on how the maritime industry is deploying and make sure the right policies are in place to meet regulatory and legal requirements to promote maritime tech innovation and shape what the future looks like for the marine ecosystem. For the first time in the literature, a method, the so-called ISMMMOD, has been developed to automate the detection of all types of ISMMMOs by statistical ML techniques that require no prior training, which will pioneer the monitoring of human footprint in the marine ecosystem.}
}
@article{DELPLANQUE2024102679,
title = {Will artificial intelligence revolutionize aerial surveys? A first large-scale semi-automated survey of African wildlife using oblique imagery and deep learning},
journal = {Ecological Informatics},
volume = {82},
pages = {102679},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102679},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002218},
author = {Alexandre Delplanque and Julie Linchant and Xavier Vincke and Richard Lamprey and Jérôme Théau and Cédric Vermeulen and Samuel Foucher and Amara Ouattara and Roger Kouadio and Philippe Lejeune},
keywords = {Wildlife population estimation, Aerial surveys, Deep learning, Biodiversity monitoring, Conservation technology, African savanna},
abstract = {Large African mammal populations are traditionally estimated using the systematic reconnaissance flights (SRF) with rear-seat observers (RSOs). The oblique-camera-count (OCC) approach, utilizing digital cameras on aircraft sides, proved to provide more reliable population estimates but incurs high manual processing costs. Addressing the urgent need for efficiency, the research explores whether a semi-automated deep learning (SADL) model coupled with OCC improves wildlife population estimates compared to the SRF-RSO method. The study area was the Comoé National Park, in Ivory Coast, spanning 11,488 km2 of savannas and open forests. It was surveyed following both SRF-RSO standards and OCC method. Key species included the elephant, western hartebeest, roan antelope, buffalo, kob, waterbuck and warthog. The deep learning model HerdNet, priorly pre-trained on images from Uganda, was incorporated in the SADL pipeline to process the 190,686 images. It involved three human verification steps to ensure quality of detections and to avoid overestimating counts. The entire pipeline aims to balance efficiency and human effort in wildlife population estimation. RSO and SADL-OCC approaches were compared using the Jolly II analysis and a verification of 200 random RSO observations. Jolly II analysis revealed SADL-OCC estimates significantly higher for small-sized species (kob, warthog) and comparable for other key species. Counting differences were mainly attributed to vegetation obstruction, RSO observations not found in the images, and suspected RSO counting errors. Human effort in the SADL-OCC approach totaled 111 h, representing a significant time savings compared to a fully manual interpretation. Introducing the SADL approach for aerial surveys in Comoé National Park enabled us to address the OCC's time-intensive image interpretation. Achieving a significant reduction in human workload, our method provided population estimates comparable to or better than SRF-RSO counts. Vegetation obstruction was a key factor explaining differences, highlighting the OCC method's limitation in vegetated areas. Method comparisons emphasized SADL-OCC's advantages in spotting isolated, small and static animals, reducing count variance between sample units. Despite limitations, the SADL-OCC approach offers transformative potential, suggesting a shift towards DL-assisted aerial surveys for increased efficiency and affordability, especially using microlight aircraft and drones in future wildlife monitoring initiatives.}
}
@article{KAUKAB2024102691,
title = {Improving real-time apple fruit detection: Multi-modal data and depth fusion with non-targeted background removal},
journal = {Ecological Informatics},
volume = {82},
pages = {102691},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102691},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002334},
author = {Shaghaf Kaukab and  Komal and Bhupendra M Ghodki and Hena Ray and Yogesh B. Kalnar and Kairam Narsaiah and Jaskaran S. Brar},
keywords = {Apple, Fruit detection, 3D localization, YOLO network, RGB-D images, Depth sensor},
abstract = {In automated fruit detection, RGB-Depth (RGB-D) images aid the detection model with additional depth information to enhance detection accuracy. However, outdoor depth images are usually of low quality, which limits the quality of depth data. In this study, an approach/technique for real-time apple fruit detection in a high-density orchard environment by using multi-modal data is presented. Non-targeted background removal using the depth fusion (NBR-DF) method was developed to reduce the high noise condition of depth images. The noise occurred due to the uncontrolled lighting condition and holes with incomplete depth information in the depth images. NBR-DF technique follows three primary steps: pre-processing of depth images (point cloud generation), target object extraction, and background removal. The NBR-DF method serves as a pipeline to pre-process multi-modal data to enhance features of depth images by filling holes to eliminate noise generated by depth holes. Further, the NBR-DF implemented with the YOLOv5 enhances the detection accuracy in dense orchard conditions by using multi-modal information as input. An attention-based depth fusion module that adaptively fuses the multi-modal features was developed. The integration of the depth-attention matrix involved pooling operations and sigmoid normalization, both of which are efficient methods for summarizing and normalizing depth information. The fusion module improves the identification of multiscale objects and strengthens the network's resistance to noise. The network then detects the fruit position using multiscale information from the RGB-D images in highly complex orchard environments. The detection results were compared and validated with other methods using different input modals and fusion strategies. The results showed that the detection accuracy using the NBR-DF approach achieved an average precision rate of 0.964 in real time. The performance comparison with other state-of-the-art methods and the model generalization study also establish that the present advanced depth-fusion attention mechanism and effective preprocessing steps in NBR-DF-YOLOv5 significantly surpass those in performance. In conclusion, the developed NBR-DF technique showed the potential to improve real-time apple fruit detection using multi-modal information.}
}
@article{CHEN2024102594,
title = {Tradeoffs among multi-source remote sensing images, spatial resolution, and accuracy for the classification of wetland plant species and surface objects based on the MRS_DeepLabV3+ model},
journal = {Ecological Informatics},
volume = {81},
pages = {102594},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102594},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001365},
author = {Zizhen Chen and Jianjun Chen and Yuemin Yue and Yanping Lan and Ming Ling and Xinhong Li and Haotian You and Xiaowen Han and Guoqing Zhou},
keywords = {Wetland, UAV image, Satellite image, Spatial resolution, DeepLabV3 +, Multi-resolution segmentation},
abstract = {Classification of wetland plant species (PlatSpe) and surface objects (SurfObj) in remote sensing images faces significant challenges due to the high diversity of PlatSpe and the fragmented nature of SurfObj. Unmanned aerial vehicle (UAV) images and satellite images are the primary data sources for the classification of wetland PlatSpe and SurfObj. However, there is still insufficient research on the effect of various data sources and spatial resolutions on the classification results. This study essentially focuses on Huixian Wetland in Guilin, Guangxi, China through utilizing UAV images and satellite images with varying spatial resolutions as data sources. To this end, the MRS_DeepLabV3+ model is constructed based on multi-resolution segmentation and DeepLabV3+, and the wetland PlatSpe and SurfObj are appropriately classified based on this model. The obtained results reveal that: (1) MRS_DeepLabV3+ model with optimal scale parameter (SP) is capable of achieving higher classification accuracy compared to DeepLabV3+. The optimal SPs for both UAV images and satellite images gradually lessen with decreasing the spatial resolution, and satellite images require larger SPs compared to UAV images. (2) In both the UAV and satellite image models, both OA and kappa exhibit a decreasing trend with the reduction of the spatial resolution. (3) The overall classification accuracies of the satellite image models are superior to the UAV image models in the spatial resolution intervals of 2 to 16 m. This investigation can be regarded as a valuable reference for selecting data sources and spatial resolutions in the wetland PlatSpe and SurfObj classification.}
}
@article{LEBIEN2020101113,
title = {A pipeline for identification of bird and frog species in tropical soundscape recordings using a convolutional neural network},
journal = {Ecological Informatics},
volume = {59},
pages = {101113},
year = {2020},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101113},
url = {https://www.sciencedirect.com/science/article/pii/S1574954120300637},
author = {Jack LeBien and Ming Zhong and Marconi Campos-Cerqueira and Julian P. Velev and Rahul Dodhia and Juan Lavista Ferres and T. Mitchell Aide},
keywords = {Acoustic monitoring, Bioacoustics, Sound classification, Convolutional neural network, Deep learning},
abstract = {Automated acoustic recorders can collect long-term soundscape data containing species-specific signals in remote environments. Ecologists have increasingly used them for studying diverse fauna around the globe. Deep learning methods have gained recent attention for automating the process of species identification in soundscape recordings. We present an end-to-end pipeline for training a convolutional neural network (CNN) for multi-species multi-label classification of soundscape recordings, starting from raw, unlabeled audio. Training data for species-specific signals are collected using a semi-automated procedure consisting of an efficient template-based signal detection algorithm and a graphical user interface for rapid detection validation. A CNN is then trained based on mel-spectrograms of sound to predict the set of species present in a recording. Transfer learning of a pre-trained model is employed to reduce the necessary training data and time. Furthermore, we define a loss function that allows for using true and false template-based detections to train a multi-class multi-label audio classifier. This approach leverages relevant absence (negative) information in training, and reduces the effort in creating multi-label training data by allowing weak labels. We evaluated the pipeline using a set of soundscape recordings collected across 749 sites in Puerto Rico. A CNN model was trained to identify 24 regional species of birds and frogs. The semi-automated training data collection process greatly reduced the manual effort required for training. The model was evaluated on an excluded set of 1000 randomly sampled 1-min soundscapes from 17 sites in the El Yunque National Forest. The test recordings contained an average of ~3 present target species per recording, and a maximum of 8. The test set also showed a large class imbalance with most species being present in less than 5% of recordings, and others present in >25%. The model achieved a mean-average-precision of 0.893 across the 24 species. Across all predictions, the total average-precision was 0.975.}
}
@article{LYU2024102383,
title = {Deer survey from drone thermal imagery using enhanced faster R-CNN based on ResNets and FPN},
journal = {Ecological Informatics},
volume = {79},
pages = {102383},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102383},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004120},
author = {Haitao Lyu and Fang Qiu and Li An and Douglas Stow and Rebecca Lewison and Eve Bohnett},
keywords = {UAV, Faster R-CNN, ResNet, FPN, Thermal image, Small object detection},
abstract = {Deer surveys play an important role in the estimation of local ecological balance. In the Chitwan National Park of Nepal, the dense tree canopies and tall vegetation often obscure the presence of wild deer, which has a negative effect on the accurate population surveys of wild deer. UAVs equipped with infrared sensors have been increasingly used to monitor wild deer by capturing a lot of images. How to automatically recognize and obtain the number of deer objects from thermal images is becoming an important research topic. Due to the difference between thermal images and true-color images, as well as the variations in deer object sizes in these two types of images, current ready-to-use object detection models, designed for true-color imagery, are ill-suited for the task of detecting small deer objects within thermal imagery. In this paper, an enhanced Faster R-CNN was constructed to detect small deer objects from thermal images, in which a Feature Pyramid Network (FPN) based on a residual network is used to improve feature extraction for small deer objects and multi-scale feature map constrution for the subsequent region proposals searching, bounding box regression, and regions of interest (RoIs) classification. In addition, small-scaled anchor boxes and a multi-scale feature map selection criterion are devised to improve the detection accuracy of small objects. Finally, based on Faster R-CNN, FPN, and different residual networks including ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152, we constructed five object detection models, and evaluated their detection performance by using COCO evaluation matrix. Under the condition of IoU≥0.5, the integration of Faster R-CNN, FPN, and ResNet18 demonstrated to perform better than others. Specifically, The COCO evaluation results revealed an Average Precision (AP) score of 91.6% for all deer objects. Small deer objects (area ≤ 200 pixels) achieved an AP score of 73.6%, medium deer objects (200 < area ≤ 400 pixels) demonstrated an AP score of 93.4%, and large deer objects (area > 400 pixels) achieved the highest AP score of 94.3%. Our research is helpful for effective wild deer monitoring and conservation and can be a valuable reference for the exploration of small object detection from low-resolution thermal images.}
}
@article{POUTARAUD2024102687,
title = {Meta-Embedded Clustering (MEC): A new method for improving clustering quality in unlabeled bird sound datasets},
journal = {Ecological Informatics},
volume = {82},
pages = {102687},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102687},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002292},
author = {Joachim Poutaraud and Jérôme Sueur and Christophe Thébaud and Sylvain Haupert},
keywords = {Bird sounds, Ecoacoustics, meta-learning, Unsupervised learning, Dimensionality reduction, Spectrogram},
abstract = {In recent years, ecoacoustics has offered an alternative to traditional biodiversity monitoring techniques with the development of passive acoustic monitoring (PAM) systems allowing, among others, to detect and identify species that are difficult to detect by human observers, automatically. PAM systems typically generate large audio datasets, but using these monitoring techniques to infer ecologically meaningful information remains challenging. In most cases, several thousand hours of recordings need to be manually labeled by experts limiting the operability of the systems. Based on recent developments of meta-learning algorithms and unsupervised learning techniques, we propose here Meta-Embedded Clustering (MEC), a new method with high potential for improving clustering quality in unlabeled bird sound datasets. MEC method is organized in two main steps, with: (a) fine-tuning of a pretrained convolutional neural network (CNN) backbone with different meta-learning algorithms using pseudo-labeled data, and (b) clustering of manually-labeled bird sounds in the latent space based on vector embeddings extracted from the fine-tuned CNN. The MEC method significantly enhanced average clustering performance from less than 1% to more than 80%, greatly outperforming the traditional approach of relying solely on CNN features extracted from a general neotropical audio database. However, this enhanced performance came with the cost of excluding a portion of the data categorized as noise. By improving the quality of clustering in unlabeled bird sound datasets, the MEC method should facilitate the work of ecoacousticians in managing acoustic units of bird song/call clustered according to their similarities, and in identifying potential clusters of species undetected using traditional approaches.}
}
@article{ORIOL2024102606,
title = {Automatic identification of Collembola with deep learning techniques},
journal = {Ecological Informatics},
volume = {81},
pages = {102606},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102606},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001481},
author = {Théo Oriol and Jérôme Pasquet and Jérôme Cortet},
keywords = {Deep learning, Object detection, Collembola, Soil quality, Bioindication},
abstract = {Collembola are very abundant organisms in soils (several thousand individuals per square meter) and are considered to be good indicators of soil quality. These indicators are mainly based on the number of individuals observed (abundance per square meter of soil), but also the singularity and number of species present (species richness). A limitation that comes with the usage of collembola as an indicator is the complexity of the identification of the species under a microscope, how time-consuming it is, and the morphological similarity between some species. Deep learning approaches have been very successful in the resolution of image-based problems. Still, no work yet exists that uses deep learning in the recognition of collembola on a microscope slide. This could be a valuable tool for experts seeking to use Collembola as a metric on a larger scale. In this work, we explore and evaluate the performance of state-of-the-art deep learning techniques over the identification of Collembola on a new manually annotated dataset.}
}
@article{MARTINEZSANCHEZ2022101757,
title = {Skyline variations allow estimating distance to trees on landscape photos using semantic segmentation},
journal = {Ecological Informatics},
volume = {70},
pages = {101757},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101757},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002072},
author = {Laura Martinez-Sanchez and Daniele Borio and Raphaël d'Andrimont and Marijn {van der Velde}},
keywords = {Semantic segmentation, Conditional random fields, COCO, Landscape, Openness, Image depth},
abstract = {Approximate distance estimation can be used to determine fundamental landscape properties including complexity and openness. We show that variations in the skyline of landscape photos can be used to estimate distances to trees on the horizon. A methodology based on the variations of the skyline has been developed and used to investigate potential relationships with the distance to skyline objects. The skyline signal, defined by the skyline height expressed in pixels, was extracted for a set of 148 Land Use/Cover Area frame Survey (LUCAS) landscape photos. Photos were semantically segmented with DeepLabV3+ trained with the Common Objects in Context (COCO) dataset. This provided pixel-level classification of the objects forming the skyline. A Conditional Random Fields (CRF) algorithm was also applied to increase the details of the skyline signal. Three metrics, able to capture the skyline signal variations, were then considered for the analysis. These metrics shows a functional relationship with distance for the class of trees, whose contours have a fractal nature. In particular, regression analysis was performed against 475 ortho-photo based distance measurements, and, in the best case, a R2 score equal to 0.47 was achieved. This is an encouraging result which shows the potential of skyline variation metrics for inferring distance related information.}
}
@article{GRIJALVA2024102540,
title = {Detecting and counting sorghum aphid alates using smart computer vision models},
journal = {Ecological Informatics},
volume = {80},
pages = {102540},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102540},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000827},
author = {Ivan Grijalva and H. Braden Adams and Nicholas Clark and Brian McCornack},
keywords = {sorghum, sorghum aphid, alates, automation, detection},
abstract = {Sorghum aphid [Melanaphis sorghi (Theobald)] is considered an economic pest causing significant yield losses in susceptible sorghum in the southern U.S. Infestations start with the migration of alates (i.e., winged adults) to sorghum and establishing aphid colonies. In favorable conditions, sorghum aphid can exponentially reproduce via asexual reproduction. A suggested strategy is to monitor alates to determine initial infestations and take preventive strategies, which can result in more efficient pest monitoring and management. To reduce the time of monitoring and better understand of alate establishment under field conditions, we propose using computer vision models, specifically deep learning, to detect and count alates using field-collected images. During pest monitoring, we captured 2527 images and assessed the performance of five models within the YOLOv5 architecture family using two different image sizes, including input resolutions of 640 × 640, and 1280 × 1280 pixels. We trained models to detect and count individual alates, which ranged between 1 and 100 alates/leaf. Among models, the YOLOv5l Pytorch detection model had the best overall performance at 1280 × 1280 input pixel resolution. The YOLOv5l model is a candidate model for quantifying alates on sorghum leaves using deep learning with a precision of 83.80%, 85.60% recall, and 89% mAP@0.5 with a lower mean percent error of misdetection. To enable the use of our best deep learning model by the research community, we developed a web-based application that is freely available to the public. Using this application, users can upload images to detect and count alates with a low error of misdetection.}
}
@article{TAKIMOTO2021101466,
title = {Using a two-stage convolutional neural network to rapidly identify tiny herbivorous beetles in the field},
journal = {Ecological Informatics},
volume = {66},
pages = {101466},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101466},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121002570},
author = {Hironori Takimoto and Yasuhiro Sato and Atsushi J. Nagano and Kentaro K. Shimizu and Akihiro Kanagawa},
keywords = {Entomology, Fine-grained image classification, Deep learning, Herbivory, Small object detection},
abstract = {Recently, deep convolutional neural networks (CNN) have been adopted to help non-experts identify insect species from field images. However, the application of these methods on the rapid identification of tiny congeneric species moving across heterogeneous background remains difficult. To improve rapid and automatic identification in the field, we customized an existing CNN-based method for a field video involving two Phyllotreta beetles. We first performed data augmentation using transformations, syntheses, and random erasing of the original images. We then proposed a two-stage method for the detection and identification of small insects based on CNN, where YOLOv4 and EfficientNet were used as a detector and a classifier, respectively. Evaluation of the model revealed that one-step object detection by YOLOv4 alone was not precise (Precision=0.55) when classifying two species of flea beetles and background objects. In contrast, the two-step CNNs improved the precision (Precision=0.89) with moderate accuracy (F-measure=0.55) and acceptable speed (ca. 5 frames per second for full HD images) of detection and identification of insect species in the field. Although real-time identification of tiny insects remains a challenge in the field, our method aids in improving small object detection on a heterogeneous background.}
}
@article{KAHL2021101236,
title = {BirdNET: A deep learning solution for avian diversity monitoring},
journal = {Ecological Informatics},
volume = {61},
pages = {101236},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101236},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000273},
author = {Stefan Kahl and Connor M. Wood and Maximilian Eibl and Holger Klinck},
keywords = {Bioacoustics, Deep learning, Convolutional neural networks, Bird sound recognition, Avian diversity, Passive acoustic monitoring, Conservation},
abstract = {Variation in avian diversity in space and time is commonly used as a metric to assess environmental changes. Conventionally, such data were collected by expert observers, but passively collected acoustic data is rapidly emerging as an alternative survey technique. However, efficiently extracting accurate species richness data from large audio datasets has proven challenging. Recent advances in deep artificial neural networks (DNNs) have transformed the field of machine learning, frequently outperforming traditional signal processing techniques in the domain of acoustic event detection and classification. We developed a DNN, called BirdNET, capable of identifying 984 North American and European bird species by sound. Our task-specific model architecture was derived from the family of residual networks (ResNets), consisted of 157 layers with more than 27 million parameters, and was trained using extensive data pre-processing, augmentation, and mixup. We tested the model against three independent datasets: (a) 22,960 single-species recordings; (b) 286 h of fully annotated soundscape data collected by an array of autonomous recording units in a design analogous to what researchers might use to measure avian diversity in a field setting; and (c) 33,670 h of soundscape data from a single high-quality omnidirectional microphone deployed near four eBird hotspots frequented by expert birders. We found that domain-specific data augmentation is key to build models that are robust against high ambient noise levels and can cope with overlapping vocalizations. Task-specific model designs and training regimes for audio event recognition perform on-par with very complex architectures used in other domains (e.g., object detection in images). We also found that high temporal resolution of input spectrograms (short FFT window length) improves the classification performance for bird sounds. In summary, BirdNET achieved a mean average precision of 0.791 for single-species recordings, a F0.5 score of 0.414 for annotated soundscapes, and an average correlation of 0.251 with hotspot observation across 121 species and 4 years of audio data. By enabling the efficient extraction of the vocalizations of many hundreds of bird species from potentially vast amounts of audio data, BirdNET and similar tools have the potential to add tremendous value to existing and future passively collected audio datasets and may transform the field of avian ecology and conservation.}
}
@article{FRAINER2023102291,
title = {Automatic detection and taxonomic identification of dolphin vocalisations using convolutional neural networks for passive acoustic monitoring},
journal = {Ecological Informatics},
volume = {78},
pages = {102291},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102291},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003205},
author = {Guilherme Frainer and Emmanuel Dufourq and Jack Fearey and Sasha Dines and Rachel Probert and Simon Elwen and Tess Gridley},
keywords = {Convolutional neural networks, Indian Ocean humpback dolphin, Machine learning, Passive acoustic monitoring, Sound detection, Species identification},
abstract = {A novel framework for acoustic detection and species identification is proposed to aid passive acoustic monitoring studies on the endangered Indian Ocean humpback dolphin (Sousa plumbea) in South African waters. Convolutional Neural Networks (CNNs) were used for both detection and identification of dolphin vocalisations tasks, and performance was evaluated using custom and pre-trained architectures (transfer learning). In total, 723 min of acoustic data were annotated for the presence of whistles, burst pulses and echolocation clicks produced by Delphinus delphis (~45.6%), Tursiops aduncus (~39%), Sousa plumbea (~14.4%), Orcinus orca (~1%). The best performing models for detecting dolphin presence and species identification used segments (spectral windows) of two second lengths and were trained using images with 70 and 90 dpi, respectively. The best detection model was built using a customised architecture and achieved an accuracy of 84.4% for all dolphin vocalisations on the test set, and 89.5% for vocalisations with a high signal to noise ratio. The best identification model was also built using the customised architecture and correctly identified S. plumbea (96.9%), T. aduncus (100%), and D. delphis (78%) encounters in the testing dataset. The developed framework was designed based on the knowledge of complex dolphin sounds and it may assists in finding suitable CNN hyper-parameters for other species or populations. Our study contributes towards the development of an open-source tool to assist long-term studies of endangered species, living in highly diverse habitats, using passive acoustic monitoring.}
}
@article{CAPINHA2021101252,
title = {Deep learning for supervised classification of temporal data in ecology},
journal = {Ecological Informatics},
volume = {61},
pages = {101252},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101252},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000431},
author = {César Capinha and Ana Ceia-Hasse and Andrew M. Kramer and Christiaan Meijer},
keywords = {Deep learning, Ecological prediction, Scalability, Sequential data, Temporal ecology, Time series},
abstract = {Temporal data is ubiquitous in ecology and ecologists often face the challenge of accurately differentiating these data into predefined classes, such as biological entities or ecological states. The usual approach consists of transforming the time series into user-defined features and then using these features as predictors in conventional statistical or machine learning models. Here we suggest the use of deep learning models as an alternative to this approach. Recent deep learning techniques can perform the classification directly from the time series, eliminating subjective and resource-consuming data transformation steps, and potentially improving classification results. We describe some of the deep learning architectures relevant for time series classification and show how these architectures and their hyper-parameters can be tested and used for the classification problems at hand. We illustrate the approach using three case studies from distinct ecological subdisciplines: i) insect species identification from wingbeat spectrograms; ii) species distribution modelling from climate time series and iii) the classification of phenological phases from continuous meteorological data. The deep learning approach delivered ecologically sensible and accurate classifications demonstrating its potential for wide applicability across subfields of ecology.}
}
@article{CHAKRABARTY2024102718,
title = {An interpretable fusion model integrating lightweight CNN and transformer architectures for rice leaf disease identification},
journal = {Ecological Informatics},
volume = {82},
pages = {102718},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102718},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002607},
author = {Amitabha Chakrabarty and Sarder Tanvir Ahmed and Md. Fahim Ul Islam and Syed Mahfuzul Aziz and Siti Sarah Maidin},
keywords = {Plant disease detection, BEiT model, Attention mapping, Deep learning, Process innovation, Food productivity},
abstract = {Swift identification of leaf diseases is crucial for sustainable rice farming, a staple grain consumed globally. The high costs and inefficiencies of manual identification underline the requirement of prompt disease detection. Traditional approaches for identifying leaf diseases in crops, particularly rice, are laborious, and and often ineffective. Given the significant impact of leaf diseases (such as Rice Blast, Brown Spot, and Rice Turgor) on rice quality and yield, computer-assisted detection can be an effective method of ensuring the long-term sustainability of rice production. This study utilizes advanced artificial intelligence (AI) as the optimized bidirectional encoder representations from the transformers for images(BEiT) model along with pre-trained CNNs (Convolutional Neural Networks), to build a comprehensive study for detecting rice leaf diseases. We train and validate two extensive datasets, featuring healthy and various types of unhealthy plant and rice leaf images respectively.Our optimized model demonstrates high accuracy, outperforming other deep learning and transformer-based models such as ViT, Xception, InceptionV3, DenseNet169, VGG16, and ResNet50. The proposed model achieves a precision of 0.97, a recall of 0.96, and an F1-score of 0.97.The explainability of our proposed model is achieved through the use of segmentation techniques in conjunction with the Local Interpretable Model-agnostic Explanations (LIME) method.}
}
@article{GHOSH2024102581,
title = {HPB3C-3PG algorithm: A new hybrid global optimization algorithm and its application to plant classification},
journal = {Ecological Informatics},
volume = {81},
pages = {102581},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102581},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001237},
author = {Sukanta Ghosh and Amar Singh and Shakti Kumar},
keywords = {Optimization problem, HPB3C-3PGA, Hybrid algorithm, CEC’21 benchmark function, PB3C, 3PGA, Exploration, Exploitation, Image classification},
abstract = {This paper proposes a hybrid bio-inspired search and optimization algorithm that combines the strengths of the PB3C (Parallel Big Bang Big Crunch) and 3PGA (3 Parent Genetic Algorithm) algorithms. The hybrid algorithm employs a single population-based evolutionary search coupled with multi-population parallel processing techniques to address optimization problems. The proposed algorithm is implemented in MATLAB software. We evaluate the performance of the proposed algorithm on the CEC2021 standard test bench suite. The performance of the proposed approach is compared with that of the other nine algorithms. The comparative analysis shows that the proposed hybrid PB3C and 3PGA algorithms performed better than the other nine optimization algorithms. Furthermore, this chapter proposes an HPB3C-3PGA-based approach to evolve the near-optimal architecture of CNN. The proposed plant image classification approach is implemented in Python and compared with 12 other approaches. The proposed approach achieved an accuracy of 98.96% on the Mendeley dataset and 98.97% on the CVIP100 dataset. The proposed approach outperforms all other approaches for the plant leaf classification problem. This research significantly contributes to overcoming limitations in existing approaches, providing a robust solution for optimization problems and image classification tasks.}
}
@article{MORTIER2024102730,
title = {Inferring the relationship between soil temperature and the normalized difference vegetation index with machine learning},
journal = {Ecological Informatics},
volume = {82},
pages = {102730},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102730},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002723},
author = {Steven Mortier and Amir Hamedpour and Bart Bussmann and Ruth Phoebe Tchana Wandji and Steven Latré and Bjarni D. Sigurdsson and Tom {De Schepper} and Tim Verdonck},
keywords = {Soil temperature, Phenology, Machine learning, Climate change, SHAP values, Subarctic grassland},
abstract = {Changes in climate can greatly affect the phenology of plants, which can have important feedback effects, such as altering the carbon cycle. These phenological feedback effects are often induced by a shift in the start or end dates of the growing season of plants. The normalized difference vegetation index (NDVI) serves as a straightforward indicator for assessing the presence of green vegetation and can also provide an estimation of the plants' growing season. In this study, we investigated the effect of soil temperature on the timing of the start of the season (SOS), timing of the peak of the season (POS), and the maximum annual NDVI value (PEAK) in subarctic grassland ecosystems between 2014 and 2019. We also explored the impact of other meteorological variables, including air temperature, precipitation, and irradiance, on the inter-annual variation in vegetation phenology. Using machine learning (ML) techniques and SHapley Additive exPlanations (SHAP) values, we analyzed the relative importance and contribution of each variable to the phenological predictions. Our results reveal a significant relationship between soil temperature and SOS and POS, indicating that higher soil temperatures lead to an earlier start and peak of the growing season. However, the Peak NDVI values showed just a slight increase with higher soil temperatures. The analysis of other meteorological variables demonstrated their impacts on the inter-annual variation of the vegetation phenology. Ultimately, this study contributes to our knowledge of the relationships between soil temperature, meteorological variables, and vegetation phenology, providing valuable insights for predicting vegetation phenology characteristics and managing subarctic grasslands in the face of climate change. Additionally, this work provides a solid foundation for future ML-based vegetation phenology studies.}
}
@article{SWAMINATHAN2024102471,
title = {Multi-label classification for acoustic bird species detection using transfer learning approach},
journal = {Ecological Informatics},
volume = {80},
pages = {102471},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102471},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400013X},
author = {Bhuvaneswari Swaminathan and M. Jagadeesh and Subramaniyaswamy Vairavasundaram},
keywords = {Wav2vec, Transformers, Transfer learning, Multi-label, Bird species classification, Audio classification},
abstract = {As part of ornithology, bird species classification is vital to understanding species distribution, habitat requirements and environmental changes that affect bird populations. It is possible for ornithologists to assess the health of a certain habitat by tracking changes in bird species distributions. This work has extended an efficient transfer learning technique for labelling and classifying multiple bird species from real-time audio recordings. For this purpose, Wav2vec is fine-tuned using the back propagation technique, which makes the feature extractor more effective in learning each bird's pitch and other sound characteristics. To perform the task, each audio recording has been clipped as chunks from the overlapping audio to determine multi-labels from it. Through the application of transfer learning, the features of audio recordings have been automatically extracted for classification and fed to a feed-forward network. Subsequently, probabilities associated with each audio segment is aggregated through the clipping approach to represent multiple species of bird call. These probability scores are then used to determine the presence of predominant bird species in the audio recording for multi-labelling. The proposed Wav2vec demonstrates remarkable performance, achieving an F1-score of 0.89 using the Xeno-Canto dataset in which outperforming other multi-label classifiers.}
}
@article{LAVNER2024102528,
title = {The bioacoustic soundscape of a pandemic: Continuous annual monitoring using a deep learning system in Agmon Hula Lake Park},
journal = {Ecological Informatics},
volume = {80},
pages = {102528},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102528},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000700},
author = {Yizhar Lavner and Ronen Melamed and Moshe Bashan and Yoni Vortman},
keywords = {Long-term bird monitoring, BirdNET, Deep learning, Bioacoustics, Passive acoustic monitoring, Avian influenza},
abstract = {Continuous bioacoustic monitoring is an emerging opportunity as well as a challenge, allowing detection of cryptic species' activity while producing high computational demands. In this paper, we present an automated framework that allows the monitoring of a large number of bird species by their vocalizations over extended periods. The framework relies on the BirdNET-Analyzer deep learning model. We applied the framework to >80 species; 20 species with the highest recall scores were selected for further analysis. We used the framework to analyze acoustic signals recorded continuously for over two years using autonomous recorders at various locations in Agmon Hula Lake Park, Israel. During this period there was an acute outbreak of avian influenza in the area. We analyzed differences in acoustic occupancy for various species between two consecutive years (November 2020 to October 2022). We examined between-year population trends for 17 species, both migratory and resident, and found a significant decline in vocal activity between the two years for 10 species. We assume that this decline is related to the avian influenza outbreak, suggesting that the impact of the pandemic may be more widespread and affected a greater number of local species than was previously realized. This further highlights the power and effectiveness of bioacoustic monitoring in detecting cryptic but dramatic dynamics.}
}
@article{CHABOT2022101547,
title = {Using Web images to train a deep neural network to detect sparsely distributed wildlife in large volumes of remotely sensed imagery: A case study of polar bears on sea ice},
journal = {Ecological Informatics},
volume = {68},
pages = {101547},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101547},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121003381},
author = {Dominique Chabot and Seth Stapleton and Charles M. Francis},
keywords = {Artificial intelligence, Big data, Computer vision, Conservation, Machine learning, Marine mammals},
abstract = {Remote sensing can be a valuable alternative or complement to traditional techniques for monitoring wildlife populations, but often entails operational bottlenecks at the image analysis stage. For example, photographic aerial surveys have several advantages over surveys employing airborne observers or other more intrusive monitoring techniques, but produce onerous amounts of imagery for manual analysis when conducted across vast areas, such as the Arctic. Deep learning algorithms, chiefly convolutional neural networks (CNNs), have shown promise for automatically detecting wildlife in large and/or complex image sets. But for sparsely distributed species, such as polar bears (Ursus maritimus), there may not be sufficient known instances of the animals in an image set to train a CNN. We investigated the feasibility of instead providing ‘synthesized’ training data to a CNN to detect polar bears throughout large volumes of aerial imagery from a survey of the Baffin Bay subpopulation. We harvested 534 miscellaneous images of polar bears from the Web that we edited to more closely resemble 21 known images of bears from the aerial survey that were solely used for validation. We combined the Web images of polar bears with 6292 random background images from the aerial survey to train a CNN (ResNet-50), which subsequently correctly classified 20/21 (95%) bear images from the survey and 1172/1179 (99.4%) random background validation images. Given that even a small background misclassification rate could produce multitudinous false positives over many thousands of photos, we describe a potential workflow to efficiently screen out erroneous detections. We also discuss potential avenues to improve CNN accuracy, and the broader applicability of our approach to other image-based wildlife monitoring scenarios. Our results demonstrate the feasibility of using miscellaneously sourced images of animals to train deep neural networks for specific wildlife detection tasks.}
}
@article{WEINSTEIN2020101061,
title = {Cross-site learning in deep learning RGB tree crown detection},
journal = {Ecological Informatics},
volume = {56},
pages = {101061},
year = {2020},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101061},
url = {https://www.sciencedirect.com/science/article/pii/S157495412030011X},
author = {Ben G. Weinstein and Sergio Marconi and Stephanie A. Bohlman and Alina Zare and Ethan P. White},
keywords = {Tree crown detection, RGB deep learning, Object detection, Airborne LiDAR},
abstract = {Tree crown detection is a fundamental task in remote sensing for forestry and ecosystem ecology. While many individual tree segmentation algorithms have been proposed, the development and testing of these algorithms is typically site specific, with few methods evaluated against data from multiple forest types simultaneously. This makes it difficult to determine the generalization of proposed approaches, and limits tree detection at broad scales. Using data from the National Ecological Observatory Network, we extend a recently developed deep learning approach to include data from a range of forest types to determine whether information from one forest can be used for tree detection in other forests, and explore the potential for building a universal tree detection algorithm. We find that the deep learning approach works well for overstory tree detection across forest conditions. Performance was best in open oak woodlands and worst in alpine forests. When models were fit to one forest type and used to predict another, performance generally decreased, with better performance when forests were more similar in structure. However, when models were pretrained on data from other sites and then fine-tuned using a relatively small amount of hand-labeled data from the evaluation site, they performed similarly to local site models. Most importantly, a model fit to data from all sites performed as well or better than individual models trained for each local site.}
}
@article{SAIGUSA2024102569,
title = {Robust minimum divergence estimation in a spatial Poisson point process},
journal = {Ecological Informatics},
volume = {81},
pages = {102569},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102569},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001110},
author = {Yusuke Saigusa and Shinto Eguchi and Osamu Komori},
keywords = {Model misspecification, Poisson point process model, Presence-only, Robustness, Sampling bias, Species distribution modeling},
abstract = {Species distribution modeling (SDM) plays a crucial role in investigating habitat suitability and addressing various ecological issues. While likelihood analysis is commonly used to draw ecological conclusions, its statistical performance is not robust when confronted with slight deviations due to model misspecification in SDM. We proposed a new robust estimation method based on a novel divergence for the Poisson point process model. The method is characterized by weighting the log-likelihood equation to mitigate the impact of heterogeneous observations in the presence-only data, which can result from model misspecification. We demonstrated that our proposed method improved the predictive performance of maximum likelihood estimation in both simulation studies and the analysis of vascular plant data in Japan.}
}
@article{JAHANBAKHT2023102303,
title = {Semi-supervised and weakly-supervised deep neural networks and dataset for fish detection in turbid underwater videos},
journal = {Ecological Informatics},
volume = {78},
pages = {102303},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102303},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003321},
author = {Mohammad Jahanbakht and Mostafa {Rahimi Azghadi} and Nathan J. Waltham},
keywords = {Weakly-supervised classifier, Self-supervised learning, Deep neural networks, Contrastive learning, Transfer learning, XGBoost ensemble, Fish detection, Highly turbid waters},
abstract = {Fish are key members of marine ecosystems, and they have a significant share in the healthy human diet. Besides, fish abundance is an excellent indicator of water quality, as they have adapted to various levels of oxygen, turbidity, nutrients, and pH. To detect various fish in underwater videos, Deep Neural Networks (DNNs) can be of great assistance. However, training DNNs is highly dependent on large, labeled datasets, while labeling fish in turbid underwater video frames is a laborious and time-consuming task, hindering the development of accurate and efficient models for fish detection. To address this problem, firstly, we have collected a dataset called FishInTurbidWater, which consists of a collection of video footage gathered from turbid waters, and quickly and weakly (i.e., giving higher priority to speed over accuracy) labeled them in a 4-times fast-forwarding software. Next, we designed and implemented a semi-supervised contrastive learning fish detection model that is self-supervised using unlabeled data, and then fine-tuned with a small fraction (20%) of our weakly labeled FishInTurbidWater data. At the next step, we trained, using our weakly labeled data, a novel weakly-supervised ensemble DNN with transfer learning from ImageNet. The results show that our semi-supervised contrastive model leads to more than 20 times faster turnaround time between dataset collection and result generation, with reasonably high accuracy (89%). At the same time, the proposed weakly-supervised ensemble model can detect fish in turbid waters with high (94%) accuracy, while still cutting the development time by a factor of four, compared to fully-supervised models trained on carefully labeled datasets. Our dataset and code are publicly available at the hyperlink FishInTurbidWater.}
}
@article{FISCHER2023101988,
title = {Drones and sound recorders increase the number of bird species identified: A combined surveys approach},
journal = {Ecological Informatics},
volume = {74},
pages = {101988},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.101988},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000171},
author = {Sarah Fischer and Andrew C. Edwards and Stephen T. Garnett and Timothy G. Whiteside and Patrice Weber},
keywords = {Bioacoustics, Drone, Bird surveys, Species identification},
abstract = {Many studies have compared results from sound recordings and traditional point-count survey observer data when surveying avian communities. None have investigated the use of a moving sound recorder to replicate line-transect surveying. We conducted point-count surveys and line-transect surveys in four urban/peri-urban habitats in Darwin, tropical Australia, with stationary and moving sound recorders, respectively, to assess whether such a combination would result in more bird species being identified than with either technique alone. More bird species were identified using sound recordings than standard observer data. Further, the difference in the number of species identified between the observer and audio from point-count surveys was found to be significant with audio identification being more accurate; however, line-transect surveys showed no significant difference between the two identification methods. Overall, there was no statistical significance between using point-count surveys and line-transect surveys for total species identified. Linear mixed modelling found the interaction between habitat and survey type (point-count vs line-transect) was strongly significant, but not so that between habitat and survey method (sound recording vs human observation). Our results indicate that the integration of bioacoustic and drone technologies with traditional avian surveying techniques adds significant additional identifications when compiling a species list of an area.}
}
@article{NOLASCO2023102258,
title = {Learning to detect an animal sound from five examples},
journal = {Ecological Informatics},
volume = {77},
pages = {102258},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102258},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300287X},
author = {Ines Nolasco and Shubhr Singh and Veronica Morfi and Vincent Lostanlen and Ariana Strandburg-Peshkin and Ester Vidaña-Vila and Lisa Gill and Hanna Pamuła and Helen Whitehead and Ivan Kiskin and Frants H. Jensen and Joe Morford and Michael G. Emmerson and Elisabetta Versace and Emily Grout and Haohe Liu and Burooj Ghani and Dan Stowell},
keywords = {Bioacoustics, Deep learning, Event detection, Few-shot learning},
abstract = {Automatic detection and classification of animal sounds has many applications in biodiversity monitoring and animal behavior. In the past twenty years, the volume of digitised wildlife sound available has massively increased, and automatic classification through deep learning now shows strong results. However, bioacoustics is not a single task but a vast range of small-scale tasks (such as individual ID, call type, emotional indication) with wide variety in data characteristics, and most bioacoustic tasks do not come with strongly-labelled training data. The standard paradigm of supervised learning, focussed on a single large-scale dataset and/or a generic pre-trained algorithm, is insufficient. In this work we recast bioacoustic sound event detection within the AI framework of few-shot learning. We adapt this framework to sound event detection, such that a system can be given the annotated start/end times of as few as 5 events, and can then detect events in long-duration audio—even when the sound category was not known at the time of algorithm training. We introduce a collection of open datasets designed to strongly test a system's ability to perform few-shot sound event detections, and we present the results of a public contest to address the task. Our analysis shows that prototypical networks are a very common used strategy and they perform well when enhanced with adaptations for general characteristics of animal sounds. However, systems with high time resolution capabilities perform the best in this challenge. We demonstrate that widely-varying sound event durations are an important factor in performance, as well as non-stationarity, i.e. gradual changes in conditions throughout the duration of a recording. For fine-grained bioacoustic recognition tasks without massive annotated training data, our analysis demonstrate that few-shot sound event detection is a powerful new method, strongly outperforming traditional signal-processing detection methods in the fully automated scenario.}
}
@article{CONCEPCION2023102344,
title = {BivalveNet: A hybrid deep neural network for common cockle (Cerastoderma edule) geographical traceability based on shell image analysis},
journal = {Ecological Informatics},
volume = {78},
pages = {102344},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102344},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003734},
author = {Ronnie Concepcion and Marielet Guillermo and Susanne E. Tanner and Vanessa Fonseca and Bernardo Duarte},
keywords = {Traceability, Morphometric analysis, Deep learning, Bivalves, Low-cost},
abstract = {Bivalve traceability is a major concern. It is of utmost importance to develop tools that allow providing important information to the consumer, not only on the origin of the product but also on its sustainability and safety, due to the harvest restrictions imposed by regulatory entities. This study evaluated the application of computer vision machine learning technologies for efficiently discriminating cockle harvesting origin based on shell geometric and morphometric analysis, improving the traceability methodologies in these organisms, and highlighting the potential of these low-cost techniques as a reliable traceability tool. Thirty Cerastoderma edule samples were collected along the five locations in Atlantic West and South Portuguese coast with individual images processed using lazysnapping segmentation, spectro-textural-morphological phenotype extraction, and feature selection through hybrid Principal Component Analysis and Neighborhood Component Analysis which resulted in R, a*, b*, entropy, and diameter. Three approaches of traceability models were developed and tested: pre-trained networks (EfficientNet-Bo, ResNet101, MobileNetV2, InceptionV3) with numerical inputs (Approach 1), image-based pre-trained networks (Approach 2), and hybrid deep neural networks of Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Bidirectional LSTM (BiLSTM) (Approach 3). Based on the test results, Approach 3 with GRU-LSTM-BiLSTM sequence exhibited the highest accuracy (96.91%) and sensitivity (96%) among the other thirteen machine learning models, hence, named as BivalveNet. Comparing the attained accuracy from the BivalveNet to other mollusc traceability studies, it was observed that an efficiency close to the attained using standard destructive, time-consuming, and expensive techniques, making BivalveNet a highly advantageous approach for common cockle geographical traceability studies, available for application to other bivalve species.}
}
@article{GIBB2024102449,
title = {Towards interpretable learned representations for ecoacoustics using variational auto-encoding},
journal = {Ecological Informatics},
volume = {80},
pages = {102449},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102449},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004788},
author = {K.A. Gibb and A. Eldridge and C.J. Sandom and I.J.A. Simpson},
keywords = {Representation learning, Variational auto-encoder, Biodiversity monitoring, Deep learning, Ecoacoustics, Passive acoustic monitoring},
abstract = {Ecoacoustics is an emerging science that seeks to understand the role of sound in ecological processes. Passive acoustic monitoring is being used to collect vast quantities of soundscape audio recordings to study variations in acoustic community and monitor biodiversity. However, extracting relevant information from soundscape recordings is non-trivial. Recent approaches to machine-learned acoustic features appear promising but are limited by at least three issues: inductive biases, lack of interpretability and crude temporal integration. In this paper we introduce a novel self-supervised representation learning algorithm for ecoacoustics - a convolutional Variational Auto-encoder (VAE) - and directly address these shortcomings. Firstly, we train the network on soundscape recordings from temperate and tropical field sites along a gradient of ecological degradation to provide a more relevant inductive bias than prior approaches. Secondly, we present a new method that allows interpretation of the latent space for the first time, giving insight into the basis of classification. Thirdly, we advance existing methods for temporal aggregation of learned embeddings by encoding latent features as a distribution over time. Under our approach to increase interpretability, we provide insight into how learned features drive habitat classification for the first time: inspection of latent space confirms that varying combinations of biophony, geophony and anthrophony are used to infer sites along a degradation gradient. Our novel temporal encoding method increases sensitivity to periodic signals and improves on previous research that uses time-averaged representations for site classification. This approach also reveals the contribution of hardware-specific frequency response that create a potential bias; we demonstrate how a simple linear transformation can be used to mitigate the effect of hardware variance on the learned representation under our approach. Our novel approach paves the way for development of a new class of deep neural networks that afford more interpretable learned ecoacoustic representations to advance both fundamental and applied science and support global conservation efforts.}
}
@article{VILLON2024102499,
title = {Toward an artificial intelligence-assisted counting of sharks on baited video},
journal = {Ecological Informatics},
volume = {80},
pages = {102499},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102499},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000414},
author = {Sébastien Villon and Corina Iovan and Morgan Mangeas and Laurent Vigliola},
keywords = {Deep learning, Neural network, Coral reef, Marine ecology, Shark conservation},
abstract = {Given the global biodiversity crisis, there is an urgent need for new tools to monitor populations of endangered marine megafauna, like sharks. To this end, Baited Remote Underwater Video Stations (BRUVS) stand as the most effective tools for estimating shark abundance, measured using the MaxN metric. However, a bottleneck exists in manually computing MaxN from extensive BRUVS video data. Although artificial intelligence methods are capable of solving this problem, their effectiveness is tested using AI metrics such as the F-measure, rather than ecologically informative metrics employed by ecologists, such as MaxN. In this study, we present both an automated and a semi-automated deep learning approach designed to produce the MaxN abundance metric for three distinct reef shark species: the grey reef shark (Carcharhinus amblyrhynchos), the blacktip reef shark (C. melanopterus), and the whitetip reef shark (Triaenodon obesus). Our approach was applied to one-hour baited underwater videos recorded in New Caledonia (South Pacific). Our fully automated model achieved F-measures of 0.85, 0.43, and 0.72 for the respective three species. It also generated MaxN abundance values that showed a high correlation with manually derived data for C. amblyrhynchos (R = 0.88). For the two other species, correlations were significant but weak (R = 0.35–0.44). Our semi-automated method significantly enhanced F-measures to 0.97, 0.86, and 0.82, resulting in high-quality MaxN abundance estimations while drastically reducing the video processing time. To our knowledge, we are the first to estimate MaxN with a deep-learning approach. In our discussion, we explore the implications of this novel tool and underscore its potential to produce innovative metrics for estimating fish abundance in videos, thereby addressing current limitations and paving the way for comprehensive ecological assessments.}
}
@article{LEVY2024102737,
title = {Improving deep learning based bluespotted ribbontail ray (Taeniura Lymma) recognition},
journal = {Ecological Informatics},
volume = {82},
pages = {102737},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102737},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002796},
author = {Avivit Levy and Adi Barash and Chen Zaguri and Ariel Hadad and Polina Polsky},
keywords = {Computer vision, Citizen science, Ecological study, Marine animal recognition, Pose handling},
abstract = {This paper presents the novel task of bluespotted ribbontail (BR) ray (Taeniura lymma) recognition using deep learning based computer vision methods to enable the identification of specific individuals of this species. Mapping the specific individuals in relation to location and time will allow marine researchers to understand their movement patterns, habitat choice, life span, size of the population and more – data which could allow monitoring and establishing a tailor-made conservation plan for this species. Our work is pioneer on this recognition problem. We give a detailed description of the three basic steps of detection, feature extraction and recognition in this vision problem and perform experiments to explore the system configuration and what improves the performance. A feature extraction enhancement as well as a crucial effect of a split into different main poses are demonstrated. Though the precision results achieved in this paper are still moderate and should be further improved, they are nevertheless promising and reasonable for practical use if the six best matches are chosen. For this scenario, almost 85% precision for upper-pose model, and almost 80% precision for left- and right-pose models, are achieved demonstrating the feasibility of the pipeline suggested as well as opportunities for improvement.}
}
@article{RANKIN2024102511,
title = {Open-source machine learning BANTER acoustic classification of beaked whale echolocation pulses},
journal = {Ecological Informatics},
volume = {80},
pages = {102511},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102511},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000530},
author = {Shannon Rankin and Taiki Sakai and Frederick I. Archer and Jay Barlow and Danielle Cholewiak and Annamaria I. DeAngelis and Jennifer L.K. McCullough and Erin M. Oleson and Anne E. Simonis and Melissa S. Soldevilla and Jennifer S. Trickey},
keywords = {Bioacoustics, Machine learning, Random forest, Species classification, Passive acoustic monitoring, Beaked whale},
abstract = {Passive acoustic monitoring is increasingly used for assessing populations of marine mammals; however, analysis of large datasets is limited by our ability to easily classify sounds detected. Classification of beaked whale acoustic events, in particular, requires evaluation of multiple lines of evidence by expert analysts. Here we present a highly automated approach to acoustic detection and classification using supervised machine learning and open source software methods. Data from four large scale surveys of beaked whales (northwestern North Atlantic, southwestern North Atlantic, Hawaii, and eastern North Pacific) were analyzed using PAMGuard (acoustic detection), PAMpal (acoustic analysis) and BANTER (hierarchical random forest classifier). Overall classification accuracy ranged from 88% for the southwestern North Atlantic data to 97% for the northwestern North Atlantic. Results for many species could likely be improved with increased sample sizes, consideration of alternative automated detectors, and addition of relevant environmental features. These methods provide a highly automated approach to acoustic detection and classification using open source methods that can be readily adopted for other species and geographic regions.}
}
@article{VEGA2024102535,
title = {Convolutional neural networks for hydrothermal vents substratum classification: An introspective study},
journal = {Ecological Informatics},
volume = {80},
pages = {102535},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102535},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000773},
author = {Pedro Juan Soto Vega and Panagiotis Papadakis and Marjolaine Matabos and Loïc {Van Audenhaege} and Annah Ramiere and Jozée Sarrazin and Gilson Alexandre Ostwald Pedro {da Costa}},
keywords = {Image classification, Deep learning, Hydrothermal vents, Uncertainty analysis},
abstract = {The increasing availability of seabed images has created new opportunities and challenges for monitoring and better understanding the spatial distribution of fauna and substrata. To date, however, deep-sea substratum classification relies mostly on visual interpretation, which is costly, time-consuming, and prone to human bias or error. Motivated by the success of convolutional neural networks in learning semantically rich representations directly from images, this work investigates the application of state-of-the-art network architectures, originally employed in the classification of non-seabed images, for the task of hydrothermal vent substrata image classification. In assessing their potential, we conduct a study on the generalization, complementarity and human interpretability aspects of those architectures. Specifically, we independently trained deep learning models with the selected architectures using images obtained from three distinct sites within the Lucky-Strike vent field and assessed the models' performances on-site as well as off-site. To investigate complementarity, we evaluated a classification decision committee (CDC) built as an ensemble of networks in which individual predictions were fused through a majority voting scheme. The experimental results demonstrated the suitability of the deep learning models for deep-sea substratum classification, attaining accuracies reaching up to 80% in terms of F1-score. Finally, by further investigating the classification uncertainty computed from the set of individual predictions of the CDC, we describe a semiautomatic framework for human annotation, which prescribes visual inspection of only the images with high uncertainty. Overall, the results demonstrated that high accuracy values of over 90% F1-score can be obtained with the framework, with a small amount of human intervention.}
}
@article{JAMES2024102580,
title = {Monitoring vegetation patterns and their drivers to infer resilience: Automated detection of vegetation and megaherbivores from drone imagery using deep learning},
journal = {Ecological Informatics},
volume = {81},
pages = {102580},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102580},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001225},
author = {Rebecca K. James and Freek Daniels and Aneesh Chauhan and Pramaditya Wicaksono and Muhammad Hafizt and Setiawan Djody Harahap and Marjolijn J.A. Christianen},
keywords = {Semantic segmentation, Object detection, Pytorch, Seagrass, Drone imagery, Turtle monitoring, Conservation},
abstract = {Ecological pattern theory has highlighted spatial vegetation patterns that can be used as indicators of ecosystem resilience. Combining this spatial pattern theory with aerial imagery from drones and automated image processing with deep learning methods, we show how monitoring of natural ecosystems can be enhanced through quantifying vegetation spatial patterns. We demonstrate this approach in a tropical seagrass ecosystem with a high abundance of turtles that generate vegetation patches when grazing. Past field observations suggest that patch size and density reflect the seagrass meadow resilience, but understanding the natural variation in vegetation patchiness is crucial. Employing the deep learning methods of semantic segmentation and object detection, we quantify vegetation patchiness metrics and turtle distribution across 12 ha of seagrass meadow in the years 2012 and 2022. The resulting output facilitates spatial and temporal comparisons, revealing areas of low resilience. In 2012, turtle grazing across the entire site yielded vegetation patch sizes averaging 2 ± 0.2 m2 (95% confidence interval). Reduced patch sizes of 0.24 ± 0.05 m2 and 0.67 ± 0.6 m2 at the reef edge and beach slope respectively, in conjunction with a reduced patch density, indicated lower resilience at the seagrass meadow edges. Analysis of the 2022 dataset indicates a general decrease in patch size over time, suggesting declining resilience. A retraining experiment of the semantic segmentation model was conducted where the initial model was retrained on the 2022 dataset and demonstrated the adaptability of the deep learning methods. Despite using different equipment, the model achieved high accuracy with only 5–10 additional training images. By providing the tools to conduct these analyses, we aim to stimulate the uptake of deep learning for enhancing the data obtained from aerial imagery to improve the monitoring and conservation of natural ecosystems.}
}
@article{LIN2024102507,
title = {A model for forest type identification and forest regeneration monitoring based on deep learning and hyperspectral imagery},
journal = {Ecological Informatics},
volume = {80},
pages = {102507},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102507},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000499},
author = {Feng-Cheng Lin and Yi-Shiang Shiu and Pei-Jung Wang and Uen-Hao Wang and Jhe-Syuan Lai and Yung-Chung Chuang},
keywords = {Remote sensing, Deep learning, VGG19, ResNet50, Hyperspectral images},
abstract = {Traditional ground-based forest survey methods involve high labor costs, and their inefficiency makes comprehensive forest resource surveys challenging. With the development of new sensors and vehicles in recent years, more diverse and novel remote sensing detection and survey techniques have emerged. This study aims to use hyperspectral imagery to classify forest types containing representative tree species. To verify the feasibility of the proposed methods, we used hyperspectral imagery from the Taiwan Forestry Experiment Institute's Liugui Research Forest in southern Taiwan, which has an area of 9882 ha and an altitude of 250–2600 m. Hyperspectral imagery offers several advantages compared to traditional multispectral imagery; it captures a broad spectrum of contiguous, narrow spectral bands, providing highly detailed spectral information, enabling differentiation of tree components that appear similar in multispectral imagery. Eight identifiable forest types were selected for the models considered, and three different deep learning algorithms, VGG19, ResNet50 and a proposed combination (VGG19 + ResNet50), were used to screen the best algorithms. Data formats and pre-processing methods that can effectively improve computational performance were explored. The research results found that: (1) Band filtering is a necessary means to improve calculation performance; (2) Flattening the original convolution kernel with cubic characteristics can significantly reduce the time required for calculation. In terms of simulation results, VGG19 + ResNet50 was identified as the best model. Its overall classification accuracy can generally reach 93% to 100%. According to the calculation process set in this study, the time required for model training can be shortened to less than 30 min. The results of this research will help process more detailed and complex information in forest resource management and more accurately quantify forest ecology and woodland conditions.}
}
@article{JOELIANTO2024102495,
title = {Convolutional neural network-based real-time mosquito genus identification using wingbeat frequency: A binary and multiclass classification approach},
journal = {Ecological Informatics},
volume = {80},
pages = {102495},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102495},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000372},
author = {Endra Joelianto and Miranti Indar Mandasari and Daniel Beltsazar Marpaung and Naufal Dzaki Hafizhan and Teddy Heryono and Maria Ekawati Prasetyo and  Dani and Susy Tjahjani and Tjandra Anggraeni and Intan Ahmad},
keywords = {Dengue infection, , Mosquito vector, Wingbeat frequency, Deep learning, Sustainability monitoring},
abstract = {Global rises in dengue hemorrhagic fever, especially in Asia and Latin America, underscore the necessity for enhanced public health interventions. Aedes spp. mosquitoes are the primary vectors; however, species such as Culex quinquefasciatus pose significant health risks by transmitting diseases such as filariasis, impacting millions of people worldwide. This study introduces a real-time convolutional neural network-based mosquito classification system using wingbeat frequency for identifying various mosquito species, with emphasis on Aedes sp. We proposed and assessed two models: a binary classification and a multiclass system. The binary system exhibited an outstanding accuracy of 91.76% in distinguishing between Aedes aegypti and Culex quinquefasciatus. The multiclass system accurately identified female and male Aedes aegypti and Culex quinquefasciatus with a precision of 87.16%. This innovative approach serves as a potential tool for dengue infection control and a versatile instrument for combating various mosquito-borne illnesses, enhancing vector surveillance for comprehensive disease management.}
}
@article{ZHENG2024102689,
title = {A video object segmentation-based fish individual recognition method for underwater complex environments},
journal = {Ecological Informatics},
volume = {82},
pages = {102689},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102689},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002310},
author = {Tao Zheng and Junfeng Wu and Han Kong and Haiyan Zhao and Boyu Qu and Liang Liu and Hong Yu and Chunyu Zhou},
keywords = {Individual fish recognition, Video object segmentation, Underwater complex environments, Deep learning, Intelligent aquaculture},
abstract = {Currently, aquaculture methods tend to combine scale and intelligence, which saves manpower and improves the survival rate of seafood at the same time. High-precision and high-efficiency fish individual recognition can provide key technical support for fish disease detection, feeding habits, body condition, etc. In the realm of intelligent aquaculture, it provides robust data support for precision fish farming. However, the current research methods for individual fish recognition struggle to maintain the network model's focus on the fish body in real marine underwater complex environments (e.g., environmental background interference such as coral reefs, overlap between fish bodies, light noise, etc.), leading to unsatisfactory recognition results. To this end, this paper proposes a method for fish individual recognition in underwater complex environments based on video object segmentation, which consists of three parts, including a fish individual segmentation detection module, a fish individual recognition module, and an all-in-one visualization module. The work adopts a combination of deep learning methods and video object segmentation algorithms to solve the problem of low attention and poor detection accuracy of fish individuals in real underwater complex environments, which effectively improves the accuracy and efficiency of fish individual recognition, and analyzes and discusses the comparison of recognition effects using different weights. The results of the simulation experiments show that the key metric Rank1 value of the method achieves more than 96% accuracy on the public datasets DlouFish, WideFish, and the Fish-seg dataset produced in this paper, and improves over the state-of-the-art methods for fish individual recognition by 2.23%, 1.33%, and 1.25%, respectively.}
}
@article{ZHANG2024102467,
title = {Marine zoobenthos recognition algorithm based on improved lightweight YOLOv5},
journal = {Ecological Informatics},
volume = {80},
pages = {102467},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102467},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000098},
author = {Lijun Zhang and Jiawen Fan and Yi Qiu and Zhe Jiang and Qingsong Hu and Bowen Xing and Jingxiang Xu},
keywords = {Ecological monitoring, Marine zoobenthos, Model lightweight, YOLOv5, EfficientnetV2, Bottleneck transformer},
abstract = {Detecting the distribution and density of marine zoobenthos is crucial for monitoring healthy coastal ecosystems and for growth reference tracking in precision aquaculture. However, current detection algorithms for marine zoobenthos have high computational complexity and cannot guarantee a balance between accuracy and speed, limiting their deployment in fishery equipment. This study used a portion of the Augmented Underwater Detection Dataset, a large underwater biological dataset containing marine zoobenthos data. A marine zoobenthos recognition algorithm was proposed for sea cucumbers, sea urchins, and scallops based on an improved lightweight YOLOv5, which can recognize the three types of marine zoobenthos. In the image enhancement module, an underwater image enhancement algorithm based on color balance and multi-input fusion is used, which turns the blurred image into a natural appearance of the seabed image. The lightweight backbone network EfficientnetV2-S was chosen to replace the original YOLOv5 backbone network, reducing network parameter calculations and improving recognition speed. A Bottleneck Transformer was introduced into the backbone network, and an attention mechanism based on the convolution module was introduced to construct the embedded Convolutional Block Attention Module in the Neck structure of YOLOv5, thereby improving the recognition accuracy of the lightweight YOLOv5 model. The experimental results showed that the mAP of the proposed algorithm reached 0.941, which is an improvement of 0.002 compared with the original YOLOv5l algorithm. The computation of this algorithm is 37.0 FLOPs (G), the model size is 54 MB, and the inference time is 5.9 ms. Compared to the original YOLOv5l algorithm, the reductions are 66.1%, 40.5%, and 39.2%. The proposed algorithm efficiently identified and classified marine zoobenthos.}
}
@article{LIU2024102401,
title = {YWnet: A convolutional block attention-based fusion deep learning method for complex underwater small target detection},
journal = {Ecological Informatics},
volume = {79},
pages = {102401},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102401},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004302},
author = {Pingzhu Liu and Wenbin Qian and Yinglong Wang},
keywords = {Underwater target detection, Deep learning, Feature fusion, Attention mechanism},
abstract = {Underwater target detection holds a noteworthy role in the field of marine exploration. However, it is difficult to extract useful feature information from blurred images with complex backgrounds, resulting in suboptimal and unsatisfactory target detection in conventional models. Among them, YOLOv5 leverages the advantages of fast detection performs better in detecting underwater samples. Nevertheless, YOLOv5 still faces difficulties including missed and incorrect detections due to the underwater environment's small scale of objects, dense distribution of organisms, and occlusion. To address these challenges, we propose a novel YoLoWaternet (YWnet) model that builds upon the YOLOv5 framework for complex underwater species detection with three main innovations: 1) A convolutional block attention module (CBAM) is introduced to enhance feature extraction for blurry images in the initial stages of the network and a new feature fusion network called the CRFPN is created to transfer important information and detect underwater targets. 2) A novel feature extraction module is presented, namely, the skip residual C3 module (SRC3), by effectively merging information from various scales to minimize the loss of original data during transmission. 3) Regression and classification algorithms are separated using the decoupled head to improve the effectiveness of detection and the EIoU loss function is employed to accelerate the convergence speed. Finally, the experimental results demonstrate that YWnet achieves remarkable accuracies of 73.2% mAp and 39.3% mAp50–95 on the underwater dataset, surpassing YOLOv5 by 2.3% and 2.4%, respectively. Furthermore, the proposed fusion model outperforms nine state-of-the-art baseline models on the undersea dataset and has generalization capabilities in other datasets.}
}
@article{LEKUNBERRI2022101495,
title = {Identification and measurement of tropical tuna species in purse seiner catches using computer vision and deep learning},
journal = {Ecological Informatics},
volume = {67},
pages = {101495},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101495},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121002867},
author = {Xabier Lekunberri and Jon Ruiz and Iñaki Quincoces and Fadi Dornaika and Ignacio Arganda-Carreras and Jose A. Fernandes},
keywords = {Computer vision, Deep learning, Fisheries, Electronic monitoring, Tropical tuna species identification},
abstract = {Fishery monitoring programs are essential for effective management of marine resources, as they provide scientists and managers with the necessary data for both the preparation of scientific advice and fisheries control and surveillance. The monitoring is generally done by human observers, both in port and onboard, with a high cost involved. Consequently, some Regional Fisheries Management Organizations (RFMO) are opting for electronic monitoring (EM) as an alternative or complement to human observers in certain fisheries. This is the case of the tropical tuna purse seine fishery operating in the Indian and Atlantic oceans, which started an EM program on a voluntary basis in 2017. However, even when the monitoring is conducted though EM, the image analysis is a tedious task manually performed by experts. In this paper, we propose a cost-effective methodology for the automatic processing of the images already being collected by cameras onboard tropical tuna purse seiners. Firstly, the images are preprocessed to homogenize them across all vessels and facilitate subsequent steps. Secondly, the fish are individually segmented using a deep neural network (Mask R-CNN). Then, all segments are passed through other deep neural network (ResNet50V2) to classify them by species and estimate their size distribution. For the classification of fish, we achieved an accuracy for all species of over 70%, i.e., about 3 out of 4 individuals are correctly classified to their corresponding species. The size distribution estimates are aligned with official port measurements but calculated using a larger number of individuals. Finally, we also propose improvements to the current image capture systems which can facilitate the work of the proposed automation methodology.}
}
@article{NAZIR2024102453,
title = {Object classification and visualization with edge artificial intelligence for a customized camera trap platform},
journal = {Ecological Informatics},
volume = {79},
pages = {102453},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102453},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300482X},
author = {Sajid Nazir and Mohammad Kaleem},
keywords = {Data science, Computer vision, Deep learning, Model generalization, Fine tuning, Explainable AI, Hyperparameter tuning, Vision transformers},
abstract = {The camera traps have revolutionized the image and video capture in ecology and are often used to monitor and record animal presence. With miniaturization of low power electronic devices, better battery technologies, and software advancements, it has become possible to use the edge devices, such as Raspberry Pi as camera traps that can not only capture images and videos, but can also enable sophisticated image processing, and off-site communications. These developments can help to provide near real-time insights and reduce the manual processing of images. The on-board image classification and visualization is facilitated by the advancements in the Deep Neural Networks (DNN), transfer learning approaches, and software libraries. This paper provides an investigation of image classification with transfer learning approaches using pre-trained DNN models, and visualizations with Explainable Artificial Intelligence (XAI) techniques on Raspberry Pi Zero (RPi-Z) edge device. The MobileNetV2 model was used for image classification on the Florida-Part1 dataset obtaining the results for precision, recall, and F1-score as 0.95, 0.96, and 0.95 respectively. We also compared the model performance of MobileNetV2, EfficientNetV2B0, and MobileViT models for classification on the Extinction dataset with the best results for precision, recall, and F1-score as 0.97, 0.96, and 0.96 respectively, obtained with the EfficientNetV2B0 model. Two XAI techniques, Gradient-weighted-Class Activation Mapping (Grad-CAM) and Occlusion Sensitivity were used for visualization through heatmaps, to highlight the relative importance of the image areas contributing to the DNN model's prediction, that can also help to understand the model's performance and bias. The results provide practical use case scenarios for utilizing the transfer learning approaches, model optimization and deployment to edge devices, and model visualizations in ecological research.}
}
@article{SIGURDARDOTTIR2023102046,
title = {Otolith age determination with a simple computer vision based few-shot learning method},
journal = {Ecological Informatics},
volume = {76},
pages = {102046},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102046},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000754},
author = {Andrea Rakel Sigurðardóttir and Þór Sverrisson and Aðalbjörg Jónsdóttir and María Gudjónsdóttir and Bjarki Þór Elvarsson and Hafsteinn Einarsson},
keywords = {Otoliths, Fish age estimation, Few-shot learning, Deep-learning, Image analysis},
abstract = {In this study, we propose a computer vision-based few-shot learning method for otolith age determination in European plaice, Atlantic cod, Greenland halibut, and haddock. Our method outperforms prior state-of-the-art approaches, and is based on a vision encoder from CLIP as a feature extractor, which is used to train shallow models. The method is computationally efficient, as it does not require fine-tuning of deep networks, and is also data efficient, as it performs better than fine-tuning on the same data. Our results suggest that in some cases, our method can achieve the same performance as state-of-the-art finetuning approaches with up to three times less training data.}
}
@article{DENG2024102546,
title = {Weed database development: An updated survey of public weed datasets and cross-season weed detection adaptation},
journal = {Ecological Informatics},
volume = {81},
pages = {102546},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102546},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000888},
author = {Boyang Deng and Yuzhen Lu and Jiajun Xu},
keywords = {Deep learning, Domain adaptation, Machine vision, Precision agriculture, Robustness, Weed detection},
abstract = {Weeds are a major threat to crop production. Automated innovations for reducing herbicides and labor needed for weeding have become a high priority for sustainable weed management. The current state-of-the-art weeding systems still cannot reliably recognize weeds in changing field conditions for precision weed control. Enhancing weed recognition accentuates the critical need to develop dedicated, labeled weed databases and whereby train advanced AI (artificial intelligence) models while ensuring the robustness of models across diverse field conditions. This study presents an up-to-date survey on publicly available image datasets for weed recognition. Among 36 datasets identified, limitations exist in terms of data variations and distribution shifts, and few of the datasets are suitable for examining the robustness of weed recognition models. A new two-season, eight-class weed dataset is described in this study, comprising two sub-datasets of images collected in the seasons of 2021 and 2022, respectively. Three state-of-the-art deep learning object detectors, i.e., YOLOX, YOLOv8, and DINO, were benchmarked and evaluated for their in-season and cross-season weed detection performance on the dataset. All three models attained in-season detection accuracies of 92% and higher in terms of mAP@50. However substantial accuracy drops of up to 14.5% were observed between in-season and cross-season testing, especially for YOLOX and YOLOv8. Unsupervised domain adaptation based on an implicit instance-invariant network (I3Net) was utilized for improved generalization of the YOLO models. The I3Net-based models resulted in accuracy improvements of 1.4% and 3.3% for YOLOX and YOLOv8, respectively, compared to modeling without domain adaptation, in the cross-season testing. Both the two-season detection dataset11https://doi.org/10.5281/zenodo.10762138 and software programs22https://github.com/vicdxxx/CrossSeasonWeedDetection for weed detection modeling in this study are made publicly available.}
}
@article{BRAVODIAZ2024102684,
title = {Evaluating the ability of convolutional neural networks for transfer learning in Pinus radiata cover predictions},
journal = {Ecological Informatics},
volume = {82},
pages = {102684},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102684},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002267},
author = {A. Bravo-Diaz and S. Moreno and J. Lopatin},
keywords = {Invasive species, Unpiloted aerial vehicles (UAV), Spatial variability, Regression, Transfer domain},
abstract = {The species Pinus radiata is highly invasive in native forests in Chile, drastically affecting the functioning and structure of ecosystems. Hence, it is imperative to develop robust approaches to detect P. radiata invasions at different scales. Models based on convolutional neural networks (CNN) have proven to be a promising alternative to detect plant invasions in high-resolution remote sensing data, such as those obtained by drones. However, studies have been limited in their spatial variability and their assessments of transferability or transfer learning to new sectors, hindering the ability to use these models in a real-world setting. We train models based on CNN architectures using unpiloted aerial vehicle data and evaluate their ability to transfer learning outside the training domain using regression approaches. We compared models trained with low spatial variability (mono-site) with those with high spatial variability (multi-site). We further sought to maximize the transference of learning outside the training domain by searching among different architectures and models, maximizing the evaluation in an independent data set. The results showed that transfer learning is better when multi-site models with higher spatial variability are used for training, obtaining a coefficient of determination R2 between 60% and 87%. On the contrary, mono-site models present a wide variability of performance attributed to the dissimilarity of information between sites, limiting the possibilities of using these models for extrapolations or model generalizations. We also obtained a significant difference between within-domain generalization using test data versus transfer learning outside the training domain, showing that testing data alone cannot depict such discrepancy without further data. Finally, the best models for transfer learning on new data domains often do not agree with those selected by the standard training/validation/testing scheme. Our findings pave the way for deeper discussions and further investigations into the limitations of CNN models when applied to high-resolution imagery.}
}
@article{BAKHT2024102631,
title = {MuLA-GAN: Multi-Level Attention GAN for Enhanced Underwater Visibility 11⁎First two authors has equal contribution⋆github code link:https://github.com/AhsanBaidar/MuLAGAN.git ORCID (s): 0000–0002–9079-0960 (A.B. Bakht); 0000–0001–9789-8483 (Z. Jia); 0000–0001–6214-1077 (M.U. Din); 0000–0002–7401-5120 (W. Akram); 0000–0003–4445-3135 (L.S. Saoud); 0000–0001–6405-8402 (L. Seneviratne); 0000–0001–6432-5187 (S. He); 0000–0003–2759-0306 (I. Hussain)},
journal = {Ecological Informatics},
volume = {81},
pages = {102631},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102631},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001730},
author = {Ahsan B. Bakht and Zikai Jia and Muhayy Ud Din and Waseem Akram and Lyes Saad Saoud and Lakmal Seneviratne and Defu Lin and Shaoming He and Irfan Hussain},
keywords = {Underwater image enhancement, Generative adversarial networks (GANs), Spatio-channel attention, Computer vision, Real-time image processing},
abstract = {The underwater environment presents unique challenges (color distortions, reduced contrast, blurriness) hindering accurate analysis. This work introduces MuLA-GAN, a novel approach leveraging Generative Adversarial Networks (GANs) and specifically adapted Multi-Level Attention for comprehensive underwater image enhancement. MuLA-GAN integrates Multi-Level Attention within the GAN architecture to prioritize learning discriminative features crucial for precise image restoration. These relevant features encompass information on local details within image regions leveraged by spatial attention and features at various scales across the entire image captured by multi-level attention. This allows MuLA-GAN to identify and enhance objects, textures, and edges obscured by underwater distortions while also reconstructing a more accurate and visually clear representation of the underwater scene by analyzing low-level information like edges and textures, as well as high-level information like object shapes and global scene information. By selectively focusing on these relevant features, MuLA-GAN excels at capturing and preserving intricate details in underwater imagery, which is essential for various marine research, exploration, and resource management applications. Extensive evaluations on diverse datasets (UIEB test, UIEB challenge, U45, UCCS) demonstrate MuLA-GAN's superior performance compared to existing methods. Additionally, a specialized bio-fouling and aquaculture dataset confirms the model's robustness in challenging environments. On the UIEB test dataset, MuLA-GAN achieves exceptional Peak Signal-to-Noise Ratio (PSNR) (25.59) and Structural Similarity Index (SSIM) (0.893) scores, surpassing Water-Net (24.36 PSNR, 0.885 SSIM). This work addresses a significant research gap in underwater image enhancement by demonstrating the effectiveness of combining GANs with specifically adapted Multi-Level Attention mechanisms. This tailored approach offers a novel and comprehensive framework for restoring underwater image quality, providing valuable insights for accurate underwater scene analysis. The source code for MuLA-GAN is publicly available on GitHub at https://github.com/AhsanBaidar/MuLA_GAN.git}
}
@article{SONG2024102466,
title = {Benchmarking wild bird detection in complex forest scenes},
journal = {Ecological Informatics},
volume = {80},
pages = {102466},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102466},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000086},
author = {Qi Song and Yu Guan and Xi Guo and Xinhui Guo and Yufeng Chen and Hongfang Wang and Jianping Ge and Tianming Wang and Lei Bao},
keywords = {Object detection, Bird detection, Deep learning, Camera trap},
abstract = {Camera traps are widely used for wildlife monitoring and making informed conservation and land-management decisions, but the resulting ‘big data’ are laborious to process. Deep learning-based methods have been adopted for wildlife detection in camera traps. However, these methods detect large mammals in uncomplicated scenes, where powerful deep-learning models work effectively. Few studies have been conducted to develop artificial intelligence for recognizing wild birds that live in complicated field scenes with protective colors and small sizes. Here we used a dataset of 9717 images from 15 bird species based on camera traps to test 8 object detection algorithms (Faster RCNN, Cascade RCNN, RetinaNet, FCOS, RepPoints, ATSS, Deformable-DETR, and Sparse RCNN) and assess their performance. We also explored the effect of different backbones on model accuracy. Among them, the Cascade RCNN model performs best, with a mAP of 0.693 in model capabilities. Models perform differently in certain species, and backbones significantly affect the accuracy of the model. Cascade RCNN utilizing the Swin-T backbone is the best-performing combination, with a mAP of 0.704. This study could help researchers identify birds efficiently and inspires research on wildlife recognition in complex ecological settings.}
}
@article{CHEN2024102660,
title = {Detecting sun glint in UAV RGB images at different times using a deep learning algorithm},
journal = {Ecological Informatics},
volume = {81},
pages = {102660},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102660},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002024},
author = {Jiahao Chen and Yi Xiao and Yahui Guo and Mingwei Li and Xiran Li and Xuan Zhang and Fanghua Hao and Xiao Pu and Yongshuo Fu},
keywords = {Unmanned aerial vehicle, Water quality monitoring, Sun glint, Convolutional neural network, Res_AUNet network, WSGD dataset},
abstract = {Unmanned aerial vehicle (UAV) remote sensing has played a crucial role in water quality monitoring. However, the presence of water sun glint, resulting from specular reflection on the water surface, poses an inevitable challenge in UAV-acquired images. These excessively bright pixels disrupt the original images' spectral and textural characteristics, significantly diminishing their usability. This disruption has repercussions on subsequent tasks, such as target object classification and water quality parameter inversion. Precise detection of sun glint is a prerequisite for removing them, but current methods suffer from missed and false detections. In this study, we collected images by UAV to construct a specialized dataset for water surface sun glint, namely water sun glint detection (WSGD) dataset, laying the groundwork for further research endeavors. We proposed the Res_AUNet network by enhancing the UNet convolutional neural network. The Convolutional Block Attention Module was integrated into the encoding-decoding skip connections of the network, we also refined the convolutional blocks to better capture the distinctive semantic features associated with water sun glint. To mitigate overfitting, the residual structures were incorporated and the number of convolutional kernels within each block was reduced. The Res_AUNet network was trained and evaluated using the WSGD dataset, achieving metrics with an Accuracy of 98.02%, an F1-score of 83.67%, and an IOU of 74.73%. These results underscore the precision of our proposed method for water sun glint detection in UAV water images, offering valuable insights for effectively eliminating water sun glint and determining the optimal timing for UAV water image acquisition.}
}
@article{HARTMANN2022101782,
title = {A text and image analysis workflow using citizen science data to extract relevant social media records: Combining red kite observations from Flickr, eBird and iNaturalist},
journal = {Ecological Informatics},
volume = {71},
pages = {101782},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101782},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002321},
author = {Maximilian C. Hartmann and Moritz Schott and Alishiba Dsouza and Yannick Metz and Michele Volpi and Ross S. Purves},
keywords = {User-generated content, Volunteered geographic information, Data integration, Image content analysis, Convolutional neural networks},
abstract = {There is an urgent need to develop new methods to monitor the state of the environment. One potential approach is to use new data sources, such as User-Generated Content, to augment existing approaches. However, to date, studies typically focus on a single date source and modality. We take a new approach, using citizen science records recording sightings of red kites (Milvus milvus) to train and validate a Convolutional Neural Network (CNN) capable of identifying images containing red kites. This CNN is integrated in a sequential workflow which also uses an off-the-shelf bird classifier and text metadata to retrieve observations of red kites in the Chilterns, England. Our workflow reduces an initial set of more than 600,000 images to just 3065 candidate images. Manual inspection of these images shows that our approach has a precision of 0.658. A workflow using only text identifies 14% less images than that including image content analysis, and by combining image and text classifiers we achieve almost perfect precision of 0.992. Images retrieved from social media records complement those recorded by citizen scientists spatially and temporally, and our workflow is sufficiently generic that it can easily be transferred to other species.}
}
@article{BHAGABATI2024102398,
title = {An automated approach for human-animal conflict minimisation in Assam and protection of wildlife around the Kaziranga National Park using YOLO and SENet Attention Framework},
journal = {Ecological Informatics},
volume = {79},
pages = {102398},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102398},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004272},
author = {Bijuphukan Bhagabati and Kandarpa Kumar Sarma and Kanak Chandra Bora},
keywords = {Computer vision, Object detection, Animal detection, Human-animal conflict, Kaziranga, Deep learning, Yolo},
abstract = {Human-animal conflict in Assam, India's north-eastern state, is rising continuously. Because it occurs year-round, it damages agricultural productivity and kills people and animals, including elephants. When a herd of wild elephants emerges from a deep forest and enters human-inhabited territory around the Kaziranga National Park (KNP) in Assam, an alert must be sounded for the neighbourhood residents and forest workers to prevent conflicts. Another concern is that many wild animals die near the KNP while crossing the national highway NH-37 which traverses the area. During floods, animals flee to the highlands for food and shelter. An automated animal identification and warning system near the KNP may reduce human-animal confrontations. This paper reports the design of a system that attempts to address the above concerns. Artificial Intelligence (AI)-based strategies are utilized to recognize wild animals from live video sequences, provide warnings to avoid encounters, and protect humans and animals. Deep learning models and YoloV5 with the SENet attention layer are used to recognize wild animals in real-time. This model is trained using a public and customized dataset of animal species. Cameras attached to the cloud-based AI system take photographs from several KNP locations to confirm the model. The model's 96% accuracy in animal photographs and videos taken day and night and in feed from contemporaneous location has shown its utility. The model also improves reliability by 1–13% over previous methods.}
}
@article{WU2022101534,
title = {SILIC: A cross database framework for automatically extracting robust biodiversity information from soundscape recordings based on object detection and a tiny training dataset},
journal = {Ecological Informatics},
volume = {68},
pages = {101534},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101534},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121003253},
author = {Shih-Hung Wu and Hsueh-Wen Chang and Ruey-Shing Lin and Mao-Ning Tuanmu},
keywords = {Sound Identification and Labeling Intelligence for Creatures, Automated wildlife sound identification, Passive acoustic monitoring, Autonomous recording unit, Object detection}
}
@article{NOMAN2023102047,
title = {Improving accuracy and efficiency in seagrass detection using state-of-the-art AI techniques},
journal = {Ecological Informatics},
volume = {76},
pages = {102047},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102047},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000766},
author = {Md Kislu Noman and Syed Mohammed Shamsul Islam and Jumana Abu-Khalaf and Seyed Mohammad Jafar Jalali and Paul Lavery},
keywords = {Deep learning, EfficientDet, Faster R-CNN, , NASNet, Seagrass, YOLOv5},
abstract = {Seagrasses provide a wide range of ecosystem services in coastal marine environments. Despite their ecological and economic importance, these species are declining because of human impact. This decline has driven the need for monitoring and mapping to estimate the overall health and dynamics of seagrasses in coastal environments, often based on underwater images. However, seagrass detection from underwater digital images is not a trivial task; it requires taxonomic expertise and is time-consuming and expensive. Recently automatic approaches based on deep learning have revolutionised object detection performance in many computer vision applications, and there has been interest in applying this to automated seagrass detection from imagery. Deep learning–based techniques reduce the need for hardcore feature extraction by domain experts which is required in machine learning-based techniques. This study presents a YOLOv5-based one-stage detector and an EfficientDetD7–based two-stage detector for detecting seagrass, in this case, Halophila ovalis, one of the most widely distributed seagrass species. The EfficientDet-D7–based seagrass detector achieves the highest mAP of 0.484 on the ECUHO-2 dataset and mAP of 0.354 on the ECUHO-1 dataset, which are about 7% and 5% better than the state-of-the-art Halophila ovalis detection performance on those datasets, respectively. The proposed YOLOv5-based detector achieves an average inference time of 0.077 s and 0.043 s respectively which are much lower than the state-of-the-art approach on the same datasets.}
}
@article{KORSCHENS2024102516,
title = {Determining the community composition of herbaceous species from images using convolutional neural networks},
journal = {Ecological Informatics},
volume = {80},
pages = {102516},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102516},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400058X},
author = {Matthias Körschens and Solveig Franziska Bucher and Paul Bodesheim and Josephine Ulrich and Joachim Denzler and Christine Römermann},
keywords = {Plant biodiversity, Plant cover, Deep learning, Convolutional neural networks, Semantic segmentation, Artificial intelligence},
abstract = {Global change has a detrimental impact on the environment and changes biodiversity patterns, which can be observed, among others, via analyzing changes in the composition of plant communities. Typically, vegetation relevées are done manually, which is time-consuming, laborious, and subjective. Applying an automatic system for such an analysis that can also identify co-occurring species would be beneficial as it is fast, effortless to use, and consistent. Here, we introduce such a system based on Convolutional Neural Networks for automatically predicting the species-wise plant cover. The system is trained on freely available image data of herbaceous plant species from web sources and plant cover estimates done by experts. With a novel extension of our original approach, the system can even be applied directly to vegetation images without requiring such cover estimates. Our extended approach, not utilizing dedicated training data, performs similarly to humans concerning the relative species abundances in the vegetation relevées. When trained on dedicated training annotations, it reflects the original estimates more closely than (independent) human experts, who manually analyzed the same sites. Our method is, with little adaptation, usable in novel domains and could be used to analyze plant community dynamics and responses of different plant species to environmental changes.}
}
@article{TUCKER2024102431,
title = {Development of a non-invasive method for species and sex identification of rare forest carnivores using footprint identification technology},
journal = {Ecological Informatics},
volume = {79},
pages = {102431},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102431},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004600},
author = {Jody M. Tucker and Caleb King and Ryan Lekivetz and Remi Murdoch and Zoe C. Jewell and Sky K. Alibhai},
keywords = {Footprint, Track, Fisher, Marten, Wildlife biometrics, Non-invasive},
abstract = {Many wildlife species have sex specific habitat requirements. Due to the unique requirements for birthing and raising offspring, female reproductive habitat is often a limiting factor for a population and has been identified as a priority for conservation. Therefore, the ability to detect where females persist on the landscape and identify these potential reproductive areas is essential in creating effective conservation strategies. Here we describe the development of a non-invasive method to identify species and sex based on track images collected at track plate stations using footprint identification technology (FIT). We developed this technique using data from the southern Sierra Nevada fisher (Pekania pennanti) population and a co-occurring species of conservation concern the Pacific marten (Martes caurina). We coupled track plate footprints with non-invasive genetic samples and camera trap images to create a reference dataset of known species for fisher and marten and known sex for fisher. We used FIT to geo-reference 167 marten and 367 fisher tracks (34 males, 27 females) using 7 landmark points and then extracted 124 morphometric variables (distances, angles, and areas) for use in identifying species and sex for fisher using linear discriminant analyses. Using a single variable, we found species classification accuracy >99% in distinguishing fisher from marten. For fisher sex identification our most parsimonious model consisting of only 2 variables achieved an accuracy of 94.0% for the training set and 89.4% for the test set. We also report a method to quantify classification uncertainty for each track. This method provides a rapid, cost effective, entirely non-invasive method to accurately identify sex that can easily be implemented in field studies.}
}
@article{KEASAR2024102521,
title = {STARdbi: A pipeline and database for insect monitoring based on automated image analysis},
journal = {Ecological Informatics},
volume = {80},
pages = {102521},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102521},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000633},
author = {Tamar Keasar and Michael Yair and Daphna Gottlieb and Liraz Cabra-Leykin and Chen Keasar},
keywords = {Classification, High-throughput screening, Insect monitoring, Machine learning, Object detection, Sticky trap},
abstract = {Insects are highly abundant and diverse, and play major roles in ecosystem functions. Monitoring of insect populations is key to their sustainable management. However, the labor and expertise needed to identify insects, and the challenges of archiving the wealth of data collected in monitoring programs, often limit these efforts. We describe a pipeline to reduce the barriers associated with curating and mining big data of insect biodiversity. The pipeline, STARdbi, includes capturing flying insects with sticky traps, scanning the traps, storing the trap-images in a public database with a web-based interface, and applying machine learning models to extract information from the images. To illustrate the insights that can be gained from STARdbi, we describe two case studies. One of them involves monitoring of circadian activity patterns of grain pests and of their natural enemies, and the other compares insect abundance, biomass and size distributions between agricultural and semi-natural habitats. We invite the community of insect ecologists to contribute to the STARdbi database, and to use its image analysis tools to address diverse ecological and evolutionary questions.}
}
@article{PIZARRO2024102403,
title = {Spatial and temporal representation of marine fish occurrences available online},
journal = {Ecological Informatics},
volume = {79},
pages = {102403},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102403},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004326},
author = {Vanessa Pizarro and Andrea G. Castillo and Andrea Piñones and Horacio Samaniego},
keywords = {Ecoinformatics, Ecological information biases, Marine fish, Spatial and temporal representativeness, Species richness},
abstract = {Despite the 243,000 marine species described by 2022, our knowledge about the oceanic biodiversity is still incomplete. This knowledge gap carries potentially adverse and far-reaching consequences for the preservation of marine ecosystems, particularly in the context of the ongoing human-induced alterations to our biosphere and the rapid progression of climate change and global environmental shifts. Recently, however, a large number of online repositories have emerged, which catalogue, store and distribute biodiversity information, including taxonomic and species occurrence data. FishBase, the Global Biodiversity Information Facility (GBIF) and the Ocean Biodiversity Information System (OBIS) are part of these publicly available repositories representing a variety of sources that have exploded in number. However, despite the incredible accumulation of biodiversity records, not all the information is actually useful, nor does it represent any new knowledge regarding global species richness patterns. In this study, we assessed the spatial and temporal representativeness of marine fish records (order Actinopterygii) found in the GBIF and OBIS global repositories. The methodological framework that we developed relies on a series of non-parametric estimators for computing species richness from incidence data. This methodology employs hexagonal grids as sampling units that overlay marine bioregions across the globe. Using standard ecological and spatial analysis tools, we identify regions that are adequately represented in terms of available records and therefore have more reliable data, as well as regions with few records that do not represent current species richness. We overlap these results with the location of marine protected areas and fishing exploitation zones to understand the anthropogenic effect on marine ichthyofauna. We additionally evaluate hypotheses regarding the taxonomic, geographic, and temporal distribution of information biases to deepen our current understanding of public records of species occurrences worldwide. Considering that more than 40 years of information was analyzed, the results showed that, on a global scale, the primary data on marine fish available on GBIF and OBIS platforms are still far from being representative and complete. Only 1.14% of the records were useful for our analyses. In addition, we found that the information seems to be biased towards coastal areas, regions close to developed countries, and areas where there is a large fishing activity. Finally, the best represented species and families are those with a small body size, which use shallow habitats and are usually recognized as having commercial or cultural value.}
}
@article{CUSICK2024102707,
title = {Using machine learning to count Antarctic shag (Leucocarbo bransfieldensis) nests on images captured by remotely piloted aircraft systems},
journal = {Ecological Informatics},
volume = {82},
pages = {102707},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102707},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002498},
author = {Andrew Cusick and Katarzyna Fudala and Piotr Pasza Storożenko and Jędrzej Świeżewski and Joanna Kaleta and W. Chris Oosthuizen and Christian Pfeifer and Robert Józef Bialik},
keywords = {Antarctic shag, Drone, , Machine learning, Object detection},
abstract = {Using 51 orthomosaics of 11 breeding locations of the Antarctic shag (Leucocarbo bransfieldensis), we propose a method for automating counting of shag nests. This is achieved by training an object detection model based on the “You Only Look Once” (YOLO) architecture and identifying nests on sections of the orthomosaic, which are later combined with predictions for the entire orthomosaic. Our results show that the current use of Remotely Piloted Aircraft Systems (RPAS) to collect images of areas with shag colonies, combined with machine learning algorithms, can provide reliable and fast estimates of shag nest counts (F1 score > 0.95). By using data from only two shag colonies for training, we show that models can be obtained that generalise well to images of both spatially and temporally distinct colonies. The proposed practical application opens the possibility of using aerial imagery to perform large-scale surveys of Antarctic islands in search of undiscovered shag colonies. We discuss the conditions for optimal performance of the model as well as its limitations. The code, data and trained model allowing for full reproducibility of the results are available at https://github.com/Appsilon/Antarctic-nests.}
}
@article{CELIS2024102578,
title = {A versatile, semi-automated image analysis workflow for time-lapse camera trap image classification},
journal = {Ecological Informatics},
volume = {81},
pages = {102578},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102578},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001201},
author = {Gerardo Celis and Peter Ungar and Aleksandr Sokolov and Natalia Sokolova and Hanna Böhner and Desheng Liu and Olivier Gilg and Ivan Fufachev and Olga Pokrovskaya and Rolf Anker Ims and Wenbo Zhou and Dan Morris and Dorothee Ehrich},
keywords = {Arctic wildlife monitoring, Deep learning, ResNet-50, MegaDetector, Time-lapse camera},
abstract = {Camera traps are a powerful, practical, and non-invasive method used widely to monitor animal communities and evaluate management actions. However, camera trap arrays can generate thousands to millions of images that require significant time and effort to review. Computer vision has emerged as a tool to accelerate this image review process. We propose a multi-step, semi-automated workflow which takes advantage of site-specific and generalizable models to improve detections and consists of (1) automatically identifying and removing low-quality images in parallel with classification into animals, humans, vehicles, and empty, (2) automatically cropping objects from images and classifying them (rock, bait, empty, and species), and (3) manually inspecting a subset of images. We trained and evaluated this approach using 548,627 images from 46 cameras in two regions of the Arctic: “Finnmark” (Finnmark County, Norway) and “Yamal” (Yamalo-Nenets Autonomous District, Russia). The automated steps yield image classification accuracies of 92% and 90% for the Finnmark and Yamal sets, respectively, reducing the number of images that required manual inspection to 9.2% of the Finnmark set and 3.9% of the Yamal set. The amount of time invested in developing models would be offset by the time saved from automation after 960 thousand images have been processed. Researchers can modify this multi-step process to develop their own site-specific models and meet other needs for monitoring and surveying wildlife, balancing the acceptable levels of false negatives and positives.}
}
@article{MORERA2024102557,
title = {Analysis of climate change impacts on the biogeographical patterns of species-specific productivity of socioeconomically important edible fungi in Mediterranean forest ecosystems},
journal = {Ecological Informatics},
volume = {81},
pages = {102557},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102557},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000992},
author = {Albert Morera and Hannah LeBlanc and Juan {Martínez de Aragón} and José Antonio Bonet and Sergio de-Miguel},
keywords = {Mushroom, , , Non-wood forest products, Global warming, Modeling},
abstract = {In Mediterranean forests, many species of fungi produce fruiting bodies every autumn, some of which are of great social and economic interest as NTFPs. In addition, these fungi are an essential part of the biodiversity network that ensures the proper functioning of natural ecosystems and that is currently in check due to global change. Therefore, understanding the biogeographic patterns of species-specific fungal productivity is fundamental to anticipate possible changes in the socioeconomic value of our forests and to understand the role they play in the functioning of ecosystems in terms of mitigation and adaptation to climate change. In this study we estimate the future impact of climate change (in Catalonia region, between 2023 and 2100) on five fungal species with high socioeconomic interest in a broad bioclimatic gradient representative of the Mediterranean basin using high resolution at the landscape scale. To achieve this, we use predictive models based on machine learning algorithms and a fungal database resulting from the sampling of more than 100 permanent sampling plots over 20 years. We estimate that current and future productivity patterns differ among species, under different climate change scenarios and bioclimatic regions. Our results suggest that optimal productivity areas may be shifted to higher elevations, making those species with higher productivity at higher elevations the most affected by climate change. This would mean that some species with high socioeconomic value, such as Lactarius deliciosus and Boletus edulis, could be negatively affected in their total productivity in the study area. This study highlights the need to anticipate the potential effects of climate change on fungal productivity and in particular on high socioeconomic value species and to develop management policies oriented to maintain the important role of fungi in natural ecosystems.}
}
@article{CARDOSO2024102602,
title = {Can citizen science and social media images support the detection of new invasion sites? A deep learning test case with Cortaderia selloana},
journal = {Ecological Informatics},
volume = {81},
pages = {102602},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102602},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001444},
author = {Ana Sofia Cardoso and Eva Malta-Pinto and Siham Tabik and Tom August and Helen E. Roy and Ricardo Correia and Joana R. Vicente and Ana Sofia Vaz},
keywords = {Artificial intelligence, Convolutional neural networks, Computer vision, Pampas grass},
abstract = {Deep learning has advanced the content analysis of digital data, unlocking opportunities for detecting, mapping, and monitoring invasive species. Here, we tested the ability of open source classification and object detection models (i.e., convolutional neural networks: CNNs) to identify and map the invasive plant Cortaderia selloana (pampas grass) in mainland Portugal. CNNs were trained over citizen science images and then applied to social media content (from Flickr, Twitter, Instagram, and Facebook), allowing to classify or detect the species in over 77% of situations. Images where the species was identified were mapped, using their georeferenced coordinates and time stamp, showing previously unreported occurrences of C. selloana, and a tendency for the species expansion from 2019 to 2021. Our study shows great potential from deep learning, citizen science and social media data for the detection, mapping, and monitoring of invasive plants, and, by extension, for supporting follow-up management options.}
}
@article{GUILBAULT2023102155,
title = {A practical approach to making use of uncertain species presence-only data in ecology: Reclassification, regularization methods and observer bias},
journal = {Ecological Informatics},
volume = {77},
pages = {102155},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102155},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300184X},
author = {Emy Guilbault and Ian Renner and Eric J. Beh and Michael Mahony},
keywords = {Poisson point pattern, Misidentification, Mixture modelling, Machine learning, Observer bias, Lasso penalty},
abstract = {Various statistical models and software platforms aim to produce species distribution models to better predict where species occur as a function of the environment. However, there are many practical challenges that arise with observations coming from opportunistic surveys. Such data may be of low quality with respect to accuracy and may also exhibit sampling bias. Here, we explore three main challenges. First, species identification can be misleading with the changes in taxonomy where the identification of species has changed for some genus, rendering older records confounded with respect to species identity. Second, the observers' sampled pattern may not reflect the true species distribution as some observers may favor some areas where the species is found. Furthermore, ecological knowledge of environmental drivers of a species distribution may be limited, which presents challenges in selecting appropriate covariates to include in species distribution models. In this paper, we extend two algorithms we recently developed which make use of misidentified observations in order to predict species distributions using spatial point processes. In particular, these algorithms incorporate sampling bias correction and address potential overfitting of the model via lasso-type penalties. We compare the performance of these algorithms to models which do not make use of the confounded species data, and explore the effects of the lasso penalty and bias correction on model performance. We apply the best performing methods to a real dataset of eastern Australian frogs for which taxonomy recently changed. Including confounded observations in the models is particularly relevant for informing management decisions regarding endangered species and species in remote areas.}
}
@article{SINGHAROY2023102265,
title = {Image background assessment as a novel technique for insect microhabitat identification},
journal = {Ecological Informatics},
volume = {77},
pages = {102265},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102265},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002947},
author = {Sesa {Singha Roy} and Reid Tingley and Alan Dorin},
keywords = {Microhabitat, Insects, Image analysis, Computer vision, Machine learning},
abstract = {Habitat fragmentation under increased urbanisation, industrial agriculture and land clearing, are changing the way insects occupy habitat. Some species are highly adaptable and may occupy urbanised areas, utilising anthropogenic microhabitat-scale features. Other species are dependent on natural elements of their habitats, having to locate small regions of natural microhabitat within increasingly hostile landscapes. Consequently, humans are encountering insects in new settings. Identifying and analysing insects’ use of natural and anthropogenic microhabitats is therefore important to assess their responses to a changing environment, for instance to improve pollination or manage invasive pests. But such studies are labour-intensive. Traditional studies of insect microhabitat use can now be supplemented by machine learning-based insect image analysis. Typically, research has focused on automatic insect classification, but valuable data appearing in image backgrounds has been ignored. In this research, we analysed the backgrounds of insect images available in the Atlas of Living Australia database to determine the microhabitats in which they were commonly photographed. We analysed the image backgrounds of three globally distributed insect species that are common across Australia: Drone flies (Eristalis tenax), European honey bees (Apis mellifera), and European wasps (Vespula germanica). Image backgrounds were classified broadly as either natural or anthropogenic using computer vision and machine learning tools benchmarked against a manual classification algorithm. Our automated image background classification achieved 97.4% accuracy when compared against manual classification. Mis-classifications were scarce, usually less than 1%, and primarily for backgrounds of wood and soil or bare ground. Our results indicate that drone flies and European honey bees were predominantly photographed against natural backgrounds (flies manual classifier 95±3%, automated classifier 94%, bees 89±2%,87%), implying frequent observations by humans in natural microhabitat. European wasps were less frequently photographed against natural backgrounds (70±6%,63%). Within this data set, observations of the wasps in anthropogenic microhabitats were more common than for flies and bees. Our results are aligned with the expectation that the wasps are relatively well-suited to urban environments, and that European honey bees and drone flies utilise natural features of their environment. In general, although biases in data collected without formal protocols limits their application, our new automated approach for image background analysis can provide valuable data about insects’ interactions with humans, our artefacts, and natural features of their environments.}
}
@article{ZHANG2024102399,
title = {Fully automatic system for fish biomass estimation based on deep neural network},
journal = {Ecological Informatics},
volume = {79},
pages = {102399},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102399},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004284},
author = {Tianye Zhang and Yuqiao Yang and Yueyue Liu and Chenglei Liu and Ran Zhao and Daoliang Li and Chen Shi},
keywords = {Aquaculture, Mass estimation, Neural network, Sustainable production},
abstract = {The approach for estimating biomass in non-contact, free-swimming fish has encountered difficulties such as fish body occlusion, bending, non-orthogonal angles, and low efficiency. To address these issues, this study had combined fish posture recognition (using deep learning technology) with biomass estimation (utilizing stereo vision technology) for the first time, and developed a fast, precise, and fully automatic fish biomass estimation system. The improved single-stage target detection algorithm significantly improved the direct detection and extraction of high-quality images of moving fish in real-time, eliminating the need for manual processing of images that may have imperfect posture. Fish body length and height were measured in the real world using binocular stereo vision technology. Finally, the fish body weight can be estimated by considering the relationship among their body length, height, and weight. The experiment confirmed that the system had successfully avoided influencing factors that could affect fully automatic estimation. The results demonstrated a strong linear relationship between the estimated and measured fish body weights, with a mean relative error (MRE) of 2.87%. There were no significant differences between the estimated and measured weights (p = 0.94). The MRE of the multi-factor model was much lower than that of the single-factor model (length-weight of 8.86% and height-weight of 7.41%). The results indicate that the system developed is a highly effective approach to fully automated biomass estimation. This can be used to guide actual production and further study of the mechanism of fish growth.}
}
@article{BJERGE2023102278,
title = {Hierarchical classification of insects with multitask learning and anomaly detection},
journal = {Ecological Informatics},
volume = {77},
pages = {102278},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102278},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003072},
author = {Kim Bjerge and Quentin Geissmann and Jamie Alison and Hjalte M.R. Mann and Toke T. Høye and Mads Dyrmann and Henrik Karstoft},
keywords = {nomaly detection, Computer vision, Deep learning, Hierarchical classification, Insects, Taxonomy},
abstract = {Cameras and computer vision are revolutionising the study of insects, creating new research opportunities within agriculture, epidemiology, evolution, ecology and monitoring of biodiversity. However, the diversity of insects and close resemblances of many species are a major challenge for image-based species-level classification. Here, we present an algorithm to hierarchically classify insects from images, leveraging a simple taxonomy to (1) classify specimens across multiple taxonomic ranks simultaneously, and (2) identify the lowest rank at which a reliable classification can be reached. Specifically, we propose multitask learning, a loss function incorporating class dependency at each taxonomic rank, and anomaly detection based on outlier analysis to quantify the uncertainty. First, we compile a dataset of 41,731 images of insects, combining images from time-lapse monitoring of floral scenes with images from the Global Biodiversity Information Facility (GBIF). Second, we adapt state-of-the-art convolutional neural networks, ResNet and EfficientNet, for the hierarchical classification of insects belonging to three orders, five families and nine species. Third, we assess model generalization for 11 species unseen by the trained models. Here, anomaly detection is used to predict the higher rank of the species which were not present in the training set. We found that incorporating a simple taxonomy into our model increased the accuracy at higher taxonomic ranks. As expected, our algorithm correctly classified new insect species at higher taxonomic ranks, while classification was uncertain at lower taxonomic ranks. Anomaly detection can effectively flag novel taxa that are visually distinct from species in the training data. However, five novel taxa were consistently mistaken for visually similar species in the training data. Above all, we have demonstrated a practical approach to hierarchical classification based on species taxonomy and uncertainty during automated in situ monitoring of live insects. Our method is simple and versatile, forming a valuable step towards high-level classification of species not found in training data.}
}
@article{RIECHMANN2022101657,
title = {Motion vectors and deep neural networks for video camera traps},
journal = {Ecological Informatics},
volume = {69},
pages = {101657},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101657},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122001066},
author = {Miklas Riechmann and Ross Gardiner and Kai Waddington and Ryan Rueger and Frederic Fol Leymarie and Stefan Rueger},
keywords = {AI, Computer vision, Machine learning, Motion vectors, Video camera trap, Video pipeline},
abstract = {Commercial camera traps are usually triggered by a Passive Infra-Red (PIR) motion sensor necessitating a delay between triggering and the image being captured. This often seriously limits the ability to record images of small and fast moving animals. It also results in many “empty” images, e.g., owing to moving foliage against a background of different temperature. In this paper we detail a new triggering mechanism based solely on the camera sensor. This is intended for use by citizen scientists and for deployment on an affordable, compact, low-power Raspberry Pi computer (RPi). Our system introduces a video frame filtering pipeline consisting of movement and image-based processing. This makes use of Machine Learning (ML) feasible on a live camera stream on an RPi. We describe our free and open-source software implementation of the system; introduce a suitable ecology efficiency measure that mediates between specificity and recall; provide ground-truth for a video clip collection from camera traps; and evaluate the effectiveness of our system thoroughly. Overall, our video camera trap turns out to be robust and effective.}
}
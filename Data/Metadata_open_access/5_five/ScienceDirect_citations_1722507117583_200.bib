@article{PIECHAUD2022101786,
title = {Fast and accurate mapping of fine scale abundance of a VME in the deep sea with computer vision},
journal = {Ecological Informatics},
volume = {71},
pages = {101786},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101786},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002369},
author = {Nils Piechaud and Kerry L. Howell},
keywords = {Benthic ecology, Computer vision, Xenophyophores, Quantitative ecology, Mapping, Automated image analysis, Marine conservation},
abstract = {With growing anthropogenic pressure on deep-sea ecosystems, large quantities of data are needed to understand their ecology, monitor changes over time and inform conservation managers. Current methods of image analysis are too slow to meet these requirements. Recently, computer vision has become more accessible to biologists, and could help address this challenge. In this study we demonstrate a method by which non-specialists can train a YOLOV4 Convolutional Neural Network (CNN) able to count and measure a single class of objects. We apply CV to the extraction of quantitative data on the density and population size structure of the xenophyophore Syringammina fragilissima, from more than 58,000 images taken by an AUV 1200 m deep in the North-East Atlantic. The workflow developed used open-source tools, cloud-base hardware, and only required a level of experience with CV commonly found among ecologists. The CNN performed well, achieving a recall of 0.84 and precision of 0.91. Individual counts per image and size measurements resulting from model predictions were highly correlated (0.96 and 0.92, respectively) with manually collected data. The analysis could be completed in less than 10 days thus bringing novel insights into the population size structure and fine scale distribution of this Vulnerable Marine Ecosystem. It showed S. fragilissima distribution is patchy. The average density is 2.5 ind.m−2 but can vary from up to 45 ind.m−2 only a few tens of meter away from areas where it is almost absent. The average size is 5.5 cm and the largest individuals (>15 cm) tend to be in areas of low density. This study demonstrates how researchers could take advantage of CV to quickly and efficiently generate large quantitative datasets data on benthic ecosystems extent and distribution. This, coupled with the large sampling capacity of AUVs could bypass the bottleneck of image analysis and greatly facilitate future deep-ocean exploration and monitoring. It also illustrates the future potential of these new technologies to meet the goals set by the UN Ocean Decade.}
}
@article{NOMAN2023102047,
title = {Improving accuracy and efficiency in seagrass detection using state-of-the-art AI techniques},
journal = {Ecological Informatics},
volume = {76},
pages = {102047},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102047},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000766},
author = {Md Kislu Noman and Syed Mohammed Shamsul Islam and Jumana Abu-Khalaf and Seyed Mohammad Jafar Jalali and Paul Lavery},
keywords = {Deep learning, EfficientDet, Faster R-CNN, , NASNet, Seagrass, YOLOv5},
abstract = {Seagrasses provide a wide range of ecosystem services in coastal marine environments. Despite their ecological and economic importance, these species are declining because of human impact. This decline has driven the need for monitoring and mapping to estimate the overall health and dynamics of seagrasses in coastal environments, often based on underwater images. However, seagrass detection from underwater digital images is not a trivial task; it requires taxonomic expertise and is time-consuming and expensive. Recently automatic approaches based on deep learning have revolutionised object detection performance in many computer vision applications, and there has been interest in applying this to automated seagrass detection from imagery. Deep learning–based techniques reduce the need for hardcore feature extraction by domain experts which is required in machine learning-based techniques. This study presents a YOLOv5-based one-stage detector and an EfficientDetD7–based two-stage detector for detecting seagrass, in this case, Halophila ovalis, one of the most widely distributed seagrass species. The EfficientDet-D7–based seagrass detector achieves the highest mAP of 0.484 on the ECUHO-2 dataset and mAP of 0.354 on the ECUHO-1 dataset, which are about 7% and 5% better than the state-of-the-art Halophila ovalis detection performance on those datasets, respectively. The proposed YOLOv5-based detector achieves an average inference time of 0.077 s and 0.043 s respectively which are much lower than the state-of-the-art approach on the same datasets.}
}
@article{BACHECHI2024102568,
title = {HypeAIR: A novel framework for real-time low-cost sensor calibration for air quality monitoring in smart cities},
journal = {Ecological Informatics},
volume = {81},
pages = {102568},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102568},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001109},
author = {Chiara Bachechi and Federica Rollo and Laura Po},
keywords = {Real-time, Sensor calibration, Air quality monitoring, Smart cities, Air pollution monitoring, Low cost sensors, Time series, Framework, Air quality, LSTM, Random forest},
abstract = {While less reliable than authorized air quality stations, low-cost sensors help monitor air quality in areas overlooked by traditional devices. A calibration process in the same environment as the sensor is crucial to enhance their accuracy. Furthermore, low-cost sensors deteriorate over time, necessitating repeated calibration for sustained performance. HypeAIR is a novel open-source framework for the management of sensor calibration in real-time. It incorporates two calibration methodologies: a combination of machine learning models (Voting Regressor and Support Vector Regression) and the Long Short-Term Memory deep learning model. To evaluate the framework, three extensive experiments were conducted over a 2-year period in the city of Modena, Italy, to monitor NO, NO2, and O3 gases. Both calibration methodologies outperform the manufacturer calibration and our baseline (i.e., a variation of the Random Forest algorithm) and maintain efficiency over time. The availability of the source code facilitates customization for monitoring additional pollutants, while shared air quality datasets ensure reproducibility.}
}
@article{KALFAS2023102037,
title = {Towards automatic insect monitoring on witloof chicory fields using sticky plate image analysis},
journal = {Ecological Informatics},
volume = {75},
pages = {102037},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102037},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000663},
author = {Ioannis Kalfas and Bart {De Ketelaere} and Klaartje Bunkens and Wouter Saeys},
keywords = {Insect recognition, Convolutional neural networks, Pest management, Automatic monitoring},
abstract = {Context
Sticky trap catches of agricultural pests can be employed for early hotspot detection, identification, and estimation of pest presence in greenhouses or in the field. However, manual procedures to produce and analyze catch results require substantial time and effort. As a result, much research has gone into creating efficient techniques for remotely monitoring possible infestations. A considerable number of these studies use Artificial Intelligence (AI) to analyze the acquired data and focus on performance metrics for various model architectures. Less emphasis, however, was devoted to the testing of the trained models to investigate how well they would perform under practical, in-field conditions.
Objective
In this study, we showcase an automatic and reliable computational method for monitoring insects in witloof chicory fields, while shifting the focus to the challenges of compiling and using a realistic insect image dataset that contains insects with common taxonomy levels.
Methods
To achieve this, we collected, imaged, and annotated 731 sticky plates - containing 74,616 bounding boxes - to train a YOLOv5 object detection model, concentrating on two pest insects (chicory leaf-miners and wooly aphids) and their two predatory counterparts (ichneumon wasps and grass flies). To better understand the object detection model's actual field performance, it was validated in a practical manner by splitting our image data on the sticky plate level.
Results and conclusions
According to experimental findings, the average mAP score for all dataset classes was 0.76. For both pest species and their corresponding predators, high mAP values of 0.73 and 0.86 were obtained. Additionally, the model accurately forecasted the presence of pests when presented with unseen sticky plate images from the test set.
Significance
The findings of this research clarify the feasibility of AI-powered pest monitoring in the field for real-world applications and provide opportunities for implementing pest monitoring in witloof chicory fields with minimal human intervention.}
}
@article{JAMEI2024102455,
title = {Quantitative improvement of streamflow forecasting accuracy in the Atlantic zones of Canada based on hydro-meteorological signals: A multi-level advanced intelligent expert framework},
journal = {Ecological Informatics},
volume = {80},
pages = {102455},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102455},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004843},
author = {Mozhdeh Jamei and Mehdi Jamei and Mumtaz Ali and Masoud Karbasi and Aitazaz A. Farooque and Anurag Malik and Saad Javed Cheema and Travis J. Esau and Zaher Mundher Yaseen},
keywords = {Streamflow forecasting, Hydro-meteorological drivers, Multivariate variational mode decomposition, CNN-BiGRU, Boruta-CART, Multi-temporal},
abstract = {Developing reliable streamflow forecasting models is critical for hydrological tasks such as improving water resource management, analyzing river patterns, and flood forecasting. In this research, for the first time, an emerging multi-level TOPSIS (technique for order preference by similarity to the ideal solution) -based hybridization comprised of the Boruta classification and regression tree (Boruta-CART) feature selection, multivariate variational mode decomposition (MVMD), and a hybrid Convolutional Neural Network (CNN) Bidirectional Gated Recurrent Unit (CNN-BiGRU) deep learning was adopted to multi-temporal (one and three days ahead) forecast the daily streamflow in the Rivers of Prince Edward Island, Canada. For this aim, in the first step, the Boruta-CART feature selection technique determines the most effective lagged components among all the antecedent two-day information (i.e., t-1 and t-2) of hydro-meteorological features (from 2015 to 2020), including the water level, mean air temperature, heat degree days, total precipitation, dew point temperature, and relative humidity in the Bear and Winter Rivers of Prince Edward Island, Canada. Afterwards, a multivariate variational mode decomposition (MVMD) decomposes the input time series to decrease the complexity and non-linearity of the non-stationary ones before feeding the deep learning (DL) models. Here, the CNN-GRU was employed as the primary DL model, along with the kernel extreme machine method (KELM), random variational function link (RVFL), and hybrid CNN bidirectional recurrent neural network (CNN-BiRNN) as the comparative models. A TOPSIS scheme applying several performance measures like the correlation coefficient (R), root mean square error (RMSE), and reliability was designed for the robustness assessment of the hybrid (MVM-CNN-BiGRU, MVM-CNN-BiRNN, MVM-RVFL, and MVM-KELM) and standalone models. The computational outcomes revealed that in the Bear River, the MVM-CNN-BiGRU, owing to its best forecasting performance (one day ahead: TOPSIS score 1, R = 0.960, RMSE = 0.098, and reliability = 65.082; three days ahead: TOPSIS score = 0.999, R = 0.924, and RMSE = 0.33) outperformed the other hybrid models, followed by the MVM-CNN-BiRNN, MVM-RVFL, and MVM-KELM, respectively. Moreover, in the Winter River, the MVM-CNN-BiGRU in terms of (one-day ahead: TOPSIS score = 0.890, R = 0.955, RMSE = 0.274, and reliability = 34.004; three-days ahead: TOPSIS score = 0.686, R = 0.924, and RMSE = 0.330) was superior to the other models. The provided expert system could be vital in the local flood decision-making process, in the absence of streamflow information as input modeling, during the flood seasons to reduce flood damage in residential areas.}
}
@article{GANJIRAD2024102498,
title = {Google Earth Engine-based mapping of land use and land cover for weather forecast models using Landsat 8 imagery},
journal = {Ecological Informatics},
volume = {80},
pages = {102498},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102498},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000402},
author = {Mohammad Ganjirad and Hossein Bagheri},
keywords = {LULC, GEE, WRF model, Machine learning, Classification, Landsat 8, Weather forecast},
abstract = {Land Use and Land Cover (LULC) maps are vital prerequisites for weather prediction models. This study proposes a framework to generate LULC maps based on the U.S. Geological Survey (USGS) 24-category scheme using Google Earth Engine. To realize a precise LULC map, a fusion of pixel-based and object-based classification strategies was implemented using various machine learning techniques across different seasons. For this purpose, feature importance analysis was conducted on the top classifiers considering the dynamic (seasonal) behavior of LULC. The results showed that ensemble approaches such as Random Forest and Gradient Tree Boosting outperformed other algorithms. The results also demonstrated that the object-based approach had better performance due to the consideration of contextual features. Finally, the proposed fusion framework produced a LULC map with higher accuracy (overall accuracy = 94.92% and kappa coefficient = 94.19%). Furthermore, the performance of the generated LULC map was assessed by applying it to the Weather Research and Forecasting (WRF) model for downscaling wind speed and 2-m air temperature (T2). The assessment indicated that the generated LULC map effectively reflected real-world conditions, thereby impacting the estimation of wind speed and T2 fields by WRF. Statistical assessments demonstrated enhancements in RMSE by 0.02 °C, MAE by 1 °C, and Bias by 0.03 °C for T2. Additionally, there was an improvement of 0.06 m/s in MAE for wind speed. Consequently, the framework can be implemented to produce accurate and up-to-date high-resolution LULC maps in various geographical areas worldwide. The source codes corresponding to this research paper are available on GitHub via https://github.com/Mganjirad/GEE-LULC-WRF.}
}
@article{NTALAMPIRAS2023102043,
title = {An integrated system for the acoustic monitoring of goat farms},
journal = {Ecological Informatics},
volume = {75},
pages = {102043},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102043},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000729},
author = {Stavros Ntalampiras and Luca A. Ludovico and Giorgio Presti and Mael Vittorio Vena and Davide Fantini and Tyfenn Ogel and Stefania Celozzi and Monica Battini and Silvana Mattiello},
keywords = {Precision livestock farming, Acoustic monitoring, Internet of things, Goat farms, Animal vocalizations bioacoustics},
abstract = {Effective precision livestock farming requires the availability of reliable and up-to-date data characterizing the animals and environment of interest. This work presents the design, architecture, and implementation of a wireless acoustic sensor network for monitoring goat farms on a 24/7 basis. In addition, we define a hierarchical organization of the involved sound classes exhaustively covering every aspect of the encountered goat vocalizations. Moreover, we developed an annotation tool tailored to the specifics of the problem at hand, i.e. a big, real-world data environment, able to meaningfully assist the annotation of goat vocalizations via a suitable sound classification module. On top of that, a mobile phone application was developed facilitating authorized users to remotely access information describing the situation on the farm site. Importantly, such a non-invasive monitoring framework was installed in 4 different sites located in Northern Italy while taking into account their diverse characteristics.}
}
@article{COMESANACEBRAL2024102612,
title = {Wildfire response of forest species from multispectral LiDAR data. A deep learning approach with synthetic data},
journal = {Ecological Informatics},
volume = {81},
pages = {102612},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102612},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001547},
author = {Lino Comesaña-Cebral and Joaquín Martínez-Sánchez and Gabriel Suárez-Fernández and Pedro Arias},
keywords = {Multispectral LiDAR, Deep learning, Fire response, Synthetic data, Wildfire},
abstract = {Forests play a crucial role as the lungs and life-support system of our planet, harbouring 80% of the Earth's biodiversity. However, we are witnessing an average loss of 480 ha of forest every hour because of destructive wildfires spreading across the globe. To effectively mitigate the threat of wildfires, it is crucial to devise precise and dependable approaches for forecasting fire dynamics and formulating efficient fire management strategies, such as the utilisation of fuel models. The objective of this study was to enhance forest fuel classification that considers only structural information, such as the Prometheus model, by integrating data on the fire responses of various tree species and other vegetation elements, such as ground litter and shrubs. This distinction can be achieved using multispectral (MS) Light Detection and Ranging (LiDAR) data in mixed forests. The methodology involves a novel approach in semantic classifications of forests by generating synthetic data with semantic labels regarding fire responses and reflectance information at different spectral bands, as a real MS scanner device would detect. Forests, which are highly intricate environments, present challenges in accurately classifying point clouds. To address this complexity, a deep learning (DL) model for semantic classification was trained on synthetic point clouds in different formats to achieve the best performance when leveraging MS data. Forest plots in the study region were scanned using different Terrestrial Laser Scanning sensors at wavelengths of 905 and 1550 nm. Subsequently, an interpolation process was applied to generate the MS point clouds of each plot, and the trained DL model was applied to classify them. These classifications surpassed the average thresholds of 90% and 75% for accuracy and intersection over union, respectively, resulting in a more precise categorisation of fuel models based on the distinct responses of forest elements to fire. The results of this study reveal the potential of MS LiDAR data and DL classification models for improving fuel model retrieval in forest ecosystems and enhancing wildfire management efforts.}
}
@article{NOORI2024102565,
title = {Lake total suspended matter retrieval by wind speed: A machine learning model trained by time-series satellite imagery},
journal = {Ecological Informatics},
volume = {81},
pages = {102565},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102565},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001079},
author = {Ashkan Noori and Seyed Hossein Mohajeri and Mojtaba Mehraein and Ahmad Sharafati},
keywords = {Chah-Nimeh reservoirs, Lake water quality, Neural network, Satellite images, Total suspended matter, Wind speed},
abstract = {This study aims to develop an affordable and continuous method for monitoring water quality in arid, remote areas with high erosion rates. It presents a hypothesis that establishes a link between the concentration of Total Suspended Matter (TSM) and wind speed, emphasizing its ecological importance in lakes with dry conditions and high erosion rates. Building upon this hypothesis, the study introduces Wind2TSM-Net, a machine learning model that effectively bridges between different regression and neural network algorithms. This model connects on-site wind speed measurements with TSM data obtained through a physically remote sensing approach. It accurately predicts TSM concentration values, overcoming challenges such as cloud interference and reducing reliance on satellite imagery. The model was applied to Iran's Chah-Nimeh Reservoirs (CNRs) as a case study in an arid and remote area. The results revealed a significant correlation between TSM concentration and wind speed measurements, with impressive performance metrics (coefficient of determination (R2) = 0.88, root mean square error (RMSE) = 1.97 g/m3, mean absolute error (MAE) = 1.33). These findings highlight the effectiveness of Wind2TSM-Net in monitoring TSM values in remote and dry regions, particularly during extreme weather conditions when on-site measurements are impractical or cloud cover obstructs satellite observations.}
}
@article{CLARK2023102065,
title = {The effect of soundscape composition on bird vocalization classification in a citizen science biodiversity monitoring project},
journal = {Ecological Informatics},
volume = {75},
pages = {102065},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102065},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000948},
author = {Matthew L. Clark and Leonardo Salas and Shrishail Baligar and Colin A. Quinn and Rose L. Snyder and David Leland and Wendy Schackwitz and Scott J. Goetz and Shawn Newsam},
keywords = {Convolutional neural networks, CNN, Ecoacoustics, Avian diversity, Bird species classification, Mixture of experts (MoE), Citizen science, Automated recording units, ARU, Soundscapes to landscapes, BirdNET, Soundscape components},
abstract = {There is a need for monitoring biodiversity at multiple spatial and temporal scales to aid conservation efforts. Autonomous recording units (ARUs) can provide cost-effective, long-term and systematic species monitoring data for sound-producing wildlife, including birds, amphibians, insects and mammals over large areas. Modern deep learning can efficiently automate the detection of species occurrences in these sound data with high accuracy. Further, citizen science can be leveraged to scale up the deployment of ARUs and collect reference vocalizations needed for training and validating deep learning models. In this study we develop a convolutional neural network (CNN) acoustic classification pipeline for detecting 54 bird species in Sonoma County, California USA, with sound and reference vocalization data collected by citizen scientists within the Soundscapes to Landscapes project (www.soundscapes2landscapes.org). We trained three ImageNet-based CNN architectures (MobileNetv2, ResNet50v2, ResNet100v2), which function as a Mixture of Experts (MoE), to evaluate the usefulness of several methods to enhance model accuracy. Specifically, we: 1) quantify accuracy with fully-labeled 1-min soundscapes for an assessment of real-world conditions; 2) assess the effect on precision and recall of additional pre-training with an external sound archive (xeno-canto) prior to fine-tuning with vocalization data from our study domain; and, 3) assess how detections and errors are influenced by the presence of coincident biotic and non-biotic sounds (i.e., soundscape components). In evaluating accuracy with soundscape data (n = 37 species) across CNN probability thresholds and models, we found acoustic pre-training followed by fine-tuning improved average precision by 10.3% relative to no pre-training, although there was a small average 0.8% reduction in recall. In selecting an optimal CNN architecture for each species based on maximum F(β = 0.5), we found our MoE approach had total precision of 84.5% and average species precision of 85.1%. Our data exhibit multiple issues arising from applying citizen science and acoustic monitoring at the county scale, including deployment of ARUs with relatively low fidelity and recordings with background noise and overlapping vocalizations. In particular, human noise was significantly associated with more incorrect species detections (false positives, decreased precision), while physical interference (e.g., recorder hit by a branch) and geophony (e.g., wind) was associated with the classifier missing detections (false negatives, decreased recall). Our process surmounted these obstacles, and our final predictions allowed us to demonstrate how deep learning applied to acoustic data from low-cost ARUs paired with citizen science can provide valuable bird diversity data for monitoring and conservation efforts.}
}
@article{LIN2024102507,
title = {A model for forest type identification and forest regeneration monitoring based on deep learning and hyperspectral imagery},
journal = {Ecological Informatics},
volume = {80},
pages = {102507},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102507},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000499},
author = {Feng-Cheng Lin and Yi-Shiang Shiu and Pei-Jung Wang and Uen-Hao Wang and Jhe-Syuan Lai and Yung-Chung Chuang},
keywords = {Remote sensing, Deep learning, VGG19, ResNet50, Hyperspectral images},
abstract = {Traditional ground-based forest survey methods involve high labor costs, and their inefficiency makes comprehensive forest resource surveys challenging. With the development of new sensors and vehicles in recent years, more diverse and novel remote sensing detection and survey techniques have emerged. This study aims to use hyperspectral imagery to classify forest types containing representative tree species. To verify the feasibility of the proposed methods, we used hyperspectral imagery from the Taiwan Forestry Experiment Institute's Liugui Research Forest in southern Taiwan, which has an area of 9882 ha and an altitude of 250–2600 m. Hyperspectral imagery offers several advantages compared to traditional multispectral imagery; it captures a broad spectrum of contiguous, narrow spectral bands, providing highly detailed spectral information, enabling differentiation of tree components that appear similar in multispectral imagery. Eight identifiable forest types were selected for the models considered, and three different deep learning algorithms, VGG19, ResNet50 and a proposed combination (VGG19 + ResNet50), were used to screen the best algorithms. Data formats and pre-processing methods that can effectively improve computational performance were explored. The research results found that: (1) Band filtering is a necessary means to improve calculation performance; (2) Flattening the original convolution kernel with cubic characteristics can significantly reduce the time required for calculation. In terms of simulation results, VGG19 + ResNet50 was identified as the best model. Its overall classification accuracy can generally reach 93% to 100%. According to the calculation process set in this study, the time required for model training can be shortened to less than 30 min. The results of this research will help process more detailed and complex information in forest resource management and more accurately quantify forest ecology and woodland conditions.}
}
@article{DUAN2024102637,
title = {SIAlex: Species identification and monitoring based on bird sound features},
journal = {Ecological Informatics},
volume = {81},
pages = {102637},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102637},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001791},
author = {Lin Duan and Lidong Yang and Yong Guo},
keywords = {Lightweight, Cascading activation function, Bird sound recognition, Structural re-parameterization, Nonlinear performance},
abstract = {The combination of deep learning and bird sound recognition is widely employed in bird species conservation monitoring. A complex network structure is not conducive for deploying bird sound recognition devices, resulting in problems such as long inference time and low efficiency. Using AlexNet as the backbone model, we explore the potential of shallow and straightforward models without complex connection techniques or attention mechanisms, named SIAlex, to recognise and classify 20 bird sound datasets, which are simultaneously validated on a 10 class UrbanSound8k dataset. Using the structural re-parameterization method, the number of model layers is reduced, computational efficiency is improved, and the inference time is significantly reduced, achieving a decoupling of training and inference time in the structure. To increase the nonlinearity of the model, a cascaded approach is utilised to increase the number of activation functions, thereby significantly improving the generalisation performance of the model. Simultaneously, in the classifier section, convolutional layer replaces the original fully connected layer, thereby reducing the inference time and increasing the feature extraction ability of the model, improving accuracy, and effectively recognising bird speech. The experimental data show that the SIAlex network on the Birdsdata dataset improves the accuracy to 93.66%, and the inference time for a piece of data is only 2.466 ms. The accuracy of the UrbanSound8k dataset reaches 96.04%, and the inference time for a piece of data is 3.031 ms. A large number of experimental comparisons have shown that the method proposed in this paper achieves good results in reducing the inference time of the model, bringing breakthroughs in the application of shallow, simple models.}
}
@article{VERMA2024102428,
title = {“Forest carbon sequestration mapping and economic quantification infusing MLPnn-Markov chain and InVEST carbon model in Askot Wildlife Sanctuary, Western Himalaya”},
journal = {Ecological Informatics},
volume = {79},
pages = {102428},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102428},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004570},
author = {Pragati Verma and Azizur Rahman Siddiqui and Nitesh Kumar Mourya and Ahanthem Rebika Devi},
keywords = {Carbon mapping, Machine learning, Net present value (NPV), Sensitivity analysis, InVEST model, Markov chain},
abstract = {Due to the expeditious rise in anthropogenic activities and rapid extractive pressure, protected areas in the Indian Himalayan Region (IHR) are experiencing significant losses in ecological diversity and a substantial decline in the potential for carbon sequestration and climate change mitigation. The quantification of carbon sequestration and the understanding of its corresponding economic gain/loss are crucial for raising strict preservative concerns in protected areas with rich ecological and economic values. Transitions in land use land cover (LULC) trajectories from historical to future scenarios impact the dynamics of carbon storage and sequestration in terrestrial ecosystems. The present study extends a comprehensive 40-year (1995–2035) assessment of carbon mapping (gain/loss) as well as an economic approximation of carbon sequestration in the Askot Wildlife Sanctuary (AWLS), Western Himalaya, using satellite data. This investigation presents a novel hybrid approach by infusing machine learning algorithms and a spatial-temporal technique-based MLPnn-Markov chain model for future LULC simulation with the InVEST model that incorporates carbon mapping and economic valuation. The results show that among all the land use classes, dense forest has the highest carbon density; however, it exhibits a decreasing trend from 1995 (5,695,878.41 Mg/ha) to 2035 (4,378,439.81 Mg/ha). The economic quantification of carbon sequestration was performed by applying sensitivity analysis combined with different carbon prices and discount rates from 2020 to 2035. The observed outcomes reveal significant economic losses due to rapid forest cover decline, as indicated by a negative net present value (NPV) ranging from a minimum of ∼US$ -8 million to a maximum of ∼US$ -53 million. This study develops a valuable database by providing evidence-based decision-making and guidance for the sustainable preservation of ecosystems in the Western Himalayas and similar regions worldwide, where forest carbon sequestration is of paramount importance. The findings of the study suggest sound provisions for the conservation of forested landscapes and the development of efficient voluntary and regulatory carbon trading markets to achieve stability in forest carbon stocks. In doing so, this interdisciplinary approach addresses the growing imperative of integrating ecological and economic aspects in the context of biodiversity conservation and climate change mitigation.}
}
@article{GYAWALI2024102706,
title = {From simple linear regression to machine learning methods: Canopy cover modelling of a young forest using planet data},
journal = {Ecological Informatics},
volume = {82},
pages = {102706},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102706},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002486},
author = {Arun Gyawali and Hari Adhikari and Mika Aalto and Tapio Ranta},
keywords = {Young boreal forest, Forest canopy cover, Light detection and ranging, PlanetScope, Vegetation index, Machine learning},
abstract = {Accurate canopy cover estimation is essential for mature and early-stage young forests, as it guides forest management and silvicultural activities necessary for their growth and regeneration. However, obtaining precise measurements of canopy cover in the field is time-consuming and challenging, especially at the regional and landscape levels. Remote sensing techniques offer a promising alternative to traditional field-based measurements for estimating forest canopy cover. In this study, our objective is to estimate forest canopy cover using vegetation indices derived from the multispectral bands of PlanetScope (Planet Lab, Inc., San Francisco, CA, USA). To the best of our knowledge, this is the first study to utilise PlanetScope imagery data for estimating canopy cover in young boreal forests. Based on the analysis of four bands (green, blue, red, and near-infrared) from PlanetScope imagery, 43 vegetation indices, including four spectral bands and 13 salinity indices, were computed to select predictors in canopy cover modelling. Six regression models were employed to model canopy cover: linear, elastic net, support vector machine, random forest, extreme gradient boosting, and light gradient boosting machine. All the models demonstrated good performance for both the training dataset (R2 = 0.58–0.69) and the testing dataset (R2 = 0.59–0.64, RMSE = 0.16–0.18, rRMSE = 22%–23%, and MAE = 0.12–0.14). Based on the fit statistics in the training and testing datasets and the paired t-test, our study identified the light gradient boosting machine as the most suitable model for predicting canopy cover in young boreal forests. For the light gradient boosting machine, the R2 value was 0.69 (training), and for testing data, the R2 = 0.64, RMSE = 0.16, rRMSE = 22%, and MAE = 0.12. Therefore, we recommend that future researchers utilise Planet multispectral data and the light gradient boosting machine regression to estimate forest canopy cover at a higher spatial resolution. However, exploring additional machine learning algorithms and explicitly boosting methods when computing forest canopy cover using satellite remote sensing is strongly advised.}
}
@article{BAYR2019220,
title = {Automatic detection of woody vegetation in repeat landscape photographs using a convolutional neural network},
journal = {Ecological Informatics},
volume = {50},
pages = {220-233},
year = {2019},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S1574954118303121},
author = {Ulrike Bayr and Oskar Puschmann},
keywords = {Repeat photography, Photo monitoring, Landscape monitoring, Landscape change, Vegetation succession, Machine learning},
abstract = {Repeat photography is an efficient method for documenting long-term landscape changes. So far, the usage of repeat photographs for quantitative analyses is limited to approaches based on manual classification. In this paper, we demonstrate the application of a convolutional neural network (CNN) for the automatic detection and classification of woody regrowth vegetation in repeat landscape photographs. We also tested if the classification results based on the automatic approach can be used for quantifying changes in woody vegetation cover between image pairs. The CNN was trained with 50 × 50 pixel tiles of woody vegetation and non-woody vegetation. We then tested the classifier on 17 pairs of repeat photographs to assess the model performance on unseen data. Results show that the CNN performed well in differentiating woody vegetation from non-woody vegetation (accuracy = 87.7%), but accuracy varied strongly between individual images. The very similar appearance of woody vegetation and herbaceous species in photographs made this a much more challenging task compared to the classification of vegetation as a single class (accuracy = 95.2%). In this regard, image quality was identified as one important factor influencing classification accuracy. Although the automatic classification provided good individual results on most of the 34 test photographs, change statistics based on the automatic approach deviated from actual changes. Nevertheless, the automatic approach was capable of identifying clear trends in increasing or decreasing woody vegetation in repeat photographs. Generally, the use of repeat photography in landscape monitoring represents a significant added value to other quantitative data retrieved from remote sensing and field measurements. Moreover, these photographs are able to raise awareness on landscape change among policy makers and public as well as they provide clear feedback on the effects of land management.}
}
@article{RIECHMANN2022101657,
title = {Motion vectors and deep neural networks for video camera traps},
journal = {Ecological Informatics},
volume = {69},
pages = {101657},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101657},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122001066},
author = {Miklas Riechmann and Ross Gardiner and Kai Waddington and Ryan Rueger and Frederic Fol Leymarie and Stefan Rueger},
keywords = {AI, Computer vision, Machine learning, Motion vectors, Video camera trap, Video pipeline},
abstract = {Commercial camera traps are usually triggered by a Passive Infra-Red (PIR) motion sensor necessitating a delay between triggering and the image being captured. This often seriously limits the ability to record images of small and fast moving animals. It also results in many “empty” images, e.g., owing to moving foliage against a background of different temperature. In this paper we detail a new triggering mechanism based solely on the camera sensor. This is intended for use by citizen scientists and for deployment on an affordable, compact, low-power Raspberry Pi computer (RPi). Our system introduces a video frame filtering pipeline consisting of movement and image-based processing. This makes use of Machine Learning (ML) feasible on a live camera stream on an RPi. We describe our free and open-source software implementation of the system; introduce a suitable ecology efficiency measure that mediates between specificity and recall; provide ground-truth for a video clip collection from camera traps; and evaluate the effectiveness of our system thoroughly. Overall, our video camera trap turns out to be robust and effective.}
}
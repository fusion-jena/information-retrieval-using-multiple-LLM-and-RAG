@article{CHAWLA2024102548,
title = {MobileNet-GRU fusion for optimizing diagnosis of yellow vein mosaic virus},
journal = {Ecological Informatics},
volume = {81},
pages = {102548},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102548},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000906},
author = {Tisha Chawla and Shubh Mittal and Hiteshwar Kumar Azad},
keywords = {Yellow vein mosaic virus, MobileNet, Gated recurrent unit, Recurrent neural networks, Transfer learning, Deep learning},
abstract = {Yellow vein mosaic virus (YVMV) is a destructive plant virus that commonly affects crops, particularly okra, in India. The virus is transmitted by whiteflies and poses significant challenges to agricultural productivity. Infection with YVMV leads to distinct yellow vein patterns on leaves, stunted growth, reduced yield, and ultimately economic losses for farmers. Timely and accurate detection of YVMV is crucial for effective disease management. In this article, we present a novel method that employs advanced deep-learning models to identify YVMV-infected okra plants. The study leverages a dataset of over 2000 okra plant leaves that implements transfer learning models, including MobileNet, EfficientNet, InceptionV3, VGG19, InceptionResNetV2, and ResNet50 and recurrent neural networks (RNN) variants, including Long short-term memory (LSTM), Bidirectional long short-term memory (BiLSTM) and Gated recurrent unit (GRU). Additionally, three hybrid models, combining MobileNet with LSTM, BiLSTM, and GRU, are incorporated to capitalize on the characteristics of both MobileNet and RNNs through superior feature extraction and detection of temporal dependencies. The results demonstrate that the MobileNet model combined with all three RNNs achieves exceptional accuracy, surpassing 99.27%. Notably, the MobileNet model integrated with GRU exhibits the most optimized performance with the least loss and greatest accuracy, facilitating improved disease management strategies and aiding in the yield of crops by reducing the impact of YVMV.}
}
@article{VIZCARRA2021101268,
title = {The Peruvian Amazon forestry dataset: A leaf image classification corpus},
journal = {Ecological Informatics},
volume = {62},
pages = {101268},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101268},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000595},
author = {Gerson Vizcarra and Danitza Bermejo and Antoni Mauricio and Ricardo {Zarate Gomez} and Erwin Dianderas},
keywords = {Leaves dataset, Peruvian Amazon, Deep learning, Visual interpretation, Interpretation},
abstract = {Forest census allows getting precise data for logging planning and elaboration of the forest management plan. Species identification blunders carry inadequate forest management plans and high risks inside forest concessions. Hence, an identification protocol prevents the exploitation of non-commercial or endangered timber species. The current Peruvian legislation allows the incorporation of non-technical experts, called “materos”, during the identification. Materos use common names given by the folklore and traditions of their communities instead of formal ones, which generally lead to misclassifications. In the real world, logging companies hire materos instead of botanists due to cost/time limitations. Given such a motivation, we explore an end-to-end software solution to automatize the species identification. This paper introduces the Peruvian Amazon Forestry Dataset, which includes 59,441 leaves samples from ten of the most profitable and endangered timber-tree species. The proposal contemplates a background removal algorithm to feed a pre-trained CNN by the ImageNet dataset. We evaluate the quantitative (accuracy metric) and qualitative (visual interpretation) impacts of each stage by ablation experiments. The results show a 96.64% training accuracy and 96.52% testing accuracy on the VGG-19 model. Furthermore, the visual interpretation of the model evidences that leaf venations have the highest correlation in the plant recognition task.}
}
@article{KATH2024102710,
title = {Leveraging transfer learning and active learning for data annotation in passive acoustic monitoring of wildlife},
journal = {Ecological Informatics},
volume = {82},
pages = {102710},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102710},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002528},
author = {Hannes Kath and Patricia P. Serafini and Ivan B. Campos and Thiago S. Gouvêa and Daniel Sonntag},
keywords = {Active learning, Transfer learning, Passive acoustic monitoring, BirdNet},
abstract = {Passive Acoustic Monitoring (PAM) has emerged as a pivotal technology for wildlife monitoring, generating vast amounts of acoustic data. However, the successful application of machine learning methods for sound event detection in PAM datasets heavily relies on the availability of annotated data, which can be laborious to acquire. In this study, we investigate the effectiveness of transfer learning and active learning techniques to address the data annotation challenge in PAM. Transfer learning allows us to use pre-trained models from related tasks or datasets to bootstrap the learning process for sound event detection. Furthermore, active learning promises strategic selection of the most informative samples for annotation, effectively reducing the annotation cost and improving model performance. We evaluate an approach that combines transfer learning and active learning to efficiently exploit existing annotated data and optimize the annotation process for PAM datasets. Our transfer learning observations show that embeddings produced by BirdNet, a model trained on high signal-to-noise recordings of bird vocalisations, can be effectively used for predicting anurans in PAM data: a linear classifier constructed using these embeddings outperforms the benchmark by 21.7%. Our results indicate that active learning is superior to random sampling, although no clear winner emerges among the strategies employed. The proposed method holds promise for facilitating broader adoption of machine learning techniques in PAM and advancing our understanding of biodiversity dynamics through acoustic data analysis.}
}
@article{HUNTER2023102076,
title = {Using hierarchical text classification to investigate the utility of machine learning in automating online analyses of wildlife exploitation},
journal = {Ecological Informatics},
volume = {75},
pages = {102076},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102076},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300105X},
author = {Sara Bronwen Hunter and Fiona Mathews and Julie Weeds},
keywords = {Machine learning, Natural language processing, iEcology, Wildlife exploitation, Digital conservation, Social media},
abstract = {Expanding digital data sources, including social media, online news articles and blogs, provide an opportunity to understand better the context and intensity of human-nature interactions, such as wildlife exploitation. However, online searches encompassing large taxonomic groups can generate vast datasets, which can be overwhelming to filter for relevant content without the use of automated tools. The variety of machine learning models available to researchers, and the need for manually labelled training data with an even balance of labels, can make applying these tools challenging. Here, we implement and evaluate a hierarchical text classification pipeline which brings together three binary classification tasks with increasingly specific relevancy criteria. Crucially, the hierarchical approach facilitates the filtering and structuring of a large dataset, of which relevant sources make up a small proportion. Using this pipeline, we also investigate how the accuracy with which text classifiers identify relevant and irrelevant texts is influenced by the use of different models, training datasets, and the classification task. To evaluate our methods, we collected data from Facebook, Twitter, Google and Bing search engines, with the aim of identifying sources documenting the hunting and persecution of bats (Chiroptera). Overall, the ‘state-of-the-art’ transformer-based models were able to identify relevant texts with an average accuracy of 90%, with some classifiers achieving accuracy of >95%. Whilst this demonstrates that application of more advanced models can lead to improved accuracy, comparable performance was achieved by simpler models when applied to longer documents and less ambiguous classification tasks. Hence, the benefits from using more computationally expensive models are dependent on the classification context. We also found that stratification of training data, according to the presence of key search terms, improved classification accuracy for less frequent topics within datasets, and therefore improves the applicability of classifiers to future data collection. Overall, whilst our findings reinforce the usefulness of automated tools for facilitating online analyses in conservation and ecology, they also highlight that the effectiveness and appropriateness of such tools is determined by the nature and volume of data collected, the complexity of the classification task, and the computational resources available to researchers.}
}
@article{GOMEZVARGAS2023102036,
title = {Re-identification of fish individuals of undulate skate via deep learning within a few-shot context},
journal = {Ecological Informatics},
volume = {75},
pages = {102036},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102036},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000651},
author = {Nuria Gómez-Vargas and Alexandre Alonso-Fernández and Rafael Blanquero and Luis T. Antelo},
keywords = {Deep learning, Few-shot learning, Photo-identification, Siamese networks},
abstract = {Individual re-identification is critical to track population changes in order to assess status, being particularly relevant in species with conservation concerns and difficult access like marine organisms. For this, we propose photo-identification via deep learning as a non-invasive technique to discriminate between individuals of the undulate skate (Raja undulata). Nevertheless, accruing enough training samples might be difficult to achieve in the case of underwater fish images. We develop a novel methodology based on a siamese neural network that incorporates statistical fundamentals as motivation to overcome the few-shot context. Our work provides a hands-on experience and highlights on pitfalls when trying to apply photo-identification in a limited scenario, concerning both data quantity and quality, yet providing remarkable results over the test set including recaptures, where the model is capable of correctly identifying the 70% of the individuals. The findings of this study can be of strong impact for the research teams becoming familiar with deep learning approaches, as it can be easily extended to re-identify individuals of other marine species of interest from a conservation or exploitation point of view.}
}
@article{WHITE2023102363,
title = {One size fits all? Adaptation of trained CNNs to new marine acoustic environments},
journal = {Ecological Informatics},
volume = {78},
pages = {102363},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102363},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003928},
author = {Ellen L. White and Holger Klinck and Jonathan M. Bull and Paul R. White and Denise Risch},
keywords = {Bioacoustics, Deep learning, Domain adaptation, Marine acoustics, Marine mammal detection, Soundscapes},
abstract = {Convolutional neural networks (CNNs) have the potential to enable a revolution in bioacoustics, allowing robust detection and classification of marine sound sources. As global Passive Acoustic Monitoring (PAM) datasets continue to expand it is critical we improve our confidence in the performance of models across different marine environments, if we are to exploit the full ecological value of information within the data. This work demonstrates the transferability of developed CNN models to new acoustic environments by using a pre-trained model developed for one location (West of Scotland, UK) and deploying it in a distinctly different soundscape (Gulf of Mexico, USA). In this work transfer learning is used to fine-tune an existing open-source ‘small-scale’ CNN, which detects odontocete tonal and broadband call types and vessel noise (operating between 0 and 48 kHz). The CNN is fine-tuned on training sets of differing sizes, from the unseen site, to understand the adaptability of a network to new marine acoustic environments. Fine-tuning with a small sample of site-specific data significantly improves the performance of the CNN in the new environment, across all classes. We demonstrate an improved performance in area-under-curve (AUC) score of 0.30, across four classes by fine-training with only 50 spectrograms per class, with a 5% improvement in accuracy between 50 frames and 500 frames. This work shows that only a small amount of site-specific data is needed to retrain a CNN, enabling researchers to harness the power of existing pre-trained models for their own datasets. The marine bioacoustic domain will benefit from a larger pool of global data for training large deep learning models, but we illustrate in this work that domain adaptation can be improved with limited site-specific exemplars.}
}
@article{MCEWEN2024102734,
title = {Active few-shot learning for rare bioacoustic feature annotation},
journal = {Ecological Informatics},
volume = {82},
pages = {102734},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102734},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002760},
author = {Ben McEwen and Kaspar Soltero and Stefanie Gutschmidt and Andrew Bainbridge-Smith and James Atlas and Richard Green},
keywords = {Active learning, Audio classification, Bioacoustics, Few-shot learning, Machine learning},
abstract = {The collection and annotation of bioacoustic data present several challenges to researchers. Bioacoustic monitoring of rare (sparse) or cryptic species generally encounter two main issues. The cost of collecting and processing field data and a lack of labelled datasets for the target species. The detection of invasive species incursions and probability of absence testing is especially challenging due to these species having population densities at or close to zero. We present a methodology specifically designed to aid in the analysis of rare acoustic events within long-term field recordings. This approach combines a wavelet-based segmentation method that automatically extracts transient features from within-field recordings. A few-shot active learning recommender system in a human-in-the-loop process prioritises the annotation of low-certainty samples. This process combines the accuracy of human classification and the speed of computational tools to greatly reduce the presence of non-target features in field recordings. We evaluate this approach using an invasive species identification case study. This methodology achieves a test accuracy of 98.4% as well as 81.2% test accuracy using 2-shot, 2-way prototypical learning without fine-tuning, demonstrating high performance at varying data availability contexts. Active learning using low-certainty samples achieves >90% test accuracy using only 20 training samples compared to 80 samples without active learning. This approach allows users to train custom audio classification models for any application with rare features. The model can be easily exported for use in the field making real-time bioacoustic monitoring of less-vocal species a possibility. All code and data are available at https://github.com/Listening-Lab/Annotator.}
}
@article{BERTOLINI2022101659,
title = {Using a clustering algorithm to identify patterns of valve-gaping behaviour in mussels reared under different environmental conditions},
journal = {Ecological Informatics},
volume = {69},
pages = {101659},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101659},
url = {https://www.sciencedirect.com/science/article/pii/S157495412200108X},
author = {C. Bertolini and J. Capelle and E. Royer and M. Milan and R. Witbaard and T.J. Bouma and R. Pastres},
keywords = {Bivalves, K-means, Precision shellfish aquaculture, Transitional ecosystem, Venice lagoon, Wadden Sea},
abstract = {Physiological adaptations for inhabiting transitional environments with strongly variable abiotic conditions can sometimes be displayed as behavioural shifts. A striking example might be found in bivalve species that inhabit estuaries characterised by fluctuations in environment. The opening and closing of their valves, so called gaping activity, represents behaviour that is required for two key physiological functions: food intake and respiration. Linking valve-gaping behaviour to environmental drivers can greatly improve our understanding and modelling of bivalve bioenergetics. Nowadays large data sets on gaping activity can be collected with automated sensors, but interpretation is difficult due to the large amount of environmental drivers and the intra-individual variability. This study aims to understand whether an unsupervised machine learning method (k-means clustering) can be used to identify patterns in gaping activity. Two commercially important congener mussels, Mytilus galloprovincialis and Mytilus edulis inhabiting two transitional coastal areas, the Venice Lagoon and the Wadden Sea, were fitted with sensors to monitor valve-gaping, while a comprehensive set of environmental parameters was also monitored. Data were analysed by applying three times a k-mean algorithm to the gaping time series. In the 1st analyses, the algorithm was applied to the overall gaping time series, including daily variations. We identified at both sites three clusters that were characterised by different average daily gaping aperture. The algorithm was subsequently reapplied to relate daily means of gaping to environmental conditions, being temperatures, oxygen saturation and chlorophyll levels. This 2nd analyses revealed that mean gaping aperture was mainly linked to food availability. A 3rd follow-up analysis aimed at exploring daily patterns. This third analysis again revealed consistent patterns amongst the two sites, where two clusters emerged that showed different degrees of oscillatory behaviour. There was however no obvious relationship between this fine scale oscillatory behaviours and environmental variables, but in the Venice Lagoon there was a site effect. Overall, we show that clustering algorithms can disentangle behavioural patterns within complex series of big data. The latter offers new opportunities to improve site-specific bioenergetic bivalve models by rephrasing the clearance and respiration terms based on the mean gaping aperture, provided that further laboratory experimentations are conducted to extrapolate parameters linking aperture with energy inputs and outputs.}
}
@article{JAMALI2022101904,
title = {3DUNetGSFormer: A deep learning pipeline for complex wetland mapping using generative adversarial networks and Swin transformer},
journal = {Ecological Informatics},
volume = {72},
pages = {101904},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101904},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003545},
author = {Ali Jamali and Masoud Mahdianpari and Brian Brisco and Dehua Mao and Bahram Salehi and Fariba Mohammadimanesh},
keywords = {Generative adversarial network, Convolutional neural networks, Wetland mapping, Vision transformers, Deep learning, Swin transformer},
abstract = {Many ecosystems, particularly wetlands, are significantly degraded or lost as a result of climate change and anthropogenic activities. Simultaneously, developments in machine learning, particularly deep learning methods, have greatly improved wetland mapping, which is a critical step in ecosystem monitoring. Yet, present deep and very deep models necessitate a greater number of training data, which are costly, logistically challenging, and time-consuming to acquire. Thus, we explore and address the potential and possible limitations caused by the availability of limited ground-truth data for large-scale wetland mapping. To overcome this persistent problem for remote sensing data classification using deep learning models, we propose 3D UNet Generative Adversarial Network Swin Transformer (3DUNetGSFormer) to adaptively synthesize wetland training data based on each class's data availability. Both real and synthesized training data are then imported to a novel deep learning architecture consisting of cutting-edge Convolutional Neural Networks and vision transformers for wetland mapping. Results demonstrated that the developed wetland classifier obtained a high level of kappa coefficient, average accuracy, and overall accuracy of 96.99%, 97.13%, and 97.39%, respectively, for the data in three pilot sites in and around Grand Falls-Windsor, Avalon, and Gros Morne National Park located in Canada. The results show that the proposed methodology opens a new window for future high-quality wetland data generation and classification. The developed codes are available at https://github.com/aj1365/3DUNetGSFormer.}
}
@article{CANOVI2024102733,
title = {Trajectory-based fish event classification through pre-training with diffusion models},
journal = {Ecological Informatics},
volume = {82},
pages = {102733},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102733},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002759},
author = {Noemi Canovi and Benjamin A. Ellis and Tonje K. Sørdalen and Vaneeda Allken and Kim T. Halvorsen and Ketil Malde and Cigdem Beyan},
keywords = {Fish behavior, Underwater videos, Event recognition, Trajectory, Generative models, Autoencoder, Diffusion model, Corkwing wrasse},
abstract = {This study contributes to advancing the field of automatic fish event recognition in natural underwater videos, addressing the current gap in studying fish interaction and competition, including predator-prey relationships and mating behaviors. We used the corkwing wrasse (Symphodus melops) as a model, a marine species of commercial importance that reproduces in sea-weed nests built and cared for by a single male. These nests attract a wide range of visitors and are the focal point for behavior such as spawning, chasing, and maintenance. We propose a deep learning methodology to analyze the movement trajectories of the nesting male and classify the associated events observed in their natural habitat. Our approach leverages unsupervised pre-training based on diffusion models, leading to improved feature learning. Additionally, we introduce a dataset comprising 16,937 trajectories across 12 event classes, making it the largest in terms of event class diversity. Our results demonstrate the superior performance of our method compared to several deep architectures. The code for the proposed method and the trajectories can be found at https://github.com/NoeCanovi/Fish_Behaviors_Generative_Models.}
}
@article{GONCALVES2024102628,
title = {Revealing forest structural "fingerprints": An integration of LiDAR and deep learning uncovers topographical influences on Central Amazon forests},
journal = {Ecological Informatics},
volume = {81},
pages = {102628},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102628},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001705},
author = {Nathan Borges Gonçalves and Diogo Martins Rosa and Dalton Freitas {do Valle} and Marielle N. Smith and Ricardo Dalagnol and Danilo Roberti Alves {de Almeida} and Bruce W. Nelson and Scott C. Stark},
keywords = {Structural "fingerprints", LiDAR, Deep learning, Amazon Forest, Terrain variation},
abstract = {Amazon forests are characterized by rich structural diversity. However, the influence of factors such as topography, soil attributes, and external disturbances on structural variability is not always well characterized, and traditional structural metrics may be inadequate to capture this type of complexity. While LiDAR offers expanded structural insights, traditional parameters used in LiDAR analysis, such as mean or maximum canopy height, are not always well directly linked to environmental variables like topography. Emerging approaches merge LiDAR with machine learning to uncover deeper structural complexities. However, work to date may fail to fully utilize the potential of fine-scale LiDAR information. Here we introduce a novel approach, leveraging 2D point cloud images derived from a profiling canopy LiDAR (PCL). The technique targets intricate details within LiDAR point clouds by using deep learning algorithms. With a dataset from the Central Amazon comprising 18 multitemporal transects of 450 m in length, our objective was to detect structural "fingerprints" of varied topographical types along a hillslope, comprising: Riparian, White-sand, and Plateau, and to detect any gradient of structural shifts based on terrain variations here represented by the height above the nearest drainage (HAND). The dataset was trained and tested using a leave-one-group-out approach (LOGO) in which, for each iteration, a complete 450 m multitemporal transect was excluded from training and tested after each iteration. The fast.ai platform and a ResNet-34 architecture, coupled with transfer learning, were used to perform a classification to distinguish between three topographical types. Furthermore, a hybrid model combining a Convolutional Autoencoder, and Partial Least Square (PLS) regression was designed to detect forest structural gradient correlations with HAND variation. Cross-validation achieved a promising high weighted F1 score of 0.83 to classify forests based on the topographical types. Additionally, a combined Convolutional Autoencoder and PLS regression revealed a strong correlation (R2 = 0.76) between actual and predicted HAND. Innovatively combining deep learning with ground-based PCL LiDAR, our study revealed unique Amazon Forest structures connected to topographic variation. Our findings underscore the transformative potential of such integrative approaches for investigating forest dynamics and promise a powerful new tool for understanding climate-related forest structure change.}
}
@article{HU2024102640,
title = {Capturing urban green view with mobile crowd sensing},
journal = {Ecological Informatics},
volume = {81},
pages = {102640},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102640},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001821},
author = {Yingqiang Hu and Yue Wu and Zhuzi Tantian and Guodong Sun},
keywords = {Urban green spaces, Green view index, Deep learning, Mobile crowd sensing},
abstract = {Urban green spaces are beneficial to ecosystems and the health of people. The Green View Index (GVI) is an essential metric for assessing urban green spaces from a human perspective. However, measuring GVI at an urban scale requires extensive collection and processing of sensing data, posing challenges in terms of high resource consumption, difficulty in implementation, and lack of user participation. Mobile crowd Sensing (MCS) is an emerging large-scale, low-cost solution for sensing data collection. To address the aforementioned issues, this study proposes an MCS system called GreenCam to measure the GVI with smartphone sensors. GreenCam guides users to capture photos of urban green spaces from human perspective. The system employs a Transformer-based model, which is trained on a customized dataset of 1200 carefully-labeled urban green images, to extract the greenery from the captured photos and calculate GVI. With widespread participation from urban users, the photos captured by users with GreenCam can cover various streets and areas of the city, enabling the measurement of GVI at an urban scale. Additionally, these photos reflect people's preferences towards specific urban landscapes, and analyzing the distribution and characteristics of popular landscapes contributes to the enhancement of urban ecosystems and landscapes.}
}
@article{ZHOU2024102680,
title = {Real-time underwater object detection technology for complex underwater environments based on deep learning},
journal = {Ecological Informatics},
volume = {82},
pages = {102680},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102680},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400222X},
author = {Hui Zhou and Meiwei Kong and Hexiang Yuan and Yanyan Pan and Xinru Wang and Rong Chen and Weiheng Lu and Ruizhi Wang and Qunhui Yang},
keywords = {Underwater object detection, You only look once, Cross stage multi-branch, Large kernel spatial pyramid},
abstract = {Underwater object detection technology is crucial in many marine-related fields, including marine environmental monitoring, marine resource development, and marine ecological protection. However, this technology faces great challenges due to the poor quality of underwater optical images and the varying sizes of underwater objects. Therefore, we proposed an underwater optical detection network (UODN) based on the you only look once version 8 (YOLOv8) framework, which addresses these issues through the cross stage multi-branch (CSMB) module and large kernel spatial pyramid (LKSP) module. The aim of the CSMB module is to extract more features from underwater optical images to address the issue of poor image quality, while the LKSP module is designed to enhance the ability of the network to detect underwater objects of various scales. Furthermore, CSMBDarknet built by CSMB and LKSP can be used as the backbone of other underwater object detection algorithms for underwater feature extraction. Extensive experimental results on the underwater robot professional contest 2020 dataset revealed that the average precision (AP) of UODN increased by 1.0%, the AP50 of UODN increased by 1.1%, and the AP75 of UODN increased by 2.1% compared with those of the original YOLOv8s. Furthermore, UODN outperforms 12 state-of-the-art models on multiple underwater optical datasets, paving the way for future real-time and high-precision underwater object detection.}
}
@article{KWON2024102588,
title = {Estimation of aquatic ecosystem health using deep neural network with nonlinear data mapping},
journal = {Ecological Informatics},
volume = {81},
pages = {102588},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102588},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001304},
author = {Yong Sung Kwon and Hyeongsik Kang and JongCheol Pyo},
keywords = {Deep learning, Aquatic ecosystem health index, Autoencoder, Convolutional neural network},
abstract = {Estimation of aquatic ecosystem health indices can assist in reducing the burden of time-consuming, labor-intensive, and cost-effective fieldwork for the sustainable evaluation of freshwater ecosystem status. In this study, we developed a deep neural network to estimate the trophic diatom index (TDI), benthic macroinvertebrate index (BMI), and fish assessment index (FAI) using water quality and hydraulic and hydrological data. A convolutional neural network (CNN) model was built to estimate health indices. In addition, an autoencoder was adopted to produce manifold features that were used as inputs for the CNN model. Conventional machine learning models, including artificial neural networks, support vector machines, random forests, and extreme gradient boosting, have been developed to estimate the TDI, BMI, and FAI. The results showed that the CNN with an autoencoder exhibited the best performance, with validation accuracies of Nash Sutcliffe Efficiency (NSE) and root mean squared error (RMSE) values of 0.592 and 17.249 for TDI, 0.669 and 12.282 for BMI, and 0.638 and 13.897 for FAI, respectively. The autoencoder enhanced the nonlinear feature learning of the time series and static input data, which contributed to improving the CNN feature extraction for accurate estimation of aquatic ecosystem health indices compared to other data-driven approaches. Therefore, deep learning techniques can be used to investigate aquatic ecosystem health by successfully reflecting the quantitative and qualitative features of health indices.}
}
@article{LAKDARI2024102457,
title = {Mel-frequency cepstral coefficients outperform embeddings from pre-trained convolutional neural networks under noisy conditions for discrimination tasks of individual gibbons},
journal = {Ecological Informatics},
volume = {80},
pages = {102457},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102457},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004867},
author = {Mohamed Walid Lakdari and Abdul Hamid Ahmad and Sarab Sethi and Gabriel A. Bohn and Dena J. Clink},
keywords = {Vocal individuality, Sound feature extraction, Mel-frequency cepstral coefficients, Convolutional neural networks, Acoustic indices, },
abstract = {Passive acoustic monitoring – an approach that utilizes autonomous acoustic recording units – allows for non-invasive monitoring of individuals, assuming that it is possible to acoustically distinguish individuals. However, identifying effective analytical approaches for individual identification remains a challenge. Our study investigates how the use of different feature representations impacts our ability to distinguish between individual female Northern grey gibbons (Hylobates funereus). We broadcast pre-recorded calls from twelve gibbon females and re-recorded the calls at varying distances (directly under the tree to ∼400 m away) using autonomous recording units. We evaluated the effectiveness of using different automated feature extraction approaches to classify gibbon calls: Mel-frequency cepstral coefficients (MFCCs), embeddings from three pre-trained neural networks (BirdNET, VGGish, and Wav2Vec2), and four commonly used acoustic indices. We used a supervised classification approach (random forest) to classify calls to the respective female and compared two unsupervised clustering approaches (affinity propagation clustering and hierarchical density-based spatial clustering) to evaluate which features were most effective for distinguishing female calls without using class labels. We used MFCCs as a baseline as previous work has shown they can be used to distinguish high-quality calls of individual gibbon females. Human annotators could only identify calls in spectrograms from recordings <350 m from the playback speaker with signal-to-noise ratio ∼ 0 dB, so our results focus on these recordings. Using supervised classification, our results confirmed the efficiency of MFCCs and the use of embeddings from one neural network (BirdNET) for effective acoustic classification of gibbon individuals at closer recording distances (signal-to-noise ratio > 10 dB), while the remaining features did not perform well. Contrary to our expectations, we found that MFCCs outperformed all other features for the unsupervised clustering tasks at closer distances and none of the features performed well at farther distances. The ability to acoustically discriminate animals under noisy conditions and from low signal-to-noise ratio calls has important implications for monitoring populations of endangered animals, such as gibbons. Focusing only on high signal-to-noise ratio calls for individual discrimination may not be possible for rare sounds, and future work should focus on developing effective approaches of feature extraction that can perform well across noisy, real-world conditions with a limited number of training samples.}
}
@article{YANG2024102527,
title = {A systematic study on transfer learning: Automatically identifying empty camera trap images using deep convolutional neural networks},
journal = {Ecological Informatics},
volume = {80},
pages = {102527},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102527},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000694},
author = {Deng-Qi Yang and De-Yao Meng and Hao-Xuan Li and Meng-Tao Li and Han-Lin Jiang and Kun Tan and Zhi-Pang Huang and Na Li and Rong-Hai Wu and Xiao-Wei Li and Ben-Hui Chen and Mei Zhang and Guo-Peng Ren and Wen Xiao},
keywords = {Transfer learning, Camera trap images, ResNext-101, Updating layer selection, Image recognition},
abstract = {Transfer learning is extensively utilized for automatically recognizing and filtering out empty camera trap images that lack animal presence. Current research that uses transfer learning for identifying empty images typically solely updates the fully connected layer of models, and they usually select a pre-trained source model only based on its relevance to the target task. However, they do not consider the optimization of update layer selection, nor do they investigate the effect of sample size and class number of source domain data set used to construct the source model on the performance of the transfer model. Both of these are issues worth exploring. We answered these two issues using three different datasets and the ResNext-101 model. Our experimental results showed that when using 20,000 training samples to transfer the model from the ImageNet dataset to the Snapshot Serengeti dataset, our proposed optimal update layers improved the accuracy of the transfer model from 92.9% to 95.5% (z = −7.087, p < 0.001, N = 8118) compared to the existing method of updating only the fully connected layer. A similar improvement was observed when transferring the model from ImageNet to the Lasha Mountain dataset. Additionally, our results indicated that when using 20,000 training samples to update the pre-trained model and increasing the sample size of the binary-class training dataset used to build the source model from 100,000 to 1 million, the accuracy of the transfer model improved from 90.4% to 93.5% (z = −3.869, p < 0.001, N = 8948). Similar results were obtained when constructing the source domain dataset using ten classifications. Based on these results, we drew the following conclusions: (1) using our proposed optimal update layers instead of the commonly used method of updating only the fully connected layers can significantly improve the model's performance. (2) The optimal update layers varied when the model transferred from different source domain datasets to the same target dataset. (3) The number of classes in the source domain dataset did not significantly impact the transfer model performance. However, the sample size of the source domain dataset positively correlated with the transfer model performance, and there might be a threshold effect.}
}
@article{CHEN2024102693,
title = {Weight-based ensemble method for crop pest identification},
journal = {Ecological Informatics},
volume = {82},
pages = {102693},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102693},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002358},
author = {Miao Chen and Jianji Wang and Yanan Chen and Minghui Guo and Nanning Zheng},
keywords = {Crop pest identification, Deep learning, Ensemble method, Convex optimization},
abstract = {Crop pests cause significant losses to agricultural production. Pests can be detected and controlled over time using accurate and effective methods, thereby reducing potential losses. However, there are challenges in realistic agricultural scenarios, such as diverse pest species and complicated environments, which render manual recognition and conventional machine learning methods insufficient. To address this issue, deep learning methods that can automatically extract features have recently been widely used for pest identification. However, accurately recognizing images that resemble complex real-world scenarios remains a challenging task for a single deep learning model. The ensemble method, which combines multiple basic models, provides a solution for improving recognition performance. In this study, we proposed two weight-based ensemble methods, VecEnsemble and MatEnsemble, constructed from vector- and matrix-based weights, respectively. The weights that combine basic models significantly influence the performance of the ensemble methods. Therefore, to effectively combine the basic models, we formulated the weight design problem as a quadratic convex optimization problem whose solution has a closed-form expression and can be computed efficiently. Our method achieved the highest accuracy of 77.39% on the large-scale complex-scene IP102 dataset, which was competitive with those of other state-of-the-art methods. Furthermore, we conducted comprehensive ablation experiments to compare our proposed methods with voting-based approaches and illustrate the scenarios in which they are applicable. These results highlight the practical significance of our method for agricultural production and provide a foundation for further research on crop pest identification. The source code is available at https://github.com/shiguangqianmo/WBEnsemble.}
}
@article{CARRIGER2024102665,
title = {Exploring coral reef communities in Puerto Rico using Bayesian networks},
journal = {Ecological Informatics},
volume = {82},
pages = {102665},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102665},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002073},
author = {John F. Carriger and William S. Fisher},
keywords = {Coral reefs, Community ecology, Bayesian networks, Cluster analysis},
abstract = {Most coral reef studies focus on scleractinian (stony) corals to indicate reef condition, but there are other prominent assemblages that play a role in ecosystem structure and function. In Puerto Rico these include fish, gorgonians, and sponges. The U.S. Environmental Protection Agency conducted unique surveys of coral reef communities across the southern coast of Puerto Rico that included simultaneous measurement of all four assemblages. Evaluating the results from a community perspective demands endpoints for all four assemblages, so patterns of community structure were explored by probabilistic clustering of measured variables with Bayesian networks. Most variables were found to have stronger associations within than between taxa, but unsupervised structure learning identified three cross-taxa relationships with potential ecological significance. Clusters for each assemblage were constructed using an expectation-maximization algorithm that created a factor node jointly characterizing the density, size, and diversity of individuals in each taxon. The clusters were characterized by the measured variables, and relationships to variables for other taxa were examined, such as stony coral clusters with fish variables. Each of the factor nodes were then used to create a set of meta-factor clusters that further summarized the aggregate monitoring variables for the four taxa. Once identified, taxon-specific and meta-clusters represent patterns of community structure that can be examined on a regional or site-specific basis to better understand risk assessment, risk management and delivery of ecosystem services.}
}
@article{HERDY2024102417,
title = {Utilization of deep learning tools to map and monitor biological soil crusts},
journal = {Ecological Informatics},
volume = {79},
pages = {102417},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102417},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004466},
author = {Stefan Herdy and Emilio Rodríguez-Caballero and Thomas Pock and Bettina Weber},
keywords = {Semantic segmentation, Joint energy-based modelling, Deep learning, Biological soil crust, Domain adaption, Neural network, Long-term monitoring},
abstract = {Biological soil crusts (biocrusts) form a layer of only one to few centimeters depth on the soil surface and occur mostly in hot and cold deserts. Biocrusts have a major impact on different processes in these ecosystems, like carbon and nitrogen cycling, biodiversity preservation, erosion protection and soil dust emission reduction, but also react highly sensitive upon climate alterations and land use intensification. Therefore, monitoring tools are required to keep track of the changes of these specialized communities in an altering environment. In the current study, we applied a semantic image segmentation approach, using neural networks. One main problem to be solved was, that the training data and target data, on which the model is applied, are often recorded with different camera devices. This leads to different statistical properties of the image data, like different scale, resolution, brightness etc., which could significantly affect the model's performance. To solve this problem, we propose a new domain adaption method using a joint energy-based approach. To test a semantic segmentation approach in general, we utilized biocrust imagery taken in Utah (United States of America) and two sub datasets from the National Park Gesäuse (Austria). Here, we achieved highly reliable results with an overall classification accuracy of 85.9% for the USA data and 88.6% and 91.4%, respectively, for the two sub datasets of the National Park Gesäuse. To test our joint energy-based domain adaption approach, we used the two sub datasets from the National Park Gesäuse, which were recorded with different camera devices. With this newly established approach, we improved the accuracy of our segmentation on the unlabeled sub dataset from 70.4% to 75.3%. The results suggest that joint energy-based modelling is a well-suited domain adaption method for semantic segmentation that could be applied to face various deep learning and image-based biomonitoring challenges.}
}
@article{PELE2024102463,
title = {A neural network encoder-decoder for time series prediction: Application on 137Cs particulate concentrations in nuclearized rivers},
journal = {Ecological Informatics},
volume = {80},
pages = {102463},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102463},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000050},
author = {Kathleen Pelé and Valérie Nicoulaud-Gouin and Hugo Lepage},
keywords = {Deep learning, Suspended particulate matter, Radioactivity, Micropollutant, Data-driven model},
abstract = {Monitoring the impact of human activities on the environment is a major challenge as many pollutants can be found in the different ecosystems. This is the case of the caesium-137 that has been present in the environment for many decades as a result of atmospheric tests, accidents such as Chernobyl and release from nuclear industries. With the recent advance in data-driven models, this study evaluate the relevance of a deep learning tool for reconstructing caesium-137 chronics particulate concentration in rivers. An encoder-decoder neural network, “Hierarchical Attention-Based Recurrent Highway Networks”(HRHN), is proposed notably for its ability to extract the most relevant temporal and spatial information from the databases. Three monitoring stations were studied, one on the Rhône River and two on the Loire River, all of them downstream nuclear industries in these catchments affected by the global fallout and the accident of Chernobyl. The objective is to predict the future concentration from a set of variables providing past information on water discharge, washout flux and industrial radioactive releases. Once optimised, the model generates first results in agreement with the real concentration curves by correctly following the main trends, with a NSE of 0.89, 0.53 and 0.35 respectively for the Rhone station and the two stations on the Loire. The main reason of inaccuracies is due to the quantity of data available. The originality of this model is its capacity to make predictions on different catchment areas. In fact the training was conducted on the Rhône station as the range of the concentration was higher (from 265.4 to 2700.0 Bq/kg) and the testing on the two Loire station. Another encoder-decoder model DA-RNN (Dual-Stage Attention-Based Recurrent Neural Network) was also evaluate in order to compare the performance of an alternative architecture, without convolution layer. The conclusion is that HRHN remains more powerful in the predictions on the 3 systems. With these first interesting results for HRHN, further investigations should be taken into account for other pollutants than caesium-137 to better understand the robustness of the model.}
}
@article{MCEWEN2023102280,
title = {Automatic noise reduction of extremely sparse vocalisations for bioacoustic monitoring},
journal = {Ecological Informatics},
volume = {77},
pages = {102280},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102280},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003096},
author = {Ben McEwen and Kaspar Soltero and Stefanie Gutschmidt and Andrew Bainbridge-Smith and James Atlas and Richard Green},
keywords = {Audio enhancement, Bioacoustics, Noise reduction, Perceptual quality, Signal processing},
abstract = {Environmental noise and data sparsity present major challenges within the field of bioacoustics. The presence of noise degrades the analysis of audio and field recordings commonly containing large quantities of data with sparse vocalisation features. This work explores noise reduction (audio enhancement) techniques in the context of extremely sparse vocalisations (< 1% occurrence rates) of invasive mammalian and marsupial species, and the clear implications for other bioacoustics applications which face similar challenges. This work compares relevant noise reduction techniques and recommends a spectral subtraction approach. Spectral subtraction achieved a 42.1 dB improvement in signal to noise power (SnNR) and a 2.7 dB improvement in noise variance (SR). We also demonstrate reliable noise reduction at bandwidths up to 250 kHz and efficiency improvements compared to alternative methods. We explore the benefits of deep audio enhancement approaches demonstrating comparable noise reduction with improvements in transient noise reduction but also key limitations such as bandwidth, efficiency and data generation in bioacoustics applications. We identify how the contributions of this work can be applied within the broader context of bioacoustics. All data and code is publicly available at https://github.com/BenMcEwen1/Sparse-Noise-Reduction}
}
@article{RYDHMER2021101456,
title = {Dynamic β-VAEs for quantifying biodiversity by clustering optically recorded insect signals},
journal = {Ecological Informatics},
volume = {66},
pages = {101456},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101456},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121002478},
author = {Klas Rydhmer and Raghavendra Selvan},
keywords = {Unsupervised clustering, VAE, Insect classification, Biodiversity},
abstract = {While insects are the largest and most diverse group of terrestrial animals, constituting ca. 80% of all known species, they are difficult to study due to their small size and similarity between species. Conventional monitoring techniques depend on time consuming trapping methods and tedious microscope-based work by skilled experts in order to identify the caught insect specimen at species, or even family level. Researchers and policy makers are in urgent need of a scalable monitoring tool in order to conserve biodiversity and secure human food production due to the rapid decline in insect numbers. Novel automated optical monitoring equipment can record tens of thousands of insect observations in a single day and the ability to identify key targets at species level can be a vital tool for entomologists, biologists and agronomists. Recent work has aimed for a broader analysis using unsupervised clustering as a proxy for conventional biodiversity measures, such as species richness and species evenness, without actually identifying the species of the detected target. In order to improve upon existing insect clustering methods, we propose an adaptive variant of the variational autoencoder (VAE) which is capable of clustering data by phylogenetic groups. The proposed dynamic β-VAE dynamically adapts the scaling of the reconstruction and regularization loss terms (β value) yielding useful latent representations of the input data. We demonstrate the usefulness of the dynamic β-VAE on optically recorded insect signals from regions of southern Scandinavia to cluster unlabelled targets into possible species. We also demonstrate improved clustering performance in a semi-supervised setting using a small subset of labelled data. These experimental results, in both unsupervised- and semi-supervised settings, with the dynamic β-VAE are promising and, in the near future, can be deployed to monitor insects and conserve the rapidly declining insect biodiversity.}
}
@article{MA2024102651,
title = {UAV equipped with infrared imaging for Cervidae monitoring: Improving detection accuracy by eliminating background information interference},
journal = {Ecological Informatics},
volume = {81},
pages = {102651},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102651},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001936},
author = {Guangkai Ma and Wenjiao Li and Heng Bao and Nathan James Roberts and Yang Li and Weihua Zhang and Kun Yang and Guangshun Jiang},
keywords = {YOLOv7, ViT, Wild Cervidae monitoring},
abstract = {Wild Cervidae(deer and their relatives) play a crucial role in maintaining ecological balance and are integral components of ecosystems. However, factors such as environmental changes and poaching behaviors have resulted in habitat degradation for Cervidae. The protection of wild Cervidae has become urgent, and Cervidae monitoring is one of the key means to ensure the effectiveness of wild Cervidae protection. Object detection algorithms based on deep learning offer promising potential for automatically detecting and identifying animals. However, when those algorithms are used for inference in unseen background environments, there will be a significant decrease in accuracy, especially in the situation that a certain type of Cervidae images are collected from single scene for algorithm training. In this paper, a two-stage localization and classification pipeline for Cervidae monitoring is proposed. The pipeline effectively reduces background interference in Cervidae monitoring and enhances monitoring accuracy. In the first stage, the YOLOv7 network is designed to automatically locate Cervidae in UAV infrared images, while implementing improved bounding box regression through the α-IoU loss function enables the network to locate Cervidae more accurately. Then, Cevdidae objects are extracted to eliminate the background information. In the second stage, a classification network named CA-Hybrid, based on Convolutional Neural Networks(CNN) and Vision Transformer(ViT), as well as Channel Attention Mechanism(CAM) enhances the expression of key features, is constructed to accurately identify Cervidae categories. Experimental results indicate that this method achieves an Average Precision (AP) of 95.9% for Cervidae location and a top-1 accuracy of 77.73% for Cervidae identification. This research contributes to a more comprehensive and accurate monitoring of wild Cervidae, and provides valuable references for subsequent UAV-based wildlife monitoring.}
}
@article{BOHNENSTIEHL2023102268,
title = {Automated cataloging of oyster toadfish (Opsanus tau) boatwhistle calls using template matching and machine learning},
journal = {Ecological Informatics},
volume = {77},
pages = {102268},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102268},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002972},
author = {DelWayne R. Bohnenstiehl},
keywords = {Fish calls, Underwater soundscapes, Call detection, Machine learning},
abstract = {Oyster toadfish (Opsanus tau) represent an ecologically significant species found throughout estuaries along the eastern coast of the United States. While these crevice-dwelling fish can be challenging to observe in their habitats, it is possible to infer their distribution and aspects of their behavior by recording the sounds they produce. The task of cataloging the distinctive advertisement boatwhistle sounds produced by male toadfish to attract females throughout the spring and summer is automated using a multi-step process. Candidate boatwhistles are first identified by template matching using a suite of synthetic spectrogram kernels formed to mimic the two lowest frequency harmonic tones within the boatwhistle. Candidate boatwhistle calls are identified based on the correlation between these kernels and a low-frequency spectrogram of the data. Next, frequency-reassigned spectrogram images of these candidates are formed and input into the pre-trained ResNet-50 convolutional neural network. Finally, the activations from a deep, fully connected layer within this network are extracted and passed to a one-vs-all support-vector-machine classifier, which separates boatwhistles from the larger set of candidate signals. This classifier model was trained and evaluated using a labeled dataset of over 20,000 candidate signals generated over diverse acoustic conditions within Pamlico Sound, North Carolina, USA. The accompanying software provides an effective and efficient tool to monitor boatwhistle calls, which may facilitate a deeper understanding of the spatial distribution, behavioral patterns, and ecological roles played by oyster toadfish.}
}
@article{MORITAKE2024102462,
title = {Sub-alpine shrub classification using UAV images: Performance of human observers vs DL classifiers},
journal = {Ecological Informatics},
volume = {80},
pages = {102462},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102462},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000049},
author = {Koma Moritake and Mariano Cabezas and Tran Thi Cam Nhung and Maximo Larry {Lopez Caceres} and Yago Diez},
keywords = {Sub-alpine vegetation, Vegetation change monitoring, Deep learning, Observer study, ConvNeXt, Swin},
abstract = {In recent years, the automatic analysis of natural environment images acquired with unmanned aerial vehicles (UAV) has rapidly gained popularity. UAVs are specially important in mountainous forests where access is difficult and large areas need to be surveyed. In Zao mountains in northeastern Japan, regenerated fir saplings are competing with sub-alpine vegetation shrubs after a severe fir tree mortality caused by bark beetle infestation. A detailed survey of vegetation distribution is key to improve our understanding of species succession and the influence of climate change in that process. To that end, we evaluated the suitability of deep-learning-based automatic image classification of UAV images in order to map sub-alpine vegetation succession in large areas and the potential of fir regeneration. In order to assess the contribution of this technology in this research field, we first conducted an observer study to assess the difficulty for humans of the task of classifying vegetation from images. Afterwards, we compared the observers' accuracy to four state-of-the art deep learning networks for automatic image classification. The best observer accuracy of 55% demonstrates the limitations of species classification using only images. Furthermore, a detailed analysis of the sources of error showed that even though humans could differentiate between deciduous and evergreen species with an accuracy of 96%, identifying the correct species within each group proved much more challenging. In contrast, deep learning networks achieved accuracy values in the range of 70–80% for species classification, clearly demonstrating capabilities beyond human experts. Our experiments also indicated that the performance of these networks was significantly influenced by the similarity between the datasets used to fine-tune them and evaluate them. This fact highlights the importance of building publicly available images databases to further improve the results. Nevertheless, the results presented in this paper show that the analysis of UAV-acquired with deep learning networks can usher in a new type of large-scale study, spanning tenths or even hundreds of hectares with high spatial resolution (of a few cms per pixel), providing the ability to assess challenging vegetation dynamics problems that go beyond the ability of conventional fieldwork methodologies.}
}
@article{PUSHPA2024102611,
title = {On the importance of integrating convolution features for Indian medicinal plant species classification using hierarchical machine learning approach},
journal = {Ecological Informatics},
volume = {81},
pages = {102611},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102611},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001535},
author = {B.R. Pushpa and N. Shobha Rani and M. Chandrajith and N. Manohar and Smitha Sunil Kumaran Nair},
keywords = {Medicinal plant classification, Fusion features, Convolution features, Smartphone images, Inter-class similarities},
abstract = {This work proposes a novel hierarchical classification framework designed to categorize hundred Indian medicinal plant species. The innovation lies in introducing a comprehensive feature representation by integrating convolutional features with geometric, texture, shape, and multispectral features for classification tasks. In this study, a two-level hierarchical plant classification model is proposed to address the challenges of inter-class similarity and intra-class variations. The first level classifies 100 medicinal plant species into 11 groups based on visual similarities among the plants. At level two, the specific plant species containing in each group are predicted using Random Forest classifier. The evaluation is performed at two levels to analyze the effectiveness of the proposed model. The performance analysis compares the effectiveness of individual feature types against the composite feature model. Performance is also evaluated based on specific groups that demonstrate high similarity between classes and intra-class variations among the plant species separately. Furthermore, the generality of the model is tested using two self-created datasets-RTL80 and RTP40, requiring more than 300 man-hours to collect. Experimental results demonstrate a promising accuracy of 94.54% on GSL100 leaf dataset and 75.46% on RTL80 and RTP40 real-time datasets reflecting the superiority of the proposed hierarchical model over state-of-the-art methods.}
}
@article{AREPALLI2024102405,
title = {Water contamination analysis in IoT enabled aquaculture using deep learning based AODEGRU},
journal = {Ecological Informatics},
volume = {79},
pages = {102405},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102405},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300434X},
author = {Peda Gopi Arepalli and K. Jairam Naik},
keywords = {Water quality, Water contamination, Water contamination index (WCI), Gated recurrent unit (GRU), Internet of things (IoT)},
abstract = {Water contamination presents a significant challenge in aquaculture, impacting the sustainability of ecosystems and the health of aquatic organisms. Precisely assessing water contamination levels is crucial for effective monitoring and safeguarding aquatic life within the aquaculture industry. Traditional methods for evaluating water contamination are characterized by their costliness, time-consuming nature, and susceptibility to errors. Integrating computer technologies such as Artificial Intelligence (AI), the Internet of Things (IoT), and Data Analytics offers promising potential in addressing this issue. Nevertheless, current deep learning solutions have limitations related to data variability, interpretability, and performance. To address these limitations, this study proposes a comprehensive framework that incorporates IoT-based data collection and data segregation techniques to enhance the accuracy of water contamination classification in aquaculture. Real-time data collected through IoT devices, encompassing parameters like temperature, pH levels, dissolved oxygen, nitrate concentration, and other water quality indicators, enables a holistic evaluation of water quality. By considering predefined acceptable ranges for aquatic life, this framework calculates a water contamination index, facilitating the classification of data into categories such as contaminated and non-contaminated. To ensure robust classification, the study introduces an innovative attention-based model known as the Ordinary Differential Equation Gated Recurrent Unit (AODEGRU). This attention mechanism directs the model's focus towards salient features associated with water contamination, while the AODEGRU architecture captures temporal patterns within the data. Experimental results underscore the effectiveness of the proposed model. It demonstrates its superiority with high performance, achieving an accuracy rate of approximately 98.69% on a publicly available dataset and an impressive 99.89% accuracy on a real-time dataset, clearly outperforming existing methodologies.}
}
@article{MATA2024102708,
title = {Drone imagery and deep learning for mapping the density of wild Pacific oysters to manage their expansion into protected areas},
journal = {Ecological Informatics},
volume = {82},
pages = {102708},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102708},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002504},
author = {Aser Mata and David Moffat and Sílvia Almeida and Marko Radeta and William Jay and Nigel Mortimer and Katie Awty-Carroll and Oliver R. Thomas and Vanda Brotas and Steve Groom},
keywords = {Pacific oysters, Invasive species, Convolutional neural networks, Deep learning, Drone, Remote sensing, Ecological management},
abstract = {The recent expansion of wild Pacific oysters already had negative repercussions on sites in Europe and has raised further concerns over their potential harmful impact on the balance of biomes within protected areas. Monitoring their colonisation, especially at early stages, has become an urgent ecological issue. Current efforts to monitor wild Pacific oysters rely on “walk-over” surveys that are highly laborious and often limited to specific areas of easy access. Remotely Piloted Aircraft Systems (RPAS), commonly known as drones, can provide an effective tool for surveying complex terrains and detect Pacific oysters. This study provides a novel workflow for automated detection, counting and mapping of individual Pacific oysters to estimate their density per square meter by using Convolutional Neural Networks (CNNs) applied to drone imagery. Drone photos were collected at low tides and altitudes of approximately 10 m across a variety of cases of rocky shore and mudflats scenarios. Using object detection, we compared how different Convolutional Neural Networks (CNNs) architectures including YOLOv5s, YOLOv5m, TPH-YOLOv5 and FR-CNN performed in the detection of Pacific oysters over the surveyed areas. We report the precision of our model at 88% with a difference in performance of 1% across the two sites. The workflow presented in this work proposes the use of grid maps to visualize the density of Pacific oysters per square meter towards ecological management and the creation of time series to identify trends.}
}
@article{JANSIRANI2024102663,
title = {A novel automated approach for fish biomass estimation in turbid environments through deep learning, object detection, and regression},
journal = {Ecological Informatics},
volume = {81},
pages = {102663},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102663},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400205X},
author = {S.V. {Jansi Rani} and Iacovos Ioannou and R. Swetha and R.M. {Dhivya Lakshmi} and Vasos Vassiliou},
keywords = {Fish biomass, Object detection, Regression, YOLOv8, mYOLOv8},
abstract = {Estimating fish biomass is crucial in the fisheries sector, where traditional methods often harm fish through manual sampling and anesthetics. A non-invasive approach is introduced using underwater films to estimate fish biomass in turbid conditions. This study presents the “Aquatic WeightNet” dataset, targeting the Genetically Improved Formed Tilapia (GIFT) Tilapia species, and addresses the challenge of unclear images with preprocessing techniques like dehazing and Contrast Limited Adaptive Histogram Equalization (CLAHE). YOLOv8, a leading object detection model modified to accommodate the custom Aquatic WeightNet dataset's varied image sizes with five detection heads, P2 to P6, is employed, achieving a recall of 0.997 and a mean Average Precision (mAP) of 0.899 within the 50–95% Intersection over Union (IoU) range. Fish biomass estimation assesses depth, length, and width using regression models for calculation. A three-phase grid search identifies the most effective models, with the Extra Trees Regressor outperforming depth estimation with mean absolute error (MAE) of 0.63 and coefficient of determination (R2) of 0.87 and the Random Forest Regressor for length and width (MAE of 0.01 and R2 of 0.99). For biomass estimation, the Extra Trees Regressor again performs well (MAE of 0.004 and R2 of 0.99), which is critical for determining optimal feed quantities to enhance aquaculture efficiency. This study emphasizes a non-invasive method to estimate fish biomass, optimizing the effectiveness and ecological sustainability of fish farming in murky waters through advanced detection algorithms and robust regression models.}
}
@article{ZBINDEN2024102623,
title = {On the selection and effectiveness of pseudo-absences for species distribution modeling with deep learning},
journal = {Ecological Informatics},
volume = {81},
pages = {102623},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102623},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001651},
author = {Robin Zbinden and Nina {van Tiel} and Benjamin Kellenberger and Lloyd Hughes and Devis Tuia},
keywords = {Species distribution modeling, Neural networks, Presence-only data, Pseudo-absences, Deep learning},
abstract = {Species distribution modeling is a highly versatile tool for understanding the intricate relationship between environmental conditions and species occurrences. However, the available data often lacks information on confirmed species absence and is limited to opportunistically sampled, presence-only observations. To overcome this limitation, a common approach is to employ pseudo-absences, which are specific geographic locations designated as negative samples. While pseudo-absences are well-established for single-species distribution models, their application in the context of multi-species neural networks remains underexplored. Notably, the significant class imbalance between species presences and pseudo-absences is often left unaddressed. Moreover, the existence of different types of pseudo-absences (e.g., random and target-group background points) adds complexity to the selection process. Determining the optimal combination of pseudo-absences types is difficult and depends on the characteristics of the data, particularly considering that certain types of pseudo-absences can be used to mitigate geographic biases. In this paper, we demonstrate that these challenges can be effectively tackled by integrating pseudo-absences in the training of multi-species neural networks through modifications to the loss function. This adjustment involves assigning different weights to the distinct terms of the loss function, thereby addressing both the class imbalance and the choice of pseudo-absence types. Additionally, we propose a strategy to set these loss weights using spatial block cross-validation with presence-only data. We evaluate our approach using a benchmark dataset containing independent presence-absence data from six different regions and report improved results when compared to competing approaches.}
}
@article{NGUYEN2024102744,
title = {Improving pollen-bearing honey bee detection from videos captured at hive entrance by combining deep learning and handling imbalance techniques},
journal = {Ecological Informatics},
pages = {102744},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102744},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002863},
author = {Dinh-Tu Nguyen and Thi-Nhung Le and Thi-Huong Phung and Duc-Manh Nguyen and Hong-Quan Nguyen and Hong-Thai Pham and Thi-Thu-Hong Phan and  Vu-Hai and Thi-Lan Le},
keywords = {Pollen foraging behavior, Pollen-bearing honey bee detection},
abstract = {The number of pollen-bearing honey bees serves as a vital indicator for assessing colony balance and health. Despite its significance, prevailing detection techniques still rely heavily on manual observation and annotation, leading to time-consuming processes that cannot sustain long-term, continuous monitoring efforts. To facilitate automatic beehive monitoring, this study introduces an efficient method for pollen-bearing bee detection. Initially, we furnish a comprehensive dataset, dubbed VnPollenBee, meticulously annotated for pollen-bearing honey bee detection and classification. The dataset comprises 60,826 annotated boxes that delineate both pollen-bearing and non-pollen-bearing bees in 2051 images captured at the entrances of beehives under various environmental conditions. To the best of our knowledge, this represents the first dedicated dataset for pollen-bearing bee detection. The VnPollenBee dataset is publicly accessible to the research community at https://comvis-hust.github.io/datasets/pollenbee.html. Subsequently, we propose the incorporation of diverse techniques into two baseline models, namely YOLOv5 and Faster RCNN, to effectively address the imbalance that arises during the detection of pollen-bearing bees due to their number being typically much lower than the total number of bees present at hive entrances. The experimental results demonstrate that our proposed method outperforms the baseline models on the VnPollenBee dataset, yielding Precision, Recall, and F1 score of 99%, 93%, and 95%, respectively. Specifically, the improvements obtained are 3% and 2% in Recall and F1 score when using YOLOv5, and 3%, 2%, and 2% in Precision, Recall, and F1 score when using Faster RCNN. These findings confirm the potential of our approach to facilitate bee foraging behavior analysis and automated bee monitoring.}
}
@article{KLADNY2024102474,
title = {Enhanced prediction of vegetation responses to extreme drought using deep learning and Earth observation data},
journal = {Ecological Informatics},
volume = {80},
pages = {102474},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102474},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000165},
author = {Klaus-Rudolf Kladny and Marco Milanta and Oto Mraz and Koen Hufkens and Benjamin D. Stocker},
keywords = {Drought impact forecasting, Sentinel-2, EarthNet2021, ConvLSTM, NDVI},
abstract = {The advent of abundant Earth observation data enables the development of novel predictive methods for forecasting climate impacts on the state and health of terrestrial ecosystems. Here, we predict the spatial and temporal variations of land surface reflectance and vegetation greenness, measuring the density of green vegetation and active foliage area, conditioned on current and past weather and the local topography. We train two alternative recurrent deep learning models that combine Long Short-Term Memory cells with convolutional layers (ConvLSTM) for forecasting the spatially resolved deviation of surface reflectance across a heterogeneous landscape from a specified initial state. Using data from diverse ecosystems and land cover types across Europe and following a standardized model evaluation framework (EarthNet2021 Challenge), our results indicate increased performance in predicting surface greenness during extreme drought events of the models presented here, compared to currently published benchmarks. This demonstrates how deep learning methods for optical Earth observation time series enable an early-warning of vegetation responses to the impacts of climatic extreme events, such as the drought-related loss of green foliage.}
}
@article{ZHANG2024102394,
title = {Evaluation of digital soil mapping projection in soil organic carbon change modeling},
journal = {Ecological Informatics},
volume = {79},
pages = {102394},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102394},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004235},
author = {Tao Zhang and Lai-Ming Huang and Ren-Min Yang},
keywords = {Soil carbon change, Digital soil mapping, Model projections, Environmental change, Temporal transferability},
abstract = {There is increasing interest in the application of digital soil mapping (DSM) projections to infer changes in soil carbon across both space and time. This approach relies on the assumption that the spatially modeled soil carbon-environment relationship can be transferred over time. However, this assumption is rarely tested due to a lack of temporally independent validation data. This paper assesses this assumption by developing models of topsoil organic carbon stocks (SOCS) with a deep learning algorithm and data covering mainland China pertaining to the 1980s and 2010s. The temporal prediction performance of models capturing a specific period was assessed by evaluating their performance in the prediction of data during another period. The results revealed that the prediction accuracy of temporal modeling decreased, as indicated by the coefficient of determination, and was lower than that of spatial modeling. The lower prediction accuracy obtained with the DSM-projection approach may result from differences in the magnitudes of influential variables across periods. We found that different levels of environmental similarity and model projection sensitivity to dynamic variables may cause discrepancies in forecast and hindcast accuracy. Our results demonstrate that the prediction error in temporal modeling is related to the degree of environmental similarity between periods. Our findings generally support the implementation of the DSM-projection approach in soil carbon change modeling. However, caution should be exercised, as there exists much uncertainty regarding the projection of spatial models over time.}
}
@article{DWIVEDI2024102451,
title = {EMViT-Net: A novel transformer-based network utilizing CNN and multilayer perceptron for the classification of environmental microorganisms using microscopic images},
journal = {Ecological Informatics},
volume = {79},
pages = {102451},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102451},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004806},
author = {Karnika Dwivedi and Malay Kishore Dutta and Jay Prakash Pandey},
keywords = {Environmental microorganisms classification, Microscopic images, Computer-aided system, Deep learning, Vision transformer},
abstract = {Environmental microbes are certainly present in our surroundings since they are essential to the growth and survival of human advancement. The detailed analysis of environmental microorganisms (EMs) is very important to recognize, understand and make use of microbes as well and prevent damage. Extracting the discriminatory features from a limited-size dataset is very challenging for a deep learning model and a pure transformer-based network cannot achieve good classification results on a limited-size dataset due to the lack of muti-scale features. In this study, a novel vision transformer-based deep neural network is proposed by integrating the transformer with CNN for the classification of EM using microscopic images. The proposed network EMViT-Net has three main modules: a transformer module, a CNN module and a multilayer perceptron module. The transformer model extracted multiscale features to generate more discriminatory information from the images. A new separable convolutional parameter-sharing attention (SCPSA) block is integrated with the CNN module in the core of EMViT-Net, which makes the model robust to capture the local and global features, and simultaneously reduces the computational complexity of the model. The data augmentation is performed to introduce the variability in the dataset and counter the problem of overfitting and data imbalance. After extensive experiments and detailed analysis, it has been determined that the proposed model EMViT-Net outperforms the other existing methods and achieves state-of-the-art results with an accuracy of 71.17% which proves the effectiveness of the model for the classification of environmental microbes.}
}
@article{BAKANA2024102541,
title = {WildARe-YOLO: A lightweight and efficient wild animal recognition model},
journal = {Ecological Informatics},
volume = {80},
pages = {102541},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102541},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000839},
author = {Sibusiso Reuben Bakana and Yongfei Zhang and Bhekisipho Twala},
keywords = {Wild animal recognition, Deep learning, Lightweight, Efficient, Loss function},
abstract = {For the protection of endangered species and successful wildlife population monitoring, wild animal recognition is essential. While deep learning models like YOLOv5 have shown promise in real-time object recognition, their practical applicability may be constrained by their high processing requirements. In this paper, we suggest a faster and lighter version of YOLOv5s for wild animal recognition. To lower computational costs for model parameters and floating-point operations (FLOPs) for the backbone, our suggested model includes Mobile Bottleneck Block modules and an improved StemBlock. We also use Focal-EIoU as a loss function to gauge the accuracy of the predicted bounding boxes during inference and employ a BiFPN-based neck. We tested our technique on three datasets, including Wild Animal Facing Extinction, Fishmarket, and MS COCO 2017. Additionally, our technique is compared with state-of-the-art deep learning models, and from the baseline model we recorded a 17.65% increase in FPS, 28.55% model parameters reduction, and 50.92% in FLOPs reduction. Furthermore, our model has a faster model loading time, which is critical for deployment in remote areas. This enables real-time species recognition on basic hardware, aiding conservation efforts through rapid analysis. The model advances deep learning in ecology by balancing efficiency with performance.}
}
@article{VANOSTA2023102233,
title = {An active learning framework and assessment of inter-annotator agreement facilitate automated recogniser development for vocalisations of a rare species, the southern black-throated finch (Poephila cincta cincta)},
journal = {Ecological Informatics},
volume = {77},
pages = {102233},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102233},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002625},
author = {John M. {van Osta} and Brad Dreis and Ed Meyer and Laura F. Grogan and J. Guy Castley},
keywords = {Bioacoustics, Machine learning, Annotator agreement, Call recognition, Active learning},
abstract = {The application of machine learning methods has led to major advances in the development of automated recognisers used to analyse bioacoustics data. To further improve the performance of automated call recognisers, we investigated the development of efficient data annotation strategies and how best to address uncertainty around ambiguous vocalisations. These challenges present a particular problem for species whose vocalisations are rare in field recordings, where collecting enough training data can be problematic and a species' vocalisations may be poorly documented. We provide an open access solution to address these challenges using two strategies. First, we applied an active learning framework to iteratively improve a convolutional neural network (CNN) model able to automate call identification for a target rare bird species, the southern black-throated finch (Poephila cincta cincta). We collected 9098 h of unlabelled audio recordings from a field study in the Desert Uplands Bioregion of Queensland, Australia, and used active learning to prioritise human annotation effort towards data that would best improve model fit. Second, we progressed methods for managing ambiguous vocalisations by applying machine learning methods more commonly used in medical image analysis and natural language processing. Specifically, we assessed agreement among human annotators and the CNN model (i.e. inter-annotator agreement) and used this to determine realistic performance outcomes for the CNN model and to identify areas where inter-annotator agreement may be improved. We also applied a classification approach that allowed the CNN model to classify sounds into an ‘uncertain’ category, which replicated a requirement of human-annotation and facilitated the comparison of human-model annotation performance. We found that active learning was an efficient strategy to build a CNN model where there was limited labelled training data available, and target calls were extremely rare in the unlabelled data. As few as five active learning iterations, generating a final labelled dataset of 1073 target calls and 5786 non-target sounds, were required to train a model to identify the target species with comparable performance to experts in the field. Assessment of inter-annotator agreement identified a bias in our model to align predictions most closely with those of the primary annotator and identified significant differences in inter-annotator agreement among subsets of our acoustic data. Our results highlight the use of inter-annotator agreement to understand model performance and identify areas for improvement in data annotation. We also show that excluding ambiguous vocalisations during data annotation results in an overestimation of model performance, an important consideration for datasets with inter-annotator disagreement.}
}
@article{JIANG2024102650,
title = {Monitoring public perceptions of contaminated sites based on social media},
journal = {Ecological Informatics},
volume = {81},
pages = {102650},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102650},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001924},
author = {Yefeng Jiang and Yingcong Ye and Congkang Sun and Xi Guo and Zhou Shi},
keywords = {Spatiotemporal distribution, Contaminated site, Public perception, Social media, Risk management},
abstract = {Contaminated sites have a negative impact on human health and the ecological environment, which can potentially lead to major pollution incidents, and frequently receive complaints and reports from the public. Therefore, monitoring public perceptions of contaminated sites is crucial for risk management. However, methods based on traditional questionnaire surveys are limited in terms of time, cost, and target audience size. The purpose of this study was to establish a method using social media to monitor public perceptions of contaminated sites in the Yangtze River Delta urban agglomeration. Thus, 6802 public opinions were collected from social media (mainly microblogs in China) and topic modeling and pollution and spatial mining techniques were employed. The topic modeling results indicated that public perceptions of contaminated sites mainly focused on the construction of prevention and control systems, law enforcement work, developments and challenges applicable to the coal industry, environmental public interest litigation (pertaining to pollution), pollution inspection and rectification, and green development and ecological governance, with intensities of 7.9%, 7.8%, 7.1%, 6.1%, 5.9%, and 4.8%, respectively. Three communities resulting from public opinions in the study area included “environment and pollution,” “enterprises,” and “environmental protection,” with proportions of 37.68%, 34.78%, and 27.54%, respectively. The field investigation results indicated that approximately 90% of the tweets in three typical cities (i.e., Taizhou City in Zhejiang Province and Wuxi and Changzhou Cities in Jiangsu Province) involved key industrial enterprises or contaminated sites located within 1 km of the surrounding areas. The emotional analysis indicated that >3401 tweets dealt with a pollution probability (i.e., the possibility of potentially contaminated sites mentioned in social media becoming contaminated sites) exceeding 0.90 for the period 2011–2021. This finding suggests that the pollution probability for the sites involved in these tweets was high. Our study provides methodological references for monitoring public perceptions of contaminated sites for large-scale, long-term, and high-resolution studies.}
}
@article{MASSARELLI2023102342,
title = {Dynamics of pesticides in surface water bodies by applying data mining to spatiotemporal big data. A case study for the Puglia Region},
journal = {Ecological Informatics},
volume = {78},
pages = {102342},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102342},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003710},
author = {Carmine Massarelli and Claudia Campanale and Mariangela Triozzi and Vito Felice Uricchio},
keywords = {Plant protection products, Knowledge discovery in databases, Glyphosate, AMPA, Pesticides mixtures, Corine land cover, Geographic information system},
abstract = {Surface water pollution by pesticides is a primary concern in many parts of the world. Therefore, an effective monitoring program is essential to assess the environmental state of aquatic ecosystems and evaluate mitigation strategies. In the Puglia Region (southern Italy), a complex seasonal monitoring program for detecting pesticide residues in water was initiated in 2018 and is still underway (over four years). The program was based on site-specific assessments and identified 170 Plant Protection Products (PPPs) to identify residues in the surface water bodies of Puglia. In this context, the present work aims to analyse pesticide data obtained from the regional monitoring of rivers over four years using a multidisciplinary approach, which includes statistical methods, data mining, and mapping tools to extract as much information as possible.To this end, data mining was applied to identify the prevalent mixtures of pesticide residues found at the monitoring stations and correlate these results with possible causative factors. The results showed that surface water bodies are subject to different pressures derived from the massive use of PPPs, and several seasonal and territorial-related factors were identified to be strictly correlated with pesticide concentration results. Nine main PPPs mixtures have been identified in the Puglia River. Glyphosate, AMPA, imidacloprid, and azoxystrobin represent the main residues detected in the surface aquatic environments regarding the amount and frequency revealed. The methodological approach proposed in the present work can represent a good “model study” to be used by researchers to interpret water quality trends and data variability from long-term monitoring studies. Moreover, our results can significantly support the decision-making process and implementation of environmental mitigation measures by optimising the results of complex monitoring programs.}
}
@article{KAUKAB2024102691,
title = {Improving real-time apple fruit detection: Multi-modal data and depth fusion with non-targeted background removal},
journal = {Ecological Informatics},
volume = {82},
pages = {102691},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102691},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002334},
author = {Shaghaf Kaukab and  Komal and Bhupendra M Ghodki and Hena Ray and Yogesh B. Kalnar and Kairam Narsaiah and Jaskaran S. Brar},
keywords = {Apple, Fruit detection, 3D localization, YOLO network, RGB-D images, Depth sensor},
abstract = {In automated fruit detection, RGB-Depth (RGB-D) images aid the detection model with additional depth information to enhance detection accuracy. However, outdoor depth images are usually of low quality, which limits the quality of depth data. In this study, an approach/technique for real-time apple fruit detection in a high-density orchard environment by using multi-modal data is presented. Non-targeted background removal using the depth fusion (NBR-DF) method was developed to reduce the high noise condition of depth images. The noise occurred due to the uncontrolled lighting condition and holes with incomplete depth information in the depth images. NBR-DF technique follows three primary steps: pre-processing of depth images (point cloud generation), target object extraction, and background removal. The NBR-DF method serves as a pipeline to pre-process multi-modal data to enhance features of depth images by filling holes to eliminate noise generated by depth holes. Further, the NBR-DF implemented with the YOLOv5 enhances the detection accuracy in dense orchard conditions by using multi-modal information as input. An attention-based depth fusion module that adaptively fuses the multi-modal features was developed. The integration of the depth-attention matrix involved pooling operations and sigmoid normalization, both of which are efficient methods for summarizing and normalizing depth information. The fusion module improves the identification of multiscale objects and strengthens the network's resistance to noise. The network then detects the fruit position using multiscale information from the RGB-D images in highly complex orchard environments. The detection results were compared and validated with other methods using different input modals and fusion strategies. The results showed that the detection accuracy using the NBR-DF approach achieved an average precision rate of 0.964 in real time. The performance comparison with other state-of-the-art methods and the model generalization study also establish that the present advanced depth-fusion attention mechanism and effective preprocessing steps in NBR-DF-YOLOv5 significantly surpass those in performance. In conclusion, the developed NBR-DF technique showed the potential to improve real-time apple fruit detection using multi-modal information.}
}
@article{KRIVOGUZ2024102513,
title = {Geo-spatial analysis of urbanization and environmental changes with deep neural networks: Insights from a three-decade study in Kerch peninsula},
journal = {Ecological Informatics},
volume = {80},
pages = {102513},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102513},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000554},
author = {Denis Krivoguz},
keywords = {Urbanization dynamics, Kerch Peninsula, Land cover change, Environmental implications, Sustainable development, Remote sensing, GIS analysis, Natural ecosystems, Climate change},
abstract = {This study presents a comprehensive analysis of land use and land cover (LULC) changes on the Kerch Peninsula over the last thirty years, utilizing advanced satellite data and spatial modeling techniques. The research used Landsat 5, 7 and 8 satellite images to capture the intricate dynamics of LULC changes from 1990 to 2020. A quantitative approach was adopted, involving the use of convolutional neural networks (CNN) for enhanced classification accuracy. This methodology allowed for a detailed and precise identification of various LULC classes, revealing significant trends and transformations in the region's landscape. The spatial modeling incorporated in this study allowed exploration of both large-scale patterns and localized changes, providing insights into the drivers and consequences of LULC dynamics. The statistical analysis revealed a notable increase in urbanized areas, coupled with a decline in natural ecosystems such as forests and wetlands. These changes reflect the impact of sustained urban growth and agricultural expansion, underscoring the need for informed land management and conservation strategies. The study findings contribute to understanding urbanization processes and their ecological implications, providing valuable guidance for sustainable regional planning and environmental protection.}
}
@article{LEBIEN2020101113,
title = {A pipeline for identification of bird and frog species in tropical soundscape recordings using a convolutional neural network},
journal = {Ecological Informatics},
volume = {59},
pages = {101113},
year = {2020},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101113},
url = {https://www.sciencedirect.com/science/article/pii/S1574954120300637},
author = {Jack LeBien and Ming Zhong and Marconi Campos-Cerqueira and Julian P. Velev and Rahul Dodhia and Juan Lavista Ferres and T. Mitchell Aide},
keywords = {Acoustic monitoring, Bioacoustics, Sound classification, Convolutional neural network, Deep learning},
abstract = {Automated acoustic recorders can collect long-term soundscape data containing species-specific signals in remote environments. Ecologists have increasingly used them for studying diverse fauna around the globe. Deep learning methods have gained recent attention for automating the process of species identification in soundscape recordings. We present an end-to-end pipeline for training a convolutional neural network (CNN) for multi-species multi-label classification of soundscape recordings, starting from raw, unlabeled audio. Training data for species-specific signals are collected using a semi-automated procedure consisting of an efficient template-based signal detection algorithm and a graphical user interface for rapid detection validation. A CNN is then trained based on mel-spectrograms of sound to predict the set of species present in a recording. Transfer learning of a pre-trained model is employed to reduce the necessary training data and time. Furthermore, we define a loss function that allows for using true and false template-based detections to train a multi-class multi-label audio classifier. This approach leverages relevant absence (negative) information in training, and reduces the effort in creating multi-label training data by allowing weak labels. We evaluated the pipeline using a set of soundscape recordings collected across 749 sites in Puerto Rico. A CNN model was trained to identify 24 regional species of birds and frogs. The semi-automated training data collection process greatly reduced the manual effort required for training. The model was evaluated on an excluded set of 1000 randomly sampled 1-min soundscapes from 17 sites in the El Yunque National Forest. The test recordings contained an average of ~3 present target species per recording, and a maximum of 8. The test set also showed a large class imbalance with most species being present in less than 5% of recordings, and others present in >25%. The model achieved a mean-average-precision of 0.893 across the 24 species. Across all predictions, the total average-precision was 0.975.}
}
@article{KIM2024102407,
title = {Predicting invasive species distributions using incremental ensemble-based pseudo-labeling},
journal = {Ecological Informatics},
volume = {79},
pages = {102407},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102407},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004363},
author = {Eunbeen Kim and Jaeuk Moon and Jonghwa Shim and Eenjun Hwang},
keywords = {Species distribution model, Semi-supervised learning, Pseudo-labeling, Ensemble model},
abstract = {Control strategies for preventing the spread of invasive species require their accurate geographical distribution. Species distribution models (SDMs) that can predict potential habitats of invasive species and thereby derive habitat suitability maps have become a valuable tool for supporting regulatory strategies. To date, machine learning (ML)-based approaches have outperformed profile and statistical-based approaches in terms of species distribution prediction accuracy. However, ML-based approaches often suffer from poor predictive performance when there is insufficient labeled data. Recently, pseudo-labeling (PL), a semi-supervised learning method, has been proposed to alleviate this problem, but pseudo-labels generated using a single-teacher model with very few labeled data points are generally biased and not suitable for training SDMs. In this paper, we propose a novel prediction scheme for invasive species distributions using incremental ensemble-based PL (SDP-EPL). We first build an ensemble-based teacher using multiple conventional SDMs and then incrementally construct a training set, starting with very few labeled data points, by repeating the following process: (i) generating pseudo-labels for unlabeled data using the teacher model, (ii) appending the pseudo-labeled data representing high or low habitat suitability to the training set, and (iii) training the teacher model using the updated training set. We then train a student SDM using the training set. Based on extensive experiments using citizen science datasets for three species, we show that the proposed scheme outperforms other commonly used SDMs in terms of diverse evaluation metrics and achieves performance improvements of up to 14.61% and 5.45% compared to the baseline and state-of-the-art models, respectively. We also demonstrate the effectiveness of the ensemble teacher model and incremental labeling in terms of predictive accuracy.}
}
@article{POUTARAUD2024102687,
title = {Meta-Embedded Clustering (MEC): A new method for improving clustering quality in unlabeled bird sound datasets},
journal = {Ecological Informatics},
volume = {82},
pages = {102687},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102687},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002292},
author = {Joachim Poutaraud and Jérôme Sueur and Christophe Thébaud and Sylvain Haupert},
keywords = {Bird sounds, Ecoacoustics, meta-learning, Unsupervised learning, Dimensionality reduction, Spectrogram},
abstract = {In recent years, ecoacoustics has offered an alternative to traditional biodiversity monitoring techniques with the development of passive acoustic monitoring (PAM) systems allowing, among others, to detect and identify species that are difficult to detect by human observers, automatically. PAM systems typically generate large audio datasets, but using these monitoring techniques to infer ecologically meaningful information remains challenging. In most cases, several thousand hours of recordings need to be manually labeled by experts limiting the operability of the systems. Based on recent developments of meta-learning algorithms and unsupervised learning techniques, we propose here Meta-Embedded Clustering (MEC), a new method with high potential for improving clustering quality in unlabeled bird sound datasets. MEC method is organized in two main steps, with: (a) fine-tuning of a pretrained convolutional neural network (CNN) backbone with different meta-learning algorithms using pseudo-labeled data, and (b) clustering of manually-labeled bird sounds in the latent space based on vector embeddings extracted from the fine-tuned CNN. The MEC method significantly enhanced average clustering performance from less than 1% to more than 80%, greatly outperforming the traditional approach of relying solely on CNN features extracted from a general neotropical audio database. However, this enhanced performance came with the cost of excluding a portion of the data categorized as noise. By improving the quality of clustering in unlabeled bird sound datasets, the MEC method should facilitate the work of ecoacousticians in managing acoustic units of bird song/call clustered according to their similarities, and in identifying potential clusters of species undetected using traditional approaches.}
}
@article{MARTINEZSANCHEZ2022101757,
title = {Skyline variations allow estimating distance to trees on landscape photos using semantic segmentation},
journal = {Ecological Informatics},
volume = {70},
pages = {101757},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101757},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002072},
author = {Laura Martinez-Sanchez and Daniele Borio and Raphaël d'Andrimont and Marijn {van der Velde}},
keywords = {Semantic segmentation, Conditional random fields, COCO, Landscape, Openness, Image depth},
abstract = {Approximate distance estimation can be used to determine fundamental landscape properties including complexity and openness. We show that variations in the skyline of landscape photos can be used to estimate distances to trees on the horizon. A methodology based on the variations of the skyline has been developed and used to investigate potential relationships with the distance to skyline objects. The skyline signal, defined by the skyline height expressed in pixels, was extracted for a set of 148 Land Use/Cover Area frame Survey (LUCAS) landscape photos. Photos were semantically segmented with DeepLabV3+ trained with the Common Objects in Context (COCO) dataset. This provided pixel-level classification of the objects forming the skyline. A Conditional Random Fields (CRF) algorithm was also applied to increase the details of the skyline signal. Three metrics, able to capture the skyline signal variations, were then considered for the analysis. These metrics shows a functional relationship with distance for the class of trees, whose contours have a fractal nature. In particular, regression analysis was performed against 475 ortho-photo based distance measurements, and, in the best case, a R2 score equal to 0.47 was achieved. This is an encouraging result which shows the potential of skyline variation metrics for inferring distance related information.}
}
@article{BEIGAITE2022101849,
title = {Multi-output regression with structurally incomplete target labels: A case study of modelling global vegetation cover},
journal = {Ecological Informatics},
volume = {72},
pages = {101849},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101849},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002990},
author = {Rita Beigaitė and Jesse Read and Indrė Žliobaitė},
keywords = {Multi-output regression, Compositional data, Incomplete targets, Weakly supervised learning, Vegetation cover modelling},
abstract = {Weakly-supervised learning has recently emerged in the classification context where true labels are often scarce or unreliable. However, this learning setting has not yet been extensively analyzed for regression problems, which are typical in macroecology. We further define a novel computational setting of structurally noisy and incomplete target labels, which arises, for example, when the multi-output regression task defines a distribution such that outputs must sum up to unity. We propose an algorithmic approach to reduce noise in the target labels and improve predictions. We evaluate this setting with a case study in global vegetation modelling, which involves building a model to predict the distribution of vegetation cover from climatic conditions based on global remote sensing data. We compare the performance of the proposed approach to several incomplete target baselines. The results indicate that the error in the targets can be reduced by our proposed partial-imputation algorithm. We conclude that handling structural incompleteness in the target labels instead of using only complete observations for training helps to better capture global associations between vegetation and climate.}
}
@article{DUFOURQ2022101688,
title = {Passive acoustic monitoring of animal populations with transfer learning},
journal = {Ecological Informatics},
volume = {70},
pages = {101688},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101688},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122001388},
author = {Emmanuel Dufourq and Carly Batist and Ruben Foquet and Ian Durbach},
keywords = {Transfer learning, Convolutional neural networks, Deep learning, Vocalisation classification, Bioacoustics},
abstract = {Progress in deep learning, more specifically in using convolutional neural networks (CNNs) for the creation of classification models, has been tremendous in recent years. Within bioacoustics research, there has been a large number of recent studies that use CNNs. Designing CNN architectures from scratch is non-trivial and requires knowledge of machine learning. Furthermore, hyper-parameter tuning associated with CNNs is extremely time consuming and requires expensive hardware. In this paper we assess whether it is possible to build good bioacoustic classifiers by adapting and re-using existing CNNs pre-trained on the ImageNet dataset – instead of designing them from scratch, a strategy known as transfer learning that has proved highly successful in other domains. This study is a first attempt to conduct a large-scale investigation on how transfer learning can be used for passive acoustic monitoring (PAM), to simplify the implementation of CNNs and the design decisions when creating them, and to remove time consuming hyper-parameter tuning phases. We compare 12 modern CNN architectures across 4 passive acoustic datasets that target calls of the Hainan gibbon Nomascus hainanus, the critically endangered black-and-white ruffed lemur Varecia variegata, the vulnerable Thyolo alethe Chamaetylas choloensis, and the Pin-tailed whydah Vidua macroura. We focus our work on data scarcity issues by training PAM binary classification models very small datasets, with as few as 25 verified examples. Our findings reveal that transfer learning can result in up to 82% F1 score while keeping CNN implementation details to a minimum, thus rendering this approach accessible, easier to design, and speeding up further vocalisation annotations to create PAM robust models.}
}
@article{WANG201969,
title = {Machine learning for inferring animal behavior from location and movement data},
journal = {Ecological Informatics},
volume = {49},
pages = {69-76},
year = {2019},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2018.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1574954118302036},
author = {Guiming Wang},
keywords = {Behavioral classification, Computational ecology, Hidden Markov model, Movement ecology, State space model, Supervised learning algorithm, Unsupervised learning algorithm},
abstract = {Movement ecology has rapidly advanced owing to recent developments of animal-attached devices and wide applications of sophisticated statistical and machine learning techniques in analysis of animal movement data. Global Positioning System (GPS) transmitters used for estimating animal locations and tri-axial accelerometers used for measuring the 3-dimensional accelerations of animal's motion aid researchers in collecting location and locomotion data at fine spatial and temporal scales. Machine learning and other advanced statistical methods bridge conceptual models to data, providing insights into ecological and physiological mechanisms underlying animal behavior and movements. This study reviews the general principles and applications of state space models, hidden Markov models, random forests, and support vector machines in the inference of animal behavior from movement data. Unsupervised learning algorithms, including Bayesian state space models implemented for robust correlated random walk models and hidden Markov models, help infer different behavioral modes using GPS location data. State space models can account for measurement error in GPS locations and estimate the true locations of animals; however, without including movement-state switching, state space models do not infer behavioral modes directly. On the contrary, hidden Markov models estimate the probabilities that animals switch between different behavioral modes. Nevertheless, hidden Markov models neither directly estimate animal locations nor account for measurement error explicitly. Supervised learning algorithms integrate data on locations and directional accelerations with synchronized behavioral observations (i.e., labels) to classify behaviors to pre-defined behavioral categories. Unlike unsupervised learning, supervised learning requires behavioral observations to label locations and accelerometer data to train the learning algorithms. However, behavioral observations synchronized with relocations and acceleration records are often missing or unattainable in many species, hindering the applications of supervised learning, making unsupervised learning a suitable tool for behavioral annotation of movement paths in secretive (cryptic) or less studied species. Environmental and behavioral annotations of animal movement paths by machine learning improve understanding the effects of environmental conditions on animal movements and behavioral decisions.}
}
@article{FRAINER2023102291,
title = {Automatic detection and taxonomic identification of dolphin vocalisations using convolutional neural networks for passive acoustic monitoring},
journal = {Ecological Informatics},
volume = {78},
pages = {102291},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102291},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003205},
author = {Guilherme Frainer and Emmanuel Dufourq and Jack Fearey and Sasha Dines and Rachel Probert and Simon Elwen and Tess Gridley},
keywords = {Convolutional neural networks, Indian Ocean humpback dolphin, Machine learning, Passive acoustic monitoring, Sound detection, Species identification},
abstract = {A novel framework for acoustic detection and species identification is proposed to aid passive acoustic monitoring studies on the endangered Indian Ocean humpback dolphin (Sousa plumbea) in South African waters. Convolutional Neural Networks (CNNs) were used for both detection and identification of dolphin vocalisations tasks, and performance was evaluated using custom and pre-trained architectures (transfer learning). In total, 723 min of acoustic data were annotated for the presence of whistles, burst pulses and echolocation clicks produced by Delphinus delphis (~45.6%), Tursiops aduncus (~39%), Sousa plumbea (~14.4%), Orcinus orca (~1%). The best performing models for detecting dolphin presence and species identification used segments (spectral windows) of two second lengths and were trained using images with 70 and 90 dpi, respectively. The best detection model was built using a customised architecture and achieved an accuracy of 84.4% for all dolphin vocalisations on the test set, and 89.5% for vocalisations with a high signal to noise ratio. The best identification model was also built using the customised architecture and correctly identified S. plumbea (96.9%), T. aduncus (100%), and D. delphis (78%) encounters in the testing dataset. The developed framework was designed based on the knowledge of complex dolphin sounds and it may assists in finding suitable CNN hyper-parameters for other species or populations. Our study contributes towards the development of an open-source tool to assist long-term studies of endangered species, living in highly diverse habitats, using passive acoustic monitoring.}
}
@article{CHAKRABARTY2024102718,
title = {An interpretable fusion model integrating lightweight CNN and transformer architectures for rice leaf disease identification},
journal = {Ecological Informatics},
volume = {82},
pages = {102718},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102718},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002607},
author = {Amitabha Chakrabarty and Sarder Tanvir Ahmed and Md. Fahim Ul Islam and Syed Mahfuzul Aziz and Siti Sarah Maidin},
keywords = {Plant disease detection, BEiT model, Attention mapping, Deep learning, Process innovation, Food productivity},
abstract = {Swift identification of leaf diseases is crucial for sustainable rice farming, a staple grain consumed globally. The high costs and inefficiencies of manual identification underline the requirement of prompt disease detection. Traditional approaches for identifying leaf diseases in crops, particularly rice, are laborious, and and often ineffective. Given the significant impact of leaf diseases (such as Rice Blast, Brown Spot, and Rice Turgor) on rice quality and yield, computer-assisted detection can be an effective method of ensuring the long-term sustainability of rice production. This study utilizes advanced artificial intelligence (AI) as the optimized bidirectional encoder representations from the transformers for images(BEiT) model along with pre-trained CNNs (Convolutional Neural Networks), to build a comprehensive study for detecting rice leaf diseases. We train and validate two extensive datasets, featuring healthy and various types of unhealthy plant and rice leaf images respectively.Our optimized model demonstrates high accuracy, outperforming other deep learning and transformer-based models such as ViT, Xception, InceptionV3, DenseNet169, VGG16, and ResNet50. The proposed model achieves a precision of 0.97, a recall of 0.96, and an F1-score of 0.97.The explainability of our proposed model is achieved through the use of segmentation techniques in conjunction with the Local Interpretable Model-agnostic Explanations (LIME) method.}
}
@article{GHOSH2024102581,
title = {HPB3C-3PG algorithm: A new hybrid global optimization algorithm and its application to plant classification},
journal = {Ecological Informatics},
volume = {81},
pages = {102581},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102581},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001237},
author = {Sukanta Ghosh and Amar Singh and Shakti Kumar},
keywords = {Optimization problem, HPB3C-3PGA, Hybrid algorithm, CEC’21 benchmark function, PB3C, 3PGA, Exploration, Exploitation, Image classification},
abstract = {This paper proposes a hybrid bio-inspired search and optimization algorithm that combines the strengths of the PB3C (Parallel Big Bang Big Crunch) and 3PGA (3 Parent Genetic Algorithm) algorithms. The hybrid algorithm employs a single population-based evolutionary search coupled with multi-population parallel processing techniques to address optimization problems. The proposed algorithm is implemented in MATLAB software. We evaluate the performance of the proposed algorithm on the CEC2021 standard test bench suite. The performance of the proposed approach is compared with that of the other nine algorithms. The comparative analysis shows that the proposed hybrid PB3C and 3PGA algorithms performed better than the other nine optimization algorithms. Furthermore, this chapter proposes an HPB3C-3PGA-based approach to evolve the near-optimal architecture of CNN. The proposed plant image classification approach is implemented in Python and compared with 12 other approaches. The proposed approach achieved an accuracy of 98.96% on the Mendeley dataset and 98.97% on the CVIP100 dataset. The proposed approach outperforms all other approaches for the plant leaf classification problem. This research significantly contributes to overcoming limitations in existing approaches, providing a robust solution for optimization problems and image classification tasks.}
}
@article{MORTIER2024102730,
title = {Inferring the relationship between soil temperature and the normalized difference vegetation index with machine learning},
journal = {Ecological Informatics},
volume = {82},
pages = {102730},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102730},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002723},
author = {Steven Mortier and Amir Hamedpour and Bart Bussmann and Ruth Phoebe Tchana Wandji and Steven Latré and Bjarni D. Sigurdsson and Tom {De Schepper} and Tim Verdonck},
keywords = {Soil temperature, Phenology, Machine learning, Climate change, SHAP values, Subarctic grassland},
abstract = {Changes in climate can greatly affect the phenology of plants, which can have important feedback effects, such as altering the carbon cycle. These phenological feedback effects are often induced by a shift in the start or end dates of the growing season of plants. The normalized difference vegetation index (NDVI) serves as a straightforward indicator for assessing the presence of green vegetation and can also provide an estimation of the plants' growing season. In this study, we investigated the effect of soil temperature on the timing of the start of the season (SOS), timing of the peak of the season (POS), and the maximum annual NDVI value (PEAK) in subarctic grassland ecosystems between 2014 and 2019. We also explored the impact of other meteorological variables, including air temperature, precipitation, and irradiance, on the inter-annual variation in vegetation phenology. Using machine learning (ML) techniques and SHapley Additive exPlanations (SHAP) values, we analyzed the relative importance and contribution of each variable to the phenological predictions. Our results reveal a significant relationship between soil temperature and SOS and POS, indicating that higher soil temperatures lead to an earlier start and peak of the growing season. However, the Peak NDVI values showed just a slight increase with higher soil temperatures. The analysis of other meteorological variables demonstrated their impacts on the inter-annual variation of the vegetation phenology. Ultimately, this study contributes to our knowledge of the relationships between soil temperature, meteorological variables, and vegetation phenology, providing valuable insights for predicting vegetation phenology characteristics and managing subarctic grasslands in the face of climate change. Additionally, this work provides a solid foundation for future ML-based vegetation phenology studies.}
}
@article{SWAMINATHAN2024102471,
title = {Multi-label classification for acoustic bird species detection using transfer learning approach},
journal = {Ecological Informatics},
volume = {80},
pages = {102471},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102471},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400013X},
author = {Bhuvaneswari Swaminathan and M. Jagadeesh and Subramaniyaswamy Vairavasundaram},
keywords = {Wav2vec, Transformers, Transfer learning, Multi-label, Bird species classification, Audio classification},
abstract = {As part of ornithology, bird species classification is vital to understanding species distribution, habitat requirements and environmental changes that affect bird populations. It is possible for ornithologists to assess the health of a certain habitat by tracking changes in bird species distributions. This work has extended an efficient transfer learning technique for labelling and classifying multiple bird species from real-time audio recordings. For this purpose, Wav2vec is fine-tuned using the back propagation technique, which makes the feature extractor more effective in learning each bird's pitch and other sound characteristics. To perform the task, each audio recording has been clipped as chunks from the overlapping audio to determine multi-labels from it. Through the application of transfer learning, the features of audio recordings have been automatically extracted for classification and fed to a feed-forward network. Subsequently, probabilities associated with each audio segment is aggregated through the clipping approach to represent multiple species of bird call. These probability scores are then used to determine the presence of predominant bird species in the audio recording for multi-labelling. The proposed Wav2vec demonstrates remarkable performance, achieving an F1-score of 0.89 using the Xeno-Canto dataset in which outperforming other multi-label classifiers.}
}
@article{HOU2024102690,
title = {Construction of regional multi-scenario spatially balanced ecological security pattern based on self-organizing feature map: A case study of Guangzhou, China},
journal = {Ecological Informatics},
volume = {82},
pages = {102690},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102690},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002322},
author = {Yin Hou and Yiming Liu and Zijing Wu and Hui Zeng},
keywords = {Ecosystem service bundles, Self-organizing feature map, Ecological nodes, Multi-scenario, Ecological security pattern},
abstract = {Constructing ecological security patterns (ESPs) is the ecological basis for achieving sustainable regional development. This study explored a framework of spatially balanced ESPs suitable for rapidly urbanizing areas to address the issues of overreliance on ecological land and subjective parameter settings in traditional ESP construction methods. Three ecosystem service bundles (product supply, water conservation, and habitat maintenance) were identified in Guangzhou using the self-organizing feature map method. These bundles were then used to identify multifunctional sources. The minimum size of the ecological source was determined to be 10 km2 through the “structure-pattern-function” framework, and a total of 19 ecological sources from three ecological functions were extracted. In these three scenarios, the 102 ecological corridors were classified into five categories based on their connected sources. Ecological node parameters were set based on local planning and dominant species characteristics, enhancing the explanatory power of the circuit theory. The multi-scenario ESP framework proposed in this study achieved a spatially balanced distribution of the entire region at the point, line, and surface levels, as confirmed by the ecological network assessment results. Future planning in Guangzhou should value the ecological functions of non-conventional ecological land, such as cropland and urban green spaces, while considering the impact of human activities on the environment and promptly modifying crucial node areas.}
}
@article{BRAVOSANCHEZ2024102593,
title = {Improved analysis of deep bioacoustic embeddings through dimensionality reduction and interactive visualisation},
journal = {Ecological Informatics},
volume = {81},
pages = {102593},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102593},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001353},
author = {Francisco J. {Bravo Sanchez} and Nathan B. English and Md Rahat Hossain and Steven T. Moore},
keywords = {Bioacoustics, Deep neural networks, Embeddings, Dimensionality reduction},
abstract = {Deep neural networks (DNN) are a popular tool to process environmental sounds and identify sound-producing animals, but it can be difficult to understand the decision-making logic, particularly when it does not produce the expected results. Here we describe a new and enhanced visual interactive analysis of embeddings and explore its application in bioacoustics. Embeddings are the output of the penultimate layer of a DNN, an N-dimensional vector that, only one step removed from the final output, represent the inner-workings of a DNN model. Using existing dimensionality reduction techniques we converted the N-dimensional embeddings into 2 or 3-dimensional arrays displayed in scatterplots. By incorporating sound samples into the scatterplots we developed a visual and aural interactive interface and demonstrate its utility in assessing the performance of trained bioacoustic models, facilitating post-processing of results, error detection, input selection and the detection of rare events, which the reader can experience in online examples with publicly available code.}
}
@article{WEINSTEIN2020101061,
title = {Cross-site learning in deep learning RGB tree crown detection},
journal = {Ecological Informatics},
volume = {56},
pages = {101061},
year = {2020},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101061},
url = {https://www.sciencedirect.com/science/article/pii/S157495412030011X},
author = {Ben G. Weinstein and Sergio Marconi and Stephanie A. Bohlman and Alina Zare and Ethan P. White},
keywords = {Tree crown detection, RGB deep learning, Object detection, Airborne LiDAR},
abstract = {Tree crown detection is a fundamental task in remote sensing for forestry and ecosystem ecology. While many individual tree segmentation algorithms have been proposed, the development and testing of these algorithms is typically site specific, with few methods evaluated against data from multiple forest types simultaneously. This makes it difficult to determine the generalization of proposed approaches, and limits tree detection at broad scales. Using data from the National Ecological Observatory Network, we extend a recently developed deep learning approach to include data from a range of forest types to determine whether information from one forest can be used for tree detection in other forests, and explore the potential for building a universal tree detection algorithm. We find that the deep learning approach works well for overstory tree detection across forest conditions. Performance was best in open oak woodlands and worst in alpine forests. When models were fit to one forest type and used to predict another, performance generally decreased, with better performance when forests were more similar in structure. However, when models were pretrained on data from other sites and then fine-tuned using a relatively small amount of hand-labeled data from the evaluation site, they performed similarly to local site models. Most importantly, a model fit to data from all sites performed as well or better than individual models trained for each local site.}
}
@article{CIAMPI2023102384,
title = {A deep learning-based pipeline for whitefly pest abundance estimation on chromotropic sticky traps},
journal = {Ecological Informatics},
volume = {78},
pages = {102384},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102384},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004132},
author = {Luca Ciampi and Valeria Zeni and Luca Incrocci and Angelo Canale and Giovanni Benelli and Fabrizio Falchi and Giuseppe Amato and Stefano Chessa},
keywords = {Smart agriculture, Smart farming, Integrated pest management, Computer vision, Object counting, Visual counting},
abstract = {Integrated Pest Management (IPM) is an essential approach used in smart agriculture to manage pest populations and sustainably optimize crop production. One of the cornerstones underlying IPM solutions is pest monitoring, a practice often performed by farm owners by using chromotropic sticky traps placed on insect hot spots to gauge pest population densities. In this paper, we propose a modular model-agnostic deep learning-based counting pipeline for estimating the number of insects present in pictures of chromotropic sticky traps, thus reducing the need for manual trap inspections and minimizing human effort. Additionally, our solution generates a set of raw positions of the counted insects and confidence scores expressing their reliability, allowing practitioners to filter out unreliable predictions. We train and assess our technique by exploiting PST - Pest Sticky Traps, a new collection of dot-annotated images we created on purpose and we publicly release, suitable for counting whiteflies. Experimental evaluation shows that our proposed counting strategy can be a valuable Artificial Intelligence-based tool to help farm owners to control pest outbreaks and prevent crop damages effectively. Specifically, our solution achieves an average counting error of approximately 9% compared to human capabilities requiring a matter of seconds, a large improvement respecting the time-intensive process of manual human inspections, which often take hours or even days.}
}
@article{JAHANBAKHT2023102303,
title = {Semi-supervised and weakly-supervised deep neural networks and dataset for fish detection in turbid underwater videos},
journal = {Ecological Informatics},
volume = {78},
pages = {102303},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102303},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003321},
author = {Mohammad Jahanbakht and Mostafa {Rahimi Azghadi} and Nathan J. Waltham},
keywords = {Weakly-supervised classifier, Self-supervised learning, Deep neural networks, Contrastive learning, Transfer learning, XGBoost ensemble, Fish detection, Highly turbid waters},
abstract = {Fish are key members of marine ecosystems, and they have a significant share in the healthy human diet. Besides, fish abundance is an excellent indicator of water quality, as they have adapted to various levels of oxygen, turbidity, nutrients, and pH. To detect various fish in underwater videos, Deep Neural Networks (DNNs) can be of great assistance. However, training DNNs is highly dependent on large, labeled datasets, while labeling fish in turbid underwater video frames is a laborious and time-consuming task, hindering the development of accurate and efficient models for fish detection. To address this problem, firstly, we have collected a dataset called FishInTurbidWater, which consists of a collection of video footage gathered from turbid waters, and quickly and weakly (i.e., giving higher priority to speed over accuracy) labeled them in a 4-times fast-forwarding software. Next, we designed and implemented a semi-supervised contrastive learning fish detection model that is self-supervised using unlabeled data, and then fine-tuned with a small fraction (20%) of our weakly labeled FishInTurbidWater data. At the next step, we trained, using our weakly labeled data, a novel weakly-supervised ensemble DNN with transfer learning from ImageNet. The results show that our semi-supervised contrastive model leads to more than 20 times faster turnaround time between dataset collection and result generation, with reasonably high accuracy (89%). At the same time, the proposed weakly-supervised ensemble model can detect fish in turbid waters with high (94%) accuracy, while still cutting the development time by a factor of four, compared to fully-supervised models trained on carefully labeled datasets. Our dataset and code are publicly available at the hyperlink FishInTurbidWater.}
}
@article{KUMAR2023102206,
title = {CO2 emission based GDP prediction using intuitionistic fuzzy transfer learning},
journal = {Ecological Informatics},
volume = {77},
pages = {102206},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102206},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002352},
author = {Sandeep Kumar and Amit K. Shukla and Pranab K. Muhuri and Q.M. Danish Lohani},
keywords = {Atanassov intuitionistic fuzzy sets, Hausdorff distance, Yager's generating function, GDP prediction, Fuzzy sets},
abstract = {The industrialization has been the primary cause of the economic boom in almost all countries. However, this happened at the cost of the environment, as industrialization also caused carbon emissions to increase exponentially. According to the established literature, Gross Domestic Product (GDP) is related to carbon emissions (CO2) which could be optimally employed to precisely estimate a country's GDP. However, the scarcity of data is a significant bottleneck that could be handled using transfer learning (TL) which uses previously learned information to resolve new tasks, more specifically, related tasks. Notably, TL is highly vulnerable to performance degradation due to the deficiency of suitable information and hesitancy in decision-making. Therefore, this paper proposes ‘Intuitionistic Fuzzy Transfer Learning (IFTL)’, which is trained to use CO2 emission data of developed nations and is tested for its prediction of GDP in a developing nation. IFTL exploits the concepts of intuitionistic fuzzy sets (IFSs) and a newly introduced function called the modified Hausdorff distance function. The proposed IFTL is investigated to demonstrate its actual capabilities for TL in modeling hesitancy. To further emphasize the role of hesitancy modelled with IFSs, we propose an ordinary fuzzy set (FS) based transfer learning. The prediction accuracy of the IFTL is further compared with widely used machine learning approaches, extreme learning machines, support vector regression, and generalized regression neural networks. It is observed that IFTL capably ensured significant improvements in the prediction accuracy over other existing approaches whenever training and testing data have huge data distribution differences. Moreover, the proposed IFTL is deterministic in nature and presents a novel way for mathematically computing the intuitionistic hesitation degree.}
}
@article{MORALES2022101909,
title = {Method for passive acoustic monitoring of bird communities using UMAP and a deep neural network},
journal = {Ecological Informatics},
volume = {72},
pages = {101909},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101909},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003594},
author = {Gabriel Morales and Víctor Vargas and Diego Espejo and Víctor Poblete and Jorge A. Tomasevic and Felipe Otondo and Juan G. Navedo},
keywords = {Passive acoustic monitoring, Bird community, Deep learning, Soundscape, Phenology},
abstract = {An effective practice for monitoring bird communities is the recognition and identification of their acoustic signals, whether simple, complex, fixed or variable. A method for the passive monitoring of diversity, activity and acoustic phenology of structural species of a bird community in an annual cycle is presented. The method includes the semi-automatic elaboration of a dataset of 22 vocal and instrumental forms of 16 species. To analyze bioacoustic richness, the UMAP algorithm was run on two parallel feature extraction channels. A convolutional neural network was trained using STFT-Mel spectrograms to perform the task of automatic identification of bird species. The predictive performance was evaluated by obtaining a minimum average precision of 0.79, a maximum equal to 1.0 and a mAP equal to 0.97. The model was applied to a huge set of passive recordings made in a network of urban wetlands for one year. The acoustic activity results were synchronized with climatological temperature data and sunlight hours. The results confirm that the proposed method allows for monitoring a taxonomically diverse group of birds that nourish the annual soundscape of an ecosystem, as well as detecting the presence of cryptic species that often go unnoticed.}
}
@article{NOLASCO2023102258,
title = {Learning to detect an animal sound from five examples},
journal = {Ecological Informatics},
volume = {77},
pages = {102258},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102258},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300287X},
author = {Ines Nolasco and Shubhr Singh and Veronica Morfi and Vincent Lostanlen and Ariana Strandburg-Peshkin and Ester Vidaña-Vila and Lisa Gill and Hanna Pamuła and Helen Whitehead and Ivan Kiskin and Frants H. Jensen and Joe Morford and Michael G. Emmerson and Elisabetta Versace and Emily Grout and Haohe Liu and Burooj Ghani and Dan Stowell},
keywords = {Bioacoustics, Deep learning, Event detection, Few-shot learning},
abstract = {Automatic detection and classification of animal sounds has many applications in biodiversity monitoring and animal behavior. In the past twenty years, the volume of digitised wildlife sound available has massively increased, and automatic classification through deep learning now shows strong results. However, bioacoustics is not a single task but a vast range of small-scale tasks (such as individual ID, call type, emotional indication) with wide variety in data characteristics, and most bioacoustic tasks do not come with strongly-labelled training data. The standard paradigm of supervised learning, focussed on a single large-scale dataset and/or a generic pre-trained algorithm, is insufficient. In this work we recast bioacoustic sound event detection within the AI framework of few-shot learning. We adapt this framework to sound event detection, such that a system can be given the annotated start/end times of as few as 5 events, and can then detect events in long-duration audio—even when the sound category was not known at the time of algorithm training. We introduce a collection of open datasets designed to strongly test a system's ability to perform few-shot sound event detections, and we present the results of a public contest to address the task. Our analysis shows that prototypical networks are a very common used strategy and they perform well when enhanced with adaptations for general characteristics of animal sounds. However, systems with high time resolution capabilities perform the best in this challenge. We demonstrate that widely-varying sound event durations are an important factor in performance, as well as non-stationarity, i.e. gradual changes in conditions throughout the duration of a recording. For fine-grained bioacoustic recognition tasks without massive annotated training data, our analysis demonstrate that few-shot sound event detection is a powerful new method, strongly outperforming traditional signal-processing detection methods in the fully automated scenario.}
}
@article{CONCEPCION2023102344,
title = {BivalveNet: A hybrid deep neural network for common cockle (Cerastoderma edule) geographical traceability based on shell image analysis},
journal = {Ecological Informatics},
volume = {78},
pages = {102344},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102344},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003734},
author = {Ronnie Concepcion and Marielet Guillermo and Susanne E. Tanner and Vanessa Fonseca and Bernardo Duarte},
keywords = {Traceability, Morphometric analysis, Deep learning, Bivalves, Low-cost},
abstract = {Bivalve traceability is a major concern. It is of utmost importance to develop tools that allow providing important information to the consumer, not only on the origin of the product but also on its sustainability and safety, due to the harvest restrictions imposed by regulatory entities. This study evaluated the application of computer vision machine learning technologies for efficiently discriminating cockle harvesting origin based on shell geometric and morphometric analysis, improving the traceability methodologies in these organisms, and highlighting the potential of these low-cost techniques as a reliable traceability tool. Thirty Cerastoderma edule samples were collected along the five locations in Atlantic West and South Portuguese coast with individual images processed using lazysnapping segmentation, spectro-textural-morphological phenotype extraction, and feature selection through hybrid Principal Component Analysis and Neighborhood Component Analysis which resulted in R, a*, b*, entropy, and diameter. Three approaches of traceability models were developed and tested: pre-trained networks (EfficientNet-Bo, ResNet101, MobileNetV2, InceptionV3) with numerical inputs (Approach 1), image-based pre-trained networks (Approach 2), and hybrid deep neural networks of Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Bidirectional LSTM (BiLSTM) (Approach 3). Based on the test results, Approach 3 with GRU-LSTM-BiLSTM sequence exhibited the highest accuracy (96.91%) and sensitivity (96%) among the other thirteen machine learning models, hence, named as BivalveNet. Comparing the attained accuracy from the BivalveNet to other mollusc traceability studies, it was observed that an efficiency close to the attained using standard destructive, time-consuming, and expensive techniques, making BivalveNet a highly advantageous approach for common cockle geographical traceability studies, available for application to other bivalve species.}
}
@article{BOHNER2023102150,
title = {A semi-automatic workflow to process images from small mammal camera traps},
journal = {Ecological Informatics},
volume = {76},
pages = {102150},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102150},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123001796},
author = {Hanna Böhner and Eivind Flittie Kleiven and Rolf Anker Ims and Eeva M. Soininen},
keywords = {Camera trap, Rodent, Automatic image classification, Adaptive monitoring, Data processing, Deep learning},
abstract = {Camera traps have become popular for monitoring biodiversity, but the huge amounts of image data that arise from camera trap monitoring represent a challenge and artificial intelligence is increasingly used to automatically classify large image data sets. However, it is still challenging to combine automatic classification with other steps and tools needed for efficient, quality-assured and adaptive processing of camera trap images in long-term monitoring programs. Here we propose a semi-automatic workflow to process images from small mammal cameras that combines all necessary steps from downloading camera trap images in the field to a quality checked data set ready to be used in ecological analyses. The workflow is implemented in R and includes (1) managing raw images, (2) automatic image classification, (3) quality check of automatic image labels, as well as the possibilities to (4) retrain the model with new images and to (5) manually review subsets of images to correct image labels. We illustrate the application of this workflow for the development of a new monitoring program of an Arctic small mammal community. We first trained a classification model for the specific small mammal community based on images from an initial set of camera traps. As the monitoring program evolved, the classification model was retrained with a small subset of images from new camera traps. This case study highlights the importance of model retraining in adaptive monitoring programs based on camera traps as this step in the workflow increases model performance and substantially decreases the total time needed for manually reviewing images and correcting image labels. We provide all R scripts to make the workflow accessible to other ecologists.}
}
@article{GIBB2024102449,
title = {Towards interpretable learned representations for ecoacoustics using variational auto-encoding},
journal = {Ecological Informatics},
volume = {80},
pages = {102449},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102449},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004788},
author = {K.A. Gibb and A. Eldridge and C.J. Sandom and I.J.A. Simpson},
keywords = {Representation learning, Variational auto-encoder, Biodiversity monitoring, Deep learning, Ecoacoustics, Passive acoustic monitoring},
abstract = {Ecoacoustics is an emerging science that seeks to understand the role of sound in ecological processes. Passive acoustic monitoring is being used to collect vast quantities of soundscape audio recordings to study variations in acoustic community and monitor biodiversity. However, extracting relevant information from soundscape recordings is non-trivial. Recent approaches to machine-learned acoustic features appear promising but are limited by at least three issues: inductive biases, lack of interpretability and crude temporal integration. In this paper we introduce a novel self-supervised representation learning algorithm for ecoacoustics - a convolutional Variational Auto-encoder (VAE) - and directly address these shortcomings. Firstly, we train the network on soundscape recordings from temperate and tropical field sites along a gradient of ecological degradation to provide a more relevant inductive bias than prior approaches. Secondly, we present a new method that allows interpretation of the latent space for the first time, giving insight into the basis of classification. Thirdly, we advance existing methods for temporal aggregation of learned embeddings by encoding latent features as a distribution over time. Under our approach to increase interpretability, we provide insight into how learned features drive habitat classification for the first time: inspection of latent space confirms that varying combinations of biophony, geophony and anthrophony are used to infer sites along a degradation gradient. Our novel temporal encoding method increases sensitivity to periodic signals and improves on previous research that uses time-averaged representations for site classification. This approach also reveals the contribution of hardware-specific frequency response that create a potential bias; we demonstrate how a simple linear transformation can be used to mitigate the effect of hardware variance on the learned representation under our approach. Our novel approach paves the way for development of a new class of deep neural networks that afford more interpretable learned ecoacoustic representations to advance both fundamental and applied science and support global conservation efforts.}
}
@article{LEVY2024102737,
title = {Improving deep learning based bluespotted ribbontail ray (Taeniura Lymma) recognition},
journal = {Ecological Informatics},
volume = {82},
pages = {102737},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102737},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002796},
author = {Avivit Levy and Adi Barash and Chen Zaguri and Ariel Hadad and Polina Polsky},
keywords = {Computer vision, Citizen science, Ecological study, Marine animal recognition, Pose handling},
abstract = {This paper presents the novel task of bluespotted ribbontail (BR) ray (Taeniura lymma) recognition using deep learning based computer vision methods to enable the identification of specific individuals of this species. Mapping the specific individuals in relation to location and time will allow marine researchers to understand their movement patterns, habitat choice, life span, size of the population and more – data which could allow monitoring and establishing a tailor-made conservation plan for this species. Our work is pioneer on this recognition problem. We give a detailed description of the three basic steps of detection, feature extraction and recognition in this vision problem and perform experiments to explore the system configuration and what improves the performance. A feature extraction enhancement as well as a crucial effect of a split into different main poses are demonstrated. Though the precision results achieved in this paper are still moderate and should be further improved, they are nevertheless promising and reasonable for practical use if the six best matches are chosen. For this scenario, almost 85% precision for upper-pose model, and almost 80% precision for left- and right-pose models, are achieved demonstrating the feasibility of the pipeline suggested as well as opportunities for improvement.}
}
@article{VEGA2024102535,
title = {Convolutional neural networks for hydrothermal vents substratum classification: An introspective study},
journal = {Ecological Informatics},
volume = {80},
pages = {102535},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102535},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000773},
author = {Pedro Juan Soto Vega and Panagiotis Papadakis and Marjolaine Matabos and Loïc {Van Audenhaege} and Annah Ramiere and Jozée Sarrazin and Gilson Alexandre Ostwald Pedro {da Costa}},
keywords = {Image classification, Deep learning, Hydrothermal vents, Uncertainty analysis},
abstract = {The increasing availability of seabed images has created new opportunities and challenges for monitoring and better understanding the spatial distribution of fauna and substrata. To date, however, deep-sea substratum classification relies mostly on visual interpretation, which is costly, time-consuming, and prone to human bias or error. Motivated by the success of convolutional neural networks in learning semantically rich representations directly from images, this work investigates the application of state-of-the-art network architectures, originally employed in the classification of non-seabed images, for the task of hydrothermal vent substrata image classification. In assessing their potential, we conduct a study on the generalization, complementarity and human interpretability aspects of those architectures. Specifically, we independently trained deep learning models with the selected architectures using images obtained from three distinct sites within the Lucky-Strike vent field and assessed the models' performances on-site as well as off-site. To investigate complementarity, we evaluated a classification decision committee (CDC) built as an ensemble of networks in which individual predictions were fused through a majority voting scheme. The experimental results demonstrated the suitability of the deep learning models for deep-sea substratum classification, attaining accuracies reaching up to 80% in terms of F1-score. Finally, by further investigating the classification uncertainty computed from the set of individual predictions of the CDC, we describe a semiautomatic framework for human annotation, which prescribes visual inspection of only the images with high uncertainty. Overall, the results demonstrated that high accuracy values of over 90% F1-score can be obtained with the framework, with a small amount of human intervention.}
}
@article{JAMES2024102580,
title = {Monitoring vegetation patterns and their drivers to infer resilience: Automated detection of vegetation and megaherbivores from drone imagery using deep learning},
journal = {Ecological Informatics},
volume = {81},
pages = {102580},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102580},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001225},
author = {Rebecca K. James and Freek Daniels and Aneesh Chauhan and Pramaditya Wicaksono and Muhammad Hafizt and Setiawan Djody Harahap and Marjolijn J.A. Christianen},
keywords = {Semantic segmentation, Object detection, Pytorch, Seagrass, Drone imagery, Turtle monitoring, Conservation},
abstract = {Ecological pattern theory has highlighted spatial vegetation patterns that can be used as indicators of ecosystem resilience. Combining this spatial pattern theory with aerial imagery from drones and automated image processing with deep learning methods, we show how monitoring of natural ecosystems can be enhanced through quantifying vegetation spatial patterns. We demonstrate this approach in a tropical seagrass ecosystem with a high abundance of turtles that generate vegetation patches when grazing. Past field observations suggest that patch size and density reflect the seagrass meadow resilience, but understanding the natural variation in vegetation patchiness is crucial. Employing the deep learning methods of semantic segmentation and object detection, we quantify vegetation patchiness metrics and turtle distribution across 12 ha of seagrass meadow in the years 2012 and 2022. The resulting output facilitates spatial and temporal comparisons, revealing areas of low resilience. In 2012, turtle grazing across the entire site yielded vegetation patch sizes averaging 2 ± 0.2 m2 (95% confidence interval). Reduced patch sizes of 0.24 ± 0.05 m2 and 0.67 ± 0.6 m2 at the reef edge and beach slope respectively, in conjunction with a reduced patch density, indicated lower resilience at the seagrass meadow edges. Analysis of the 2022 dataset indicates a general decrease in patch size over time, suggesting declining resilience. A retraining experiment of the semantic segmentation model was conducted where the initial model was retrained on the 2022 dataset and demonstrated the adaptability of the deep learning methods. Despite using different equipment, the model achieved high accuracy with only 5–10 additional training images. By providing the tools to conduct these analyses, we aim to stimulate the uptake of deep learning for enhancing the data obtained from aerial imagery to improve the monitoring and conservation of natural ecosystems.}
}
@article{CHAIALLAH2023102332,
title = {Mining crowdsourced text to capture hikers' perceptions associated with landscape features and outdoor physical activities},
journal = {Ecological Informatics},
volume = {78},
pages = {102332},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102332},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003618},
author = {Abdesslam Chai-allah and Nathan Fox and Fritz Günther and Fadila Bentayeb and Gilles Brunschwig and Sandro Bimonte and Frédéric Joly},
keywords = {Cultural ecosystem services, Hiking, Natural language processing, Text analysis, Wikiloc},
abstract = {Outdoor recreation provides vital interactions between humans and ecological systems with a range of mental and physical benefits for people. Despite the increased number of studies using crowdsourced online data to assess how people interact with the landscape during recreational activities, the focus remains largely on mapping the spatial distribution of visitors or analyzing the content of shared images and little work has been done to quantify the perceptions and emotions people assign to the landscape. In this study, we used crowdsourced textual data from an outdoor activity-sharing platform (Wikiloc), and applied Natural Language Processing (NLP) methods and correlation analysis to capture hikers' perceptions associated with landscape features and physical outdoor activities. Our results indicate eight clusters based on the semantic similarity between words ranging from four clusters describing landscape features (“ecosystems, animals & plants”, “geodiversity”, “climate & weather”, and “built cultural heritage”), to one cluster describing the range of physical outdoor activities and three clusters indicating hikers' perceptions and emotions (“aesthetics”, “joy & restoration” and “physical effort sensation”). The association analysis revealed that the cluster “ecosystems, animals & plants” is likely to stimulate all three identified perceptions, suggesting that these natural features are important for hikers during their outdoor experience. Moreover, hikers strongly associate the cluster “outdoor physical activities” with both “joy & restoration” and “physical effort sensation” perceptions, highlighting the health and well-being benefits of physical activities in natural landscapes. Our study shows the potential of Wikiloc as a valuable data source to assess human-nature interactions and how textual data can provide significant advances in understanding peoples' preferences and perceptions while recreating. These findings can help inform outdoor recreation planners in the study region by focusing on the elements of the landscape that peoples perceive to be important (i.e. “ecosystems, animals & plants”).}
}
@article{ZHANG2024102467,
title = {Marine zoobenthos recognition algorithm based on improved lightweight YOLOv5},
journal = {Ecological Informatics},
volume = {80},
pages = {102467},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102467},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000098},
author = {Lijun Zhang and Jiawen Fan and Yi Qiu and Zhe Jiang and Qingsong Hu and Bowen Xing and Jingxiang Xu},
keywords = {Ecological monitoring, Marine zoobenthos, Model lightweight, YOLOv5, EfficientnetV2, Bottleneck transformer},
abstract = {Detecting the distribution and density of marine zoobenthos is crucial for monitoring healthy coastal ecosystems and for growth reference tracking in precision aquaculture. However, current detection algorithms for marine zoobenthos have high computational complexity and cannot guarantee a balance between accuracy and speed, limiting their deployment in fishery equipment. This study used a portion of the Augmented Underwater Detection Dataset, a large underwater biological dataset containing marine zoobenthos data. A marine zoobenthos recognition algorithm was proposed for sea cucumbers, sea urchins, and scallops based on an improved lightweight YOLOv5, which can recognize the three types of marine zoobenthos. In the image enhancement module, an underwater image enhancement algorithm based on color balance and multi-input fusion is used, which turns the blurred image into a natural appearance of the seabed image. The lightweight backbone network EfficientnetV2-S was chosen to replace the original YOLOv5 backbone network, reducing network parameter calculations and improving recognition speed. A Bottleneck Transformer was introduced into the backbone network, and an attention mechanism based on the convolution module was introduced to construct the embedded Convolutional Block Attention Module in the Neck structure of YOLOv5, thereby improving the recognition accuracy of the lightweight YOLOv5 model. The experimental results showed that the mAP of the proposed algorithm reached 0.941, which is an improvement of 0.002 compared with the original YOLOv5l algorithm. The computation of this algorithm is 37.0 FLOPs (G), the model size is 54 MB, and the inference time is 5.9 ms. Compared to the original YOLOv5l algorithm, the reductions are 66.1%, 40.5%, and 39.2%. The proposed algorithm efficiently identified and classified marine zoobenthos.}
}
@article{ZHENG2024102689,
title = {A video object segmentation-based fish individual recognition method for underwater complex environments},
journal = {Ecological Informatics},
volume = {82},
pages = {102689},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102689},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002310},
author = {Tao Zheng and Junfeng Wu and Han Kong and Haiyan Zhao and Boyu Qu and Liang Liu and Hong Yu and Chunyu Zhou},
keywords = {Individual fish recognition, Video object segmentation, Underwater complex environments, Deep learning, Intelligent aquaculture},
abstract = {Currently, aquaculture methods tend to combine scale and intelligence, which saves manpower and improves the survival rate of seafood at the same time. High-precision and high-efficiency fish individual recognition can provide key technical support for fish disease detection, feeding habits, body condition, etc. In the realm of intelligent aquaculture, it provides robust data support for precision fish farming. However, the current research methods for individual fish recognition struggle to maintain the network model's focus on the fish body in real marine underwater complex environments (e.g., environmental background interference such as coral reefs, overlap between fish bodies, light noise, etc.), leading to unsatisfactory recognition results. To this end, this paper proposes a method for fish individual recognition in underwater complex environments based on video object segmentation, which consists of three parts, including a fish individual segmentation detection module, a fish individual recognition module, and an all-in-one visualization module. The work adopts a combination of deep learning methods and video object segmentation algorithms to solve the problem of low attention and poor detection accuracy of fish individuals in real underwater complex environments, which effectively improves the accuracy and efficiency of fish individual recognition, and analyzes and discusses the comparison of recognition effects using different weights. The results of the simulation experiments show that the key metric Rank1 value of the method achieves more than 96% accuracy on the public datasets DlouFish, WideFish, and the Fish-seg dataset produced in this paper, and improves over the state-of-the-art methods for fish individual recognition by 2.23%, 1.33%, and 1.25%, respectively.}
}
@article{DEROT2024102437,
title = {Improved climate time series forecasts by machine learning and statistical models coupled with signature method: A case study with El Niño},
journal = {Ecological Informatics},
volume = {79},
pages = {102437},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102437},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004661},
author = {Jonathan Derot and Nozomi Sugiura and Sangyeob Kim and Shinya Kouketsu},
keywords = {Signature method, El Niño, Random Forest model, Long-short-term-memory model, Lasso model, Partial dependence plot, Climate time series},
abstract = {The different phases of ENSO (El Niño Southern Oscillation) directly influence the occurrence of natural disasters and global warming. To limit the socio-economic impact, it is essential to develop simple and fast numerical models that can predict these different cycles. Here, we aimed to improve the predictive performance and extracting relevant information from climatic events by applying the signature method to time series models. After transforming the data using this signature method, we performed a comparative analysis of the statistical and machine learning models. In addition, we used PDP (Partial Dependence Plot) and SPRC (Standard Partial Regression Coefficient) to better understand the interactions between different climate indices. Our results showed that the best predictive performance was obtained when we use the signature method with the LSTM (R2 = 0.74) and Lasso (R2 = 0.79) models. Two complementary methods were used to highlight the influence of the following climate indices on ENSO cycle changes: NINO3, NINO3.4 and NPI (North Pacific Index). These methodologies also enabled us to determine the switchover thresholds, and order temporal variations. With this first application of the signature method to this type of time series, we obtained accurate forecasts on a 6-month scale with reduced computation time. This suggests that our methodology can be applied to many other fields of research that use multivariate time-series analyses.}
}
@article{KIM2024102576,
title = {Application of the domain adaptation method using a phenological classification framework for the land-cover classification of North Korea},
journal = {Ecological Informatics},
volume = {81},
pages = {102576},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102576},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001183},
author = {Joon Kim and Hyun-Woo Jo and Whijin Kim and Yujeong Jeong and Eunbeen Park and Sujong Lee and Moonil Kim and Woo-Kyun Lee},
keywords = {Land-cover mapping, Phenological classification, Domain adaptation, North Korea, Deep learning},
abstract = {Efforts to achieve carbon neutrality are global, encompassing a wide range of factors. For the estimation of greenhouse gas emissions from the agriculture, forestry, and other land use (AFOLU) sector, the Intergovernmental Panel on Climate Change has proposed an advanced method that requires Approach 3, the highest level of suggested method, of activity data. Accordingly, we propose a phenological classification framework (PCF) that can perform land-cover classification by training the climatic repeatability of the annual cycle using a U-Net deep learning model. Additionally, the domain adaptation (DA) method can be applied to classify areas with insufficient data. We applied these methods to classify North Korea (i.e., using South Korean data), with an accuracy of 81.31%; overall this effort culminated in the simultaneous classification of the Korean Peninsula. Domain distribution comparison showed that the results for the two regions were similar. The PCF and DA methods proposed in this study allow for annual production of a land-cover map and change matrix, regardless of the presence or absence of data. The application of these methods is expected to provide a scientific basis for policy decisions that can facilitate the global attainment of carbon neutrality.}
}
@article{LIU2024102401,
title = {YWnet: A convolutional block attention-based fusion deep learning method for complex underwater small target detection},
journal = {Ecological Informatics},
volume = {79},
pages = {102401},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102401},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004302},
author = {Pingzhu Liu and Wenbin Qian and Yinglong Wang},
keywords = {Underwater target detection, Deep learning, Feature fusion, Attention mechanism},
abstract = {Underwater target detection holds a noteworthy role in the field of marine exploration. However, it is difficult to extract useful feature information from blurred images with complex backgrounds, resulting in suboptimal and unsatisfactory target detection in conventional models. Among them, YOLOv5 leverages the advantages of fast detection performs better in detecting underwater samples. Nevertheless, YOLOv5 still faces difficulties including missed and incorrect detections due to the underwater environment's small scale of objects, dense distribution of organisms, and occlusion. To address these challenges, we propose a novel YoLoWaternet (YWnet) model that builds upon the YOLOv5 framework for complex underwater species detection with three main innovations: 1) A convolutional block attention module (CBAM) is introduced to enhance feature extraction for blurry images in the initial stages of the network and a new feature fusion network called the CRFPN is created to transfer important information and detect underwater targets. 2) A novel feature extraction module is presented, namely, the skip residual C3 module (SRC3), by effectively merging information from various scales to minimize the loss of original data during transmission. 3) Regression and classification algorithms are separated using the decoupled head to improve the effectiveness of detection and the EIoU loss function is employed to accelerate the convergence speed. Finally, the experimental results demonstrate that YWnet achieves remarkable accuracies of 73.2% mAp and 39.3% mAp50–95 on the underwater dataset, surpassing YOLOv5 by 2.3% and 2.4%, respectively. Furthermore, the proposed fusion model outperforms nine state-of-the-art baseline models on the undersea dataset and has generalization capabilities in other datasets.}
}
@article{LEKUNBERRI2022101495,
title = {Identification and measurement of tropical tuna species in purse seiner catches using computer vision and deep learning},
journal = {Ecological Informatics},
volume = {67},
pages = {101495},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101495},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121002867},
author = {Xabier Lekunberri and Jon Ruiz and Iñaki Quincoces and Fadi Dornaika and Ignacio Arganda-Carreras and Jose A. Fernandes},
keywords = {Computer vision, Deep learning, Fisheries, Electronic monitoring, Tropical tuna species identification},
abstract = {Fishery monitoring programs are essential for effective management of marine resources, as they provide scientists and managers with the necessary data for both the preparation of scientific advice and fisheries control and surveillance. The monitoring is generally done by human observers, both in port and onboard, with a high cost involved. Consequently, some Regional Fisheries Management Organizations (RFMO) are opting for electronic monitoring (EM) as an alternative or complement to human observers in certain fisheries. This is the case of the tropical tuna purse seine fishery operating in the Indian and Atlantic oceans, which started an EM program on a voluntary basis in 2017. However, even when the monitoring is conducted though EM, the image analysis is a tedious task manually performed by experts. In this paper, we propose a cost-effective methodology for the automatic processing of the images already being collected by cameras onboard tropical tuna purse seiners. Firstly, the images are preprocessed to homogenize them across all vessels and facilitate subsequent steps. Secondly, the fish are individually segmented using a deep neural network (Mask R-CNN). Then, all segments are passed through other deep neural network (ResNet50V2) to classify them by species and estimate their size distribution. For the classification of fish, we achieved an accuracy for all species of over 70%, i.e., about 3 out of 4 individuals are correctly classified to their corresponding species. The size distribution estimates are aligned with official port measurements but calculated using a larger number of individuals. Finally, we also propose improvements to the current image capture systems which can facilitate the work of the proposed automation methodology.}
}
@article{VABO2021101322,
title = {Automatic interpretation of salmon scales using deep learning},
journal = {Ecological Informatics},
volume = {63},
pages = {101322},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101322},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121001138},
author = {Rune Vabø and Endre Moen and Szymon Smoliński and Åse Husebø and Nils Olav Handegard and Ketil Malde},
keywords = {Fish scales, Deep learning, EfficientNet, Transfer learning, Age reading, Maturity staging},
abstract = {For several fish species, age and other important biological information is manually inferred from visual scrutinization of scales, and reliable automatic methods are not widely available. Here, we apply Convolutional Neural Networks (CNN) with transfer learning on a novel dataset of 9056 images of Atlantic salmon scales for four different prediction tasks. We predicted fish origin (wild/farmed), spawning history (previous spawner/non-spawner), river age, and sea age. We obtained high prediction accuracy for fish origin (96.70%), spawning history (96.40%), and sea age (86.99%), but lower accuracy for river age (63.20%). Against six human expert readers with an additional dataset of 150 scales, the CNN showed the second-highest percentage agreement for sea age (94.00%, range 87.25±97.30%), but the lowest agreement for river age (66.00%, range 66.00– 84.68%). Estimates of river age by expert readers exhibited higher variance and lower levels of agreement compared to sea age and may indicate why this task is also more difficult for the CNN. Automatic interpretation of scales may provide a cost- and time-efficient method of predicting fish age and life-history traits.}
}
@article{NAZIR2024102453,
title = {Object classification and visualization with edge artificial intelligence for a customized camera trap platform},
journal = {Ecological Informatics},
volume = {79},
pages = {102453},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102453},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300482X},
author = {Sajid Nazir and Mohammad Kaleem},
keywords = {Data science, Computer vision, Deep learning, Model generalization, Fine tuning, Explainable AI, Hyperparameter tuning, Vision transformers},
abstract = {The camera traps have revolutionized the image and video capture in ecology and are often used to monitor and record animal presence. With miniaturization of low power electronic devices, better battery technologies, and software advancements, it has become possible to use the edge devices, such as Raspberry Pi as camera traps that can not only capture images and videos, but can also enable sophisticated image processing, and off-site communications. These developments can help to provide near real-time insights and reduce the manual processing of images. The on-board image classification and visualization is facilitated by the advancements in the Deep Neural Networks (DNN), transfer learning approaches, and software libraries. This paper provides an investigation of image classification with transfer learning approaches using pre-trained DNN models, and visualizations with Explainable Artificial Intelligence (XAI) techniques on Raspberry Pi Zero (RPi-Z) edge device. The MobileNetV2 model was used for image classification on the Florida-Part1 dataset obtaining the results for precision, recall, and F1-score as 0.95, 0.96, and 0.95 respectively. We also compared the model performance of MobileNetV2, EfficientNetV2B0, and MobileViT models for classification on the Extinction dataset with the best results for precision, recall, and F1-score as 0.97, 0.96, and 0.96 respectively, obtained with the EfficientNetV2B0 model. Two XAI techniques, Gradient-weighted-Class Activation Mapping (Grad-CAM) and Occlusion Sensitivity were used for visualization through heatmaps, to highlight the relative importance of the image areas contributing to the DNN model's prediction, that can also help to understand the model's performance and bias. The results provide practical use case scenarios for utilizing the transfer learning approaches, model optimization and deployment to edge devices, and model visualizations in ecological research.}
}
@article{SIGURDARDOTTIR2023102046,
title = {Otolith age determination with a simple computer vision based few-shot learning method},
journal = {Ecological Informatics},
volume = {76},
pages = {102046},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102046},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000754},
author = {Andrea Rakel Sigurðardóttir and Þór Sverrisson and Aðalbjörg Jónsdóttir and María Gudjónsdóttir and Bjarki Þór Elvarsson and Hafsteinn Einarsson},
keywords = {Otoliths, Fish age estimation, Few-shot learning, Deep-learning, Image analysis},
abstract = {In this study, we propose a computer vision-based few-shot learning method for otolith age determination in European plaice, Atlantic cod, Greenland halibut, and haddock. Our method outperforms prior state-of-the-art approaches, and is based on a vision encoder from CLIP as a feature extractor, which is used to train shallow models. The method is computationally efficient, as it does not require fine-tuning of deep networks, and is also data efficient, as it performs better than fine-tuning on the same data. Our results suggest that in some cases, our method can achieve the same performance as state-of-the-art finetuning approaches with up to three times less training data.}
}
@article{WANG2024102538,
title = {Hierarchical-taxonomy-aware and attentional convolutional neural networks for acoustic identification of bird species: A phylogenetic perspective},
journal = {Ecological Informatics},
volume = {80},
pages = {102538},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102538},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000803},
author = {Qingyu Wang and Yanzhi Song and Yeqian Du and Zhouwang Yang and Peng Cui and Binnan Luo},
keywords = {Bioacoustics, Bird sound recognition, Hierarchical architecture, Attention module, Path correction},
abstract = {The study of bird populations is crucial for biodiversity research and conservation. Deep artificial neural networks have revolutionized bird acoustic recognition; however, most methods overlook inherent relationships among bird populations, resulting in the loss of biological information. To address this limitation, we propose the Phylogenetic Perspective Neural Network (PPNN), which incorporates hierarchical multilevel labels for each bird. PPNN uses a hierarchical semantic embedding framework to capture feature information at different levels. Attention mechanisms are employed to extract and select common and distinguishing features, thereby improving classification accuracy. We also propose a path correction strategy to rectify inconsistent predictions. Experimental results on bird acoustic datasets demonstrate that PPNN outperforms current methods, achieving classification accuracies of 90.450%, 91.883%, and 89.950% on the Lishui-Zhejiang Birdsdata (100 species), BirdCLEF2018-Small (150 species), and BirdCLEF2018-Large (500 species) datasets respectively, with the lowest hierarchical distance of a mistake across all datasets. Our proposed method is applicable to any bird acoustic dataset and presents significant advantages as the number of categories increases.}
}
@article{KUMAR2024102510,
title = {Bird species recognition using transfer learning with a hybrid hyperparameter optimization scheme (HHOS)},
journal = {Ecological Informatics},
volume = {80},
pages = {102510},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102510},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000529},
author = {Samparthi V.S. Kumar and Hari Kishan Kondaveeti},
keywords = {Automatic bird species recognition, Convolutional neural network, Deep learning, Hyperparameter tuning, Transfer learning},
abstract = {The use of automatic bird species recognition methods reduces the burden on scientists, ornithologists, and bird watchers as these methods help identify birds with minimal human effort and intervention. The current study employs a transfer learning approach combined with a Hybrid hyperparameter Optimization Scheme (HHOS) to enhance the efficiency and accuracy of automatic bird species recognition. First, the weights of selected pre-trained deep learning models are downloaded from ImageNet, and a few new trainable layers are added at the top. Thereafter, the selected models are trained using HHOS, which strategically integrates both manual and random searches to achieve favorable results. The manual search relies on domain knowledge and experience to identify the best hyperparameter settings, thereby making the search space smaller and more focused. Random search tests various combinations of the hyperparameters identified in manual search and trains the selected models to achieve the maximum possible accuracy through multiple iterations. Experimental analysis revealed that the Fine-tuned EfficientNetB0 model exhibited superior performance, achieving an accuracy of 99.12%. In contrast, the performance of the ResNet18 model was disappointing with an accuracy of 93.24%, while other models outperformed it.}
}
@article{GAME2024102619,
title = {Machine learning for non-experts: A more accessible and simpler approach to automatic benthic habitat classification},
journal = {Ecological Informatics},
volume = {81},
pages = {102619},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102619},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001614},
author = {Chloe A. Game and Michael B. Thompson and Graham D. Finlayson},
keywords = {Machine learning, Computer vision, Convolutional neural network, Support vector machine, User-friendly, Benthic habitats},
abstract = {Automating identification of benthic habitats from imagery, with Machine Learning (ML), is necessary to contribute efficiently and effectively to marine spatial planning. A promising method is to adapt pre-trained general convolutional neural networks (CNNs) to a new classification task (transfer learning). However, this is often inaccessible to a non-specialist, requiring large investments in computational resources and time (for user comprehension and model training). In this paper, we demonstrate a simpler transfer learning framework for classifying broad deep-sea benthic habitats. Specifically, we take an ‘off-the-shelf’ CNN (VGG16) and use it to extract features (pixel patterns) from benthic images (without further training). The default outputs of VGG16 are then fed in to a Support Vector Machine (SVM), a classical and simpler method than deep networks. For comparison, we also train the remaining classification layers of VGG16 using stochastic gradient descent. The discriminative power of these approaches is demonstrated on three benthic datasets (574–8353 images) from Norwegian waters; each using a unique imaging platform. Benthic habitats are broadly classified as Soft Substrate (sands, muds), Hard Substrate (gravels, cobbles and boulders) and Reef (Desmophyllum pertusum). We found that the relatively simplicity of the SVM classifier did not compromise performance. Results were competitive with the CNN classifier and consistently high, with test accuracy ranging from 0.87 to 0.95 (average = 0.9 (±0.04)) across datasets, somewhat increasing with dataset size. Impressively, these results were achieved 2.4–5× faster than CNN training and had significantly less dependency on high-specification hardware. Our suggested approach maximises conceptual and practical simplicity, representing a realistic baseline for novice users when approaching benthic habitat classification. This method has wide potential. It allows automated image grouping to aid annotation or further model selection, as well as screening of old-datasets. It is especially suited to offshore scenarios as it can provide quick, albeit crude, insights into habitat presence, allowing adaptation of sampling protocols in near real-time.}
}
@article{DENG2024102546,
title = {Weed database development: An updated survey of public weed datasets and cross-season weed detection adaptation},
journal = {Ecological Informatics},
volume = {81},
pages = {102546},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102546},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000888},
author = {Boyang Deng and Yuzhen Lu and Jiajun Xu},
keywords = {Deep learning, Domain adaptation, Machine vision, Precision agriculture, Robustness, Weed detection},
abstract = {Weeds are a major threat to crop production. Automated innovations for reducing herbicides and labor needed for weeding have become a high priority for sustainable weed management. The current state-of-the-art weeding systems still cannot reliably recognize weeds in changing field conditions for precision weed control. Enhancing weed recognition accentuates the critical need to develop dedicated, labeled weed databases and whereby train advanced AI (artificial intelligence) models while ensuring the robustness of models across diverse field conditions. This study presents an up-to-date survey on publicly available image datasets for weed recognition. Among 36 datasets identified, limitations exist in terms of data variations and distribution shifts, and few of the datasets are suitable for examining the robustness of weed recognition models. A new two-season, eight-class weed dataset is described in this study, comprising two sub-datasets of images collected in the seasons of 2021 and 2022, respectively. Three state-of-the-art deep learning object detectors, i.e., YOLOX, YOLOv8, and DINO, were benchmarked and evaluated for their in-season and cross-season weed detection performance on the dataset. All three models attained in-season detection accuracies of 92% and higher in terms of mAP@50. However substantial accuracy drops of up to 14.5% were observed between in-season and cross-season testing, especially for YOLOX and YOLOv8. Unsupervised domain adaptation based on an implicit instance-invariant network (I3Net) was utilized for improved generalization of the YOLO models. The I3Net-based models resulted in accuracy improvements of 1.4% and 3.3% for YOLOX and YOLOv8, respectively, compared to modeling without domain adaptation, in the cross-season testing. Both the two-season detection dataset11https://doi.org/10.5281/zenodo.10762138 and software programs22https://github.com/vicdxxx/CrossSeasonWeedDetection for weed detection modeling in this study are made publicly available.}
}
@article{BRAVODIAZ2024102684,
title = {Evaluating the ability of convolutional neural networks for transfer learning in Pinus radiata cover predictions},
journal = {Ecological Informatics},
volume = {82},
pages = {102684},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102684},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002267},
author = {A. Bravo-Diaz and S. Moreno and J. Lopatin},
keywords = {Invasive species, Unpiloted aerial vehicles (UAV), Spatial variability, Regression, Transfer domain},
abstract = {The species Pinus radiata is highly invasive in native forests in Chile, drastically affecting the functioning and structure of ecosystems. Hence, it is imperative to develop robust approaches to detect P. radiata invasions at different scales. Models based on convolutional neural networks (CNN) have proven to be a promising alternative to detect plant invasions in high-resolution remote sensing data, such as those obtained by drones. However, studies have been limited in their spatial variability and their assessments of transferability or transfer learning to new sectors, hindering the ability to use these models in a real-world setting. We train models based on CNN architectures using unpiloted aerial vehicle data and evaluate their ability to transfer learning outside the training domain using regression approaches. We compared models trained with low spatial variability (mono-site) with those with high spatial variability (multi-site). We further sought to maximize the transference of learning outside the training domain by searching among different architectures and models, maximizing the evaluation in an independent data set. The results showed that transfer learning is better when multi-site models with higher spatial variability are used for training, obtaining a coefficient of determination R2 between 60% and 87%. On the contrary, mono-site models present a wide variability of performance attributed to the dissimilarity of information between sites, limiting the possibilities of using these models for extrapolations or model generalizations. We also obtained a significant difference between within-domain generalization using test data versus transfer learning outside the training domain, showing that testing data alone cannot depict such discrepancy without further data. Finally, the best models for transfer learning on new data domains often do not agree with those selected by the standard training/validation/testing scheme. Our findings pave the way for deeper discussions and further investigations into the limitations of CNN models when applied to high-resolution imagery.}
}
@article{BAKHT2024102631,
title = {MuLA-GAN: Multi-Level Attention GAN for Enhanced Underwater Visibility 11⁎First two authors has equal contribution⋆github code link:https://github.com/AhsanBaidar/MuLAGAN.git ORCID (s): 0000–0002–9079-0960 (A.B. Bakht); 0000–0001–9789-8483 (Z. Jia); 0000–0001–6214-1077 (M.U. Din); 0000–0002–7401-5120 (W. Akram); 0000–0003–4445-3135 (L.S. Saoud); 0000–0001–6405-8402 (L. Seneviratne); 0000–0001–6432-5187 (S. He); 0000–0003–2759-0306 (I. Hussain)},
journal = {Ecological Informatics},
volume = {81},
pages = {102631},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102631},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001730},
author = {Ahsan B. Bakht and Zikai Jia and Muhayy Ud Din and Waseem Akram and Lyes Saad Saoud and Lakmal Seneviratne and Defu Lin and Shaoming He and Irfan Hussain},
keywords = {Underwater image enhancement, Generative adversarial networks (GANs), Spatio-channel attention, Computer vision, Real-time image processing},
abstract = {The underwater environment presents unique challenges (color distortions, reduced contrast, blurriness) hindering accurate analysis. This work introduces MuLA-GAN, a novel approach leveraging Generative Adversarial Networks (GANs) and specifically adapted Multi-Level Attention for comprehensive underwater image enhancement. MuLA-GAN integrates Multi-Level Attention within the GAN architecture to prioritize learning discriminative features crucial for precise image restoration. These relevant features encompass information on local details within image regions leveraged by spatial attention and features at various scales across the entire image captured by multi-level attention. This allows MuLA-GAN to identify and enhance objects, textures, and edges obscured by underwater distortions while also reconstructing a more accurate and visually clear representation of the underwater scene by analyzing low-level information like edges and textures, as well as high-level information like object shapes and global scene information. By selectively focusing on these relevant features, MuLA-GAN excels at capturing and preserving intricate details in underwater imagery, which is essential for various marine research, exploration, and resource management applications. Extensive evaluations on diverse datasets (UIEB test, UIEB challenge, U45, UCCS) demonstrate MuLA-GAN's superior performance compared to existing methods. Additionally, a specialized bio-fouling and aquaculture dataset confirms the model's robustness in challenging environments. On the UIEB test dataset, MuLA-GAN achieves exceptional Peak Signal-to-Noise Ratio (PSNR) (25.59) and Structural Similarity Index (SSIM) (0.893) scores, surpassing Water-Net (24.36 PSNR, 0.885 SSIM). This work addresses a significant research gap in underwater image enhancement by demonstrating the effectiveness of combining GANs with specifically adapted Multi-Level Attention mechanisms. This tailored approach offers a novel and comprehensive framework for restoring underwater image quality, providing valuable insights for accurate underwater scene analysis. The source code for MuLA-GAN is publicly available on GitHub at https://github.com/AhsanBaidar/MuLA_GAN.git}
}
@article{HASAN2023102361,
title = {Image patch-based deep learning approach for crop and weed recognition},
journal = {Ecological Informatics},
volume = {78},
pages = {102361},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102361},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003904},
author = {A S M Mahmudul Hasan and Dean Diepeveen and Hamid Laga and Michael G.K. Jones and Ferdous Sohel},
keywords = {Weed classification, Deep learning, Patch-based technique, Digital agriculture},
abstract = {Accurate classification of weed species in crop plants plays a crucial role in precision agriculture by enabling targeted treatment. Recent studies show that artificial intelligence deep learning (DL) models achieve promising solutions. However, several challenging issues, such as lack of adequate training data, inter-class similarity between weed species and intra-class dissimilarity between the images of the same weed species at different growth stages or for other reasons (e.g., variations in lighting conditions, image capturing mechanism, agricultural field environments) limit their performance. In this research, we propose an image based weed classification pipeline where a patch of the image is considered at a time to improve the performance. We first enhance the images using generative adversarial networks. The enhanced images are divided into overlapping patches, a subset of which are used for training the DL models. For selecting the most informative patches, we use the variance of Laplacian and the mean frequency of Fast Fourier Transforms. At test time, the model's outputs are fused using a weighted majority voting technique to infer the class label of an image. The proposed pipeline was evaluated using 10 state-of-the-art DL models on four publicly available crop weed datasets: DeepWeeds, Cotton weed, Corn weed, and Cotton Tomato weed. Our pipeline achieved significant performance improvements on all four datasets. DenseNet201 achieved the top performance with F1 scores of 98.49%, 99.83% and 100% on Deepweeds, Corn weed and Cotton Tomato weed datasets, respectively. The highest F1 score on the Cotton weed dataset was 98.96%, obtained by InceptionResNetV2. Moreover, the proposed pipeline addressed the issues of intra-class dissimilarity and inter-class similarity in the DeepWeeds dataset and more accurately classified the minority weed classes in the Cotton weed dataset. This performance indicates that the proposed pipeline can be used in farming applications.}
}
@article{SONG2024102466,
title = {Benchmarking wild bird detection in complex forest scenes},
journal = {Ecological Informatics},
volume = {80},
pages = {102466},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102466},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000086},
author = {Qi Song and Yu Guan and Xi Guo and Xinhui Guo and Yufeng Chen and Hongfang Wang and Jianping Ge and Tianming Wang and Lei Bao},
keywords = {Object detection, Bird detection, Deep learning, Camera trap},
abstract = {Camera traps are widely used for wildlife monitoring and making informed conservation and land-management decisions, but the resulting ‘big data’ are laborious to process. Deep learning-based methods have been adopted for wildlife detection in camera traps. However, these methods detect large mammals in uncomplicated scenes, where powerful deep-learning models work effectively. Few studies have been conducted to develop artificial intelligence for recognizing wild birds that live in complicated field scenes with protective colors and small sizes. Here we used a dataset of 9717 images from 15 bird species based on camera traps to test 8 object detection algorithms (Faster RCNN, Cascade RCNN, RetinaNet, FCOS, RepPoints, ATSS, Deformable-DETR, and Sparse RCNN) and assess their performance. We also explored the effect of different backbones on model accuracy. Among them, the Cascade RCNN model performs best, with a mAP of 0.693 in model capabilities. Models perform differently in certain species, and backbones significantly affect the accuracy of the model. Cascade RCNN utilizing the Swin-T backbone is the best-performing combination, with a mAP of 0.704. This study could help researchers identify birds efficiently and inspires research on wildlife recognition in complex ecological settings.}
}
@article{EFFROSYNIDIS2018158,
title = {Seagrass detection in the mediterranean: A supervised learning approach},
journal = {Ecological Informatics},
volume = {48},
pages = {158-170},
year = {2018},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2018.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1574954118301560},
author = {Dimitrios Effrosynidis and Avi Arampatzis and Georgios Sylaios},
keywords = {Seagrass classification, Dataset integration and fusion, Machine learning, Data mining, Mediterranean Sea},
abstract = {We deal with the problem of detecting seagrass presence/absence and distinguishing seagrass families in the Mediterranean via supervised learning methods. By merging datasets about seagrass presence and other external environmental variables, we develop suitable training data, enhanced by seagrass absence data algorithmically produced based on certain hypotheses. Experiments comparing several popular classification algorithms yield up to 93.4% accuracy in detecting seagrass presence. In a feature strength analysis, the most important variables determining presence–absence are found to be Chlorophyll-α levels and Distance-to-Coast. For determining family, variables cannot be easily singled out; several different variables seem to be of importance, with Chlorophyll-α surpassing all others. In both problems, tree-based classification algorithms perform better than others, with Random Forest being the most effective. Hidden preferences reveal that Cymodocea and Posidonia favor the low, limited-range chlorophyll-α levels (<0.5 mg/m3), Halophila tolerates higher salinities (>39), while Ruppia prefers euryhaline conditions (37.5–39).}
}
@article{CAMERON2022101658,
title = {Estimating boreal forest ground cover vegetation composition from nadir photographs using deep convolutional neural networks},
journal = {Ecological Informatics},
volume = {69},
pages = {101658},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101658},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122001078},
author = {Hilary A. Cameron and Pranoy Panda and Martin Barczyk and Jennifer L. Beverly},
abstract = {Ground cover and surface vegetation information are key inputs to wildfire propagation models and are important indicators of ecosystem health. Often these variables are approximated using visual estimation by trained professionals but the results are prone to bias and error. This study analyzed the viability of using nadir or downward photos from smartphones (iPhone 7) to provide quantitative ground cover and biomass loading estimates. Good correlations were found between field measured values and pixel counts from manually segmented photos delineating a pre-defined set of 10 discrete cover types. Although promising, segmenting photos manually was labor intensive and therefore costly. We explored the viability of using a trained deep convolutional neural network (DCNN) to perform image segmentation automatically. The DCNN was able to segment nadir images with 95% accuracy when compared with manually delineated photos. To validate the flexibility and robustness of the automated image segmentation algorithm, we applied it to an independent dataset of nadir photographs captured at a different study site with similar surface vegetation characteristics to the training site with promising results.}
}
@article{HARTMANN2022101782,
title = {A text and image analysis workflow using citizen science data to extract relevant social media records: Combining red kite observations from Flickr, eBird and iNaturalist},
journal = {Ecological Informatics},
volume = {71},
pages = {101782},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101782},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002321},
author = {Maximilian C. Hartmann and Moritz Schott and Alishiba Dsouza and Yannick Metz and Michele Volpi and Ross S. Purves},
keywords = {User-generated content, Volunteered geographic information, Data integration, Image content analysis, Convolutional neural networks},
abstract = {There is an urgent need to develop new methods to monitor the state of the environment. One potential approach is to use new data sources, such as User-Generated Content, to augment existing approaches. However, to date, studies typically focus on a single date source and modality. We take a new approach, using citizen science records recording sightings of red kites (Milvus milvus) to train and validate a Convolutional Neural Network (CNN) capable of identifying images containing red kites. This CNN is integrated in a sequential workflow which also uses an off-the-shelf bird classifier and text metadata to retrieve observations of red kites in the Chilterns, England. Our workflow reduces an initial set of more than 600,000 images to just 3065 candidate images. Manual inspection of these images shows that our approach has a precision of 0.658. A workflow using only text identifies 14% less images than that including image content analysis, and by combining image and text classifiers we achieve almost perfect precision of 0.992. Images retrieved from social media records complement those recorded by citizen scientists spatially and temporally, and our workflow is sufficiently generic that it can easily be transferred to other species.}
}
@article{OLIVARESPINTO2024102653,
title = {Using honey bee flight activity data and a deep learning model as a toxicovigilance tool},
journal = {Ecological Informatics},
volume = {81},
pages = {102653},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102653},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400195X},
author = {Ulises Olivares-Pinto and Cédric Alaux and Yves {Le Conte} and Didier Crauser and Alberto Prado},
keywords = {Honeybee flight activity, Toxicovigilance, Neurotoxic pesticides, Recurrent neural network},
abstract = {Automatic monitoring devices placed at the entrances of honey bee hives have facilitated the detection of various sublethal effects related to pesticide exposure, such as homing failure and reduced flight activity. These devices have further demonstrated that different neurotoxic pesticide molecules produce similar sublethal impacts on flight activity. The detection of these effects was conducted a posteriori, following the recording of flight activity data. This study introduces a method using an artificial intelligence model, specifically a recurrent neural network, to detect the sublethal effects of pesticides in real-time based on honey bee flight activity. This model was trained on a flight activity dataset comprising 42,092 flight records from 1107 control and 1689 pesticide-exposed bees. The model was able to classify honey bees as healthy or pesticide-exposed based on the number of flights and minutes spent foraging per day. The model was the least accurate (68.46%) when only five days of records per bee were used for training. However, the highest classification accuracy of 99%, a Cohen Kappa of 0.9766, a precision of 0.99, a recall of 0.99, and an F1-score of 0.99 was achieved with the model trained on 25 days of activity data, signifying near-perfect classification ability. These results underscore the highly predictive performance of AI models for toxicovigilance and highlight the potential of our approach for real-time and cost-effective monitoring of risks due to exposure to neurotoxic pesticide in honey bee populations.}
}
@article{KORSCHENS2024102516,
title = {Determining the community composition of herbaceous species from images using convolutional neural networks},
journal = {Ecological Informatics},
volume = {80},
pages = {102516},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102516},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400058X},
author = {Matthias Körschens and Solveig Franziska Bucher and Paul Bodesheim and Josephine Ulrich and Joachim Denzler and Christine Römermann},
keywords = {Plant biodiversity, Plant cover, Deep learning, Convolutional neural networks, Semantic segmentation, Artificial intelligence},
abstract = {Global change has a detrimental impact on the environment and changes biodiversity patterns, which can be observed, among others, via analyzing changes in the composition of plant communities. Typically, vegetation relevées are done manually, which is time-consuming, laborious, and subjective. Applying an automatic system for such an analysis that can also identify co-occurring species would be beneficial as it is fast, effortless to use, and consistent. Here, we introduce such a system based on Convolutional Neural Networks for automatically predicting the species-wise plant cover. The system is trained on freely available image data of herbaceous plant species from web sources and plant cover estimates done by experts. With a novel extension of our original approach, the system can even be applied directly to vegetation images without requiring such cover estimates. Our extended approach, not utilizing dedicated training data, performs similarly to humans concerning the relative species abundances in the vegetation relevées. When trained on dedicated training annotations, it reflects the original estimates more closely than (independent) human experts, who manually analyzed the same sites. Our method is, with little adaptation, usable in novel domains and could be used to analyze plant community dynamics and responses of different plant species to environmental changes.}
}
@article{RICCARDI2024102590,
title = {First woody cover vegetation map of Kruger National Park in 1939–1944: Evidence from historical black and white aerial photography},
journal = {Ecological Informatics},
volume = {81},
pages = {102590},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102590},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001328},
author = {Tullia Riccardi and Benjamin J. Wigley and Linda Kleyn and Corli Coetsee and Sandra MacFadyen and Fabio Attorre and Luca Malatesta},
keywords = {Savanna ecosystem, Vegetation mapping, Object-based image analysis (OBIA), Google earth engine (GEE), Random forest, Historical baseline},
abstract = {Long-term spatial studies are crucial for understanding how the Earth's surface has changed. Before satellite imagery, landscapes were monitored using black and white (B&W) aerial photographs. However, surveys were infrequent and image analysis was a manual process that was both time-consuming and costly. In this study, we created a composite of high spatial resolution (0.5–0.75 m) B&W aerial images from 1939–1944, covering about 91% of Kruger National Park (KNP)’s nearly 2 million ha. We used this to produce the first historical woody cover (tall trees and shrubs) map of KNP, which until now was only partially understood through fragmented descriptions in period literature and small-area case studies. We established a supervised learning workflow using Google Earth Engine (GEE) which included performing an Object-based Image Analysis (OBIA) with a Random Forest classifier. This approach, enhanced by integrating texture, shape, neighboring features, and spectral variables into the training/validation dataset, enabled the identification of woody vegetation from B&W landscape objects. To enhance accuracy, we guided our sampling method using vegetation types with comparable woody cover and species composition. Initially, we tested our method on a smaller set of images (25 km2), and after confirming its effectiveness, we then expanded the approach to cover all available historical aerial imagery. Our results show that in 1939–1944, 26% of KNP was covered in woody vegetation (overall accuracy of 89%, producer's accuracy (non-woody = 88%, woody = 90%), and user's accuracy (non-woody = 90%, woody = 87%)). The importance of geological substrate in driving vegetation pattern is reflected in a higher woody cover percentage on granite (28%) than on basalt (21%) soils, with the lowest woody cover on northern basalts (11%) and the highest on north-central granites (32%). This study highlights the potential of GEE and OBIA for analyzing large-area, high spatial resolution B&W aerial photographs in a systematic and efficient manner and the importance of creating large-scale historical land cover baselines to support environmental planning and landscape management.}
}
@article{ALI2024102618,
title = {An ensemble of deep learning architectures for accurate plant disease classification},
journal = {Ecological Informatics},
volume = {81},
pages = {102618},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102618},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001602},
author = {Ali Hussein Ali and Ayman Youssef and Mahmoud Abdelal and Muhammad Adil Raja},
keywords = {Plant leaf disease, Agriculture, Ensemble deep learning models},
abstract = {A substantial fraction of agricultural produce loss can be attributed to plant diseases. Agricultural yield loss can have far-reaching consequences for a country's economy and contribute to global food insecurity. Early detection of plant diseases can be instrumental in maintaining global health and welfare. A pathologist's visual evaluation is typically used to make an early diagnosis of plant diseases. This technique involves experts or farmers examining plants with the naked eye and classifying the disease depending on their previous experience. This conventional approach includes drawbacks like low accuracy and the need for human expertise. This motivates researchers to investigate automated systems for the early diagnosis of plant diseases. To achieve this goal an ensemble of different deep learning architectures (DenseNet201, efficientNetB0, inceptionresnetV2, efficientNetB3) is introduced to increase the classification accuracy of plant leaf diseases. In this work, a novel image-processing technique is proposed to increase the efficiency of deep-learning models. Also, a data balancing technique is used to solve the problem of the imbalanced dataset. Five different deep-learning models are trained and tested using the largest plant disease dataset; PlantVillage. Ten different ensembles (chosen randomly) of the deep learning models are tested and compared to find the ensemble with the highest accuracy. The proposed ensemble model was able to achieve 99.89% accuracy on the New PlantVillage dataset. PlantVillage is a challenging dataset with 38 classes. Achieving high accuracies on such a dataset proves the ability of the system to generalize on unseen data or real-world scenarios. A comparison with the state-of-the-art is made with other available models from the literature. A section about this is added to show the superior performance of the proposed ensemble model in terms of accuracy and F1-score.}
}
@article{KUMAR2024102699,
title = {Improving learning-based birdsong classification by utilizing combined audio augmentation strategies},
journal = {Ecological Informatics},
volume = {82},
pages = {102699},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102699},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002413},
author = {Arunodhayan Sampath Kumar and Tobias Schlosser and Stefan Kahl and Danny Kowerko},
keywords = {Audio classification, Augmentation strategies, Birdsong soundscapes, Computer vision and pattern recognition, Convolutional neural networks, Vision transformers},
abstract = {In ecology, changes in environmental conditions are often closely linked to shifts in species diversity. This relationship can be investigated by analyzing avian vocalizations, which are robust indicators of trends in biodiversity. Within this contribution, we explored various data augmentation techniques and deep learning strategies for the classification of birdsong within natural soundscapes. For this purpose, we employed three fundamental deep neural network architectures, such as vision transformers, to classify 397 different bird species. To improve both the accuracy and generalizability of our models, we incorporated up to 19 well-established data augmentation techniques commonly used in audio classification. This included an iterative selection process where only augmentations that enhanced classification performance were selected. The primary augmentation technique involved the integration of various noise samples and non-bird audio elements, which significantly improved model performance as assessed on the BirdCLEF 2021 data set. Individual augmentations achieved F1-scores from 48.0 % (vertical flip) to 72.6 % (primary background noise soundscapes). Through the strategic combination of key techniques – namely simulated pink noise, interspecies sound mixing, and loudness normalization – we achieved a top F1-score of 73.7%. Depending on the selected classification model, this corresponds to an improvement by 4.81 % to 10.5 %. Improvements and deteriorations of all applied augmentation techniques appeared to be robust across our three evaluated models. Therefore, our approach highlights the potential of sophisticated audio augmentations in refining the accuracy and robustness of birdsong classification models.}
}
@article{CARDOSO2024102602,
title = {Can citizen science and social media images support the detection of new invasion sites? A deep learning test case with Cortaderia selloana},
journal = {Ecological Informatics},
volume = {81},
pages = {102602},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102602},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001444},
author = {Ana Sofia Cardoso and Eva Malta-Pinto and Siham Tabik and Tom August and Helen E. Roy and Ricardo Correia and Joana R. Vicente and Ana Sofia Vaz},
keywords = {Artificial intelligence, Convolutional neural networks, Computer vision, Pampas grass},
abstract = {Deep learning has advanced the content analysis of digital data, unlocking opportunities for detecting, mapping, and monitoring invasive species. Here, we tested the ability of open source classification and object detection models (i.e., convolutional neural networks: CNNs) to identify and map the invasive plant Cortaderia selloana (pampas grass) in mainland Portugal. CNNs were trained over citizen science images and then applied to social media content (from Flickr, Twitter, Instagram, and Facebook), allowing to classify or detect the species in over 77% of situations. Images where the species was identified were mapped, using their georeferenced coordinates and time stamp, showing previously unreported occurrences of C. selloana, and a tendency for the species expansion from 2019 to 2021. Our study shows great potential from deep learning, citizen science and social media data for the detection, mapping, and monitoring of invasive plants, and, by extension, for supporting follow-up management options.}
}
@article{GUILBAULT2023102155,
title = {A practical approach to making use of uncertain species presence-only data in ecology: Reclassification, regularization methods and observer bias},
journal = {Ecological Informatics},
volume = {77},
pages = {102155},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102155},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300184X},
author = {Emy Guilbault and Ian Renner and Eric J. Beh and Michael Mahony},
keywords = {Poisson point pattern, Misidentification, Mixture modelling, Machine learning, Observer bias, Lasso penalty},
abstract = {Various statistical models and software platforms aim to produce species distribution models to better predict where species occur as a function of the environment. However, there are many practical challenges that arise with observations coming from opportunistic surveys. Such data may be of low quality with respect to accuracy and may also exhibit sampling bias. Here, we explore three main challenges. First, species identification can be misleading with the changes in taxonomy where the identification of species has changed for some genus, rendering older records confounded with respect to species identity. Second, the observers' sampled pattern may not reflect the true species distribution as some observers may favor some areas where the species is found. Furthermore, ecological knowledge of environmental drivers of a species distribution may be limited, which presents challenges in selecting appropriate covariates to include in species distribution models. In this paper, we extend two algorithms we recently developed which make use of misidentified observations in order to predict species distributions using spatial point processes. In particular, these algorithms incorporate sampling bias correction and address potential overfitting of the model via lasso-type penalties. We compare the performance of these algorithms to models which do not make use of the confounded species data, and explore the effects of the lasso penalty and bias correction on model performance. We apply the best performing methods to a real dataset of eastern Australian frogs for which taxonomy recently changed. Including confounded observations in the models is particularly relevant for informing management decisions regarding endangered species and species in remote areas.}
}
@article{ZHANG2024102517,
title = {Automatic bioacoustics noise reduction method based on a deep feature loss network},
journal = {Ecological Informatics},
volume = {80},
pages = {102517},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102517},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000591},
author = {Chengyun Zhang and Kaiying He and Xinghui Gao and Yingying Guo},
keywords = {Bioacoustics, Noise reduction, Deep learning, Deep feature loss network},
abstract = {Acoustic sensors that collect acoustic data over extended periods and broad ranges are widely used in bioacoustics monitoring. However, in open environments, acoustic data collected using acoustic sensors can be subject to interference from various real-world noises, thereby influencing the subsequent analysis and processing of bioacoustic data. Existing bioacoustic noise reduction methods are limited in their application because of their low efficiency, unsuitability for non-stationary noise, generally unimproved signal-to-noise ratio (SNR) efficacy, and considerable amounts of residual noise. These limitations hinder the effective processing of recorded signals for which extraneous noise overlaps with bird vocalizations. In this study, we propose a bioacoustic noise reduction method based on a deep feature loss network for bird sounds. The method has a rapid denoising speed and can more effectively remove background noise from field recording signals without distorting the bird acoustic spectrum. The denoising effects of the proposed method were compared with those of a speech enhancement generative adversarial network, web real-time communications denoising, and other noise reduction methods. The denoising ability of these methods for different noises was evaluated using spectrograms and objective evaluation measures such as the SNR and perceptual evaluation of speech quality (PESQ). The experimental results revealed that our proposed noise reduction method can obtain higher SNRs and PESQ scores than other noise reduction methods, with the SNR increasing by up to 35.83 dB following denoising.}
}
@article{BJERGE2023102278,
title = {Hierarchical classification of insects with multitask learning and anomaly detection},
journal = {Ecological Informatics},
volume = {77},
pages = {102278},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102278},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003072},
author = {Kim Bjerge and Quentin Geissmann and Jamie Alison and Hjalte M.R. Mann and Toke T. Høye and Mads Dyrmann and Henrik Karstoft},
keywords = {nomaly detection, Computer vision, Deep learning, Hierarchical classification, Insects, Taxonomy},
abstract = {Cameras and computer vision are revolutionising the study of insects, creating new research opportunities within agriculture, epidemiology, evolution, ecology and monitoring of biodiversity. However, the diversity of insects and close resemblances of many species are a major challenge for image-based species-level classification. Here, we present an algorithm to hierarchically classify insects from images, leveraging a simple taxonomy to (1) classify specimens across multiple taxonomic ranks simultaneously, and (2) identify the lowest rank at which a reliable classification can be reached. Specifically, we propose multitask learning, a loss function incorporating class dependency at each taxonomic rank, and anomaly detection based on outlier analysis to quantify the uncertainty. First, we compile a dataset of 41,731 images of insects, combining images from time-lapse monitoring of floral scenes with images from the Global Biodiversity Information Facility (GBIF). Second, we adapt state-of-the-art convolutional neural networks, ResNet and EfficientNet, for the hierarchical classification of insects belonging to three orders, five families and nine species. Third, we assess model generalization for 11 species unseen by the trained models. Here, anomaly detection is used to predict the higher rank of the species which were not present in the training set. We found that incorporating a simple taxonomy into our model increased the accuracy at higher taxonomic ranks. As expected, our algorithm correctly classified new insect species at higher taxonomic ranks, while classification was uncertain at lower taxonomic ranks. Anomaly detection can effectively flag novel taxa that are visually distinct from species in the training data. However, five novel taxa were consistently mistaken for visually similar species in the training data. Above all, we have demonstrated a practical approach to hierarchical classification based on species taxonomy and uncertainty during automated in situ monitoring of live insects. Our method is simple and versatile, forming a valuable step towards high-level classification of species not found in training data.}
}
@article{ZOU2024102562,
title = {Bacterial community characterization by deep learning aided image analysis in soil chips},
journal = {Ecological Informatics},
volume = {81},
pages = {102562},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102562},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001043},
author = {Hanbang Zou and Alexandros Sopasakis and François Maillard and Erik Karlsson and Julia Duljas and Simon Silwer and Pelle Ohlsson and Edith C. Hammer},
keywords = {Soil bacterial cell counting, Segmentation, Microfluidics, Microbial image recognition, Morphological biodiversity, Bacterial traits},
abstract = {Soil microbes play an important role in governing global processes such as carbon cycling, but it is challenging to study them embedded in their natural environment and at the single cell level due to the opaque nature of the soil. Nonetheless, progress has been achieved in recent years towards visualizing microbial activities and organo-mineral interaction at the pore scale, especially thanks to the development of microfluidic ‘soil chips’ creating transparent soil model habitats. Image-based analyses come with new challenges as manual counting of bacteria in thousands of digital images taken from the soil chips is excessively time-consuming, while simple thresholding cannot be applied due to the background of soil minerals and debris. Here, we adopt the well-developed deep learning algorithm Mask-RCNN to quantitatively analyze the bacterial communities in soil samples from different locations in the world. This work demonstrates analysis of bacterial abundance from three contrasting locations (Greenland, Sweden and Kenya) using deep learning in microfluidic soil chips in order to characterize population and community dynamics. We additionally quantified cell- and colony morphology including cell size, shape and the cell aggregation level via calculation of the distance to the nearest neighbor. This approach allows for the first time an automated visual investigation of soil bacterial communities, and a crude biodiversity measure based on phenotypic cell morphology, which could become a valuable complement to molecular studies.}
}
@article{GULZAR2024102586,
title = {Environmental and anthropogenic drivers of invasive plant diversity and distribution in the Himalaya},
journal = {Ecological Informatics},
volume = {81},
pages = {102586},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102586},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001286},
author = {Ruquia Gulzar and Rameez Ahmad and Tabasum Hassan and Irfan Rashid and Anzar Ahmad Khuroo},
keywords = {Global change, Biological invasions, Invasive alien species, Drivers, Himalaya},
abstract = {Invasive alien species (IAS) are currently considered as one of the major causes of global environmental change. To manage the IAS, it is crucial to identify the different environmental and associated anthropogenic drivers that contribute to invasion of alien species in non-native regions. Although multiple drivers of invasion have been identified at a global scale, the relative roles of these are known to vary considerably at regional scales. Here, we investigate the role of key environmental and anthropogenic drivers in determining the diversity and distribution of selected invasive alien plant species in Kashmir Himalaya. We generated an extensive distribution dataset of these species through field sampling across the region and supplemented it with novel herbarium records. We also extracted data on the relevant environmental (climatic, soil and topographic) and anthropogenic drivers for the study region. The random forest model was employed to quantify the relative contribution of these drivers to determine the two common diversity metrics (species richness and abundance) of selected invasive alien plants. We found that soil water content followed by distance to city, the maximum air temperature, soil pH, soil temperature and human population density exerted the greatest influence on species richness of the invasive plants. Species abundance was significantly influenced by the maximum air temperature followed by soil temperature, distance to city, slope, soil pH and human population density. Overall, our findings help in disentangling the individual and interactive roles of multiple drivers of plant invasions, with wide-ranging implications for management in this Himalayan region and similar landscapes elsewhere.}
}
@article{HU2024102695,
title = {A long-term multivariate time series prediction model for dissolved oxygen},
journal = {Ecological Informatics},
volume = {82},
pages = {102695},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102695},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002371},
author = {Jingzhe Hu and Peixuan Wang and Dashe Li and Shue Liu},
keywords = {Dissolved oxygen, Multivariate, Time series, Long-term prediction, Transformer},
abstract = {Accurate and efficient long-term prediction of marine dissolved oxygen (DO) is crucial for the sustainable development of aquaculture. However, the multidimensional time dependency and lag effects of marine chemical variables present significant challenges when handling multiple inputs in univariate long-term prediction tasks. To address these issues, we designed a multivariate time-series long-term prediction model (LMFormer) based on the Transformer architecture. The proposed multivariate time-series decomposition strategy effectively leverages the feature information of prediction variables at different scales, thereby reducing the loss of critical information. Additionally, a dynamic variable selection strategy based on a gating mechanism was designed to optimize the collinearity problem in the multivariate data feature extraction process. Finally, an efficient two-stage attention architecture is proposed to effectively capture the long-range dependencies between dynamic features. This study conducted high-precision 7-day advance DO long-term predictions in two case studies, the environmentally stable Shandong Peninsula in China and the San Juan Islands in the United States, which are affected by extreme conditions such as ocean currents. The experimental results demonstrate the superior prediction performance and generalizability of the designed model. In the Shandong Peninsula case, the mean absolute error (MAE), root mean square error (RMSE), coefficient of determination (R2), and Kling–Gupta efficiency (KGE) reached 0.0159, 0.126, 0.9743, and 0.9625, respectively. In the San Juan Islands case, the MAE was reduced by an average of 42.34% compared to that of the baseline model, the RMSE was reduced by an average of 24.57%, the R2 increased by 22.54%, and the KGE improved by an average of 12.04%. Overall, the proposed prediction model effectively achieves long-term prediction of multivariate marine chemical data, providing valuable references for sustainable management and decision-making in aquaculture.}
}
@article{GONG2023102334,
title = {Research on facial recognition of sika deer based on vision transformer},
journal = {Ecological Informatics},
volume = {78},
pages = {102334},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102334},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003631},
author = {He Gong and Tianye Luo and Lingyun Ni and Ji Li and Jie Guo and Tonghe Liu and Ruilong Feng and Ye Mu and Tianli Hu and Yu Sun and Ying Guo and Shijun Li},
keywords = {Sika deer, Vision transformer, DenseNet, Face recognition, Patch flattening},
abstract = {In the face of global concerns about endangered ecosystems, it is vital to identify individual animals. Along these lines, in this work, a Vision Transformer (ViT) based model for sika deer individual recognition using facial data was designed. To get the satisfactory results, both low-level aspects like texture and color must also be considered, in addition to the high-level semantic information. Consequently, it was difficult to get good results by only applying advanced retrieval features. The standard ViT or ViT with ResNet (Residual neural network) as the backbone network may not be the best solution, as the direct patch flattening method of feature embedded in the conventional ViT is not applicable for performing deer face recognition. Therefore, DenseNet (Densely connected convolutional networks) block as Module 1 was used for extracting low-level features. DenseNet layers enable feature reuse through dense connections, and any layer can communicate directly. Thus maximum exchange of information flow between layers in the network is enabled. In Module 2, the mask approach was also used to eliminate extraneous information from the images and reduce interference from complicated backgrounds on the identification accuracy. In addition, the pixel multiplication of the feature map output from the two modules enabled the fusion of the local features with global features, enriching hence the expressiveness of the feature map. Finally, the ViT structure was run through pre-trained. The experimental results showed that the proposed model can reach an accuracy of 97.68% for identifying sika deer individuals and exhibited excellent generalization capabilities. A valid database for the individual identification of sika deer is provided by our work, significantly contributing to the conservation and promotion of the ecosystem.}
}
@article{KATH2024102710,
title = {Leveraging transfer learning and active learning for data annotation in passive acoustic monitoring of wildlife},
journal = {Ecological Informatics},
volume = {82},
pages = {102710},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102710},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002528},
author = {Hannes Kath and Patricia P. Serafini and Ivan B. Campos and Thiago S. Gouvêa and Daniel Sonntag},
keywords = {Active learning, Transfer learning, Passive acoustic monitoring, BirdNet},
abstract = {Passive Acoustic Monitoring (PAM) has emerged as a pivotal technology for wildlife monitoring, generating vast amounts of acoustic data. However, the successful application of machine learning methods for sound event detection in PAM datasets heavily relies on the availability of annotated data, which can be laborious to acquire. In this study, we investigate the effectiveness of transfer learning and active learning techniques to address the data annotation challenge in PAM. Transfer learning allows us to use pre-trained models from related tasks or datasets to bootstrap the learning process for sound event detection. Furthermore, active learning promises strategic selection of the most informative samples for annotation, effectively reducing the annotation cost and improving model performance. We evaluate an approach that combines transfer learning and active learning to efficiently exploit existing annotated data and optimize the annotation process for PAM datasets. Our transfer learning observations show that embeddings produced by BirdNet, a model trained on high signal-to-noise recordings of bird vocalisations, can be effectively used for predicting anurans in PAM data: a linear classifier constructed using these embeddings outperforms the benchmark by 21.7%. Our results indicate that active learning is superior to random sampling, although no clear winner emerges among the strategies employed. The proposed method holds promise for facilitating broader adoption of machine learning techniques in PAM and advancing our understanding of biodiversity dynamics through acoustic data analysis.}
}
@article{GOMEZVARGAS2023102036,
title = {Re-identification of fish individuals of undulate skate via deep learning within a few-shot context},
journal = {Ecological Informatics},
volume = {75},
pages = {102036},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102036},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000651},
author = {Nuria Gómez-Vargas and Alexandre Alonso-Fernández and Rafael Blanquero and Luis T. Antelo},
keywords = {Deep learning, Few-shot learning, Photo-identification, Siamese networks},
abstract = {Individual re-identification is critical to track population changes in order to assess status, being particularly relevant in species with conservation concerns and difficult access like marine organisms. For this, we propose photo-identification via deep learning as a non-invasive technique to discriminate between individuals of the undulate skate (Raja undulata). Nevertheless, accruing enough training samples might be difficult to achieve in the case of underwater fish images. We develop a novel methodology based on a siamese neural network that incorporates statistical fundamentals as motivation to overcome the few-shot context. Our work provides a hands-on experience and highlights on pitfalls when trying to apply photo-identification in a limited scenario, concerning both data quantity and quality, yet providing remarkable results over the test set including recaptures, where the model is capable of correctly identifying the 70% of the individuals. The findings of this study can be of strong impact for the research teams becoming familiar with deep learning approaches, as it can be easily extended to re-identify individuals of other marine species of interest from a conservation or exploitation point of view.}
}
@article{ZHANG2024102556,
title = {A reliable unmanned aerial vehicle multi-target tracking system with global motion compensation for monitoring Procapra przewalskii},
journal = {Ecological Informatics},
volume = {81},
pages = {102556},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102556},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000980},
author = {Guoqing Zhang and Yongxiang Zhao and Ping Fu and Wei Luo and Quanqin Shao and Tongzuo Zhang and Zhongde Yu},
keywords = { monitoring, UAV MOT, GMC, Deep SORT},
abstract = {Procapra przewalskii, which inhabits plateau areas, faces the constant threat of poaching and unpredictable risks that impede its survival. The implementation of a comprehensive, real-time monitoring and tracking system for Procapra przewalskii using artificial intelligence and unmanned aerial vehicle (UAV) technology is crucial to safeguard its existence. Therefore, a UAV multi-object-tracking (MOT) system with global motion compensation (GMC) was proposed in this study. YOLOv7 and Deep SORT were employed for object detection and tracking, respectively. Furthermore, the Kalman filter (KF) in Deep SORT is optimized to enhance the accuracy of object-tracking. Moreover, a novel appearance feature-extraction network (FEN) is introduced to enable more effective multi-scale feature (MSF) extraction. In addition, a GMC module was proposed to align neighboring frames through feature matching. This facilitates the correction of the position of the target in the subsequent frame, mitigating the impact of UAV camera motion on tracking. The results demonstrated the remarkable tracking accuracy of the system. Compared with the Deep SORT model, the proposed system exhibited an increase of 6.4% in MOTA, 2.7% in MOTP, and 7.9% in IDF1. Through a comprehensive evaluation and analysis of real-world tracking scenarios, the system proposed in this study exhibits reliability in complex scenes and holds the potential to significantly enhance the protection of Procapra przewalskii from threats.}
}
@article{WHITE2023102363,
title = {One size fits all? Adaptation of trained CNNs to new marine acoustic environments},
journal = {Ecological Informatics},
volume = {78},
pages = {102363},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102363},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003928},
author = {Ellen L. White and Holger Klinck and Jonathan M. Bull and Paul R. White and Denise Risch},
keywords = {Bioacoustics, Deep learning, Domain adaptation, Marine acoustics, Marine mammal detection, Soundscapes},
abstract = {Convolutional neural networks (CNNs) have the potential to enable a revolution in bioacoustics, allowing robust detection and classification of marine sound sources. As global Passive Acoustic Monitoring (PAM) datasets continue to expand it is critical we improve our confidence in the performance of models across different marine environments, if we are to exploit the full ecological value of information within the data. This work demonstrates the transferability of developed CNN models to new acoustic environments by using a pre-trained model developed for one location (West of Scotland, UK) and deploying it in a distinctly different soundscape (Gulf of Mexico, USA). In this work transfer learning is used to fine-tune an existing open-source ‘small-scale’ CNN, which detects odontocete tonal and broadband call types and vessel noise (operating between 0 and 48 kHz). The CNN is fine-tuned on training sets of differing sizes, from the unseen site, to understand the adaptability of a network to new marine acoustic environments. Fine-tuning with a small sample of site-specific data significantly improves the performance of the CNN in the new environment, across all classes. We demonstrate an improved performance in area-under-curve (AUC) score of 0.30, across four classes by fine-training with only 50 spectrograms per class, with a 5% improvement in accuracy between 50 frames and 500 frames. This work shows that only a small amount of site-specific data is needed to retrain a CNN, enabling researchers to harness the power of existing pre-trained models for their own datasets. The marine bioacoustic domain will benefit from a larger pool of global data for training large deep learning models, but we illustrate in this work that domain adaptation can be improved with limited site-specific exemplars.}
}
@article{NOLAN2023102330,
title = {Distance sampling and spatial capture-recapture for estimating density of Northern Bobwhite},
journal = {Ecological Informatics},
volume = {78},
pages = {102330},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102330},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300359X},
author = {Victoria Nolan and Nathan Wilhite and Paige E. Howell and Richard B. Chandler and Dallas Ingram and John M. Yeiser and Chris Scott and James A. Martin},
keywords = {Bias, Density, Di-Lane, Distance sampling, Northern bobwhite, Precision, Spatial capture-recapture, Telemetry},
abstract = {Obtaining accurate density estimates is critical for managers making decisions regarding wildlife populations. A variety of methods have been used to estimate population density, with two of the most common being distance sampling (DS) and spatial capture recapture (SCR). We evaluated precision and bias of density estimators through a simulation study of DS using data collected from either human observer point counts or automated recording units (ARUs), and SCR using trapping data or trapping data augmented with telemetry data. We then fit each of the four models to empirical data collected during autumn 2016–2018 for a population of northern bobwhite Colinus virginianus on Di-Lane Wildlife Management Area in Georgia. Density estimates in our simulation study were relatively unbiased using all four methods but were most accurate for the two SCR methods. Empirical density estimates were similar across methods and years, with annual averages of 0.41 birds per ha in 2016, 0.44 birds per ha in 2017 and 0.31 birds per ha in 2018. In general, both SCR methods were also the most precise using the empirical data, although ARU distance sampling did also produce comparable density estimates and precision values. Although SCR methods performed the best overall, cost and increased labor of these monitoring programs should be evaluated in relation to the relative ease of ARU deployment or point count surveys, which also provided adequate, and often similar, density estimates. Integrating these constraints into a structured decision-making framework will aid managers weighing decisions regarding how to monitor, and ultimately make management decisions.}
}
@article{BOHNETT2023102214,
title = {Comparison of two individual identification algorithms for snow leopards (Panthera uncia) after automated detection},
journal = {Ecological Informatics},
volume = {77},
pages = {102214},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102214},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002431},
author = {Eve Bohnett and Jason Holmberg and Sorosh Poya Faryabi and Li An and Bilal Ahmad and Wajid Rashid and Stephane Ostrowski},
keywords = {Background subtraction, Deep learning, Hotspotter, Individual identification, PIE v2, Snow leopards},
abstract = {Photo-identification of individual snow leopards (Panthera uncia) is the primary data source for density estimation via capture-recapture statistical methods. To identify individual snow leopards in camera trap imagery, it is necessary to match individuals from a large number of images from multiple cameras and historical catalogues, which is both time-consuming and costly. The camouflaged snow leopards also make it difficult for machine learning to classify photos, as they blend in so well with the surrounding mountain environment, rendering applicable software solutions unavailable for the species. To potentially make snow leopard individual identification available via an artificial intelligence (AI) software interface, we first trained and evaluated image classification techniques for a convolutional neural network, pose invariant embeddings (PIE) (a triplet loss network), and compared the accuracy of PIE to that of the HotSpotter algorithm (a SIFT-based algorithm). Data were acquired from a curated library of free-ranging snow leopards taken in Afghanistan between 2012 and 2019 and from captive animals in zoos in Finland, Sweden, Germany, and the United States. We discovered several flaws in the initial PIE model, such as a small amount of background matching, that was addressed, albeit likely not fixed, using background subtraction (BGS) and left-right mirroring (LR) techniques which demonstrated reasonable accuracy (Rank 1: 74% Rank-5: 92%) comparable to the Hotspotter results (Rank 1: 74% Rank 2: 84%)The PIE BGS LR model, in conjunction with Hotspotter, yielded the following results: Rank-1: 85%, Rank-5: 95%, Rank-20: 99%. In general, our findings indicate that PIE BGS LR, in conjunction with HotSpotter, can classify snow leopards more accurately than using either algorithm alone.}
}
@article{XIE2022101893,
title = {Multi-view features fusion for birdsong classification},
journal = {Ecological Informatics},
volume = {72},
pages = {101893},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101893},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003430},
author = {Shanshan Xie and Jing Lu and Jiang Liu and Yan Zhang and Danjv Lv and Xu Chen and Youjie Zhao},
keywords = {Birdsong recognition, Deep features, Handcrafted features, mRMR, Feature selection},
abstract = {As important members of the ecosystem, birds are good monitors of the ecological environment. Bird recognition, especially birdsong recognition, has attracted more and more attention in the field of artificial intelligence. At present, traditional machine learning and deep learning are widely used in birdsong recognition. Deep learning can not only classify and recognize the spectrums of birdsong, but also be used as a feature extractor. Machine learning is often used to classify and recognize the extracted birdsong handcrafted feature parameters. As the data samples of the classifier, the feature of birdsong directly determines the performance of the classifier. Multi-view features from different methods of feature extraction can obtain more perfect information of birdsong. Therefore, aiming at enriching the representational capacity of single feature and getting a better way to combine features, this paper proposes a birdsong classification model based multi-view features, which combines the deep features extracted by convolutional neural network (CNN) and handcrafted features. Firstly, four kinds of handcrafted features are extracted. Those are wavelet transform (WT) spectrum, Hilbert-Huang transform (HHT) spectrum, short-time Fourier transform (STFT) spectrum and Mel-frequency cepstral coefficients (MFCC). Then CNN is used to extract the deep features from WT, HHT and STFT spectrum, and the minimal-redundancy-maximal-relevance (mRMR) to select optimal features. Finally, three classification models (random forest, support vector machine and multi-layer perceptron) are built with the deep features and handcrafted features, and the probability of classification results of the two types of features are fused as the new features to recognize birdsong. Taking sixteen species of birds as research objects, the experimental results show that the three classifiers obtain the accuracy of 95.49%, 96.25% and 96.16% respectively for the features of the proposed method, which are better than the seven single features and three fused features involved in the experiment. This proposed method effectively combines the deep features and handcrafted features from the perspectives of signal. The fused features can more comprehensively express the information of the bird audio itself, and have higher classification accuracy and lower dimension, which can effectively improve the performance of bird audio classification.}
}
@article{JAMALI2022101904,
title = {3DUNetGSFormer: A deep learning pipeline for complex wetland mapping using generative adversarial networks and Swin transformer},
journal = {Ecological Informatics},
volume = {72},
pages = {101904},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101904},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003545},
author = {Ali Jamali and Masoud Mahdianpari and Brian Brisco and Dehua Mao and Bahram Salehi and Fariba Mohammadimanesh},
keywords = {Generative adversarial network, Convolutional neural networks, Wetland mapping, Vision transformers, Deep learning, Swin transformer},
abstract = {Many ecosystems, particularly wetlands, are significantly degraded or lost as a result of climate change and anthropogenic activities. Simultaneously, developments in machine learning, particularly deep learning methods, have greatly improved wetland mapping, which is a critical step in ecosystem monitoring. Yet, present deep and very deep models necessitate a greater number of training data, which are costly, logistically challenging, and time-consuming to acquire. Thus, we explore and address the potential and possible limitations caused by the availability of limited ground-truth data for large-scale wetland mapping. To overcome this persistent problem for remote sensing data classification using deep learning models, we propose 3D UNet Generative Adversarial Network Swin Transformer (3DUNetGSFormer) to adaptively synthesize wetland training data based on each class's data availability. Both real and synthesized training data are then imported to a novel deep learning architecture consisting of cutting-edge Convolutional Neural Networks and vision transformers for wetland mapping. Results demonstrated that the developed wetland classifier obtained a high level of kappa coefficient, average accuracy, and overall accuracy of 96.99%, 97.13%, and 97.39%, respectively, for the data in three pilot sites in and around Grand Falls-Windsor, Avalon, and Gros Morne National Park located in Canada. The results show that the proposed methodology opens a new window for future high-quality wetland data generation and classification. The developed codes are available at https://github.com/aj1365/3DUNetGSFormer.}
}
@article{ZANG2022101892,
title = {Ages of giant panda can be accurately predicted using facial images and machine learning},
journal = {Ecological Informatics},
volume = {72},
pages = {101892},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101892},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003429},
author = {Hang-Xing Zang and Han Su and Yu Qi and Lin Feng and Rong Hou and Mengnan He and Peng Liu and Ping Xu and Yanglina Yu and Peng Chen},
keywords = {Biodiversity, Giant panda, Machine learning, Age estimation, Ordinal regression},
abstract = {To forecast giant panda (Ailuropoda melanoleuca) population dynamics in the wild, it is crucial to comprehend their age distribution. Traditional methods for estimating the age of panda are costly, time-consuming, and inaccurate. Additionally, these methods only forecast an age group rather than a real age, and lack a uniform standard. However, advances in deep learning and computer vision have given rise to fresh approaches to this problem. Classification models can be improved by using ordinal regression, which uses ordinal correlations across ages to reduce the non-stationary nature of aging tasks. In this study, we collected 8002 images from 272 pandas in various environments, whose ages ranged from 0 to 38. We applied a five-fold subject-exclusive (SE) protocol to train seven Convolutional Neural Networks (CNN) based on ordinal regression. Experiments were conducted on the Panda Age Dataset (PAD Full) and the Lite Panda Age Dataset (PAD Lite). The results were very encouraging and achieved a Mean Absolute Error (MAE) of 2.51 and 2.41, respectively. Our findings demonstrate that this new tool can noninvasively predict the age of giant pandas in captivity and the wild. Continued development of computer vision technology will drive progress in ecology and conservation.}
}
@article{TORRESANI2023102082,
title = {LiDAR GEDI derived tree canopy height heterogeneity reveals patterns of biodiversity in forest ecosystems},
journal = {Ecological Informatics},
volume = {76},
pages = {102082},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102082},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123001115},
author = {Michele Torresani and Duccio Rocchini and Alessandro Alberti and Vítězslav Moudrý and Michael Heym and Elisa Thouverai and Patrick Kacic and Enrico Tomelleri},
keywords = {GEDI, Height heterogeneity, Remote sensing, Canopy height model, Rao’s Q index, Species diversity},
abstract = {The “Height Variation Hypothesis” is an indirect approach used to estimate forest biodiversity through remote sensing data, stating that greater tree height heterogeneity (HH) measured by CHM LiDAR data indicates higher forest structure complexity and tree species diversity. This approach has traditionally been analyzed using only airborne LiDAR data, which limits its application to the availability of the dedicated flight campaigns. In this study we analyzed the relationship between tree species diversity and HH, calculated with four different heterogeneity indices using two freely available CHMs derived from the new space-borne GEDI LiDAR data. The first, with a spatial resolution of 30 m, was produced through a regression tree machine learning algorithm integrating GEDI LiDAR data and Landsat optical information. The second, with a spatial resolution of 10 m, was created using Sentinel-2 images and a deep learning convolutional neural network. We tested this approach separately in 30 forest plots situated in the northern Italian Alps, in 100 plots in the forested area of Traunstein (Germany) and successively in all the 130 plots through a cross-validation analysis. Forest density information was also included as influencing factor in a multiple regression analysis. Our results show that the GEDI CHMs can be used to assess biodiversity patterns in forest ecosystems through the estimation of the HH that is correlated to the tree species diversity. However, the results also indicate that this method is influenced by different factors including the GEDI CHMs dataset of choice and their related spatial resolution, the heterogeneity indices used to calculate the HH and the forest density. Our finding suggest that GEDI LIDAR data can be a valuable tool in the estimation of forest tree heterogeneity and related tree species diversity in forest ecosystems, which can aid in global biodiversity estimation.}
}
@article{ELRAWY2024102652,
title = {Assessing and segmenting salt-affected soils using in-situ EC measurements, remote sensing, and a modified deep learning MU-NET convolutional neural network},
journal = {Ecological Informatics},
volume = {81},
pages = {102652},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102652},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001948},
author = {Mustafa El-Rawy and Sally Y. Sayed and Mohamed A.E. AbdelRahman and Atef Makhloof and Nassir Al-Arifi and Mahmoud Khaled Abd-Ellah},
keywords = {Soil salinity, Artificial neural networks, Deep learning, Remote sensing, Salinity indices},
abstract = {A study revealed that the Siwa Oasis faces high soil salinity, which negatively impacts agricultural areas and crop productivity, despite its significant economic and agricultural importance. The current research proposes an approach to detect and segment salinity and vegetation areas at Siwa Oasis, Egypt, by combining remote sensing and building a deep learning neural network model-based U-NET algorithm to detect salinity change areas, anticipate further degradation, and predict soil quality indicators. To locate changes among the available images, standard image improvement, classification, and change detection methods have been used. We applied a deep learning modified U-Net (MU-NET) algorithm to segment and produce salinity maps. The MU-NET architecture is a two-level nested U-structure merged with a residual U-block (RUb), which consists of an encoder and a decoder. We applied RUb, which consists of several layers and skip connections. Different combinations of the salinity and vegetation indices were added to the original image to improve segmentation precision. The model was validated and trained using actual data samples collected over a 10-year period from the Landsat 8 satellite, which can monitor and analyse present land cover changes. The dataset consisted of 91 OLI and TIRS spectral images. Each one consists of eleven bands with a spatial resolution of 30 m for bands 1 to 7 and 9. A field survey was used as the main source of data for comparing the proposed model's outputs to assess the error rate. The study region is experiencing an increase in soil salinity in all directions, particularly with regard to the spatial distribution of saline soils, not just the quantitative increase in salt-affected soils. These findings supported the acceleration of soil salinization and vegetation death. The proposed model achieved the highest performance results among the other models and literature and was based on applying method 12 using 13 image layers, with the highest accuracies of 91.27% and 90.83% for salinity and vegetation, respectively.}
}
@article{KALFAS2023102037,
title = {Towards automatic insect monitoring on witloof chicory fields using sticky plate image analysis},
journal = {Ecological Informatics},
volume = {75},
pages = {102037},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102037},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000663},
author = {Ioannis Kalfas and Bart {De Ketelaere} and Klaartje Bunkens and Wouter Saeys},
keywords = {Insect recognition, Convolutional neural networks, Pest management, Automatic monitoring},
abstract = {Context
Sticky trap catches of agricultural pests can be employed for early hotspot detection, identification, and estimation of pest presence in greenhouses or in the field. However, manual procedures to produce and analyze catch results require substantial time and effort. As a result, much research has gone into creating efficient techniques for remotely monitoring possible infestations. A considerable number of these studies use Artificial Intelligence (AI) to analyze the acquired data and focus on performance metrics for various model architectures. Less emphasis, however, was devoted to the testing of the trained models to investigate how well they would perform under practical, in-field conditions.
Objective
In this study, we showcase an automatic and reliable computational method for monitoring insects in witloof chicory fields, while shifting the focus to the challenges of compiling and using a realistic insect image dataset that contains insects with common taxonomy levels.
Methods
To achieve this, we collected, imaged, and annotated 731 sticky plates - containing 74,616 bounding boxes - to train a YOLOv5 object detection model, concentrating on two pest insects (chicory leaf-miners and wooly aphids) and their two predatory counterparts (ichneumon wasps and grass flies). To better understand the object detection model's actual field performance, it was validated in a practical manner by splitting our image data on the sticky plate level.
Results and conclusions
According to experimental findings, the average mAP score for all dataset classes was 0.76. For both pest species and their corresponding predators, high mAP values of 0.73 and 0.86 were obtained. Additionally, the model accurately forecasted the presence of pests when presented with unseen sticky plate images from the test set.
Significance
The findings of this research clarify the feasibility of AI-powered pest monitoring in the field for real-world applications and provide opportunities for implementing pest monitoring in witloof chicory fields with minimal human intervention.}
}
@article{KWON2024102588,
title = {Estimation of aquatic ecosystem health using deep neural network with nonlinear data mapping},
journal = {Ecological Informatics},
volume = {81},
pages = {102588},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102588},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001304},
author = {Yong Sung Kwon and Hyeongsik Kang and JongCheol Pyo},
keywords = {Deep learning, Aquatic ecosystem health index, Autoencoder, Convolutional neural network},
abstract = {Estimation of aquatic ecosystem health indices can assist in reducing the burden of time-consuming, labor-intensive, and cost-effective fieldwork for the sustainable evaluation of freshwater ecosystem status. In this study, we developed a deep neural network to estimate the trophic diatom index (TDI), benthic macroinvertebrate index (BMI), and fish assessment index (FAI) using water quality and hydraulic and hydrological data. A convolutional neural network (CNN) model was built to estimate health indices. In addition, an autoencoder was adopted to produce manifold features that were used as inputs for the CNN model. Conventional machine learning models, including artificial neural networks, support vector machines, random forests, and extreme gradient boosting, have been developed to estimate the TDI, BMI, and FAI. The results showed that the CNN with an autoencoder exhibited the best performance, with validation accuracies of Nash Sutcliffe Efficiency (NSE) and root mean squared error (RMSE) values of 0.592 and 17.249 for TDI, 0.669 and 12.282 for BMI, and 0.638 and 13.897 for FAI, respectively. The autoencoder enhanced the nonlinear feature learning of the time series and static input data, which contributed to improving the CNN feature extraction for accurate estimation of aquatic ecosystem health indices compared to other data-driven approaches. Therefore, deep learning techniques can be used to investigate aquatic ecosystem health by successfully reflecting the quantitative and qualitative features of health indices.}
}
@article{DEMELOLIMA2024102543,
title = {A lightweight and enhanced model for detecting the Neotropical brown stink bug, Euschistus heros (Hemiptera: Pentatomidae) based on YOLOv8 for soybean fields},
journal = {Ecological Informatics},
volume = {80},
pages = {102543},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102543},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000852},
author = {Bruno Pinheiro {de Melo Lima} and Lurdineide {de Araújo Barbosa Borges} and Edson Hirose and Díbio Leandro Borges},
keywords = {Heteroptera, Insect pest detection, Improved YOLO model, Image-based detection and counting, Deep learning, Soybean field images},
abstract = {Insect pest detection and monitoring are vital in an agricultural crop to help prevent losses and be more precise and sustainable regarding the consequent actions to be taken. Deep learning (DL) approaches have attracted attention, showing triumphant performance in many image-based applications. In the adult stage, this research considers detecting a vital insect pest in soybean crops, the Neotropical brown stink bug (Euschistus heros), from field images acquired by drones and cellphones. We develop and test an improved YOLO-model convolutional neural network (CNN) with fewer parameters than other state-of-the-art models and demonstrate its superior generalization and average precision on public image datasets and the new field data provided here. Considering the proposal's precision and time of response, the possibility of deploying this technology for automatic monitoring and pest management in the near future is promising. We provide open code and data for all the experiments performed.}
}
@article{LAKDARI2024102457,
title = {Mel-frequency cepstral coefficients outperform embeddings from pre-trained convolutional neural networks under noisy conditions for discrimination tasks of individual gibbons},
journal = {Ecological Informatics},
volume = {80},
pages = {102457},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102457},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004867},
author = {Mohamed Walid Lakdari and Abdul Hamid Ahmad and Sarab Sethi and Gabriel A. Bohn and Dena J. Clink},
keywords = {Vocal individuality, Sound feature extraction, Mel-frequency cepstral coefficients, Convolutional neural networks, Acoustic indices, },
abstract = {Passive acoustic monitoring – an approach that utilizes autonomous acoustic recording units – allows for non-invasive monitoring of individuals, assuming that it is possible to acoustically distinguish individuals. However, identifying effective analytical approaches for individual identification remains a challenge. Our study investigates how the use of different feature representations impacts our ability to distinguish between individual female Northern grey gibbons (Hylobates funereus). We broadcast pre-recorded calls from twelve gibbon females and re-recorded the calls at varying distances (directly under the tree to ∼400 m away) using autonomous recording units. We evaluated the effectiveness of using different automated feature extraction approaches to classify gibbon calls: Mel-frequency cepstral coefficients (MFCCs), embeddings from three pre-trained neural networks (BirdNET, VGGish, and Wav2Vec2), and four commonly used acoustic indices. We used a supervised classification approach (random forest) to classify calls to the respective female and compared two unsupervised clustering approaches (affinity propagation clustering and hierarchical density-based spatial clustering) to evaluate which features were most effective for distinguishing female calls without using class labels. We used MFCCs as a baseline as previous work has shown they can be used to distinguish high-quality calls of individual gibbon females. Human annotators could only identify calls in spectrograms from recordings <350 m from the playback speaker with signal-to-noise ratio ∼ 0 dB, so our results focus on these recordings. Using supervised classification, our results confirmed the efficiency of MFCCs and the use of embeddings from one neural network (BirdNET) for effective acoustic classification of gibbon individuals at closer recording distances (signal-to-noise ratio > 10 dB), while the remaining features did not perform well. Contrary to our expectations, we found that MFCCs outperformed all other features for the unsupervised clustering tasks at closer distances and none of the features performed well at farther distances. The ability to acoustically discriminate animals under noisy conditions and from low signal-to-noise ratio calls has important implications for monitoring populations of endangered animals, such as gibbons. Focusing only on high signal-to-noise ratio calls for individual discrimination may not be possible for rare sounds, and future work should focus on developing effective approaches of feature extraction that can perform well across noisy, real-world conditions with a limited number of training samples.}
}
@article{YANG2024102527,
title = {A systematic study on transfer learning: Automatically identifying empty camera trap images using deep convolutional neural networks},
journal = {Ecological Informatics},
volume = {80},
pages = {102527},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102527},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000694},
author = {Deng-Qi Yang and De-Yao Meng and Hao-Xuan Li and Meng-Tao Li and Han-Lin Jiang and Kun Tan and Zhi-Pang Huang and Na Li and Rong-Hai Wu and Xiao-Wei Li and Ben-Hui Chen and Mei Zhang and Guo-Peng Ren and Wen Xiao},
keywords = {Transfer learning, Camera trap images, ResNext-101, Updating layer selection, Image recognition},
abstract = {Transfer learning is extensively utilized for automatically recognizing and filtering out empty camera trap images that lack animal presence. Current research that uses transfer learning for identifying empty images typically solely updates the fully connected layer of models, and they usually select a pre-trained source model only based on its relevance to the target task. However, they do not consider the optimization of update layer selection, nor do they investigate the effect of sample size and class number of source domain data set used to construct the source model on the performance of the transfer model. Both of these are issues worth exploring. We answered these two issues using three different datasets and the ResNext-101 model. Our experimental results showed that when using 20,000 training samples to transfer the model from the ImageNet dataset to the Snapshot Serengeti dataset, our proposed optimal update layers improved the accuracy of the transfer model from 92.9% to 95.5% (z = −7.087, p < 0.001, N = 8118) compared to the existing method of updating only the fully connected layer. A similar improvement was observed when transferring the model from ImageNet to the Lasha Mountain dataset. Additionally, our results indicated that when using 20,000 training samples to update the pre-trained model and increasing the sample size of the binary-class training dataset used to build the source model from 100,000 to 1 million, the accuracy of the transfer model improved from 90.4% to 93.5% (z = −3.869, p < 0.001, N = 8948). Similar results were obtained when constructing the source domain dataset using ten classifications. Based on these results, we drew the following conclusions: (1) using our proposed optimal update layers instead of the commonly used method of updating only the fully connected layers can significantly improve the model's performance. (2) The optimal update layers varied when the model transferred from different source domain datasets to the same target dataset. (3) The number of classes in the source domain dataset did not significantly impact the transfer model performance. However, the sample size of the source domain dataset positively correlated with the transfer model performance, and there might be a threshold effect.}
}
@article{CLARK2023102065,
title = {The effect of soundscape composition on bird vocalization classification in a citizen science biodiversity monitoring project},
journal = {Ecological Informatics},
volume = {75},
pages = {102065},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102065},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000948},
author = {Matthew L. Clark and Leonardo Salas and Shrishail Baligar and Colin A. Quinn and Rose L. Snyder and David Leland and Wendy Schackwitz and Scott J. Goetz and Shawn Newsam},
keywords = {Convolutional neural networks, CNN, Ecoacoustics, Avian diversity, Bird species classification, Mixture of experts (MoE), Citizen science, Automated recording units, ARU, Soundscapes to landscapes, BirdNET, Soundscape components},
abstract = {There is a need for monitoring biodiversity at multiple spatial and temporal scales to aid conservation efforts. Autonomous recording units (ARUs) can provide cost-effective, long-term and systematic species monitoring data for sound-producing wildlife, including birds, amphibians, insects and mammals over large areas. Modern deep learning can efficiently automate the detection of species occurrences in these sound data with high accuracy. Further, citizen science can be leveraged to scale up the deployment of ARUs and collect reference vocalizations needed for training and validating deep learning models. In this study we develop a convolutional neural network (CNN) acoustic classification pipeline for detecting 54 bird species in Sonoma County, California USA, with sound and reference vocalization data collected by citizen scientists within the Soundscapes to Landscapes project (www.soundscapes2landscapes.org). We trained three ImageNet-based CNN architectures (MobileNetv2, ResNet50v2, ResNet100v2), which function as a Mixture of Experts (MoE), to evaluate the usefulness of several methods to enhance model accuracy. Specifically, we: 1) quantify accuracy with fully-labeled 1-min soundscapes for an assessment of real-world conditions; 2) assess the effect on precision and recall of additional pre-training with an external sound archive (xeno-canto) prior to fine-tuning with vocalization data from our study domain; and, 3) assess how detections and errors are influenced by the presence of coincident biotic and non-biotic sounds (i.e., soundscape components). In evaluating accuracy with soundscape data (n = 37 species) across CNN probability thresholds and models, we found acoustic pre-training followed by fine-tuning improved average precision by 10.3% relative to no pre-training, although there was a small average 0.8% reduction in recall. In selecting an optimal CNN architecture for each species based on maximum F(β = 0.5), we found our MoE approach had total precision of 84.5% and average species precision of 85.1%. Our data exhibit multiple issues arising from applying citizen science and acoustic monitoring at the county scale, including deployment of ARUs with relatively low fidelity and recordings with background noise and overlapping vocalizations. In particular, human noise was significantly associated with more incorrect species detections (false positives, decreased precision), while physical interference (e.g., recorder hit by a branch) and geophony (e.g., wind) was associated with the classifier missing detections (false negatives, decreased recall). Our process surmounted these obstacles, and our final predictions allowed us to demonstrate how deep learning applied to acoustic data from low-cost ARUs paired with citizen science can provide valuable bird diversity data for monitoring and conservation efforts.}
}
@article{CHEN2024102693,
title = {Weight-based ensemble method for crop pest identification},
journal = {Ecological Informatics},
volume = {82},
pages = {102693},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102693},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002358},
author = {Miao Chen and Jianji Wang and Yanan Chen and Minghui Guo and Nanning Zheng},
keywords = {Crop pest identification, Deep learning, Ensemble method, Convex optimization},
abstract = {Crop pests cause significant losses to agricultural production. Pests can be detected and controlled over time using accurate and effective methods, thereby reducing potential losses. However, there are challenges in realistic agricultural scenarios, such as diverse pest species and complicated environments, which render manual recognition and conventional machine learning methods insufficient. To address this issue, deep learning methods that can automatically extract features have recently been widely used for pest identification. However, accurately recognizing images that resemble complex real-world scenarios remains a challenging task for a single deep learning model. The ensemble method, which combines multiple basic models, provides a solution for improving recognition performance. In this study, we proposed two weight-based ensemble methods, VecEnsemble and MatEnsemble, constructed from vector- and matrix-based weights, respectively. The weights that combine basic models significantly influence the performance of the ensemble methods. Therefore, to effectively combine the basic models, we formulated the weight design problem as a quadratic convex optimization problem whose solution has a closed-form expression and can be computed efficiently. Our method achieved the highest accuracy of 77.39% on the large-scale complex-scene IP102 dataset, which was competitive with those of other state-of-the-art methods. Furthermore, we conducted comprehensive ablation experiments to compare our proposed methods with voting-based approaches and illustrate the scenarios in which they are applicable. These results highlight the practical significance of our method for agricultural production and provide a foundation for further research on crop pest identification. The source code is available at https://github.com/shiguangqianmo/WBEnsemble.}
}
@article{LEORNA2022101876,
title = {Human vs. machine: Detecting wildlife in camera trap images},
journal = {Ecological Informatics},
volume = {72},
pages = {101876},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101876},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003260},
author = {Scott Leorna and Todd Brinkman},
keywords = {Confusion matrix, Deep learning, Image analysis, Software, Trail camera},
abstract = {As the capacity to collect and store large amounts of data expands, identifying and evaluating strategies to efficiently convert raw data into meaningful information is increasingly necessary. Across disciplines, this data processing task has become a significant challenge, delaying progress and actionable insights. In ecology, the growing use of camera traps (i.e., remotely triggered cameras) to collect information on wildlife has led to an enormous volume of raw data (i.e., images) in need of review and annotation. To expedite camera trap image processing, many have turned to the field of artificial intelligence (AI) and use machine learning models to automate tasks such as detecting and classifying wildlife in images. To contribute understanding of the utility of AI tools for processing wildlife camera trap images, we evaluated the performance of a state-of-the-art computer vision model developed by Microsoft AI for Earth named MegaDetector using data from an ongoing camera trap study in Arctic Alaska, USA. Compared to image labels determined by manual human review, we found MegaDetector reliably determined the presence or absence of wildlife in images generated by motion detection camera settings (≥94.6% accuracy), however, performance was substantially poorer for images collected with time-lapse camera settings (≤61.6% accuracy). By examining time-lapse images where MegaDetector failed to detect wildlife, we gained practical insights into animal size and distance detection limits and discuss how those may impact the performance of MegaDetector in other systems. We anticipate our findings will stimulate critical thinking about the tradeoffs of using automated AI tools or manual human review to process camera trap images and help to inform effective implementation of study designs.}
}
@article{CEIAHASSE2023102272,
title = {Forecasting the abundance of disease vectors with deep learning},
journal = {Ecological Informatics},
volume = {78},
pages = {102272},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102272},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003011},
author = {Ana Ceia-Hasse and Carla A. Sousa and Bruna R. Gouveia and César Capinha},
keywords = {Machine learning, Mosquito, Dengue, Forecast, Time series classification},
abstract = {Arboviral diseases such as dengue, Zika, chikungunya or yellow fever are a worldwide concern. The abundance of vector species plays a key role in the emergence of outbreaks of these diseases, so forecasting these numbers is fundamental in preventive risk assessment. Here we describe and demonstrate a novel approach that uses state-of-the-art deep learning algorithms to forecast disease vector abundances. Unlike classical statistical and machine learning methods, deep learning models use time series data directly as predictors and identify the features that are most relevant from a predictive perspective. We demonstrate for the first time the application of this approach to predict short-term temporal trends in the number of Aedes aegypti mosquito eggs across Madeira Island for the period 2013 to 2019. Specifically, we apply the deep learning models to predict whether, in the following week, the number of Ae. aegypti eggs will remain unchanged, or whether it will increase or decrease, considering different percentages of change. We obtained high predictive performance for all years considered (mean AUC = 0.92 ± 0.05 SD). Our approach performed better than classical machine learning methods. We also found that the preceding numbers of eggs is a highly informative predictor of future trends. Linking our approach to disease transmission or importation models will contribute to operational, early warning systems of arboviral disease risk.}
}
@article{PIECHAUD2022101786,
title = {Fast and accurate mapping of fine scale abundance of a VME in the deep sea with computer vision},
journal = {Ecological Informatics},
volume = {71},
pages = {101786},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101786},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002369},
author = {Nils Piechaud and Kerry L. Howell},
keywords = {Benthic ecology, Computer vision, Xenophyophores, Quantitative ecology, Mapping, Automated image analysis, Marine conservation},
abstract = {With growing anthropogenic pressure on deep-sea ecosystems, large quantities of data are needed to understand their ecology, monitor changes over time and inform conservation managers. Current methods of image analysis are too slow to meet these requirements. Recently, computer vision has become more accessible to biologists, and could help address this challenge. In this study we demonstrate a method by which non-specialists can train a YOLOV4 Convolutional Neural Network (CNN) able to count and measure a single class of objects. We apply CV to the extraction of quantitative data on the density and population size structure of the xenophyophore Syringammina fragilissima, from more than 58,000 images taken by an AUV 1200 m deep in the North-East Atlantic. The workflow developed used open-source tools, cloud-base hardware, and only required a level of experience with CV commonly found among ecologists. The CNN performed well, achieving a recall of 0.84 and precision of 0.91. Individual counts per image and size measurements resulting from model predictions were highly correlated (0.96 and 0.92, respectively) with manually collected data. The analysis could be completed in less than 10 days thus bringing novel insights into the population size structure and fine scale distribution of this Vulnerable Marine Ecosystem. It showed S. fragilissima distribution is patchy. The average density is 2.5 ind.m−2 but can vary from up to 45 ind.m−2 only a few tens of meter away from areas where it is almost absent. The average size is 5.5 cm and the largest individuals (>15 cm) tend to be in areas of low density. This study demonstrates how researchers could take advantage of CV to quickly and efficiently generate large quantitative datasets data on benthic ecosystems extent and distribution. This, coupled with the large sampling capacity of AUVs could bypass the bottleneck of image analysis and greatly facilitate future deep-ocean exploration and monitoring. It also illustrates the future potential of these new technologies to meet the goals set by the UN Ocean Decade.}
}
@article{JEANTET2023102256,
title = {Improving deep learning acoustic classifiers with contextual information for wildlife monitoring},
journal = {Ecological Informatics},
volume = {77},
pages = {102256},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102256},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002856},
author = {Lorène Jeantet and Emmanuel Dufourq},
keywords = {Bioacoustics, Deep learning, Convolutional neural networks, Passive acoustic monitoring, Species identification, Birds, Hainan gibbons},
abstract = {Bioacoustics, the exploration of animal vocalizations and natural soundscapes, has emerged as a valuable tool for studying species within their habitats, particularly those that are challenging to observe. This approach has broadened the horizons of biodiversity assessment and ecological research. However, monitoring wildlife with acoustic recorders produces large volumes of data that can be labor-intensive to analyze. Deep learning has recently transformed many computational disciplines by enabling the automated processing of large and complex datasets and has gained attention within the bioacoustics community. Despite the revolutionary impact of deep learning on acoustic detection and classification, attaining both high detection accuracy and low false positive rates in bioacoustics remains a significant challenge. An intriguing yet unexplored avenue for enhancing deep learning in bioacoustics involves the utilization of contextual information, such as time and location, to discern animal vocalizations within acoustic recordings. As a first case study, a multi-branch Convolutional Neural Network (CNN) was developed to classify 22 different bird songs using spectrograms as a first input, and spatial metadata as a secondary input. A comparison was made to a baseline model with only spectrogram input. A geographical prior neural network was trained, separately, to estimate the probability of a species occurring at a given location. The output of this network was combined with the baseline CNN. As a second case study, temporal data and spectrograms were used as input to a multi-branch CNN for the detection of Hainan gibbon (Nomascus hainanus) calls, the world’s rarest primate. Our findings demonstrate that adding metadata to the bird song classifier significantly improves classification performance, with the highest improvement achieved using the geographical prior model (F1-score of 87.78% compared to 61.02% for the baseline model). The multi-branch CNNs also proved efficient (F1-scores of 76.87% and 78.77%) and simpler to use than the geographical prior. In the second case study, our findings revealed a decrease in false positives by 63% (94% of the calls were detected) when the metadata was used by the multi-branch CNN, and an increase of 19% in gibbon detection. This study has uncovered an exciting new avenue for improving classifier performance in bioacoustics. The methodology described in this study can assist ecologists, wildlife management teams, and researchers in reducing the amount of time spent analyzing large acoustic datasets obtained from passive acoustic monitoring studies. Our approach can be adapted and applied to other calling species, and thus tailored to other use cases.}
}
@article{BOHNENSTIEHL2023102268,
title = {Automated cataloging of oyster toadfish (Opsanus tau) boatwhistle calls using template matching and machine learning},
journal = {Ecological Informatics},
volume = {77},
pages = {102268},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102268},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002972},
author = {DelWayne R. Bohnenstiehl},
keywords = {Fish calls, Underwater soundscapes, Call detection, Machine learning},
abstract = {Oyster toadfish (Opsanus tau) represent an ecologically significant species found throughout estuaries along the eastern coast of the United States. While these crevice-dwelling fish can be challenging to observe in their habitats, it is possible to infer their distribution and aspects of their behavior by recording the sounds they produce. The task of cataloging the distinctive advertisement boatwhistle sounds produced by male toadfish to attract females throughout the spring and summer is automated using a multi-step process. Candidate boatwhistles are first identified by template matching using a suite of synthetic spectrogram kernels formed to mimic the two lowest frequency harmonic tones within the boatwhistle. Candidate boatwhistle calls are identified based on the correlation between these kernels and a low-frequency spectrogram of the data. Next, frequency-reassigned spectrogram images of these candidates are formed and input into the pre-trained ResNet-50 convolutional neural network. Finally, the activations from a deep, fully connected layer within this network are extracted and passed to a one-vs-all support-vector-machine classifier, which separates boatwhistles from the larger set of candidate signals. This classifier model was trained and evaluated using a labeled dataset of over 20,000 candidate signals generated over diverse acoustic conditions within Pamlico Sound, North Carolina, USA. The accompanying software provides an effective and efficient tool to monitor boatwhistle calls, which may facilitate a deeper understanding of the spatial distribution, behavioral patterns, and ecological roles played by oyster toadfish.}
}
@article{MA2024102651,
title = {UAV equipped with infrared imaging for Cervidae monitoring: Improving detection accuracy by eliminating background information interference},
journal = {Ecological Informatics},
volume = {81},
pages = {102651},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102651},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001936},
author = {Guangkai Ma and Wenjiao Li and Heng Bao and Nathan James Roberts and Yang Li and Weihua Zhang and Kun Yang and Guangshun Jiang},
keywords = {YOLOv7, ViT, Wild Cervidae monitoring},
abstract = {Wild Cervidae(deer and their relatives) play a crucial role in maintaining ecological balance and are integral components of ecosystems. However, factors such as environmental changes and poaching behaviors have resulted in habitat degradation for Cervidae. The protection of wild Cervidae has become urgent, and Cervidae monitoring is one of the key means to ensure the effectiveness of wild Cervidae protection. Object detection algorithms based on deep learning offer promising potential for automatically detecting and identifying animals. However, when those algorithms are used for inference in unseen background environments, there will be a significant decrease in accuracy, especially in the situation that a certain type of Cervidae images are collected from single scene for algorithm training. In this paper, a two-stage localization and classification pipeline for Cervidae monitoring is proposed. The pipeline effectively reduces background interference in Cervidae monitoring and enhances monitoring accuracy. In the first stage, the YOLOv7 network is designed to automatically locate Cervidae in UAV infrared images, while implementing improved bounding box regression through the α-IoU loss function enables the network to locate Cervidae more accurately. Then, Cevdidae objects are extracted to eliminate the background information. In the second stage, a classification network named CA-Hybrid, based on Convolutional Neural Networks(CNN) and Vision Transformer(ViT), as well as Channel Attention Mechanism(CAM) enhances the expression of key features, is constructed to accurately identify Cervidae categories. Experimental results indicate that this method achieves an Average Precision (AP) of 95.9% for Cervidae location and a top-1 accuracy of 77.73% for Cervidae identification. This research contributes to a more comprehensive and accurate monitoring of wild Cervidae, and provides valuable references for subsequent UAV-based wildlife monitoring.}
}
@article{MATA2024102708,
title = {Drone imagery and deep learning for mapping the density of wild Pacific oysters to manage their expansion into protected areas},
journal = {Ecological Informatics},
volume = {82},
pages = {102708},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102708},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002504},
author = {Aser Mata and David Moffat and Sílvia Almeida and Marko Radeta and William Jay and Nigel Mortimer and Katie Awty-Carroll and Oliver R. Thomas and Vanda Brotas and Steve Groom},
keywords = {Pacific oysters, Invasive species, Convolutional neural networks, Deep learning, Drone, Remote sensing, Ecological management},
abstract = {The recent expansion of wild Pacific oysters already had negative repercussions on sites in Europe and has raised further concerns over their potential harmful impact on the balance of biomes within protected areas. Monitoring their colonisation, especially at early stages, has become an urgent ecological issue. Current efforts to monitor wild Pacific oysters rely on “walk-over” surveys that are highly laborious and often limited to specific areas of easy access. Remotely Piloted Aircraft Systems (RPAS), commonly known as drones, can provide an effective tool for surveying complex terrains and detect Pacific oysters. This study provides a novel workflow for automated detection, counting and mapping of individual Pacific oysters to estimate their density per square meter by using Convolutional Neural Networks (CNNs) applied to drone imagery. Drone photos were collected at low tides and altitudes of approximately 10 m across a variety of cases of rocky shore and mudflats scenarios. Using object detection, we compared how different Convolutional Neural Networks (CNNs) architectures including YOLOv5s, YOLOv5m, TPH-YOLOv5 and FR-CNN performed in the detection of Pacific oysters over the surveyed areas. We report the precision of our model at 88% with a difference in performance of 1% across the two sites. The workflow presented in this work proposes the use of grid maps to visualize the density of Pacific oysters per square meter towards ecological management and the creation of time series to identify trends.}
}
@article{JANSIRANI2024102663,
title = {A novel automated approach for fish biomass estimation in turbid environments through deep learning, object detection, and regression},
journal = {Ecological Informatics},
volume = {81},
pages = {102663},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102663},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400205X},
author = {S.V. {Jansi Rani} and Iacovos Ioannou and R. Swetha and R.M. {Dhivya Lakshmi} and Vasos Vassiliou},
keywords = {Fish biomass, Object detection, Regression, YOLOv8, mYOLOv8},
abstract = {Estimating fish biomass is crucial in the fisheries sector, where traditional methods often harm fish through manual sampling and anesthetics. A non-invasive approach is introduced using underwater films to estimate fish biomass in turbid conditions. This study presents the “Aquatic WeightNet” dataset, targeting the Genetically Improved Formed Tilapia (GIFT) Tilapia species, and addresses the challenge of unclear images with preprocessing techniques like dehazing and Contrast Limited Adaptive Histogram Equalization (CLAHE). YOLOv8, a leading object detection model modified to accommodate the custom Aquatic WeightNet dataset's varied image sizes with five detection heads, P2 to P6, is employed, achieving a recall of 0.997 and a mean Average Precision (mAP) of 0.899 within the 50–95% Intersection over Union (IoU) range. Fish biomass estimation assesses depth, length, and width using regression models for calculation. A three-phase grid search identifies the most effective models, with the Extra Trees Regressor outperforming depth estimation with mean absolute error (MAE) of 0.63 and coefficient of determination (R2) of 0.87 and the Random Forest Regressor for length and width (MAE of 0.01 and R2 of 0.99). For biomass estimation, the Extra Trees Regressor again performs well (MAE of 0.004 and R2 of 0.99), which is critical for determining optimal feed quantities to enhance aquaculture efficiency. This study emphasizes a non-invasive method to estimate fish biomass, optimizing the effectiveness and ecological sustainability of fish farming in murky waters through advanced detection algorithms and robust regression models.}
}
@article{NGUYEN2024102744,
title = {Improving pollen-bearing honey bee detection from videos captured at hive entrance by combining deep learning and handling imbalance techniques},
journal = {Ecological Informatics},
pages = {102744},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102744},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002863},
author = {Dinh-Tu Nguyen and Thi-Nhung Le and Thi-Huong Phung and Duc-Manh Nguyen and Hong-Quan Nguyen and Hong-Thai Pham and Thi-Thu-Hong Phan and  Vu-Hai and Thi-Lan Le},
keywords = {Pollen foraging behavior, Pollen-bearing honey bee detection},
abstract = {The number of pollen-bearing honey bees serves as a vital indicator for assessing colony balance and health. Despite its significance, prevailing detection techniques still rely heavily on manual observation and annotation, leading to time-consuming processes that cannot sustain long-term, continuous monitoring efforts. To facilitate automatic beehive monitoring, this study introduces an efficient method for pollen-bearing bee detection. Initially, we furnish a comprehensive dataset, dubbed VnPollenBee, meticulously annotated for pollen-bearing honey bee detection and classification. The dataset comprises 60,826 annotated boxes that delineate both pollen-bearing and non-pollen-bearing bees in 2051 images captured at the entrances of beehives under various environmental conditions. To the best of our knowledge, this represents the first dedicated dataset for pollen-bearing bee detection. The VnPollenBee dataset is publicly accessible to the research community at https://comvis-hust.github.io/datasets/pollenbee.html. Subsequently, we propose the incorporation of diverse techniques into two baseline models, namely YOLOv5 and Faster RCNN, to effectively address the imbalance that arises during the detection of pollen-bearing bees due to their number being typically much lower than the total number of bees present at hive entrances. The experimental results demonstrate that our proposed method outperforms the baseline models on the VnPollenBee dataset, yielding Precision, Recall, and F1 score of 99%, 93%, and 95%, respectively. Specifically, the improvements obtained are 3% and 2% in Recall and F1 score when using YOLOv5, and 3%, 2%, and 2% in Precision, Recall, and F1 score when using Faster RCNN. These findings confirm the potential of our approach to facilitate bee foraging behavior analysis and automated bee monitoring.}
}
@article{LI2024102704,
title = {Multi-species identification and number counting of fish passing through fishway at hydropower stations with LigTraNet},
journal = {Ecological Informatics},
volume = {82},
pages = {102704},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102704},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002462},
author = {Jianyuan Li and Chunna Liu and Luhai Wang and Yi Liu and Rui Li and Xiaochun Lu and Jia Lu and Jian Shen},
keywords = {LigTraNet, Fishway monitoring, Artificial intelligence, Lightweight model, Computer vision, Revolutionizes},
abstract = {Fishway monitoring can verify the effectiveness of the fishway, optimise the operation mode, and achieve scientific management of fishway operations. Traditional fishway monitoring approaches, hindered by their inefficiency and substantial disruption of fish, are ill-suited for long-term surveillance; thus, employing video monitoring coupled with object detection technology presents an alternative or complementary solution. However, challenges such as the constrained computational capacity of onsite equipment in fishways, complexities involved in model deployment, and sluggish pace of detection are significant hurdles. In this study, by utilising the YOLOv8n model as a benchmark, we engineered a cross-stage partial module with a single convolution (C1) module to replace the existing C2f module with the aim of enhancing performance. We replaced the conventional 2D convolutions in the bottleneck configuration with depthwise separable convolutions and integrated the SimAM module to extract the detailed characteristics of the fish species. By amalgamating LigObNet detection with the DeepSORT algorithm, we established LigTraNet, which is designed to enable precise tracking, identification, and counting of individual fish. The results showed that LigObNet exhibited the lowest complexity and fastest detection speed for underwater fish among similar object recognition and detection models. Compared with the benchmark YOLOv8n model, there were reductions of 8.9% in the network layers, 40.5% in the parameter count, 39.3% in the memory footprint, and 35.8% in the giga floating-point operations and a 38.1% improvement in the inference speed. LigTraNet achieved a total count accuracy rate of 91.8%, demonstrating superior species quantification capabilities over other models with minimal resource usage and rapid inference capabilities, thus offering enhanced practicality for deployment on devices in real-world engineering contexts. This represents a departure from traditional manual monitoring methods for assessing fishway effectiveness, revolutionising aquatic ecological monitoring tools and methodologies and fostering the collaborative advancement of water resource project operations and ecological conservation.}
}
@article{DEMIRANDA2024102430,
title = {Cellphone picture-based, genus-level automated identification of Chagas disease vectors: Effects of picture orientation on the performance of five machine-learning algorithms},
journal = {Ecological Informatics},
volume = {79},
pages = {102430},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102430},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004594},
author = {Vinícius Lima {de Miranda} and Ewerton Pacheco {de Souza} and Deborah Bambil and Ali Khalighifar and A. Townsend Peterson and Francisco Assis {de Oliveira Nascimento} and Rodrigo Gurgel-Gonçalves and Fernando Abad-Franch},
keywords = {Automated identification, Machine learning, Accuracy, Specificity, Low-resolution pictures, Triatominae},
abstract = {Chagas disease (CD) is a public-health concern across Latin America. It is caused by Trypanosoma cruzi, a parasite transmitted by blood-sucking triatomine bugs. Automated identification of triatomine bugs is a potential means to strengthen CD vector surveillance. To be broadly useful, however, automated systems must draw on algorithms capable of correctly identifying bugs from images taken with ordinary cellphone cameras at varying angles or positions. Here, we assess the performance of five machine-learning algorithms at identifying the main CD vector genera (Triatoma, Panstrongylus, and Rhodnius) based on bugs photographed at different angles/positions with a 72-dpi cellphone camera. Each bug (N = 730; 13 species) was photographed at nine angles representing three positions: dorsal-flat, dorsal-oblique, and front/back-oblique. We randomly split the 6570-picture database into training (80%) and testing sets (20%), and then trained and tested a convolutional neural network (AlexNet, AN); three boosting-based classifiers (AdaBoost, AB; Gradient Boosting, GB; and Histogram-based Gradient Boosting, HB); and a linear discriminant model (LD). We assessed identification accuracy and specificity with logit-binomial generalized linear mixed models fit in a Bayesian framework. Differences in performance across algorithms were mainly driven by AN's essentially perfect accuracy and specificity, irrespective of picture angle or bug position. HB predicted accuracies ranged from ∼0.987 (Panstrongylus, dorsal-oblique) to >0.999 (Triatoma, dorsal-flat). AB accuracy was poor for Rhodnius (∼0.224–0.282) and Panstrongylus (∼0.664–0.729), but high for Triatoma (∼0.988–0.991). For Panstrongylus, LD and GB had predicted accuracies in the ∼0.970–0.984 range. AB misclassified ∼57% of Rhodnius and Panstrongylus as Triatoma, whereas specificity ranged from ∼0.92 to ∼1.0 for the remaining algorithm-genus combinations. Dorsal-flat pictures appeared to improve algorithm performance slightly, but angle/position effects were overall weak-to-negligible. We conclude that, when high-performance algorithms such as AN are used, the angles or positions at which bugs are photographed seem unlikely to hinder cellphone picture-based automated identification of CD vectors, at least at the genus level. Future research should focus on combining mixed-quality pictures and state-of-the-art algorithms to (i) identify triatomine adults to the species level and (ii) distinguish triatomine nymphs (i.e., immature stages) from adults and from other insects.}
}
@article{DING2024102664,
title = {Algal blooms forecasting with hybrid deep learning models from satellite data in the Zhoushan fishery},
journal = {Ecological Informatics},
volume = {82},
pages = {102664},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102664},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002061},
author = {Wenxiang Ding and Changlin Li},
keywords = {Hybrid deep learning model, Algal bloom, Chlorophyll, Forecast, Zhoushan fishery},
abstract = {Algal blooms are increasingly frequent in coastal areas, posing a significant threat to coastal ecosystems. The Zhoushan fishery, one of the most affected regions along the Chinese coast, faces severe challenges from algal blooms. In this study, Convolutional Neural Network (CNN), Long Short-term Memory (LSTM) and hybrid CNN-LSTM deep learning models were constructed to forecast chlorophyll (Chl) concentrations and algal blooms from satellite data. The hybrid CNN-LSTM model outperformed the individual models, achieving the highest determination coefficient and the lowest root mean square error for Chl concentration forecasts. It also excelled in predicting algal blooms, with the highest probability of detection and Heidke skill score, effectively capturing the trends in algal bloom development. In areas with high Chl concentration, the Chl parameter significantly influences model forecasts, while meridional wind and current are the main influence factors in the regions with medium and low Chl concentration. The powerful algal bloom forecast provided by the hybrid CNN-LSTM model offers valuable support for the efficient management and sustainable development of the Zhoushan fishery.}
}
@article{STAHL2022101557,
title = {Identifying wetland areas in historical maps using deep convolutional neural networks},
journal = {Ecological Informatics},
volume = {68},
pages = {101557},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101557},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122000061},
author = {Niclas Ståhl and Lisa Weimann},
keywords = {Analysis of historical maps, Convolutional neural networks, Wetland management, Wetland restoration},
abstract = {The local environment and land usages have changed a lot during the past one hundred years. Historical documents and materials are crucial in understanding and following these changes. Historical documents are, therefore, an important piece in the understanding of the impact and consequences of land usage change. This, in turn, is important in the search of restoration projects that can be conducted to turn and reduce harmful and unsustainable effects originating from changes in the land-usage. This work extracts information on the historical location and geographical distribution of wetlands, from hand-drawn maps. This is achieved by using deep learning (DL), and more specifically a convolutional neural network (CNN). The CNN model is trained on a manually pre-labelled dataset on historical wetlands in the area of Jönköping county in Sweden. These are all extracted from the historical map called “Generalstabskartan”. The presented CNN performs well and achieves a F1-score of 0.886 when evaluated using a 10-fold cross validation over the data. The trained models are additionally used to generate a GIS layer of the presumable historical geographical distribution of wetlands for the area that is depicted in the southern collection in Generalstabskartan, which covers the southern half of Sweden. This GIS layer is released as an open resource and can be freely used. To summarise, the presented results show that CNNs can be a useful tool in the extraction and digitalisation of non-textual information in historical documents, such as historical maps. A modern GIS material that can be used to further understand the past land-usage change is produced within this research. Previously, no material of this detail and extent have been available, due to the large effort needed to manually create such. However, with the presented resource better quantifications and estimations of historical wetlands that have been lost can be made.}
}
@article{YOU2023102200,
title = {Segmentation of individual mangrove trees using UAV-based LiDAR data},
journal = {Ecological Informatics},
volume = {77},
pages = {102200},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102200},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002297},
author = {Haotian You and Yao Liu and Peng Lei and Zhigang Qin and Qixu You},
keywords = {LiDAR data, CHM, Segmentation algorithm, Stand density, Spatial resolution},
abstract = {Accurate assessment of structural parameters is essential to effectively monitor the mangrove resources. However, the extraction results of mangrove structural parameters are closely related to the segmentation results of individual trees. Although the results of individual tree segmentation are influenced by many factors, the specific factors affecting the segmentation results of individual mangrove trees, such as data source, image resolution, segmentation algorithm, and stand density, have not yet been elucidated. Therefore, in this study, canopy height models (CHMs) with different spatial resolutions were derived from unmanned aerial vehicle (UAV)-based light detection and ranging (LiDAR) data. Moreover, the watershed algorithm (WA), regional growth (RG), and improved K-nearest neighbour (KNN) and bird's eye view (BEV) faster region-based convolutional neural network (R-CNN) algorithms were used to segment the individual mangrove trees based on CHMs and LiDAR data at three sites with varying stand densities. Finally, different segmentation algorithms, image resolutions, and forest densities were comparatively assessed to determine their influence on the segmentation results of individual trees. Segmentation accuracy of the improved KNN algorithm was the highest among the CHM-based algorithms, such as the WA, RG, and improved KNN algorithms, with an optimal F of 0.893 and minimum F of 0.628. R-CNN algorithm based on LiDAR data had an optimal F value of 0.931 and minimum F value of 0.612. Based on the segmentation results, the overall accuracy ranking of the different segmentation algorithms was BEV Faster R-CNN > improved KNN > RG > WA. The ranking of the segmentation results for sites with different stand densities was low-density (LD) > medium-density (MD) > high-density (HD). For LD and MD sites, the BEV Faster R-CNN algorithm had the highest F values (0.931 and 0.712, respectively). For the HD site, all algorithms performed poorly, and the F values of all algorithms, except the RG algorithm, were higher than 0.6. Based on the segmentation results of different spatial resolutions, CHM result with 0.1 m was the best, being better than the CHM results with 0.25 and 0.5 m. Our results demonstrated that all segmentation algorithms, spatial resolutions, and stand densities affected the segmentation results for individual mangrove trees. Although the segmentation results of the deep learning algorithm were better than those of the other algorithms, the segmentation results at the HD site were limited. Therefore, further research is necessary to improve the accuracy of the segmentation results for individual mangrove trees at HD sites.}
}
@article{DWIVEDI2024102451,
title = {EMViT-Net: A novel transformer-based network utilizing CNN and multilayer perceptron for the classification of environmental microorganisms using microscopic images},
journal = {Ecological Informatics},
volume = {79},
pages = {102451},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102451},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004806},
author = {Karnika Dwivedi and Malay Kishore Dutta and Jay Prakash Pandey},
keywords = {Environmental microorganisms classification, Microscopic images, Computer-aided system, Deep learning, Vision transformer},
abstract = {Environmental microbes are certainly present in our surroundings since they are essential to the growth and survival of human advancement. The detailed analysis of environmental microorganisms (EMs) is very important to recognize, understand and make use of microbes as well and prevent damage. Extracting the discriminatory features from a limited-size dataset is very challenging for a deep learning model and a pure transformer-based network cannot achieve good classification results on a limited-size dataset due to the lack of muti-scale features. In this study, a novel vision transformer-based deep neural network is proposed by integrating the transformer with CNN for the classification of EM using microscopic images. The proposed network EMViT-Net has three main modules: a transformer module, a CNN module and a multilayer perceptron module. The transformer model extracted multiscale features to generate more discriminatory information from the images. A new separable convolutional parameter-sharing attention (SCPSA) block is integrated with the CNN module in the core of EMViT-Net, which makes the model robust to capture the local and global features, and simultaneously reduces the computational complexity of the model. The data augmentation is performed to introduce the variability in the dataset and counter the problem of overfitting and data imbalance. After extensive experiments and detailed analysis, it has been determined that the proposed model EMViT-Net outperforms the other existing methods and achieves state-of-the-art results with an accuracy of 71.17% which proves the effectiveness of the model for the classification of environmental microbes.}
}
@article{MANZANORUBIO2022101910,
title = {Low-cost open-source recorders and ready-to-use machine learning approaches provide effective monitoring of threatened species},
journal = {Ecological Informatics},
volume = {72},
pages = {101910},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101910},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003600},
author = {Robert Manzano-Rubio and Gerard Bota and Lluís Brotons and Eduardo Soto-Largo and Cristian Pérez-Granados},
keywords = {Autonomous recording unit, BirdNET, , Eurasian bittern, Kaleidoscope Pro, Passive acoustic monitoring, Wildlife monitoring},
abstract = {Passive acoustic monitoring is a powerful tool for monitoring vocally active taxa. Automated signal recognition software reduces the expert time needed for recording analyses and allows researchers and managers to manage large acoustic datasets. The application of state-of-the-art techniques for automated identification, such as Convolutional Neural Networks, may be challenging for ecologists and managers without informatics or engineering expertise. Here, we evaluated the use of AudioMoth — a low-cost and open-source sound recorder — to monitor a threatened and patchily distributed species, the Eurasian bittern (Botaurus stellaris). Passive acoustic monitoring was carried out across 17 potential wetlands in north Spain. We also assessed the performance of BirdNET — an automated and freely available classifier able to identify over 3000 bird species — and Kaleidoscope Pro — a user-friendly recognition software — to detect the vocalizations and the presence of the target species. The percentage of presences and vocalizations of the Eurasian bittern automatically detected by BirdNET and Kaleidoscope Pro software was compared to manual annotations of 205 recordings. The species was effectively recorded up to distances of 801–900 m, with at least 50% of the vocalizations uttered within that distance being manually detected; this distance was reduced to 601–700 m when considering the analyses carried out using Kaleidoscope Pro. BirdNET detected the species in 59 of the 63 (93.7%) recordings with known presence of the species, while Kaleidoscope detected the bittern in 62 recordings (98.4%). At the vocalization level, BirdNet and Kaleidoscope Pro were able to detect between 76 and 78%, respectively, of the vocalizations detected by a human observer. Our study highlights the ability of AudioMoth for detecting the bittern at large distances, which increases the potential of that technique for monitoring the species at large spatial scales. According to our results, a single AudioMoth could be useful for monitoring the species' presence in wetlands of up to 150 ha. Our study proves the utility of passive acoustic monitoring, coupled with BirdNET or Kaleidoscope Pro, as an accurate, repeatable, and cost-efficient method for monitoring the Eurasian bittern at large spatial and temporal scales. Nonetheless, further research should evaluate the performance of BirdNET on a larger number of species, and under different recording conditions (e.g., more closed habitats), to improve our knowledge about BirdNET's ability to perform bird monitoring. Future studies should also aim to develop an adequate protocol to perform effective passive acoustic monitoring of the Eurasian bittern.}
}
@article{CLARFELD2023102257,
title = {Evaluating a tandem human-machine approach to labelling of wildlife in remote camera monitoring},
journal = {Ecological Informatics},
volume = {77},
pages = {102257},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102257},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002868},
author = {Laurence A. Clarfeld and Alexej P.K. Sirén and Brendan M. Mulhall and Tammy L. Wilson and Elena Bernier and John Farrell and Gus Lunde and Nicole Hardy and Katherina D. Gieder and Robert Abrams and Sue Staats and Scott McLellan and Therese M. Donovan},
keywords = {Artificial intelligence, Camera trap, Data labeling, Machine learning, Trail camera, Wildlife monitoring, Bounding box},
abstract = {Remote cameras (“trail cameras”) are a popular tool for non-invasive, continuous wildlife monitoring, and as they become more prevalent in wildlife research, machine learning (ML) is increasingly used to automate or accelerate the labor-intensive process of labelling (i.e., tagging) photos. Human-machine hybrid tagging approaches have been shown to greatly increase tagging efficiency (i.e., time to tag a single image). However, those potential increases hinge on the extent to which an ML model makes correct vs. incorrect predictions. We performed an experiment using a ML model that produces bounding boxes around animals, people, and vehicles in remote camera imagery (MegaDetector) to consider the impact of a ML model's performance on its ability to accelerate human labeling. Six participants tagged trail camera images collected from 12 sites in Vermont and Maine, USA (January–September 2022) using three tagging methods (one with ML bounding box assistance and two without assistance). We used a generalized linear mixed model to examine the influence of ML model performance and tagging method on tagging efficiency. We found that ML bounding boxes offer significant improvement in tagging efficiency when labelling data compared to unassisted tagging. Additionally, the time taken to label with bounding boxes was not statistically different from an unassisted tagging approach. However, we found that gains in efficiency are contingent on the ML algorithm's performance and that incorrect ML predictions, particularly the 4.2% false positive and 3.6% false negative predictions, can slow the tagging process compared to a non-hybrid approach. These findings indicate that although practitioners usually forgo the production of bounding boxes when selecting a data labelling process due to the increased effort, ML bounding box-assisted tagging can offer an efficient method for labeling. More broadly, ML-assisted data labelling offers an opportunity to accelerate the analysis of trail camera imagery, but an assessment of the ML model's performance can illuminate whether the hybrid-tagging approach is ultimately a help or hinderance.}
}
@article{BAKANA2024102541,
title = {WildARe-YOLO: A lightweight and efficient wild animal recognition model},
journal = {Ecological Informatics},
volume = {80},
pages = {102541},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102541},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000839},
author = {Sibusiso Reuben Bakana and Yongfei Zhang and Bhekisipho Twala},
keywords = {Wild animal recognition, Deep learning, Lightweight, Efficient, Loss function},
abstract = {For the protection of endangered species and successful wildlife population monitoring, wild animal recognition is essential. While deep learning models like YOLOv5 have shown promise in real-time object recognition, their practical applicability may be constrained by their high processing requirements. In this paper, we suggest a faster and lighter version of YOLOv5s for wild animal recognition. To lower computational costs for model parameters and floating-point operations (FLOPs) for the backbone, our suggested model includes Mobile Bottleneck Block modules and an improved StemBlock. We also use Focal-EIoU as a loss function to gauge the accuracy of the predicted bounding boxes during inference and employ a BiFPN-based neck. We tested our technique on three datasets, including Wild Animal Facing Extinction, Fishmarket, and MS COCO 2017. Additionally, our technique is compared with state-of-the-art deep learning models, and from the baseline model we recorded a 17.65% increase in FPS, 28.55% model parameters reduction, and 50.92% in FLOPs reduction. Furthermore, our model has a faster model loading time, which is critical for deployment in remote areas. This enables real-time species recognition on basic hardware, aiding conservation efforts through rapid analysis. The model advances deep learning in ecology by balancing efficiency with performance.}
}
@article{VANOSTA2023102233,
title = {An active learning framework and assessment of inter-annotator agreement facilitate automated recogniser development for vocalisations of a rare species, the southern black-throated finch (Poephila cincta cincta)},
journal = {Ecological Informatics},
volume = {77},
pages = {102233},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102233},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002625},
author = {John M. {van Osta} and Brad Dreis and Ed Meyer and Laura F. Grogan and J. Guy Castley},
keywords = {Bioacoustics, Machine learning, Annotator agreement, Call recognition, Active learning},
abstract = {The application of machine learning methods has led to major advances in the development of automated recognisers used to analyse bioacoustics data. To further improve the performance of automated call recognisers, we investigated the development of efficient data annotation strategies and how best to address uncertainty around ambiguous vocalisations. These challenges present a particular problem for species whose vocalisations are rare in field recordings, where collecting enough training data can be problematic and a species' vocalisations may be poorly documented. We provide an open access solution to address these challenges using two strategies. First, we applied an active learning framework to iteratively improve a convolutional neural network (CNN) model able to automate call identification for a target rare bird species, the southern black-throated finch (Poephila cincta cincta). We collected 9098 h of unlabelled audio recordings from a field study in the Desert Uplands Bioregion of Queensland, Australia, and used active learning to prioritise human annotation effort towards data that would best improve model fit. Second, we progressed methods for managing ambiguous vocalisations by applying machine learning methods more commonly used in medical image analysis and natural language processing. Specifically, we assessed agreement among human annotators and the CNN model (i.e. inter-annotator agreement) and used this to determine realistic performance outcomes for the CNN model and to identify areas where inter-annotator agreement may be improved. We also applied a classification approach that allowed the CNN model to classify sounds into an ‘uncertain’ category, which replicated a requirement of human-annotation and facilitated the comparison of human-model annotation performance. We found that active learning was an efficient strategy to build a CNN model where there was limited labelled training data available, and target calls were extremely rare in the unlabelled data. As few as five active learning iterations, generating a final labelled dataset of 1073 target calls and 5786 non-target sounds, were required to train a model to identify the target species with comparable performance to experts in the field. Assessment of inter-annotator agreement identified a bias in our model to align predictions most closely with those of the primary annotator and identified significant differences in inter-annotator agreement among subsets of our acoustic data. Our results highlight the use of inter-annotator agreement to understand model performance and identify areas for improvement in data annotation. We also show that excluding ambiguous vocalisations during data annotation results in an overestimation of model performance, an important consideration for datasets with inter-annotator disagreement.}
}
@article{DELPLANQUE2024102679,
title = {Will artificial intelligence revolutionize aerial surveys? A first large-scale semi-automated survey of African wildlife using oblique imagery and deep learning},
journal = {Ecological Informatics},
volume = {82},
pages = {102679},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102679},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002218},
author = {Alexandre Delplanque and Julie Linchant and Xavier Vincke and Richard Lamprey and Jérôme Théau and Cédric Vermeulen and Samuel Foucher and Amara Ouattara and Roger Kouadio and Philippe Lejeune},
keywords = {Wildlife population estimation, Aerial surveys, Deep learning, Biodiversity monitoring, Conservation technology, African savanna},
abstract = {Large African mammal populations are traditionally estimated using the systematic reconnaissance flights (SRF) with rear-seat observers (RSOs). The oblique-camera-count (OCC) approach, utilizing digital cameras on aircraft sides, proved to provide more reliable population estimates but incurs high manual processing costs. Addressing the urgent need for efficiency, the research explores whether a semi-automated deep learning (SADL) model coupled with OCC improves wildlife population estimates compared to the SRF-RSO method. The study area was the Comoé National Park, in Ivory Coast, spanning 11,488 km2 of savannas and open forests. It was surveyed following both SRF-RSO standards and OCC method. Key species included the elephant, western hartebeest, roan antelope, buffalo, kob, waterbuck and warthog. The deep learning model HerdNet, priorly pre-trained on images from Uganda, was incorporated in the SADL pipeline to process the 190,686 images. It involved three human verification steps to ensure quality of detections and to avoid overestimating counts. The entire pipeline aims to balance efficiency and human effort in wildlife population estimation. RSO and SADL-OCC approaches were compared using the Jolly II analysis and a verification of 200 random RSO observations. Jolly II analysis revealed SADL-OCC estimates significantly higher for small-sized species (kob, warthog) and comparable for other key species. Counting differences were mainly attributed to vegetation obstruction, RSO observations not found in the images, and suspected RSO counting errors. Human effort in the SADL-OCC approach totaled 111 h, representing a significant time savings compared to a fully manual interpretation. Introducing the SADL approach for aerial surveys in Comoé National Park enabled us to address the OCC's time-intensive image interpretation. Achieving a significant reduction in human workload, our method provided population estimates comparable to or better than SRF-RSO counts. Vegetation obstruction was a key factor explaining differences, highlighting the OCC method's limitation in vegetated areas. Method comparisons emphasized SADL-OCC's advantages in spotting isolated, small and static animals, reducing count variance between sample units. Despite limitations, the SADL-OCC approach offers transformative potential, suggesting a shift towards DL-assisted aerial surveys for increased efficiency and affordability, especially using microlight aircraft and drones in future wildlife monitoring initiatives.}
}
@article{ZHANG2024102605,
title = {Response of spectral vegetation indices to Erannis jacobsoni Djak. damage in larch forests},
journal = {Ecological Informatics},
volume = {81},
pages = {102605},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102605},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400147X},
author = {Siyuan Zhang and Xiaojun Huang and Lei Ma and Ganbat Dashzevegd and Mungunkhuyag Ariunaa and Gang Bao and Siqin Tong and Yuhai Bao and Altanchimeg Dorjsuren and Davaadorj Enkhnasan},
keywords = {Erannis jacobsoni Djak., Spectral vegetation indices, Larch, Pest damage level, Variation characteristics},
abstract = {Erannis jacobsoni Djak. (EJD), a typical pest of coniferous forests in Mongolia, has severely threatened forest areas in recent years owing to its rapid development and spread. EJD feeds on needles and leaves, killing many trees and causing severe damage to forest ecosystems，which results in substantial local economic losses. The rapid and effective monitoring of forest pests is crucial for preventing or controlling infestations in a timely manner. To this end, in this study, we calculated spectral vegetation indices using UAV multispectral data, assessed ground survey data to determine the degree of pest damage, and conducted sensitivity analysis on the spectral vegetation indices. Nine sensitive spectral vegetation indices were selected to analyze the intramonthly and intermonthly variations in the spectral vegetation indices of forests during EJD infestation: the chlorophyll red-edge parameter index (CIreg), corrected NIR/IR simple ratio (GMSR), intensity index (Int and Int2), improved NIR/red-edge simple ratio (MSRreg), normalized difference NIR vegetation index (NDSI), soil adjusted vegetation index (SAVI), and salinity index (SI2reg and SI3). The results demonstrated that the variance F values of the sensitive spectral vegetation indices after screening using the successive projection algorithm were highly significant at the α=10−10 level, suggesting that these indices are highly sensitive to the level of pest damage. The intramonthly results were as follows: in June, CIreg, GMSR, Int, Int2, MSRreg, SAVI, SI2reg, and SI3 decreased with increasing pest damage, whereas NDSI increased; in August, the difference in index values between light, medium, and heavy damage and healthy stands was not significant; and in September, most of the index differences changed to mild > moderate > severe. Regarding the intermonthly results, the magnitude of the vegetation index values for each sensitive spectrum at different hazard levels was ranked as June > September > August, and the overall difference varied as δ3>δ2>δ1. The spectral vegetation indices apparently responded to different levels of pest damage, making them suitable for quickly and accurately monitoring the occurrence and development of forest pests. These results provide a reference for the monitoring of forest pests at spatial and temporal scales.}
}
@article{KRIVOGUZ2024102513,
title = {Geo-spatial analysis of urbanization and environmental changes with deep neural networks: Insights from a three-decade study in Kerch peninsula},
journal = {Ecological Informatics},
volume = {80},
pages = {102513},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102513},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000554},
author = {Denis Krivoguz},
keywords = {Urbanization dynamics, Kerch Peninsula, Land cover change, Environmental implications, Sustainable development, Remote sensing, GIS analysis, Natural ecosystems, Climate change},
abstract = {This study presents a comprehensive analysis of land use and land cover (LULC) changes on the Kerch Peninsula over the last thirty years, utilizing advanced satellite data and spatial modeling techniques. The research used Landsat 5, 7 and 8 satellite images to capture the intricate dynamics of LULC changes from 1990 to 2020. A quantitative approach was adopted, involving the use of convolutional neural networks (CNN) for enhanced classification accuracy. This methodology allowed for a detailed and precise identification of various LULC classes, revealing significant trends and transformations in the region's landscape. The spatial modeling incorporated in this study allowed exploration of both large-scale patterns and localized changes, providing insights into the drivers and consequences of LULC dynamics. The statistical analysis revealed a notable increase in urbanized areas, coupled with a decline in natural ecosystems such as forests and wetlands. These changes reflect the impact of sustained urban growth and agricultural expansion, underscoring the need for informed land management and conservation strategies. The study findings contribute to understanding urbanization processes and their ecological implications, providing valuable guidance for sustainable regional planning and environmental protection.}
}
@article{LEBIEN2020101113,
title = {A pipeline for identification of bird and frog species in tropical soundscape recordings using a convolutional neural network},
journal = {Ecological Informatics},
volume = {59},
pages = {101113},
year = {2020},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101113},
url = {https://www.sciencedirect.com/science/article/pii/S1574954120300637},
author = {Jack LeBien and Ming Zhong and Marconi Campos-Cerqueira and Julian P. Velev and Rahul Dodhia and Juan Lavista Ferres and T. Mitchell Aide},
keywords = {Acoustic monitoring, Bioacoustics, Sound classification, Convolutional neural network, Deep learning},
abstract = {Automated acoustic recorders can collect long-term soundscape data containing species-specific signals in remote environments. Ecologists have increasingly used them for studying diverse fauna around the globe. Deep learning methods have gained recent attention for automating the process of species identification in soundscape recordings. We present an end-to-end pipeline for training a convolutional neural network (CNN) for multi-species multi-label classification of soundscape recordings, starting from raw, unlabeled audio. Training data for species-specific signals are collected using a semi-automated procedure consisting of an efficient template-based signal detection algorithm and a graphical user interface for rapid detection validation. A CNN is then trained based on mel-spectrograms of sound to predict the set of species present in a recording. Transfer learning of a pre-trained model is employed to reduce the necessary training data and time. Furthermore, we define a loss function that allows for using true and false template-based detections to train a multi-class multi-label audio classifier. This approach leverages relevant absence (negative) information in training, and reduces the effort in creating multi-label training data by allowing weak labels. We evaluated the pipeline using a set of soundscape recordings collected across 749 sites in Puerto Rico. A CNN model was trained to identify 24 regional species of birds and frogs. The semi-automated training data collection process greatly reduced the manual effort required for training. The model was evaluated on an excluded set of 1000 randomly sampled 1-min soundscapes from 17 sites in the El Yunque National Forest. The test recordings contained an average of ~3 present target species per recording, and a maximum of 8. The test set also showed a large class imbalance with most species being present in less than 5% of recordings, and others present in >25%. The model achieved a mean-average-precision of 0.893 across the 24 species. Across all predictions, the total average-precision was 0.975.}
}
@article{LYU2024102383,
title = {Deer survey from drone thermal imagery using enhanced faster R-CNN based on ResNets and FPN},
journal = {Ecological Informatics},
volume = {79},
pages = {102383},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102383},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004120},
author = {Haitao Lyu and Fang Qiu and Li An and Douglas Stow and Rebecca Lewison and Eve Bohnett},
keywords = {UAV, Faster R-CNN, ResNet, FPN, Thermal image, Small object detection},
abstract = {Deer surveys play an important role in the estimation of local ecological balance. In the Chitwan National Park of Nepal, the dense tree canopies and tall vegetation often obscure the presence of wild deer, which has a negative effect on the accurate population surveys of wild deer. UAVs equipped with infrared sensors have been increasingly used to monitor wild deer by capturing a lot of images. How to automatically recognize and obtain the number of deer objects from thermal images is becoming an important research topic. Due to the difference between thermal images and true-color images, as well as the variations in deer object sizes in these two types of images, current ready-to-use object detection models, designed for true-color imagery, are ill-suited for the task of detecting small deer objects within thermal imagery. In this paper, an enhanced Faster R-CNN was constructed to detect small deer objects from thermal images, in which a Feature Pyramid Network (FPN) based on a residual network is used to improve feature extraction for small deer objects and multi-scale feature map constrution for the subsequent region proposals searching, bounding box regression, and regions of interest (RoIs) classification. In addition, small-scaled anchor boxes and a multi-scale feature map selection criterion are devised to improve the detection accuracy of small objects. Finally, based on Faster R-CNN, FPN, and different residual networks including ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152, we constructed five object detection models, and evaluated their detection performance by using COCO evaluation matrix. Under the condition of IoU≥0.5, the integration of Faster R-CNN, FPN, and ResNet18 demonstrated to perform better than others. Specifically, The COCO evaluation results revealed an Average Precision (AP) score of 91.6% for all deer objects. Small deer objects (area ≤ 200 pixels) achieved an AP score of 73.6%, medium deer objects (200 < area ≤ 400 pixels) demonstrated an AP score of 93.4%, and large deer objects (area > 400 pixels) achieved the highest AP score of 94.3%. Our research is helpful for effective wild deer monitoring and conservation and can be a valuable reference for the exploration of small object detection from low-resolution thermal images.}
}
@article{POUTARAUD2024102687,
title = {Meta-Embedded Clustering (MEC): A new method for improving clustering quality in unlabeled bird sound datasets},
journal = {Ecological Informatics},
volume = {82},
pages = {102687},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102687},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002292},
author = {Joachim Poutaraud and Jérôme Sueur and Christophe Thébaud and Sylvain Haupert},
keywords = {Bird sounds, Ecoacoustics, meta-learning, Unsupervised learning, Dimensionality reduction, Spectrogram},
abstract = {In recent years, ecoacoustics has offered an alternative to traditional biodiversity monitoring techniques with the development of passive acoustic monitoring (PAM) systems allowing, among others, to detect and identify species that are difficult to detect by human observers, automatically. PAM systems typically generate large audio datasets, but using these monitoring techniques to infer ecologically meaningful information remains challenging. In most cases, several thousand hours of recordings need to be manually labeled by experts limiting the operability of the systems. Based on recent developments of meta-learning algorithms and unsupervised learning techniques, we propose here Meta-Embedded Clustering (MEC), a new method with high potential for improving clustering quality in unlabeled bird sound datasets. MEC method is organized in two main steps, with: (a) fine-tuning of a pretrained convolutional neural network (CNN) backbone with different meta-learning algorithms using pseudo-labeled data, and (b) clustering of manually-labeled bird sounds in the latent space based on vector embeddings extracted from the fine-tuned CNN. The MEC method significantly enhanced average clustering performance from less than 1% to more than 80%, greatly outperforming the traditional approach of relying solely on CNN features extracted from a general neotropical audio database. However, this enhanced performance came with the cost of excluding a portion of the data categorized as noise. By improving the quality of clustering in unlabeled bird sound datasets, the MEC method should facilitate the work of ecoacousticians in managing acoustic units of bird song/call clustered according to their similarities, and in identifying potential clusters of species undetected using traditional approaches.}
}
@article{DUFOURQ2022101688,
title = {Passive acoustic monitoring of animal populations with transfer learning},
journal = {Ecological Informatics},
volume = {70},
pages = {101688},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101688},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122001388},
author = {Emmanuel Dufourq and Carly Batist and Ruben Foquet and Ian Durbach},
keywords = {Transfer learning, Convolutional neural networks, Deep learning, Vocalisation classification, Bioacoustics},
abstract = {Progress in deep learning, more specifically in using convolutional neural networks (CNNs) for the creation of classification models, has been tremendous in recent years. Within bioacoustics research, there has been a large number of recent studies that use CNNs. Designing CNN architectures from scratch is non-trivial and requires knowledge of machine learning. Furthermore, hyper-parameter tuning associated with CNNs is extremely time consuming and requires expensive hardware. In this paper we assess whether it is possible to build good bioacoustic classifiers by adapting and re-using existing CNNs pre-trained on the ImageNet dataset – instead of designing them from scratch, a strategy known as transfer learning that has proved highly successful in other domains. This study is a first attempt to conduct a large-scale investigation on how transfer learning can be used for passive acoustic monitoring (PAM), to simplify the implementation of CNNs and the design decisions when creating them, and to remove time consuming hyper-parameter tuning phases. We compare 12 modern CNN architectures across 4 passive acoustic datasets that target calls of the Hainan gibbon Nomascus hainanus, the critically endangered black-and-white ruffed lemur Varecia variegata, the vulnerable Thyolo alethe Chamaetylas choloensis, and the Pin-tailed whydah Vidua macroura. We focus our work on data scarcity issues by training PAM binary classification models very small datasets, with as few as 25 verified examples. Our findings reveal that transfer learning can result in up to 82% F1 score while keeping CNN implementation details to a minimum, thus rendering this approach accessible, easier to design, and speeding up further vocalisation annotations to create PAM robust models.}
}
@article{TAKIMOTO2021101466,
title = {Using a two-stage convolutional neural network to rapidly identify tiny herbivorous beetles in the field},
journal = {Ecological Informatics},
volume = {66},
pages = {101466},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101466},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121002570},
author = {Hironori Takimoto and Yasuhiro Sato and Atsushi J. Nagano and Kentaro K. Shimizu and Akihiro Kanagawa},
keywords = {Entomology, Fine-grained image classification, Deep learning, Herbivory, Small object detection},
abstract = {Recently, deep convolutional neural networks (CNN) have been adopted to help non-experts identify insect species from field images. However, the application of these methods on the rapid identification of tiny congeneric species moving across heterogeneous background remains difficult. To improve rapid and automatic identification in the field, we customized an existing CNN-based method for a field video involving two Phyllotreta beetles. We first performed data augmentation using transformations, syntheses, and random erasing of the original images. We then proposed a two-stage method for the detection and identification of small insects based on CNN, where YOLOv4 and EfficientNet were used as a detector and a classifier, respectively. Evaluation of the model revealed that one-step object detection by YOLOv4 alone was not precise (Precision=0.55) when classifying two species of flea beetles and background objects. In contrast, the two-step CNNs improved the precision (Precision=0.89) with moderate accuracy (F-measure=0.55) and acceptable speed (ca. 5 frames per second for full HD images) of detection and identification of insect species in the field. Although real-time identification of tiny insects remains a challenge in the field, our method aids in improving small object detection on a heterogeneous background.}
}
@article{KAHL2021101236,
title = {BirdNET: A deep learning solution for avian diversity monitoring},
journal = {Ecological Informatics},
volume = {61},
pages = {101236},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101236},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000273},
author = {Stefan Kahl and Connor M. Wood and Maximilian Eibl and Holger Klinck},
keywords = {Bioacoustics, Deep learning, Convolutional neural networks, Bird sound recognition, Avian diversity, Passive acoustic monitoring, Conservation},
abstract = {Variation in avian diversity in space and time is commonly used as a metric to assess environmental changes. Conventionally, such data were collected by expert observers, but passively collected acoustic data is rapidly emerging as an alternative survey technique. However, efficiently extracting accurate species richness data from large audio datasets has proven challenging. Recent advances in deep artificial neural networks (DNNs) have transformed the field of machine learning, frequently outperforming traditional signal processing techniques in the domain of acoustic event detection and classification. We developed a DNN, called BirdNET, capable of identifying 984 North American and European bird species by sound. Our task-specific model architecture was derived from the family of residual networks (ResNets), consisted of 157 layers with more than 27 million parameters, and was trained using extensive data pre-processing, augmentation, and mixup. We tested the model against three independent datasets: (a) 22,960 single-species recordings; (b) 286 h of fully annotated soundscape data collected by an array of autonomous recording units in a design analogous to what researchers might use to measure avian diversity in a field setting; and (c) 33,670 h of soundscape data from a single high-quality omnidirectional microphone deployed near four eBird hotspots frequented by expert birders. We found that domain-specific data augmentation is key to build models that are robust against high ambient noise levels and can cope with overlapping vocalizations. Task-specific model designs and training regimes for audio event recognition perform on-par with very complex architectures used in other domains (e.g., object detection in images). We also found that high temporal resolution of input spectrograms (short FFT window length) improves the classification performance for bird sounds. In summary, BirdNET achieved a mean average precision of 0.791 for single-species recordings, a F0.5 score of 0.414 for annotated soundscapes, and an average correlation of 0.251 with hotspot observation across 121 species and 4 years of audio data. By enabling the efficient extraction of the vocalizations of many hundreds of bird species from potentially vast amounts of audio data, BirdNET and similar tools have the potential to add tremendous value to existing and future passively collected audio datasets and may transform the field of avian ecology and conservation.}
}
@article{FRAINER2023102291,
title = {Automatic detection and taxonomic identification of dolphin vocalisations using convolutional neural networks for passive acoustic monitoring},
journal = {Ecological Informatics},
volume = {78},
pages = {102291},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102291},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003205},
author = {Guilherme Frainer and Emmanuel Dufourq and Jack Fearey and Sasha Dines and Rachel Probert and Simon Elwen and Tess Gridley},
keywords = {Convolutional neural networks, Indian Ocean humpback dolphin, Machine learning, Passive acoustic monitoring, Sound detection, Species identification},
abstract = {A novel framework for acoustic detection and species identification is proposed to aid passive acoustic monitoring studies on the endangered Indian Ocean humpback dolphin (Sousa plumbea) in South African waters. Convolutional Neural Networks (CNNs) were used for both detection and identification of dolphin vocalisations tasks, and performance was evaluated using custom and pre-trained architectures (transfer learning). In total, 723 min of acoustic data were annotated for the presence of whistles, burst pulses and echolocation clicks produced by Delphinus delphis (~45.6%), Tursiops aduncus (~39%), Sousa plumbea (~14.4%), Orcinus orca (~1%). The best performing models for detecting dolphin presence and species identification used segments (spectral windows) of two second lengths and were trained using images with 70 and 90 dpi, respectively. The best detection model was built using a customised architecture and achieved an accuracy of 84.4% for all dolphin vocalisations on the test set, and 89.5% for vocalisations with a high signal to noise ratio. The best identification model was also built using the customised architecture and correctly identified S. plumbea (96.9%), T. aduncus (100%), and D. delphis (78%) encounters in the testing dataset. The developed framework was designed based on the knowledge of complex dolphin sounds and it may assists in finding suitable CNN hyper-parameters for other species or populations. Our study contributes towards the development of an open-source tool to assist long-term studies of endangered species, living in highly diverse habitats, using passive acoustic monitoring.}
}
@article{CHAKRABARTY2024102718,
title = {An interpretable fusion model integrating lightweight CNN and transformer architectures for rice leaf disease identification},
journal = {Ecological Informatics},
volume = {82},
pages = {102718},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102718},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002607},
author = {Amitabha Chakrabarty and Sarder Tanvir Ahmed and Md. Fahim Ul Islam and Syed Mahfuzul Aziz and Siti Sarah Maidin},
keywords = {Plant disease detection, BEiT model, Attention mapping, Deep learning, Process innovation, Food productivity},
abstract = {Swift identification of leaf diseases is crucial for sustainable rice farming, a staple grain consumed globally. The high costs and inefficiencies of manual identification underline the requirement of prompt disease detection. Traditional approaches for identifying leaf diseases in crops, particularly rice, are laborious, and and often ineffective. Given the significant impact of leaf diseases (such as Rice Blast, Brown Spot, and Rice Turgor) on rice quality and yield, computer-assisted detection can be an effective method of ensuring the long-term sustainability of rice production. This study utilizes advanced artificial intelligence (AI) as the optimized bidirectional encoder representations from the transformers for images(BEiT) model along with pre-trained CNNs (Convolutional Neural Networks), to build a comprehensive study for detecting rice leaf diseases. We train and validate two extensive datasets, featuring healthy and various types of unhealthy plant and rice leaf images respectively.Our optimized model demonstrates high accuracy, outperforming other deep learning and transformer-based models such as ViT, Xception, InceptionV3, DenseNet169, VGG16, and ResNet50. The proposed model achieves a precision of 0.97, a recall of 0.96, and an F1-score of 0.97.The explainability of our proposed model is achieved through the use of segmentation techniques in conjunction with the Local Interpretable Model-agnostic Explanations (LIME) method.}
}
@article{CAPINHA2021101252,
title = {Deep learning for supervised classification of temporal data in ecology},
journal = {Ecological Informatics},
volume = {61},
pages = {101252},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101252},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000431},
author = {César Capinha and Ana Ceia-Hasse and Andrew M. Kramer and Christiaan Meijer},
keywords = {Deep learning, Ecological prediction, Scalability, Sequential data, Temporal ecology, Time series},
abstract = {Temporal data is ubiquitous in ecology and ecologists often face the challenge of accurately differentiating these data into predefined classes, such as biological entities or ecological states. The usual approach consists of transforming the time series into user-defined features and then using these features as predictors in conventional statistical or machine learning models. Here we suggest the use of deep learning models as an alternative to this approach. Recent deep learning techniques can perform the classification directly from the time series, eliminating subjective and resource-consuming data transformation steps, and potentially improving classification results. We describe some of the deep learning architectures relevant for time series classification and show how these architectures and their hyper-parameters can be tested and used for the classification problems at hand. We illustrate the approach using three case studies from distinct ecological subdisciplines: i) insect species identification from wingbeat spectrograms; ii) species distribution modelling from climate time series and iii) the classification of phenological phases from continuous meteorological data. The deep learning approach delivered ecologically sensible and accurate classifications demonstrating its potential for wide applicability across subfields of ecology.}
}
@article{GHOSH2024102581,
title = {HPB3C-3PG algorithm: A new hybrid global optimization algorithm and its application to plant classification},
journal = {Ecological Informatics},
volume = {81},
pages = {102581},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102581},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001237},
author = {Sukanta Ghosh and Amar Singh and Shakti Kumar},
keywords = {Optimization problem, HPB3C-3PGA, Hybrid algorithm, CEC’21 benchmark function, PB3C, 3PGA, Exploration, Exploitation, Image classification},
abstract = {This paper proposes a hybrid bio-inspired search and optimization algorithm that combines the strengths of the PB3C (Parallel Big Bang Big Crunch) and 3PGA (3 Parent Genetic Algorithm) algorithms. The hybrid algorithm employs a single population-based evolutionary search coupled with multi-population parallel processing techniques to address optimization problems. The proposed algorithm is implemented in MATLAB software. We evaluate the performance of the proposed algorithm on the CEC2021 standard test bench suite. The performance of the proposed approach is compared with that of the other nine algorithms. The comparative analysis shows that the proposed hybrid PB3C and 3PGA algorithms performed better than the other nine optimization algorithms. Furthermore, this chapter proposes an HPB3C-3PGA-based approach to evolve the near-optimal architecture of CNN. The proposed plant image classification approach is implemented in Python and compared with 12 other approaches. The proposed approach achieved an accuracy of 98.96% on the Mendeley dataset and 98.97% on the CVIP100 dataset. The proposed approach outperforms all other approaches for the plant leaf classification problem. This research significantly contributes to overcoming limitations in existing approaches, providing a robust solution for optimization problems and image classification tasks.}
}
@article{SWAMINATHAN2024102471,
title = {Multi-label classification for acoustic bird species detection using transfer learning approach},
journal = {Ecological Informatics},
volume = {80},
pages = {102471},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102471},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400013X},
author = {Bhuvaneswari Swaminathan and M. Jagadeesh and Subramaniyaswamy Vairavasundaram},
keywords = {Wav2vec, Transformers, Transfer learning, Multi-label, Bird species classification, Audio classification},
abstract = {As part of ornithology, bird species classification is vital to understanding species distribution, habitat requirements and environmental changes that affect bird populations. It is possible for ornithologists to assess the health of a certain habitat by tracking changes in bird species distributions. This work has extended an efficient transfer learning technique for labelling and classifying multiple bird species from real-time audio recordings. For this purpose, Wav2vec is fine-tuned using the back propagation technique, which makes the feature extractor more effective in learning each bird's pitch and other sound characteristics. To perform the task, each audio recording has been clipped as chunks from the overlapping audio to determine multi-labels from it. Through the application of transfer learning, the features of audio recordings have been automatically extracted for classification and fed to a feed-forward network. Subsequently, probabilities associated with each audio segment is aggregated through the clipping approach to represent multiple species of bird call. These probability scores are then used to determine the presence of predominant bird species in the audio recording for multi-labelling. The proposed Wav2vec demonstrates remarkable performance, achieving an F1-score of 0.89 using the Xeno-Canto dataset in which outperforming other multi-label classifiers.}
}
@article{CHABOT2022101547,
title = {Using Web images to train a deep neural network to detect sparsely distributed wildlife in large volumes of remotely sensed imagery: A case study of polar bears on sea ice},
journal = {Ecological Informatics},
volume = {68},
pages = {101547},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101547},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121003381},
author = {Dominique Chabot and Seth Stapleton and Charles M. Francis},
keywords = {Artificial intelligence, Big data, Computer vision, Conservation, Machine learning, Marine mammals},
abstract = {Remote sensing can be a valuable alternative or complement to traditional techniques for monitoring wildlife populations, but often entails operational bottlenecks at the image analysis stage. For example, photographic aerial surveys have several advantages over surveys employing airborne observers or other more intrusive monitoring techniques, but produce onerous amounts of imagery for manual analysis when conducted across vast areas, such as the Arctic. Deep learning algorithms, chiefly convolutional neural networks (CNNs), have shown promise for automatically detecting wildlife in large and/or complex image sets. But for sparsely distributed species, such as polar bears (Ursus maritimus), there may not be sufficient known instances of the animals in an image set to train a CNN. We investigated the feasibility of instead providing ‘synthesized’ training data to a CNN to detect polar bears throughout large volumes of aerial imagery from a survey of the Baffin Bay subpopulation. We harvested 534 miscellaneous images of polar bears from the Web that we edited to more closely resemble 21 known images of bears from the aerial survey that were solely used for validation. We combined the Web images of polar bears with 6292 random background images from the aerial survey to train a CNN (ResNet-50), which subsequently correctly classified 20/21 (95%) bear images from the survey and 1172/1179 (99.4%) random background validation images. Given that even a small background misclassification rate could produce multitudinous false positives over many thousands of photos, we describe a potential workflow to efficiently screen out erroneous detections. We also discuss potential avenues to improve CNN accuracy, and the broader applicability of our approach to other image-based wildlife monitoring scenarios. Our results demonstrate the feasibility of using miscellaneously sourced images of animals to train deep neural networks for specific wildlife detection tasks.}
}
@article{CIAMPI2023102384,
title = {A deep learning-based pipeline for whitefly pest abundance estimation on chromotropic sticky traps},
journal = {Ecological Informatics},
volume = {78},
pages = {102384},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102384},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004132},
author = {Luca Ciampi and Valeria Zeni and Luca Incrocci and Angelo Canale and Giovanni Benelli and Fabrizio Falchi and Giuseppe Amato and Stefano Chessa},
keywords = {Smart agriculture, Smart farming, Integrated pest management, Computer vision, Object counting, Visual counting},
abstract = {Integrated Pest Management (IPM) is an essential approach used in smart agriculture to manage pest populations and sustainably optimize crop production. One of the cornerstones underlying IPM solutions is pest monitoring, a practice often performed by farm owners by using chromotropic sticky traps placed on insect hot spots to gauge pest population densities. In this paper, we propose a modular model-agnostic deep learning-based counting pipeline for estimating the number of insects present in pictures of chromotropic sticky traps, thus reducing the need for manual trap inspections and minimizing human effort. Additionally, our solution generates a set of raw positions of the counted insects and confidence scores expressing their reliability, allowing practitioners to filter out unreliable predictions. We train and assess our technique by exploiting PST - Pest Sticky Traps, a new collection of dot-annotated images we created on purpose and we publicly release, suitable for counting whiteflies. Experimental evaluation shows that our proposed counting strategy can be a valuable Artificial Intelligence-based tool to help farm owners to control pest outbreaks and prevent crop damages effectively. Specifically, our solution achieves an average counting error of approximately 9% compared to human capabilities requiring a matter of seconds, a large improvement respecting the time-intensive process of manual human inspections, which often take hours or even days.}
}
@article{BAYR2019220,
title = {Automatic detection of woody vegetation in repeat landscape photographs using a convolutional neural network},
journal = {Ecological Informatics},
volume = {50},
pages = {220-233},
year = {2019},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S1574954118303121},
author = {Ulrike Bayr and Oskar Puschmann},
keywords = {Repeat photography, Photo monitoring, Landscape monitoring, Landscape change, Vegetation succession, Machine learning},
abstract = {Repeat photography is an efficient method for documenting long-term landscape changes. So far, the usage of repeat photographs for quantitative analyses is limited to approaches based on manual classification. In this paper, we demonstrate the application of a convolutional neural network (CNN) for the automatic detection and classification of woody regrowth vegetation in repeat landscape photographs. We also tested if the classification results based on the automatic approach can be used for quantifying changes in woody vegetation cover between image pairs. The CNN was trained with 50 × 50 pixel tiles of woody vegetation and non-woody vegetation. We then tested the classifier on 17 pairs of repeat photographs to assess the model performance on unseen data. Results show that the CNN performed well in differentiating woody vegetation from non-woody vegetation (accuracy = 87.7%), but accuracy varied strongly between individual images. The very similar appearance of woody vegetation and herbaceous species in photographs made this a much more challenging task compared to the classification of vegetation as a single class (accuracy = 95.2%). In this regard, image quality was identified as one important factor influencing classification accuracy. Although the automatic classification provided good individual results on most of the 34 test photographs, change statistics based on the automatic approach deviated from actual changes. Nevertheless, the automatic approach was capable of identifying clear trends in increasing or decreasing woody vegetation in repeat photographs. Generally, the use of repeat photography in landscape monitoring represents a significant added value to other quantitative data retrieved from remote sensing and field measurements. Moreover, these photographs are able to raise awareness on landscape change among policy makers and public as well as they provide clear feedback on the effects of land management.}
}
@article{JAHANBAKHT2023102303,
title = {Semi-supervised and weakly-supervised deep neural networks and dataset for fish detection in turbid underwater videos},
journal = {Ecological Informatics},
volume = {78},
pages = {102303},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102303},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003321},
author = {Mohammad Jahanbakht and Mostafa {Rahimi Azghadi} and Nathan J. Waltham},
keywords = {Weakly-supervised classifier, Self-supervised learning, Deep neural networks, Contrastive learning, Transfer learning, XGBoost ensemble, Fish detection, Highly turbid waters},
abstract = {Fish are key members of marine ecosystems, and they have a significant share in the healthy human diet. Besides, fish abundance is an excellent indicator of water quality, as they have adapted to various levels of oxygen, turbidity, nutrients, and pH. To detect various fish in underwater videos, Deep Neural Networks (DNNs) can be of great assistance. However, training DNNs is highly dependent on large, labeled datasets, while labeling fish in turbid underwater video frames is a laborious and time-consuming task, hindering the development of accurate and efficient models for fish detection. To address this problem, firstly, we have collected a dataset called FishInTurbidWater, which consists of a collection of video footage gathered from turbid waters, and quickly and weakly (i.e., giving higher priority to speed over accuracy) labeled them in a 4-times fast-forwarding software. Next, we designed and implemented a semi-supervised contrastive learning fish detection model that is self-supervised using unlabeled data, and then fine-tuned with a small fraction (20%) of our weakly labeled FishInTurbidWater data. At the next step, we trained, using our weakly labeled data, a novel weakly-supervised ensemble DNN with transfer learning from ImageNet. The results show that our semi-supervised contrastive model leads to more than 20 times faster turnaround time between dataset collection and result generation, with reasonably high accuracy (89%). At the same time, the proposed weakly-supervised ensemble model can detect fish in turbid waters with high (94%) accuracy, while still cutting the development time by a factor of four, compared to fully-supervised models trained on carefully labeled datasets. Our dataset and code are publicly available at the hyperlink FishInTurbidWater.}
}
@article{MORALES2022101909,
title = {Method for passive acoustic monitoring of bird communities using UMAP and a deep neural network},
journal = {Ecological Informatics},
volume = {72},
pages = {101909},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101909},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122003594},
author = {Gabriel Morales and Víctor Vargas and Diego Espejo and Víctor Poblete and Jorge A. Tomasevic and Felipe Otondo and Juan G. Navedo},
keywords = {Passive acoustic monitoring, Bird community, Deep learning, Soundscape, Phenology},
abstract = {An effective practice for monitoring bird communities is the recognition and identification of their acoustic signals, whether simple, complex, fixed or variable. A method for the passive monitoring of diversity, activity and acoustic phenology of structural species of a bird community in an annual cycle is presented. The method includes the semi-automatic elaboration of a dataset of 22 vocal and instrumental forms of 16 species. To analyze bioacoustic richness, the UMAP algorithm was run on two parallel feature extraction channels. A convolutional neural network was trained using STFT-Mel spectrograms to perform the task of automatic identification of bird species. The predictive performance was evaluated by obtaining a minimum average precision of 0.79, a maximum equal to 1.0 and a mAP equal to 0.97. The model was applied to a huge set of passive recordings made in a network of urban wetlands for one year. The acoustic activity results were synchronized with climatological temperature data and sunlight hours. The results confirm that the proposed method allows for monitoring a taxonomically diverse group of birds that nourish the annual soundscape of an ecosystem, as well as detecting the presence of cryptic species that often go unnoticed.}
}
@article{CORO2024102644,
title = {Climate change effects on animal presence in the Massaciuccoli Lake basin},
journal = {Ecological Informatics},
volume = {81},
pages = {102644},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102644},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001869},
author = {Gianpaolo Coro and Pasquale Bove and Ilaria Baneschi and Andrea Bertini and Lorenzo Calvisi and Antonello Provenzale},
keywords = {Ecological niche modelling, Species richness, Wetlands, Artificial intelligence, Open Science, Maximum entropy},
abstract = {Big-data mining approaches based on Artificial Intelligence models can help forecast biodiversity changes before they happen. These approaches can predict macroscopic species distribution patterns and trends that can inform preventive measures to avoid the loss of ecosystem functions and services. They can, therefore, help study and mitigate climate change implications on biodiversity conservation in fragile ecosystems. Wetlands are particularly fragile ecosystems where climate change poses severe risks and has dramatically reduced their size over the past century, with profound consequences on biodiversity and ecosystem services. Through big-data mining approaches, we can predict future wetland biodiversity trends in the context of climate change. This paper proposes such predictive analysis for a specific wetland: The Massaciuccoli Lake basin in Tuscany, Italy. This basin is a critical tourist attraction due to its rich biodiversity, making it an area of interest for citizens, tourists, and scientists. However, the region's suitability for native and non-native species is at risk due to climate and land-use change. Using machine-learning models, we predict the potential effects of climate change on animal spatial distribution in the basin under different greenhouse gas emission scenarios. The results suggest that habitat suitability has generally improved from 1950 to today, presumably owing to the targeted conservation strategies adopted in the area, but climate change will severely reduce bird biodiversity by 2050 while favouring several insect species' proliferation and other species' habitat change, even under a medium-emission scenario. This will lead to significant changes in the basin's biodiversity. Our methodology is adaptable to other wetland basins, being fully based on open data and models. The spatially explicit modelling used in this research provides valuable information for policymakers and spatial planners, complementing traditional biodiversity trend analyses.}
}
@article{CONCEPCION2023102344,
title = {BivalveNet: A hybrid deep neural network for common cockle (Cerastoderma edule) geographical traceability based on shell image analysis},
journal = {Ecological Informatics},
volume = {78},
pages = {102344},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102344},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003734},
author = {Ronnie Concepcion and Marielet Guillermo and Susanne E. Tanner and Vanessa Fonseca and Bernardo Duarte},
keywords = {Traceability, Morphometric analysis, Deep learning, Bivalves, Low-cost},
abstract = {Bivalve traceability is a major concern. It is of utmost importance to develop tools that allow providing important information to the consumer, not only on the origin of the product but also on its sustainability and safety, due to the harvest restrictions imposed by regulatory entities. This study evaluated the application of computer vision machine learning technologies for efficiently discriminating cockle harvesting origin based on shell geometric and morphometric analysis, improving the traceability methodologies in these organisms, and highlighting the potential of these low-cost techniques as a reliable traceability tool. Thirty Cerastoderma edule samples were collected along the five locations in Atlantic West and South Portuguese coast with individual images processed using lazysnapping segmentation, spectro-textural-morphological phenotype extraction, and feature selection through hybrid Principal Component Analysis and Neighborhood Component Analysis which resulted in R, a*, b*, entropy, and diameter. Three approaches of traceability models were developed and tested: pre-trained networks (EfficientNet-Bo, ResNet101, MobileNetV2, InceptionV3) with numerical inputs (Approach 1), image-based pre-trained networks (Approach 2), and hybrid deep neural networks of Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Bidirectional LSTM (BiLSTM) (Approach 3). Based on the test results, Approach 3 with GRU-LSTM-BiLSTM sequence exhibited the highest accuracy (96.91%) and sensitivity (96%) among the other thirteen machine learning models, hence, named as BivalveNet. Comparing the attained accuracy from the BivalveNet to other mollusc traceability studies, it was observed that an efficiency close to the attained using standard destructive, time-consuming, and expensive techniques, making BivalveNet a highly advantageous approach for common cockle geographical traceability studies, available for application to other bivalve species.}
}
@article{BOHNER2023102150,
title = {A semi-automatic workflow to process images from small mammal camera traps},
journal = {Ecological Informatics},
volume = {76},
pages = {102150},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102150},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123001796},
author = {Hanna Böhner and Eivind Flittie Kleiven and Rolf Anker Ims and Eeva M. Soininen},
keywords = {Camera trap, Rodent, Automatic image classification, Adaptive monitoring, Data processing, Deep learning},
abstract = {Camera traps have become popular for monitoring biodiversity, but the huge amounts of image data that arise from camera trap monitoring represent a challenge and artificial intelligence is increasingly used to automatically classify large image data sets. However, it is still challenging to combine automatic classification with other steps and tools needed for efficient, quality-assured and adaptive processing of camera trap images in long-term monitoring programs. Here we propose a semi-automatic workflow to process images from small mammal cameras that combines all necessary steps from downloading camera trap images in the field to a quality checked data set ready to be used in ecological analyses. The workflow is implemented in R and includes (1) managing raw images, (2) automatic image classification, (3) quality check of automatic image labels, as well as the possibilities to (4) retrain the model with new images and to (5) manually review subsets of images to correct image labels. We illustrate the application of this workflow for the development of a new monitoring program of an Arctic small mammal community. We first trained a classification model for the specific small mammal community based on images from an initial set of camera traps. As the monitoring program evolved, the classification model was retrained with a small subset of images from new camera traps. This case study highlights the importance of model retraining in adaptive monitoring programs based on camera traps as this step in the workflow increases model performance and substantially decreases the total time needed for manually reviewing images and correcting image labels. We provide all R scripts to make the workflow accessible to other ecologists.}
}
@article{VILLON2024102499,
title = {Toward an artificial intelligence-assisted counting of sharks on baited video},
journal = {Ecological Informatics},
volume = {80},
pages = {102499},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102499},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000414},
author = {Sébastien Villon and Corina Iovan and Morgan Mangeas and Laurent Vigliola},
keywords = {Deep learning, Neural network, Coral reef, Marine ecology, Shark conservation},
abstract = {Given the global biodiversity crisis, there is an urgent need for new tools to monitor populations of endangered marine megafauna, like sharks. To this end, Baited Remote Underwater Video Stations (BRUVS) stand as the most effective tools for estimating shark abundance, measured using the MaxN metric. However, a bottleneck exists in manually computing MaxN from extensive BRUVS video data. Although artificial intelligence methods are capable of solving this problem, their effectiveness is tested using AI metrics such as the F-measure, rather than ecologically informative metrics employed by ecologists, such as MaxN. In this study, we present both an automated and a semi-automated deep learning approach designed to produce the MaxN abundance metric for three distinct reef shark species: the grey reef shark (Carcharhinus amblyrhynchos), the blacktip reef shark (C. melanopterus), and the whitetip reef shark (Triaenodon obesus). Our approach was applied to one-hour baited underwater videos recorded in New Caledonia (South Pacific). Our fully automated model achieved F-measures of 0.85, 0.43, and 0.72 for the respective three species. It also generated MaxN abundance values that showed a high correlation with manually derived data for C. amblyrhynchos (R = 0.88). For the two other species, correlations were significant but weak (R = 0.35–0.44). Our semi-automated method significantly enhanced F-measures to 0.97, 0.86, and 0.82, resulting in high-quality MaxN abundance estimations while drastically reducing the video processing time. To our knowledge, we are the first to estimate MaxN with a deep-learning approach. In our discussion, we explore the implications of this novel tool and underscore its potential to produce innovative metrics for estimating fish abundance in videos, thereby addressing current limitations and paving the way for comprehensive ecological assessments.}
}
@article{LEVY2024102737,
title = {Improving deep learning based bluespotted ribbontail ray (Taeniura Lymma) recognition},
journal = {Ecological Informatics},
volume = {82},
pages = {102737},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102737},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002796},
author = {Avivit Levy and Adi Barash and Chen Zaguri and Ariel Hadad and Polina Polsky},
keywords = {Computer vision, Citizen science, Ecological study, Marine animal recognition, Pose handling},
abstract = {This paper presents the novel task of bluespotted ribbontail (BR) ray (Taeniura lymma) recognition using deep learning based computer vision methods to enable the identification of specific individuals of this species. Mapping the specific individuals in relation to location and time will allow marine researchers to understand their movement patterns, habitat choice, life span, size of the population and more – data which could allow monitoring and establishing a tailor-made conservation plan for this species. Our work is pioneer on this recognition problem. We give a detailed description of the three basic steps of detection, feature extraction and recognition in this vision problem and perform experiments to explore the system configuration and what improves the performance. A feature extraction enhancement as well as a crucial effect of a split into different main poses are demonstrated. Though the precision results achieved in this paper are still moderate and should be further improved, they are nevertheless promising and reasonable for practical use if the six best matches are chosen. For this scenario, almost 85% precision for upper-pose model, and almost 80% precision for left- and right-pose models, are achieved demonstrating the feasibility of the pipeline suggested as well as opportunities for improvement.}
}
@article{VEGA2024102535,
title = {Convolutional neural networks for hydrothermal vents substratum classification: An introspective study},
journal = {Ecological Informatics},
volume = {80},
pages = {102535},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102535},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000773},
author = {Pedro Juan Soto Vega and Panagiotis Papadakis and Marjolaine Matabos and Loïc {Van Audenhaege} and Annah Ramiere and Jozée Sarrazin and Gilson Alexandre Ostwald Pedro {da Costa}},
keywords = {Image classification, Deep learning, Hydrothermal vents, Uncertainty analysis},
abstract = {The increasing availability of seabed images has created new opportunities and challenges for monitoring and better understanding the spatial distribution of fauna and substrata. To date, however, deep-sea substratum classification relies mostly on visual interpretation, which is costly, time-consuming, and prone to human bias or error. Motivated by the success of convolutional neural networks in learning semantically rich representations directly from images, this work investigates the application of state-of-the-art network architectures, originally employed in the classification of non-seabed images, for the task of hydrothermal vent substrata image classification. In assessing their potential, we conduct a study on the generalization, complementarity and human interpretability aspects of those architectures. Specifically, we independently trained deep learning models with the selected architectures using images obtained from three distinct sites within the Lucky-Strike vent field and assessed the models' performances on-site as well as off-site. To investigate complementarity, we evaluated a classification decision committee (CDC) built as an ensemble of networks in which individual predictions were fused through a majority voting scheme. The experimental results demonstrated the suitability of the deep learning models for deep-sea substratum classification, attaining accuracies reaching up to 80% in terms of F1-score. Finally, by further investigating the classification uncertainty computed from the set of individual predictions of the CDC, we describe a semiautomatic framework for human annotation, which prescribes visual inspection of only the images with high uncertainty. Overall, the results demonstrated that high accuracy values of over 90% F1-score can be obtained with the framework, with a small amount of human intervention.}
}
@article{LIN2024102507,
title = {A model for forest type identification and forest regeneration monitoring based on deep learning and hyperspectral imagery},
journal = {Ecological Informatics},
volume = {80},
pages = {102507},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102507},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000499},
author = {Feng-Cheng Lin and Yi-Shiang Shiu and Pei-Jung Wang and Uen-Hao Wang and Jhe-Syuan Lai and Yung-Chung Chuang},
keywords = {Remote sensing, Deep learning, VGG19, ResNet50, Hyperspectral images},
abstract = {Traditional ground-based forest survey methods involve high labor costs, and their inefficiency makes comprehensive forest resource surveys challenging. With the development of new sensors and vehicles in recent years, more diverse and novel remote sensing detection and survey techniques have emerged. This study aims to use hyperspectral imagery to classify forest types containing representative tree species. To verify the feasibility of the proposed methods, we used hyperspectral imagery from the Taiwan Forestry Experiment Institute's Liugui Research Forest in southern Taiwan, which has an area of 9882 ha and an altitude of 250–2600 m. Hyperspectral imagery offers several advantages compared to traditional multispectral imagery; it captures a broad spectrum of contiguous, narrow spectral bands, providing highly detailed spectral information, enabling differentiation of tree components that appear similar in multispectral imagery. Eight identifiable forest types were selected for the models considered, and three different deep learning algorithms, VGG19, ResNet50 and a proposed combination (VGG19 + ResNet50), were used to screen the best algorithms. Data formats and pre-processing methods that can effectively improve computational performance were explored. The research results found that: (1) Band filtering is a necessary means to improve calculation performance; (2) Flattening the original convolution kernel with cubic characteristics can significantly reduce the time required for calculation. In terms of simulation results, VGG19 + ResNet50 was identified as the best model. Its overall classification accuracy can generally reach 93% to 100%. According to the calculation process set in this study, the time required for model training can be shortened to less than 30 min. The results of this research will help process more detailed and complex information in forest resource management and more accurately quantify forest ecology and woodland conditions.}
}
@article{JOELIANTO2024102495,
title = {Convolutional neural network-based real-time mosquito genus identification using wingbeat frequency: A binary and multiclass classification approach},
journal = {Ecological Informatics},
volume = {80},
pages = {102495},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102495},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000372},
author = {Endra Joelianto and Miranti Indar Mandasari and Daniel Beltsazar Marpaung and Naufal Dzaki Hafizhan and Teddy Heryono and Maria Ekawati Prasetyo and  Dani and Susy Tjahjani and Tjandra Anggraeni and Intan Ahmad},
keywords = {Dengue infection, , Mosquito vector, Wingbeat frequency, Deep learning, Sustainability monitoring},
abstract = {Global rises in dengue hemorrhagic fever, especially in Asia and Latin America, underscore the necessity for enhanced public health interventions. Aedes spp. mosquitoes are the primary vectors; however, species such as Culex quinquefasciatus pose significant health risks by transmitting diseases such as filariasis, impacting millions of people worldwide. This study introduces a real-time convolutional neural network-based mosquito classification system using wingbeat frequency for identifying various mosquito species, with emphasis on Aedes sp. We proposed and assessed two models: a binary classification and a multiclass system. The binary system exhibited an outstanding accuracy of 91.76% in distinguishing between Aedes aegypti and Culex quinquefasciatus. The multiclass system accurately identified female and male Aedes aegypti and Culex quinquefasciatus with a precision of 87.16%. This innovative approach serves as a potential tool for dengue infection control and a versatile instrument for combating various mosquito-borne illnesses, enhancing vector surveillance for comprehensive disease management.}
}
@article{ZHENG2024102689,
title = {A video object segmentation-based fish individual recognition method for underwater complex environments},
journal = {Ecological Informatics},
volume = {82},
pages = {102689},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102689},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002310},
author = {Tao Zheng and Junfeng Wu and Han Kong and Haiyan Zhao and Boyu Qu and Liang Liu and Hong Yu and Chunyu Zhou},
keywords = {Individual fish recognition, Video object segmentation, Underwater complex environments, Deep learning, Intelligent aquaculture},
abstract = {Currently, aquaculture methods tend to combine scale and intelligence, which saves manpower and improves the survival rate of seafood at the same time. High-precision and high-efficiency fish individual recognition can provide key technical support for fish disease detection, feeding habits, body condition, etc. In the realm of intelligent aquaculture, it provides robust data support for precision fish farming. However, the current research methods for individual fish recognition struggle to maintain the network model's focus on the fish body in real marine underwater complex environments (e.g., environmental background interference such as coral reefs, overlap between fish bodies, light noise, etc.), leading to unsatisfactory recognition results. To this end, this paper proposes a method for fish individual recognition in underwater complex environments based on video object segmentation, which consists of three parts, including a fish individual segmentation detection module, a fish individual recognition module, and an all-in-one visualization module. The work adopts a combination of deep learning methods and video object segmentation algorithms to solve the problem of low attention and poor detection accuracy of fish individuals in real underwater complex environments, which effectively improves the accuracy and efficiency of fish individual recognition, and analyzes and discusses the comparison of recognition effects using different weights. The results of the simulation experiments show that the key metric Rank1 value of the method achieves more than 96% accuracy on the public datasets DlouFish, WideFish, and the Fish-seg dataset produced in this paper, and improves over the state-of-the-art methods for fish individual recognition by 2.23%, 1.33%, and 1.25%, respectively.}
}
@article{WANG2024102409,
title = {Vegetation coverage precisely extracting and driving factors analysis in drylands},
journal = {Ecological Informatics},
volume = {79},
pages = {102409},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102409},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004387},
author = {Haolin Wang and Dongwei Gui and Qi Liu and Xinlong Feng and Jia Qu and Jianping Zhao and Guangyan Wang and Guanghui Wei},
keywords = {Image segmentation, Fractional vegetation coverage, Arid region, Ecological restoration, Deep learning},
abstract = {Fractional Vegetation Coverage (FVC) is an essential indicator that captures variations in vegetation and documents the impacts of climate change and human activity for environmental assessment. However, conventional methods encounter challenges in accurately extracting fine-scale FVC in drylands due to the vegetation distribution being very heterogeneous in space with patches and inter-patches. Using the lower Tarim River Basin as a typical study case, we investigated three deep convolutional neural networks—Unet, Pspnet, and Deeplabv3 + —to generate high-precision FVC in drylands with high-resolution (0.8 m) remote sensing images. Among these models, the Unet model performed better, with an accuracy of 93.38%, while the accuracy of Pspnet and Deeplabv3+ was 88.14% and 88.91%, respectively. Comparison with the FVC derived from normalized difference vegetation index (NDVI), and land use/land cover data from ESRI and ESA indicated that the FVC map produced by Unet was more consistent with on-site field observations. Delving into drivers influencing dryland FVC, we found that groundwater depth plays a pivotal role compared to topographical and climatic variables. Specifically, when the groundwater depth exceeds −3 m, the probability of occurring high FVC is reduced to 50%. This study innovatively extracted the FVC of drylands with high vegetation spatial heterogeneity, which better solves the insufficient accuracy of the existing dataset, serves as a valuable reference for monitoring vegetation change, and facilitates more precise quantification of carbon storage.}
}
@article{LEKUNBERRI2022101495,
title = {Identification and measurement of tropical tuna species in purse seiner catches using computer vision and deep learning},
journal = {Ecological Informatics},
volume = {67},
pages = {101495},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101495},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121002867},
author = {Xabier Lekunberri and Jon Ruiz and Iñaki Quincoces and Fadi Dornaika and Ignacio Arganda-Carreras and Jose A. Fernandes},
keywords = {Computer vision, Deep learning, Fisheries, Electronic monitoring, Tropical tuna species identification},
abstract = {Fishery monitoring programs are essential for effective management of marine resources, as they provide scientists and managers with the necessary data for both the preparation of scientific advice and fisheries control and surveillance. The monitoring is generally done by human observers, both in port and onboard, with a high cost involved. Consequently, some Regional Fisheries Management Organizations (RFMO) are opting for electronic monitoring (EM) as an alternative or complement to human observers in certain fisheries. This is the case of the tropical tuna purse seine fishery operating in the Indian and Atlantic oceans, which started an EM program on a voluntary basis in 2017. However, even when the monitoring is conducted though EM, the image analysis is a tedious task manually performed by experts. In this paper, we propose a cost-effective methodology for the automatic processing of the images already being collected by cameras onboard tropical tuna purse seiners. Firstly, the images are preprocessed to homogenize them across all vessels and facilitate subsequent steps. Secondly, the fish are individually segmented using a deep neural network (Mask R-CNN). Then, all segments are passed through other deep neural network (ResNet50V2) to classify them by species and estimate their size distribution. For the classification of fish, we achieved an accuracy for all species of over 70%, i.e., about 3 out of 4 individuals are correctly classified to their corresponding species. The size distribution estimates are aligned with official port measurements but calculated using a larger number of individuals. Finally, we also propose improvements to the current image capture systems which can facilitate the work of the proposed automation methodology.}
}
@article{VABO2021101322,
title = {Automatic interpretation of salmon scales using deep learning},
journal = {Ecological Informatics},
volume = {63},
pages = {101322},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101322},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121001138},
author = {Rune Vabø and Endre Moen and Szymon Smoliński and Åse Husebø and Nils Olav Handegard and Ketil Malde},
keywords = {Fish scales, Deep learning, EfficientNet, Transfer learning, Age reading, Maturity staging},
abstract = {For several fish species, age and other important biological information is manually inferred from visual scrutinization of scales, and reliable automatic methods are not widely available. Here, we apply Convolutional Neural Networks (CNN) with transfer learning on a novel dataset of 9056 images of Atlantic salmon scales for four different prediction tasks. We predicted fish origin (wild/farmed), spawning history (previous spawner/non-spawner), river age, and sea age. We obtained high prediction accuracy for fish origin (96.70%), spawning history (96.40%), and sea age (86.99%), but lower accuracy for river age (63.20%). Against six human expert readers with an additional dataset of 150 scales, the CNN showed the second-highest percentage agreement for sea age (94.00%, range 87.25±97.30%), but the lowest agreement for river age (66.00%, range 66.00– 84.68%). Estimates of river age by expert readers exhibited higher variance and lower levels of agreement compared to sea age and may indicate why this task is also more difficult for the CNN. Automatic interpretation of scales may provide a cost- and time-efficient method of predicting fish age and life-history traits.}
}
@article{NAZIR2024102453,
title = {Object classification and visualization with edge artificial intelligence for a customized camera trap platform},
journal = {Ecological Informatics},
volume = {79},
pages = {102453},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102453},
url = {https://www.sciencedirect.com/science/article/pii/S157495412300482X},
author = {Sajid Nazir and Mohammad Kaleem},
keywords = {Data science, Computer vision, Deep learning, Model generalization, Fine tuning, Explainable AI, Hyperparameter tuning, Vision transformers},
abstract = {The camera traps have revolutionized the image and video capture in ecology and are often used to monitor and record animal presence. With miniaturization of low power electronic devices, better battery technologies, and software advancements, it has become possible to use the edge devices, such as Raspberry Pi as camera traps that can not only capture images and videos, but can also enable sophisticated image processing, and off-site communications. These developments can help to provide near real-time insights and reduce the manual processing of images. The on-board image classification and visualization is facilitated by the advancements in the Deep Neural Networks (DNN), transfer learning approaches, and software libraries. This paper provides an investigation of image classification with transfer learning approaches using pre-trained DNN models, and visualizations with Explainable Artificial Intelligence (XAI) techniques on Raspberry Pi Zero (RPi-Z) edge device. The MobileNetV2 model was used for image classification on the Florida-Part1 dataset obtaining the results for precision, recall, and F1-score as 0.95, 0.96, and 0.95 respectively. We also compared the model performance of MobileNetV2, EfficientNetV2B0, and MobileViT models for classification on the Extinction dataset with the best results for precision, recall, and F1-score as 0.97, 0.96, and 0.96 respectively, obtained with the EfficientNetV2B0 model. Two XAI techniques, Gradient-weighted-Class Activation Mapping (Grad-CAM) and Occlusion Sensitivity were used for visualization through heatmaps, to highlight the relative importance of the image areas contributing to the DNN model's prediction, that can also help to understand the model's performance and bias. The results provide practical use case scenarios for utilizing the transfer learning approaches, model optimization and deployment to edge devices, and model visualizations in ecological research.}
}
@article{SIGURDARDOTTIR2023102046,
title = {Otolith age determination with a simple computer vision based few-shot learning method},
journal = {Ecological Informatics},
volume = {76},
pages = {102046},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102046},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000754},
author = {Andrea Rakel Sigurðardóttir and Þór Sverrisson and Aðalbjörg Jónsdóttir and María Gudjónsdóttir and Bjarki Þór Elvarsson and Hafsteinn Einarsson},
keywords = {Otoliths, Fish age estimation, Few-shot learning, Deep-learning, Image analysis},
abstract = {In this study, we propose a computer vision-based few-shot learning method for otolith age determination in European plaice, Atlantic cod, Greenland halibut, and haddock. Our method outperforms prior state-of-the-art approaches, and is based on a vision encoder from CLIP as a feature extractor, which is used to train shallow models. The method is computationally efficient, as it does not require fine-tuning of deep networks, and is also data efficient, as it performs better than fine-tuning on the same data. Our results suggest that in some cases, our method can achieve the same performance as state-of-the-art finetuning approaches with up to three times less training data.}
}
@article{WANG2024102538,
title = {Hierarchical-taxonomy-aware and attentional convolutional neural networks for acoustic identification of bird species: A phylogenetic perspective},
journal = {Ecological Informatics},
volume = {80},
pages = {102538},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102538},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000803},
author = {Qingyu Wang and Yanzhi Song and Yeqian Du and Zhouwang Yang and Peng Cui and Binnan Luo},
keywords = {Bioacoustics, Bird sound recognition, Hierarchical architecture, Attention module, Path correction},
abstract = {The study of bird populations is crucial for biodiversity research and conservation. Deep artificial neural networks have revolutionized bird acoustic recognition; however, most methods overlook inherent relationships among bird populations, resulting in the loss of biological information. To address this limitation, we propose the Phylogenetic Perspective Neural Network (PPNN), which incorporates hierarchical multilevel labels for each bird. PPNN uses a hierarchical semantic embedding framework to capture feature information at different levels. Attention mechanisms are employed to extract and select common and distinguishing features, thereby improving classification accuracy. We also propose a path correction strategy to rectify inconsistent predictions. Experimental results on bird acoustic datasets demonstrate that PPNN outperforms current methods, achieving classification accuracies of 90.450%, 91.883%, and 89.950% on the Lishui-Zhejiang Birdsdata (100 species), BirdCLEF2018-Small (150 species), and BirdCLEF2018-Large (500 species) datasets respectively, with the lowest hierarchical distance of a mistake across all datasets. Our proposed method is applicable to any bird acoustic dataset and presents significant advantages as the number of categories increases.}
}
@article{KUMAR2024102510,
title = {Bird species recognition using transfer learning with a hybrid hyperparameter optimization scheme (HHOS)},
journal = {Ecological Informatics},
volume = {80},
pages = {102510},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102510},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000529},
author = {Samparthi V.S. Kumar and Hari Kishan Kondaveeti},
keywords = {Automatic bird species recognition, Convolutional neural network, Deep learning, Hyperparameter tuning, Transfer learning},
abstract = {The use of automatic bird species recognition methods reduces the burden on scientists, ornithologists, and bird watchers as these methods help identify birds with minimal human effort and intervention. The current study employs a transfer learning approach combined with a Hybrid hyperparameter Optimization Scheme (HHOS) to enhance the efficiency and accuracy of automatic bird species recognition. First, the weights of selected pre-trained deep learning models are downloaded from ImageNet, and a few new trainable layers are added at the top. Thereafter, the selected models are trained using HHOS, which strategically integrates both manual and random searches to achieve favorable results. The manual search relies on domain knowledge and experience to identify the best hyperparameter settings, thereby making the search space smaller and more focused. Random search tests various combinations of the hyperparameters identified in manual search and trains the selected models to achieve the maximum possible accuracy through multiple iterations. Experimental analysis revealed that the Fine-tuned EfficientNetB0 model exhibited superior performance, achieving an accuracy of 99.12%. In contrast, the performance of the ResNet18 model was disappointing with an accuracy of 93.24%, while other models outperformed it.}
}
@article{GAME2024102619,
title = {Machine learning for non-experts: A more accessible and simpler approach to automatic benthic habitat classification},
journal = {Ecological Informatics},
volume = {81},
pages = {102619},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102619},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001614},
author = {Chloe A. Game and Michael B. Thompson and Graham D. Finlayson},
keywords = {Machine learning, Computer vision, Convolutional neural network, Support vector machine, User-friendly, Benthic habitats},
abstract = {Automating identification of benthic habitats from imagery, with Machine Learning (ML), is necessary to contribute efficiently and effectively to marine spatial planning. A promising method is to adapt pre-trained general convolutional neural networks (CNNs) to a new classification task (transfer learning). However, this is often inaccessible to a non-specialist, requiring large investments in computational resources and time (for user comprehension and model training). In this paper, we demonstrate a simpler transfer learning framework for classifying broad deep-sea benthic habitats. Specifically, we take an ‘off-the-shelf’ CNN (VGG16) and use it to extract features (pixel patterns) from benthic images (without further training). The default outputs of VGG16 are then fed in to a Support Vector Machine (SVM), a classical and simpler method than deep networks. For comparison, we also train the remaining classification layers of VGG16 using stochastic gradient descent. The discriminative power of these approaches is demonstrated on three benthic datasets (574–8353 images) from Norwegian waters; each using a unique imaging platform. Benthic habitats are broadly classified as Soft Substrate (sands, muds), Hard Substrate (gravels, cobbles and boulders) and Reef (Desmophyllum pertusum). We found that the relatively simplicity of the SVM classifier did not compromise performance. Results were competitive with the CNN classifier and consistently high, with test accuracy ranging from 0.87 to 0.95 (average = 0.9 (±0.04)) across datasets, somewhat increasing with dataset size. Impressively, these results were achieved 2.4–5× faster than CNN training and had significantly less dependency on high-specification hardware. Our suggested approach maximises conceptual and practical simplicity, representing a realistic baseline for novice users when approaching benthic habitat classification. This method has wide potential. It allows automated image grouping to aid annotation or further model selection, as well as screening of old-datasets. It is especially suited to offshore scenarios as it can provide quick, albeit crude, insights into habitat presence, allowing adaptation of sampling protocols in near real-time.}
}
@article{DENG2024102546,
title = {Weed database development: An updated survey of public weed datasets and cross-season weed detection adaptation},
journal = {Ecological Informatics},
volume = {81},
pages = {102546},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102546},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000888},
author = {Boyang Deng and Yuzhen Lu and Jiajun Xu},
keywords = {Deep learning, Domain adaptation, Machine vision, Precision agriculture, Robustness, Weed detection},
abstract = {Weeds are a major threat to crop production. Automated innovations for reducing herbicides and labor needed for weeding have become a high priority for sustainable weed management. The current state-of-the-art weeding systems still cannot reliably recognize weeds in changing field conditions for precision weed control. Enhancing weed recognition accentuates the critical need to develop dedicated, labeled weed databases and whereby train advanced AI (artificial intelligence) models while ensuring the robustness of models across diverse field conditions. This study presents an up-to-date survey on publicly available image datasets for weed recognition. Among 36 datasets identified, limitations exist in terms of data variations and distribution shifts, and few of the datasets are suitable for examining the robustness of weed recognition models. A new two-season, eight-class weed dataset is described in this study, comprising two sub-datasets of images collected in the seasons of 2021 and 2022, respectively. Three state-of-the-art deep learning object detectors, i.e., YOLOX, YOLOv8, and DINO, were benchmarked and evaluated for their in-season and cross-season weed detection performance on the dataset. All three models attained in-season detection accuracies of 92% and higher in terms of mAP@50. However substantial accuracy drops of up to 14.5% were observed between in-season and cross-season testing, especially for YOLOX and YOLOv8. Unsupervised domain adaptation based on an implicit instance-invariant network (I3Net) was utilized for improved generalization of the YOLO models. The I3Net-based models resulted in accuracy improvements of 1.4% and 3.3% for YOLOX and YOLOv8, respectively, compared to modeling without domain adaptation, in the cross-season testing. Both the two-season detection dataset11https://doi.org/10.5281/zenodo.10762138 and software programs22https://github.com/vicdxxx/CrossSeasonWeedDetection for weed detection modeling in this study are made publicly available.}
}
@article{BRAVODIAZ2024102684,
title = {Evaluating the ability of convolutional neural networks for transfer learning in Pinus radiata cover predictions},
journal = {Ecological Informatics},
volume = {82},
pages = {102684},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102684},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002267},
author = {A. Bravo-Diaz and S. Moreno and J. Lopatin},
keywords = {Invasive species, Unpiloted aerial vehicles (UAV), Spatial variability, Regression, Transfer domain},
abstract = {The species Pinus radiata is highly invasive in native forests in Chile, drastically affecting the functioning and structure of ecosystems. Hence, it is imperative to develop robust approaches to detect P. radiata invasions at different scales. Models based on convolutional neural networks (CNN) have proven to be a promising alternative to detect plant invasions in high-resolution remote sensing data, such as those obtained by drones. However, studies have been limited in their spatial variability and their assessments of transferability or transfer learning to new sectors, hindering the ability to use these models in a real-world setting. We train models based on CNN architectures using unpiloted aerial vehicle data and evaluate their ability to transfer learning outside the training domain using regression approaches. We compared models trained with low spatial variability (mono-site) with those with high spatial variability (multi-site). We further sought to maximize the transference of learning outside the training domain by searching among different architectures and models, maximizing the evaluation in an independent data set. The results showed that transfer learning is better when multi-site models with higher spatial variability are used for training, obtaining a coefficient of determination R2 between 60% and 87%. On the contrary, mono-site models present a wide variability of performance attributed to the dissimilarity of information between sites, limiting the possibilities of using these models for extrapolations or model generalizations. We also obtained a significant difference between within-domain generalization using test data versus transfer learning outside the training domain, showing that testing data alone cannot depict such discrepancy without further data. Finally, the best models for transfer learning on new data domains often do not agree with those selected by the standard training/validation/testing scheme. Our findings pave the way for deeper discussions and further investigations into the limitations of CNN models when applied to high-resolution imagery.}
}
@article{HASAN2023102361,
title = {Image patch-based deep learning approach for crop and weed recognition},
journal = {Ecological Informatics},
volume = {78},
pages = {102361},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102361},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003904},
author = {A S M Mahmudul Hasan and Dean Diepeveen and Hamid Laga and Michael G.K. Jones and Ferdous Sohel},
keywords = {Weed classification, Deep learning, Patch-based technique, Digital agriculture},
abstract = {Accurate classification of weed species in crop plants plays a crucial role in precision agriculture by enabling targeted treatment. Recent studies show that artificial intelligence deep learning (DL) models achieve promising solutions. However, several challenging issues, such as lack of adequate training data, inter-class similarity between weed species and intra-class dissimilarity between the images of the same weed species at different growth stages or for other reasons (e.g., variations in lighting conditions, image capturing mechanism, agricultural field environments) limit their performance. In this research, we propose an image based weed classification pipeline where a patch of the image is considered at a time to improve the performance. We first enhance the images using generative adversarial networks. The enhanced images are divided into overlapping patches, a subset of which are used for training the DL models. For selecting the most informative patches, we use the variance of Laplacian and the mean frequency of Fast Fourier Transforms. At test time, the model's outputs are fused using a weighted majority voting technique to infer the class label of an image. The proposed pipeline was evaluated using 10 state-of-the-art DL models on four publicly available crop weed datasets: DeepWeeds, Cotton weed, Corn weed, and Cotton Tomato weed. Our pipeline achieved significant performance improvements on all four datasets. DenseNet201 achieved the top performance with F1 scores of 98.49%, 99.83% and 100% on Deepweeds, Corn weed and Cotton Tomato weed datasets, respectively. The highest F1 score on the Cotton weed dataset was 98.96%, obtained by InceptionResNetV2. Moreover, the proposed pipeline addressed the issues of intra-class dissimilarity and inter-class similarity in the DeepWeeds dataset and more accurately classified the minority weed classes in the Cotton weed dataset. This performance indicates that the proposed pipeline can be used in farming applications.}
}
@article{SONG2024102466,
title = {Benchmarking wild bird detection in complex forest scenes},
journal = {Ecological Informatics},
volume = {80},
pages = {102466},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102466},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000086},
author = {Qi Song and Yu Guan and Xi Guo and Xinhui Guo and Yufeng Chen and Hongfang Wang and Jianping Ge and Tianming Wang and Lei Bao},
keywords = {Object detection, Bird detection, Deep learning, Camera trap},
abstract = {Camera traps are widely used for wildlife monitoring and making informed conservation and land-management decisions, but the resulting ‘big data’ are laborious to process. Deep learning-based methods have been adopted for wildlife detection in camera traps. However, these methods detect large mammals in uncomplicated scenes, where powerful deep-learning models work effectively. Few studies have been conducted to develop artificial intelligence for recognizing wild birds that live in complicated field scenes with protective colors and small sizes. Here we used a dataset of 9717 images from 15 bird species based on camera traps to test 8 object detection algorithms (Faster RCNN, Cascade RCNN, RetinaNet, FCOS, RepPoints, ATSS, Deformable-DETR, and Sparse RCNN) and assess their performance. We also explored the effect of different backbones on model accuracy. Among them, the Cascade RCNN model performs best, with a mAP of 0.693 in model capabilities. Models perform differently in certain species, and backbones significantly affect the accuracy of the model. Cascade RCNN utilizing the Swin-T backbone is the best-performing combination, with a mAP of 0.704. This study could help researchers identify birds efficiently and inspires research on wildlife recognition in complex ecological settings.}
}
@article{CHEN2024102660,
title = {Detecting sun glint in UAV RGB images at different times using a deep learning algorithm},
journal = {Ecological Informatics},
volume = {81},
pages = {102660},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102660},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002024},
author = {Jiahao Chen and Yi Xiao and Yahui Guo and Mingwei Li and Xiran Li and Xuan Zhang and Fanghua Hao and Xiao Pu and Yongshuo Fu},
keywords = {Unmanned aerial vehicle, Water quality monitoring, Sun glint, Convolutional neural network, Res_AUNet network, WSGD dataset},
abstract = {Unmanned aerial vehicle (UAV) remote sensing has played a crucial role in water quality monitoring. However, the presence of water sun glint, resulting from specular reflection on the water surface, poses an inevitable challenge in UAV-acquired images. These excessively bright pixels disrupt the original images' spectral and textural characteristics, significantly diminishing their usability. This disruption has repercussions on subsequent tasks, such as target object classification and water quality parameter inversion. Precise detection of sun glint is a prerequisite for removing them, but current methods suffer from missed and false detections. In this study, we collected images by UAV to construct a specialized dataset for water surface sun glint, namely water sun glint detection (WSGD) dataset, laying the groundwork for further research endeavors. We proposed the Res_AUNet network by enhancing the UNet convolutional neural network. The Convolutional Block Attention Module was integrated into the encoding-decoding skip connections of the network, we also refined the convolutional blocks to better capture the distinctive semantic features associated with water sun glint. To mitigate overfitting, the residual structures were incorporated and the number of convolutional kernels within each block was reduced. The Res_AUNet network was trained and evaluated using the WSGD dataset, achieving metrics with an Accuracy of 98.02%, an F1-score of 83.67%, and an IOU of 74.73%. These results underscore the precision of our proposed method for water sun glint detection in UAV water images, offering valuable insights for effectively eliminating water sun glint and determining the optimal timing for UAV water image acquisition.}
}
@article{HO2023102289,
title = {LaDeco: A tool to analyze visual landscape elements},
journal = {Ecological Informatics},
volume = {78},
pages = {102289},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102289},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003187},
author = {Li-Chih Ho},
keywords = {Landscape assessment, Deep learning, Natural feature index, Semantic segmentation, Visual landscape elements},
abstract = {The assessment of visual landscape elements plays a crucial role in landscape change studies, aesthetic evaluation, and visual impact assessment. The proportions and statistical distributions of these elements are key factors that significantly influence these domains. Historically, the analysis has been performed manually, a process that is both labor intensive and time consuming, particularly when dealing with large assessment regions. To address this limitation, this study employs cutting-edge artificial intelligence technology to introduce an automated tool called LaDeco (Landscape Decoder). This tool enables researchers, planners, and evaluators to rapidly and objectively calculate the proportions of visual elements in images, thereby streamlining the assessment process.}
}
@article{CAMERON2022101658,
title = {Estimating boreal forest ground cover vegetation composition from nadir photographs using deep convolutional neural networks},
journal = {Ecological Informatics},
volume = {69},
pages = {101658},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101658},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122001078},
author = {Hilary A. Cameron and Pranoy Panda and Martin Barczyk and Jennifer L. Beverly},
abstract = {Ground cover and surface vegetation information are key inputs to wildfire propagation models and are important indicators of ecosystem health. Often these variables are approximated using visual estimation by trained professionals but the results are prone to bias and error. This study analyzed the viability of using nadir or downward photos from smartphones (iPhone 7) to provide quantitative ground cover and biomass loading estimates. Good correlations were found between field measured values and pixel counts from manually segmented photos delineating a pre-defined set of 10 discrete cover types. Although promising, segmenting photos manually was labor intensive and therefore costly. We explored the viability of using a trained deep convolutional neural network (DCNN) to perform image segmentation automatically. The DCNN was able to segment nadir images with 95% accuracy when compared with manually delineated photos. To validate the flexibility and robustness of the automated image segmentation algorithm, we applied it to an independent dataset of nadir photographs captured at a different study site with similar surface vegetation characteristics to the training site with promising results.}
}
@article{HARTMANN2022101782,
title = {A text and image analysis workflow using citizen science data to extract relevant social media records: Combining red kite observations from Flickr, eBird and iNaturalist},
journal = {Ecological Informatics},
volume = {71},
pages = {101782},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101782},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002321},
author = {Maximilian C. Hartmann and Moritz Schott and Alishiba Dsouza and Yannick Metz and Michele Volpi and Ross S. Purves},
keywords = {User-generated content, Volunteered geographic information, Data integration, Image content analysis, Convolutional neural networks},
abstract = {There is an urgent need to develop new methods to monitor the state of the environment. One potential approach is to use new data sources, such as User-Generated Content, to augment existing approaches. However, to date, studies typically focus on a single date source and modality. We take a new approach, using citizen science records recording sightings of red kites (Milvus milvus) to train and validate a Convolutional Neural Network (CNN) capable of identifying images containing red kites. This CNN is integrated in a sequential workflow which also uses an off-the-shelf bird classifier and text metadata to retrieve observations of red kites in the Chilterns, England. Our workflow reduces an initial set of more than 600,000 images to just 3065 candidate images. Manual inspection of these images shows that our approach has a precision of 0.658. A workflow using only text identifies 14% less images than that including image content analysis, and by combining image and text classifiers we achieve almost perfect precision of 0.992. Images retrieved from social media records complement those recorded by citizen scientists spatially and temporally, and our workflow is sufficiently generic that it can easily be transferred to other species.}
}
@article{BHAGABATI2024102398,
title = {An automated approach for human-animal conflict minimisation in Assam and protection of wildlife around the Kaziranga National Park using YOLO and SENet Attention Framework},
journal = {Ecological Informatics},
volume = {79},
pages = {102398},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102398},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004272},
author = {Bijuphukan Bhagabati and Kandarpa Kumar Sarma and Kanak Chandra Bora},
keywords = {Computer vision, Object detection, Animal detection, Human-animal conflict, Kaziranga, Deep learning, Yolo},
abstract = {Human-animal conflict in Assam, India's north-eastern state, is rising continuously. Because it occurs year-round, it damages agricultural productivity and kills people and animals, including elephants. When a herd of wild elephants emerges from a deep forest and enters human-inhabited territory around the Kaziranga National Park (KNP) in Assam, an alert must be sounded for the neighbourhood residents and forest workers to prevent conflicts. Another concern is that many wild animals die near the KNP while crossing the national highway NH-37 which traverses the area. During floods, animals flee to the highlands for food and shelter. An automated animal identification and warning system near the KNP may reduce human-animal confrontations. This paper reports the design of a system that attempts to address the above concerns. Artificial Intelligence (AI)-based strategies are utilized to recognize wild animals from live video sequences, provide warnings to avoid encounters, and protect humans and animals. Deep learning models and YoloV5 with the SENet attention layer are used to recognize wild animals in real-time. This model is trained using a public and customized dataset of animal species. Cameras attached to the cloud-based AI system take photographs from several KNP locations to confirm the model. The model's 96% accuracy in animal photographs and videos taken day and night and in feed from contemporaneous location has shown its utility. The model also improves reliability by 1–13% over previous methods.}
}
@article{WU2022101534,
title = {SILIC: A cross database framework for automatically extracting robust biodiversity information from soundscape recordings based on object detection and a tiny training dataset},
journal = {Ecological Informatics},
volume = {68},
pages = {101534},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101534},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121003253},
author = {Shih-Hung Wu and Hsueh-Wen Chang and Ruey-Shing Lin and Mao-Ning Tuanmu},
keywords = {Sound Identification and Labeling Intelligence for Creatures, Automated wildlife sound identification, Passive acoustic monitoring, Autonomous recording unit, Object detection}
}
@article{SOOM2022101817,
title = {Environmentally adaptive fish or no-fish classification for river video fish counters using high-performance desktop and embedded hardware},
journal = {Ecological Informatics},
volume = {72},
pages = {101817},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101817},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002679},
author = {Jürgen Soom and Vishwajeet Pattanaik and Mairo Leier and Jeffrey A. Tuhtan},
keywords = {Fish detection, Deep learning, Underwater video, Environmental classification, Embedded hardware},
abstract = {Automated fish counters featuring robust, real-time computer vision capabilities can provide a cost-effective means to count migrating freshwater fish. In this work, we propose a four-stage process for automatically sorting videos with and without fish. Underwater fish counter videos provide a challenging range of environmental conditions including clear water, biofilm growth, bubbles, turbidity, low light and overexposure. To address this, our method also includes the automated classification of these six environmental conditions. The proposed methods are computationally efficient and can be implemented on servers, high-performance desktop computers and low-cost, energy-efficient embedded hardware. The models were trained, tested, and validated using a collection of 3000 videos taken from underwater fish counter installations in several alpine and lowland European rivers provided by commercial and governmental collaborators. This work demonstrates a fast, accurate, and robust computer vision workflow for large-scale automated freshwater fish counting systems.}
}
@article{MENG2024102549,
title = {Classification of inland lake water quality levels based on Sentinel-2 images using convolutional neural networks and spatiotemporal variation and driving factors of algal bloom},
journal = {Ecological Informatics},
volume = {80},
pages = {102549},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102549},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124000918},
author = {Haobin Meng and Jing Zhang and Zhen Zheng and Yongyu Song and Yuequn Lai},
keywords = {Convolutional neural networks, Sentinel-2, Water quality level classification, Algal blooms, Driving factor},
abstract = {Water quality monitoring in inland lakes is crucial to ensuring the health and stability of aquatic ecosystems. For regional water environment agencies and researchers, remote sensing offers a cost-effective alternative to traditional in-situ water sampling methods. In this study, we designed a convolutional neural network (CNN) based on AlexNet to represent the relationship between Sentinel-2 images and in situ water quality levels of Lake Dianchi from November 2020 to April 2023. The model incorporated an algal bloom extraction algorithm and utilized correlation analysis, redundancy analysis (RDA), and random forest (RF) method to establish connections between two environmental factors: water quality and meteorology, to the area of algal bloom (AAB). The findings revealed an improvement in Lake Dianchi's water quality, with Levels A (good water quality) and B (mildly polluted water quality) averaging 1.24% and 84.28%, respectively. Starting in October 2022, water quality stabilized at Level B, averaging at 98.17%. Seasonal variations demonstrated the best water quality in spring and the worst in summer (Level C, severely polluted water quality, accounting for 5.19% and 21.68%, respectively). Algal bloom presence was minimally observed, with an average AAB value of 1.75%, peaking in autumn (4.05%) and hitting a low in winter (0.38%). A significant correlation was identified between water quality levels and AAB, with a notable spatial trend of decreasing Level C water quality and AAB from north to south, featuring lower AAB in the Southern Waihai compared to the Central Waihai. Statistical analysis pinpointed total phosphorus (TP) as the dominant factor influencing AAB, while meteorological factors such as wind speed (WS), relative humidity (RH), and precipitation (PP) playing secondary roles. Despite fluctuations in TP concentration, a recent stabilization at 0.05 mg/L suggests a positive trajectory for future algal bloom management in Lake Dianchi.}
}
@article{NOMAN2023102047,
title = {Improving accuracy and efficiency in seagrass detection using state-of-the-art AI techniques},
journal = {Ecological Informatics},
volume = {76},
pages = {102047},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102047},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000766},
author = {Md Kislu Noman and Syed Mohammed Shamsul Islam and Jumana Abu-Khalaf and Seyed Mohammad Jafar Jalali and Paul Lavery},
keywords = {Deep learning, EfficientDet, Faster R-CNN, , NASNet, Seagrass, YOLOv5},
abstract = {Seagrasses provide a wide range of ecosystem services in coastal marine environments. Despite their ecological and economic importance, these species are declining because of human impact. This decline has driven the need for monitoring and mapping to estimate the overall health and dynamics of seagrasses in coastal environments, often based on underwater images. However, seagrass detection from underwater digital images is not a trivial task; it requires taxonomic expertise and is time-consuming and expensive. Recently automatic approaches based on deep learning have revolutionised object detection performance in many computer vision applications, and there has been interest in applying this to automated seagrass detection from imagery. Deep learning–based techniques reduce the need for hardcore feature extraction by domain experts which is required in machine learning-based techniques. This study presents a YOLOv5-based one-stage detector and an EfficientDetD7–based two-stage detector for detecting seagrass, in this case, Halophila ovalis, one of the most widely distributed seagrass species. The EfficientDet-D7–based seagrass detector achieves the highest mAP of 0.484 on the ECUHO-2 dataset and mAP of 0.354 on the ECUHO-1 dataset, which are about 7% and 5% better than the state-of-the-art Halophila ovalis detection performance on those datasets, respectively. The proposed YOLOv5-based detector achieves an average inference time of 0.077 s and 0.043 s respectively which are much lower than the state-of-the-art approach on the same datasets.}
}
@article{OLIVARESPINTO2024102653,
title = {Using honey bee flight activity data and a deep learning model as a toxicovigilance tool},
journal = {Ecological Informatics},
volume = {81},
pages = {102653},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102653},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400195X},
author = {Ulises Olivares-Pinto and Cédric Alaux and Yves {Le Conte} and Didier Crauser and Alberto Prado},
keywords = {Honeybee flight activity, Toxicovigilance, Neurotoxic pesticides, Recurrent neural network},
abstract = {Automatic monitoring devices placed at the entrances of honey bee hives have facilitated the detection of various sublethal effects related to pesticide exposure, such as homing failure and reduced flight activity. These devices have further demonstrated that different neurotoxic pesticide molecules produce similar sublethal impacts on flight activity. The detection of these effects was conducted a posteriori, following the recording of flight activity data. This study introduces a method using an artificial intelligence model, specifically a recurrent neural network, to detect the sublethal effects of pesticides in real-time based on honey bee flight activity. This model was trained on a flight activity dataset comprising 42,092 flight records from 1107 control and 1689 pesticide-exposed bees. The model was able to classify honey bees as healthy or pesticide-exposed based on the number of flights and minutes spent foraging per day. The model was the least accurate (68.46%) when only five days of records per bee were used for training. However, the highest classification accuracy of 99%, a Cohen Kappa of 0.9766, a precision of 0.99, a recall of 0.99, and an F1-score of 0.99 was achieved with the model trained on 25 days of activity data, signifying near-perfect classification ability. These results underscore the highly predictive performance of AI models for toxicovigilance and highlight the potential of our approach for real-time and cost-effective monitoring of risks due to exposure to neurotoxic pesticide in honey bee populations.}
}
@article{KORSCHENS2024102516,
title = {Determining the community composition of herbaceous species from images using convolutional neural networks},
journal = {Ecological Informatics},
volume = {80},
pages = {102516},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102516},
url = {https://www.sciencedirect.com/science/article/pii/S157495412400058X},
author = {Matthias Körschens and Solveig Franziska Bucher and Paul Bodesheim and Josephine Ulrich and Joachim Denzler and Christine Römermann},
keywords = {Plant biodiversity, Plant cover, Deep learning, Convolutional neural networks, Semantic segmentation, Artificial intelligence},
abstract = {Global change has a detrimental impact on the environment and changes biodiversity patterns, which can be observed, among others, via analyzing changes in the composition of plant communities. Typically, vegetation relevées are done manually, which is time-consuming, laborious, and subjective. Applying an automatic system for such an analysis that can also identify co-occurring species would be beneficial as it is fast, effortless to use, and consistent. Here, we introduce such a system based on Convolutional Neural Networks for automatically predicting the species-wise plant cover. The system is trained on freely available image data of herbaceous plant species from web sources and plant cover estimates done by experts. With a novel extension of our original approach, the system can even be applied directly to vegetation images without requiring such cover estimates. Our extended approach, not utilizing dedicated training data, performs similarly to humans concerning the relative species abundances in the vegetation relevées. When trained on dedicated training annotations, it reflects the original estimates more closely than (independent) human experts, who manually analyzed the same sites. Our method is, with little adaptation, usable in novel domains and could be used to analyze plant community dynamics and responses of different plant species to environmental changes.}
}
@article{JAMEI2024102455,
title = {Quantitative improvement of streamflow forecasting accuracy in the Atlantic zones of Canada based on hydro-meteorological signals: A multi-level advanced intelligent expert framework},
journal = {Ecological Informatics},
volume = {80},
pages = {102455},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102455},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004843},
author = {Mozhdeh Jamei and Mehdi Jamei and Mumtaz Ali and Masoud Karbasi and Aitazaz A. Farooque and Anurag Malik and Saad Javed Cheema and Travis J. Esau and Zaher Mundher Yaseen},
keywords = {Streamflow forecasting, Hydro-meteorological drivers, Multivariate variational mode decomposition, CNN-BiGRU, Boruta-CART, Multi-temporal},
abstract = {Developing reliable streamflow forecasting models is critical for hydrological tasks such as improving water resource management, analyzing river patterns, and flood forecasting. In this research, for the first time, an emerging multi-level TOPSIS (technique for order preference by similarity to the ideal solution) -based hybridization comprised of the Boruta classification and regression tree (Boruta-CART) feature selection, multivariate variational mode decomposition (MVMD), and a hybrid Convolutional Neural Network (CNN) Bidirectional Gated Recurrent Unit (CNN-BiGRU) deep learning was adopted to multi-temporal (one and three days ahead) forecast the daily streamflow in the Rivers of Prince Edward Island, Canada. For this aim, in the first step, the Boruta-CART feature selection technique determines the most effective lagged components among all the antecedent two-day information (i.e., t-1 and t-2) of hydro-meteorological features (from 2015 to 2020), including the water level, mean air temperature, heat degree days, total precipitation, dew point temperature, and relative humidity in the Bear and Winter Rivers of Prince Edward Island, Canada. Afterwards, a multivariate variational mode decomposition (MVMD) decomposes the input time series to decrease the complexity and non-linearity of the non-stationary ones before feeding the deep learning (DL) models. Here, the CNN-GRU was employed as the primary DL model, along with the kernel extreme machine method (KELM), random variational function link (RVFL), and hybrid CNN bidirectional recurrent neural network (CNN-BiRNN) as the comparative models. A TOPSIS scheme applying several performance measures like the correlation coefficient (R), root mean square error (RMSE), and reliability was designed for the robustness assessment of the hybrid (MVM-CNN-BiGRU, MVM-CNN-BiRNN, MVM-RVFL, and MVM-KELM) and standalone models. The computational outcomes revealed that in the Bear River, the MVM-CNN-BiGRU, owing to its best forecasting performance (one day ahead: TOPSIS score 1, R = 0.960, RMSE = 0.098, and reliability = 65.082; three days ahead: TOPSIS score = 0.999, R = 0.924, and RMSE = 0.33) outperformed the other hybrid models, followed by the MVM-CNN-BiRNN, MVM-RVFL, and MVM-KELM, respectively. Moreover, in the Winter River, the MVM-CNN-BiGRU in terms of (one-day ahead: TOPSIS score = 0.890, R = 0.955, RMSE = 0.274, and reliability = 34.004; three-days ahead: TOPSIS score = 0.686, R = 0.924, and RMSE = 0.330) was superior to the other models. The provided expert system could be vital in the local flood decision-making process, in the absence of streamflow information as input modeling, during the flood seasons to reduce flood damage in residential areas.}
}
@article{NAPPA2024102723,
title = {Probabilistic Bayesian Neural Networks for olive phenology prediction in precision agriculture},
journal = {Ecological Informatics},
volume = {82},
pages = {102723},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102723},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002656},
author = {A. Nappa and M. Quartulli and I. Azpiroz and S. Marchi and D. Guidotti and M. Staiano and R. Siciliano},
keywords = {Olive phenology, Modeling neural networks, Bayesian inference, Monte Carlo Dropout, Precision agriculture},
abstract = {Plant phenology is the study of cyclical events in a plant life cycle such as leaf bud burst, flowering, and fruiting. In this article the problem of olive phenology prediction is addressed through the use of Deep Learning. Although Neural Networks have already been used in this area, to the best of our knowledge, this is the first implementation of Probabilistic Bayesian Neural Networks for olive phenology prediction. This architecture gives particular emphasis to estimating the model uncertainty, both aleatoric and epistemic. The Bayesian Inference method, more precisely the Variational Inference one, is compared with the Monte Carlo Dropout technique, which is known to be a less computationally intensive approximation of Variational Inference. For validation purposes, models performance is compared to the state-of-the-art results.}
}
@article{DUBUS2024102642,
title = {From citizen science to AI models: Advancing cetacean vocalization automatic detection through multi-annotator campaigns},
journal = {Ecological Informatics},
volume = {81},
pages = {102642},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102642},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001845},
author = {Gabriel Dubus and Dorian Cazau and Maëlle Torterotot and Anatole Gros-Martial and Paul {Nguyen Hong Duc} and Olivier Adam},
keywords = {Marine bioacoustics, Passive acoustic monitoring, Citizen science, Multi-annotation, Deep learning for automatic detection, Convolutional neural networks, Soft labeling},
abstract = {Continuous underwater Passive Acoustic Monitoring (PAM) has emerged as a strong tool for cetacean research. To handle the vast volume of collected data, it is essential to employ automated detection and classification methods. The recent advancement of deep learning, involving model training and testing, requires a large amount of labeled data. These labels are derived through the manual annotation of audio files often reliant on human experts. Based on an annotation campaign focusing on blue whale calls in the Indian Ocean involving 19 novice annotators and one expert in bioacoustics, this study explores the integration of novice annotators in marine bioacoustics research, through citizen science programs, which could drastically increase the size of labeled datasets and enhance the performance of detection and classification models. The analysis reveals distinctive annotation profiles influenced by the complexity of vocalizations and the annotators' strategies, ranging from conservative to permissive. To address the challenges of annotation discrepancies, Convolutional Neural Networks (CNNs) are trained on annotations from both novices and the expert. The results show variations in model performance. Our work highlights the importance of annotation guidelines encouraging a more conservative approach to improve overall annotation quality. In an effort to optimize the potential of multi-annotation and mitigate the presence of noisy labels, two annotation aggregation methods (majority voting and soft labeling) are proposed and tested. The results demonstrate that both methods, particularly when a sufficient number of annotators are involved, significantly improve model performance and reduce variability: the standard deviation of the area under PR and ROC curves fall under 0.02 for both vocalizations with 13 aggregated annotators, while it was at 0.17 and 0.21 for the Blue Whale Dcalls and 0.05 and 0.04 for the SEIO PBW vocalizations with all annotators separately. Moreover, these aggregation methods enable the training of models using non-expert annotations that achieve performance of models trained with expert annotations. These findings suggest that crowdsourced annotations from novice annotators can be a viable alternative to expert annotations.}
}
@article{ALI2024102618,
title = {An ensemble of deep learning architectures for accurate plant disease classification},
journal = {Ecological Informatics},
volume = {81},
pages = {102618},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102618},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001602},
author = {Ali Hussein Ali and Ayman Youssef and Mahmoud Abdelal and Muhammad Adil Raja},
keywords = {Plant leaf disease, Agriculture, Ensemble deep learning models},
abstract = {A substantial fraction of agricultural produce loss can be attributed to plant diseases. Agricultural yield loss can have far-reaching consequences for a country's economy and contribute to global food insecurity. Early detection of plant diseases can be instrumental in maintaining global health and welfare. A pathologist's visual evaluation is typically used to make an early diagnosis of plant diseases. This technique involves experts or farmers examining plants with the naked eye and classifying the disease depending on their previous experience. This conventional approach includes drawbacks like low accuracy and the need for human expertise. This motivates researchers to investigate automated systems for the early diagnosis of plant diseases. To achieve this goal an ensemble of different deep learning architectures (DenseNet201, efficientNetB0, inceptionresnetV2, efficientNetB3) is introduced to increase the classification accuracy of plant leaf diseases. In this work, a novel image-processing technique is proposed to increase the efficiency of deep-learning models. Also, a data balancing technique is used to solve the problem of the imbalanced dataset. Five different deep-learning models are trained and tested using the largest plant disease dataset; PlantVillage. Ten different ensembles (chosen randomly) of the deep learning models are tested and compared to find the ensemble with the highest accuracy. The proposed ensemble model was able to achieve 99.89% accuracy on the New PlantVillage dataset. PlantVillage is a challenging dataset with 38 classes. Achieving high accuracies on such a dataset proves the ability of the system to generalize on unseen data or real-world scenarios. A comparison with the state-of-the-art is made with other available models from the literature. A section about this is added to show the superior performance of the proposed ensemble model in terms of accuracy and F1-score.}
}
@article{KUMAR2024102699,
title = {Improving learning-based birdsong classification by utilizing combined audio augmentation strategies},
journal = {Ecological Informatics},
volume = {82},
pages = {102699},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102699},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002413},
author = {Arunodhayan Sampath Kumar and Tobias Schlosser and Stefan Kahl and Danny Kowerko},
keywords = {Audio classification, Augmentation strategies, Birdsong soundscapes, Computer vision and pattern recognition, Convolutional neural networks, Vision transformers},
abstract = {In ecology, changes in environmental conditions are often closely linked to shifts in species diversity. This relationship can be investigated by analyzing avian vocalizations, which are robust indicators of trends in biodiversity. Within this contribution, we explored various data augmentation techniques and deep learning strategies for the classification of birdsong within natural soundscapes. For this purpose, we employed three fundamental deep neural network architectures, such as vision transformers, to classify 397 different bird species. To improve both the accuracy and generalizability of our models, we incorporated up to 19 well-established data augmentation techniques commonly used in audio classification. This included an iterative selection process where only augmentations that enhanced classification performance were selected. The primary augmentation technique involved the integration of various noise samples and non-bird audio elements, which significantly improved model performance as assessed on the BirdCLEF 2021 data set. Individual augmentations achieved F1-scores from 48.0 % (vertical flip) to 72.6 % (primary background noise soundscapes). Through the strategic combination of key techniques – namely simulated pink noise, interspecies sound mixing, and loudness normalization – we achieved a top F1-score of 73.7%. Depending on the selected classification model, this corresponds to an improvement by 4.81 % to 10.5 %. Improvements and deteriorations of all applied augmentation techniques appeared to be robust across our three evaluated models. Therefore, our approach highlights the potential of sophisticated audio augmentations in refining the accuracy and robustness of birdsong classification models.}
}
@article{CARDOSO2024102602,
title = {Can citizen science and social media images support the detection of new invasion sites? A deep learning test case with Cortaderia selloana},
journal = {Ecological Informatics},
volume = {81},
pages = {102602},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102602},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001444},
author = {Ana Sofia Cardoso and Eva Malta-Pinto and Siham Tabik and Tom August and Helen E. Roy and Ricardo Correia and Joana R. Vicente and Ana Sofia Vaz},
keywords = {Artificial intelligence, Convolutional neural networks, Computer vision, Pampas grass},
abstract = {Deep learning has advanced the content analysis of digital data, unlocking opportunities for detecting, mapping, and monitoring invasive species. Here, we tested the ability of open source classification and object detection models (i.e., convolutional neural networks: CNNs) to identify and map the invasive plant Cortaderia selloana (pampas grass) in mainland Portugal. CNNs were trained over citizen science images and then applied to social media content (from Flickr, Twitter, Instagram, and Facebook), allowing to classify or detect the species in over 77% of situations. Images where the species was identified were mapped, using their georeferenced coordinates and time stamp, showing previously unreported occurrences of C. selloana, and a tendency for the species expansion from 2019 to 2021. Our study shows great potential from deep learning, citizen science and social media data for the detection, mapping, and monitoring of invasive plants, and, by extension, for supporting follow-up management options.}
}
@article{SINGHAROY2023102265,
title = {Image background assessment as a novel technique for insect microhabitat identification},
journal = {Ecological Informatics},
volume = {77},
pages = {102265},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102265},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123002947},
author = {Sesa {Singha Roy} and Reid Tingley and Alan Dorin},
keywords = {Microhabitat, Insects, Image analysis, Computer vision, Machine learning},
abstract = {Habitat fragmentation under increased urbanisation, industrial agriculture and land clearing, are changing the way insects occupy habitat. Some species are highly adaptable and may occupy urbanised areas, utilising anthropogenic microhabitat-scale features. Other species are dependent on natural elements of their habitats, having to locate small regions of natural microhabitat within increasingly hostile landscapes. Consequently, humans are encountering insects in new settings. Identifying and analysing insects’ use of natural and anthropogenic microhabitats is therefore important to assess their responses to a changing environment, for instance to improve pollination or manage invasive pests. But such studies are labour-intensive. Traditional studies of insect microhabitat use can now be supplemented by machine learning-based insect image analysis. Typically, research has focused on automatic insect classification, but valuable data appearing in image backgrounds has been ignored. In this research, we analysed the backgrounds of insect images available in the Atlas of Living Australia database to determine the microhabitats in which they were commonly photographed. We analysed the image backgrounds of three globally distributed insect species that are common across Australia: Drone flies (Eristalis tenax), European honey bees (Apis mellifera), and European wasps (Vespula germanica). Image backgrounds were classified broadly as either natural or anthropogenic using computer vision and machine learning tools benchmarked against a manual classification algorithm. Our automated image background classification achieved 97.4% accuracy when compared against manual classification. Mis-classifications were scarce, usually less than 1%, and primarily for backgrounds of wood and soil or bare ground. Our results indicate that drone flies and European honey bees were predominantly photographed against natural backgrounds (flies manual classifier 95±3%, automated classifier 94%, bees 89±2%,87%), implying frequent observations by humans in natural microhabitat. European wasps were less frequently photographed against natural backgrounds (70±6%,63%). Within this data set, observations of the wasps in anthropogenic microhabitats were more common than for flies and bees. Our results are aligned with the expectation that the wasps are relatively well-suited to urban environments, and that European honey bees and drone flies utilise natural features of their environment. In general, although biases in data collected without formal protocols limits their application, our new automated approach for image background analysis can provide valuable data about insects’ interactions with humans, our artefacts, and natural features of their environments.}
}
@article{BJERGE2023102278,
title = {Hierarchical classification of insects with multitask learning and anomaly detection},
journal = {Ecological Informatics},
volume = {77},
pages = {102278},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102278},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003072},
author = {Kim Bjerge and Quentin Geissmann and Jamie Alison and Hjalte M.R. Mann and Toke T. Høye and Mads Dyrmann and Henrik Karstoft},
keywords = {nomaly detection, Computer vision, Deep learning, Hierarchical classification, Insects, Taxonomy},
abstract = {Cameras and computer vision are revolutionising the study of insects, creating new research opportunities within agriculture, epidemiology, evolution, ecology and monitoring of biodiversity. However, the diversity of insects and close resemblances of many species are a major challenge for image-based species-level classification. Here, we present an algorithm to hierarchically classify insects from images, leveraging a simple taxonomy to (1) classify specimens across multiple taxonomic ranks simultaneously, and (2) identify the lowest rank at which a reliable classification can be reached. Specifically, we propose multitask learning, a loss function incorporating class dependency at each taxonomic rank, and anomaly detection based on outlier analysis to quantify the uncertainty. First, we compile a dataset of 41,731 images of insects, combining images from time-lapse monitoring of floral scenes with images from the Global Biodiversity Information Facility (GBIF). Second, we adapt state-of-the-art convolutional neural networks, ResNet and EfficientNet, for the hierarchical classification of insects belonging to three orders, five families and nine species. Third, we assess model generalization for 11 species unseen by the trained models. Here, anomaly detection is used to predict the higher rank of the species which were not present in the training set. We found that incorporating a simple taxonomy into our model increased the accuracy at higher taxonomic ranks. As expected, our algorithm correctly classified new insect species at higher taxonomic ranks, while classification was uncertain at lower taxonomic ranks. Anomaly detection can effectively flag novel taxa that are visually distinct from species in the training data. However, five novel taxa were consistently mistaken for visually similar species in the training data. Above all, we have demonstrated a practical approach to hierarchical classification based on species taxonomy and uncertainty during automated in situ monitoring of live insects. Our method is simple and versatile, forming a valuable step towards high-level classification of species not found in training data.}
}
@article{ZOU2024102562,
title = {Bacterial community characterization by deep learning aided image analysis in soil chips},
journal = {Ecological Informatics},
volume = {81},
pages = {102562},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102562},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124001043},
author = {Hanbang Zou and Alexandros Sopasakis and François Maillard and Erik Karlsson and Julia Duljas and Simon Silwer and Pelle Ohlsson and Edith C. Hammer},
keywords = {Soil bacterial cell counting, Segmentation, Microfluidics, Microbial image recognition, Morphological biodiversity, Bacterial traits},
abstract = {Soil microbes play an important role in governing global processes such as carbon cycling, but it is challenging to study them embedded in their natural environment and at the single cell level due to the opaque nature of the soil. Nonetheless, progress has been achieved in recent years towards visualizing microbial activities and organo-mineral interaction at the pore scale, especially thanks to the development of microfluidic ‘soil chips’ creating transparent soil model habitats. Image-based analyses come with new challenges as manual counting of bacteria in thousands of digital images taken from the soil chips is excessively time-consuming, while simple thresholding cannot be applied due to the background of soil minerals and debris. Here, we adopt the well-developed deep learning algorithm Mask-RCNN to quantitatively analyze the bacterial communities in soil samples from different locations in the world. This work demonstrates analysis of bacterial abundance from three contrasting locations (Greenland, Sweden and Kenya) using deep learning in microfluidic soil chips in order to characterize population and community dynamics. We additionally quantified cell- and colony morphology including cell size, shape and the cell aggregation level via calculation of the distance to the nearest neighbor. This approach allows for the first time an automated visual investigation of soil bacterial communities, and a crude biodiversity measure based on phenotypic cell morphology, which could become a valuable complement to molecular studies.}
}
@article{AZADNIA2024102683,
title = {Medicinal and poisonous plants classification from visual characteristics of leaves using computer vision and deep neural networks},
journal = {Ecological Informatics},
volume = {82},
pages = {102683},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102683},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124002255},
author = {Rahim Azadnia and Faramarz Noei-Khodabadi and Azad Moloudzadeh and Ahmad Jahanbakhshi and Mahmoud Omid},
keywords = {Medicinal plants, Image classification, Machine learning, Deep learning, Data augmentation, Fast AutoAugment},
abstract = {Poisonous plants are the third largest category of poisons known globally, which pose a risk of poisoning and death to humans. Currently, the identification of medicinal and poisonous plants is done by humans using experimental methods, which are not accurate and are associated with many errors, and also the use of laboratory methods requires experts and this method is very costly and time-consuming. Therefore, distinguishing between medicinal and poisonous plants is very important using emerging, non-destructive, fast and accurate methods such as computer vision and artificial intelligence. In this study, we propose a robust and generalized model using spatial attention (SA) and channel attention (CA) modules for the classification of different plants. A dataset containing 900 confirmed images of three plant classes (oregano, poisonous and weed) was used. The attention mechanisms enhance efficiency of deep learning (DL) networks by allowing them to precisely focus on all relevant input elements. In order to enhance the performance of the proposed model, the CA was implemented based on four pooling operations including global average pooling-based CA (GAP-CA), mixed pooling-based CA (Mixed-CA), gated pooling-based CA (Gated-CA), and tree pooling-based CA (Tree-CA) operations. The results showed that the DL model based on Tree-CA had promising performance and outperformed other state-of-the-art models, achieving the values of 99.63%, 99.38%, 99.52%, 99.74%, and 99.42%, for accuracy, precision, recall, specificity, and F1-score, respectively. The findings support our proposed attention model's success in identifying medicinal plants from similar poisonous plants. Recent advancements in computer-based technologies and artificial intelligence enable automatic detection of medicinal and poisonous plants, revolutionizing traditional identification methods.}
}
@article{ISLER2024102439,
title = {Hybrid model-based prediction of biomass density in case studies in Turkiye},
journal = {Ecological Informatics},
volume = {79},
pages = {102439},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102439},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004685},
author = {B. İşler and Z. Aslan and F. Sunar and A. Güneş and E. Feoli and D. Gabriels},
keywords = {ANN/W-ANN modelling, Atmospheric data, EVI, LST, NDBI},
abstract = {Growing global concern over natural resource degradation due to urbanisation and population growth emphasizes the critical need for innovative solutions. Addressing this imperative, our study pioneers the integration of cutting-edge artificial intelligence (AI) methods to investigate crucial changes in vegetation density. In this context, a hybrid model, which harmoniously integrates conventional artificial neural network (ANN) models with the innovative Wavelet-ANN (W-ANN) approach, was employed in two case pilot areas, namely on Alanya in Antalya and Iznik in Bursa, Turkiye, renowned for their distinct ecosystems and land cover patterns. By employing diverse data sources, encompassing satellite-derived metrics such as the Enhanced Vegetation Index (EVI) and Land Surface Temperature (LST) from the MODIS/Terra satellite, alongside atmospheric data, our investigation intricately models temporal vegetation dynamics extending to the year 2030. Remarkably, the W-ANN model demonstrates better predictive performance compared to conventional methodologies. It anticipates a substantial 21.4% reduction in vegetation biomass density for Iznik, achieving a minimal 5.4% error probability. Similarly, for Alanya, the model forecasts a notable 6.6% decrease with a remarkably low 2% error probability, both projections extending to the year 2030. Our study reveals a significant reduction in vegetation biomass density by comparing the projected values of the W-ANN model for 2030 with the observed data from 2018. These findings gain further support from an analysis of the Normalised Difference Built-up Index (NDBI) derived from Landsat satellites, affirming the exceptional efficacy of our innovative AI-driven approach in advancing the understanding of urbanisation's impact on ecosystems.}
}
@article{GONG2023102334,
title = {Research on facial recognition of sika deer based on vision transformer},
journal = {Ecological Informatics},
volume = {78},
pages = {102334},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102334},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003631},
author = {He Gong and Tianye Luo and Lingyun Ni and Ji Li and Jie Guo and Tonghe Liu and Ruilong Feng and Ye Mu and Tianli Hu and Yu Sun and Ying Guo and Shijun Li},
keywords = {Sika deer, Vision transformer, DenseNet, Face recognition, Patch flattening},
abstract = {In the face of global concerns about endangered ecosystems, it is vital to identify individual animals. Along these lines, in this work, a Vision Transformer (ViT) based model for sika deer individual recognition using facial data was designed. To get the satisfactory results, both low-level aspects like texture and color must also be considered, in addition to the high-level semantic information. Consequently, it was difficult to get good results by only applying advanced retrieval features. The standard ViT or ViT with ResNet (Residual neural network) as the backbone network may not be the best solution, as the direct patch flattening method of feature embedded in the conventional ViT is not applicable for performing deer face recognition. Therefore, DenseNet (Densely connected convolutional networks) block as Module 1 was used for extracting low-level features. DenseNet layers enable feature reuse through dense connections, and any layer can communicate directly. Thus maximum exchange of information flow between layers in the network is enabled. In Module 2, the mask approach was also used to eliminate extraneous information from the images and reduce interference from complicated backgrounds on the identification accuracy. In addition, the pixel multiplication of the feature map output from the two modules enabled the fusion of the local features with global features, enriching hence the expressiveness of the feature map. Finally, the ViT structure was run through pre-trained. The experimental results showed that the proposed model can reach an accuracy of 97.68% for identifying sika deer individuals and exhibited excellent generalization capabilities. A valid database for the individual identification of sika deer is provided by our work, significantly contributing to the conservation and promotion of the ecosystem.}
}
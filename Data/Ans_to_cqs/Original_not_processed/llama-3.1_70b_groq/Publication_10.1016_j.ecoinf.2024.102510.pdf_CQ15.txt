Answer:::

The provided context does not explicitly mention the specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention the "Optimizer" as one of the hyperparameters in the table, which suggests that the study may have used different optimizers for training the models.

Unfortunately, the context does not provide further information on the specific optimizers used. Therefore, I cannot provide a definitive answer to the question.

However, I can suggest that the study may have used popular optimizers such as SGD, Adam, or RMSProp, as these are commonly used in deep learning pipelines. But without more information, I cannot confirm this.

In summary, the answer to the question is not explicitly mentioned in the provided context, and I do not want to make any assumptions or provide incorrect information.
Answer:::

According to the provided context, one regularization method used to prevent overfitting in the deep learning pipeline is dropout. The dropout layer is described as a regularization method that randomly excludes some inputs from activation and weight updates while training a network. Inputs not set to zero are scaled up by 1/(1 - rate) such that the sum over all inputs remains the same.

There is no mention of L2 regularization in the provided context. However, it is mentioned that the training set is pre-processed using a MinMaxScaler, which transforms each feature by scaling it to a [0-1] range. This is a data normalization technique, but not a regularization method.

It is also worth noting that the context mentions the use of an elastic net, which is a regularization method that combines L1 and L2 regularization. However, this is mentioned in a reference (Zou and Hastie, 2005) and not in the context of the deep learning pipeline being described.
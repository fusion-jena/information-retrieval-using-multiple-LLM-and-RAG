Answer:::
The provided context does not explicitly mention regularization methods used to prevent overfitting in the deep learning pipeline. However, it does discuss the concept of overconfidence in the context of transfer learning (TL) and domain adaptation. The context mentions that overconfidence in models such as ELM, GRNN, or SVR arises when they make predictions on a dataset with a huge data distribution difference from the source domain data on which they are trained.

The context also discusses the proposed approach of IFTL (Iterative Feature Transfer Learning), which aims to restrict overconfidence by considering the distribution divergence between the source and target domains. However, it does not mention specific regularization methods such as dropout or L2 regularization.

Therefore, based on the provided context, I do not have enough information to provide a specific answer to the query about regularization methods used to prevent overfitting in the deep learning pipeline.
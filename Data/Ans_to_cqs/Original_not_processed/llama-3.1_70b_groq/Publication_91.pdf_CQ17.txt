Answer:::

Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that the images in the training set were augmented in various ways, including flipping, adjusting brightness, saturation, contrast, and hue. This data augmentation can be considered a regularization technique, as it helps to increase the diversity of the training data and prevent overfitting.

Additionally, the context mentions that the learning rate was exponentially decayed every 3 epochs with a factor of 0.94, which can also help to prevent overfitting by reducing the learning rate as the model trains.

However, it does not mention the use of dropout or L2 regularization, which are common regularization techniques used in deep learning. Therefore, I cannot confirm whether these methods were used in the pipeline.
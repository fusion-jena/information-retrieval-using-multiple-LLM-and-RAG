Answer:::

The provided context does not specifically discuss techniques used to address data bias during preprocessing of the deep learning pipeline. However, it does mention techniques related to ensemble learning and handling missing values.

The context mentions bagging (bootstrap aggregating) as a technique to reduce the variance of an estimated forecast function by averaging models containing errors but approximately unbiased. It also mentions random forests, which employ a modified tree learning algorithm that uses a random subset of the features at each candidate split during the learning process.

Regarding handling missing values, the context mentions that the proposed neural network models are exploited for ETa predictions and gap-filling due to the presence of 854 missing values in the ETa feature.

The context does not mention stratified splitting, oversampling, undersampling, or diverse data collection as techniques used to address data bias during preprocessing. Therefore, I do not have enough information to provide a comprehensive answer to the query.
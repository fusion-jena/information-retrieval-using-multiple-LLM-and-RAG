Answer:::
Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. The context primarily discusses the concepts of pooling layers, convolution operations, and their applications in deep learning. It also mentions some research papers and studies related to deep learning, but it does not provide information on regularization methods.

However, it is worth noting that the context does mention the use of rectified linear units (ReLU) in deep learning, as mentioned in reference [65]. While ReLU is an activation function and not a regularization method, it is often used in conjunction with other regularization techniques to prevent overfitting.

To answer the query, I would need more information or context about the specific deep learning pipeline being referred to. However, based on general knowledge, common regularization methods used to prevent overfitting in deep learning include dropout, L2 regularization (also known as weight decay), and L1 regularization. These methods help to reduce the capacity of the model and prevent it from overfitting to the training data.
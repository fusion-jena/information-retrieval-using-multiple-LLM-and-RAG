Answer: 

The provided context does not specifically mention the optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention that the learning rate was varied from 0.01 to 1.0 in increments of 0.05, and the optimal learning rate was selected using a trial-and-error method. This suggests that the learning rate was tuned as a hyperparameter, but the specific optimization algorithm used is not mentioned.

Additionally, the context mentions the use of early stopping as a regularization technique to prevent overfitting, which is a common technique used in conjunction with optimization algorithms. However, the specific optimization algorithm used is not specified.

Therefore, based on the provided context, I do not know the specific optimization techniques applied in the deep learning pipeline.
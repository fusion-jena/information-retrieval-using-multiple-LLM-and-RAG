Answer:::
The regularization methods used to prevent overfitting in the deep learning pipeline, as mentioned in the provided context, include:

1. Dropout: The context mentions that a dropout of 50% was used as a learning parameter. This means that during training, 50% of the neurons in the network were randomly dropped out, which helps prevent overfitting by preventing the network from relying too heavily on any single neuron or group of neurons.

2. Exponential learning decay: The context mentions that an exponential learning decay with a Gamma of 0.95 was used. This means that the learning rate was decreased exponentially during training, which helps prevent overfitting by reducing the impact of large updates to the network's weights.

No mention is made of L2 regularization in the provided context.
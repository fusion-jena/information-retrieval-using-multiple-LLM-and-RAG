Answer:

Based on the provided context, the preprocessing steps involved before training a deep learning model, specifically a transformer-based model like BERT, include:

1. **Tokenization**: Texts are tokenized using the NLTK word-tokenizer to split them into chunks of 300 tokens.
2. **Chunk selection**: Chunks that contain any of the original query terms (e.g., bat-related or exploitation-related terms) are retained.
3. **Truncation**: If more than one chunk contains keywords, only the first such chunk is retained.
4. **Tokenization (again)**: Before being fed into the BERT model, texts are re-tokenized using the BERT tokenizer.
5. **Truncation (again)**: Texts are truncated to a maximum number of tokens, which is set to 400 for social media posts and 512 for web articles.

There is no mention of normalization or scaling in the provided context. Additionally, the context does not explicitly mention any cleaning steps, but it can be inferred that the selection of chunks containing keywords and the truncation of texts may help remove irrelevant information. However, the exact cleaning steps are not specified.
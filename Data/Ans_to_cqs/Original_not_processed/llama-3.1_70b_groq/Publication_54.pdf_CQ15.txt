Answer:::
The optimization techniques applied in the deep learning pipeline, as mentioned in the provided context, include the mini-batch gradient descent algorithm and the Adam optimizer. 

The mini-batch gradient descent algorithm is used to train the Inception-ResNet-v2 model with a batch size set to 32 and a learning rate maintained at 0.0001. 

The Adam optimizer is used to fine-tune the model with specific parameters, including an exponential decay rate of first-moment estimation of 0.9. 

There is no mention of SGD (Stochastic Gradient Descent) in the provided context.
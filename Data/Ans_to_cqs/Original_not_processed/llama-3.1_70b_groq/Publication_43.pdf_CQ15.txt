Answer:::

The provided context mentions the use of Stochastic Gradient Descent (SGD) as an optimization technique in the deep learning pipeline. Specifically, it is used to update the weights of the network based on the loss function. The update rule is given by the equation:

wt+1 = µwt − α∆J(wt)

where µ is the momentum weight for the current weights wt and α is the learning rate.

However, it does not mention the use of Adam as an optimization technique. Adam is a popular optimization algorithm that combines the benefits of adaptive learning rates and momentum, but it is not mentioned in the provided context.

Therefore, based on the provided context, the answer to the query is that Stochastic Gradient Descent (SGD) is the optimization technique applied in the deep learning pipeline.
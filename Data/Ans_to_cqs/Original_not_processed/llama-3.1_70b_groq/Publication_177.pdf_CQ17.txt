Answer:::
The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that the Bayesian optimization technique was used to optimize the network for batch size and initial learning rate, and that the optimization was performed by minimizing the loss on the validation dataset. Additionally, the context mentions that the models were trained using the stochastic gradient descent with a momentum ('sgdm') optimizer.

One of the references provided, [33] H. Chongomweru, A. Kasem, A novel ensemble method for classification in imbalanced datasets using split balancing technique based on instance hardness (sBal_IH), mentions the use of instance hardness, but it is not clear if this is related to regularization.

Another reference, [23] D. Wu, X. Han, G. Wang, Y. Sun, H. Zhang, H. Fu, Deep Learning with Taxonomic Loss for Plant Identification, mentions the use of taxonomic loss, but it is not clear if this is a regularization method.

The context does mention "regularization strength" as one of the variables that can be optimized using the Bayesian optimization technique, but it does not provide further information on the specific regularization methods used.

Therefore, based on the provided context, I do not have enough information to provide a specific answer to the query.
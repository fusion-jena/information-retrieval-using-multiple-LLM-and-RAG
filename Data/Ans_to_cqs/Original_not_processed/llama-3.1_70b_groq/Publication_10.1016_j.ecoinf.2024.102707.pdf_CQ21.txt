Unfortunately, the provided context does not explicitly mention the postprocessing steps involved after the model training. However, based on the information provided in the context, we can infer some possible postprocessing steps that might have been involved.

The context mentions that the model achieved high F1 scores on the training, validation, and testing data. This suggests that some evaluation metrics were calculated after the model training. Specifically, the context mentions that the model achieved F1 scores above 0.991 on all the training and validation data, and scores above 0.934 on the testing data.

Additionally, the context mentions that the authors report the F1 scores as well as the number of actual nests, true positives (TP), false positives (FP), and false negatives (FN) for each individual orthomosaic in Table S2. This suggests that a confusion matrix might have been generated as part of the postprocessing steps.

However, there is no explicit mention of saliency maps or other postprocessing steps in the provided context. Therefore, we can only confirm that some evaluation metrics (e.g., F1 scores) and possibly a confusion matrix were calculated after the model training.

In summary, the postprocessing steps that can be inferred from the provided context are:

* Calculation of evaluation metrics (e.g., F1 scores)
* Generation of a confusion matrix (possibly)

Other postprocessing steps, such as saliency maps, are not mentioned in the provided context.
Answer:::

The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention the use of Batch Normalisation (BN) layers to prevent overfitting. According to the text, the BN layer can "accelerate the network convergence and prevent overfitting" during the training process. Additionally, the text mentions that the BN layer can help solve the problems of gradient disappearance and explosion.

It is also worth noting that the text mentions the use of Leaky ReLU activation function, which can help alleviate the vanishing gradient problem, but it is not a regularization method per se.

Therefore, based on the provided context, the only regularization method mentioned is Batch Normalisation (BN). However, it is possible that other regularization methods are used in the pipeline, but they are not mentioned in the provided text.
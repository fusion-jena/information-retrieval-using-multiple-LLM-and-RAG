Answer: The optimization techniques applied in the deep learning pipeline, as mentioned in the context, are:

1. RMSProp
2. Adam
3. SGD (Stochastic Gradient Descent)

These optimizers are used to optimize the model parameters and improve the performance of the convolutional neural network (CNN) deep learning model. The learning rate, which is another hyperparameter, is also varied (0.001 and 0.01) to evaluate the performance of the model with different optimizer and learning rate combinations.
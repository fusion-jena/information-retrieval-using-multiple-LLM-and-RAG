Answer: According to the provided context, the optimization technique used in the deep learning pipeline is Stochastic Gradient Descent (SGD) with a momentum of 0.9. This is mentioned in the text as follows: "Stochastic Gradient Descent (SGD) was used as the modelâ€™s optimizer with a momentum of 0.9." There is no mention of Adam or any other optimization techniques being used in the pipeline.
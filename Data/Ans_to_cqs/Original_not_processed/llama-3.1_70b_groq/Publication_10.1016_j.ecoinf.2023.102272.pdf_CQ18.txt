Answer: The strategy implemented to monitor the model performance during training is through the use of a validation data set (data partition Av) to compare the accuracy of candidate models, and later, to determine the optimal number of epochs to train the selected candidate model using another validation data set (data partition Bv). 

Additionally, the model's performance is evaluated on a separate test set (partition 'T') for each year, which is not used for training. This is done to mimic an operational setting where data from previous years is used to train a model that will be employed in real-time forecasting of changes in the number of eggs for the coming week. 

The performance of the model is evaluated using the Area Under the Curve (AUC) metric, and the results are reported for each class (e.g., "increase >50%") and for each year. The AUC values are used to determine the model's predictive performance, with values ranging from 0.7 to 0.8 considered fair, and values above 0.9 considered excellent.
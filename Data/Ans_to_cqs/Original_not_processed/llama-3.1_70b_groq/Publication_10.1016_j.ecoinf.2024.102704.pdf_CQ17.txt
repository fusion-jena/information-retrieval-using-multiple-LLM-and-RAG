Answer:::

Unfortunately, the provided context does not mention specific regularization methods used to prevent overfitting in the deep learning pipeline. The context primarily focuses on optimizing model performance, reducing memory access, and accelerating operations in convolutional neural networks (CNNs). It discusses techniques such as reconfiguring computation sequences, lowering memory access, and utilizing different model scales (e.g., 'n', 's', 'm', 'l', 'x') to improve model efficiency.

However, it does mention batch normalization, which is a technique used to normalize the inputs to each layer, helping to improve the stability and speed of training. But it does not explicitly mention dropout or L2 regularization, which are common regularization methods used to prevent overfitting.

Therefore, based on the provided context, I do not have enough information to provide a comprehensive answer to the query about regularization methods used to prevent overfitting in the deep learning pipeline.
Answer:::

Unfortunately, the provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the history and development of neural networks, their applications, and some specific neural network models like Wavelet networks and Back Propagation neural networks. It also mentions the use of an adaptive strategy to modulate the learning rate to improve the neural network convergence rate, but it does not provide details on the specific optimization techniques used.

However, it does mention the use of a learning rate up to 1.07 as the error steadily decreases, and a learning rate down to 0.70 as the error increases, but this is not enough information to determine the specific optimization technique used.

Therefore, based on the provided context, I do not know the specific optimization techniques applied in the deep learning pipeline.
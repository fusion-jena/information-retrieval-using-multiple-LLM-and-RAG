Answer:::
The optimization technique applied in the deep learning pipeline is Stochastic Gradient Descent (SGD). This is mentioned in the context as the optimization technique used for training the models. Specifically, it is used for the ResNet variants and the SlowFast architecture. The SGD parameters, such as learning rate, momentum, and weight decay, are also mentioned in the context, but the specific values vary depending on the model being trained. There is no mention of Adam or any other optimization techniques being used in the pipeline.
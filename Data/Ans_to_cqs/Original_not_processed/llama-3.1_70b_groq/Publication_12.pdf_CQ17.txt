Answer:::
The provided context mentions the use of a new kind of BNorm (Batch Normalization) layers behind the 22 convolutional layers to prevent overfitting in the BatNet model. However, it does not explicitly mention the use of dropout or L2 regularization as regularization methods. 

Batch Normalization is a technique used to normalize the input data for each layer, which can help to prevent overfitting by reducing the effect of internal covariate shift. The context does not provide information on other regularization methods that may be used in the deep learning pipeline.
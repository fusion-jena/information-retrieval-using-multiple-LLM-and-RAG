Answer:::

The hyperparameters used in the deep learning model are as follows:

* Optimizer: Adagrad
* Dropout rate: 0.5
* Batch size: 20
* Number of epochs: 100

The learning rate is not explicitly mentioned in the provided context. However, it is mentioned that the Adagrad optimizer is used, which is a variant of stochastic gradient descent that adapts the learning rate for each parameter based on the magnitude of the gradient. 

Note that the context does not provide information on other hyperparameters such as the learning rate schedule, weight decay, or momentum.
Answer:::
To avoid overfitting, a dropout layer is added to the LSTM model. The dropout layer is a regularization method that randomly excludes some inputs from activation and weight updates while training a network. Inputs not set to zero are scaled up by 1/(1 - rate) such that the sum over all inputs remains the same.
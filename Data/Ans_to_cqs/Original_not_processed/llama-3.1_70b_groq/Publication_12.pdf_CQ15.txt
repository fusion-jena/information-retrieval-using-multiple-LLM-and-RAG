Answer:::
Unfortunately, the provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The text discusses the use of batch normalization (BNorm) layers to prevent overfitting and the introduction of shortcuts among layers to mitigate information loss. It also mentions the use of a large batch size (128) for training, but it does not provide information on the specific optimization algorithm used. Therefore, I do not know the answer to this question based on the provided context.
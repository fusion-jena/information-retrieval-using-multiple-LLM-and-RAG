Answer:::
The deep learning pipeline applies the following optimization techniques: Adam, Stochastic Gradient Descent with Momentum (sgdm), and RMSProp. These optimization techniques are used as solvers for training the network, as shown in Table 1. Specifically, the performance of Adam is highlighted as superior compared to other optimizers, with lower RMSE values in most instances where Adam is used as the solver and the input time series length is set to 5 days.
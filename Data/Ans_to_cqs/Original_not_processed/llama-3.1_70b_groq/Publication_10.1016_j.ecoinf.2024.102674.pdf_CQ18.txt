Answer:

Unfortunately, the provided context does not explicitly mention the strategy implemented to monitor the model performance during training. However, it does mention the evaluation metrics used to assess the performance of the models after calibration.

According to the context, the performance of the models was evaluated using metrics such as KGE scores, percent bias (pbias), mean absolute error (MAE), and root mean squared error (RMSE). These metrics were used to assess the performance of the models in interpolating weather station data and downscaling WorldClim and CHELSA data.

While the context does not provide information on the specific strategy used to monitor model performance during training, it does suggest that the models were evaluated using a combination of metrics after calibration.
Answer: 
The optimization technique applied in the deep learning pipeline is the Adam optimizer. According to the text, the Adam optimizer was used with a learning rate of 10−4 for the first 25 epochs, and then the learning rate was reduced to 10−6 for the next 20 epochs.
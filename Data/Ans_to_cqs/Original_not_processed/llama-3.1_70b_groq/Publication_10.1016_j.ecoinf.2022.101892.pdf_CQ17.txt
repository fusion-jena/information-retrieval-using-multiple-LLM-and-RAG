Answer:::

The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that "aggressive data augmentation" was applied to avoid overfitting and imbalanced problems. Additionally, it mentions the use of Batch Normalization (BN) to accelerate the training of networks, which can also help with regularization.

It does not mention dropout or L2 regularization specifically. Therefore, I can only confirm that data augmentation and Batch Normalization were used, but I do not know if any other regularization methods were employed.

It's worth noting that the context focuses on the architecture and training of the models, rather than the specific regularization techniques used. If you're looking for information on regularization methods, you might need to look elsewhere.
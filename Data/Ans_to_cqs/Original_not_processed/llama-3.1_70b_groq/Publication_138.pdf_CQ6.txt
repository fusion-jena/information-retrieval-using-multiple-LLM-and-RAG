Answer:::
The provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. However, it does mention the use of Local Response Normalization (LRN) in the network architecture, which is a type of normalization technique. 

LRN is used to normalize the local response, enhancing the larger response values and reducing the smaller response values. This helps to improve the generalization of the model. 

Additionally, the context mentions the use of ReLU as the activation function, which can be considered as a form of scaling, but it is not a traditional preprocessing step.

It does not provide information on other preprocessing steps such as data cleaning, scaling, or normalization. Therefore, it is not possible to provide a comprehensive answer to the query based on the provided context.
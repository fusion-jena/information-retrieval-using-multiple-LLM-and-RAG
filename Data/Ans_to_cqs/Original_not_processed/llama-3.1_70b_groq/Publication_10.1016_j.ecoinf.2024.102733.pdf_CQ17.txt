Answer:

The regularization methods used to prevent overfitting in the deep learning pipeline include:

1. Dropout: This method was explored in the training of the MLP, where dropout was applied to the input layer, the hidden layers, and all layers, with different dropout rates (0.05, 0.1, 0.2, and 0.25).
2. Weight decay (L2 regularization): Weight decay values of 0, 0.00001, 0.0001, and 0.001 were tested during the training of the MLP.
3. Batch normalization: This method was tested on each layer of the network separately and on all layers simultaneously, with the best results achieved when applied to the third layer.

These regularization methods were used in combination with other hyperparameters, such as learning rates and batch sizes, to prevent overfitting and improve the performance of the MLP.
Answer:::
Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the architecture of the Convolutional Neural Networks (CNNs) used for image classification, including the layers and tensor/vector shapes. It also mentions the use of Mathematica notebooks for defining and training the CNNs, but it does not provide information on the optimization techniques used during training.
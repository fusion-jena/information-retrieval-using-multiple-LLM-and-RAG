Answer:::

Unfortunately, the provided context does not explicitly mention regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention optimization algorithms such as Stochastic Gradient Descent (SGD), Adaptive Gradient (AdaGrad), Root Mean Square Prop (RMSProp), and Adam, which are used to assess each parameter's contribution to the loss function and update the model's parameters accordingly.

While these optimization algorithms can help mitigate overfitting to some extent, they are not typically considered regularization methods. Regularization methods, such as dropout and L2 regularization, are specifically designed to prevent overfitting by adding a penalty term to the loss function or randomly dropping out neurons during training.

Since the provided context does not mention regularization methods, I cannot provide a definitive answer to the query. However, I can suggest that the authors may have used regularization methods in their deep learning pipeline, but it is not explicitly stated in the provided context.
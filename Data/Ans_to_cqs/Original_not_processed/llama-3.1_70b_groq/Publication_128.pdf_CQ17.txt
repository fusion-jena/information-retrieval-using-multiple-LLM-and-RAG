Answer:::

The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of dropout in the model training section. According to the context, the dropout rate is set to 0.001, which suggests that dropout is used as a regularization method to prevent overfitting.

Additionally, the context mentions that for neural networks with lower depth, higher training epochs can lead to a higher probability of overfitting. To mitigate this, the models with less than 100 layers are trained for 60 epochs, while those with more than 100 layers are trained for 80 epochs. This suggests that early stopping is also used as a regularization method to prevent overfitting.

There is no mention of L2 regularization in the provided context. However, it is mentioned that the bias value is set to 0, which could be related to L2 regularization, but it is not explicitly stated.

In summary, the regularization methods used to prevent overfitting in the deep learning pipeline, as mentioned in the context, are dropout and early stopping.
Answer:::
Unfortunately, the provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention the use of dropout as a form of regularization to reduce overfitting, which is a technique used to prevent the model from becoming too complex and fitting the training data too closely. Additionally, the context mentions the use of a batch size of 100 for 20,000 steps during training, which suggests that some form of mini-batch optimization may be used. However, the specific optimization algorithm used is not mentioned.
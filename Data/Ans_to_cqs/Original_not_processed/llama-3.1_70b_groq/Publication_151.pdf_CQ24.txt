Answer:::

The provided context does not explicitly mention strategies to handle randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention that the batch size is fixed to 32, and the training loss is checked every 50 epochs to stop the training when the value of the loss function fails to decrease in two consecutive checks. 

Additionally, the context mentions that the learning rate is adjusted by a running average of its recent magnitude to speed up training, which is a common technique in deep learning. However, this is not directly related to handling randomness.

It is also worth noting that the context does not mention any specific techniques for handling randomness, such as setting a random seed value, using a fixed random seed, or using techniques like dropout or data augmentation to reduce overfitting.

Therefore, based on the provided context, it is not possible to provide a detailed answer to the query.
Answer:::
The provided context mentions some of the hyperparameters used in the deep learning model. 

The learning rate, denoted by α, is mentioned to be 0.001 for the heads layer training. However, during all layers training, α decays by 10, which implies that the learning rate is reduced by a factor of 10 during the training process.

The context does not explicitly mention the optimizer used in the model. However, it does mention that the model is implemented using TensorFlow version 1.14.0 and Keras version 2.2.4, which suggests that the optimizer might be one of the standard optimizers available in Keras, such as Adam, SGD, or RMSprop. However, without further information, it is not possible to determine the exact optimizer used.

Additionally, the context mentions that the model uses a batch size that is a product of the number of GPUs and images per GPU, but the exact batch size is not specified. The number of epochs used for training is 20, with 500 steps per epoch for training images and 100 steps per epoch for validation images.
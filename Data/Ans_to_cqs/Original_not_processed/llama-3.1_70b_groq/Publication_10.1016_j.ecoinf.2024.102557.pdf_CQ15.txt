Answer:::

Unfortunately, the provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline. The context appears to be a collection of references and citations related to ecology, conservation biology, and machine learning, but it does not provide specific information on optimization techniques used in deep learning.

However, I can provide some general information on common optimization techniques used in deep learning. Stochastic Gradient Descent (SGD) and Adam are indeed two popular optimization algorithms used in deep learning. SGD is a widely used optimization algorithm that iteratively updates the model parameters to minimize the loss function. Adam, on the other hand, is an adaptive learning rate optimization algorithm that adapts the learning rate for each parameter based on the magnitude of the gradient.

Other optimization techniques commonly used in deep learning include RMSProp, Adagrad, and Nadam, among others. However, without more specific information from the context, I cannot determine which optimization techniques are applied in the deep learning pipeline.

Therefore, I must conclude that I don't know the specific optimization techniques applied in the deep learning pipeline based on the provided context.
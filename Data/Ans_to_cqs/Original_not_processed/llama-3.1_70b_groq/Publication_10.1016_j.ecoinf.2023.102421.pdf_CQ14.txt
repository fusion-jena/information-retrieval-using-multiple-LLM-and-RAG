Answer: The hyperparameters of the models (XGBoost and LightGBM) are optimized using several optimizers, including Bayesian-based Optimisers and a novel Tasmanian Devil Optimisation algorithm. The optimization process uses the root mean square error (RMSE) as the objective (loss) function. The study does not mention the use of grid search or random search as the default method for selecting optimal hyperparameters, but rather mentions that these methods are commonly used in many packages. Instead, the study focuses on comparing the performance of different optimization algorithms, including Bayesian optimization and the Tasmanian Devil Optimisation algorithm, to find the best set of hyperparameters for the machine learning models.
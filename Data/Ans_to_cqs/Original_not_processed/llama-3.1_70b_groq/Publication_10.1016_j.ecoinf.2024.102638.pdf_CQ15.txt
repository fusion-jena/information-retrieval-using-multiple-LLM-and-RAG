Answer:

Unfortunately, the provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context appears to focus on the hyperparameter optimization process for various machine learning models, including KNN, GP, TREE, RF, XGB, SVR, and MLP. However, it does not delve into the specifics of the optimization techniques used in the deep learning pipeline.

That being said, the context does mention the use of learning rates in the context of XGB and MLP models. For example, it mentions the hyperparameter "learning_rate" for XGB and MLP models. However, this is not sufficient to determine the specific optimization techniques used in the deep learning pipeline.

Therefore, I must conclude that the answer to this query is not explicitly mentioned in the provided context.
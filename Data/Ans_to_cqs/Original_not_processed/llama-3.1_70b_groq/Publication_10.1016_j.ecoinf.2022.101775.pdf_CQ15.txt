According to the provided context, the optimization technique applied in the deep learning pipeline is Adam. This is mentioned in the following sentence:

"We used a batch size of 64 samples and trained the algorithm during 100 epochs using Adam as the algorithm to optimize the network parameters."

Additionally, Stochastic Gradient Descent (SGD) is also mentioned as an optimization technique used in the context of regularized cost function optimization:

"Consequently, the unknown parameters bs, bt, ps, qt can be estimated optimizing the regularized cost function using stochastic gradient descent:"
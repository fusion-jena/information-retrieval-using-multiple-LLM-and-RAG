Answer: The hyperparameters of the model were optimized through initial computational experiments with different combinations of epochs and batch sizes. Specifically, the researchers tried combinations of epochs of 300, 400, and 500 with batch sizes of 8, 16, and 32. After conducting these experiments, they determined that the optimal hyperparameters for their training were 400 epochs with a batch size of 8. This suggests that the hyperparameters were optimized through a form of grid search, where a range of possible hyperparameter values were systematically tried and evaluated. However, it is not explicitly stated that a grid search was used, and it is possible that the researchers simply tried a few different combinations of hyperparameters and selected the best-performing ones.
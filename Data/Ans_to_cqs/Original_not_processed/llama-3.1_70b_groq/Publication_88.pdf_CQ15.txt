Answer:::
The optimization technique applied in the deep learning pipeline is Adam optimizer. According to the provided context, Adam optimizer was selected empirically with weight decay equal to 0.9 and learning rate equal to 10âˆ’3. This suggests that the Adam optimizer is used to update the model's parameters during training. No other optimization techniques, such as SGD, are mentioned in the provided context.
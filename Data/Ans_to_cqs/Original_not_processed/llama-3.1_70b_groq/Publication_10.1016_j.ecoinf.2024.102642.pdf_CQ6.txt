Answer:::

Unfortunately, the provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. The text focuses on the annotation process, the challenges of annotation discrepancies, and the evaluation metrics used to assess the performance of the models. It also discusses the importance of annotation guidelines and the benefits of using a conservative approach to improve overall annotation quality.

However, it does mention that the models were trained on annotations from both novices and experts, and that two annotation aggregation methods (majority voting and soft labeling) were proposed and tested to optimize the potential of multi-annotation and mitigate the presence of noisy labels.

Regarding the hyperparameters, the text mentions that consistent hyperparameters were maintained across all training instances for each task, including batch sizes, learning rates, and the number of epochs. Early stopping was also used to prevent overfitting.

Unfortunately, there is no information on preprocessing steps such as normalization, scaling, or cleaning.
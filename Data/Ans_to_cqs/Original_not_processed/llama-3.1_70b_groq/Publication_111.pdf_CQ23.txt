Answer:::

To ensure the generalizability of the deep learning model, the authors employed two strategies: early stopping and dropout. 

Early stopping is a technique used to prevent overfitting by stopping the training process when the model's performance on the validation set starts to degrade. This is achieved by separating the training data into training and validation groups. The model's performance on the validation set is monitored during training, and the training process is stopped when the performance starts to degrade.

Dropout is another technique used to prevent overfitting. It is a regularization strategy that randomly drops out a certain proportion of neurons in the fully connected layers during training. This helps to prevent the model from becoming too specialized to the training data and improves its ability to generalize to new, unseen data.

While the authors do not mention using cross-validation or stratified splitting, they do mention that the training data is separated into training and validation groups. However, the exact method used to split the data is not specified. Additionally, there is no mention of using a diverse dataset to ensure generalizability.
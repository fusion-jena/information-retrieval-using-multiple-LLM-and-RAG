Answer: 

Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that the model adjusts too well to the training data but does not generalize well to the unseen test dataset, which is a common sign of overfitting.

The context does mention that data augmentation techniques such as rotation, noise, and flip are used to make the CNN classification more robust and generalize better. Data augmentation can be considered a regularization technique as it helps to prevent overfitting by increasing the size of the training dataset and providing more diverse examples for the model to learn from.

Additionally, the context mentions that the learning rate was reduced following an exponential decay parameter of 0.5 by an SGD scheduler after 50,000 iterations. Learning rate scheduling can also be considered a regularization technique as it helps to prevent overfitting by reducing the learning rate as the model converges.

However, the context does not mention other common regularization methods such as dropout or L2 regularization.
Answer:

The provided context does not specifically mention the optimization techniques applied in a deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context appears to focus on hyperparameter optimization for machine learning models, specifically XGBoost and LightGBM, using Bayesian optimizers and Tree-based Density Optimization (TDO). 

However, it does mention that the objective function used for hyperparameter tuning is the Root Mean Squared Error (RMSE), and that the performance of different optimizers is compared using metrics such as RMSE, Mean Absolute Error (MAE), and R-squared (R2). 

Therefore, I do not have enough information to provide a specific answer to the query about the optimization techniques applied in a deep learning pipeline.
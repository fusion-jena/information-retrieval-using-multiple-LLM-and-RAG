Answer: To handle randomness in the deep learning pipeline, the authors employed several strategies. Firstly, they used a random initialization of the trainable parameters for each of the three executions of each network. This means that each time a network was trained, the initial values of its weights and biases were randomly set, which helps to reduce the impact of randomness on the results.

Additionally, the authors applied data augmentation to all extracted patches, which included a 90âˆ˜ rotation and vertical and horizontal flips. This increases the diversity of the training data and helps the network to learn more robust features that are less dependent on the specific orientation or reflection of the input data.

Furthermore, the authors used a classification decision committee (CDC) which combines the outcomes of different network architectures into a single decision. This ensemble approach can help to reduce the impact of randomness on the results, as the predictions of multiple networks are combined to produce a single output.

Finally, the authors evaluated the performance of each network three times, each time with a different data fold. This means that the training and testing data were split into three different folds, and the network was trained and evaluated on each fold separately. This helps to ensure that the results are not dependent on a single random split of the data.

Overall, the authors employed a combination of random initialization, data augmentation, ensemble methods, and multiple evaluations to handle randomness in the deep learning pipeline.
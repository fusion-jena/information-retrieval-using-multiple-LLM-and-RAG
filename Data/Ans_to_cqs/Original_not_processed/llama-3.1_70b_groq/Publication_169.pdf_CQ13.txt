Answer: 
The hyperparameters used in the deep learning model are not explicitly stated in the provided context, except for the optimizer. The optimizer used is the Adam algorithm, which is a variant of stochastic gradient descent. However, the learning rate is not specified, as it is mentioned that all parameters of the Adam algorithm are set to their default values.
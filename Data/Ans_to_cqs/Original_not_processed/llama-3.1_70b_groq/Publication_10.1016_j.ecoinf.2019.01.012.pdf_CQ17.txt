Answer: 

The text mentions the use of dropout as a regularization method to prevent overfitting in the deep learning pipeline. Specifically, it states that "The gap between training and validation accuracy is caused by using dropout on the training data only." This suggests that dropout is used to randomly drop out units during training, which helps to prevent overfitting by reducing the capacity of the network.

However, there is no mention of L2 regularization in the provided text. It is possible that L2 regularization was used, but it is not explicitly mentioned.

It is also worth noting that the text mentions the use of data augmentation, which can also be considered a regularization method. Data augmentation involves artificially increasing the size of the training dataset by applying transformations to the existing data, which can help to prevent overfitting by reducing the risk of the network memorizing the training data.
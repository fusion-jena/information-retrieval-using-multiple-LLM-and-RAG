Answer:::

According to the provided context, the regularization method used to prevent overfitting in the deep learning pipeline is dropout regularization. Specifically, it is mentioned that "We apply dropout regularization in the forward pass during training so that random [neurons] are dropped out." This suggests that dropout is used to randomly drop out neurons during training to prevent overfitting.

Additionally, the context mentions that weight decay is used, with a value of 1.0â‹…10^-3. Weight decay is a form of L2 regularization, which adds a penalty term to the loss function to discourage large weights. However, it is not explicitly stated that L2 regularization is used as a separate regularization method.

It is also worth noting that the context mentions that the Adam optimizer is used, which has a built-in mechanism to adapt the learning rate and prevent overfitting. However, this is not a regularization method per se, but rather an optimization algorithm that can help prevent overfitting.

Overall, the regularization methods used to prevent overfitting in the deep learning pipeline are dropout regularization and weight decay (L2 regularization).
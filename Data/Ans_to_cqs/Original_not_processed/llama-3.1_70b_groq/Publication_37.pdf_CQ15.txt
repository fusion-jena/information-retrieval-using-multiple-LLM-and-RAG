Answer:::
The optimization technique applied in the deep learning pipeline is mini-batch stochastic gradient descent (SGD). This is mentioned in the provided context as "During training the model optimizes cross-entropy loss via mini-batch stochastic gradient descent [26]". Additionally, the context also mentions that a constant learning rate of 0.01 is used, and dropout is applied to the input of the last two layers with a probability of 0.5. L2-regularization is also applied to the weights of the last two layers with a penalty factor of 0.001.
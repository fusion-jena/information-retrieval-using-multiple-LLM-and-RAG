Answer:::

The hyperparameters used in the deep learning models are as follows:

1. For the three ResNet variants:
   - Initial learning rate: 0.001
   - Learning rate decay: every 10 epochs by multiplying with γ = 0.1
   - Optimizer: Stochastic Gradient Descent (SGD)
   - Momentum: 0.9

2. For the Stochastic Gradient Descent (SGD) used in another context:
   - Initial learning rate: 0.0005
   - Learning rate decay: every 10 epochs by multiplying with γ = 0.1
   - Momentum: 0.9
   - Weight decay: 0.0005
   - Warmup iterations: first epoch with a warmup factor of 1/1000

3. For the SlowFast architecture:
   - Initial base learning rate: 0.1
   - Warmup epochs: 5 with a warmup learning rate of 0.001
   - Momentum: 0.9
   - Weight decay: 1/4
   - Learning rate decay: cosine strategy
   - Optimizer: Stochastic Gradient Descent (SGD)
   - Loss function: cross entropy loss
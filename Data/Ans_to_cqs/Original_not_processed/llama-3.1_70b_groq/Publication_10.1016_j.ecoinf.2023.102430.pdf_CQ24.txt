Answer:::

To handle randomness in the deep learning pipeline, the researchers employed a few strategies. Firstly, they used a random subset of 5256 pictures (80% of the 6570-picture dataset) for training all algorithms. This random subset was used consistently across all five algorithms, ensuring that the comparison between them was fair.

Secondly, the training-testing process was repeated 10 times for each algorithm, with each replicate run using the same training and testing picture subsets. This approach, known as pseudo-replication, helps to gauge performance consistency and reduces the impact of randomness in the results.

Lastly, the researchers included a 'replicate' random effect in the models to account for any remaining variability between replicate runs. This approach acknowledges that there may still be some randomness in the results and attempts to quantify its impact.

It's worth noting that the researchers did not mention using a fixed random seed value to ensure reproducibility. However, by using the same training and testing subsets across all algorithms and replicate runs, they were able to minimize the impact of randomness and ensure a fair comparison between the different algorithms.
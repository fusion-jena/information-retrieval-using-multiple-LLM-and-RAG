Answer:::

Based on the provided context, the regularization method used to prevent overfitting in the deep learning pipeline is dropout. The dropout rate for each task was set at 0.5. This means that during training, 50% of the neurons in the network were randomly dropped out to prevent overfitting.

There is no mention of L2 regularization or any other regularization methods being used in the provided context. Therefore, I can only confirm the use of dropout as a regularization method.

It's worth noting that data augmentation is also used in the pipeline, which can help to prevent overfitting by increasing the diversity of the training data. However, data augmentation is not a traditional regularization method like dropout or L2 regularization.
Answer: The provided context does not explicitly discuss strategies for handling randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention the use of dropout as a regularization technique to prevent overfitting in the convolutional neural network (CNN) architecture.

Specifically, the text mentions that dropout was applied on the training set only, with a dropout rate of 0.7 on the fully connected layer and 0.3 on each convolutional layer. This suggests that the authors employed dropout as a way to introduce randomness in the training process, which can help prevent overfitting and improve generalization.

However, it does not provide information on whether a random seed value was set to ensure reproducibility of the results. Therefore, I do not know if the authors employed any specific strategies to handle randomness in the deep learning pipeline beyond the use of dropout.
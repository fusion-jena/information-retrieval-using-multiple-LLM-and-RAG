Answer: Unfortunately, the provided context does not mention the specific optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context only mentions the model selection procedure, the architectures used (e.g., DeepConvLSTM, CNN), and the hyperparameters tested for classical machine learning models (Table 3). However, it does mention the use of "Eta" as a parameter in the xgbTree model, which is likely a learning rate parameter, but it does not provide information on the optimization technique used in the deep learning pipeline.
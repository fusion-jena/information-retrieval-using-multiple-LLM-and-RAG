Answer:::

Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the architecture and loss functions used in the Siamese network, as well as the comparison of different classification methods and the use of contrastive loss. It does not provide information on the optimization techniques used for training the models. Therefore, I do not know the answer to this question based on the provided context.
Answer: The optimization techniques applied in the deep learning pipeline include SGD, RMSProp, LAMB, LARS, and Adam. However, after a series of experimentation, the Adam optimizer was chosen for model training. Additionally, the Bayesian-optimization library was used to test and optimize the number of architecture layers, the size of each layer, and dropout rates for the network.
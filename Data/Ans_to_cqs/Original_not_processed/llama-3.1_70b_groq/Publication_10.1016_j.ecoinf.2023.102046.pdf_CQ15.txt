Answer:::

The optimization techniques applied in the deep learning pipeline, as mentioned in the provided context, are Adam and AdamW. 

For the Hugging Face models, specifically ViT, the AdamW optimizer is used, which is the default optimizer in the transformers library by Hugging-Face. 

On the other hand, for the Inception-V3 model, the Adam optimizer is used, as it was the optimizer used in previous work, and AdamW is not available in Keras. 

Additionally, the learning rates used for the different models are also mentioned. For Vanilla Inception-V3 and ViT, the learning rate is set to 10^-5, while for Deep Otolith ResNet, the learning rate is set to 10^-4, as suggested by Politikos et al. (2021). For Deep Otolith Inception, the learning rate is set to 4 Ã— 10^-5. 

It is also mentioned that class weights are included in the cross-entropy loss function to deal with dataset imbalance.
Answer: 

Unfortunately, the provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the architecture of the DCNN model (AlexNet), its layers, and the activation functions used, but it does not provide information on the optimization techniques used for training the model.
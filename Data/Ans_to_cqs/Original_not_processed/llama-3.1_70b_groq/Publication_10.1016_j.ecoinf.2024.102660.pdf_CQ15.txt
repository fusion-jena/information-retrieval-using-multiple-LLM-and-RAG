Answer: 

The provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention that the learning rate was adjusted using the "equal-interval adjustment strategy (StepLR)", which involves halving the original learning rate every 10 epochs of iteration. The initial learning rate was set to 0.001. 

Additionally, the context mentions that the network training process included random rotations, flips, and other image augmentation operations to enhance the network's robustness, and that the residual structures were incorporated to mitigate overfitting. But it does not provide information about the specific optimization algorithm used.
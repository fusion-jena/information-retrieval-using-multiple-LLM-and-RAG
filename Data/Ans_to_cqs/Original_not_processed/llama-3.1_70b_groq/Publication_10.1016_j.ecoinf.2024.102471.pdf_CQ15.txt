Answer: 

The optimization technique applied in the deep learning pipeline is Adam. According to the context, Adam is used as the optimizer with an initial learning rate of the order 10e-5, which is suitable for small weight updates throughout the network while fine-tuning.
Answer:::
The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention that a common remedy for overfitting is the use of validation data, which are set aside while training, yet their annotations by the models are monitored during the training duration. This helps to limit the risk of overfitting. 

In the context, it is mentioned that the validation data help to limit the training duration, thereby reducing the risk of overfitting. The study used 30 epochs per training task, based on their validation runs, and the validation loss stabilizes after 30 epochs, suggesting that further training might lead to overfitting. 

Therefore, the only regularization method mentioned in the context is the use of validation data to monitor the model's performance and prevent overfitting. Other regularization methods, such as dropout or L2 regularization, are not mentioned.
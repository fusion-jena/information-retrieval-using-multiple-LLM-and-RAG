Answer:::

Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training. However, it does mention the use of a validation phase, where the model's performance is evaluated on a separate set of data. 

It is mentioned that during the validation phase, the model combines both softmax outputs and computes the final class scores using fusion methods: average (ave) or max voting (mav). This suggests that the model is being evaluated on a separate validation set, but the exact criteria for splitting the data into training, validation, and test sets are not specified.

In general, common practices for splitting data into training, validation, and test sets include:

* Random splitting: randomly dividing the data into training, validation, and test sets.
* Stratified splitting: dividing the data into training, validation, and test sets while maintaining the same class distribution in each set.
* K-fold cross-validation: dividing the data into k folds and using each fold as a test set while the remaining folds are used for training.

However, without more information, it is not possible to determine the specific criteria used in this case.
Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, I can provide some general information on regularization methods that are commonly used in deep learning.

Regularization methods are techniques used to prevent overfitting in machine learning models. Overfitting occurs when a model is too complex and performs well on the training data but poorly on new, unseen data.

Some common regularization methods used in deep learning include:

* Dropout: This involves randomly dropping out neurons during training to prevent the model from becoming too dependent on any single neuron.
* L2 regularization (also known as weight decay): This involves adding a penalty term to the loss function to discourage large weights.
* L1 regularization: This involves adding a penalty term to the loss function to discourage non-zero weights.
* Early stopping: This involves stopping the training process when the model's performance on the validation set starts to degrade.

It's worth noting that the context mentions that the authors used heavier augmentation for training the ResNet-101 network, which may help to prevent overfitting by increasing the diversity of the training data. However, the specific regularization methods used are not mentioned.

Therefore, I do not have enough information to provide a specific answer to the query.
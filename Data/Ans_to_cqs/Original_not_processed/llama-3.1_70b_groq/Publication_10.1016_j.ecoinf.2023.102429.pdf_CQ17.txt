Answer: 

Unfortunately, the provided context does not mention the regularization methods used to prevent overfitting in the deep learning pipeline. The context appears to be related to ecological informatics, spectral data, and soil property estimation, but it does not specifically discuss deep learning or regularization methods.

However, I can provide some general information on regularization methods commonly used in deep learning. Regularization methods are used to prevent overfitting by adding a penalty term to the loss function or by modifying the model's weights. Some common regularization methods include:

* Dropout: randomly drops out neurons during training to prevent the model from relying too heavily on any single neuron.
* L2 regularization (also known as weight decay): adds a penalty term to the loss function that is proportional to the magnitude of the model's weights.
* L1 regularization: adds a penalty term to the loss function that is proportional to the absolute value of the model's weights.

These methods are not mentioned in the provided context, but they are commonly used in deep learning pipelines to prevent overfitting.
Answer:

Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that data augmentation techniques such as scaling, panning, rotation, color transformation, and mosaic were employed to increase the number of training samples and improve model generalization ability. This can be seen as a form of regularization, as it helps to prevent overfitting by increasing the diversity of the training data.

Additionally, the context mentions that the models were trained using a stochastic gradient descent (SGD) optimizer with a learning rate decay, which can also help to prevent overfitting by reducing the learning rate as the model trains.

However, it does not mention the use of dropout or L2 regularization, which are common regularization methods used to prevent overfitting in deep learning models. Therefore, I cannot confirm whether these methods were used in this specific pipeline.
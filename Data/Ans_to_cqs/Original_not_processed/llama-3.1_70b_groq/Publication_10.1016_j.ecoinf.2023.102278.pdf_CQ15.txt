Answer:::

The deep learning pipeline uses the Adam optimizer with a fixed learning rate of 1.0⋅10⁻⁴. This choice was made after comparing with stochastic gradient descent (SGD) with the same learning rate, weight decay of 1.0⋅10⁻⁴, and momentum of 1.0⋅10⁻³. The Adam optimizer was chosen because it outperformed SGD, achieving a 1%–2% increase in accuracy. The parameters for SGD were chosen based on the recommendations by L. Smith (2018).
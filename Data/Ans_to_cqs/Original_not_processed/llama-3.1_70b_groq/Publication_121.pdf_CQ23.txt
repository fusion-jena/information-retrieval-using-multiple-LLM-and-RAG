Answer:::

To ensure the generalizability of the deep learning model, several measures were taken. 

Firstly, the dataset was divided into a training set and a test set in a ratio of 75:25, which is a common practice to evaluate the model's performance on unseen data. The training set consisted of 5789 images, and the test set consisted of 1930 images.

Secondly, the use of transfer learning was employed, where pre-trained models (MobileNetV2 and MobileNetV3) were used as feature extractors. This approach allows the model to leverage the knowledge gained from a large dataset (ImageNet) and adapt it to the new task, which can improve the model's generalizability.

Thirdly, a dropout layer with a dropout rate of 0.2 was added to the model to reduce overfitting. Overfitting occurs when a model is too complex and learns the noise in the training data, resulting in poor performance on unseen data. The dropout layer helps to prevent overfitting by randomly dropping out neurons during training.

However, there is no mention of using cross-validation or stratified splitting in the provided context. Cross-validation is a technique where the model is trained and evaluated on multiple folds of the data to ensure that the results are not biased towards a particular subset of the data. Stratified splitting is a technique where the data is split into training and test sets while maintaining the same class distribution in both sets.
Answer: The text does not explicitly mention how the hyperparameters of the model are optimized. It only mentions that ADAM optimization with a learning rate of 0.0001 is used to find optimal weights in the neural network, but it does not provide information on how the hyperparameters such as the learning rate, batch size, and dropout rate are chosen or optimized. It does not mention grid search, random search, or any other hyperparameter optimization technique.
Answer: 
The provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention that the initial learning rate of 0.001 was exponentially decayed every 3 epochs with a factor of 0.94. This suggests that some form of learning rate scheduling was used, but the specific optimization algorithm is not mentioned.
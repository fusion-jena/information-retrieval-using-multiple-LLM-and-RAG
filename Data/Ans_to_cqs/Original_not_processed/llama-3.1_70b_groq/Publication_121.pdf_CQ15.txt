Answer:::

The deep learning pipeline utilizes two optimization techniques: Stochastic Gradient Descent (SGD) and Adam. 

For the MobileNetV3 model, the optimization technique used is SGD with a learning rate of 0.5 and momentum of 0.9. 

On the other hand, the MobileNetV2 model is optimized using the Adam optimizer. 

Additionally, a dropout layer with a dropout rate of 0.2 is used to reduce overfitting of the model.
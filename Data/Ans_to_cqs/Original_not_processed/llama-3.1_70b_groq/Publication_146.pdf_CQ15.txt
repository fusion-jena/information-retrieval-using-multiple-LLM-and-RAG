Answer:::
The optimization technique applied in the deep learning pipeline is the Adam learning rate optimizer. This choice was made over the static stochastic gradient descent (SGD) due to the findings of previous studies, which proved Adam's usefulness on relatively small datasets (less than 1000 images). The Adam optimizer is an adaptive learning rate algorithm that usually yields higher model accuracies than static ones.
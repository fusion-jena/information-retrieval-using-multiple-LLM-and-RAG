Answer:::
Answer: The optimization technique applied in the deep learning pipeline is the Adam algorithm. The default learning rate parameter for the Adam algorithm was also experimented with, and its decrease was tested to see if it would lead to a significant reduction of loss function fluctuations. However, this decrease did not lead to a significant reduction of loss function fluctuations but increased the number of epochs required for network training.
Unfortunately, the provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. The context primarily focuses on the model selection procedure, hyperparameter tuning, and the performance of different deep learning architectures.

However, it does mention that the data was randomly partitioned into training, validation, and testing sets using the R package "dismo". This suggests that some form of data partitioning or splitting was performed, but it does not provide information on other preprocessing steps such as normalization, scaling, or cleaning.

Therefore, I do not know the specific preprocessing steps involved before training a deep learning model in this context.
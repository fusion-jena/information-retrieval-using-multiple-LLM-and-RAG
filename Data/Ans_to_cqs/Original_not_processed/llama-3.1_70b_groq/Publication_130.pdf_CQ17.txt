Answer:::
The regularization methods used to prevent overfitting in the deep learning pipeline, as mentioned in the provided context, include:

1. Dropout: A dropout layer was added just before the first dense layer to limit overfitting by randomly ignoring units of the CNN (i.e., neurons) during the training process.

2. Limited oversampling: Oversampling was restricted to the training dataset only, and not applied to the validation dataset, to avoid overestimating the model's learning progress.

3. Data augmentation: A data augmentation procedure was used to artificially increase the sample size by applying transformations to an existing set of images.

4. Using images from different days in the training and validation datasets: This was done to limit overfitting caused by having very similar pictures in the training and validation datasets.

5. Not freezing the layers of the network: Although freezing the layers can prevent overfitting, it can also prevent the model from learning key features for performing the classification task. Considering the size of the training datasets, the models were trained without freezing any of the layers of the network.

Note that L2 regularization is not mentioned in the provided context as a method used to prevent overfitting in the deep learning pipeline.
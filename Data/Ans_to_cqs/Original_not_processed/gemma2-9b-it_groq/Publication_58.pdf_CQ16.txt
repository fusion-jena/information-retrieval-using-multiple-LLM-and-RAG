The provided text describes the training process for deep learning architectures and XGBoost but doesn't specify the exact criteria used to determine when training is complete. 

It mentions:

* **Learning rate scheduling:** The learning rate is divided by 10 every 20 epochs until it reaches 10^-6. This is a common technique to help the model converge.
* **Epochs:** Training is performed for a certain number of epochs (iterations over the entire training dataset). The text doesn't state the specific number of epochs used.

However, it doesn't explicitly mention criteria like validation loss plateau, test accuracy, or other common stopping conditions. 

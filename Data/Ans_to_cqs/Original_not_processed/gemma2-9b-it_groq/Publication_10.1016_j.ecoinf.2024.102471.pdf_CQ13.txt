Answer:::
The provided text states that the Adam optimizer is used with an initial learning rate of order 10e-5.  

It also mentions that 30 epochs with a batch size of 1 were chosen as optimal values through manual fine-tuning. 




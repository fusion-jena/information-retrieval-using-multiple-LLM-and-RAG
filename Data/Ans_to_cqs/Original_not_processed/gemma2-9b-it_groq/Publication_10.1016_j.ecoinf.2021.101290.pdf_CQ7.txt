Answer:::

The provided text focuses on event-stream processing, automated annotation, and related topics in biology. It doesn't delve into the specific criteria used for splitting data in deep learning for training, testing, and validation. 

To answer your query, here's a general explanation of data splitting in deep learning:

Data is typically split into three sets:

* **Training set:** This is the largest portion of the data used to train the deep learning model. The model learns patterns and relationships within this data.
* **Validation set:** This set is used to evaluate the model's performance during training. It helps monitor how well the model is generalizing to unseen data and prevents overfitting (where the model performs well on training data but poorly on new data).
* **Test set:** This set is held out entirely from the training process and is only used once at the end to provide a final evaluation of the model's performance on completely unseen data.

The typical split ratios are:

* 70% - 80% for training
* 10% - 15% for validation
* 10% - 20% for testing

The exact split ratios can vary depending on the size and nature of the dataset. 




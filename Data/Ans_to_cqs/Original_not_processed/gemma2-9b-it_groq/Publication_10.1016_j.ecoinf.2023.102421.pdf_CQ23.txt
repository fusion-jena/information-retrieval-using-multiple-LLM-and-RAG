Answer:::
The provided text focuses on the use of XGBoost and LightGBM models, not deep learning models.  Therefore, it doesn't describe measures taken to ensure the generalizability of a deep learning model. 

However, the text does mention the following steps taken to improve the generalizability of the XGBoost and LightGBM models:

* **Data Split:** The dataset was divided into 80% for training/validation and 20% for testing to evaluate the model's performance on unseen data.
* **Cross-Validation:** 10-fold cross-validation was used during training to reduce bias and improve the model's ability to generalize to new data.
* **Hyperparameter Optimization:** Bayesian optimizers (BO) and Tree-based Optimization (TDO) were used to tune the hyperparameters of the models, aiming to find the best performing configuration. 




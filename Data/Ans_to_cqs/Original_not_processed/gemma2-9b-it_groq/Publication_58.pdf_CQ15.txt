Answer:::
The provided text states that all architectures were trained with **stochastic gradient descent (SGD)**.  It also mentions a momentum of 0.9 and a learning rate schedule that divides the learning rate by 10 every 20 epochs until it reaches 10^-6. 

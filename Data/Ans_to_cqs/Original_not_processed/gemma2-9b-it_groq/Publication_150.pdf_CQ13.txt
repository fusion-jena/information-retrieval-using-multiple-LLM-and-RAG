Answer:::
The provided text states that the SGD (stochastic gradient descent) optimizer was used with an initial learning rate of 0.01 for all models except MansNet, which was initially set at 0.1.  The learning rate was reduced by a factor of 10 after 30 epochs.  Batch normalization was also used. 

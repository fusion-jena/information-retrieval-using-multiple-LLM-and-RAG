Answer:::
The provided text focuses on the application of deep learning in various fields like wetland classification, mangrove crown diameter measurement, and tree canopy segmentation.  It doesn't delve into the specific regularization methods used to prevent overfitting in deep learning pipelines. 

However,  common regularization techniques used in deep learning to combat overfitting include:

* **Dropout:** Randomly "dropping out" (ignoring) a percentage of neurons during training. This prevents the network from relying too heavily on any single neuron and encourages the learning of more robust features.

* **L2 Regularization (Weight Decay):** Adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This discourages the weights from becoming too large, effectively preventing overfitting.

* **L1 Regularization (Lasso):** Similar to L2 regularization but uses the absolute value of the weights. This can lead to some weights being driven to exactly zero, effectively performing feature selection.


Let me know if you have any other questions. 


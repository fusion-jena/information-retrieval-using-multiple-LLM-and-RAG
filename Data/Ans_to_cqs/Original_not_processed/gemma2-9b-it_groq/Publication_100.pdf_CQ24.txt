Answer:::
The provided text describes several strategies for handling randomness in the deep learning pipeline:

* **Random Initialization of Model Weights:** Each model in the deep ensemble is initialized with a different set of random weights. This introduces variation in the learning process and helps explore different parts of the solution space.

* **Random Batch Sampling:** During each training iteration, a random batch of data patches is selected. This prevents the model from overfitting to any particular subset of the data and encourages generalization. 

The text doesn't explicitly mention strategies like setting a fixed random seed for reproducibility. 



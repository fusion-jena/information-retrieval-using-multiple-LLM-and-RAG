Answer:::
The provided text describes the use of several metrics to evaluate the deep learning model's performance:

* **IoU (Intersection over Union):** This metric quantifies the overlap between the model's predicted bounding box and the ground truth annotation. An IoU of 0.5 is used as a threshold to determine a match.

* **mAP (Mean Average Precision):** This metric provides an overall score for the model's performance across all species. It considers the average precision at different recall levels.

* **AP (Average Precision):** This metric calculates the average precision for each individual species.

* **Precision-Recall Curve:** This graph visualizes the trade-off between precision and recall at various threshold settings. 


The text also mentions that the model's performance is evaluated based on its ability to achieve 95% precision for each species. 

Answer:::
The provided text focuses on the optimization of machine learning models (Random Forest, Artificial Neural Networks, and LightGBM) and doesn't specify the criteria used for splitting data into training, testing, and validation sets for deep learning. 

To answer your query generally:

Data splitting for deep learning typically involves these sets:

* **Training Set:** The largest portion of the data, used to train the model and adjust its parameters.
* **Validation Set:** A subset used during training to evaluate the model's performance on unseen data and help prevent overfitting (where the model performs well on training data but poorly on new data).

* **Test Set:** Held out entirely from training and validation, used only once at the end to provide an unbiased estimate of the model's final performance.

Common splitting ratios are 70% for training, 15% for validation, and 15% for testing. 

The specific criteria for splitting can vary depending on the dataset size, complexity, and the goals of the project. 


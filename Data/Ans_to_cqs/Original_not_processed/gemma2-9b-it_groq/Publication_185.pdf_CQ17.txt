Answer:::
The provided text mentions two regularization methods used to prevent overfitting:

* **Dropout:**  A dropout ratio of 0.5 is mentioned, indicating that 50% of the neurons in a layer are randomly deactivated during each training iteration. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features.

* **L2 Weight Decay:** The text states that L2 weight decay with a penalty multiplier of 5 Ã— 10-4 is applied. L2 weight decay adds a penalty term to the loss function that is proportional to the square of the magnitude of the weights. This discourages the model from learning weights that are too large, which can lead to overfitting. 



Answer:::
The text states that the researchers evaluated three optimization techniques: Adam, SGD (Stochastic Gradient Descent), and RMSProp. 

They found that Adam generally performed the best, leading to lower RMSE (Root Mean Squared Error) values in most cases, especially when the input time series length was set to 5 days. 


Therefore, Adam was chosen as the optimization technique for training the deep learning models in this study. 

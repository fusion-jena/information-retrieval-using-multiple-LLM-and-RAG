Answer:::
The provided text mentions the use of the Adam optimizer with a learning rate of 0.0001 for training the EPDNet model. 

For training the SegFormer model, the text states that the AdamW optimizer is used with a momentum parameter (β1) set to 0.9, (β2) set to 0.999, and ε set to 1e-8.  A cosine annealing schedule is also employed for learning rate scheduling. 




Answer:::
The provided text specifically mentions the following hyperparameters used in the deep learning models:

* **Learning rate:** This controls the step size during model training. The text highlights its importance and shows how optimizing it using Hyperband and Bayesian Optimization techniques led to better model performance. 
* **Batch size:**  This determines the number of samples processed in each training iteration. While mentioned as a hyperparameter, the text doesn't delve into its optimization in this specific context.
* **Dropout:** This regularization technique randomly drops out neurons during training to prevent overfitting.  The text mentions it as a common hyperparameter for DNNs but doesn't discuss its optimization in this case.

The text focuses primarily on learning rate optimization and doesn't explicitly list all hyperparameters used for the EfficientNetV2B0 and MobileNetV2 models. 




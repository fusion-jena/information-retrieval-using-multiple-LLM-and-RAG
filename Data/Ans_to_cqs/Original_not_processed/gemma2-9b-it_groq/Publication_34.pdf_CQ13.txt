Answer:::
The provided text states that the models were fitted with a uniform distribution U(0.03, 0.03) for initialization of parameters.  

The optimization algorithm used was stochastic gradient descent with the following hyperparameters:

* **Momentum:** 0.9
* **Batch size:** 50
* **Initial learning rate:** 10<sup>-8</sup> 


The text also mentions that the initial learning rate was critical for good optimization behavior and that a learning rate that was too large could lead to divergence of the training loss, while a learning rate that was too small would result in slow learning. 

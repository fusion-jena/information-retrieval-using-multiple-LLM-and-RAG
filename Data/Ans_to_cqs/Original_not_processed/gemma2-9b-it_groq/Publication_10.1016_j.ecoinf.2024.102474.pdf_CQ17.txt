Answer:::
The text mentions the use of L2 loss during training. L2 regularization is a common technique to prevent overfitting. It adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to learn smaller weights, which can help generalize better to unseen data. 

The text does not explicitly mention other regularization methods like dropout. 




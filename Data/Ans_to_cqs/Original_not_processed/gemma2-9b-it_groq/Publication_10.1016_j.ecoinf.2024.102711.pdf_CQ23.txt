Answer:::
To enhance the model's generalizability, the researchers employed k-fold cross-validation.  

They divided the development dataset (excluding the evaluation data) into five equal groups (k=5). In each iteration, one group served as the tuning dataset, while the remaining four groups were used for training. This process was repeated five times, with each group taking turns as the tuning dataset. The average performance across these five iterations provided an estimate of the model's performance on unseen data. 

Additionally, the authors aimed to utilize all development data records for training to further improve generalization. 

Answer:::
The provided text lists several optimization techniques used in deep learning:

* **Stochastic Gradient Descent (SGD)** (Herbert and Sutton, 1951)
* **Adaptive Gradient (AdaGrad)** (Duchi et al., 2011)
* **Root Mean Square Prop (RMSProp)** (Tieleman and Hinton, 2012)
* **Adam** (Kingma and Ba, 2014) 


These algorithms are employed to adjust the parameters of a neural network during the training process, aiming to minimize the loss function and improve the network's performance. 

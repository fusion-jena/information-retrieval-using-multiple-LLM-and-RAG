Answer:::
The provided text focuses on hyperparameter optimization for Convolutional Neural Networks (CNNs) and doesn't explicitly mention specific regularization methods used to prevent overfitting. 

However, it does state: "Dropout is used to help the model reduce the gap between the training and validation results and improve its generalizability."

Dropout is a common regularization technique that randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features, thus reducing overfitting. 

The text doesn't discuss other regularization methods like L2 regularization (adding a penalty term to the loss function based on the magnitude of the weights) or L1 regularization (similar to L2 but using the absolute value of the weights). 




Answer:::
The provided text mentions the use of **dropout** as a regularization method in the deep learning pipeline. 

Specifically, it states: "Additionally, Adam optimizer was selected empirically with weight decay equal to 0.9 and learning rate equal to 10âˆ’3. As loss function, we used the binary cross-entropy. **Additionally, a dropout rate of 0.2 in the final FC layer** was used."


The text does not mention other regularization methods like L2 regularization. 

Answer:::
The provided text doesn't explicitly state the specific strategy used to monitor model performance during training. 

However, it does mention the following:

* **Training, validation, and test sets:** The IP102 dataset was partitioned into these sets. This suggests a common practice of using the validation set to evaluate the model's performance during training and make adjustments as needed.
* **Standard evaluation metrics:** Accuracy, macro-precision, macro-recall, and macro-F1 score were employed to assess the models. These metrics would likely be calculated on the validation set during training.
* **Weights updated on training set:** The text states that the parameters of the basic models were updated using only the training set. This implies that the validation set was primarily used for monitoring performance and not for directly influencing model updates. 


Based on these clues, it can be inferred that a standard approach of monitoring performance on the validation set using the mentioned metrics was likely implemented. 


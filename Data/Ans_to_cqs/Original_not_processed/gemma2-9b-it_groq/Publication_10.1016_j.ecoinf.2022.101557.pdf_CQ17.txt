Answer:::
The text mentions the use of **dropout** as a regularization method to prevent overfitting. 

Dropout is implemented with a rate of 0.3, meaning that during each training step, 30% of the neurons in the network are randomly "dropped out" (i.e., their outputs are ignored). This forces the network to learn more robust features that are not reliant on any single neuron. 

The text does not mention any other regularization methods, such as L2 regularization. 

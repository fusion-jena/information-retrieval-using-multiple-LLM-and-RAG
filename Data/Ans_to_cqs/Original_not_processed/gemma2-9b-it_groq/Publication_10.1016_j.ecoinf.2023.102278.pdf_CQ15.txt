Answer:::
The provided text states that the Adam optimizer was used for training the models.  It also mentions that stochastic gradient descent (SGD) was tested but achieved lower accuracy compared to Adam. 



Answer:::
The provided text mentions the use of the following optimization techniques:

* **Momentum optimiser:**  A specific type of optimization algorithm used to update model weights during training. It incorporates a fraction of the previous update into the current update, helping to accelerate convergence and overcome local minima. 


The text does not explicitly mention other common optimization techniques like SGD (Stochastic Gradient Descent) or Adam. 

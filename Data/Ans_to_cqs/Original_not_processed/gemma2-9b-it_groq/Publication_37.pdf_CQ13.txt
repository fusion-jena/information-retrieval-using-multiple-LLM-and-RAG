Answer: The text states that the model uses a constant learning rate of 0.01 and optimizes cross-entropy loss via mini-batch stochastic gradient descent.  It also mentions dropout with a probability of 0.5 applied to the input of the last two layers and L2-regularization with a penalty factor of 0.001 applied to the weights of the last two layers.  The model is trained for 100 epochs. 

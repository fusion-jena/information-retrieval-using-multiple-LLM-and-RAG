The provided text mentions the use of Batch Normalization (BNorm) layers to prevent overfitting.  

It states: "We add new kind of BNorm layers behind the 22 convolutional layers to prevent overfitting when we trained models using BatNet". 


The text does not mention other regularization methods like dropout or L2 regularization. 

Answer:::
The provided text focuses on traditional machine learning models (XGBoost, LightGBM) and doesn't delve into the specifics of data splitting for deep learning models. 

However, the general principles apply.  

Data is typically split into three sets:

* **Training set:** Used to train the model. This is the largest portion of the data (e.g., 70-80%).
* **Validation set:** Used to evaluate the model's performance during training and tune hyperparameters. It's a smaller portion (e.g., 10-15%).
* **Test set:** Used only once at the end to provide an unbiased estimate of the model's performance on unseen data. This set is kept separate throughout the training process (e.g., 10-15%).

The specific percentages can vary depending on the dataset size and the complexity of the model. 


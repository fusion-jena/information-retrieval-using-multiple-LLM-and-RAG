Answer:::
The provided text describes the process of splitting the data for training a deep learning model, but it doesn't explicitly state the criteria used. 

However, we can infer some information:

* **Training dataset:** This dataset is used to train the model and consists of the majority of the available data. The text mentions using 900 images for training and 100 for validation for sociable weavers and great tits.
* **Validation dataset:** This dataset is used to evaluate the model's performance during training and to prevent overfitting. It's a separate set of data that the model doesn't see during training. The text states that 100 images were selected for the validation dataset.
* **Test dataset:** The text doesn't mention a separate test dataset. In general, a test dataset is used *after* training to get a final evaluation of the model's performance on unseen data. 

Therefore, based on the text, the data is split into training and validation sets. The criteria for splitting seem to be a proportion of the data (90% for training, 10% for validation) and the need to ensure the validation set is independent of the training data. 

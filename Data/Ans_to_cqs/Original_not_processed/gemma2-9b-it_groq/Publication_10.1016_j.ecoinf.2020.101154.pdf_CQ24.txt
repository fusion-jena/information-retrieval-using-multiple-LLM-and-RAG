Answer:::

The provided text focuses on data splitting and hyperparameter tuning techniques for a Gradient Boosting Regression Tree (GBRT) model, not deep learning. Therefore, it doesn't offer information about strategies for handling randomness in a deep learning pipeline. 

Strategies commonly used in deep learning to manage randomness include:

* **Setting a random seed:** This initializes random number generators used in various parts of the deep learning process (e.g., weight initialization, data shuffling) to ensure reproducibility.

* **Data augmentation:**  Artificially increasing the size and diversity of the training dataset by applying random transformations (e.g., rotations, flips, crops) to existing data. This helps reduce overfitting and improves generalization.
* **Dropout:** Randomly "dropping out" (ignoring) a percentage of neurons during each training iteration. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features.
* **Batch normalization:**  Normalizing the activations of neurons within each mini-batch during training. This helps stabilize training and accelerate convergence.




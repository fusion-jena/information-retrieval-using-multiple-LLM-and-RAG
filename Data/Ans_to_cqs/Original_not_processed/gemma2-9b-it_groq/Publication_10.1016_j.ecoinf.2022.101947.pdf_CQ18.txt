Answer:::
The provided text describes a data validation process to determine the optimal size for training and test sets, but it doesn't detail the specific strategy used to monitor model performance *during* training. 

We know the following:

* **Holdback trial partitioning:** The researchers used a holdback trial partitioning method to evaluate the model's performance. This involves splitting the data into training and test sets multiple times and iteratively assessing the model's predictions against the known test set size.
* **Classification accuracy:** The text mentions that optimal classification accuracy was achieved with a specific test/training set ratio. This suggests that accuracy was likely a key metric used to monitor performance during training.

However, the text doesn't elaborate on other potential strategies like:

* **Loss function:**  What loss function was used to measure the difference between predicted and actual outcomes during training?
* **Validation set:** Was a separate validation set used to tune hyperparameters and prevent overfitting?
* **Early stopping:** Was early stopping implemented to halt training when performance on the validation set plateaued or started to decline? 


Without further information, we can't definitively state the precise strategy used to monitor model performance during training. 

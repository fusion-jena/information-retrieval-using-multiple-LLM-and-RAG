Answer:::
The provided text mentions that Stochastic Gradient Descent (SGD) is commonly used to update the weights of deep CNNs during training.  The formula for SGD is also given: 

wt+1 = µwt − α∆J(wt)

where µ is the momentum weight, α is the learning rate, wt are the current weights, and ∆J(wt) is the gradient of the loss function with respect to the weights. 


The text does not mention any other optimization techniques like Adam. 

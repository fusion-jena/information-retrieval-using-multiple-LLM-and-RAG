Answer:::
The provided text focuses on the performance of CNNs for age estimation in fish and mentions the potential benefits of using ensembles of CNNs. It doesn't delve into specific strategies for handling randomness in the deep learning pipeline. 

However, it's common practice in deep learning to address randomness through techniques like:

* **Setting a random seed:** This ensures that the random numbers generated during training (e.g., for weight initialization, shuffling data) are the same each time you run the code. This allows for reproducibility of results.

* **Using dropout:** This technique randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents overfitting and introduces some randomness into the network's learning process.

* **Data augmentation:** This involves applying random transformations to the training data (e.g., rotations, flips, crops) to increase the size and diversity of the dataset, mitigating the impact of randomness in the data itself. 




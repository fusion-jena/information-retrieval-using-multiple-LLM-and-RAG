Answer:::
The provided text describes the model selection process and performance evaluation metrics but doesn't explicitly state the strategy used to monitor model performance *during* training. 

We can infer some information:

* **Performance metrics:** The text mentions metrics like R2, NSE, MEF, Z, RMSE, and MAE. These are likely used to evaluate the model's performance at various stages during training.
* **Training and testing data split:** The data was split into 80% training and 20% testing sets. This suggests that the model's performance was likely monitored on the training data during each training epoch.

However, without further details, we cannot definitively say what specific strategy was used to monitor performance during training (e.g., plotting loss curves, using validation sets, etc.). 



Answer:::

The provided text focuses on specific deep learning applications and doesn't explain the criteria for splitting data into training, testing, and validation sets. 

However, here's a general explanation of those criteria:

The goal of splitting data is to train a model that generalizes well to unseen data.  

* **Training set:** This is the largest portion of the data used to train the deep learning model. The model learns patterns and relationships within this data.

* **Validation set:** A smaller portion of the data used to evaluate the model's performance during training. This helps monitor how well the model is learning and prevent overfitting (where the model performs well on training data but poorly on new data).

* **Test set:**  A separate, unseen portion of the data used only once at the end of the training process to provide an unbiased estimate of the model's final performance.

**Common criteria for splitting data:**

* **Percentage-based split:**  A common approach is to split the data into 70% training, 15% validation, and 15% test sets.
* **Stratified sampling:**  If the data has imbalanced classes (e.g., more examples of one category than another), stratified sampling ensures that each class is proportionally represented in each split.
* **Time-based split:** For time-series data, the data might be split based on chronological order.


Let me know if you have any other questions. 


Answer: The provided text mentions dropout as a regularization method used to prevent overfitting.  It states: "Learning rate and dropout rate adopt same value 0.001". 

The text does not discuss other regularization methods like L2 regularization. 

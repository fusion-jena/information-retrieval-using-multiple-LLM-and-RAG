Answer:::
The provided text mentions Dropout as a regularization method used to mitigate overfitting. 

Dropout randomly "drops out" (sets to zero) a percentage of neurons during each training step. This prevents the network from relying too heavily on any single neuron and encourages the learning of more robust features. 

The text does not mention L2 regularization. 



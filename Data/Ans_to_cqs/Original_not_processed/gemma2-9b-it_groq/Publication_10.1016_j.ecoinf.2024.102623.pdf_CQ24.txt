Answer:::
The provided text focuses on species distribution modeling using deep learning and doesn't delve into specific strategies for handling randomness within the deep learning pipeline. 

However, it does mention that the performance is "averaged over 10 random seeds." This suggests that the authors are aware of the potential for randomness in the training process and are using techniques like averaging over multiple random seeds to mitigate its impact. 

Common strategies for handling randomness in deep learning include:

* **Setting a fixed random seed:** This ensures that the same random numbers are generated each time the code is run, leading to reproducible results.
* **Using techniques like dropout:** Dropout randomly "drops out" (ignores) a percentage of neurons during training, preventing overfitting and introducing some randomness into the network.
* **Data augmentation:** This involves creating variations of existing data (e.g., by rotating images, adding noise) to increase the size and diversity of the training set, reducing the impact of randomness in the data itself.


It's important to note that these are general strategies, and the specific techniques used in the referenced study might not be explicitly stated in the provided text. 


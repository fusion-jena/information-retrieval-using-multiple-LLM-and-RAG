Answer:::
The provided text mentions the use of Stochastic Gradient Descent (SGD) as the optimization algorithm for training CNNs in two specific settings:

* **No-overfitting:** SGD is used with a learning rate of 0.0001, a weight decay of 0.5, a batch size of 32, and 200 training epochs. Batch normalization and dropout (0.5) are also employed to mitigate overfitting.

* **Overfitting:**  The text states that SGD is used here as well, but it doesn't specify the exact hyperparameters. It only mentions the removal of batch normalization, weight decay, and dropout techniques to induce overfitting.

Additionally, a different optimization technique is described in the context of an attack model:

* **Attack Model:**  The attack model uses SGD with a momentum of 0.9, a learning rate of 0.001, a batch size of 32, and a decay applied every 512 iterations for a total of 2048 iterations. 


The text does not mention the use of other optimization algorithms like Adam. 


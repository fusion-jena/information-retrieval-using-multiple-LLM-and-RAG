Answer: The provided text mentions several metrics used to evaluate the performance of the CNN and XGB models:

* **Overall Accuracy:**  A measure of the percentage of correctly classified samples.
* **Kappa Statistic:**  A measure of inter-rater reliability for categorical items. It accounts for the possibility of agreement occurring by chance.
* **Mean F1-Score:** The harmonic mean of precision and recall. F1-score is a balanced metric that considers both false positives and false negatives.
* **Per-class F1-scores:**  Similar to the mean F1-score, but calculated for each individual class. 

The text also refers to **Producer Accuracy (PA)** and **User Accuracy (UA)**, which are metrics used in confusion matrices to evaluate the performance of a classification model. 



Answer:::

The provided text focuses on specific aspects of machine learning models like XGBoost, MLPNN, and their hyperparameter tuning. It doesn't delve into the strategies used to handle randomness within the broader deep learning pipeline. 

However, we can infer some common practices based on general deep learning knowledge:

* **Random Seed Initialization:**  As mentioned in the context, initial connection weights in MLPNNs are assigned "arbitrarily." This implies the use of a random seed to initialize these weights.  A random seed ensures reproducibility by starting the training process from the same point each time it's run.

* **Dropout:** This regularization technique randomly "drops out" (ignores) a percentage of neurons during each training iteration. This prevents overfitting by forcing the network to learn more robust features.

* **Batch Normalization:** This technique normalizes the activations of each layer, reducing internal covariate shift and stabilizing training. While not directly addressing randomness, it can help mitigate the effects of random noise in the data.

* **Data Augmentation:**  This involves introducing variations (e.g., rotations, flips, crops) to the training data. This increases the diversity of the training set and helps the model generalize better to unseen data, reducing the impact of randomness in the original dataset.


It's important to note that these are general strategies, and specific implementations within a deep learning pipeline can vary. 

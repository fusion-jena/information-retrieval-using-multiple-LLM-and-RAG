Answer:::
The text mentions the use of **dropout** as a regularization method to prevent overfitting.  

Dropout randomly "ignores" a percentage of neurons during each training step. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features. 


The text does not mention the use of L2 regularization. 

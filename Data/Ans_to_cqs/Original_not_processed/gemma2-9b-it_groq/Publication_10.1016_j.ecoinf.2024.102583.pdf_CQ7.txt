Answer:::
The provided text focuses on ensemble methods in landslide susceptibility modeling and doesn't offer information about data splitting criteria for deep learning models.  

To answer your query, here's a general explanation of data splitting for deep learning:

Deep learning models are typically trained using a split of the available data into three main sets:

* **Training set:** This is the largest portion of the data (e.g., 70-80%) used to train the model. The model learns patterns and relationships within this data.
* **Validation set:** A smaller portion of the data (e.g., 10-15%) used to evaluate the model's performance during training. This helps monitor overfitting (when the model performs well on training data but poorly on unseen data) and tune hyperparameters.
* **Test set:**  A separate, held-out portion of the data (e.g., 10-15%) used only once at the end of training to provide an unbiased estimate of the model's final performance on unseen data. 

The choice of splitting ratios can vary depending on the size and nature of the dataset. 


Answer:::
The provided text mentions stochastic gradient descent (SGD) as an optimization algorithm used for training DNNs. It also states that momentum was shown to help overcome SGD's tendency to get stuck in local minima by accelerating convergence. 

The text does not mention other optimization techniques like Adam. 



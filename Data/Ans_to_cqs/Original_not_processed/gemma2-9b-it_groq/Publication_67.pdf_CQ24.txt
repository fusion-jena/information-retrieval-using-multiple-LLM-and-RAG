Answer:::
The provided text focuses on the application of deep learning for tree cover mapping and doesn't delve into strategies for handling randomness within the deep learning pipeline.  

To answer your query about handling randomness in deep learning, here are some common strategies:

* **Random Seed:** Setting a random seed ensures that the initial weights of the neural network and the random shuffling of data during training are reproducible. This allows for consistent experiments and comparisons.

* **Dropout:** This regularization technique randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents overfitting and introduces stochasticity into the network, making it more robust.

* **Batch Normalization:** This technique normalizes the activations of each layer, reducing the internal covariate shift and stabilizing training. It introduces randomness through the normalization process.
* **Data Augmentation:**  Artificially increasing the size and diversity of the training dataset by applying random transformations (e.g., rotations, flips, crops) to existing images. This helps the model generalize better to unseen data.

Let me know if you have any other questions. 

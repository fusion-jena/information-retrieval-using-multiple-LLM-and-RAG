Answer:::
The provided text focuses on machine learning algorithms like random forests and generalized linear models, and doesn't discuss data splitting criteria for deep learning. 

To answer your query about data splitting for deep learning, here's a common approach:

* **Training Set:** This is the largest portion of the data (e.g., 70-80%) used to train the deep learning model. The model learns patterns and relationships within this data.

* **Validation Set:** A smaller portion of the data (e.g., 10-15%) is held out for validation. During training, the model's performance is evaluated on this set after each training epoch. This helps monitor the model's progress and prevent overfitting (where the model performs well on training data but poorly on unseen data).

* **Test Set:**  A separate portion of the data (e.g., 10-15%) is kept completely separate from both training and validation. This set is only used once at the end of training to provide an unbiased evaluation of the final model's performance on unseen data. 

The goal is to choose these splits carefully to ensure that the training, validation, and test sets are representative of the overall data distribution. 


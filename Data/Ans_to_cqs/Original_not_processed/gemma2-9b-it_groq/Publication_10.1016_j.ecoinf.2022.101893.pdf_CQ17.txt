Answer:::
The provided text mentions the use of **dropout** as a regularization method. 

Dropout is a technique where randomly selected neurons are "dropped out" (set to zero) during each training iteration. This prevents the network from relying too heavily on any single neuron and encourages the learning of more robust features. 

The text does not mention any other regularization methods like L2 regularization. 



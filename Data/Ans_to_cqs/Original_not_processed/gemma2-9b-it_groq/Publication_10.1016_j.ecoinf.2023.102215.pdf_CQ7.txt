Answer:::
The provided text focuses on the architecture and training of a specific deep learning model called TrunkNet and doesn't mention the criteria used to split data for training, testing, and validation. 

However, in general, the data for deep learning models is typically split into these sets:

* **Training set:** This is the largest portion of the data used to train the model. The model learns patterns and relationships within this data.
* **Validation set:** This set is used to evaluate the model's performance during training. It helps to tune hyperparameters and prevent overfitting (where the model performs well on the training data but poorly on unseen data).
* **Test set:** This set is held out entirely from the training process and is only used to evaluate the final performance of the trained model on unseen data.

The exact proportions of these sets can vary depending on the size of the dataset and the specific task. A common split is 70% for training, 15% for validation, and 15% for testing. 




The provided text states that the performance of the deep learning models is evaluated using the following metrics:

* **Overall Accuracy (OA)**:  A measure of the overall correctness of the model's predictions.
* **Kappa**: A statistic that measures inter-rater reliability for categorical items. It's often used to assess the agreement between two raters who each classify items into mutually exclusive categories.
* **User's Accuracy (UA)**:  A measure of the proportion of correctly classified positive instances out of all instances predicted as positive.
* **Producer's Accuracy (PA)**: A measure of the proportion of correctly classified positive instances out of all actual positive instances.
* **F1-Score**: The harmonic mean of precision and recall, providing a balanced measure of a model's performance. 



Answer:::
The provided text focuses on the performance comparison of a specific deep learning model, EMViT-Net, and doesn't delve into the strategies used to handle randomness in the deep learning pipeline. 

However, it's common practice in deep learning to address randomness through techniques like:

* **Setting a fixed random seed:** This ensures that the random numbers generated during processes like weight initialization, data shuffling, and dropout are consistent across multiple runs. This helps in reproducibility and allows for controlled experimentation.

* **Using deterministic algorithms:**  Some deep learning operations, like matrix multiplication, are inherently deterministic. Utilizing these algorithms where possible can minimize the impact of randomness.

* **Regularization techniques:** Techniques like dropout and weight decay introduce controlled randomness during training, which can help prevent overfitting and improve generalization.


Keep in mind that some level of randomness is often beneficial in deep learning, as it introduces exploration during the training process and prevents the model from getting stuck in local optima. 


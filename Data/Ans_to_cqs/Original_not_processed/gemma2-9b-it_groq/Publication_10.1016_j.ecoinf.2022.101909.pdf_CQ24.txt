Answer:::

The provided text focuses on the application of deep learning in bird species identification and acoustic phenology. It doesn't delve into the specific strategies employed to handle randomness within the deep learning pipeline. 

However, it's common practice in deep learning to address randomness through techniques like:

* **Setting a fixed random seed:** This ensures that the random initialization of weights and the shuffling of data during training are reproducible. 

* **Using dropout:** This technique randomly "drops out" (ignores) a percentage of neurons during each training step, preventing overfitting and introducing some randomness into the network.

* **Batch normalization:** This method normalizes the activations of neurons within each mini-batch, reducing the internal covariate shift and stabilizing training.

For a more detailed explanation of these strategies and others, you would need to consult deep learning textbooks or research papers specifically addressing the topic of randomness mitigation. 


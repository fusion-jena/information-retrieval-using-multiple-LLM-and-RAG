Answer:::
The provided text mentions several regularization methods used to prevent overfitting in CNNs:

* **Batch normalization:** This technique normalizes the activations of each layer, reducing the internal covariate shift and stabilizing training.

* **Weight decay (L2 regularization):** This method adds a penalty term to the loss function that is proportional to the square of the weight magnitudes. This encourages the model to learn smaller weights, preventing them from becoming too large and overfitting.

* **Dropout:** This technique randomly "drops out" (sets to zero) a percentage of neurons during each training step. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features. 


The text also states that these techniques are removed in the "overfitting" setting to intentionally cause the model to overfit. 


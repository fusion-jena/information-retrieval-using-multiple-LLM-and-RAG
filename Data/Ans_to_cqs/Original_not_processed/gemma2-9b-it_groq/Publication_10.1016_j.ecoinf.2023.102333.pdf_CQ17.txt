Answer:::
The provided text mentions **dropout regularization** as a method used to prevent overfitting in deep learning. 

Dropout works by randomly "dropping out" (ignoring) a percentage of neurons during each training step. This prevents the network from relying too heavily on any single neuron and encourages the learning of more robust features.

The text does not mention **L2 regularization** as a method used. 




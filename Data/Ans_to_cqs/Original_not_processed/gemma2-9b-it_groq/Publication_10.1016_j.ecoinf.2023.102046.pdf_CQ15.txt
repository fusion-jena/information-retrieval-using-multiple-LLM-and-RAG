Answer:::
The text states that AdamW optimizer was used for Hugging Face models (ViT and ResNet), which is the default optimizer in the transformers library by HuggingFace. 

For Inception-V3, the Adam optimizer was used because AdamW is not available in Keras. 

The learning rate was set to 10<sup>-5</sup> for Vanilla Inception-V3 and ViT, and 4 Ã— 10<sup>-4</sup> for Deep Otolith ResNet, as suggested by Politikos et al. (2021). 




Answer:::
The text mentions the use of dropout as a regularization method.  

It states: "All of the first 7 layers of AlexNet have a ReLu non-linearity activation unit associated with them, and the first two fully connected layers (fc{6, 7}) have a dropout layer associated with them, with a dropout ratio of 0.5." 


It does not mention the use of L2 regularization. 

The context provided does not give specific details about how the hyperparameters of the model were optimized. However, it does mention that the model underwent a series of improvements and optimizations. This could potentially include hyperparameter tuning using methods such as grid search, random search, or Bayesian optimization.

Hyperparameter tuning is a common practice in machine learning to find the best set of hyperparameters for a model. Grid search involves systematically trying out different combinations of hyperparameters, while random search selects hyperparameters at random within a given range. Bayesian optimization, on the other hand, uses a probabilistic model to make informed decisions about which hyperparameters to try next.

Without further information, it is difficult to provide a definitive answer to the query. However, it is reasonable to assume that some form of hyperparameter tuning was used in the development of the model, given the emphasis on optimization and improvements.
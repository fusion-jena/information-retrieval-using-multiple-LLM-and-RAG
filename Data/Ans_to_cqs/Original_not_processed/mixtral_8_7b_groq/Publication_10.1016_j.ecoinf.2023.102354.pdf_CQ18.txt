Based on the provided context, there is no information given about the specific strategy implemented to monitor the model performance during training. The text mainly discusses the relevance of drought research for policy and management, as well as the methods used to evaluate drought characteristics.

However, it is generally important to monitor model performance during training in order to ensure that the model is learning effectively and not overfitting or underfitting the data. Common strategies for monitoring model performance during training include using validation sets, early stopping, and learning curve analysis.

Validation sets involve setting aside a portion of the training data to use as a separate set for evaluating the model's performance. This allows you to get an estimate of how well the model is likely to perform on new, unseen data.

Early stopping involves monitoring the model's performance on the validation set during training, and stopping the training process early if the model's performance on the validation set starts to degrade. This can help prevent overfitting, which occurs when the model becomes too complex and starts to memorize the training data rather than learning the underlying patterns.

Learning curve analysis involves plotting the model's performance on the training and validation sets as a function of the amount of training data. This can help you determine whether the model is overfitting or underfitting, and whether increasing the amount of training data or adjusting the model's complexity would improve its performance.

In summary, while the provided context does not give information on the specific strategy used to monitor the model's performance during training, it is generally important to use strategies such as validation sets, early stopping, and learning curve analysis to ensure that the model is learning effectively and not overfitting or underfitting the data.
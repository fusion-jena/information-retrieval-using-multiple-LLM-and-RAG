The provided context does not explicitly mention the methods used to optimize the hyperparameters of the model, such as grid search or random search. However, it does discuss the use of deep learning and neural networks in image signal processing. In the context of deep learning, hyperparameters are often optimized using methods like grid search, random search, or Bayesian optimization (Bergstra et al., 2011). These methods involve systematically searching through a range of hyperparameter values to find the set that yields the best model performance.

Moreover, the context introduces a compact non-local network for predicting parameters in the image signal processing pipeline. This network is trained using optimization algorithms such as Stochastic Gradient Descent (SGD), Adaptive Gradient (AdaGrad), Root Mean Square Prop (RMSProp), and Adam (Duchi et al., 2011; Kingma and Ba, 2014). While these algorithms are primarily used to optimize model weights during training, they can also be employed in the process of hyperparameter optimization.

In summary, although the provided context does not explicitly state the methods used for hyperparameter optimization, it can be inferred that techniques such as grid search, random search, or Bayesian optimization are commonly used in deep learning for hyperparameter tuning. Additionally, optimization algorithms employed during training can also play a role in hyperparameter optimization.

Bergstra, J., Bardenet, R., Bengio, Y., Kégl, B., 2011. Algorithms for hyper-parameter optimization. In: Advances in Neural Information Processing Systems 24. pp. 2546–2554.

Duchi, J., Hazan, E., Singer, Y., 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research 12, 2121–2159.

Kingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
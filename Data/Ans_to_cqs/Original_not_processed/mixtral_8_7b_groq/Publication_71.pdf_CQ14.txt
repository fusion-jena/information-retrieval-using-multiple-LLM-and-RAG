Based on the information provided, there is no explicit mention of how the hyperparameters of the DeepLabV3+ model were optimized. The context does not specify whether a grid search, random search, or any other method was used for hyperparameter tuning.

However, the context does provide the specific values of the hyperparameters used in the DeepLabV3+ training process. These include the learning policy, base learning rate, learning rate decay factor, learning rate decay step, learning power, training number of steps, momentum, train batch size, weight decay, train crop size, last layer gradient multiplier, upsample logits, drop path keep prob, tf\_initial\_checkpoint, initialize\_last\_layer, last\_layers\_contain\_logits\_only, slow\_start\_step, slow\_start\_learning\_rate, fine\_tune\_batch\_norm, min\_scale\_factor, max\_scale\_factor, scale\_factor\_step\_size, atrous\_rates, and output\_stride.

For the Vasiliko site data, the context explains that the training image was divided into four quadrants due to memory issues. The model was then trained sequentially on each quadrant, updating the weights with each new quadrant. The specific patch size, learning rate, and training samples used in this process are provided. However, there is no information on how these parameters were determined or optimized.

In conclusion, while the context provides detailed information about the specific hyperparameters and training process used for the DeepLabV3+ model and the Vasiliko site data, it does not explicitly mention the method used for hyperparameter optimization.
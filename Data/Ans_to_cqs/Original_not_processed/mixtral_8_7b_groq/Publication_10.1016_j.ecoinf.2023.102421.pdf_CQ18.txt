Answer: The strategy implemented to monitor the model performance during training involves dividing the training dataset into two parts, using 80% for training/validation and keeping the remaining 20% as unseen data for testing. To eliminate bias, 10-fold cross-validation was used during the training process, and mean RMSE was used as the objective function. This allowed for a more robust evaluation of the model's performance by averaging the RMSE over multiple iterations and data splits.

In addition, the Wilcoxon Signed-Rank test with paired samples was used to test the differences between the RMSEs of different models. This statistical test helped determine if the differences in performance between models were significant or due to random chance.

Furthermore, the training process included hyperparameter optimization, which involved searching for the best set of hyperparameters for the machine learning models. This optimization process included comparing various versions of Bayesian optimizers and Swarm-based algorithms. The hyperparameters controlled by the optimization algorithm included boosting parameters such as Max_depth, Learning_rate, and gamma, as well as learning task parameters such as loss function type, evaluation metric, and number of iterations.

Overall, the strategy for monitoring model performance during training involved a comprehensive approach that included data splitting, cross-validation, statistical testing, and hyperparameter optimization. This allowed for a thorough evaluation of the model's performance and helped ensure that the final model was robust and well-optimized.
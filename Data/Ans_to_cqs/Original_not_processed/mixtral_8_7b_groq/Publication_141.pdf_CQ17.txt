Answer:::
The regularization methods used to prevent overfitting in the deep learning pipeline, as mentioned in the provided context, are dropout and data augmentation.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and less prone to overfitting. In the given context, dropout was used in combination with the ResNet-50 network architecture for the biomonitoring study.

Data augmentation is another regularization technique used to increase the amount of training data by creating modified versions of the images in the dataset. This can include techniques such as rotation, scaling, and flipping, which can help the model generalize better and be less susceptible to overfitting. In the provided context, data augmentation was used in the deep learning pipeline for the Chironomidae biomonitoring study.

It is worth noting that the context does not explicitly mention L2 regularization as a method used to prevent overfitting. L2 regularization, also known as weight decay, is a technique that adds a penalty to the loss function based on the size of the weights in the model. This can help to reduce the size of the weights and prevent overfitting, but it is not mentioned as being used in this particular deep learning pipeline.
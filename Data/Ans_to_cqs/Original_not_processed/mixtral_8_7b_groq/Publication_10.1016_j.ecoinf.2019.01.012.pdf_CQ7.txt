Answer:::

The criteria used to split the data for deep learning model training involves dividing the collected tiles into three sets: training, validation, and test set. Initially, a random sample of 10% is taken from the total number of tiles as the validation set, which is used during training for the fine-tuning of hyperparameters and model selection. After finding the best model setup, its prediction error is assessed once again on the test set. The remaining 80% of the tiles are used as the training set. To further increase the number of sample tiles, data augmentation is performed on the training set. This is a common practice in machine learning to artificially increase the number of training samples by applying slight transformations to the original data.

Additionally, to prevent spatial autocorrelation between neighboring tiles, all retrieved tiles from the 50 training photographs are collected in one folder and shuffled before being split into training, validation, and test sets. The test set consists of 10% of the total number of tiles, which was separated before training.

The performance of the final model is tested on two different data sets: 1) on individual tiles and 2) on whole repeat photographs. Prediction accuracy on individual tiles is calculated using the 5796 tiles from the test set. The accuracy on whole repeat photographs is evaluated based on the image pairs of the second set of photographs, and the classification results for each of these 34 images are compared to the corresponding manual classification (reference data). A confusion matrix is prepared for each photograph individually, consisting of pixel numbers for true positives, true negatives, false positives, and false negatives.

The choice of the best model is based on two parameters: accuracy and loss. Loss serves as a measure of how far model predictions differ from the actual class. Model accuracy and loss are calculated for both the training and validation sets.
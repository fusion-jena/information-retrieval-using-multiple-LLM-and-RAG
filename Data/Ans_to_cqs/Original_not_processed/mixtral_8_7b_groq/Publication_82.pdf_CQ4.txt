Based on the provided context, there is no explicit information about the data augmentation techniques applied in the deep learning pipeline. Data augmentation techniques such as flipping, rotating, and scaling are often used to increase the size and variability of the training set in deep learning. These techniques can help improve the model's ability to generalize and reduce overfitting.

However, the context does provide details about the experimental configurations used in the study, including the choice of deep learning architecture, training mechanism, dataset type, and train-test set distribution. The architectures used are AlexNet and GoogLeNet, which are pre-trained models that have been widely used in computer vision tasks. The training mechanisms used are transfer learning and training from scratch. The dataset types used are color, grayscale, and leaf segmented. The train-test set distributions used are various combinations of 80-20, 60-40, 50-50, 40-60, and 20-80.

Additionally, the context provides details about the training parameters used in the study, including the number of epochs, learning rate, momentum, weight decay, gamma, and batch size. These parameters are important for optimizing the training of the deep learning models.

Therefore, while the context does not provide explicit information about the data augmentation techniques used in the study, it does provide details about other important aspects of the deep learning pipeline.
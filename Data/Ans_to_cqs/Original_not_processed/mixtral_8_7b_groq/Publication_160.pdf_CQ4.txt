Answer:::

The data augmentation techniques applied in the deep learning pipeline include random rotations, brightness scaling, and horizontal/vertical flipping. These techniques were used to compensate for class imbalance and improve the generalization of the models. Specifically, random rotations between 0 and 180 degrees, brightness scaling between 0.4 and 1.6, and horizontal/vertical flipping were applied to all patches on the fly as data was fed to the networks. These techniques were used during the training of ResNet18 models, and the same optimizer and parameters were used as in previous trainings. The trainings lasted for at least 100 epochs and were stopped when validation accuracy did not improve for 25 epochs.

These data augmentation techniques are commonly used in deep learning to increase the amount of training data, reduce overfitting, and improve model performance. Random rotations help the model to be invariant to the orientation of the input data, brightness scaling helps the model to be invariant to changes in lighting conditions, and horizontal/vertical flipping helps the model to be invariant to changes in the position of the input data.

It is worth noting that there are other data augmentation techniques that could be used to further improve the performance of the models. For example, further modifications to the layers, such as altering the order of convolutions in residual blocks or widening the network, could be explored. Additionally, other data augmentation techniques such as cropping, padding, and adding noise could also be considered.

Another important aspect to consider is the batch size, which can have an impact on the performance of the model. In the study, different batch sizes were used for different ResNet architectures, such as 16 for ResNet152-224, 128 for ResNet50-128 and ResNet18-128, and 200 for ResNet18-224. The batch size can affect the convergence of the model, the generalization, and the memory usage.

In conclusion, the data augmentation techniques used in this deep learning pipeline include random rotations, brightness scaling, and horizontal/vertical flipping. These techniques were used to compensate for class imbalance and improve the generalization of the models. However, there are other data augmentation techniques and modifications to the layers that could be explored to further improve the performance of the models.
The provided context discusses the use of Transfer Learning, a technique that leverages the knowledge gained from one classification task to improve the performance of a model in another related task. However, it does not explicitly mention any optimization techniques such as Stochastic Gradient Descent (SGD) or Adam.

Nonetheless, it can be inferred that some form of optimization algorithm was used during the training of the models, as the text mentions the training of models using TensorFlow, a deep learning platform that supports various optimization algorithms.

For instance, TensorFlow provides several optimization algorithms, including SGD, Adam, and Adagrad, among others. These optimization algorithms are used to minimize the loss function and update the model's parameters during training.

Therefore, while the specific optimization technique is not mentioned in the provided context, it is reasonable to assume that one or more of the optimization algorithms supported by TensorFlow were used during the training of the models.
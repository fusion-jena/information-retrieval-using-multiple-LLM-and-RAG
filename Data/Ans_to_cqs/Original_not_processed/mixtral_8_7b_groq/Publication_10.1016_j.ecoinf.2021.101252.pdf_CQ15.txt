The provided context discusses the use of AutoML in deep learning models, which includes the optimization of hyperparameters. However, it does not explicitly mention the specific optimization techniques used for updating model weights, such as Stochastic Gradient Descent (SGD) or Adam.

In general, deep learning models use optimization algorithms to minimize the loss function and update model weights during training. Some common optimization techniques include:

1. Stochastic Gradient Descent (SGD): This is a simple optimization algorithm that updates model weights based on the gradient of the loss function with respect to each weight. SGD uses a single training example at a time to calculate the gradient, making it computationally efficient.
2. Momentum: Momentum is a technique that can be added to SGD to improve convergence by incorporating the previous weight updates into the current update. This helps the model overcome local minima and move towards the global minimum.
3. Adagrad: Adagrad is an optimization algorithm that adapts the learning rate for each weight based on the historical gradient information. This allows the algorithm to learn faster for weights with infrequent updates and slower for weights with frequent updates.
4. Adam: Adam is an optimization algorithm that combines the ideas from momentum and Adagrad. It calculates the exponentially decaying averages of the gradient and squared gradient, and uses these values to adapt the learning rate for each weight.

The choice of optimization technique depends on the specific deep learning model and the characteristics of the dataset. In some cases, the default optimization algorithm provided by the deep learning library may be sufficient. However, in other cases, it may be necessary to experiment with different optimization techniques to find the one that works best for the specific problem.

Therefore, while the provided context discusses the use of AutoML for hyperparameter optimization in deep learning models, it does not explicitly mention the specific optimization techniques used for updating model weights. Common optimization techniques include SGD, momentum, Adagrad, and Adam, and the choice of optimization technique depends on the specific deep learning model and the characteristics of the dataset.
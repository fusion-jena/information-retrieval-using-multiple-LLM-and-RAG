Based on the provided context, there is no explicit information about the specific criteria used to determine when the training of the models is complete. However, it is mentioned that the performance and accuracy of each model were evaluated and compared using AUC (area under the receiver operating curve) and TSS (true skill statistic). It can be inferred that the training of the models might have been stopped based on the performance on the validation set, looking for a plateau or improvement in the AUC and TSS values.

In general, when training machine learning models, it is common to monitor the performance on a validation set during the training process. The training is typically stopped when the performance on the validation set does not improve for a certain number of epochs or iterations, indicating that the model is starting to overfit the training data. This is often referred to as early stopping.

In the context of the models mentioned in the text (MaxEnt, RF, ANN, MARS, GBM, CTA, FDA, and SRE), each of them has their own specific training procedure and stopping criteria. For example, in some cases, the training might stop when a maximum number of iterations or epochs is reached, or when the gradient of the loss function is smaller than a certain threshold. In other cases, the training might stop when the performance on the validation set does not improve for a certain number of iterations, as mentioned earlier.

In summary, while the specific criteria used to determine when the training is complete are not provided in the context, it can be inferred that the training might have been stopped based on the performance on the validation set, looking for a plateau or improvement in the AUC and TSS values. This is a common practice in machine learning model training, and each model has its own specific training procedure and stopping criteria.
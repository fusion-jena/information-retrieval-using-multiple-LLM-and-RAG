Based on the provided context, there is no explicit information about specific postprocessing steps such as saliency maps, metrics calculation, or confusion matrix after the model training. The text mainly focuses on the dataset, data pre-processing, training, and the comparison of the auto-encoder model with MFCC.

However, it does mention that the model's performance is evaluated on unseen data, which implies that metrics for evaluating the model's performance, such as accuracy, precision, recall, or F1-score, were calculated. It can be inferred that these metrics were calculated to assess the model's ability to generalize to new, unseen data.

Additionally, the text states that the extensive annotation of the dataset allowed for the easy evaluation of the feature representation produced by the auto-encoder. This suggests that some form of evaluation or analysis was performed on the model's output, although the specific methods are not mentioned.

In summary, while the text does not explicitly mention postprocessing steps like saliency maps, metrics calculation, or confusion matrix, it can be inferred that performance metrics were calculated to evaluate the model's ability to generalize to new data.
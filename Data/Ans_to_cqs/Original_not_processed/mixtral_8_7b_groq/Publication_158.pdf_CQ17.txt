Answer:::
The deep learning pipeline described in the given context employs several regularization methods to prevent overfitting. 

Firstly, L2 weight regularization is used with a factor of 0.0001 for the ResNet model. This method adds a penalty to the loss function that is proportional to the square of the magnitude of the weights, which encourages the model to have smaller weights and thus reduces overfitting.

Secondly, batch normalization is applied after all convolution or dense layers, except for the final output layer. Batch normalization normalizes the inputs of each layer to have zero mean and unit variance, which can help reduce overfitting by making the model less sensitive to the specific distribution of the training data.

Thirdly, dropout layers are added in an attempt to improve generalization. Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model less reliant on any individual neuron.

Lastly, early stopping criteria is applied if no improvement is found in 50 epochs. This method stops the training process if the validation loss does not decrease for a certain number of epochs, which can help prevent overfitting by stopping the training process before it starts to overfit the training data.

Therefore, the deep learning pipeline described in the given context uses L2 weight regularization, batch normalization, dropout layers, and early stopping criteria as regularization methods to prevent overfitting.
Answer:::
In the deep learning pipeline, various optimization techniques are employed to effectively train models and optimize their performance. These techniques include Stochastic Gradient Descent (SGD), Adam, and other methods.

Stochastic Gradient Descent (SGD) is a popular optimization algorithm used in deep learning. It updates the model parameters iteratively based on the gradient of the loss function with respect to the parameters. SGD uses a single training example at a time to compute the gradient, making it computationally efficient. However, it might not always converge to the optimal solution due to its stochastic nature.

Adam, which stands for Adaptive Moment Estimation, is another optimization algorithm that combines the advantages of SGD with momentum and root mean square propagation (RMSProp). Adam adaptively adjusts the learning rate for each parameter, considering the historical gradient information. This makes it more robust and efficient than SGD in many scenarios.

In the study by Sitepu et al., 2022, the authors used the Adam optimizer for training their Convolutional Neural Network (CNN) model for bird species classification. They fine-tuned the pre-trained CNN models using hyperparameters optimized through the Hyperparameter Optimization Search (HHOS) approach.

The study by Yang, Harjoseputro, Hu, and Chen, 2022a, also utilized the Adam optimizer for training their deep learning models in their transfer learning approach. They aimed to improve the performance of pre-trained models on a specific task by fine-tuning them using a smaller dataset.

In summary, Stochastic Gradient Descent (SGD) and Adam are two widely used optimization techniques in deep learning pipelines. These methods help adjust the model parameters to minimize the loss function and improve the model's performance. The provided context indicates that the Adam optimizer is more commonly used in recent studies, but SGD remains a fundamental optimization algorithm in deep learning.
Answer:::

The provided context discusses the use of dilated convolution and depthwise separable convolution as methods to reduce computational complexity in a deep learning pipeline, but it does not directly mention any regularization methods used to prevent overfitting, such as dropout or L2 regularization.

Regularization is an important technique in deep learning to prevent overfitting by adding a penalty to the loss function to reduce the complexity of the model. Dropout is a regularization technique that randomly drops out a proportion of the neurons during training, preventing over-reliance on any single neuron and promoting the development of a more robust model. L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights, encouraging smaller weight values and reducing the complexity of the model.

Therefore, based on the provided context, we cannot determine what regularization methods were used to prevent overfitting in the deep learning pipeline. However, it is common practice to use regularization techniques such as dropout and L2 regularization in deep learning pipelines to prevent overfitting.
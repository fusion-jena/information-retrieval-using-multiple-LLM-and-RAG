The provided context discusses machine learning models, including LightGBM, used for predicting canopy cover from planet data. However, it does not mention deep learning pipelines or optimization techniques such as SGD or Adam. Therefore, based on the given information, I cannot provide an accurate answer to the query.

Answer:::
(Note: The following section provides a general explanation of optimization techniques in deep learning pipelines for reference purposes.)

In deep learning pipelines, optimization techniques are crucial for minimizing the difference between predicted and actual values, also known as the loss function. Some commonly used optimization techniques include Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), and Root Mean Square Propagation (RMSprop).

1. Stochastic Gradient Descent (SGD): SGD is a simple optimization algorithm that iteratively adjusts the model's parameters in the direction of the steepest gradient. It calculates the gradient for each training example (or a mini-batch of examples) and updates the parameters accordingly. SGD is computationally efficient but may converge slowly and oscillate around the minimum.

2. Adaptive Moment Estimation (Adam): Adam is an extension of SGD that combines the advantages of two other optimization methods: Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSprop). Adam calculates the exponentially decaying averages of past gradients and squared gradients. It adaptively adjusts the learning rate for each parameter, making it suitable for sparse data and non-stationary objectives.

3. Root Mean Square Propagation (RMSprop): RMSprop is an optimization algorithm that divides the learning rate by an exponentially decaying average of past squared gradients. It is similar to Adagrad but has a separate decay rate for the moving average of the squared gradients. RMSprop is particularly effective for online and non-stationary objectives.

These optimization techniques help deep learning models converge faster and more accurately, preventing overfitting and improving the overall performance of the model.
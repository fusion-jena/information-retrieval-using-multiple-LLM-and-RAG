Based on the provided context, there is no specific information about the criteria used to determine when training is complete, such as a validation loss plateau. The text focuses on the classification validation results of Daniel Sarna-Wojcicki's (DSW) ground truth validation points compared to M.V. Eitzel's (MVE) and Sean Hogan's (SH) 2020 classifications. It also discusses potential improvements for mixed-methods analytical historical ecology and the involvement of different team members in the investigation, resources, validation, and writing - review & editing processes.

However, generally, in machine learning and deep learning models, training is considered complete when the validation loss reaches a plateau or shows minimal improvement over a certain number of epochs. This indicates that the model has learned the underlying patterns in the training data and is unlikely to improve further by training on the same data. Additionally, other criteria like early stopping or monitoring the performance on a separate test dataset can also be used to determine when training is complete.

In the context of the given text, it can be inferred that the classification validation process involved comparing the ground truth validation points with the classifications done by different collaborators. The kappa value, which indicates the agreement between the classifications, was calculated and found to be 0.84 for MVE and 0.74 for SH. These values suggest strong and moderate agreement, respectively, indicating the reliability of the classifications.

In summary, while there is no specific information provided in the context about the criteria used to determine when training is complete, in general, it can be determined based on the validation loss plateau or other performance metrics.
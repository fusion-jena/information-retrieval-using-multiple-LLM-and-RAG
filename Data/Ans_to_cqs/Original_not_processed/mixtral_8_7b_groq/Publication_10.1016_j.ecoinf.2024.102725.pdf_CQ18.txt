Based on the provided context, there is no specific information about the strategy implemented to monitor the model performance during training. However, the text does provide several metrics that are used to evaluate the performance of the models after training is complete. These metrics include the R-squared (R2) score, Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and the Relative Dimensionless Global Error (ERGAS).

The R2 score is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. The MAE measures the average magnitude of the errors between the fused results and observed values, while the RMSE measures the standard deviation of the prediction errors. A lower value for both the MAE and RMSE indicates a better fit of the model to the data. The ERGAS calculates the quality of the fused image as the normalized average error of the fused image bands, which can be used to assess the accuracy of image reconstruction and the smoothness in preserving the spatial resolution.

It can be inferred that these metrics are used to evaluate the performance of the models after training, but the context does not provide information on how the model performance is monitored during training. It's possible that the model performance is monitored using validation data, where the model's performance is evaluated on a separate dataset that was not used in the training process. This approach can help to ensure that the model is not overfitting to the training data and can generalize well to new, unseen data. Additionally, it's also possible that other techniques such as early stopping, learning rate scheduling, and regularization are used to prevent overfitting and improve the model's performance during training. However, without further information, it's not possible to provide a definitive answer.
Based on the provided context, there is no specific information given about the criteria used to split the data for deep learning model training (e.g., train, test, validation).

However, it is mentioned that the study associated 978 hiking trails with textual data, and performed natural language processing (NLP) methods to perform automated text mining in R. It is also mentioned that Word2vec, a widely known word embedding technique, was used for semantic analysis. Word2vec can be carried out in two ways: by creating and training your own word2vec space or by using pre-trained models.

In general, when it comes to splitting data for deep learning model training, there are several common approaches. One common method is to randomly split the data into training, validation, and testing sets. The training set is used to train the model, the validation set is used to tune hyperparameters and select the best model, and the testing set is used to evaluate the final model's performance.

Another approach is to use k-fold cross-validation, where the data is split into k subsets or "folds". The model is then trained on k-1 folds, and the remaining fold is used for testing. This process is repeated k times, with a different fold used for testing each time. The average performance across all k runs is then used as the final performance metric.

The specific criteria used to split the data can depend on various factors, such as the size of the dataset, the complexity of the model, and the specific problem being addressed. It is important to carefully consider the split strategy and ensure that it is representative of the overall dataset.

In summary, while the provided context does not give specific information on the criteria used to split the data for deep learning model training, there are several common approaches to splitting data for deep learning, including random splitting and k-fold cross-validation. The specific criteria used can depend on various factors and should be carefully considered.
The provided context does not contain specific information on how the hyperparameters of the model were optimized in the studies. Hyperparameter optimization is an essential step in machine learning models to ensure the best performance. Common methods for hyperparameter optimization include grid search, random search, and Bayesian optimization.

In grid search, the model trains for each combination of hyperparameters within a predefined range, and the best combination is selected based on the performance metric. Random search, on the other hand, randomly selects hyperparameters from a defined range and trains the model for each selection. Random search has been found to perform better than grid search in some cases, as it explores the hyperparameter space more efficiently.

Bayesian optimization is another method that uses probabilistic models to estimate the performance of the model for different hyperparameters. This method is more efficient than grid search and random search, as it focuses on exploring the most promising regions of the hyperparameter space.

Without specific information from the context, it is not possible to determine which method was used for hyperparameter optimization in the studies. However, it is essential to note that hyperparameter optimization is crucial for achieving optimal model performance.
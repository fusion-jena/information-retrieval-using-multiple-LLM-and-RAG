The provided context discusses the use of Bayesian optimization for hyperparameter tuning in deep neural networks and various applications of deep learning for plant identification. However, it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline.

Nonetheless, it is well-established in the literature that common regularization methods used to prevent overfitting in deep learning include:

1. L1 and L2 regularization: These methods add a penalty term to the loss function, which discourages large weights and promotes weight shrinkage. L1 regularization adds the absolute value of the weights as a penalty, while L2 regularization adds the square of the weights.

2. Dropout: This method randomly sets a fraction of the hidden units to zero during training, which helps prevent overfitting by reducing co-adaptation of feature detectors.

3. Early stopping: This method stops training when the performance on a validation set stops improving, which prevents the model from continuing to fit noise in the training data.

4. Data augmentation: This method generates new training examples by applying random transformations to the existing data, which increases the amount of training data and helps prevent overfitting.

Therefore, while the provided context does not specify the regularization methods used, it is reasonable to assume that one or more of these methods were used to prevent overfitting in the deep learning pipeline.
Based on the provided context, there is no information about the specific techniques used during the preprocessing phase of a deep learning pipeline to address data bias. The context focuses on the use of ensemble methods, specifically bagging and boosting, in the XGBoost model to avoid overfitting and improve model performance.

However, it is important to note that data preprocessing is a crucial step in any machine learning pipeline, including deep learning, to ensure that the data is clean, relevant, and free from bias. Some common techniques used during data preprocessing to address bias include:

1. Stratified sampling: This technique involves dividing the data into subgroups based on specific features or labels and then randomly sampling from each subgroup to ensure that each subgroup is proportionally represented in the training and testing datasets.
2. Oversampling: This technique involves increasing the number of instances of a specific subgroup in the dataset to ensure that it is adequately represented in the training and testing datasets.
3. Undersampling: This technique involves reducing the number of instances of a specific subgroup in the dataset to prevent overrepresentation and bias in the training and testing datasets.
4. Diverse data collection: This technique involves collecting data from a variety of sources and ensuring that the data is representative of the population being studied.

While the provided context does not mention these specific techniques, it is important to consider them during the data preprocessing phase of any machine learning pipeline, including deep learning, to ensure that the data is free from bias and that the resulting model is accurate and reliable.
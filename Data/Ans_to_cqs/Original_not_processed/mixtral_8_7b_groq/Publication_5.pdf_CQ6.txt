Based on the provided context, there is no explicit information about the preprocessing steps taken before training the deep learning model. However, it is mentioned that the deep learning model is a species richness estimation model developed using TensorFlow, an end-to-end open-source platform for deep learning. TensorFlow provides various preprocessing functions for data preprocessing, such as normalization, scaling, cleaning, and feature engineering.

Typically, data preprocessing steps in deep learning include:

1. Data cleaning: This involves removing missing or irrelevant data points that can negatively impact the model's performance.
2. Data normalization: This is the process of scaling numerical data to a common range, often between 0 and 1. Normalization ensures that all features have equal importance and prevents any single feature from dominating the learning process.
3. Data scaling: This is the process of resizing the data to fit the model's input layer. For example, if the input layer expects data of size 28x28, and the input data is of size 100x100, data scaling can be applied to resize the data to the required dimensions.
4. Feature engineering: This involves creating new features from existing ones to improve model performance. Feature engineering techniques include one-hot encoding, binning, polynomial features, and interaction features.

In the context of the provided information, it can be inferred that some form of data preprocessing was applied before training the deep learning model. However, the specific preprocessing steps are not mentioned. It is also possible that the data was preprocessed using functions provided by TensorFlow or other data preprocessing libraries.
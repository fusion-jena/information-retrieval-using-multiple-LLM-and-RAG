The hyperparameters of the model are optimized using three different methods in the study: Random Search (RS), Hyperband, and Bayesian Optimization.

Random Search is a simple and effective optimization approach that selects and evaluates a set of hyperparameters randomly to find the set with the best accuracy. It is widely used in machine learning and artificial intelligence applications and is often used as a baseline for comparing other optimization methods. RS suits ill-structured global optimization problems where the objective function may be non-convex and non-differentiable. The study found that RS provided the largest architecture with 34 hidden layers; however, randomly identifying the best hyperparameter combination required additional time.

Hyperband is another optimization method that improved the classification accuracy in the study. It involves making several assumptions about resource allocation that may only hold in some scenarios. Hyperband proved to be the most efficient method, having the lowest computation time compared to the other methods.

Lastly, Bayesian Optimization is a systematic approach towards hyperparameters tuning that can be computationally expensive and may not always converge to the global optimum. The study found that the two main differences between the models were the number of hidden layers and the activation function.

Therefore, the hyperparameters of the model are optimized using Random Search, Hyperband, and Bayesian Optimization methods, each with its strengths and weaknesses.
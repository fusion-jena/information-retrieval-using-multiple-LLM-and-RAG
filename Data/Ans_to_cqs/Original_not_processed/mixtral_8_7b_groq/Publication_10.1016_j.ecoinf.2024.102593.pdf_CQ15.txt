Answer:::

The provided context discusses the training of Deep Neural Network (DNN) models for bioacoustic analysis but does not explicitly mention the optimization techniques used in the deep learning pipeline. However, it can be inferred that some optimization algorithms were likely employed during the training process.

Common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adam, RMSprop, and Adagrad. SGD is a simple optimization algorithm that updates the model parameters by computing the gradient of the loss function concerning each parameter for a single training example at a time. Adam, RMSprop, and Adagrad are adaptive learning rate methods that modify the learning rate for each parameter during training.

In the context, the authors mention training models from scratch using the NIPS4Bplus and BirdCLEF 2020 datasets. They also mention using residual networks (ResNet152-architecture) and a SincNet model. These models have been developed using deep learning frameworks such as TensorFlow or PyTorch, which typically have built-in optimization algorithms like SGD, Adam, and others.

For instance, He et al. (2015) introduced the ResNet architecture, which has been widely adopted in deep learning research and applications. The original paper used SGD with momentum and a weight decay of 0.0001 for optimization. Similarly, Hershey et al. (2017) used the Adam optimizer with a learning rate of 0.001 for training their CNN architectures for large-scale audio classification.

Therefore, while the specific optimization techniques are not mentioned in the provided context, it is reasonable to assume that standard optimization algorithms like SGD, Adam, or RMSprop were used during the training of the DNN models.

References:

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, Sun, Jian, 2015. ‘Deep residual learning for image recognition’. arXiv. <http://arxiv.org/abs/1512.03385>.

Hershey, Shawn, Chaudhuri, Sourish, Ellis, Daniel P.W., Gemmeke, Jort F., Jansen, Aren, Moore, Channing, Plakal, Manoj, et al., 2017. CNN Architectures for Large-Scale Audio Classification. In: International Conference on Acoustics, Speech and Signal Processing (ICASSP). <https://arxiv.org/abs/1609.09430>.
Answer:::

The criteria used to determine when training is complete is not explicitly stated in the provided context. However, it is mentioned that the model was trained further after being partially trained on a large dataset. This implies that some evaluation metric was used to determine when the initial training phase was complete and when the model was ready to be used for self-annotation.

In the context, it is mentioned that a new dataset was annotated by the partially trained model and then manually checked and edited by a human. This suggests that the new dataset was used for evaluation purposes, and the model was further trained when the new dataset was ready.

One possible evaluation metric that could have been used to determine when training is complete is validation loss. Validation loss is a measure of how well the model generalizes to new data, and it is calculated using a separate validation dataset that is not used for training. When the validation loss plateaus or starts to increase, it may indicate that the model is overfitting to the training data, and further training may not improve the model's performance.

Another possible evaluation metric that could have been used is accuracy or precision. Accuracy is a measure of how often the model's predictions are correct, while precision is a measure of how often the model's positive predictions are correct. When the accuracy or precision plateaus or starts to decrease, it may indicate that the model is not improving or is starting to degrade, and further training may not be beneficial.

In summary, while the specific criteria used to determine when training is complete are not mentioned in the provided context, it is likely that some evaluation metric, such as validation loss, accuracy, or precision, was used to assess the model's performance and determine when further training was necessary.
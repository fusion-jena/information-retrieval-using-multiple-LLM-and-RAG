Answer:::
The deep learning model used in the study described in the provided context utilized several hyperparameters to control the training process.

The learning rate is a critical hyperparameter that determines the size of the steps that the model takes during gradient descent. In this study, the learning rate was set using a one-cycle policy, which varies the learning rate between a minimum value of 0.000001 and a maximum value of 0.001 over the course of the training epochs.

The optimizer is another essential hyperparameter that determines how the model updates its weights during training. In this study, the Adam optimizer was used, which is a popular choice for deep learning models due to its ability to adaptively adjust the learning rate during training.

The ResNet50 architecture was used for the model, which is a pre-trained convolutional neural network (CNN) that has been shown to be highly effective for image classification tasks. The model was trained from scratch, meaning that it was initialized with random weights and trained on the provided dataset.

The model was implemented using the R programming language and the keras package, which provides an interface to the TensorFlow deep learning library.

The dataset used to train the model consisted of images of various small mammals and birds, including voles, lemmings, stoats, least weasels, shrews, and birds. The images were resized to 224 x 224 pixels and augmented using shifts, horizontal flips, rotations, zooms, and shears to increase the size of the training dataset.

The training process involved 55 epochs, and the model's performance was evaluated using a validation dataset and an out-of-sample test dataset. After training, the model was retrained using a new set of images taken between summer 2020 and summer 2021.

Overall, the hyperparameters used in this deep learning model include a one-cycle learning rate policy with a minimum learning rate of 0.000001 and a maximum learning rate of 0.001, the Adam optimizer, and the ResNet50 CNN architecture. These hyperparameters were chosen based on the authors' exploration of different options and their evaluation of the resulting model performance.
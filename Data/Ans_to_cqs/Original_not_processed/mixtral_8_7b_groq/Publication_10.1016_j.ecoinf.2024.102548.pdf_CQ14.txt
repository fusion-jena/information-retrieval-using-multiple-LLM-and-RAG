Based on the information provided, there is no explicit mention of how the hyperparameters of the model were optimized. Common methods for hyperparameter optimization include grid search, random search, and more sophisticated techniques like Bayesian optimization or gradient-based optimization.

However, we do have some context about the general parameter settings used in the experiments:

1. GPU acceleration was used to speed up computations.
2. Models were initialized with pre-trained weights from the 'imagenet' dataset.
3. The evaluation of model performance was based on the accuracy metric.
4. Each model was trained for 10 epochs with a batch size of 32.
5. The class mode was set to 'categorical' for multi-class classification tasks.

These settings were consistently applied across all models, ensuring comparability and standardization throughout the study experiments. However, these are not hyperparameters being optimized; instead, they are fixed settings for the experiments.

It is possible that the authors used a default set of hyperparameters provided by a deep learning library or framework, or they might have manually tuned the hyperparameters without describing the process in the text. Unfortunately, based on the given context, it is not possible to provide a definitive answer to the query.
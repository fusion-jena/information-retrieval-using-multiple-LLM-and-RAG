The provided context does not directly discuss strategies employed to handle randomness in deep learning pipelines. However, it does mention the use of a random seed value in the context of audio signal processing.

In the paper "Utility of acoustic indices for ecological monitoring in complex sonic environments" by Ross et al. (2021), the authors use a random seed value to ensure reproducibility in their analysis of acoustic entropy. Specifically, they use a random seed value when generating surrogate data to compare with real-world acoustic data. Surrogate data are generated using a Fourier transform to convert the data to the frequency domain, randomizing the phases, and then converting back to the time domain. By using a fixed random seed value, the authors ensure that the surrogate data generation process is reproducible.

In the context of deep learning, handling randomness is crucial for ensuring the reproducibility and generalizability of models. Several strategies are commonly employed to handle randomness in deep learning pipelines:

1. **Random seed value**: Setting a fixed random seed value ensures that the random number generator used in the pipeline produces the same sequence of random numbers every time the pipeline is run. This can help ensure reproducibility of results, especially when training models.
2. **Data augmentation**: Data augmentation techniques such as random cropping, flipping, and rotation can help increase the size and diversity of the training dataset. These techniques can also help reduce overfitting by introducing randomness into the training process.
3. **Dropout**: Dropout is a regularization technique that randomly sets a fraction of the output of a layer to zero during training. This helps prevent overfitting by introducing randomness into the model and reducing co-adaptation between neurons.
4. **Batch normalization**: Batch normalization scales and shifts the activations of each layer to have zero mean and unit variance. This can help reduce internal covariate shift, which is the change in the distribution of inputs to a layer due to changes in the parameters of previous layers. Batch normalization can also introduce randomness into the model by using a different batch of data for normalization in each training iteration.
5. **Initialization**: The initialization of model parameters can have a significant impact on the training process. Techniques such as Xavier initialization and He initialization can help ensure that the initial weights are set to reasonable values, reducing the impact of randomness on the training process.

In summary, while the provided context does not directly discuss strategies employed to handle randomness in deep learning pipelines, it does mention the use of a random seed value in the context of audio signal processing. In deep learning, several strategies are commonly employed to handle randomness, including setting a fixed random seed value, data augmentation, dropout, batch normalization, and initialization.
Answer: The performance of the deep learning models was evaluated using several metrics, including accuracy, precision, recall, and the F1-score. However, the specific values for precision, recall, and F1-score for each class are not provided in the context.

The context indicates that all models performed well, with macro-F1 averages above 91%. The macro-F1 score is a type of F1-score that calculates the F1-score for each class and then takes the average, giving equal weight to each class. This is particularly useful when dealing with imbalanced datasets, as it treats all classes equally.

The best performing model was Inception-V3, with a macro average F1-score of 0.93. The model size for Inception-V3 was 175 MB. Other metrics used to evaluate the models include accuracy, where Inception-V3 achieved 94%. However, the context does not provide specific accuracy values for the other models.

The context also provides a confusion matrix for the TFLite Inception-V3 model, which shows the number of true positives, true negatives, false positives, and false negatives for each class. This matrix can be used to calculate precision, recall, and F1-score for each class.

In summary, the performance of the deep learning models was evaluated using accuracy, macro-F1 score, and a confusion matrix. However, the specific values for precision, recall, and F1-score for each class are not provided in the context.
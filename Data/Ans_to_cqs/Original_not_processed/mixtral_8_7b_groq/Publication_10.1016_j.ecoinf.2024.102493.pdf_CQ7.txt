Based on the provided context, there is no direct information about the criteria used to split the data for deep learning model training (e.g., train, test, validation). However, the text does provide some insight into how data is processed and prepared for analysis in related research.

In the context of the Optimal Parameter-based Geographical Detector (OPGD) model, data is split into different spatial scales for comparison. Specifically, the Q values of each driver are compared at grid scales of 1 km*1 km, 2 km*2 km, 3 km*3 km, 4 km*4 km, 5 km*5 km, and 6 km*6 km. This process helps determine the optimal sampling grid based on the criterion that the 90% quartile of Q values is the largest.

Additionally, when discussing the discretization of continuous data for the OPGD model, the authors consider a range of breakpoints (from 4 to 13) to avoid information redundancy or insufficiency. This step is crucial for accurately identifying driver explanatory power.

However, these processes are specific to the OPGD model and are not directly related to splitting data for deep learning model training. Common practices for splitting data in deep learning include:

1. Random sampling: randomly splitting the dataset into training, validation, and test sets.
2. Time-based splitting: splitting the dataset based on time, using earlier data for training and validation and later data for testing.
3. K-fold cross-validation: dividing the dataset into 'k' subsets or folds, where each fold is used as the test set once, and the remaining 'k-1' folds are used for training.

Without further information specific to the deep learning model in question, it is impossible to provide a definitive answer. However, the most common criteria for splitting data in deep learning model training are random sampling, time-based splitting, and k-fold cross-validation.
Based on the provided context, there is no explicit information given about the specific criteria used to determine when the training of the model is complete. However, it is mentioned that the model is trained for 150 epochs, but it is not clear if this number is determined based on a validation loss plateau or any other convergence criteria.

In general, training of machine learning models is often stopped when the performance on a validation set stops improving or when a plateau in the validation loss is reached. This approach helps to prevent overfitting, which is a common issue when training models for a long time. In the context provided, dropout is used to prevent overfitting, but the validation loss as a stopping criterion is not mentioned.

It is also possible that the model is trained for a fixed number of epochs, as mentioned in the context. This approach can be useful when there is no clear convergence criterion or when the computational resources are limited. However, this method may not always result in the best possible model, especially if the model starts to overfit or underfit before the training is completed.

In summary, based on the provided context, it is not explicitly stated what criteria are used to determine when the training of the model is complete. However, it can be inferred that dropout is used to prevent overfitting, and it is possible that a fixed number of epochs is used for training. It would be beneficial to include information about the stopping criterion in the context to provide a better understanding of the training process and the model's performance.
Answer:::

Based on the provided context, there is no information available about the code repository for the deep learning pipeline used in the study. The context includes detailed information about the methodology, dataset, and architecture of the convolutional neural network (CNN) used in the study. However, there is no mention of the code repository or where the code can be accessed.

The study was published in the journal Ecological Informatics and the authors used a CNN to classify landslides in repeat photographs. They trained the CNN using ReLU activation for the convolutional layers and an exponential linear unit (ELU) activation on the fully connected layer. The optimizer used was the adaptive ADAM with a learning rate of lr = 0.0001. Training was performed with a batch size of 256 over 90 epochs.

The authors also mentioned that they trained the model from scratch instead of using pre-trained models because they considered their specific classification problem to be too different from pre-trained networks. They had nearly 30,000 labeled samples per class, which they considered large enough for training their own network.

The study used repeat photography to document five years of landsliding after the 2005 Kashmir earthquake. The authors used a tile size of 50 × 50 pixel for training the classifier and applied it on whole repeat photographs, which were also split into 50 × 50 pixel tiles. The classifier predicted the output for each single tile and reassembled them to the original image size.

In summary, while there is no information available about the code repository for the deep learning pipeline used in the study, the context provides detailed information about the methodology and architecture of the CNN used in the study.
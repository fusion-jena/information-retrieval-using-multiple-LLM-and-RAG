Answer:::

The provided context discusses some preprocessing steps taken before training deep learning models, particularly for remote sensing data. However, it does not explicitly mention normalization, scaling, or cleaning.

The data is split into three mutually exclusive parts: a learning set, a validation set, and a test set. This is a common practice in machine learning to ensure the model's performance can be accurately evaluated on unseen data.

For the "central-pixel labeling" architectures, balanced data sets are created from the initial pixels selection used for learning. This balance is achieved using the median frequency balancing method, as other methods showed negligible differences in performance. However, creating balanced data sets could be seen as a form of preprocessing.

The XGBoost model uses neo-channels and multiple texture filters, including dissimilarity, entropy, homogeneity, and mean. These could be considered feature engineering steps, a type of preprocessing where new features are created from the original data to improve model performance.

The context also mentions that the same training data were used for the Deep Learning architectures and the XGBoost model. This implies that the data was preprocessed in the same way for all models, ensuring a fair comparison.

However, the context does not mention any steps for data normalization, scaling, or cleaning. Normalization and scaling are often used to ensure that all features are on a similar scale, which can help some machine learning algorithms converge faster and perform better. Cleaning refers to handling missing values or outliers, which can also affect model performance. Therefore, it is possible that these steps were performed but not mentioned in the provided context.
In the provided context, there is no information about the deep learning pipeline or the regularization methods used to prevent overfitting. However, I can provide some general information about regularization methods used in deep learning.

Regularization methods are used in deep learning to prevent overfitting, which occurs when a model learns the training data too well and performs poorly on unseen data. Overfitting can be reduced by adding a penalty term to the loss function, which encourages the model to have simpler weights and avoid memorizing the training data.

Two common regularization methods used in deep learning are L2 regularization and dropout.

L2 regularization, also known as weight decay, adds a penalty term proportional to the square of the magnitude of the weights to the loss function. This penalty term discourages large weight values, which can lead to overfitting. L2 regularization can be applied to all the weights in the model or only to some of them, depending on the specific implementation.

Dropout is a regularization method that randomly sets a fraction of the activations in a layer to zero during training. This prevents the co-adaptation of the activations and encourages the model to learn more robust features. Dropout can be applied to any layer in the model, but it is typically used in fully connected layers.

In summary, L2 regularization and dropout are two common regularization methods used in deep learning to prevent overfitting. However, the provided context does not contain information about the regularization methods used in the deep learning pipeline.
The text provided does not give specific information about strategies employed to handle randomness in the deep learning pipeline, such as the use of a random seed value. However, it does discuss some strategies used to improve the performance of deep learning models, which can indirectly relate to handling randomness.

One such strategy is the careful selection of batch size. The text mentions that using a larger batch size of 256 or 512 improved the performance of a ResNet18 model on a fine-grained classification task, as opposed to smaller batch sizes used in previous studies. This could be because larger batches are more representative of the intra-class variability, allowing the network to focus on inter-class variance.

Another strategy discussed is the choice of patch size for input images. The text explains that a patch size of 224 Ã— 224 pixels gave the highest micro-F1 score on the test set, as it included enough contextual noise to regularize overfitting and enabled better generalization. This suggests that selecting an appropriate patch size can help reduce the impact of randomness in the data.

Lastly, the text discusses the use of an ensemble network based on four tested patch sizes, which improved classification of the local-SPP. Ensemble methods, which combine the predictions of multiple models, can help reduce the impact of randomness by averaging out the errors and biases of individual models.

In summary, while the text does not directly discuss strategies for handling randomness in the deep learning pipeline, it does touch on strategies for improving model performance through careful selection of batch size, patch size, and ensemble methods. These strategies can indirectly help reduce the impact of randomness in the deep learning process.
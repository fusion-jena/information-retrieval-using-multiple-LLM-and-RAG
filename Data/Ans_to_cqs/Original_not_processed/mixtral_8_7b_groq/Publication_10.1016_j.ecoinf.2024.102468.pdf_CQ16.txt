Based on the provided context, there is no direct information about the specific criteria used to determine when training is complete for the WT-HMM model. However, it does mention that the experiments are performed over different runs, which suggests that some form of cross-validation or repeated training may be used to assess model performance and determine training completion.

In general, when training machine learning models, including hidden Markov models (HMMs) and artificial neural networks (ANNs), training is often considered complete when the validation loss plateaus or stops decreasing significantly. This is because further training beyond this point may lead to overfitting, where the model becomes too specialized to the training data and performs poorly on new, unseen data.

To prevent overfitting, early stopping techniques can be used, where training is halted when the validation loss stops decreasing for a certain number of epochs or iterations. Alternatively, regularization techniques, such as L1 or L2 regularization, can be applied to the model to reduce overfitting and improve generalization to new data.

In the context of the WT-HMM model, it is possible that similar techniques are used to determine training completion and prevent overfitting. However, without more specific information, this is only a hypothesis based on general practices in machine learning model training.
The text provided does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the problem of overfitting in the context of artificial neural networks (ANNs) and the importance of repeated training and testing to develop an optimal neural network.

In the context of deep learning, overfitting is a common problem where the model learns the training data too well, to the point where it performs poorly on unseen data. Regularization methods are techniques used to prevent overfitting by adding a penalty term to the loss function, which discourages the model from learning overly complex patterns in the training data.

Two common regularization methods used in deep learning are L2 regularization and dropout. L2 regularization, also known as weight decay, adds a penalty term proportional to the square of the model's weights to the loss function. This encourages the model to have smaller weights, which in turn reduces the complexity of the model and prevents overfitting.

Dropout, on the other hand, is a regularization technique that randomly sets a fraction of the model's hidden units to zero during training. This prevents the co-adaptation of hidden units and encourages the model to learn more robust features that are useful for making predictions on unseen data.

In summary, while the provided text does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, it does highlight the importance of preventing overfitting in ANNs and the general concept of regularization. L2 regularization and dropout are two common regularization methods used in deep learning to prevent overfitting.
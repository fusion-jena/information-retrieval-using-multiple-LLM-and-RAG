Answer:::

Based on the provided context, there are no explicit criteria mentioned for determining when training is complete in the described machine learning models. However, the text does mention the use of cross-validation to optimize parameters and the calculation of the output error of neurons in the output layer.

In the context of the artificial neural network model, the learning process becomes a backward error propagation process when the output error of the neurons in the output layer does not satisfy the accuracy requirement. The backpropagation algorithm adjusts the weights and biases of the network by computing the gradient of the error function with respect to the weights and biases, and then updating them in the opposite direction of the gradient. This process continues until the error satisfies the accuracy requirement or reaches a predetermined number of iterations.

In the context of the Partial Least Squares (PLS) model, the text mentions the use of cross-validation to optimize the parameters and the calculation of the cumulative explanatory power of the independent and dependent variables. Cross-validation is a technique used to assess the performance and generalization ability of a model by dividing the data into multiple subsets, training the model on one subset, and testing it on another. This process is repeated for each subset, and the overall performance of the model is calculated as the average performance across all subsets.

Therefore, while the provided context does not explicitly mention the use of validation loss plateau as a criterion for determining when training is complete, it does mention the use of cross-validation and the calculation of the output error of the neurons in the output layer as methods for assessing the performance and generalization ability of the model. These methods can be used to determine when training is complete by monitoring the performance of the model on the validation set and stopping the training process when the performance plateaus or starts to degrade.
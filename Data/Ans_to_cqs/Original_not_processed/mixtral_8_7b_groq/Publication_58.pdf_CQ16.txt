Based on the provided context, there are no specific details mentioned about the criteria used to determine when training is complete for the Deep Learning architectures and XGBoost model. However, it is mentioned that the models were trained using stochastic gradient descent with a specific learning rate schedule. According to this schedule, the learning rate is divided by 10 every 20 epochs until it reaches 10âˆ’6. This learning rate decay strategy is often used to help the model converge to a minimum loss value.

Moreover, it is mentioned that the models were trained on balanced datasets using the median frequency balancing method. This method is used to address the issue of class imbalance, which can affect the performance of neural networks. Balancing the dataset can help the model learn the features of each class more effectively.

The models were evaluated using four quality metrics: Overall Accuracy (OA), Producer Accuracy (PA), User Accuracy (UA), and the F1-score. However, there is no information provided about how these metrics were used to determine when training is complete.

Therefore, based on the provided context, it is not possible to determine the specific criteria used to determine when training is complete for the Deep Learning architectures and XGBoost model.
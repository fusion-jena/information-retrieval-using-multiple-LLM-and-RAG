The provided context does not include specific details about the methods used for collecting raw data in the deep learning pipeline. It mainly focuses on the data processing, model architecture, and attention mechanisms used in the research.

However, based on general practices in data science and machine learning, there are several common methods for collecting raw data:

1. Surveys: These are structured sets of questions designed to collect information from a sample of individuals or organizations. Surveys can be conducted in various ways, such as online, on paper, or through face-to-face interviews.

2. Sensors: Sensors are devices that measure physical or chemical properties of the environment and convert them into electrical signals. They can be used to collect data on various factors, such as temperature, humidity, pressure, and chemical concentrations.

3. Public datasets: Many organizations make datasets publicly available for research and development purposes. These datasets can be a valuable resource for deep learning applications, as they provide large amounts of labeled or unlabeled data that can be used to train models.

4. Experiments: Researchers may conduct their own experiments to collect data for their deep learning pipelines. For example, they might set up a controlled environment to measure the effects of different variables on a particular outcome.

5. Scraping: Data can be collected by scraping websites or other digital sources. This involves using automated tools to extract data from online sources and save it in a structured format for analysis.

Without more specific information from the context, it is difficult to say which of these methods were used in the deep learning pipeline described.
The context provided does not give specific information about the hardware resources used for training the deep learning model. However, it is mentioned that the training was conducted over 500 epochs. This suggests that significant computational resources were required for the training process.

Deep learning models like Convolutional Neural Networks (CNNs) can be computationally intensive, especially when training large models over many epochs. Therefore, it is common to use specialized hardware such as Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs) to accelerate the training process.

GPUs are particularly well-suited for deep learning tasks because they are designed to handle parallel computations, which is a key requirement for training large neural networks. TPUs are custom-built hardware developed by Google for machine learning tasks, and they are optimized for tensor operations, which are the basic building blocks of neural networks.

Therefore, while the context does not provide specific information about the hardware resources used for training the deep learning model, it is likely that specialized hardware such as GPUs or TPUs were used to accelerate the training process.
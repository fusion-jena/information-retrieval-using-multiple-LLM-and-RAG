Answer:::

The data was split for deep learning model training using a specific ratio for each subset. The datasets were divided into three exclusive groups: 80% for training, 10% for validation, and 10% for testing. This division allows for a sufficient amount of data to be used for training the model, while also reserving a portion of the data for validating and testing the model's performance.

The datasets used in this study were BirdCLEF2018-Small (BC-S), BirdCLEF2018-Large (BC-L), Lishui-Zhejiang Birdsdata (LS), and Lishui-Zhejiang Birdsdata (LS*). Each dataset had a different number of sounds and categories, which were taken into account during the data splitting process.

For BC-S, there were 3118 sounds in the training set, 308 sounds in the validation set, and 308 sounds in the test set. For BC-L, there were 10,216 sounds in the training set, 1005 sounds in the validation set, and 1005 sounds in the test set. For LS, there were 20,512 sounds in the training set, 2514 sounds in the validation set, and 2513 sounds in the test set. Finally, for LS*, there were 10,279 sounds in the training set, 2514 sounds in the validation set, and 2513 sounds in the test set.

This specific data splitting approach was used to ensure that the model was trained, validated, and tested on a sufficient amount of data, while also avoiding overfitting and underfitting issues. By using a validation set, the model's performance could be evaluated during the training process, allowing for adjustments to be made as needed. Additionally, the test set provided a final evaluation of the model's performance on unseen data.

In summary, the data was split for deep learning model training using a ratio of 80% for training, 10% for validation, and 10% for testing. This approach allowed for a sufficient amount of data to be used for training, while also evaluating the model's performance during and after the training process.
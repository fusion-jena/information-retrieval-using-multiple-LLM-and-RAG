Answer:::

When preparing data for training deep learning models, several preprocessing steps are typically involved, including normalization, scaling, and cleaning. These steps help to enhance the quality of the data, improve model performance, and reduce training time.

Normalization is the process of rescaling numeric columns in the dataset to a common range, often between 0 and 1. This is important because deep learning models are sensitive to the scale of input features. Features with larger scales can dominate the learning process, causing the model to perform poorly on other features. Normalization ensures that all features contribute equally to the learning process.

Scaling is similar to normalization but rescales features to a common range, often specified by the user. This is useful when features have different units or orders of magnitude. For example, in a dataset containing both temperature (in Celsius) and precipitation (in millimeters), scaling can resize both features to a similar range, preventing one feature from dominating the learning process.

Cleaning is the process of identifying and handling missing or invalid data in the dataset. Missing data can be replaced with imputed values, such as the mean or median of the feature, or by using advanced techniques like multiple imputation. Invalid data, such as outliers or incorrect entries, can be corrected or removed, depending on the severity and frequency of the issue.

In the context of the provided pieces of information, the study by Ma, Z. (2023) uses social media data to analyze street view features for identifying urban vitality. While the preprocessing steps are not explicitly mentioned, it can be inferred that data cleaning and normalization were performed. Data cleaning would involve handling missing or invalid data in the social media dataset, while normalization would ensure that the features contributing to urban vitality are on a similar scale.

In the study by Ma, Y., Yang, Y., & Jiao, H. (2021), social media data is used to explore the impact of urban built environment on public emotions. Here, data preprocessing would likely involve cleaning, normalization, and scaling, as the study analyzes both text data (public emotions) and image data (urban built environment). Text data would require cleaning and normalization, while image data would require scaling to ensure that features like building height and green space are on a similar scale.

In summary, preprocessing steps like normalization, scaling, and cleaning are crucial for training deep learning models. These steps help to enhance data quality, improve model performance, and reduce training time. While the specific preprocessing steps are not mentioned in the provided pieces of context, it can be inferred that data cleaning, normalization, and scaling were performed in the studies using social media data.
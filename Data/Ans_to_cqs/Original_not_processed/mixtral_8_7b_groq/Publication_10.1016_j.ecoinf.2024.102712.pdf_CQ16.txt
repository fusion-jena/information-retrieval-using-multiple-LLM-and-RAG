Based on the provided context, there is no explicit information given about the specific criteria used to determine when training is complete in the context of the models and methods discussed. However, it is mentioned that the model performance was evaluated using a leave-one-out cross-validation (LOOCV) approach, which is a technique used to assess the performance and avoid overfitting of a model by iteratively training and testing the model on different subsets of the data.

In LOOCV, the model is trained on all but one of the observations, and then the trained model is used to predict the left-out observation. This process is repeated for each observation in the dataset, and the predictions are compared to the actual values to evaluate the model's performance. This approach can help to identify if the model is overfitting the data, which is when the model performs well on the training data but poorly on new, unseen data. If the model is overfitting, it may indicate that the model is too complex and that training should be stopped or the model should be simplified.

Another common method to determine when training is complete is by monitoring the validation loss, which is the error of the model on a separate validation dataset that is not used for training. If the validation loss starts to increase while the training loss continues to decrease, it may indicate that the model is overfitting and that training should be stopped. This is known as early stopping.

In summary, while the provided context does not give specific information about the criteria used to determine when training is complete, it is mentioned that a LOOCV approach was used to evaluate the model performance and it can be inferred that early stopping or monitoring the validation loss could be used as a criteria to determine when training is complete.
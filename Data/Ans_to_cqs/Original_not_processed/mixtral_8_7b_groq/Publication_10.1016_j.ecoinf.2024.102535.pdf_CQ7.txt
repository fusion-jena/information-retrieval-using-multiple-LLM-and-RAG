Answer:::

The context does not provide explicit information on the criteria used to split the data for training the deep learning models. However, it does mention that each network was trained and executed three times with a different data fold, suggesting that some form of cross-validation may have been used. In cross-validation, the data is divided into a number of subsets, or folds, and the model is trained and tested multiple times, each time with a different fold used as the test set and the remaining folds used as the training set. This approach can help to provide a more robust estimate of the model's performance and can reduce the risk of overfitting. Additionally, the early stopping procedure was used, which involves stopping the training process if the validation loss does not improve for a certain number of epochs (in this case, 10). This can also help to prevent overfitting by stopping the training process before it has a chance to fully memorize the training data.
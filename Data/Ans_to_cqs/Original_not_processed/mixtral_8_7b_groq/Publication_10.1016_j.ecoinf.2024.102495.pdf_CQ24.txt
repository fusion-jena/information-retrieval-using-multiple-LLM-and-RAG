Based on the provided context, there are several strategies employed to handle randomness in the deep learning pipeline. However, the text does not explicitly mention the use of a random seed value.

Firstly, the authors use statistical measures, such as standard deviation, to compare the performance of different configurations in the deep learning model. Configuration 8 was determined as the ideal parameter owing to its reduced standard deviation value. This suggests that the authors are employing a strategy to manage randomness by evaluating the stability and consistency of the model's performance across multiple runs.

Secondly, during the feature extraction phase, the authors use the best performance parameters obtained from previous experiments. This approach reduces the impact of randomness on the feature extraction process by utilizing parameters that have already demonstrated good performance.

Thirdly, the authors use a stratified 10-fold cross-validation method to evaluate the model's performance. This technique involves dividing the dataset into ten subsets, training the model on nine of these subsets, and testing it on the remaining subset. This process is repeated ten times, with each subset used once as the test set. The use of cross-validation helps reduce the impact of randomness on the model's performance by providing a more robust estimate of its generalization ability.

Lastly, the authors tune the hyperparameters of the CNN algorithm using a root mean square propagation (RMSProp) optimizer. The RMSProp optimizer is an adaptive learning rate method that adjusts the learning rate based on the gradient's magnitude. This approach helps manage randomness by providing a more stable and consistent learning rate during the training process.

In summary, while the provided context does not explicitly mention the use of a random seed value, the authors employ several strategies to handle randomness in the deep learning pipeline. These strategies include using statistical measures to compare the stability and consistency of the model's performance, utilizing previously obtained best performance parameters during feature extraction, employing stratified 10-fold cross-validation, and tuning hyperparameters using an adaptive learning rate method.
Based on the provided context, the hyperparameters of the model, such as the learning rate and the number of epochs, are optimized using the Adam optimizer and set to specific values, but there is no explicit mention of the optimization methods like grid search or random search. However, the hyperparameter λ, used to adjust the balance relationship between the nonlinear factors of the activation function and the identity mapping, is dynamically adjusted based on the current number of training rounds (e) and the total number of training rounds (E). This adjustment is done using the formula λ = e^E, which changes the nonlinear factor between the two convolutional layers of the network. Additionally, an innovative method of cascading activation functions is adopted to enhance the nonlinear expression ability of the model. However, the context does not provide information on how the hyperparameter 'a' of the activation function is optimized.

In summary, while some hyperparameters are optimized using specific values and a dynamic approach for λ is mentioned, the explicit optimization methods like grid search or random search are not specified in the provided context.
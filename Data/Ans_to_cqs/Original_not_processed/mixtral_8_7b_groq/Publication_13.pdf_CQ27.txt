Based on the provided context, there is no information given about the specific process followed to deploy the trained deep learning model. However, we can infer some steps that might have been taken based on common deep learning practices.

First, the model would likely have been saved or serialized after training. This involves saving the model's architecture, weights, and other parameters to a file so that it can be loaded and used later without having to retrain the model. TensorFlow provides a function called `save_model` for this purpose.

Next, the serialized model would need to be loaded onto a platform for deployment. This could be a local machine, a server, or a cloud-based platform such as Google Cloud or Amazon Web Services. The choice of platform would depend on factors such as the expected volume of data, the need for scalability, and the available resources.

Once the model is loaded onto the platform, it can be integrated with an application or service that will use it to make predictions. This might involve writing code to preprocess input data, load the model, make predictions, and postprocess the output. The programming language and framework used for this would depend on the platform and the application.

Finally, the deployed model would need to be monitored and maintained to ensure that it continues to perform well as new data is introduced. This might involve tracking metrics such as accuracy and loss, retraining the model periodically, and updating the platform or application as needed.

It's important to note that the provided context does not specify whether any of these steps were taken, so this answer is based on best practices and inferences rather than explicit information.
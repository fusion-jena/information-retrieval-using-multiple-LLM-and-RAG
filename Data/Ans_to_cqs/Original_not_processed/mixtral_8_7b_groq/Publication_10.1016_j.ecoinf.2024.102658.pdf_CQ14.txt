The context provided does not give specific details about how the hyperparameters of the random forest model were optimized. However, it does mention that random forest is used as the baseline model and provides a detailed description of how the random forest model works.

In general, there are several methods for hyperparameter tuning in machine learning models, including grid search, random search, and Bayesian optimization. Grid search involves specifying a range of possible values for each hyperparameter and then training the model with all possible combinations of these values. Random search is similar, but instead of trying every possible combination, it selects a subset of combinations at random. Bayesian optimization uses a probabilistic model to make informed suggestions for which hyperparameter values to try next.

Without more information, it is difficult to say for sure which method was used to optimize the hyperparameters of the random forest model in this study. However, given that the study follows the Bayesian workflow described by Gelman et al. (Gelman et al., 2020), it is possible that Bayesian optimization was used.

It is also worth noting that the study mentions that the final settings of the random forest model were chosen so that the models fit on random-CV features resemble the results from Bayesian and MLE logistic regression. This suggests that some form of hyperparameter tuning was performed to ensure that the random forest model was not overly complex and was able to make realistic predictions.

In summary, while the context provided does not give specific details about how the hyperparameters of the random forest model were optimized, it is possible that Bayesian optimization was used as part of the Bayesian workflow followed in the study. Additionally, some form of hyperparameter tuning was likely performed to ensure that the random forest model was not overly complex and could make realistic predictions.
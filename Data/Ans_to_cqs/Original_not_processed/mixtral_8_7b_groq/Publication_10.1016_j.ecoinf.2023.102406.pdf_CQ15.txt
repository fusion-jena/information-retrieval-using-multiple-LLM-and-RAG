Based on the provided context, the text does not mention any specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adam that are applied in the deep learning pipeline. The context instead focuses on the optimization techniques used in random forests, a type of machine learning algorithm, but not deep learning.

In random forests, the standard optimization techniques used are bagging and tree averaging, rather than pruning trees or limiting tree size, to prevent over-fitting. This approach allows for the creation of the largest trees possible, which results in the best accuracy. Fine-tuning parameters is typically unnecessary to achieve accurate performance with random forests.

In summary, the provided context does not contain information on the optimization techniques used in deep learning pipelines such as SGD or Adam.
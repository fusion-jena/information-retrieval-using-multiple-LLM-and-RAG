The provided context does not include information about regularization methods used in deep learning pipelines, such as dropout or L2 regularization. The context mainly focuses on species distribution modeling and various statistical methods used for predicting and understanding species distributions.

However, I can provide a brief overview of the regularization methods mentioned in the query:

1. Dropout: Dropout is a regularization technique used in deep learning, particularly in neural networks. During training, dropout randomly sets a fraction of input units to zero, effectively preventing over-reliance on any one unit and promoting the development of redundant representations within the network. This helps reduce overfitting and improves the network's generalization performance.

2. L2 Regularization: L2 regularization, also known as weight decay, is a technique that adds a penalty term to the loss function, proportional to the square of the model's weights. This encourages smaller weights during training, which in turn reduces the complexity of the model and helps prevent overfitting.

While these techniques are not explicitly mentioned in the provided context, they are commonly used in deep learning pipelines and can be applied to species distribution modeling or other machine learning tasks when dealing with high-dimensional data or complex models.
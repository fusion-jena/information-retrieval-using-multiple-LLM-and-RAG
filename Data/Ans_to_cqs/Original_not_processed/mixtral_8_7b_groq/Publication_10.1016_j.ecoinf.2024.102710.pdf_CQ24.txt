Based on the provided context, there is no direct mention of strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, the text does describe several instances where randomness is introduced and controlled in the active learning experiments.

First, in the context of sampling strategies, 5% of the samples are selected at random in all cases. This ensures a consistent and reproducible proportion of random samples across different sampling methods.

Second, when emulating an active learning scenario, class labels are hidden from the classifier at first and incrementally revealed for each batch of samples queried by the sampling methods. The batch size is set to 20 samples, and the classifier heads used in active learning are identical to those from the transfer learning training process. This controlled introduction of label information can be seen as a way to manage randomness in the learning process.

Third, in the investigation of uncertainty sampling strategies, the score aggregation methods 'max' and 'mean' are used. This introduces an element of randomness, as the 'max' method selects the sample with the highest uncertainty score, while the 'mean' method calculates the mean uncertainty score across all samples and selects the one with the highest value.

Lastly, the context states that the experiments are carried out using the embedding space of the selected transfer learning model, BirdNet-1. The choice of this specific model implies a level of consistency and reproducibility in the feature embedding process, reducing the impact of randomness.

In summary, while the provided context does not explicitly discuss strategies for handling randomness related to deep learning pipeline settings like random seed values, it does describe controlled instances of randomness in the active learning experiments, such as random sampling, incremental label revelation, and uncertainty score aggregation methods.
Answer: The optimization technique applied in the deep learning pipeline is Stochastic Gradient Descent (SGD). This information can be inferred from the context provided, which mentions that "Stochastic Gradient Descent (SGD) was used as the optimizer for all four models." No other optimization techniques like Adam are mentioned to have been used in the deep learning pipeline.
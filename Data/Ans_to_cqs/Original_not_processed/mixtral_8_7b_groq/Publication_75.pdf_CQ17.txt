Based on the provided context, the specific regularization methods used to prevent overfitting in the deep learning pipeline of SeedNet are not explicitly mentioned. However, it is mentioned that a pooling layer is periodically inserted between successive convolutional layers to reduce the amount of parameters and computation in the network, which can help control overfitting.

In general, deep learning models like SeedNet are prone to overfitting due to the large number of parameters that need to be learned. Regularization techniques are commonly used to prevent overfitting by adding a penalty term to the loss function, which discourages the model from learning overly complex patterns in the training data that may not generalize well to unseen data.

Two commonly used regularization techniques in deep learning are L2 regularization and dropout. L2 regularization adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to learn smaller weights, which can help prevent overfitting. Dropout, on the other hand, randomly sets a fraction of the model's outputs to zero during training. This prevents the model from relying too heavily on any single neuron, which can help reduce overfitting.

While the specific regularization techniques used in SeedNet are not mentioned, it is possible that one or both of these techniques were used. It is also possible that other regularization techniques were used, or that a combination of techniques was employed. Without further information, it is difficult to provide a definitive answer to this question.
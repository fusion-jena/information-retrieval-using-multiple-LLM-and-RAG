Answer:::

The provided context does not give specific information about how the hyperparameters of the model were optimized. However, it does mention that the Adam optimizer was used for training the model. The Adam optimizer is a type of optimization algorithm that can be used to update the parameters of a model during training. It adaptively adjusts the learning rate based on the past gradients, which can help to improve the convergence of the training process.

In addition to using the Adam optimizer, the training process also involved cross validation with 80% of the training data used for training and 20% used for validation. This approach can help to prevent overfitting and can provide a more robust estimate of the model's performance.

The data was also augmented using spatial and color transformations, which can help to increase the size and diversity of the training set and improve the model's ability to generalize to new data.

However, the context does not mention any specific techniques for hyperparameter optimization, such as grid search or random search. These techniques involve systematically searching through a range of possible hyperparameter values in order to find the combination that results in the best performance. It is possible that these techniques were used, but the context does not provide enough information to confirm this.
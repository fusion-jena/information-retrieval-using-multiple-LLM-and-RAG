The text provided gives information about a deep learning model that was trained on a small dataset without data augmentation. The authors report that they used two regularization methods to prevent overfitting: L2 weight decay and dropout.

L2 weight decay, also known as Ridge Regression or weight decay, is a type of regularization that adds a penalty to the loss function that is proportional to the square of the magnitude of the model's weights. This penalty term discourages the model from assigning too much importance to any single feature, which can help to prevent overfitting. In this case, the authors set the L2 weight decay penalty multiplier to 5 Ã— 10-4.

Dropout is another regularization technique that is used to prevent overfitting in deep learning models. It works by randomly dropping out, or "turning off," a fraction of the model's neurons during training. This helps to prevent the model from becoming too reliant on any single neuron, which can help to improve its generalization performance. In this case, the authors set the dropout ratio to 0.5.

It's worth noting that the authors also used a small batch size of 69 and a momentum of 0.9 during training. These settings can also help to prevent overfitting by slowing down the learning process and encouraging the model to take smaller steps during training.

In addition, the authors also mention that they used stochastic gradient descent (SGD) and the backpropagation algorithm for training the model. These are standard optimization algorithms used in deep learning.

In summary, the authors used L2 weight decay, dropout, a small batch size, and a momentum of 0.9 to prevent overfitting during training of their deep learning model.
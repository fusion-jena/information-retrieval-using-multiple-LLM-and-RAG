Based on the provided context, there is no explicit information given about the specific hardware resources used for training the deep learning model. The text mentions the use of deep learning strategies, including neural networks, but it does not specify the hardware used to implement them.

However, it is common practice to use Graphics Processing Units (GPUs) for training deep learning models due to their ability to perform parallel processing, which significantly speeds up the training process compared to using a Central Processing Unit (CPU) alone. GPUs are specialized hardware designed to accelerate graphical processing, but they are also well-suited for other tasks that require parallel computation, such as deep learning.

Google's Tensor Processing Units (TPUs) are another type of hardware that can be used for training deep learning models. TPUs are custom-built application-specific integrated circuits (ASICs) designed by Google specifically for machine learning tasks. They are optimized for tensor operations, which are the fundamental building blocks of deep learning models.

Given the increasing use of deep learning strategies in the field of ecological informatics, as mentioned in the text, it is likely that researchers are using hardware resources such as GPUs or TPUs to train their deep learning models. However, without explicit information provided in the context, this is only an educated guess.
Answer:::

To monitor the model performance during training, the authors implemented a strategy involving the use of a validation set for hyperparameter tuning and model selection. Specifically, they first collected all tiles from the 50 training photographs, shuffled them, and then split them into training, validation, and test sets. The validation set, which consisted of 10% of the total number of tiles, was used during training to fine-tune hyperparameters and select the best model setup. Only after the best model setup was found was its prediction error assessed once again on the test set.

Additionally, the authors used early stopping and loss curves to monitor the model's performance during training. They stopped the learning process after 90 epochs, where the learning curve converged and loss values did not decrease further. The improvement in loss and accuracy during the training process was illustrated using a figure, which showed that the validation accuracy exceeded the training accuracy. This is a common effect when dropout and data augmentation are applied only to training data, indicating that the training set contains more difficult tiles than the validation set.

Finally, the authors evaluated the performance of the final model on a separate test dataset containing n = 5796 individual tiles, which were not used during the process of model selection. This allowed them to further monitor the model's performance and ensure that it generalized well to new, unseen data.

In summary, the authors implemented a rigorous strategy for monitoring the model's performance during training, involving the use of a validation set, early stopping, and evaluation on a separate test dataset. This allowed them to select the best model setup, prevent overfitting, and ensure that the model generalized well to new data.
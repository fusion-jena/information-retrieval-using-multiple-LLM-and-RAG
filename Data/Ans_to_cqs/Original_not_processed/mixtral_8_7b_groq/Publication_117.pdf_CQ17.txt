The provided context discusses a study that compares the performance of citizen science and deep learning methods in counting wildebeest in aerial images. However, it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline.

In general, deep learning models are prone to overfitting, especially when trained on small datasets. Regularization techniques are used to prevent overfitting by adding a penalty to the loss function, which encourages the model to be simpler and make more generalized predictions.

Two common regularization methods used in deep learning are dropout and L2 regularization.

Dropout is a technique where randomly selected neurons are "dropped out" during training, which helps prevent overfitting by making the model less reliant on any single neuron. This forces the model to learn more robust features that can be used even when some neurons are dropped out.

L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to have smaller weights, which helps prevent overfitting by reducing the impact of any single weight on the model's predictions.

Without explicit information from the provided context, it is unclear which regularization techniques were used in the deep learning pipeline. However, it is mentioned that transfer learning was used, which involves initializing the model's weights with pre-trained weights from a different dataset. This can help prevent overfitting by providing a good starting point for the model's weights, which reduces the risk of overfitting during training.

In summary, while the provided context does not explicitly mention the regularization techniques used to prevent overfitting in the deep learning pipeline, it is likely that one or more techniques such as dropout or L2 regularization were used. Transfer learning was also used, which can help prevent overfitting by providing a good starting point for the model's weights.
Answer:::

Based on the provided context, the postprocessing steps involved after the model training include the calculation of several performance metrics and the selection of the best models for each target variable.

After the hyperparameter optimization process, where the performance metrics are evaluated using the test set, the most effective models for each target variable are selected. This selection process is illustrated in Figure 4. The models included in this process are KNN, GP, TREE, RF, XGB, SVR, MLP, and BP (which stands for Best Parameter).

Next, the dataset is divided into training sets, which consist of 80% of the data, and a test set representing the remaining 20%. This division is done by random sampling to ensure that both subsets are representative of the overall dataset and minimize the risk of bias. The scikit-learn function GridSearchCV is then used to fine-tune the hyperparameters of each model within the confines of the training set, aiming to enhance their predictive performance. A five-fold cross-validation is used, and the negative mean square error is set as the target measure.

Following the model training and hyperparameter tuning, the performance of the H, LE, and CO2 prediction models is evaluated. The performance evaluation includes the calculation of several metrics, such as the name of the prediction model, its hyperparameters, the parameters selected during the 5-fold cross-validation, and the hyperparameters adjusted according to the selection criteria to determine the most appropriate configuration for the prediction model. These metrics are presented in Table 4 and include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), and Mean Absolute Scaled Error (MASE). However, the context does not mention the calculation of a confusion matrix or the use of saliency maps.

Answer:::
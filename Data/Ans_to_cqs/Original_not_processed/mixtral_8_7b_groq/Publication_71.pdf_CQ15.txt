Answer:::

The provided context does not explicitly mention the optimization techniques used in the deep learning pipeline. However, it does give information about the learning policy used in DeepLabV3+, which is a type of poly learning policy.

In the poly learning policy, the learning rate decreases over time according to a power function. The base learning rate is set to 0.0001, the learning rate decay factor is 0.1, and the power is 0.9. The learning rate decay step is 2000, and the training number of steps is greater than or equal to 100,000.

While this is not an optimization technique per se, the learning policy can significantly impact the training of deep learning models. The learning rate is a hyperparameter that controls how much the model's weights are updated during training. Choosing an appropriate learning rate is crucial for the model to converge to the optimal solution.

The poly learning policy is a type of learning rate schedule that adjusts the learning rate dynamically during training. It starts with a high learning rate to allow the model to explore the solution space rapidly. As training progresses, the learning rate decreases, allowing the model to fine-tune the weights and converge to the optimal solution.

Therefore, while the context does not explicitly mention optimization techniques such as SGD or Adam, it does provide information about the learning policy used in DeepLabV3+, which is a crucial aspect of deep learning training.
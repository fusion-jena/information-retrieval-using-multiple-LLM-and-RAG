The provided context does not directly mention the regularization methods used to prevent overfitting in deep learning pipelines, such as dropout or L2 regularization. However, it does discuss Self-Organizing Map (SOM) networks and Support Vector Machines (SVM), which can be used in the context of machine learning and deep learning.

SOM networks are a type of artificial neural network that is trained using unsupervised learning. They are used for clustering and visualization of high-dimensional data. The context states that changing the number of categories significantly affects the clustering results in SOM networks, and determining the number of categories is the key. It also highlights the need for further optimization of the training algorithm for SVM, especially for large sample datasets.

Coming to the query, regularization methods are used to prevent overfitting in deep learning pipelines. Dropout is a regularization technique used in deep learning to prevent overfitting. It involves randomly dropping out, or "turning off," neurons during training, which helps to make the model more robust and less prone to overfitting. L2 regularization, also known as weight decay, is another regularization technique that adds a penalty term to the loss function, which encourages the weights to be small and helps to prevent overfitting.

Therefore, while the provided context does not directly mention dropout or L2 regularization, these are important regularization techniques used in deep learning pipelines to prevent overfitting.
Answer:::

Based on the provided context, the following data augmentation techniques are applied in the deep learning pipeline:

1. Flipping: The training dataset is augmented by adding vertically and horizontally flipped versions of the training images.
2. Rotating: The training dataset can be augmented by rotating the training images. However, the specific degrees of rotation are not mentioned in the context.
3. Scaling: The training dataset is augmented using different brightness levels and adding noisy versions of the training images. These techniques can be considered as scaling, as they change the intensity values of the pixels. Additionally, the context mentions the use of min_scale_factor and max_scale_factor parameters in the DeepLabV3+ model, which could be related to scaling the input images.

Answer:::

References:
makes the deep learning method’s loss function to be biased toward the overrepresented classes resulting in poor classification performance for the underrepresented classes [50]. One should also pay attention when applying deep learning methods to new applications because one requirement for deep learning is the availability of a vast amount of training data. Moreover, the training data needs to have similar characteristics as the testing data. Otherwise, deep learning methods may not yield good performance. Augmenting the training dataset using different brightness levels, adding vertically and horizontally flipped versions, shifting, rotating, or adding noisy versions of the training images could be potential strategies to mitigate the issues when test data characteristics differ from the training data. 5. Conclusions In this paper, we investigated the performance of three methods for vegetation detection. Two of these methods are based on deep learning and another one is an unnamed method.

Training Parameter Value Learning policy Poly Base learning rate 0.0001 Learning rate decay factor 0.1 Learning rate decay step 2000 Learning power 0.9 Training number of steps ≥100,000 Momentum 0.9 Train batch size 2 Weight decay 0.00004 Train crop size ‘513,513′ Last layer gradient multiplier 1 Upsample logits True Drop path keep prob 1 tf_initial_checkpoint deeplabv3_pascal_train_aug initialize_last_layer False last_layers_contain_logits_only True slow_start_step 0 slow_start_learning_rate 1e-4 fine_tune_batch_norm False min_scale_factor 0.5 max_scale_factor 2 scale_factor_step_size 0.25 atrous_rates [6,12,18] output_stride 16
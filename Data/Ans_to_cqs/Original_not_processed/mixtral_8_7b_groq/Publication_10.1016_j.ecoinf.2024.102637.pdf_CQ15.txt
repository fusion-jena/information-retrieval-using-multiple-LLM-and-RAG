The provided context discusses the optimization of deep learning models through algorithm design and structural changes, specifically in the context of a lightweight SIAlex model. However, it does not directly mention the use of optimization techniques such as Stochastic Gradient Descent (SGD) or Adam.

In general, optimization techniques are used to minimize the loss function of a deep learning model during training. Some common optimization techniques include:

1. Stochastic Gradient Descent (SGD): This is a simple optimization algorithm that updates the model parameters in the direction of the negative gradient of the loss function.
2. Momentum: This technique helps the model overcome local minima by adding a fraction of the previous update to the current update.
3. Adagrad: This is an optimization algorithm that adapts the learning rate for each parameter based on the historical gradient information.
4. Adam: This is an optimization algorithm that combines the advantages of both Adagrad and momentum, making it a popular choice for deep learning models.

The provided context discusses the optimization of the SIAlex model through the use of a lightweight AlexNet backbone and the cascading of multiple activation functions. These techniques aim to improve the model's performance while reducing computational complexity and deployment costs.

The AlexNet backbone is a deep convolutional neural network (CNN) that consists of a series of convolution layers, normalization layers, and activation function layers. The normalization layer is used to accelerate training and prevent overfitting, while the activation function layer increases the nonlinear expression ability of the model.

The cascading of multiple activation functions introduces nonlinear factors and improves gradient propagation, which helps the model approximate the nonlinear expression function of the learning features.

In addition, the context mentions the use of techniques such as merging convolution layers and introducing skip connections between traditional convolutional layers to improve computational efficiency and retain low-level feature information.

In summary, while the provided context does not explicitly mention the use of optimization techniques such as SGD or Adam, it discusses the optimization of the SIAlex model through the use of a lightweight AlexNet backbone, the cascading of multiple activation functions, and the merging of convolution layers and skip connections. These techniques aim to improve the model's performance while reducing computational complexity and deployment costs.
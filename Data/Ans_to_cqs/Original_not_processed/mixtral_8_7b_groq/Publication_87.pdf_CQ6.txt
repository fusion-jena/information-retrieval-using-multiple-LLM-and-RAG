Answer:::

The provided context discusses a few preprocessing steps that were taken before training a deep learning model, specifically a convolutional neural network (CNN) using the ResNet-50 architecture. These steps include:

1. **Data Splitting**: The authors split the dataset into training and testing sets, with 80% of the annotated images for each species used for training and 20% for testing.

2. **Data Augmentation**: To improve training, various transformations were applied to the images, including flipping, brightness, and contrast modifications. This technique, known as data augmentation, helps increase the size and diversity of the training set, thereby reducing overfitting and improving the model's ability to generalize.

3. **Transfer Learning**: The authors used transfer learning, a technique where a pre-trained model is used as a starting point for training a new model on a different but related task. This approach leverages the knowledge gained from the pre-trained model, reducing the need for a large amount of labeled data and training time.

4. **Model Architecture**: A deep convolutional neural network (ResNet-50) architecture was used. ResNet-50 is a pre-trained model that has been trained on a large dataset (ImageNet), and it has proven to be effective for various computer vision tasks.

5. **Library and Tools**: The authors used the fastai library, which implements the PyTorch library, to train the models. The fastai library comes with an R interface, allowing R users to access up-to-date deep learning tools.

However, the context does not explicitly mention other common preprocessing steps such as normalization, scaling, or cleaning. Normalization and scaling are often used to standardize the input data's range or distribution, making it easier for the model to learn. Cleaning, on the other hand, involves removing or correcting erroneous or irrelevant data points that could negatively impact the model's performance.

In summary, the provided context discusses several preprocessing steps, including data splitting, data augmentation, transfer learning, and the use of a specific model architecture and library. However, it does not explicitly mention normalization, scaling, or cleaning as preprocessing steps.
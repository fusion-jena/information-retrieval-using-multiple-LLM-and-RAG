Based on the provided context, there is no explicit information about postprocessing steps such as Saliency maps or specific metrics calculation. However, there is information about the calculation of a Confusion matrix.

In the context, Table 4 is a Classification table, which is similar to a Confusion matrix. A Confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known. It allows visualization of the performance of an algorithm.

The Confusion matrix in the context has four categories: True Negatives (TN), False Positives (FP), False Negatives (FN), and True Positives (TP). The terms "Negative" and "Positive" refer to the classifier's prediction (negative or positive) and the terms "True" or "False" refer to whether or not the classifier's prediction was correct.

From Table 4, we can see that the classifier has predicted 1334 "present" cases correctly out of 1536 total "present" cases, and 1414 "absent" cases correctly out of 1556 total "absent" cases.

The classifier's overall accuracy can be calculated as: (TP + TN) / (TP + TN + FP + FN) = (1334 + 1414) / (1536 + 1556) = 0.8617 or 86.17%.

As for Saliency maps, they are not mentioned in the provided context. Saliency maps are a way to visualize the importance of each input feature to a model's prediction. They can be useful for understanding which features the model is relying on most heavily, but they are not a postprocessing step that is necessarily applied after model training.
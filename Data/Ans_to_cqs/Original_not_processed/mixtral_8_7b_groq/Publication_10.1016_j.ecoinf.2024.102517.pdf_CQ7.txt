Answer:::
Based on the provided context, there is no explicit information about how the data was split for training the deep learning model. The text describes the experimental setting, which includes the construction of a deep learning environment based on Python, Tensorflow, GPU, CUDA, and the Windows operating system. It also mentions that the deep feature loss bird sound reduction network model was trained for 50 epochs on an RTX 2080 Ti GPU using the Adam optimizer with a learning rate of 10. However, it does not specify how the data was divided into training, testing, and validation sets.

Additionally, the context describes the comparative algorithms used to evaluate the performance of the deep feature loss-based bird sound noise reduction network. These algorithms include SEGAN, WebRTC, wavelet transform, Wiener filter, and MMSE STSA. The evaluation was conducted using the same test set for all algorithms. However, the data splitting method for this test set is also not provided in the context.

Therefore, based on the given context, it is not possible to determine the criteria used to split the data for deep learning model training.
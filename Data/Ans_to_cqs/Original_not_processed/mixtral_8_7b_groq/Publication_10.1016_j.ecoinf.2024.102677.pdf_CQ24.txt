In the context provided, there is no explicit mention of a specific strategy used to handle randomness in the deep learning pipeline, such as setting a random seed value. However, there is a description of how the dataset was divided into training, validation, and testing sets, which was done in a stratified manner to maintain class proportions across all parts. This process involves a certain level of randomness, as the dataset is divided randomly, but the stratified sampling ensures that the distribution of classes remains consistent, which can help improve the model's performance and reduce the impact of randomness.

In addition, the learning rate, batch size, and number of epochs were determined through parameter tuning, which may also involve some level of randomness. However, the text does not provide details about how this tuning was performed or whether a random search or a more systematic approach was used.

Furthermore, the deep learning models used in the study were not explicitly mentioned, so it is difficult to determine whether they have any built-in mechanisms to handle randomness. For instance, some models use dropout regularization, which randomly drops out a certain percentage of neurons during training to prevent overfitting and reduce the impact of randomness.

Overall, while the text does not provide explicit information about strategies used to handle randomness in the deep learning pipeline, the dataset division and parameter tuning processes involve some level of randomness, which is common in deep learning. However, without more information, it is difficult to provide a more specific answer.
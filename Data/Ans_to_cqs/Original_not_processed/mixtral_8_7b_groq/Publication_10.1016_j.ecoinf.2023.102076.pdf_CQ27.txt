The provided context does not give detailed information on the process followed to deploy the trained deep learning model. However, it does mention that classifiers were trained and retrained using different approaches, such as random selection, Embedding K-means, and Least Confidence active learning algorithms. After training, the model's performance was evaluated based on the F1 score.

The context also explains that the BERT model was used, which requires tokenization of the input text. The tokenizer splits the text into chunks of 300 tokens using the NLTK word-tokenizer. If a chunk contains any of the original query terms, it is retained for use in model training and evaluation. The maximum number of tokens is set to 400 for social media posts and 512 for web articles.

However, the process of model serialization, platform selection, or any other deployment-related steps are not mentioned in the provided context. Therefore, it is not possible to provide a complete answer to the query based on the given information.
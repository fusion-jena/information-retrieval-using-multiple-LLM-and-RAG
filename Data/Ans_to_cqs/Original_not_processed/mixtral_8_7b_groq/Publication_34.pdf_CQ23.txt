Based on the provided context, it appears that several measures were taken to ensure the generalizability of the deep learning model, although the specific terms "diverse dataset," "cross-validation," and "stratified splitting" are not explicitly mentioned.

Firstly, the authors used a large and diverse dataset to train and test their models. They started with 7626 initial species, selected species with more than 300 observations, and then randomly selected a subset of 1000 species (E1000). From this, they further randomly selected 200 species (E200) and finally 50 species (E50) to constitute the main dataset used for comparison. This process ensured that the model was trained and tested on a diverse range of species, thereby increasing its generalizability.

Secondly, the authors used three different evaluation metrics to assess the performance of their models. These metrics include test loss, test Rmse, and test A10%DQ. By using multiple metrics, they ensured that the model's performance is not overly dependent on a single metric, thereby increasing its generalizability.

Thirdly, the authors noted that the superiority of the CNN model, which was their best model, was consistent across all metrics. This consistency suggests that the model's performance is not a result of random fluctuations or overfitting, thereby increasing its generalizability.

Lastly, the authors observed that the performance of the DNN model improved as the number of species in output increased. This improvement suggests that the multi-response regularization used in the DNN model is effective in increasing its generalizability when the model is complex or the input dimensionality is high.

In conclusion, while the specific terms were not mentioned, the authors took several measures to ensure the generalizability of their deep learning model. These measures include using a large and diverse dataset, multiple evaluation metrics, consistency in model performance, and effective regularization techniques.
Answer:

The criteria used to split the data for deep learning model training, such as train, test, and validation sets, are not explicitly stated in the provided context. However, it does mention that an "early stopping method" was used during training to keep the model weights before the models overfitted. This suggests that a validation set might have been used to monitor the model's performance during the training process and stop training when the model's performance on the validation set started to degrade.

Additionally, the text mentions that different ratios of positive/negative sections were tested for the blue whale’s Dcall model training. This was done by randomly removing negative sections from the testing set (undersampling method). This implies that the data was split into training and testing sets, and the testing set was further manipulated to create different positive/negative ratios.

Furthermore, the text mentions that the number of 50-s sections for each dataset is presented in Table 1. However, the specific details of this table are not provided in the text. It is possible that this table contains information about the data split, such as the number of sections in the train, test, and validation sets.

In summary, while the specific criteria used to split the data for deep learning model training are not explicitly stated in the provided context, there are indications that a validation set might have been used, and the data was split into training and testing sets for the blue whale’s Dcall model training. The specific details of the data split, if any, are likely to be found in Table 1.
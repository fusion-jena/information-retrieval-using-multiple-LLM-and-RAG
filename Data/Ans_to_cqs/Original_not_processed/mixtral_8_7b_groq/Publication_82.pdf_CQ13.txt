Answer:::

The hyperparameters used in the deep learning models in the context provided are as follows:

1. Solver type: The solver type used for all the experiments is Stochastic Gradient Descent.

2. Base learning rate: The base learning rate used for all the experiments is 0.005.

3. Learning rate policy: The learning rate policy used is Step, which decreases by a factor of 10 every specified number of epochs or iterations.

4. Momentum: The momentum value used is 0.9.

5. Weight decay: The weight decay value used is 0.0005.

6. Gamma: The gamma value used is 0.1.

7. Batch size: The batch size used varies depending on the architecture of the deep learning model. For AlexNet, the batch size used is 100, while for GoogLeNet, the batch size used is 24.

8. Number of epochs: Each experiment runs for a total of 30 epochs, where one epoch is defined as the number of training iterations in which the particular neural network has completed a full pass of the whole training set.

9. SoftMax layer: The softMax layer is used at the end of the model, which takes the output from the last fully connected layer (fc8) and produces a distribution of values across the 38 classes that add up to 1. These values can be interpreted as the confidences of the network that a given input image is represented by the corresponding classes.

10. ReLu non-linearity activation unit: The first seven layers of AlexNet have a ReLu non-linearity activation unit associated with them.

11. Dropout layer: The first two fully connected layers (fc6, fc7) of AlexNet have a dropout layer associated with them, with a dropout ratio of 0.5.

These hyperparameters were standardized across all the experiments to enable a fair comparison between the results of all the experimental configurations. The deep learning models were implemented using the authors' fork of Cafe, a fast, open-source framework for deep learning.
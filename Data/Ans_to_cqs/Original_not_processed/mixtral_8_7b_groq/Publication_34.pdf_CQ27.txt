The text provided does not give specific details about the process of deploying the trained deep learning model. However, it does provide some context about the training process and data used.

The models (SNN, DNN, and CNN) were trained using the mxnet package and stochastic gradient descent algorithm with a momentum of 0.9, a batch-size of 50, and an initial learning rate of 10^-3. The learning rate was critical for the optimization behavior, and the learning process took approximately 5 minutes for SNN, 10 minutes for DNN, and 5 hours for CNN.

To ensure the results were not biased by the choice of a particular category of species, the authors worked with a high number of randomly chosen species. They selected species with more than 300 observations and then randomly selected a subset of 1000 species (E1000), 200 species (E200), and finally 50 species (E50). E50 was the main dataset used to compare the model to the baselines.

The environmental data used for the models is not explicitly mentioned, but the authors do mention that in the NN models learning, there is still work to be done on quick automated procedures for tuning optimization hyper-parameters, especially the initial learning rate. They also suggest minimizing the number of non-null connections in the network to make it more interpretable and introducing an L1-type penalty on the network parameters as potential future work.

Therefore, while the text does not provide explicit information about the deployment process, it can be inferred that the trained models were likely serialized and then deployed on a platform suitable for running the mxnet package. However, the specific details of this process are not provided.
Based on the provided context, there is no explicit mention of specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adam used in the deep learning pipeline. However, the text by Do and Tran (2023) does mention the use of a deep learning model combined with an optimization algorithm for detecting the dispersal of spotted butterfish in northern Vietnam under global warming.

In the context of machine learning, optimization algorithms are often used to minimize the loss function or error between the predicted and actual values. Common optimization techniques used in deep learning include SGD, Adam, RMSprop, and Adagrad.

SGD is a simple optimization algorithm that updates the model parameters by taking a step in the direction of the negative gradient of the loss function. Adam, on the other hand, is an adaptive optimization algorithm that adjusts the learning rate for each parameter based on the historical gradient information.

The absence of specific optimization techniques mentioned in the provided context could be due to the fact that the authors assumed the readers' familiarity with commonly used optimization algorithms in deep learning. Alternatively, the authors may have used a deep learning framework that implements these optimization algorithms by default.

In summary, while the provided context does not explicitly mention the use of specific optimization techniques such as SGD or Adam, it is likely that they were used in the deep learning pipeline.
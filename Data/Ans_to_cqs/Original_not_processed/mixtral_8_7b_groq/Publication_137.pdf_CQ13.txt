Answer:::

The hyperparameters used in the deep learning model include the learning rate, the decay rate of the learning rate, batch size, and regularization strength. The learning rate is not explicitly mentioned in the provided context, but it is indicated that the learning decay rate is set to 0.7. This means that the learning rate gradually decays as the training progresses. The batch size is another hyperparameter, which represents the batch number of point cloud samples in training. The regularization strength is also a hyperparameter that influences the coarseness of the minimal partition. A larger regularization strength value results in a coarser partition, while a smaller value leads to a finer partition. However, the context suggests that the best result is obtained when the regularization strength is 0.2. The hold-out method is used to preprocess the original data by distinguishing the test set and the training set at a ratio of 3:7. The authors also adjusted the parameters of networks to explore the influence of parameters on experimental results and discussed the selection of frame parameters.

It is important to note that the authors did not mention any specific optimizer used in the model. Common optimizers used in deep learning models include stochastic gradient descent (SGD), Adam, and RMSprop. The choice of optimizer can significantly affect the training process and the final result.

In summary, the hyperparameters used in the deep learning model include the learning rate, learning decay rate, batch size, and regularization strength. However, the specific optimizer used in the model is not mentioned in the provided context.
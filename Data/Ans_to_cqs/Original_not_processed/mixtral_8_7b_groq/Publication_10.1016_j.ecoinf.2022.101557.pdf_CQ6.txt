The provided context discusses the data pre-processing steps in the specific context of training a Convolutional Neural Network (CNN) for ecological informatics. However, it does not mention the general pre-processing steps like normalization, scaling, or cleaning. Therefore, I will first provide a general answer and then relate it to the given context.

Answer:::
In general, data pre-processing for deep learning models involves several steps, including data cleaning, normalization, scaling, and splitting the data into training and validation/test sets. Data cleaning helps remove outliers, missing values, or incorrect data points. Normalization and scaling are used to adjust the data's range and distribution, making it easier for the model to learn. Splitting the data into separate sets is crucial to evaluate the model's performance and prevent overfitting.

Now, I will relate this answer to the given context.

In the context of the ecological informatics study, the data pre-processing includes splitting the data into larger blocks based on coordinates for cross-validation and then into smaller areas of 80 Ã— 80 pixels due to limitations in available data. These samples are selected randomly from all areas in the training set and are never used to fit the model. This process can be seen as a form of data splitting and cleaning. However, the context does not explicitly mention normalization, scaling, or other typical pre-processing techniques.

Therefore, based on the provided context, it is unclear whether data normalization, scaling, or other general pre-processing steps were applied before training the CNN. Nonetheless, the context does provide a detailed explanation of other pre-processing steps specific to this study.
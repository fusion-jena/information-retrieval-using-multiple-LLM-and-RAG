Based on the provided context, there is no explicit information about the preprocessing steps taken before training the deep learning model (EF network architecture). However, it can be inferred that the training data has been prepared in a way that it can be used as input for the model.

Firstly, the training data is divided into three sets: training, validation, and test sets. The training set consists of tiles with available definitions (def.) and no definitions (no-def.) samples. These tiles are used as input for the EF network architecture.

Secondly, the training data is balanced by having an equal number of samples per class. This is evident in Table 1, where the balanced samples (per class) are presented for the Amazon database.

Thirdly, the training data is used to train the SVMs filter bank. Mini-patches are extracted from input patches, and they are vectorized to compose the training set of SVMs. This step is performed to evaluate the influence of the number of training samples.

Lastly, the training data is used to train the EF and SN models with a specific setup. The setup includes a batch size of 32, 100 number of epochs, early stopping after 10 epochs with no improvement, a dropout rate of 0.2 in the final FC layer, and the binary cross-entropy as a loss function.

Therefore, while the context does not explicitly mention preprocessing steps such as normalization, scaling, or cleaning, it can be inferred that the training data has been prepared by dividing it into sets, balancing the samples, extracting mini-patches, and vectorizing them. However, there is no explicit information about normalization, scaling, or cleaning of the data.
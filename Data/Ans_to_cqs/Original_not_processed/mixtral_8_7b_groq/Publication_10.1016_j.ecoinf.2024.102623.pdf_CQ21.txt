Based on the provided context, there is no explicit information about the postprocessing steps involved after the model training. However, it does mention some details about the evaluation of the model during the training process.

In the "3.5. Evaluation and baselines" section, it is mentioned that the performance of the model is evaluated using the Mean AUC (Area Under the Curve) metric. This metric is averaged over 10 random seeds, which suggests that the model's performance is being evaluated with some degree of statistical significance.

Additionally, there is a table (Table B.4) that compares the Mean AUC performance of various alternative ML loss functions. This comparison helps in selecting the most suitable loss function for the specific problem and dataset.

However, there is no information about other postprocessing steps such as generating saliency maps, calculating other metrics (e.g., precision, recall, F1 score), or creating confusion matrices. Saliency maps are often used to visualize the regions of the input that the model is focusing on for its predictions. Metrics like precision, recall, and F1 score provide a more comprehensive evaluation of the model's performance, especially in multi-class or multi-label classification problems. Confusion matrices are useful for understanding the types of errors the model is making, such as false positives and false negatives.

Therefore, based on the given context, it is not possible to provide a detailed answer to the query. Nonetheless, it is important to note that the evaluation of the model's performance during training is an essential step in the machine learning pipeline, and postprocessing steps like generating saliency maps, calculating additional metrics, and creating confusion matrices can provide valuable insights into the model's behavior and performance.
Answer:::

The provided context does not give specific details about the exact hyperparameters used in the deep learning models. However, it does mention that a random hyperparameter search was conducted to determine the optimum model structure. This search would have included various hyperparameters such as the learning rate, the type of optimizer, the number of layers, and the number of nodes per layer. The specific values and ranges used in this search are not provided in the context.

Additionally, the deep learning models were constructed using the H2O package in R. H2O supports a variety of hyperparameters for deep learning, including:

1. Learning rate: The step size used in gradient descent.
2. Activation function: The function used to transform the summed weighted input from the node into an output that is then sent to the next layer.
3. Number of layers and nodes per layer: The architecture of the neural network.
4. Adaptive learning rate: A mechanism for adjusting the learning rate during training.
5. Regularization: Techniques for preventing overfitting, such as L1 and L2 regularization.
6. Epochs: The number of times the learning algorithm will work through the entire training dataset.
7. Training rate: The fraction of the data used for training.

While the context does not specify which of these hyperparameters were searched, it is mentioned that the number of layers, nodes per layer, and training rate were varied. However, the specific values and ranges used are not provided.
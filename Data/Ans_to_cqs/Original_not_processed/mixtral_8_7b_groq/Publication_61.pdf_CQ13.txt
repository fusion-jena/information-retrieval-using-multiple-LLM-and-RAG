Answer:::
The hyperparameters used in the deep learning model, as described in the given context, include the learning rate, number of epochs, and batch size. The learning rate is set to 0.01, the number of epochs is 150, and the batch size is 2500. The model uses the Adam optimizer, which is a variant of stochastic gradient optimization that adjusts the learning rate automatically. The learning rate is a crucial hyperparameter that defines how much the weights are adjusted to decrease the loss. A larger learning rate could prevent the network from learning, while a smaller one might require more steps for learning. The rest of the parameters are set empirically, as detailed in the experiments section.
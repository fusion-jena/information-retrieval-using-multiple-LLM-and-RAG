Answer:::

The provided context does not contain information specific to handling randomness in deep learning pipelines. However, it does discuss the use of random forests, a machine learning method that also utilizes randomness in its algorithm. In random forests, randomness is introduced by selecting a random subset of features at each node for splitting, which helps to reduce overfitting and improve generalization (Probst et al., 2019). This is controlled by the `mtry` hyperparameter in random forest algorithms, which sets the number of features to randomly select at each node.

In deep learning, randomness can be handled in several ways, although the context does not provide specific strategies used in the deep learning pipeline. Here are some common ways to handle randomness in deep learning:

1. Random seed value: Deep learning models often initialize their weights randomly. To ensure reproducibility, a fixed random seed value can be set so that the same set of random weights is used across different runs.
2. Data augmentation: Random transformations, such as rotation, scaling, or flipping, can be applied to the training data to increase its size and introduce randomness, which can help improve model generalization.
3. Dropout: During training, dropout randomly sets a fraction of the output of a hidden layer to zero, which helps prevent overfitting by reducing co-adaptation of feature detectors.
4. Batch normalization: Batch normalization normalizes the inputs of each layer, which can help reduce internal covariate shift and improve model convergence. It also introduces randomness by using a different mini-batch of data at each iteration.
5. Early stopping: Early stopping is a regularization technique that stops training when the model's performance on a validation set stops improving, which can help prevent overfitting and reduce the impact of randomness in the training process.

In summary, while the provided context does not discuss specific strategies for handling randomness in deep learning pipelines, it does mention the use of randomness in random forests. Common strategies for handling randomness in deep learning include setting a random seed value, data augmentation, dropout, batch normalization, and early stopping.
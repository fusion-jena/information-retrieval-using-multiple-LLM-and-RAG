Answer:::

The optimization technique applied in the deep learning pipeline is the Adam optimizer. The Adam optimizer is a popular optimization algorithm used in training deep learning models. It is an adaptive learning rate optimization algorithm, a type of stochastic gradient descent, that is designed to improve the training process by computing individual learning rates for different parameters. This is particularly useful in deep learning models, where the number of parameters can be very large. The Adam optimizer was used with a categorical-cross entropy loss function, and the network weights with the best performance were kept based on monitoring the validation data across training epochs. Early stopping criteria were also applied if no improvement was found in 50 epochs. Additionally, an ensemble of three models were generated, and the average of the final output layer was taken to enhance the results. This approach helps to reduce variability in the results caused by weight initialization and random selection for batch training.
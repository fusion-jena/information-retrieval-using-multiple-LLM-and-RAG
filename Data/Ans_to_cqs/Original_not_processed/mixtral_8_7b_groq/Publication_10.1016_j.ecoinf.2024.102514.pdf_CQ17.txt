The provided context does not contain specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of cross-validation, which is a technique used to assess the performance and generalization capability of machine learning and artificial intelligence models, including deep learning models. Cross-validation can help to reduce overfitting by providing an estimate of how well a model will perform on unseen data.

Regularization methods, such as dropout and L2 regularization, are specifically used to prevent overfitting in deep learning models. Dropout is a technique in which randomly selected neurons are ignored during training, which helps to prevent over-reliance on any single neuron and therefore reduces overfitting. L2 regularization, also known as weight decay, adds a penalty to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to have smaller weights, which can help to reduce overfitting.

In summary, while the provided context does not specifically mention the use of regularization methods to prevent overfitting in deep learning models, it does mention the use of cross-validation, which can help to reduce overfitting. Regularization methods such as dropout and L2 regularization are specifically used to prevent overfitting in deep learning models.
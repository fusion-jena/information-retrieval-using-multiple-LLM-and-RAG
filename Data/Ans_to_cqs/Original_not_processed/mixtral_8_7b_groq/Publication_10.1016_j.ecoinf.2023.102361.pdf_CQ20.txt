Answer:::
The deep learning models for this research were trained using a desktop computer equipped with a powerful NVIDIA GeForce RTX 2080 Ti graphics card. This GPU provides the necessary computational resources to handle the demanding requirements of training deep learning models.

The use of a GPU, as opposed to a CPU, is crucial for training deep learning models due to the parallel processing capabilities of GPUs. Training deep learning models typically involves large matrix and vector operations, which can be executed simultaneously on a GPU, leading to significant speedups compared to a CPU.

In this specific research, the deep learning models were developed using Python 3.8 and the Tensorflow 2.4 library, which further streamlines the process of training deep learning models on GPUs. Tensorflow is a popular open-source library for machine learning and artificial intelligence, which supports GPU acceleration through CUDA (Compute Unified Device Architecture), a parallel computing platform and application programming interface model created by NVIDIA.

In summary, the deep learning models in this research were trained using a desktop computer with an Intel Core i9-9900Ã— processor, 128 GB of RAM, and an NVIDIA GeForce RTX 2080 Ti graphics card. The use of a GPU, specifically the NVIDIA GeForce RTX 2080 Ti, enabled efficient training of deep learning models through parallel processing capabilities. The models were developed using Python 3.8 and Tensorflow 2.4 library, which further facilitated GPU-accelerated training.
The context provided does not give specific information about the hardware resources used for training the deep learning model. However, it is mentioned that the ResNet152-architecture model has around 58 million trainable parameters, which suggests that significant computational power would be required for training. It is common practice to use Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs) for training deep learning models with large numbers of parameters, due to their ability to perform many calculations simultaneously. Therefore, it is likely that either GPUs or TPUs were used for training the model in question, but the context does not provide sufficient information to confirm this.
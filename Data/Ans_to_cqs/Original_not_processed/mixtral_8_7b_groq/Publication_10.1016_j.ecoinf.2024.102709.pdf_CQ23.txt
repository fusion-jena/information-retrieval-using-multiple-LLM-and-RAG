The deep learning model's generalizability was ensured through several measures, including the use of a diverse dataset, k-fold cross-validation, and the investigation of training results.

Firstly, a diverse dataset was used to train the model. This dataset consisted of three models (Models 4, 5, and 6) trained using the researchers' own data, which were clusters of extracted numbers. In contrast, the other three models (Models 1, 2, and 3) were trained using the modified National Institute of Standards and Technology (MNIST) dataset of handwritten digits. This diverse dataset allowed the model to learn from different data distributions and improve its ability to generalize.

Secondly, k-fold cross-validation was used to determine the ideal training-test split for the model. Specifically, k-fold cross-validation with k = 5 was used. This method involves splitting the data into k different training-test sets and training the model on each training split. The result was an investigation of the best data split, and the split with the highest mAP50 (0.987) was used for the prediction of the model. This approach helped to reduce overfitting and ensured that the model was trained and tested on different data subsets, thereby improving its ability to generalize.

Lastly, the training results were investigated to ensure the model's generalizability. Functions of the Python library scikit-learn were used to split the data and investigate the results of each trained model. The YOLO training losses and mAP50 values of each model were investigated, and the split with the highest mAP50 indicated the highest number of correctly predicted labels for the model trained on a specific split. This investigation helped to identify the best model and training parameters, which further improved the model's ability to generalize.

Therefore, the generalizability of the deep learning model was ensured through the use of a diverse dataset, k-fold cross-validation, and the investigation of training results.
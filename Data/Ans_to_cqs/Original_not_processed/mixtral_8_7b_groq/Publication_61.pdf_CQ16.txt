Based on the given context, the criteria used to determine when training is complete is not explicitly stated. However, it can be inferred that one possible criterion used might be when the validation loss plateaus or stops decreasing significantly. This is suggested by the description of the training process and the analysis of the results.

In section 5.3, it is mentioned that the network is trained using the Adam optimizer, which is a variant of the stochastic gradient optimization where the learning rate is adjusted automatically. This suggests that the training process is adaptive and can potentially adjust the learning rate to continue learning even if the validation loss plateaus.

However, in section 5.2, it is mentioned that the accuracy is very similar for both variations (with and without data augmentation) in the validation test. But at the end of the training, the use of vertical and horizontal flip overcomes the precision of no data augmentation. This implies that the training process is evaluated based on both the validation loss and accuracy, and the training is continued until the model with data augmentation outperforms the one without data augmentation.

Furthermore, in the query, it is mentioned that "validation loss plateau" is one of the criteria used to determine when training is complete. In the context, it is mentioned that using data augmentation, the loss decreases faster. Therefore, it can be inferred that the validation loss plateau is used as a criterion, and the training is continued until the validation loss for the model with data augmentation plateaus or stops decreasing significantly.

In summary, while the given context does not explicitly state the criteria used to determine when training is complete, it can be inferred that one possible criterion used might be when the validation loss plateaus or stops decreasing significantly. Additionally, the training process is evaluated based on both the validation loss and accuracy, and the training is continued until the model with data augmentation outperforms the one without data augmentation.
Answer:::
The deep learning models used in the study, including the CNN model and CNN with an autoencoder, utilize the Adam optimizer for the optimization process. The Adam optimizer is a method that combines the advantages of two other optimization techniques: AdaGrad and RMSProp. It is a stochastic gradient descent method that adaptively adjusts the learning rate for each parameter. This optimization technique is known for its efficiency, computational simplicity, and ability to handle sparse gradients. The learning rate is set at 0.001 for both deep learning models. The mean squared error is used as the loss function for these models.
Based on the provided context, there is no explicit information about the measures taken to ensure the generalizability of a deep learning model. The text focuses on the description of figures, data sources, and contributions of the article related to the analysis of ecological informatics. Therefore, I cannot provide a specific answer to the query.

However, I can mention some common measures taken to ensure the generalizability of a deep learning model:

1. Diverse dataset: Gathering data from various sources, locations, and conditions to ensure that the model learns a wide range of patterns and relationships.
2. Cross-validation: A technique used to assess the performance and overfitting of a model by dividing the dataset into multiple subsets and training/testing the model on each subset.
3. Stratified splitting: A method of dividing the dataset into training and testing sets while preserving the original distribution of classes or important variables.
4. Data augmentation: Artificially increasing the size of the dataset by applying transformations or modifications to the existing data.
5. Regularization: Techniques used to prevent overfitting by adding a penalty term to the loss function, such as L1 or L2 regularization.
6. Early stopping: A method of halting the training process before the model starts overfitting, based on the performance on a validation set.

These measures help improve the generalizability of a deep learning model by reducing overfitting, ensuring that the model can perform well on unseen data. Nonetheless, without specific information from the context, it is not possible to determine if these measures were taken in this particular case.
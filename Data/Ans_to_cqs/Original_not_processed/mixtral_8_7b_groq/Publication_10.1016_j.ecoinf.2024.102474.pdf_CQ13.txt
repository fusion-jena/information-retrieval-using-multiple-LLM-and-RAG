Answer:::
The specific hyperparameters used in the deep learning models mentioned in the text are not provided. However, the text does mention that the models were implemented using PyTorch Lightning, a deep learning framework built on PyTorch, and that the hyperparameters were tuned using an Optuna-based hyperparameter optimization procedure. Optuna is an open-source hyperparameter optimization framework that uses a Bayesian optimization method to find the best set of hyperparameters for a given model and task.

Some examples of hyperparameters that could have been tuned using Optuna include the learning rate, the type of optimizer, the number of layers in the model, the number of units in each layer, the batch size, the dropout rate, the regularization strength, and the number of epochs. The learning rate controls how quickly the model learns from the data, and the optimizer is the algorithm used to update the model's weights based on the gradient of the loss function. The number of layers and units in each layer determine the complexity of the model, while the batch size, dropout rate, and regularization strength control overfitting. The number of epochs determines how many times the model will see the entire training dataset.

Additionally, the authors mentioned that the model training on their hardware (NVIDIA GTX 1080 GPU) took ~460 h, and the optimization was stopped once the validation ENS repeatedly failed to improve compared to the score evaluated from previous epochs. The model at the epoch with the highest attained validation score was selected.

In summary, while the specific hyperparameters used in the models are not provided, the text does mention that an Optuna-based hyperparameter optimization procedure was used, and that the model training took ~460 h on a NVIDIA GTX 1080 GPU. Optuna is a powerful hyperparameter optimization framework that can tune a wide range of hyperparameters, including learning rate, optimizer, number of layers, number of units, batch size, dropout rate, regularization strength, and number of epochs.
The text provided does not give specific information about the optimization techniques used in the deep learning pipeline, such as Stochastic Gradient Descent (SGD) or Adam. However, it does mention that the batch size was set to a large value of 128 to train the model with 128 images at each iteration. This is a common technique used to make training deep learning models more memory-efficient and manageable, as it allows the model to learn from a subset of the training data before updating the model's parameters.

Additionally, the text mentions that BatNet, a cascade of multiple layers of nonlinear processing units for feature extraction, was modified to include new BNorm layers behind the 22 convolutional layers. This was done to prevent overfitting and improve the model's ability to generalize from both "known" and "unknown" datasets. While this is not an optimization technique in the traditional sense, it is a common practice in deep learning to improve model performance.

In summary, while the text does not explicitly mention the use of optimization techniques such as SGD or Adam, it does describe the use of batch size and the addition of BNorm layers to prevent overfitting, which are common techniques used in deep learning pipelines.
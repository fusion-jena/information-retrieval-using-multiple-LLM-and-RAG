Based on the provided context, it can be inferred that the generalizability of the deep learning model was addressed through the use of diverse datasets and between-dataset classification. 

Firstly, the model was trained on a source dataset and used to predict images from a different target dataset. This approach, referred to as between-dataset prediction, is a way of testing the model's ability to generalize to new, unseen data. The authors used three source-target pairs, simulating two scenarios: a global image database predicting local samples and conventional images being predicted. This strategy allowed the model to be tested on a variety of datasets, thereby enhancing its generalizability.

Secondly, although the context does not explicitly mention cross-validation or stratified splitting, it does state that the CNN model was trained with N images randomly selected from the source dataset. This implies that some form of random sampling was employed, which can help to reduce overfitting and improve the model's ability to generalize.

However, the context does not provide detailed information about the composition of the datasets, such as the number of classes, the distribution of samples across classes, or the similarity between source and target datasets. Therefore, it is not possible to definitively say whether other measures such as class balance or stratified sampling were employed.

In summary, the generalizability of the deep learning model was addressed through the use of between-dataset prediction and random sampling. However, more information would be needed to fully assess the measures taken to ensure the model's generalizability.
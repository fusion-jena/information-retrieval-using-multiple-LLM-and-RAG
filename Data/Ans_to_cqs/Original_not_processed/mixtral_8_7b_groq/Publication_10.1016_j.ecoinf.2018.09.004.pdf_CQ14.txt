Based on the provided context, there is no explicit information about how the hyperparameters of the models were optimized in the mentioned paper. The text does mention the use of several machine learning algorithms, including k-Nearest Neighbors and Decision Trees, but it does not go into detail about the optimization of their hyperparameters.

However, it is common practice in machine learning to optimize hyperparameters using methods such as grid search or random search. Grid search involves specifying a range of possible values for each hyperparameter and then training the model with all possible combinations of these values. The combination that results in the best performance, as measured by a chosen metric, is then selected as the optimal set of hyperparameters.

Random search is a more efficient alternative to grid search that involves randomly selecting values for each hyperparameter within a specified range, rather than testing all possible combinations. This can be particularly useful when the number of possible combinations is very large, as is often the case with deep learning models.

In summary, while the provided context does not give specific information about how the hyperparameters of the models were optimized, it is common practice to use methods such as grid search or random search for this purpose.
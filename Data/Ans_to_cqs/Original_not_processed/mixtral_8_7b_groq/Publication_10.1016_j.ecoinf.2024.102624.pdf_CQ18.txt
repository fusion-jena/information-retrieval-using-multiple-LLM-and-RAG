Answer:

In the study described in the given context, the model performance during training was monitored using two main strategies: 10-fold cross-validation with three repetitions and the evaluation of the distribution of 30 overall accuracies (OAs).

During the model building phase, the training dataset was split into 10 folds. Nine of these folds were used for model training, and the remaining one was used for testing. This process was repeated until all folds had been used as testing data. The whole sequence was then repeated using two new random splits. This approach allowed for the assessment of model performance on different subsets of the training data, thereby providing a more robust estimate of model performance.

Additionally, the distribution of 30 OAs was evaluated to further monitor model performance. This distribution was obtained by repeating the 10-fold cross-validation process three times, resulting in a total of 30 OAs. The minimums, maximums, quartiles, and median of this distribution were then calculated. A smaller range of possible model outcomes (OAs) indicated a more reliable prediction, as it suggested that the model was less sensitive to the specific training data used.

It is important to note that this strategy was used in conjunction with hyperparameter tuning to find the best models with the largest overall accuracy. This approach helped to ensure that the models were not overfitting the training data and were able to generalize well to new, unseen data.

In summary, the strategy implemented to monitor the model performance during training involved the use of 10-fold cross-validation with three repetitions and the evaluation of the distribution of 30 OAs. This approach allowed for the robust estimation of model performance and helped to ensure that the models were able to generalize well to new data.
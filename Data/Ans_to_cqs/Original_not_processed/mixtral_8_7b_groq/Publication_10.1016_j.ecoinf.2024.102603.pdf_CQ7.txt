Based on the provided context, the criteria used to split the data for deep learning model training (e.g., train, test, validation) are not explicitly stated. However, it is mentioned that deep learning models typically involve computational requirements such as GPUs or TPUs and longer training times compared to XGBoost. It can be inferred that the process of splitting data for deep learning models might be similar to that of XGBoost, but the specific criteria are not given.

In the context of XGBoost, the training and validation sets are divided in an 80:20 ratio. XGBoost trains a series of trees, with each subsequent tree starting its learning process anew, taking into account the correction for the residual values of the previous tree. This process continues until the optimal number of trees is reached or other stopping criteria are met.

In some cases, for deep learning models, data splitting can be based on random sampling, time series, or other strategies. For instance, in the paper "Estimation of Fv/Fm in spring wheat using UAV-based multispectral and RGB imagery with multiple machine learning methods" by Wu et al. (2023), it is mentioned that the dataset was divided into training, validation, and testing sets, but the specific criteria or ratios are not provided.

In summary, while the specific criteria used to split the data for deep learning model training are not explicitly stated in the provided context, it can be inferred that the process might be similar to XGBoost or other machine learning methods. The data can be split based on various strategies, such as random sampling, time series, or other domain-specific approaches.
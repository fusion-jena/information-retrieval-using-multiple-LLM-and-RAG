Based on the provided context, the following regularization methods were not used in the deep learning pipeline to prevent overfitting:

1. Dropout: The context does not mention the use of dropout, which is a regularization technique where randomly selected neurons are ignored during training to prevent over-reliance on specific neurons.
2. L2 regularization: The context also does not mention the use of L2 regularization, which adds a penalty term to the loss function proportional to the square of the magnitude of the weights. This encourages smaller weight values and helps to distribute the importance of features more evenly.

However, the context does mention the use of 2D batch normalization between each layer. Batch normalization standardizes the inputs to each layer, which can help to reduce internal covariate shift and improve generalization. While not a traditional regularization method, batch normalization can have a regularizing effect on the model.

Additionally, the "Adam" optimization algorithm was used, which has an adaptive learning rate that can help to prevent overfitting by adjusting the learning rate for each parameter based on its historical gradient.

In summary, the provided context does not mention the use of dropout or L2 regularization, but it does mention the use of 2D batch normalization and the "Adam" optimization algorithm, which can have a regularizing effect on the model.
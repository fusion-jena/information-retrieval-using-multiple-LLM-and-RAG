Answer: The performances of the deep learning models were evaluated using precision, recall, and F1-score metrics. These metrics were calculated at both micro and macro levels. Micro-F1, precision, and recall were computed across all classes, taking into account their distribution on the whole dataset. On the other hand, macro statistics placed equal weight on all classes, meaning that class-wise differences in performance had a more significant impact on the scores. As a result, micro-recall was equivalent to the overall top-1 accuracy of the models.

In the context provided, the specific metric mentioned is the F1-score, which is a harmonic mean of precision and recall. It ranges from 0 to 1, where 1 indicates a perfect classifier. The F1-score was calculated in two ways: macro-F1 and micro-F1. Macro-F1 computes the F1-score for each class and then takes the average, treating all classes equally. In contrast, micro-F1 calculates the F1-score globally by counting the total true positives, false negatives, and false positives.

Additionally, the models' calibration was assessed using the F1-score. For instance, ResNet50-128 and ResNet18-128 performed similarly on validation and test sets, with ResNet18-128 achieving a micro-F1 of 63.44 and ResNet50 reaching 63.89 on the test set. However, the ResNet50 took about three times longer to train than ResNet18 due to its deeper architecture.

In summary, the deep learning models' performance was evaluated using precision, recall, and F1-score metrics, with a particular focus on macro-F1 and micro-F1. These metrics were used to assess the models' classification abilities and calibration.
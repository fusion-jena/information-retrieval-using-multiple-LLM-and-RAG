Based on the provided context, there is no direct mention of optimization techniques used in deep learning pipelines such as Stochastic Gradient Descent (SGD) or Adam. However, the context does discuss optimization of model parameters for Support Vector Regression (SVR) and XGBoost, which are machine learning algorithms but not deep learning algorithms.

For SVR, the optimization of the "C" parameter is discussed, which balances margin width and misclassification tolerance. The selection of the appropriate kernel function is also mentioned as part of SVR tuning.

For XGBoost, several key parameters for tuning are mentioned, including "nrounds" (boosting iterations), "max depth," "min child weight," "gamma," and "subsample."

In the context, the term "deep learning" is not used, and the focus is on machine learning algorithms such as SVR and XGBoost. Therefore, the provided context does not directly answer the query about optimization techniques used in deep learning pipelines such as SGD or Adam.
Answer:::

The deep learning model used in the study is the EfficientNet, which is a family of convolutional neural networks (CNNs) that were scaled up from the baseline EfficientNet-B0 to B7. To train the EfficientNet, the following hyperparameters were used:

1. Learning rate: The learning rate was not explicitly mentioned in the provided context. However, it was mentioned that the Adagrad optimizer was used for training the EfficientNet. The Adagrad optimizer is an adaptive learning rate algorithm that adjusts the learning rate for each parameter based on the historical gradient information.
2. Optimizer: The Adagrad optimizer was used for training the EfficientNet. Adagrad adapts the learning rate for each parameter by considering the historical gradient information. It is well-suited for sparse gradients and can handle non-stationary objectives.
3. Number of epochs: The EfficientNet was trained for 100 epochs. An epoch is one complete pass through the entire training dataset.
4. Dropout rate: The dropout rate for each task was set at 0.5. Dropout is a regularization technique that prevents overfitting by randomly dropping out a fraction of the neurons during training.
5. Batch size: The batch size was set at 20. The batch size is the number of samples processed in one iteration.
6. Data augmentation: Data augmentation was used to prepare 4000 images for each category. Data augmentation techniques such as rotation, flipping, and shifting were used to increase the diversity of the training dataset.
7. Evaluation: The evaluations were performed on the NVIDIA GeForce RTX 2080Ti GPU with 11G memory, and the software was Windows 10, Python 3.7, CUDA 10.2. Keras 2.4.3 was used to build all models.
8. Intersection over Union (IoU) threshold: IoU was defined as the intersection over the union of the two bounding boxes, which were the ground-truth bounding box and the predicted bounding box. In this paper, the threshold of IoU was set at 0.3 to consider correct or incorrect detection results.

Therefore, the hyperparameters used in the deep learning model include the Adagrad optimizer, a dropout rate of 0.5, a batch size of 20, data augmentation, and an IoU threshold of 0.3. The learning rate was not explicitly mentioned in the provided context.
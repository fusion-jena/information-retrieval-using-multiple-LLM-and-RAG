The provided context does not include information about the specific regularization methods used in the deep learning pipeline to prevent overfitting. However, it does mention the use of a model to predict the risk of Hyphantria cunea spreading and the validation of the model's accuracy.

In the context of machine learning and deep learning, overfitting is a common problem where a model learns the training data too well, including its noise and outliers, and performs poorly on unseen data. Regularization methods are techniques used to prevent overfitting by adding a penalty term to the loss function, which discourages the model from learning overly complex patterns in the training data.

Two common regularization methods are L2 regularization and dropout. L2 regularization, also known as weight decay, adds a penalty term proportional to the sum of the squares of the model's weights. This penalty term discourages the model from assigning too much importance to any single feature, leading to a simpler and more generalizable model. Dropout, on the other hand, is a technique used in neural networks where randomly selected neurons are "dropped out" during training, effectively preventing any single neuron from becoming too important.

Without specific information from the context, it is not possible to determine which regularization methods were used in this case. However, it is important to note that regularization is a crucial step in preventing overfitting and improving the generalizability of deep learning models.
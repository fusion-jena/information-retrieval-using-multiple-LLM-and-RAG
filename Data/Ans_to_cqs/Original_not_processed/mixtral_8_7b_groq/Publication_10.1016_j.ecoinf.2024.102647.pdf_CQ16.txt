Based on the provided context, the criteria used to evaluate the performance of machine learning models for predicting groundwater (GW) salinity hazard maps include coefficient of determination (R2), Nash-Sutcliffe efficiency (NSE), Kling-Gupta efficiency (KGE), and correlation coefficient (COR). However, the context does not provide information on the specific criteria used to determine when training is complete, such as a validation loss plateau.

In general, machine learning models require a training process to learn patterns from input data and optimize their parameters. To prevent overfitting and ensure the model's generalization ability, a validation dataset is often used to monitor the model's performance during training. One common approach to determine when training is complete is to observe the validation loss, which measures the difference between the model's predictions and the actual values in the validation dataset. When the validation loss stops decreasing or starts increasing, it may indicate that the model has learned enough from the training data and further training may lead to overfitting.

In the context of the provided table, the NSE values for the training and testing datasets are given for each model. NSE is a widely used criterion for evaluating the accuracy of hydrological models, which measures the proportion of the variance in the observed data that is explained by the model. NSE ranges from negative infinity to 1, where a value of 1 indicates a perfect match between the model's predictions and the observed data. In the table, the NSE values for the training and testing datasets are generally higher for the machine learning models (Random Forest and Classification and Regression Trees) than for the statistical models (frequency ratio and statistical index), indicating that the machine learning models have better predictive accuracy.

However, the NSE values alone may not be sufficient to determine when training is complete, as they only reflect the model's performance on the training and testing datasets. Other criteria, such as the validation loss or the model's complexity, should also be considered to ensure the model's generalization ability.

Therefore, based on the provided context, the specific criteria used to determine when training is complete for the machine learning models are not specified. However, in general, observing the validation loss or other criteria that reflect the model's generalization ability is recommended to prevent overfitting and ensure the model's performance on unseen data.
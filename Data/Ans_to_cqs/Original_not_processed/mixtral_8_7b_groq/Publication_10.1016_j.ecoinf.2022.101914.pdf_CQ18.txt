Answer: The strategy to monitor model performance during training is not explicitly stated in the provided context. However, it is mentioned that they used a common approach to split the data into evaluation and training datasets prior to training the models. This allowed them to evaluate individual averaged modeling approaches and the final ensemble models using a relatively independent evaluation dataset.

To assess the optimal number of background points for each individual modeling approach, they calculated the True Skill Statistic (TSS) and the Area Under the Receiver Operating Curve (AUC-ROC) for each model using external evaluation datasets set aside prior to training models. These metrics are commonly used to evaluate the performance of machine learning models, with TSS being based on a binary classification of model predictions.

It is also mentioned that they found Random Forest (RF) to be more sensitive to the number of background points used to train the model compared to other modeling methods. This suggests that they may have monitored the performance of the RF model during training with varying numbers of background points to assess its sensitivity.

Overall, while the specific strategy to monitor model performance during training is not explicitly stated, it can be inferred that they used external evaluation datasets and performance metrics to assess the optimal number of background points for each modeling approach. Additionally, they may have monitored the performance of the RF model during training with varying numbers of background points to assess its sensitivity.
Answer:::

The provided context includes information about various deep learning models and techniques, but it does not explicitly mention optimization techniques such as Stochastic Gradient Descent (SGD) or Adam. However, it does mention a "communication-efficient parallelization strategy for deep convolutional neural network training" (Lee et al., 2018b), which implies the use of some optimization technique for training.

In deep learning, optimization techniques are used to minimize the loss function and find the optimal set of weights and biases for the model. Some commonly used optimization techniques include:

1. Stochastic Gradient Descent (SGD): It is a simple optimization algorithm that updates the weights and biases in the direction of the negative gradient of the loss function.
2. Momentum: It is an optimization technique that helps SGD to overcome local minima by adding a fraction of the previous weight update to the current weight update.
3. Adagrad: It is an optimization algorithm that adapts the learning rate for each parameter based on the historical gradient information.
4. Adadelta: It is an optimization algorithm that adapts the learning rate based on the historical gradient information, similar to Adagrad, but with a running average of squared historical gradients.
5. Adam: It is an optimization algorithm that combines the ideas from Adagrad and momentum to adapt the learning rate for each parameter.

The choice of optimization technique depends on the specific problem and the characteristics of the data. For example, Adam is often used for training deep neural networks because it performs well in practice and adapts the learning rate for each parameter.

Therefore, while the provided context does not explicitly mention any optimization technique, it is likely that some optimization technique was used during the training of the deep learning models mentioned in the context.
Based on the provided context, there is no specific information given about the criteria used to determine when training is complete, such as a validation loss plateau. However, the text does mention that they used a method called "cross-validation" during model training.

In cross-validation, the data is divided into several subsets, or "folds". In this particular case, they set aside one fold for external evaluation and used the remaining four folds for internal cross-validation during model training. This means that they trained the model on three of the folds and tested it on the fourth fold, then repeated this process for each of the four folds, so that each fold was used once for testing and three times for training. This method allows them to evaluate the performance of the model and make adjustments to the training process as needed.

However, it's not specified if they used early stopping or any other specific criteria to determine when to stop the training process. Early stopping is a technique used to stop training when the model's performance on a validation set stops improving, even if the training loss is still decreasing. This can help prevent overfitting, which occurs when the model performs well on the training data but poorly on new, unseen data.

In addition, the text also mentions that they used different models such as RF, GLM, MaxEnt and ensemble models. Each of these models might have different criteria to determine when training is complete.

In summary, based on the provided context, it is not explicitly stated what criteria were used to determine when training is complete. However, it is mentioned that they used cross-validation during model training and it's likely that they used some criteria to determine when to stop the training process, but it's not specified what that criteria was.
Answer:::
The provided context does not give specific information about the criteria used to determine when training is complete in the machine learning models (MLMs) comparison. However, it does mention that a 10-fold cross-validation (CV) design was used for calibrating and validating each model. This method involves reserving 30% of the forest fire occurrence data as a validation set for each run of the cross-validation procedure, while the remaining 70% of the data is used to train the model. The approach generated 50 resulting models, consisting of 10 models for each machine learning algorithm.

In addition, the context explains that model performance was evaluated using several metrics, such as AUC, TSS, Kappa, and phi. These metrics could potentially be used to determine when training is complete. For instance, training could be stopped when the validation loss (e.g., 1-AUC) plateaus or shows only slight variations within a small range.

Moreover, the context explains that model complexity was evaluated by computing the difference between the AUC (train) and AUC (CV), represented as Δ AUC. This score offers insights into the sensitivity of the model to the training data. If the Δ AUC is small, it suggests that the model is not overfitting the training data, and therefore, training could be considered complete.

Overall, while the provided context does not explicitly mention the criteria used to determine when training is complete, it suggests that validation loss plateau or small Δ AUC could be potential criteria for determining when training is complete in the MLMs comparison.
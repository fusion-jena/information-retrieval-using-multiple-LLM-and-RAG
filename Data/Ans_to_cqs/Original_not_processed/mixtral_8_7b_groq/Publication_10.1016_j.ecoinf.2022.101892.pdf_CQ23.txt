The study took several measures to ensure the generalizability of the deep learning model. Firstly, they used a diverse dataset to train the model. The original images were captured under different illumination and angles, which helped to increase the variability of the data. To further augment the diversity of the dataset, they applied aggressive data augmentation techniques such as horizontal flipping. This allowed the model to learn from a wider range of variations and improved its ability to generalize to new, unseen data.

Secondly, the study employed a five-fold subject-exclusive (SE) strategy to ensure that the same subject did not appear in the training set and testing set at the same time. This approach helped to eliminate the influence of individual subjects on the model's performance, thereby increasing its generalizability.

Thirdly, the model was pre-trained on the ImageNet dataset, a large-scale dataset consisting of over a million images, and then fine-tuned on the PAD Full and PAD Lite datasets. This two-step training process allowed the model to learn general features from the ImageNet dataset and then adapt to the specific features of the PAD datasets. By using a pre-trained model, the study leveraged the knowledge gained from a larger and more diverse dataset, which improved the model's ability to generalize.

Lastly, the study measured the inference-time of the models to determine the most computationally efficient model. While this measure is not directly related to the model's generalizability, it is an important factor in ensuring that the model can be deployed in real-world conditions. A computationally efficient model is more likely to be used in practice, which increases its chances of being exposed to a wider range of data, thereby further improving its generalizability.

In summary, the study took several measures to ensure the generalizability of the deep learning model, including using a diverse dataset, employing a five-fold subject-exclusive strategy, pre-training the model on a large-scale dataset, and measuring the inference-time of the models. These measures helped to improve the model's ability to generalize to new, unseen data, thereby increasing its applicability in real-world conditions.
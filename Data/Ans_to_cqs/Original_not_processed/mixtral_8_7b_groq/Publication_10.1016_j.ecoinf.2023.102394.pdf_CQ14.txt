Based on the provided context, the hyperparameters of the model were optimized by minimizing the mean squared error. However, there is no explicit information given about the specific method used for hyperparameter optimization, such as grid search, random search, or any other optimization techniques.

In the text, it is mentioned that "The parameter selection was performed by minimizing the mean squared error" (Riggers et al., 2019), which implies that some sort of optimization technique was used to find the best set of hyperparameters. However, the specific method is not mentioned.

Hyperparameter optimization is crucial for the performance of any machine learning model, including Multi-Layer Perceptron (MLP) models. Common methods for hyperparameter optimization include grid search, random search, Bayesian optimization, and gradient-based optimization. Grid search and random search are exhaustive search methods that evaluate the model performance for a predefined set of hyperparameters. Bayesian optimization and gradient-based optimization methods are more sophisticated and adaptive, using probabilistic models and gradient information to search for the optimal hyperparameters.

Therefore, while it is clear that the hyperparameters of the MLP model were optimized in the study, the specific method used for hyperparameter optimization remains unclear from the provided context.
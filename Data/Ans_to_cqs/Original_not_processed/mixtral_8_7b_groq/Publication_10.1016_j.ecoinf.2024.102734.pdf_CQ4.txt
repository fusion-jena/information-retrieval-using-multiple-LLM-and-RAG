Based on the provided context, there is no explicit mention of data augmentation techniques such as flipping, rotating, or scaling being applied in the deep learning pipeline. The focus of the text is on comparing the performance of different models, specifically ResNet-based architectures, Audio Spectrogram Transformer (AST), and HuBERT, in the context of ecological informatics and audio classification.

The ResNet-50 model's performance is evaluated using mel-frequency cepstral coefficients (MFCC) represented as single-channel images. The AST and HuBERT models are transformer-based models that operate on the spectrogram and 1-D waveform, respectively. The comparison is made using a two-class binary output with a two-layer fully connected network.

The context also discusses semi-supervised learning techniques, such as transductive propagation and active learning, but these do not involve traditional data augmentation techniques. Instead, they focus on intelligently selecting or propagating labels from labeled instances to unlabeled instances based on the graph structure of the embeddings or querying high-certainty samples for model fine-tuning.

Therefore, based on the given context, there is no information about the application of data augmentation techniques like flipping, rotating, or scaling in the deep learning pipeline.
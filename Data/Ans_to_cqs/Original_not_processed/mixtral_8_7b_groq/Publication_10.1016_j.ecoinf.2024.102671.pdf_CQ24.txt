Answer:::

The provided context does not contain direct information about strategies employed to handle randomness in deep learning pipelines. However, it does mention the use of a random seed value in the context of a spatial Bayesian-network approach used for ecological-risk prevention in land ecosystems (Guo et al., 2020). The random seed value is used to initialize the random number generator, which can impact the training process of deep learning models by influencing the order in which the training dataset is presented and the weights are initialized.

In general, deep learning pipelines employ several strategies to handle randomness:

1. **Random seed value**: As mentioned, a random seed value is used to ensure reproducibility of the results. By setting a specific seed value, the same sequence of random numbers is generated, which ensures consistent initialization of weights and the order of training samples.

2. **Data augmentation**: Random transformations, such as rotation, scaling, and flipping, are applied to the training data to increase its diversity and reduce overfitting. These transformations introduce randomness during training, helping the model generalize better.

3. **Dropout**: A regularization technique used during training to prevent overfitting. Neurons are randomly dropped out with a certain probability, forcing the model to learn redundant representations and making it more robust to random variations in the input data.

4. **Batch normalization**: This technique normalizes the inputs of each layer, reducing internal covariate shift and making the network more stable during training. It introduces randomness by using a running estimate of the mean and variance of the inputs.

5. **Shuffling**: Training data is randomly shuffled before each epoch to ensure that the model does not learn a specific order of the data, which could lead to overfitting.

6. **Weight initialization**: Random weight initialization is used to break symmetry and ensure that all neurons in a layer can learn different features. Different initialization strategies, such as Xavier initialization and He initialization, are used to ensure that the variance of the inputs to each layer remains stable, preventing the vanishing or exploding gradient problem.

While the provided context does not explicitly mention these strategies, they are commonly used in deep learning pipelines to handle randomness and improve model performance.
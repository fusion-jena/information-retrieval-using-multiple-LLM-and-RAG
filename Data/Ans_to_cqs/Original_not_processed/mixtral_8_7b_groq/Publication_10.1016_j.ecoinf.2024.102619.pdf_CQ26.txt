Answer:::

The provided context discusses the use of stratified splitting during data preparation in the deep learning pipeline. Stratified sampling is a technique used to ensure that the distribution of labels remains similar in both training and testing datasets. This is particularly important in cases where there is a class imbalance in the dataset. By using stratified sampling, the class ratio is preserved, and the model is trained on a more representative sample of the data.

In addition to stratified splitting, other techniques such as oversampling, undersampling, and diverse data collection can also be used to address data bias during preprocessing. Oversampling involves increasing the number of instances of the minority class, while undersampling involves reducing the number of instances of the majority class. Both techniques can help to balance the class distribution in the dataset. However, oversampling can lead to overfitting, while undersampling can result in loss of information.

Diverse data collection is another technique that can be used to address data bias. By collecting data from a variety of sources, the model is exposed to a wider range of variations in the data, which can help to improve its generalization performance.

During data preparation, the images are resized, cropped, and normalized to a specific format expected by the pre-trained CNNs. This ensures that the data is in a consistent format and can be processed efficiently by the model.

In summary, the provided context discusses the use of stratified splitting to address data bias during preprocessing in the deep learning pipeline. Other techniques such as oversampling, undersampling, and diverse data collection can also be used to address data bias. The data is prepared by resizing, cropping, and normalizing the images to a specific format expected by the pre-trained CNNs.
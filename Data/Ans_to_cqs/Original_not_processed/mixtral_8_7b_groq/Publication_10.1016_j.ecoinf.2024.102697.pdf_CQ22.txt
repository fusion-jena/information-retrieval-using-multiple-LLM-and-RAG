The provided context discusses the evaluation of model performance in various aspects, including model complexity and contrasting remote sensing and meteorological measurements. However, it does not explicitly mention the metrics used to evaluate the performance of a deep learning model.

Nonetheless, it is generally common to use several evaluation metrics for assessing the performance of a deep learning model. These metrics can include accuracy, precision, recall, and F1-score, among others.

1. Accuracy: This is the ratio of the number of correct predictions to the total number of input samples. It is a common and straightforward metric for evaluating a model's performance. However, it may not be the best metric for imbalanced datasets.

Accuracy = (True Positives + True Negatives) / (Total Samples)

2. Precision: Also known as Positive Predictive Value, precision is the ratio of true positive predictions to the total predicted positives. It is useful when the cost of a false positive is high.

Precision = True Positives / (True Positives + False Positives)

3. Recall: Also known as Sensitivity or True Positive Rate, recall is the ratio of true positive predictions to the total actual positives. It is useful when the cost of a false negative is high.

Recall = True Positives / (True Positives + False Negatives)

4. F1-score: The F1-score is the harmonic mean of precision and recall, providing a balance between the two metrics. It is a more robust metric for imbalanced datasets.

F1-score = 2 * (Precision * Recall) / (Precision + Recall)

In the context of the study, it is possible that the authors used one or more of these metrics to evaluate the performance of their deep learning model. However, without explicit information, it is not possible to provide a definitive answer.
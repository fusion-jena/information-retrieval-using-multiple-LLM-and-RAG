The provided context discusses various models and techniques used in deep learning for sound recognition, particularly bird song classification. However, it does not explicitly mention data annotation techniques used in the deep learning pipeline.

In general, data annotation techniques used in deep learning pipelines can include:

1. Image-level annotation: This is the simplest form of annotation where a label is assigned to an entire image. For example, classifying an image as containing a bird or not.

2. Bounding box annotation: This involves drawing a bounding box around an object of interest in an image. It provides a rough localization of the object within the image.

3. Semantic segmentation: This is a more detailed form of annotation where each pixel in an image is assigned a label. It provides a precise segmentation of different objects within the image.

4. Instance segmentation: This is a further refinement of semantic segmentation where each instance of an object is segmented and distinguished from other instances of the same object.

5. Key-point annotation: This involves identifying and labeling key points or landmarks on an object, such as the corners of a bounding box or the joints of a body.

The context mentions the use of convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in sound recognition tasks. In the context of image recognition, CNNs typically use image-level annotation for training, while RNNs can use sequential data with temporal annotations.

For example, the 3DCNN-LSTM model proposed by Zhang et al. (2019) incorporates an LSTM network to make the network more sensitive to temporal changes in birdsong information. This suggests the use of temporal annotations in their data.

However, the specific data annotation techniques used in the models and studies mentioned in the context are not provided. Therefore, a precise answer to the query cannot be given based on the provided context.
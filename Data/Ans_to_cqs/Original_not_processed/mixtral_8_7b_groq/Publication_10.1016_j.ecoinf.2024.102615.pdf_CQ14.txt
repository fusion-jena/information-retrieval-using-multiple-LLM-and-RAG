Answer: The hyperparameters of the model are optimized using the grid search method. This traditional hyperparameter optimization method involves comparing the values of evaluation metrics for different parameter combinations and selecting the optimal parameters from a large search space. In this particular study, the grid search method was used to optimize five hyperparameters of the Multi-Layer Perceptron (MLP) model, including the number of neural network layers, the number of neurons per layer, dropout, learning rate, and batch size. The tuning ranges for these hyperparameters are as follows: integer values between 1 and 5 for the number of neural network layers, integer values of 64, 128, 256, 512, 1024, and 2048 for the number of neurons per layer, float values of 0.1, 0.25, 0.35, and 0.5 for dropout, float values of 0.001, 0.0001, and 0.00001 for the learning rate, and integer values of 32, 64, 128, and 256 for the batch size.

The raw dataset was divided into training, validation, and test sets, with a data partitioning ratio of 8:1:1. The training set was used to learn the parameters of the model during the training process, while the validation set was used to evaluate the modeling and tune its hyperparameters. The performance and generalizability of the optimal model were evaluated using the test set. The data partitioning was repeated ten times to reduce the influence of random splitting while ensuring converged performance.

The model performance optimization process was divided into two stages. During the first stage, the neural network structure and the number of neurons in the hidden layers were adjusted, considering 60 sets of combinations, including one to five hidden layers and neurons with a range of 64-2048 (nth power of 2). The second stage involved adjusting the percentage of dropout layers, the learning rate, and the batch size, resulting in a total of 192 combinations (4*4*3*4). The R value of the validation set was used as the performance indicator during the optimization process, where R was maximized using the grid search method. The optimal MLP architecture and parameters are presented in Table 2, with a total of 10,715,137 parameters.
Answer:::
Based on the provided context, there is no explicit mention of the regularization methods used to prevent overfitting in the deep learning pipeline. However, there are some techniques and approaches mentioned that can help mitigate overfitting to some extent.

First, the datasets were divided into three parts: training, validation, and testing, with a 60-20-20 split. This practice allows for a more robust evaluation of the models' performance and can help in identifying overfitting issues early on.

Second, the study used pre-trained models like VGG16, VGG19, and ResNet-50. These models have been trained on large datasets, which can help in reducing overfitting when fine-tuning them for smaller datasets.

Third, during training, all the patches of images were resized to a uniform size of 256 Ã— 256 pixels. This step ensures that the models are trained on a consistent input size and reduces the likelihood of overfitting to specific image resolutions or aspect ratios.

Lastly, the study employed an image classification approach that involves dividing an image into patches, predicting the class label for each patch, and then using weighted majority voting to predict the class label of the image. This method can help reduce overfitting by considering multiple patches within an image, thus providing a more holistic view of the image content.

While these techniques may not explicitly include regularization methods like dropout or L2 regularization, they can still contribute to preventing overfitting in the deep learning pipeline. Nonetheless, the context does not provide sufficient information to definitively answer the query.
The text provided discusses the use of Batch Normalization (BNorm) layers to prevent overfitting in the Waveman model, but it does not explicitly mention the use of dropout or L2 regularization.

Batch Normalization layers are added behind the 22 convolutional layers in the BatNet architecture to prevent overfitting. BNorm normalizes the activations of the neurons in a layer to have zero mean and unit variance, which can help reduce internal covariate shift and improve generalization. This is a regularization technique that works by changing the distribution of the activations during training, making the network more stable and less prone to overfitting.

The text also mentions that the model is trained using a large batch size of 128 images, which can also help reduce overfitting. A larger batch size can lead to a more stable estimate of the gradient, which can result in better convergence and less overfitting.

In summary, the regularization methods used to prevent overfitting in the Waveman model include the use of Batch Normalization layers and training with a large batch size. The text does not mention the use of dropout or L2 regularization.
Based on the provided context, there is no explicit information about the specific postprocessing steps taken after model training. However, it does mention that multiple machine learning models were evaluated for estimating water Chla concentration and turbidity in Nansi Lake using Sentinel-2 imagery.

The models included Random Forest (RF), Gradient Boosting Decision Tree (GBDT), LightGBM, XGBoost, and CatBoost. Hyperparameter optimization was performed using both random grid search and Bayesian optimization methods. The best parameters for each model were selected and compared. It was found that XGBoost demonstrated the highest robustness for estimating the water Chla concentration and turbidity.

While the text does not mention specific postprocessing steps, it does mention that "All algorithms were evaluated using the same dataset" and "performance of RF and GBDT was weaker than that of CatBoost." These statements suggest that metrics calculation was likely performed to compare the performance of different models.

In addition, the use of model fusion based on stacking in section 3.3.2 implies that some form of postprocessing was involved. Model fusion combines multiple models to improve overall performance. In this case, the authors used a meta-model to make the final prediction based on the outputs of the individual models. This process involves training the meta-model using the outputs of the individual models as inputs.

Therefore, while the specific postprocessing steps are not mentioned, it is likely that metrics calculation and possibly confusion matrix analysis were performed to compare model performance. Additionally, model fusion using stacking implies that some form of postprocessing was involved in combining the outputs of individual models.
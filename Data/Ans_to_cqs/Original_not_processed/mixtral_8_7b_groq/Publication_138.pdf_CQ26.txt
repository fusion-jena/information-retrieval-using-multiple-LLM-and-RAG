The text provided does not give specific information about data bias techniques used during the preprocessing of a deep learning pipeline. However, it does describe some techniques used in the deep learning model architecture, such as pooling, local response normalization (LRN), dropout, and different types of layers like convolution and fully connected layers.

To address the question, I will provide some common techniques used to address data bias during preprocessing:

1. **Stratified sampling**: This technique is used to ensure that the training and testing sets have a similar distribution of classes as the original dataset. It is especially useful when dealing with imbalanced datasets.
2. **Oversampling**: This technique involves increasing the number of instances from the minority class to balance the dataset. This can be done by randomly duplicating instances or by using techniques like SMOTE (Synthetic Minority Over-sampling Technique), which creates new instances based on the existing data.
3. **Undersampling**: This technique involves reducing the number of instances from the majority class to balance the dataset. This can be done randomly or by using techniques like Tomek links, which identifies and removes instances that are close to instances from the minority class.
4. **Diverse data collection**: This technique focuses on collecting data from various sources and ensuring that the dataset is representative of the population. This can help reduce bias and improve the model's ability to generalize.

In summary, while the provided text does not discuss techniques for addressing data bias during preprocessing, it is an essential aspect of deep learning pipeline development. Common techniques include stratified sampling, oversampling, undersampling, and diverse data collection.
Answer: Data augmentation techniques are commonly used in the deep learning pipeline to increase the amount of training data and improve model performance. These techniques involve creating modified versions of the existing images in the training set, such as flipping, rotating, and scaling.

Flipping is a data augmentation technique that involves creating a mirrored image of the original image. This can be done horizontally or vertically, and it helps the model to generalize better and be more robust to variations in the input data.

Rotating is another data augmentation technique that involves rotating the original image by a certain angle. This helps the model to learn invariance to rotations and be more robust to images taken at different angles.

Scaling is a data augmentation technique that involves changing the size of the original image. This can be done by zooming in or out, and it helps the model to learn to recognize objects at different scales.

These techniques are commonly used in the deep learning pipeline, as mentioned in the paper by Wang and Perez (2017) "The Effectiveness of Data Augmentation in Image Classification Using Deep Learning". They also mention other data augmentation techniques such as cropping, brightness and contrast adjustment, and adding noise.

In the context of the new task with a very limited dataset, data augmentation techniques can be particularly useful to increase the size of the training set. As mentioned in the paper by Wong et al. (2016) "Understanding data augmentation for classification: when to warp?", data augmentation can be used to create new training examples by applying warping transformations to the original images.

Additionally, data augmentation techniques can also be used in the context of few-shot learning (FSL) algorithms, which are designed to compute a classification task with only a few thumbnails to train. As mentioned in the paper by Yanbin et al. (2019) "Learning to Propagate Labels: Transductive Propagation Network for Few-shot Learning", data augmentation can be used to create new examples for the support sets, which can help the model to learn better and generalize to new classes.

In summary, data augmentation techniques such as flipping, rotating, and scaling are commonly used in the deep learning pipeline to increase the size of the training set and improve model performance. These techniques can be particularly useful in the context of few-shot learning algorithms and limited datasets.
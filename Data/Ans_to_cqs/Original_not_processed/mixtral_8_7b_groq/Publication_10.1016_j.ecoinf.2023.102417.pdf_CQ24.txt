Based on the provided context, there is no direct mention of strategies employed to handle randomness in the deep learning pipeline, such as the use of a random seed value. However, the text does discuss the importance of creating specific datasets for training deep learning models, particularly in the context of long-term biomonitoring where data distributions may change over time. This is related to the concept of data drift or domain drift, which can impact the performance of deep learning models.

In the context of deep learning model development, the text mentions the use of well-established collections of image datasets for training and evaluation. These datasets are likely curated and may include strategies to handle randomness, such as using a fixed random seed value during data splitting or augmentation.

Furthermore, in the context of semantic segmentation for biocrust coverage estimation, the text mentions the use of neural network architectures like UNet and DeepLabV3. These architectures may employ strategies to handle randomness, such as using dropout regularization to prevent overfitting or using batch normalization to reduce internal covariate shift.

In summary, while the provided context does not directly mention strategies employed to handle randomness in the deep learning pipeline, it is likely that such strategies are used during the creation of image datasets and the development of neural network architectures for semantic segmentation.
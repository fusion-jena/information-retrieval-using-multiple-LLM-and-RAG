Answer:

 Based on the provided context, there is no explicit mention of preprocessing steps such as normalization, scaling, or cleaning before training a deep learning model. However, it does mention the use of voxelization subsampling (voxel size), regularization strength, and batch size as parameters that were adjusted during the training process. 

 Voxelization subsampling is a method of reducing the point cloud data's size and complexity by dividing it into smaller 3D grids or voxels. This process can help improve the training efficiency of the deep learning model. 

 Regularization strength is a technique used to prevent overfitting during training by adding a penalty term to the loss function. This penalty term encourages the model to learn simpler and more generalizable representations of the data. 

 Batch size refers to the number of point cloud samples used in each training iteration. Adjusting the batch size can affect the model's convergence rate and accuracy.

 Additionally, the context mentions the use of the hold-out method to distinguish the test set and the training set at a ratio of 3:7 for preprocessing the original data. This method is a common technique for evaluating the performance of machine learning models by splitting the dataset into separate training and testing sets.

 Therefore, while the context does not explicitly mention normalization, scaling, or cleaning as preprocessing steps, it does mention several other preprocessing techniques used during the training of the deep learning model.
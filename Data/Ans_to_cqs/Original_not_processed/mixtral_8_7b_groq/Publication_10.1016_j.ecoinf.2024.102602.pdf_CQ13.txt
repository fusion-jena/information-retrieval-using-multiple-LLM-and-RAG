Answer:::

The hyperparameters used in the deep learning models for the classification task include the Adam optimizer algorithm, a batch size of 10, and 100 epochs. Two learning rates were chosen for each model through empirical trials over 100 epochs: 0.0001 and 0.01 (referred to as 1e-4 and 0.01, respectively). The best results were obtained by the EfficientNetB0 model with a learning rate of 0.01 (10), achieving an accuracy of 97.50%, sensitivity of 96.00%, and F1-score of 97.46%. The Inception-v3 model also showed high performance with the same learning rate, yielding an accuracy of 96.88%, sensitivity of 94.00%, specificity of 99.75%, and F1-score of 96.78%.

The deep learning models were trained and validated using 5-fold cross-validation over the dataset described in Sections 2.3 and 2.4. At each iteration, one subset was used for evaluation, while the remaining four subsets were used for training (90% of the images) and validation (10% of the images).

The performance metrics for both learning rates scenarios trained for each classification model are presented in Table 2, which includes accuracy (ACC), sensitivity (TPR), specificity (TNR), and F1-score. The best performance results for each metric and learning rate are highlighted in light grey cells.

Additionally, Table 3 provides the performance metric (mAP@0.50IOU), average inference time per image, and total loss (sum of the classification and localization losses) for each model. The best model results for performance (mAP), total loss, and speed are highlighted in light grey cells.

Answer:::
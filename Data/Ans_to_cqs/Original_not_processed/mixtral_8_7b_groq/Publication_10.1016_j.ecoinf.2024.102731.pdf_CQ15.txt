The provided context does not contain information about optimization techniques applied in deep learning pipelines such as Stochastic Gradient Descent (SGD) or Adam. However, I can provide a brief overview of these techniques based on general knowledge.

Optimization techniques are crucial in training deep learning models as they help minimize the difference between predicted and actual values, also known as the loss function. Two popular optimization techniques are Stochastic Gradient Descent (SGD) and Adam.

Stochastic Gradient Descent (SGD) is an optimization algorithm used to minimize the loss function in deep learning. It works by iteratively updating the model's parameters in the opposite direction of the gradient of the loss function concerning each parameter. The "stochastic" part of SGD refers to the random selection of training examples for each iteration, which helps the algorithm converge faster than traditional batch gradient descent.

Adam, which stands for Adaptive Moment Estimation, is another optimization algorithm used in deep learning. It combines the advantages of two other optimization techniques, AdaGrad and RMSProp, to adaptively adjust the learning rate for each parameter. Adam estimates the first and second moments of the gradient to calculate the learning rate, which results in faster convergence and better performance compared to SGD in some cases.

These optimization techniques are widely used in deep learning pipelines to train models efficiently. However, the provided context does not contain specific information about their use in deep learning.
After the model training process, there are several post-processing steps involved, as explained in the provided context. One of these steps is the calculation of quality metrics. Specifically, four quality metrics are used: Overall Accuracy (OA), Producer Accuracy (PA), User Accuracy (UA), and the F1-score. The OA takes the sum of the diagonal of the confusion matrix, while the PA and UA take the number of well-ranked individuals divided by the sum of the column and line, respectively. The F1-score is calculated as the harmonic mean of precision and recall, giving equal importance to the PA and UA.

Another post-processing step mentioned is the creation of a confusion matrix. A confusion matrix is a table that is often used to describe the performance of a classification model on a set of data for which the true values are known. It allows visualization of the performance of an algorithm by comparing the actual values (from the test set) with those predicted by the model.

In the context provided, two confusion matrices are presented. The first one (Table 10) is a land cover (LC) classification task with the raw channels as input, and the second one (Table 11) is a land use (LU) classification task with the LCE in addition to the raw channels. The resulting maps of the LU labelling task are also shown in Figure 9.

However, the context does not mention any other post-processing steps such as the creation of saliency maps. Saliency maps are a way of visualizing the parts of the input that are most important to a model's prediction. They can be useful for understanding which features the model is using to make its predictions and for identifying any potential biases in the model.

In summary, the post-processing steps described in the provided context include the calculation of quality metrics (OA, PA, UA, and F1-score) and the creation of confusion matrices for both land cover and land use classification tasks. However, it does not mention the creation of saliency maps.
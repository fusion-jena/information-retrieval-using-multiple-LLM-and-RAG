Answer:::

The data format used in the deep learning pipeline described in the given context is a multi-channel image format. The input image to the Convolutional Neural Network (CNN) is preprocessed by decomposing the spectrogram into a multi-channel image. Specifically, the original spectrogram in decibels (dB) is decomposed into a 3-channel “sound image,” with the preprocessing of harmonic, percussive, and delta (HPD) features.

The harmonic (H) and percussive (P) components are obtained using the Harmonic and Percussive Separation (HPS) method. The HPS method involves filtering the original spectrogram in the horizontal (time) and vertical (frequency) axis using median filtering. This decomposition establishes meaningful latent space representations related to the type of sound components and improves the distinction between certain animal vocalizations.

Therefore, the data format used in this deep learning pipeline is a multi-channel image format, specifically a 3-channel “sound image” obtained by decomposing the spectrogram using the HPS method. Other data formats such as audio, video, or CSV are not explicitly mentioned in the given context.

It is worth noting that the data used in this study is related to animal vocalizations, and the recordings are acquired by sending query parameters through the application programming interface (API) of Xeno-Cano, which returned a JSON object containing recording metadata. The recordings are then downloaded based on four query parameters, including audio quality, duration, maximum number of recordings allowed per species, and geographic coordinates of the recordings. However, these data formats are not directly used in the deep learning pipeline but are used to acquire and preprocess the input data for the pipeline.
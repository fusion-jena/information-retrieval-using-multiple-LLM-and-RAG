The text provided does not mention the use of a deep learning model for the experiments conducted. However, it does describe the use of cross-validation and stratified splitting to ensure the generalizability of the machine learning models used in the study.

Cross-validation is an iterative method used to calculate the expected value of a particular measure by splitting the dataset into K equally-sized parts called folds. In each iteration, different (K-1)-folds are used for training and the remaining fold for testing. The overall measure of its performance is the average of the measures of the individual iterations. This method guarantees that every instance will be used both for training and testing.

In addition to cross-validation, the study also used a variant called stratified cross-validation. In this method, in each fold, a balanced number of instances for each class is selected. This is particularly useful when dealing with imbalanced datasets, where some classes may have significantly more instances than others.

It is important to note that while these methods can help improve the generalizability of machine learning models, they may not be applicable to deep learning models. Deep learning models typically require larger amounts of data and may not benefit as much from cross-validation or stratified splitting. Instead, other techniques such as data augmentation or regularization may be used to improve the generalizability of deep learning models.

Therefore, while the study does describe the use of cross-validation and stratified splitting to ensure the generalizability of the machine learning models used, it does not mention any measures taken to ensure the generalizability of a deep learning model.
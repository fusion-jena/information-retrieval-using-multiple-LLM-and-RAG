Answer:::

Based on the provided context, the preprocessing steps involved before training a deep learning model include normalization, scaling, and cleaning. However, the specific preprocessing steps are not explicitly mentioned for each model. Therefore, I will provide a general overview of the preprocessing techniques based on the given context and common practices in deep learning.

Firstly, normalization is a crucial preprocessing step for many deep learning models. Normalization rescales the input features to a common range, often between 0 and 1. This helps the model converge faster during training and prevents exploding or vanishing gradients. In the given context, the normalization might have been applied to the input data before feeding it into the deep learning models. However, the specific normalization method is not mentioned. Common normalization techniques include min-max scaling and z-score normalization.

Secondly, scaling is another preprocessing step that rescales the input features while preserving their relative differences. Scaling is often applied when working with tree-based models or distance-based algorithms. In the given context, scaling might have been applied to the input features for some of the models. However, the specific scaling method is not mentioned. Common scaling techniques include standardization and robust scaling.

Thirdly, cleaning is an essential preprocessing step that involves removing or correcting erroneous or missing data. Cleaning helps improve the model's performance and prevent errors during training. In the given context, cleaning might have been applied to the input data before training the models. However, the specific cleaning methods are not mentioned. Common cleaning techniques include imputation, outlier detection, and data transformation.

In summary, based on the provided context, the preprocessing steps involved before training a deep learning model include normalization, scaling, and cleaning. However, the specific preprocessing methods are not mentioned for each model. Normalization helps converge the model faster during training and prevents exploding or vanishing gradients. Scaling preserves the relative differences between the input features. Cleaning removes or corrects erroneous or missing data. These preprocessing steps are crucial for improving the model's performance and preventing errors during training.
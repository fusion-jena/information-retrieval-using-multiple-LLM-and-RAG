Based on the provided context, there is no explicit information given about how the hyperparameters of the model are optimized, such as through methods like grid search or random search. However, we do have information about the specific values chosen for some of the hyperparameters in the model.

The learning rate is initially set to 10−3 for all layers, and is decreased by a factor of 10 every 20,000 iterations, stopping after 100,000 iterations. The learning rate for the last fully connected layer is set to 10−2, which is higher than the other layers due to the weights being trained starting from random. The momentum is set to 0.9, and L2 weight decay is applied with a penalty multiplier of 5×10−4. A dropout ratio of 0.5 is also used.

Additionally, the learning rate is decreased by a factor of 10 when the validation set accuracy stops improving. The momentum is set to 0.9 and weight decay to 10−4.

Therefore, while the context does not explicitly mention hyperparameter optimization methods like grid search or random search, we do have information about the specific values chosen for some of the hyperparameters and the learning rate adjustment strategy based on validation set accuracy.
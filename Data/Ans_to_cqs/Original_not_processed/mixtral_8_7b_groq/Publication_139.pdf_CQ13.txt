Answer:::

The hyperparameters used in the deep learning model include the optimizer Adam with an initial learning rate of 7 and a decay of 1x10. Early stopping was used to prevent overfitting, which typically occurred between 10 and 15 epochs. Each epoch took roughly 1 minute to train on an NVIDIA GeForce RTX 2080 Ti GPU. The model was trained using both the conventional accuracy metric and the area under the curve, as computed by a Riemann sum. In each of the five cross-validation folds, 10% of the data was reserved for comparing the training and validation loss/accuracy after each epoch, and a further 10% was reserved for performance testing after all training had ceased. The results reported are the average after five-fold cross-validation. Pre-training on ResNet50 and Inception neural networks did not meaningfully improve classification performance, so a simpler architecture was used instead.
Answer:::

The context provided does not explicitly mention the method used to optimize the hyperparameters of the model. However, it does give some insight into the specific hyperparameters used in the model.

The model optimizes cross-entropy loss via mini-batch stochastic gradient descent during training. The learning rate is set to a constant value of 0.01. Dropout is applied to the input of the last two layers with a probability of 0.5. L2-regularization is applied to the weights of the last two layers with a penalty factor of 0.001. The model is trained for 100 epochs and is checkpointed after each epoch. A validation set is used to identify the parameter setting achieving the highest classification accuracy.

Based on this information, it can be inferred that the hyperparameters were likely set using manual tuning or expert knowledge, rather than a systematic search method such as grid search or random search. This is a common approach when the number of hyperparameters is relatively small, as in this case.

In summary, while the context does not explicitly mention the method used to optimize the hyperparameters, it can be inferred that manual tuning or expert knowledge was used to set the learning rate, dropout probability, regularization penalty factor, and number of epochs.
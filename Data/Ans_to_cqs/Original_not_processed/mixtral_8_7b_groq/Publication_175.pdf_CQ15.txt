Answer:::

The text does not provide explicit information about the specific optimization techniques used in the ResNet 100 deep learning pipeline. However, it is mentioned that the ResNet architecture uses back-propagation, a process that adjusts the inner parameters of the model to minimize classification error. Common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adam, and Adagrad. SGD is a simple optimization algorithm that iteratively adjusts the model's parameters in the direction that minimally reduces the loss function. Adam, which stands for Adaptive Moment Estimation, is a more advanced optimization algorithm that combines the ideas of momentum and adaptive learning rates. Adagrad is another optimization algorithm that adapts the learning rate for each parameter individually, allowing for better performance on problems with sparse gradients. It is possible that any of these optimization techniques, or others, were used in the deep learning pipeline described in the text.
Based on the provided context, there is no explicit mention of techniques used to address data bias during the preprocessing of the deep learning pipeline. However, it is mentioned that the training set's data is normalized and standardized to fit into the network. Data normalization and standardization are common preprocessing steps that can help reduce data bias by ensuring that all features have similar scales and ranges.

Moreover, the batch size of 32 was chosen to make full use of computer resources and not exceed the memory limit. This technique can indirectly help reduce data bias by ensuring that the model is trained on a diverse and representative sample of the data.

The context also mentions that the optimizer used is Adam processing, and the learning rate varies with group based on the cosine function's law, often in the range between 0.0 and 1.0. Adaptive learning rates can help reduce data bias by adjusting the learning rate for different groups or subsets of the data, ensuring that each group is appropriately weighted and represented in the model.

Finally, the use of an early stopping function can help prevent overfitting and reduce data bias by halting the learning process if there is no improvement in accuracy after 20 epochs.

In summary, while the context does not explicitly mention techniques such as stratified splitting, oversampling, undersampling, or diverse data collection, it does mention several preprocessing and optimization techniques that can help reduce data bias in the deep learning pipeline. These techniques include data normalization and standardization, batch size selection, adaptive learning rates, and early stopping.
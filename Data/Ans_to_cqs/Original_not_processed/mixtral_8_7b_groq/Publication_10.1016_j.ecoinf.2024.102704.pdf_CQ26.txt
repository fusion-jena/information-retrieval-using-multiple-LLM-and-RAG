The provided context does not include information about techniques used to address data bias during preprocessing of the deep learning pipeline. However, it does mention that a "targeted fish species dataset" was divided into training and validation sets in an 8:2 ratio using a random selection process. This method of splitting the dataset, without any mention of stratified splitting, oversampling, or undersampling, does not seem to be addressing data bias.

In general, data bias can be addressed during preprocessing by using techniques such as:

1. Stratified splitting: This method involves dividing the dataset into subgroups based on certain characteristics (e.g., class labels) and then ensuring that each subgroup is proportionally represented in both the training and validation sets.

2. Oversampling: This technique involves increasing the number of instances from the minority class in the dataset to balance it with the majority class.

3. Undersampling: This method involves reducing the number of instances from the majority class in the dataset to balance it with the minority class.

4. Diverse data collection: This involves collecting data from various sources and ensuring that the dataset is representative of the population.

It's important to note that reducing memory access in deep learning models, as mentioned in the provided context, is not a technique used to address data bias during preprocessing. It is a technique used to improve the computational efficiency of the models.
The text provided gives some information about how the deep learning model was trained, but it does not directly describe the process of deploying the trained model. However, it does provide some clues.

The model was implemented in Python using Lasagne, a lightweight library for building and training neural networks. This suggests that the model could be deployed in a Python environment, such as a web server or a desktop application.

Data stream multiplexing was implemented using Pescador, which is a tool for managing data streams in real-time systems. This suggests that the model may be intended for use in a real-time system, where it needs to process data as it arrives.

The model was trained for 100 epochs and checkpointed after each epoch. Checkpointing is the process of saving the model's parameters to disk at regular intervals during training. This allows the training process to be restarted from the last checkpoint if it is interrupted, which is useful for long-running training jobs.

The fact that the model was checkpointed after each epoch also suggests that the trained model may be saved to disk for later use. Model serialization is the process of saving a trained model to disk in a format that can be loaded and used later. This is a common practice in deep learning, as it allows trained models to be shared and reused.

Therefore, based on the information provided, it seems likely that the trained deep learning model was deployed by serializing it to disk and loading it in a Python environment, possibly as part of a real-time data processing system. However, the text does not provide enough information to confirm this with certainty.
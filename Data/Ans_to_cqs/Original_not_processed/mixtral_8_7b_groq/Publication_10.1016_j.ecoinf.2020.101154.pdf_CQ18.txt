Answer:::

The strategy implemented to monitor the model performance during training involves the use of the early stopping technique. This method is used to determine when to stop the model training to avoid overfitting. By using early stopping, the number of iterations required until convergence is automatically determined, and the training process is halted once this point is reached. This approach ensures that the model does not continue to train and fit to the data beyond the point of optimal performance, reducing the risk of overfitting and improving the model's ability to generalize to new data.

In addition to early stopping, a manual tuning process is also implemented to optimize the model's hyperparameters. For the RI models, a grid search is performed on both the CS and NW datasets, with a two-part split, to identify the hyperparameters that result in the highest level of accuracy. These hyperparameters include a learning rate of 0.05 and a maximum tree depth of 4 for both the CS and NW models. For the RY models, a slow learning rate of 0.005 is found to be the best performing, and the number of iterations is kept at 500.

It is also worth noting that the data splitting approach used for the RY models is different from the standard approach of randomly splitting all individual observations. Instead, individual observations are grouped according to year of capture, and years are randomly selected for inclusion in the train and test datasets. This ensures that the model is tested using previously unseen combinations of explanatory variables, providing a more robust test of model performance in this context.

Overall, the strategy implemented to monitor the model performance during training involves the use of early stopping, manual tuning of hyperparameters, and a data splitting approach that ensures the model is tested using previously unseen combinations of explanatory variables. This comprehensive approach helps to ensure that the model is optimized for performance and able to generalize to new data.
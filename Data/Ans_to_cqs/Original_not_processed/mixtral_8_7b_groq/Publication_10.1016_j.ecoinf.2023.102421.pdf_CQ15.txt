The provided context discusses the hyperparameter optimization techniques used in machine learning models, specifically XGBoost and LightGBM, for a given task. However, it does not mention any optimization techniques used in deep learning pipelines such as Stochastic Gradient Descent (SGD) or Adam.

Hyperparameter optimization techniques used in the study include Bayesian optimizers, Tree-structured Parzen Estimator (TDO), and Bayesian Optimization with HyperBand (BOHB). These optimization algorithms are applied to tune the hyperparameters of XGBoost and LightGBM models. The objective function used for optimization is Root Mean Square Error (RMSE).

The study compares the performance of XGBoost and LightGBM models with different optimizers using 10-fold cross-validation and mean RMSE. The Wilcoxon Signed-Rank test with paired samples is used to test the differences between the RMSEs. The results show that the performance of XGBoost-TDO and LightGBM-TDO appears to be the same.

Furthermore, the study interprets the best model to understand what operates inside the 'black box' by evaluating several performance indicators such as feature importance, feature interactions, and contributions of all features.

In summary, the provided context discusses the hyperparameter optimization techniques used in machine learning models, XGBoost and LightGBM, but it does not mention any optimization techniques used in deep learning pipelines such as SGD or Adam.
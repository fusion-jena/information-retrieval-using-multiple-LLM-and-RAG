Answer:::

In the deep learning pipeline, data augmentation techniques such as flipping, rotating, and scaling were applied to the moths dataset. Specifically, vertical and horizontal flipping, zooming, adjusting illumination intensity, and rotating the images by different degrees were used to increase the number of training data and create a uniform distribution of species. This operation resulted in a scaled dataset that was 32 times larger than the original, with 72,000 images and 8,000 data points for each class after augmentation. Out of this dataset, 80% was used for training, and 20% was used for validation of the CNN model.

It is worth noting that the limited number of images (250) for each species was mainly due to the fewer occurrences of some species, such as Agrotis puta and Mythimna pallens. Therefore, data augmentation was essential to provide more training data and prevent overfitting when using customized CNN architectures and state-of-the-art CNN optimized architectures, such as EfficientNetB0 and DenseNet121.

For instance, the customized CNN architectures were compared with EfficientNetB0, which is scaled to work with a small image input size of 224 Ã— 224 pixels and has 4,030,358 learnable parameters. Using the moths dataset with the same data augmentation, the EfficientNetB0 achieved a F1-score of 88.62%, which is lower than the top five best architectures. Similarly, DenseNet121 with 7,047,754 learnable parameters gave a F1-score of 84.93% with data augmentation, which is even lower than the top five best architectures.

Therefore, data augmentation techniques played a crucial role in the deep learning pipeline by increasing the number of training data, creating a uniform distribution of species, and preventing overfitting when using customized and state-of-the-art CNN architectures.
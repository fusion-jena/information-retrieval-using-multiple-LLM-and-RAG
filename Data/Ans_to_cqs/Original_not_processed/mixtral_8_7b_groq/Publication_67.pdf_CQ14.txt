Based on the provided context, the hyperparameters of the model are optimized using two optimization techniques, namely transfer learning and data augmentation. However, there is no explicit mention of methods such as grid search or random search for hyperparameter optimization.

In the training phase, the authors trained five Convolutional Neural Network (CNN) models based on Inception v.3. The use of the Inception v.3 architecture suggests that the authors utilized a pre-trained model and fine-tuned it for their specific task, which is an example of transfer learning. Transfer learning is a technique where a pre-trained model is used as a starting point for a new model, and the weights of the pre-trained model are fine-tuned during the training process. This approach leverages the knowledge gained from the pre-trained model and can help improve the performance of the new model, especially when the amount of training data is limited.

Additionally, the authors used data augmentation as another optimization technique. Data augmentation is a technique where the training data is artificially expanded by applying various transformations such as rotation, scaling, and flipping. This approach can help improve the generalization ability of the model by providing more diverse training data.

While the context does not provide specific details about methods such as grid search or random search for hyperparameter optimization, it is possible that the authors used these methods for optimizing other aspects of the model, such as regularization parameters or learning rates. However, without further information, it is not possible to provide a definitive answer.
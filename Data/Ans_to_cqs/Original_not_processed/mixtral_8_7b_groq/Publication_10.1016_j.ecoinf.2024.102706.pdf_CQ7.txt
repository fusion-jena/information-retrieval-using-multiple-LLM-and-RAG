The provided context discusses the methods used to train and evaluate machine learning models for predicting canopy cover, but it does not explicitly mention the criteria used to split the data for deep learning model training. Therefore, based on the information given, it is not possible to provide a specific answer to the query.

However, I can explain the general approach to splitting data for model training in machine learning and deep learning.

In machine learning and deep learning, it is common to split the available data into three subsets: training, validation, and testing. The training set is used to train the model, the validation set is used to tune hyperparameters and select the best model, and the testing set is used to evaluate the final model's performance.

The splitting criteria can vary depending on the problem, dataset size, and specific requirements. Here are some common approaches:

1. Random splitting: The data is randomly divided into the three subsets with a predefined ratio, such as 70% for training, 15% for validation, and 15% for testing. This method is simple and unbiased but may not be suitable for time-series or spatial data.
2. Time-based splitting: For time-series data, it is common to use a time-based split. The data is divided into training, validation, and testing sets based on time. For example, the first 70% of the time series is used for training, the next 15% for validation, and the last 15% for testing. This approach ensures that the model is trained on past data and tested on future data, which is essential for time-series predictions.
3. Stratified splitting: When the data is imbalanced or contains categorical variables, stratified splitting can be used. This method ensures that each subset has a similar distribution of classes or categories as the original data. For example, if the canopy cover data has two classes (high and low cover), stratified splitting would ensure that both classes are represented proportionally in the training, validation, and testing sets.

In the provided context, the authors used a 5-fold cross-validation grid search strategy to tune hyperparameters within the training dataset for all machine learning models. Cross-validation is a technique used to assess the performance of a model by dividing the data into k subsets (folds) and averaging the performance metrics. In this case, the data was divided into five folds, and each fold was used as a validation set while the remaining four folds were used for training. This approach helps to reduce overfitting and provides a more robust estimate of the model's performance.

In summary, the provided context does not mention the criteria used to split the data for deep learning model training. However, I have explained the general approaches to splitting data in machine learning and deep learning, which can be applied depending on the problem and dataset characteristics.
The provided context discusses an instance of overfitting in a deep learning model, specifically the YOLOv8n model, and the measures taken to address it. However, it does not explicitly mention the regularization methods used to prevent overfitting in the first place. Therefore, I will provide a general explanation of these methods.

Regularization methods are techniques used in deep learning to prevent overfitting by reducing the complexity of the model and discouraging overly large weights. Two common regularization methods are L2 regularization and dropout.

L2 regularization, also known as weight decay, adds a penalty term to the loss function, which is the sum of the squares of all the model's weights. This penalty term discourages large weights, making the model simpler and less prone to overfitting. During training, the model learns to balance the trade-off between minimizing the loss function and keeping the weights small.

Dropout is another regularization technique that randomly sets a fraction of the model's neurons to zero during training. This prevents the neurons from co-adapting too much, making the model more robust and less likely to overfit. Dropout effectively creates an ensemble of models, as each training iteration uses a different subset of the neurons.

In the context provided, the authors addressed overfitting by modifying the training data and using early stopping. They observed overfitting when training the YOLOv8n model on a combination of their dataset and the MNIST dataset. To tackle this issue, they trained a separate model (Model 4) solely on their dataset, which resulted in better performance. However, they did not explicitly mention using L2 regularization or dropout as regularization methods.

In summary, L2 regularization and dropout are two common regularization methods used to prevent overfitting in deep learning pipelines. The provided context discusses an instance of overfitting and how it was addressed but does not explicitly mention the use of these regularization techniques.
Answer:::

In the described study, a specific strategy was implemented to monitor and evaluate the performance of the predictive models during their training phase. This strategy involved dividing the preprocessed data into three sets: a training set (80%), a validation set (10%), and a test set (10%).

The training set was used to train the models, while the validation set was employed to assess the models' performance and validate the training process. The test set, which the model had not previously seen during the training or validation phases, was used to indicate the model's performance in real-world scenarios.

Moreover, the study implemented cross-validation, a technique used to assess how well a model will generalize to an independent data set. In this case, cv = 10 was used, meaning that the data was divided into ten subsets, and the model was trained and validated ten times, with each subset used once as a validation set. The average performance across these ten runs was then used as the overall measure of the model's performance.

This strategy allowed for a more extensive training set, which could potentially lead to a better-generalized model. It also provided a means to monitor the models' performance during training, ensuring that they were not overfitting to the training data and were able to generalize well to unseen data.

In addition to this strategy, the study also emphasized the importance of the choice and tuning of model parameters in model performance. Parameters like 'max_depth' in tree-based models and 'learning_rate' in boosting models, as well as the number of estimators or iterations, were identified as critical factors impacting the models' ability to learn the data without overfitting.

In summary, the study implemented a comprehensive strategy to monitor the models' performance during training. This strategy involved dividing the data into training, validation, and test sets, using cross-validation, and focusing on the choice and tuning of model parameters. This approach allowed for a better understanding of how well the models were learning from the data and generalizing to unseen data, thereby ensuring the creation of more effective predictive models for oil palm yield prediction.
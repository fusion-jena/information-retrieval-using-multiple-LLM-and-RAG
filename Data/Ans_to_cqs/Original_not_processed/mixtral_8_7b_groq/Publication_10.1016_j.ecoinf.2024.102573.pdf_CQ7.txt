Answer:::

The data is split for deep learning model training using specific criteria, including a one-time run on the test set, computational expense, and limited machine memory. The model initially used five-second chunks with a one-second overlap for the test set, predicting all five-second excerpts and averaging their outputs to indicate the recording-level decision. However, this strategy was deemed too expensive computationally.

To address this, two different cropping methods were employed for the training and validation recordings to expedite the speed of experimentation. For the train and validation files, the criteria for splitting the data are not explicitly stated in the text. Still, it can be inferred that they were split according to a different strategy than the test files due to the use of different cropping methods.

Furthermore, a good evaluation strategy without temporal information involves using the whole recording. This approach requires windowing the lengthy recordings because of limited machine memory. Each test recording is thus split into several chunks. However, this method is not explicitly stated as being used for the train and validation files.

The text also provides information on the number of recording files and their total duration for various species, including train files, validation files, and test files. This information is presented in Tables 1 and 2. However, the specific criteria used to split the data for these species are not provided.

In summary, the criteria used to split the data for deep learning model training include a one-time run on the test set, computational expense, and limited machine memory. Two different cropping methods were used for the training and validation recordings. However, the specific criteria used to split the data for these files are not explicitly stated in the text.
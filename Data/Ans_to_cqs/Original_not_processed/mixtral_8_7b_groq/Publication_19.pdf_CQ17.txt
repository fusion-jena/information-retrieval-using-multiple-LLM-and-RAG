Based on the provided context, there are no explicit mentions of specific regularization methods such as dropout or L2 regularization that were used in the deep learning pipeline to prevent overfitting. However, there are some techniques mentioned that can help mitigate overfitting to some extent.

First, a 60-20-20 train/validate/test regime was used to split the data into training, validation, and testing sets. This approach allows for a portion of the data to be set aside for validation and testing, which can help ensure that the model is not overfitting to the training data.

Second, the models were initially trained by freezing the feature extraction layers and training only the classification layers. This technique, known as transfer learning, can help prevent overfitting by leveraging pre-trained weights that have already been trained on a large dataset (ImageNet, in this case).

Third, stochastic gradient descent (SGD) was used as the optimizer for all four models. While SGD does not include any built-in regularization techniques, it can be used in conjunction with other regularization methods.

Finally, the authors mention that they will pursue techniques such as SMOTE oversampling or class weights in a second phase of this research. These techniques can help mitigate the impact of class imbalance, which can sometimes lead to overfitting.

Overall, while the context does not provide explicit evidence of regularization methods such as dropout or L2 regularization, there are several techniques mentioned that can help prevent overfitting in the deep learning pipeline.
Answer:::

The hyperparameters of the models used in this study were not optimized using methods like grid search or random search. Instead, default values were used for most hyperparameters, with a few exceptions. For the stochastic gradient descent with momentum optimizer used to train the Artificial Neural Network (AN) in MATLAB, the initial learning rate was set to 0.001, and the maximum number of epochs was set to 15. For the 'classical' machine-learning algorithms (AB, GB, HB, and LD) run in Python using the scikit-learn toolbox, hyperparameters were left at scikit-learn default values, except for the learning rate of GB and HB (which were set to 0.15) and the tolerance threshold value for LD (set to 1 × 10−4).

The study prioritized comparing the performance of different algorithms over fine-tuning hyperparameters. The authors noted that the models did not include picture or specimen random effects, which could lead to convergence issues and potentially underestimate uncertainty in model-based estimates and predictions.

Answer:::
Answer:

I don't know the exact method used to optimize the hyperparameters of the model in the described research because no data or specific techniques were mentioned in the provided context. However, I can suggest some common methods used for hyperparameter optimization in similar domains.

Hyperparameter optimization is an essential step in machine learning and deep learning model development. It involves tuning the internal parameters of a model to improve performance. Some popular methods for hyperparameter optimization include grid search, random search, Bayesian optimization, and gradient-based optimization.

Grid search is a systematic method that involves testing all possible combinations of hyperparameters within a predefined range. Although it is exhaustive, it guarantees finding the best combination of hyperparameters. However, it can be computationally expensive, especially for models with many hyperparameters.

Random search is a more efficient alternative to grid search. Instead of testing all possible combinations, it selects a subset of hyperparameters at random. This method reduces the computational cost while still providing a good chance of finding a well-performing set of hyperparameters.

Bayesian optimization is a more sophisticated method that uses Bayesian inference to model the relationship between hyperparameters and model performance. It iteratively selects the most promising hyperparameters to test, focusing on areas of the search space likely to contain good solutions. This method is particularly useful for models with many hyperparameters or expensive-to-evaluate objective functions.

Gradient-based optimization is another method that leverages gradient information to optimize hyperparameters. It requires differentiable objective functions and is particularly suitable for neural networks. Popular gradient-based optimization algorithms include Adam, RMSprop, and stochastic gradient descent.

In summary, while I cannot provide a specific answer regarding the hyperparameter optimization method used in the described research, I have outlined some common techniques used in similar domains. These methods include grid search, random search, Bayesian optimization, and gradient-based optimization. The choice of method depends on the specific problem, the model, and the available computational resources.
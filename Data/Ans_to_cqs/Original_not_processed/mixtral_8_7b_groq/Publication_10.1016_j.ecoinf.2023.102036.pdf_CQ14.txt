Answer:::

The hyperparameters of the model are optimized using Bayesian optimization. This method is preferred over traditional techniques like grid search or random search because it is more efficient and effective in exploring the search space.

In grid search, the algorithm would evaluate the function at every combination of the parameters in the predefined grid. This can be very time-consuming and computationally expensive, especially when the number of parameters and the size of the search space increase. On the other hand, random search randomly selects parameter values within the specified ranges and evaluates the function at these points. While random search is faster than grid search, it may still require a large number of function evaluations to find the optimal parameters.

In contrast, Bayesian optimization uses the information learned from previous function evaluations to choose the next set of parameters to evaluate. It calculates the posterior predictive distribution for the function's value at each point in the search space and selects the point with the highest expected improvement. This approach allows Bayesian optimization to focus on the most promising regions of the search space, making it more efficient than grid search or random search.

In this specific system, Bayesian optimization is used to tune the learning rate, momentum, standard deviation threshold, and input size of the model. The optimization process consists of 3000 training iterations with a batch size of 25 individuals, validating every 300 iterations. TensorBoard is used to study the performance of the runs, and the selected parameters correspond to the run shown in Supplementary Fig. S3.

The optimization is performed under Python 3.9.5, using TensorFlow 2.4.1, on an Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz processor and 32GB RAM memory. The learning is performed with the Stochastic Gradient Descendent (SGD) method, optimizing its parameters and binary cross-entropy as a loss function. The algorithm is also asked to exceed a certain threshold of variability in its predictions, i.e., that the standard deviation of the predicted probabilities exceeds a certain threshold, so that the output of the classifier is not almost the same for all images that feed the model.

Overall, Bayesian optimization is a powerful tool for hyperparameter tuning, allowing for efficient exploration of the search space and accurate identification of the optimal parameters.
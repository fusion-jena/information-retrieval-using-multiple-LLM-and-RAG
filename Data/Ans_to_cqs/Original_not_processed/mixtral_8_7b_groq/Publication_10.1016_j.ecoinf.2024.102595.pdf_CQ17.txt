The text provided discusses the development of predictive models for oil palm yield using both deep learning algorithms and traditional machine learning methods. However, it does not explicitly mention the regularization techniques used to prevent overfitting in the deep learning pipeline. 

However, it is mentioned that hyperparameter tuning was performed using GridSearchCV with cross-validation, which is a technique used to avoid overfitting by validating the model on different subsets of the data. 

In the context of deep learning, common regularization techniques used to prevent overfitting include L1 and L2 regularization, dropout, and data augmentation. L1 and L2 regularization add a penalty term to the loss function, which discourages the model from learning overly complex patterns in the training data. Dropout is a technique where randomly selected neurons are "dropped out" during training, which helps to prevent over-reliance on specific neurons and therefore overfitting. Data augmentation is a technique where the training data is artificially expanded by applying transformations such as rotation, scaling, and flipping to the existing data.

Without specific information from the text, it is not possible to say for certain which regularization techniques were used in this study. However, based on common practices in deep learning, it is likely that one or more of these techniques were used to prevent overfitting.
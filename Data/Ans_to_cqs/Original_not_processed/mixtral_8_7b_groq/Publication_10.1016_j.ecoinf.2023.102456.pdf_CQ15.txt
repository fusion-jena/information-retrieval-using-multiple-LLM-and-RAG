The provided context discusses the use of the Gumbel softmax function in a machine learning algorithm, but it does not mention any optimization techniques used in the deep learning pipeline, such as Stochastic Gradient Descent (SGD) or Adam.

However, I will provide some information about these optimization techniques for your knowledge.

Optimization techniques are essential in deep learning to minimize the loss function and find the optimal set of weights and biases. Some popular optimization techniques are:

1. Stochastic Gradient Descent (SGD): It is a simple optimization algorithm that updates the weights and biases in the direction of the negative gradient of the loss function concerning a single training example. SGD is widely used due to its simplicity and efficiency.
2. Momentum: It is a technique that helps SGD overcome local minima by adding a fraction of the previous weight update to the current weight update. Momentum helps the optimization process converge faster.
3. Adagrad: It is an optimization algorithm that adapts the learning rate for each parameter. Adagrad adjusts the learning rate based on the historical gradient information of each parameter.
4. Adadelta: It is an extension of Adagrad that uses a running average of squared gradients instead of the historical gradient information. Adadelta is more robust to noisy gradients.
5. Adam: It is an optimization algorithm that combines the advantages of Momentum and Adagrad. Adam uses the running average of gradients and squared gradients to adapt the learning rate for each parameter.

These optimization techniques are widely used in deep learning pipelines to minimize the loss function and find the optimal set of weights and biases. The choice of optimization technique depends on the specific problem and the characteristics of the data.

Therefore, the provided context does not mention any optimization techniques used in the deep learning pipeline. However, optimization techniques such as SGD, Momentum, Adagrad, Adadelta, and Adam are widely used in deep learning pipelines.
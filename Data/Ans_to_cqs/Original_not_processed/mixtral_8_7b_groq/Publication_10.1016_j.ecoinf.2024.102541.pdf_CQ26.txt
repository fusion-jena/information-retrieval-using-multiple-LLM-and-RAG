The provided context does not contain specific information about techniques used to address data bias during preprocessing of the deep learning pipeline. However, it does mention the use of the Microsoft COCO dataset in a deep learning study (Lin, T.-Y., et al., May 2014). The Microsoft COCO dataset is known for its diverse set of common objects in context, which can help reduce data bias.

The techniques you mentioned, such as stratified splitting, oversampling, undersampling, and diverse data collection, are common methods used to address data bias during preprocessing.

Stratified splitting involves dividing the data into subgroups that are representative of the overall population, and then randomly sampling from each subgroup to ensure that each group is proportionally represented in the training and testing sets.

Oversampling and undersampling are techniques used to adjust the class distribution in the dataset. Oversampling involves duplicating samples from the minority class to increase its representation in the dataset, while undersampling involves removing samples from the majority class to reduce its representation.

Diverse data collection involves actively seeking out and collecting data from a wide range of sources to ensure that the dataset is representative of the overall population.

In summary, while the provided context does not specifically mention the techniques used to address data bias during preprocessing, it does mention the use of a diverse dataset, which can help reduce data bias. Additionally, common techniques used to address data bias include stratified splitting, oversampling, undersampling, and diverse data collection.
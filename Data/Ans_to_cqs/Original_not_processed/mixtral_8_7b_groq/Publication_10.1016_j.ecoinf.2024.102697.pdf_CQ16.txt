Based on the provided context, there is no explicit information given about the specific criteria used to determine when training is complete for the machine learning models. However, it can be inferred that the training process was repeated multiple times and evaluated using various cross-validation techniques, which could have been used to assess convergence or a plateau in validation loss.

In the 2.7 section, it is mentioned that a repeated nested k-fold cross-validation approach was employed for all ML models. This method involves an outer 10 k-fold cross-validation, which provides an initial division of the data into holdout data and data for model development. Then, an inner 5 k-fold cross-validation is used to minimize the bias caused by tuning of hyperparameters on training results. The training performance is calculated based on the inner cross-validated ensemble, while the testing performance is determined by selecting the optimal set of hyperparameters from the entire training set to predict on the holdout data. This process was repeated 15 times for each unique model.

Additionally, in the 3.1 section, it is mentioned that proper selection of variables, i.e. feature selection, can minimize training costs, measurement requirements, and decrease the likelihood of overfitting. This suggests that the training process may also involve some form of feature selection or dimensionality reduction, which could be used to monitor the convergence of the training process.

Therefore, while the specific criteria for determining when training is complete are not provided in the context, it can be inferred that the training process was evaluated using various cross-validation techniques and potentially feature selection methods, which could have been used to assess convergence or a plateau in validation loss.
Answer:::
In the provided context, the specific regularization methods used to prevent overfitting in the deep learning pipeline are not explicitly mentioned. However, it does mention that residual structures were incorporated into the network to mitigate overfitting issues.

Residual modules, also known as residual connections or residual learning, are a type of regularization technique that helps to alleviate the vanishing gradient problem and enables the training of much deeper networks. They work by allowing the gradient to be directly backpropagated to earlier layers, preserving the flow of information and gradients through the network. This can help to reduce overfitting by promoting the learning of identity functions, which in turn can make the network more robust and generalizable.

Although not explicitly stated in the context, other regularization techniques such as dropout or L2 regularization could have also been used in conjunction with residual modules. Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent overfitting by promoting redundancy and making the network more robust. L2 regularization, on the other hand, adds a penalty term to the loss function that is proportional to the square of the magnitude of the weights, which encourages the weights to be small and distributed evenly, thereby reducing overfitting.

It's worth noting that the context also mentions that the number of convolutional kernels within each block was reduced to mitigate overfitting. This is another form of regularization, known as architectural or structural regularization, which involves reducing the complexity of the model by limiting the number of parameters or the capacity of the model.

In summary, while the specific regularization methods used to prevent overfitting in the deep learning pipeline are not explicitly mentioned in the provided context, it does mention the use of residual modules, which are a type of regularization technique that helps to alleviate the vanishing gradient problem and enables the training of much deeper networks. Other regularization techniques such as dropout or L2 regularization could have also been used in conjunction with residual modules. Additionally, the context also mentions the use of architectural regularization by reducing the number of convolutional kernels within each block.
The performance of the deep learning model is evaluated using the Average Precision (AP) metric, which is calculated by averaging the precision values at various Intersection over Union (IoU) thresholds. Specifically, for the COCO dataset, the AP is computed as the average AP for IoU thresholds between 0.50 and 0.95 in steps of 0.05. Additionally, the AP is also averaged over all classes, resulting in the mean Average Precision (mAP).

Furthermore, the Average Recall (AR) is also used as an evaluation metric. The AR is averaged over different IoU thresholds, and for the COCO dataset, the AR is calculated for the 10 IoU thresholds between 0.50 and 0.95. The AR is also averaged over all K classes, resulting in the mean Average Recall (mAR).

In the context of object detection, IoU is defined as the area of overlap between the predicted bounding box (Bp) and the ground truth bounding box (Bgt), divided by the area of union of the predicted and ground truth bounding boxes.

It is important to note that the deep learning model's performance is evaluated using these metrics for each class, and the results are reported separately for each class. Additionally, the ResNet variants are trained for 40 epochs, starting with an initial learning rate of 0.001, and the learning rate is decreased every 10 epochs by multiplying it with a gamma of 0.1. The Stochastic Gradient Descent (SGD) with a momentum of 0.9 is used for optimization.

In summary, the performance of the deep learning model is evaluated using the Average Precision (AP) and Average Recall (AR) metrics, which are calculated at various Intersection over Union (IoU) thresholds and averaged over all classes. The IoU is defined as the area of overlap between the predicted and ground truth bounding boxes, divided by the area of union of the predicted and ground truth bounding boxes.
Based on the provided context, there is no explicit mention of the techniques used for addressing data bias during preprocessing in the deep learning pipeline. However, it does mention the use of data augmentation techniques, such as rotation, noise, and flipping, which can help increase the diversity of the training data and reduce overfitting.

Data augmentation is a technique used to generate more training data by applying transformations to the existing data. By applying these transformations, such as rotation, flipping, and adding noise, the model can learn to be more invariant to these changes, which can help improve its ability to generalize to new, unseen data. In this study, data augmentation was used to increase the size of the dataset by six times, which can help reduce overfitting and improve the model's ability to generalize.

However, data augmentation alone may not be sufficient to address data bias, and other techniques such as stratified sampling, oversampling, and undersampling may be required. Stratified sampling is a technique used to ensure that the training data is representative of the overall population by dividing the data into subgroups (or strata) based on certain characteristics, and then sampling from each subgroup proportionally. Oversampling and undersampling are techniques used to address imbalanced classes in the training data. Oversampling involves increasing the number of instances in the minority class, while undersampling involves reducing the number of instances in the majority class.

Without more information on the dataset used in this study, it is difficult to determine whether these techniques were used to address data bias during preprocessing. However, it is important to consider these techniques when working with imbalanced or biased datasets to ensure that the model is able to learn from a representative sample of the population.
Answer:::

The strategy implemented to monitor the model performance during training involves evaluating and comparing the performance scores of all possible combinations of hyperparameters for the best model selection. Specifically, for the model proposed by Sowmya et al. (2020), the hyperparameters considered include the number of trees, maximum features, tree depth, minimum number of samples required to split a node, minimum number of samples required at each leaf node, bootstrap sampling, and minimal cost-complexity pruning. The setup with 1000 trees, max features = auto, 50 levels, samples split = 2, samples leaf = 2, bootstrap = True, and cpp alpha = 0 was chosen for investigation since it achieved the best prediction accuracy.

Moreover, feature analysis was performed to test different combinations of input features among a set of 12 possibilities. Additionally, machine learning algorithms were employed to compensate for missing values in the dataset.

Regarding the hyperparameters selection for the MLP predictor, the grid search technique was applied to compute the optimum values of hyperparameters. Specifically, the MLP model was implemented with an input layer, three hidden layers, and an output layer. The performance obtained using different hyperparameters was compared, including solvers, the number of neurons in the hidden layers, regularization factor "alpha" (L2 penalty), activation function, and learning rate.

Therefore, the strategy for monitoring the model performance during training involves evaluating and comparing the performance of different hyperparameters combinations, including the number of trees, maximum features, tree depth, minimum number of samples required to split a node, minimum number of samples required at each leaf node, bootstrap sampling, and minimal cost-complexity pruning for the proposed model. Additionally, the performance of different hyperparameters combinations was evaluated for the MLP predictor, including solvers, the number of neurons in the hidden layers, regularization factor "alpha" (L2 penalty), activation function, and learning rate.
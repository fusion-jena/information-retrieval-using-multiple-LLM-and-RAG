Answer:::

The deep learning models in the study were trained using NVIDIA K80 Graphics Processing Units (GPUs). GPUs are specialized hardware designed to accelerate graphics rendering, but they have also proven to be effective for machine learning tasks due to their ability to perform many calculations simultaneously. In this case, the NVIDIA K80 GPUs were used to train deep convolutional neural networks (CNNs) for image classification in natural applications. The use of GPUs allowed for faster training times and more efficient computation compared to using only a central processing unit (CPU).

Annotated notebooks and Mathematica notebooks, which contain the code used to define and train the CNNs, are provided as supplementary materials. The CNNs were built in Mathematica version 11.1, and the training process involved partitioning the images into three sets: a training set (70% of the images), a validation set (20% of the images), and a test dataset (10% of the images). The images were resized to 256 × 256 pixels, creating a 3 × 256 × 256 tensor for the input layer.

The CNN architectures varied for the two datasets (stained/unstained and clubmoss/spikemoss). The stained/unstained CNN included four convolutional layers, four pooling layers, a flatten layer, a dropout layer, and two linear layers, while the clubmoss/spikemoss CNN consisted of three convolutional layers, three pooling layers, a flatten layer, a dropout layer, and two linear layers. The specific details of the layers, including the number of filters, kernel sizes, and activation functions, are provided in the supplementary materials.

In summary, NVIDIA K80 GPUs were used to train deep convolutional neural networks for image classification in natural applications. The use of GPUs allowed for efficient computation and faster training times during the model training process.
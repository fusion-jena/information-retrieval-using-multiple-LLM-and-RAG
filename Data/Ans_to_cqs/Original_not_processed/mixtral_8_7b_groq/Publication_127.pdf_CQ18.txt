Answer:::

The text provided does not give a specific strategy that was implemented to monitor the model performance during training. However, it does mention that they used different architectures (AlexNet, NiN, VGG, GoogLeNet, ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152) and an ensemble of models for image classification on a dataset of 1.4 million images. They also mentioned that they assigned the label of each capture event to all of the images in that event. The performance of each model was evaluated using a test set of 105,000 images. The VGG model achieved the best accuracy of 96.8%.

Additionally, the authors mentioned that they are publishing the final DNNs trained on the dataset so that others can use them as is or for transfer learning. They also published the software required to run their experiments as open-source code. This allows other researchers to replicate their findings and build upon their work.

It is also worth noting that the authors used various state-of-the-art techniques in the field of deep learning such as Sequence to sequence learning with neural networks (Sutskever et al., 2014), Learning phrase representations using RNN encoder-decoder for statistical machine translation (Cho et al., 2014), Deep residual learning for image recognition (He et al., 2016), Very deep convolutional networks for large-scale image recognition (Simonyan and Zisserman, 2014) and Deep Learning (Goodfellow et al., 2016)

Answer:::

In summary, while the text does not provide specific information on the strategy used to monitor the model performance during training, it does mention that they used different architectures and an ensemble of models, and evaluated the performance of each model using a test set. They also published the final DNNs trained on the dataset and the software required to run their experiments as open-source code. Additionally, they used various state-of-the-art techniques in the field of deep learning.
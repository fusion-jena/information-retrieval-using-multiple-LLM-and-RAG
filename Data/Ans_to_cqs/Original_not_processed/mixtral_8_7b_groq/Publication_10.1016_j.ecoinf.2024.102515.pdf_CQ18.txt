Answer:

Based on the provided context, there is no explicit information given about the strategy implemented to monitor the model performance during training. However, it can be inferred that the model's performance is evaluated by comparing the predicted categories of species observations from the trained model with the actual categories of species observations in a test dataset.

The study gathered tweets from the beginning of the series until the end of 2022, which amounted to approximately 11,000 tweets. These tweets were manually supervised and annotated by a taxonomist. The taxonomist was able to review dozens of entries per minute by scrolling through the spreadsheet, which embedded a thumbnail of each tweet. Only when there was a species of interest did the taxonomist stop the procedure to annotate the category.

After the retrospective study was completed, the algorithm could be run daily or integrated into an alert system, which would mean checking a bunch of tweets a day at best. This suggests that the model's performance could be monitored by evaluating its predictions on a daily basis or as new tweets are collected.

Moreover, the study mentions that using exclusively textual information from tweets to obtain a training dataset would only lead to the creation of a deficient model. This implies that other sources of information, such as the location of the species observation, are used to improve the model's performance. Therefore, it can be inferred that the strategy to monitor the model's performance during training includes evaluating its predictions on new data and ensuring that the model is trained on a diverse and informative dataset.

However, it is important to note that the study does not provide specific details about the evaluation metrics used to monitor the model's performance or the frequency of performance monitoring. Therefore, further information would be needed to fully answer the query.
The provided context discusses research on the urban acoustic environment (AE) and various methods used to quantify it, but it does not directly mention the regularization methods used to prevent overfitting in deep learning pipelines such as dropout or L2 regularization.

To answer the query, I will provide some background information on regularization methods used in deep learning:

Overfitting is a common problem in machine learning where a model learns the training data too well, including its noise and outliers, and performs poorly on unseen data. Regularization techniques are used to prevent overfitting by adding a penalty term to the loss function, which discourages the model from learning overly complex patterns in the training data.

Two commonly used regularization methods in deep learning are L2 regularization and dropout:

1. L2 regularization, also known as weight decay, adds a penalty term proportional to the square of the model's weights to the loss function. This penalty term discourages the model from assigning too much importance to any single feature, leading to a simpler and more generalizable model.
2. Dropout is a regularization technique that randomly sets a fraction of the model's neurons to zero during training. This prevents the model from relying too heavily on any single neuron, leading to a more robust and generalizable model.

These regularization techniques can be applied to various layers of a deep learning pipeline, including the dense layers, convolutional layers, and recurrent layers.

In summary, while the provided context does not directly mention the regularization methods used to prevent overfitting in deep learning pipelines, L2 regularization and dropout are two commonly used techniques that can help prevent overfitting and improve the generalizability of deep learning models.
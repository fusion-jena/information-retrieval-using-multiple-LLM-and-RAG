The text provided does not give a detailed description of the process followed to deploy the trained deep learning model. However, it does provide some context that can help us infer a general idea of the steps that might have been taken.

The paper "Deep Learning for Large Scale Biodiversity Monitoring" by Klein, McKown, and Tershy discusses the use of deep learning for analyzing wildlife data streams. They mention that they use big data infrastructure and machine learning techniques like deep learning to train computational models to detect and classify events of interest. This suggests that they likely followed a standard process for developing and deploying their deep learning model.

One possible step in this process is model serialization, which involves saving the trained model in a format that can be easily loaded and used for making predictions. This is often necessary when deploying a model in a production environment, as it allows the model to be easily transferred between different systems and platforms.

Another possible step is platform selection, which involves choosing the appropriate hardware and software environment for deploying the model. This may include selecting a cloud-based platform or a local server, and configuring the necessary software and dependencies for running the model.

The text does not provide specific details about these steps, so it is not possible to give a definitive answer to the query. However, it is likely that the authors followed a standard process for developing and deploying their deep learning model, which would include steps such as model serialization and platform selection.
The experimental results suggest that the generalization capacity of the deep learning classifiers was improved by better exploiting the training data. This was achieved through additional data augmentation techniques and the use of recent advances in unsupervised learning techniques, such as self-supervised methods. The classifiers' complexity was also reduced by simplifying them in terms of the number of parameters, thereby minimizing the risk of overfitting.

Another direction for continuing this research was to exploit the classification uncertainty further in an active learning context. The uncertainty measure was used in interactively training the deep learning models, selecting high-uncertainty samples in the datasets, and increasing their importance in the computation of the loss function.

In the experiments, the generalization capacity of the deep learning models was assessed using cross-site evaluations. Each row in the figure represents a different site combination for training and testing the CDC (Classification Decision Committee), which is an ensemble of networks with different architectures. The individual predictions are fused through a majority voting mechanism. The models were evaluated on single and multi-label classification problems, and three different sites were considered in the experiments.

The VGG (Visual Geometry Group) network, which is one of the most popular image classification architectures, was evaluated in the experiments. Pre-trained VGG models are commonly used in transfer learning schemes, such as fine-tuning. The VGG network was proposed in Simonyan and Zisserman (2015) and outperformed all previous participants in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2015.

However, the context does not explicitly mention any measures such as diverse dataset, cross-validation, or stratified splitting to ensure the generalizability of the deep learning model.
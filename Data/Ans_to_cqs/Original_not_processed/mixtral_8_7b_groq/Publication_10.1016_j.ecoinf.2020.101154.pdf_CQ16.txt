Answer:::

The pieces of context provided discuss various aspects of model training and hyperparameter tuning for different datasets and models, but they do not directly mention the specific criteria used to determine when training is complete. However, they do imply the use of two techniques that are commonly used to prevent overfitting and determine when training should be stopped: early stopping and validation loss plateau.

Early stopping is a technique where the model training is stopped as soon as the performance on a validation set starts to degrade, indicating that the model is starting to overfit to the training data. This technique was explicitly mentioned in the context for the RI models, where the number of iterations required until convergence was 120 for CS and 106 for NW.

Validation loss plateau is a technique where the model training is stopped when the loss on a validation set stops decreasing or plateaus, indicating that the model has learned the underlying patterns in the training data and further training is unlikely to improve the model's performance. Although this technique was not explicitly mentioned in the context, it is a common practice in machine learning and is often used in conjunction with early stopping.

In addition, the context also mentions the use of a three-part split, where the model is trained and validated on different subsets of the data, and the validation score is used to tune the hyperparameters. This approach allows for a more robust test of model performance and helps prevent overfitting by ensuring that the model is tested on previously unseen combinations of explanatory variables.

In summary, while the pieces of context do not provide a direct answer to the query, they do imply the use of early stopping and validation loss plateau as techniques to determine when training is complete and to prevent overfitting. These techniques are commonly used in machine learning and are crucial for building accurate and robust models.
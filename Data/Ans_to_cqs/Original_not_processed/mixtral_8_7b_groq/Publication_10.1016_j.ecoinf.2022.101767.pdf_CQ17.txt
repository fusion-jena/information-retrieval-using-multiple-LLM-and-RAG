The provided context does not contain information about regularization methods used in deep learning pipelines, such as dropout or L2 regularization. The context consists of studies and statistics related to ecosystem services, video length on YouTube and Facebook, bird statistics in Japan, and a method for quantifying the effects of wild bird species in YouTube videos. Therefore, I cannot provide an answer to the query based on the given context.

Answer::: 
(Answer to the query)

Regularization methods are essential in deep learning pipelines to prevent overfitting. Overfitting occurs when a model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization techniques introduce a penalty to the loss function, which discourages complex models and promotes generalization.

Two common regularization methods used in deep learning are dropout and L2 regularization:

1. Dropout: Dropout is a regularization technique that randomly sets a fraction of neurons in a layer to zero during training. This prevents over-reliance on any single neuron and encourages the network to learn more robust features. The dropout rate, which determines the fraction of neurons to drop, is a hyperparameter that can be tuned for optimal performance.

2. L2 Regularization: L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the norm of the model's weights. This encourages smaller weights, which reduces the complexity of the model and helps prevent overfitting. The strength of the L2 regularization, denoted by the lambda parameter, is another hyperparameter that can be adjusted for better generalization.

These regularization techniques can be applied together or separately, depending on the specific deep learning pipeline and the complexity of the problem.
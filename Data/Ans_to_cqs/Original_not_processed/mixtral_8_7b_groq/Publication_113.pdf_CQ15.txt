Answer:::

The provided context discusses the use of deep learning for dimension-reduction and active learning, but it does not explicitly mention the optimization techniques used during the training of the deep neural network. However, it does mention the use of Stochastic Gradient Descent (SGD) in the context of active learning.

In the given Algorithm 1, SGD is used as the optimization algorithm in the deep learning pipeline. Specifically, the underlying model is trained using SGD with the available labelled samples in each iteration. SGD is a popular optimization algorithm for training deep neural networks due to its simplicity and efficiency. It updates the parameters of the model by taking a step in the direction of the negative gradient of the loss function, with respect to the current parameters, for a single training example or a mini-batch of training examples.

Additionally, the context mentions the use of modern enhancements of SGD, such as Adam (Kingma & Ba, 2014) and Adadelta (Tieleman & Hinton, 2012). These optimization algorithms are based on SGD but incorporate additional features such as adaptive learning rates and momentum, which can improve the convergence and generalization performance of the deep learning model.

Therefore, we can infer that optimization techniques such as SGD, Adam, and Adadelta are applied in the deep learning pipeline.
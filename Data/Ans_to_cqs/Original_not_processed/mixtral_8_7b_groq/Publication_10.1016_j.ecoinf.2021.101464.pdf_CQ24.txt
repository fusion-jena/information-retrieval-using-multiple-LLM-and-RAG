The provided context does not contain information about strategies employed to handle randomness in the deep learning pipeline. The text mainly discusses the Random Forest (RF) model, which is a machine learning technique but not deep learning. In the RF model, randomness is handled through the use of random permutation variables, bootstrap sampling, and random selection of features for splitting at each node. However, these strategies are specific to the RF model and not necessarily applicable to deep learning pipelines.

In deep learning, randomness can be handled through various strategies such as:

1. Initialization of weights: Deep learning models have a large number of parameters, and initializing these parameters randomly can help the model converge to a better minimum.
2. Regularization: Techniques such as dropout, weight decay, and data augmentation can help reduce overfitting by introducing randomness during training.
3. Batch normalization: This technique normalizes the inputs of each layer, reducing the internal covariate shift and making the network more stable during training.
4. Learning rate scheduling: Adaptively changing the learning rate during training can help the model escape local minima and improve generalization.
5. Model ensemble: Combining multiple models trained with different random seeds or initialization can improve the performance and robustness of the final model.

In summary, while the provided context does not contain information about strategies employed to handle randomness in deep learning pipelines, there are several strategies that can be used, such as initialization of weights, regularization, batch normalization, learning rate scheduling, and model ensemble.
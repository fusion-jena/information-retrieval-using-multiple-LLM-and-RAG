The provided context does not include information about the specific regularization methods used in the deep learning pipeline to prevent overfitting. Therefore, I cannot provide a definitive answer based on the given context.

However, I can give some general information about common regularization methods used in deep learning.

Regularization is a technique used to prevent overfitting in machine learning models, including deep learning models. Overfitting occurs when a model learns the training data too well, including its noise and outliers, and performs poorly on unseen data. Regularization adds a penalty to the loss function to reduce the complexity of the model and improve its generalization ability.

Two common regularization methods used in deep learning are L2 regularization and dropout.

L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the weights. This encourages the weights to be small and distributed evenly, reducing the capacity of the model and preventing overfitting.

Dropout is a regularization technique that randomly sets a fraction of the output units of a layer to zero during training. This prevents the co-adaptation of the units and introduces noise into the model, making it more robust and less prone to overfitting.

Other regularization methods used in deep learning include L1 regularization, early stopping, data augmentation, and batch normalization.

In summary, the provided context does not specify the regularization methods used in the deep learning pipeline for the high-resolution CHM of the Earth. However, L2 regularization and dropout are two common regularization methods used in deep learning to prevent overfitting.
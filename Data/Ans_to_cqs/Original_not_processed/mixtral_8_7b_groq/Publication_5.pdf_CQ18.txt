Answer:::

The strategy implemented to monitor the model performance during training involves using a loss function that needs to reach its minimum value. The training process was performed for a number of iterations, with all the training data exposed to the network until the loss function reached its minimum. The model score reached its maximum after approximately 5000 iterations with a NVIDIA Titan X GPU. The number of trainable parameters was 85,569 and the computational run-time was approximately 4 hours with a training batch size of 1024.

Additionally, the performance of the proposed deep learning model was compared with a random forest (RF) regression model as a baseline. A grid search was used to determine the best values of the hyperparameters (number of trees, maximum tree depths, and the maximum number of features) in the RF model.

Furthermore, a statistical feature importance test (SFIT) was performed to explain which feature had the greatest significance in the species richness retrievals and to determine the optimized features in an operational retrieval system. The feature importance shows the extent to which the model performance decreased with random shuffling, using the root mean square error (RMSE) as the performance metric.

Therefore, the strategy to monitor the model performance during training includes using a loss function, comparing the model with a baseline, and performing a statistical feature importance test.
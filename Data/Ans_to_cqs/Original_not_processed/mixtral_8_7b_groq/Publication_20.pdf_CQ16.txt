Answer:::

The criteria used to determine when training is complete is not explicitly stated in the provided context. However, it is mentioned that training was stopped after 70 epochs to prevent overfitting. Overfitting occurs when a model learns the training data too well, to the point where it performs poorly on unseen data. By stopping the training process early, the model is less likely to overfit to the training data. Additionally, the use of a learning rate decay and dropout further helps to prevent overfitting. The learning rate decay causes the model to gradually reduce the size of its weight updates as training progresses, while dropout randomly sets a fraction of the model's inputs to zero during training, which helps to prevent the model from becoming too reliant on any single input.

It is also mentioned that the training lasted for 8 days, during which the model was trained on 900,000 images. However, it is not specified if the training was stopped based on the number of images seen or if it was based on the number of epochs.

In summary, while the exact criteria used to determine when training is complete is not specified in the provided context, it is mentioned that training was stopped after 70 epochs to prevent overfitting and it is also mentioned that the training lasted for 8 days and the model was trained on 900,000 images.
Answer:::

The data for deep learning model training was split using a process called repeated random sub-sampling, which is also known as k-fold cross-validation. In this process, the total dataset is divided into 'k' subsets or folds. In this particular case, five such folds were created. For each of these folds, a model was trained using 80% of the data (9600 frames) and tested on the remaining 20% (2400 frames). This process was repeated for all five folds, and the performance of each model was averaged to get the final training and testing accuracy.

The data was divided into training and testing sets in a ratio of 80:20. This means that 80% of the data was used for training the model, and the remaining 20% was used for testing the model's performance. This ratio is commonly used in machine learning as it provides a good balance between training the model sufficiently and testing its performance on unseen data.

The data was divided into subsets while ensuring that the distribution of environmental conditions was maintained in each subset. This was important because the model needed to learn to classify the different environmental conditions accurately. The potential influence of fish being in the extracted frames was considered negligible, and hence, the presence or absence of fish did not affect the data splitting process.

The data was split in this manner for five different models, each with the same architecture. These models were trained and tested over 100 epochs, and the average loss value and accuracy during training and testing were calculated. The individual model's performance is provided in the supplementary material.

In summary, the data was split for deep learning model training using repeated random sub-sampling, maintaining the distribution of environmental conditions in each subset, and dividing the data into training and testing sets in a ratio of 80:20. This process was repeated for five different models, and the final training and testing accuracy were calculated as the average of all five models' performance.
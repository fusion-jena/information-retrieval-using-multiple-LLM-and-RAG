Answer:::
The regularization methods used to prevent overfitting in the deep learning pipeline, as described in the provided context, include dropout and the early stopping method.

Dropout is explicitly mentioned as a technique used to reduce overfitting and to imitate the training of a set of different models. It is implemented in the top network of the ResNet50 model, which is reconfigured with two fully connected layers. Dropout works by randomly dropping out, or "turning off," a proportion of the neurons in a layer during training, which helps to prevent over-reliance on any single neuron and encourages the model to learn more robust features.

The early stopping method is also used to avoid overfitting. This method involves monitoring the loss of the validation subset during training, and stopping the training process early if the validation loss starts to increase. In the provided context, the early stopping method is used with a patience equal to 5, which means that the training process will stop if the validation loss does not improve for 5 epochs. This helps to ensure that the model does not continue to train and overfit to the training data once it has already learned the underlying patterns.

It is worth noting that the context does not explicitly mention L2 regularization as a regularization method used in the deep learning pipeline. L2 regularization, also known as weight decay, is a technique that involves adding a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This helps to encourage the model to learn smaller, more conservative weights and can help to prevent overfitting. However, in the provided context, the focus is on dropout and the early stopping method as the primary regularization techniques used.
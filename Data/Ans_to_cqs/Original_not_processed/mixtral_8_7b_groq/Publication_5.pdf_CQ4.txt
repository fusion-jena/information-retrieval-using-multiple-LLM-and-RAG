Based on the provided context, there is no explicit mention of data augmentation techniques such as flipping, rotating, or scaling being applied in the deep learning (DL) pipeline. Data augmentation is a technique used to increase the amount of training data by applying various transformations to the existing data, which can help improve the model's performance and prevent overfitting. Common data augmentation techniques include flipping, rotating, scaling, and translating images or other types of data.

In the given context, the DL model was trained using a dataset with specific characteristics, and the authors mentioned that general image-based CNN architectures may not be appropriate for this data. Instead, they used a one-dimensional CNN or recurrent neural networks in the temporal domain and graph convolutional networks for handling graph structures between samples. These approaches are more advanced techniques that can help improve the model's performance without requiring data augmentation.

Furthermore, the authors compared the DL model's performance with a random forest (RF) regression model, and to determine the best values of the hyperparameters in the RF model, a grid search was used. However, there is no mention of data augmentation being applied during the training of the RF model.

Therefore, based on the provided context, it appears that data augmentation techniques such as flipping, rotating, and scaling were not applied in the DL pipeline. However, it is important to note that data augmentation is not always necessary, and other advanced techniques can be used to improve the model's performance.
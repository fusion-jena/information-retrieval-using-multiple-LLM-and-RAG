The provided context does not include information about the optimization techniques applied in a deep learning pipeline. Therefore, I cannot provide a specific answer to the query.

However, I can provide some general information about optimization techniques that are commonly used in deep learning.

Optimization techniques are used to minimize the loss function of a deep learning model during training. Some of the most commonly used optimization techniques include:

1. Stochastic Gradient Descent (SGD): This is a simple optimization algorithm that updates the model parameters in the direction of the negative gradient of the loss function.
2. Momentum: This is an optimization technique that accumulates the past gradients and uses them to update the model parameters. This helps to overcome the problem of oscillations that can occur during training.
3. Adagrad: This is an optimization algorithm that adapts the learning rate for each parameter based on the historical gradient information.
4. Adadelta: This is an optimization algorithm that adapts the learning rate based on the historical gradient information, similar to Adagrad. However, it uses a running average of the historical gradient information instead of the sum of the historical gradients.
5. Adam: This is an optimization algorithm that combines the ideas from Momentum and Adagrad. It uses a running average of the historical gradients and squared gradients to adapt the learning rate for each parameter.

These optimization techniques can be used in various combinations and configurations in a deep learning pipeline. The choice of optimization technique depends on the specific problem and the characteristics of the data.

Therefore, while the provided context does not include information about the optimization techniques used in the deep learning pipeline, it is important to carefully consider the choice of optimization technique when developing a deep learning model.
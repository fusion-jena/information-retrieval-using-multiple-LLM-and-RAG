The provided context does not mention the use of any type of deep learning models such as Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Transformers in the described pipeline. Instead, it focuses on Gradient Boosting Regression Trees (GBRT) and Generalized Additive Models (GAM) for the analysis.

The GBRT models were used for the initial analysis, and the top four most influential variables were selected based on their variable importance (VI) scores to be included in GAM models. Hyperparameter tuning for the GBRT models was done manually, starting with a low learning rate and a high number of iterations, and then adjusting them as needed.

For the RI models, the same initial parameters were set for both CS and NW datasets, and a grid search was performed to find the hyperparameters with the highest accuracy. The learning rate was set to 0.05, and the maximum tree depth was set to 4 for both the CS and NW models. Early stopping was used to determine when to stop the model training to avoid overfitting.

For the RY models, the best performing models were those with a slow learning rate of 0.005, and the NW model had a very shallow tree with a tree depth of 2. The number of iterations was kept at 500.

In summary, the pipeline described in the provided context uses Gradient Boosting Regression Trees (GBRT) and Generalized Additive Models (GAM) for the analysis, and there is no mention of any deep learning models such as CNN, RNN, or Transformers.
Based on the provided context, the criteria used to split the data for model training do not specifically mention deep learning models or the use of a test set. Instead, the data was split into a training set and an evaluation set, with 80% of the data used for training and 20% for evaluation.

The training set was used to create different models, and the evaluation set was utilized to assess the performance of these models. Two metrics were used to evaluate model accuracy: the area under the curve (AUC) of the receiver operating characteristics (ROC) and the true skill statistics (TSS).

To ensure robustness, committee averaging and weighted mean approaches were applied to create separate ensemble models for each climate scenario and period. The final ensemble models were selected based on a TSS score of â‰¥0.7.

It is important to note that while the text does not explicitly mention deep learning models, the mentioned machine learning algorithms, such as XGBOOST, GBM, RF, and MaxEnt, can be used in deep learning frameworks. In such cases, the data splitting criteria would be similar, with a portion of the data used for training and the rest for validation or testing, depending on the deep learning framework's specific requirements.
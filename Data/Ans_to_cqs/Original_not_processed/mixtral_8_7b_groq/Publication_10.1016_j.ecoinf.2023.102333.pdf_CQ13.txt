Answer:::

The hyperparameters used in the deep learning model include the number of layers and nodes, learning rate, and dropout rate. These were adjusted to optimize the network architecture, optimization procedure, and regularization. The model's accuracy, computation time and costs, and the impact of each hyperparameter on the model's performance were considered during the optimization process.

A dropout layer was added to the model to prevent overfitting and reduce statistical noise. The dropout regularization randomly removed different sets of neurons using a predetermined dropout rate, resulting in training different neural networks and reducing overfitting.

The learning rate, which controls how much the weights in the model are adjusted with each training iteration, was also included in the hyperparameters. The optimizer, which determines how the weights are updated during training, was not explicitly mentioned in the provided context. However, it can be inferred that an optimizer was used in conjunction with the learning rate to update the model's weights.

In summary, the hyperparameters used in the deep learning model include the number of layers and nodes, learning rate, and dropout rate. These were adjusted during the optimization process to obtain high-performing models while leading to more efficient utilization of computational resources.
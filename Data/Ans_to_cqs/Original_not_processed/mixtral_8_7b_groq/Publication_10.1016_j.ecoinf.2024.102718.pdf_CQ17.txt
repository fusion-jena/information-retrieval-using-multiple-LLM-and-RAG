Based on the provided context, there is no explicit mention of regularization methods such as dropout or L2 regularization that are used to prevent overfitting in the deep learning pipeline. However, the context does mention the use of callbacks like ReduceLROnPlateau, ModelCheckpoint, and EarlyStopping.

ReduceLROnPlateau can be used as a form of regularization to prevent overfitting by reducing the learning rate when a metric has stopped improving. This can help the model to avoid getting stuck in a poor local minimum and enable it to continue learning.

EarlyStopping is another form of regularization that can be used to prevent overfitting by stopping the training process before the model starts to overfit. This is done by monitoring the model's performance on a validation set and stopping the training when the performance on the validation set stops improving.

ModelCheckpoint can also be used as a form of regularization by saving the best model during the training process and reloading it for use. This can help to prevent overfitting by ensuring that the final model is based on the best performing weights rather than the weights at the end of the training process.

While these callbacks are not traditional regularization methods like dropout or L2 regularization, they can still be effective in preventing overfitting in the deep learning pipeline.
The provided context discusses a deep learning model used for species identification, which includes a single parameter for calibrating the model across all classes. However, it does not explicitly mention the specific hyperparameters used in the model, such as the learning rate or optimizer.

In general, hyperparameters are configuration variables that govern the training process of a machine learning model. Some common hyperparameters include:

1. Learning rate: The step size at which the model updates its weights during training. A learning rate that is too high may cause the model to converge too quickly to a suboptimal solution, while a learning rate that is too low may cause the training process to be overly slow.
2. Optimizer: The algorithm used to update the model's weights during training. Common optimizers include stochastic gradient descent (SGD), Adam, and RMSprop.
3. Number of hidden layers and number of neurons per layer: These parameters determine the complexity of the model and its capacity to learn.
4. Regularization parameters: These parameters control overfitting by adding a penalty term to the loss function. Common regularization techniques include L1 and L2 regularization.
5. Batch size: The number of samples used in each iteration of the training process.

The specific hyperparameters used in the deep learning model described in the context are not specified. However, the model was built using a convolutional neural network (CNN), which is a type of deep learning architecture commonly used for image classification tasks. CNNs typically have several hyperparameters that need to be tuned, such as the number of convolutional layers, the number of filters in each layer, and the size of the filters.

In addition, the model was trained using a second and independent database (T1) to tune a risk threshold specific to each class. This suggests that the model includes a hyperparameter for the risk threshold, which is used to convert the vector output from the neural network into a real probability.

Overall, while the specific hyperparameters used in the deep learning model described in the context are not provided, it is likely that the model includes several hyperparameters that were tuned during the training process, such as the learning rate, optimizer, and number of hidden layers.
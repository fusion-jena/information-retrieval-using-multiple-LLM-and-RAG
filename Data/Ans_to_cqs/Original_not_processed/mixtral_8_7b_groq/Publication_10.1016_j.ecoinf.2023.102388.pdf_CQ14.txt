Based on the provided context, there is no explicit information about how the hyperparameters of the model are optimized, such as through methods like grid search or random search. However, the text does provide specific details about the hyperparameters used in both the "no-overfitting" and "overfitting" settings.

In the "no-overfitting" setting, the optimization algorithm for Convolutional Neural Networks (CNNs) is Stochastic Gradient Descent (SGD), with a learning rate of 0.0001 and a weight decay of 0.5. The batch size is set to 32, the number of training epochs to 200, and batch normalization and dropout (with a rate of 0.5) are used to reduce overfitting.

In the "overfitting" setting, the same hyperparameters are used, except that batch normalization, weight decay, and dropout techniques are removed to ensure that the model overfits.

Therefore, while the text does not explicitly describe the method used to optimize these hyperparameters, it does provide a specific set of hyperparameters that are used in both the "no-overfitting" and "overfitting" settings. It is possible that these hyperparameters were determined through some hyperparameter optimization method, but the text does not provide information on this process.
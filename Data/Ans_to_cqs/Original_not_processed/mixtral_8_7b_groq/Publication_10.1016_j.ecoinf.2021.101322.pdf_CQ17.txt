Answer:::

Based on the provided context, there is no explicit mention of the regularization methods used to prevent overfitting in the deep learning pipeline. However, there are some implicit practices that can be inferred.

Firstly, the use of transfer learning from a pre-trained network (EfficientNet) can be seen as a form of regularization. The pre-trained network has already been trained on a large and diverse dataset (ImageNet), which can help in providing a good initial point in the parameter space and reducing overfitting on the smaller salmon scale dataset.

Secondly, during the training of CNN models, minibatch gradient descent was used. This method can also act as a form of regularization by introducing some stochasticity in the parameter updates.

Thirdly, early stopping was used in the training process. The training was set to terminate if there was no improvement in validation error for a certain number of epochs. This can help in preventing overfitting by stopping the training before the model starts to learn the noise in the training data.

However, there is no explicit mention of other regularization methods such as dropout, L1/L2 regularization, data augmentation, etc. Therefore, it can be inferred that these methods were not used in this specific deep learning pipeline.

Answer:::
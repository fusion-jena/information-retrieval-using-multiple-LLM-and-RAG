Based on the information provided, it is not explicitly stated which measures were taken to ensure the generalizability of the predictive model. However, there are some clues that suggest that cross-validation might have been used.

The study by Singh et al. (n.d.) reports the performance of their predictive model using evaluation metrics such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and R-squared (R2). These metrics are commonly used to assess the accuracy and generalizability of predictive models. The fact that these metrics are reported suggests that some form of validation was used to estimate their values.

One possible validation method that could have been used is cross-validation. Cross-validation is a technique where the dataset is split into multiple subsets, and the model is trained and tested on different combinations of these subsets. This helps to ensure that the model is not overfitting to the training data and is able to generalize well to new, unseen data.

Another validation method that could have been used is stratified splitting. Stratified splitting is a technique where the dataset is split in such a way that each subset contains roughly the same proportion of each class or category. This can be useful when the dataset is imbalanced, meaning that some classes or categories are more common than others.

However, without explicit mention of these techniques in the text, it is difficult to say for certain whether they were used. Therefore, I cannot definitively answer the query without additional information.
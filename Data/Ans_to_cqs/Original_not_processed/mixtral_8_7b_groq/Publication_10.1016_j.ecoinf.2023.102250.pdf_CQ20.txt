The context provided does not give specific information about the hardware resources used for training the deep learning model. However, it is mentioned that increasing the network depth may lead to significant computational resources requirements. This implies that high-end computational hardware, such as GPUs or TPUs, are likely used for training the deep learning model. These types of hardware are commonly used for training large and complex models due to their ability to perform parallel processing, which significantly speeds up the training process compared to using a CPU.

In addition, the context mentions the use of various deep learning models such as MobileNetV2, ResNet18, and VGG16, which are known to have high computational requirements. Training these models on large datasets would require significant computational resources, making it likely that GPUs or TPUs were used.

Furthermore, the context mentions the use of data augmentation techniques such as ACGAN, which can increase the size of the training dataset, further increasing the computational requirements.

Therefore, while the context does not explicitly state the hardware resources used for training the deep learning model, it is strongly implied that high-end computational hardware such as GPUs or TPUs were used due to the high computational requirements of the models and techniques mentioned.
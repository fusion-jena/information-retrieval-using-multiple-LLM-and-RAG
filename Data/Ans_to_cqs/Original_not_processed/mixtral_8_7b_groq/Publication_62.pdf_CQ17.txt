Answer:::
The provided context discusses the use of deep convolutional neural networks (CNNs) for image classification in the context of natural history collections. However, it does not explicitly mention the specific regularization methods used to prevent overfitting in the deep learning pipeline.

However, based on the common practices in deep learning, we can infer that the following regularization methods were likely used in the CNNs described in the supplementary materials:

1. Dropout: Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps to prevent overfitting by reducing co-adaptation of feature detectors. Dropout layers are visible in Tables 1 and 2 in the context, suggesting that dropout was used in the unstained/stained CNN.

2. L2 regularization: L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the weights. This encourages smaller weights, reducing overfitting. Although not explicitly mentioned, L2 regularization is a common practice and might have been used in conjunction with dropout.

3. Data augmentation: Data augmentation is a technique where artificial training examples are generated by applying transformations (e.g., rotation, scaling, flipping) to the original images. Data augmentation can help improve the generalization of CNNs and reduce overfitting. While not explicitly stated, data augmentation is a common practice in image classification tasks and might have been employed during the training of the CNNs.

In summary, based on the common practices in deep learning and the information provided in the context, it is likely that dropout, L2 regularization, and data augmentation were used to prevent overfitting in the deep learning pipeline. However, the context does not explicitly confirm the use of these regularization methods.
Based on the information provided, the regularization methods used to prevent overfitting in the deep learning pipeline are dropout and decay. 

In the deep learning models used, hidden_dropout and visible_dropout are used as regularization techniques. Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent overfitting by making the network more robust and less dependent on the training data. Hidden_dropout refers to dropout applied to the hidden layers of the neural network, while visible_dropout refers to dropout applied to the input layer.

Additionally, decay is used as a regularization technique in the deep neural network (DNN) model. Decay is a form of L2 regularization, which adds a penalty term to the loss function that is proportional to the square of the magnitude of the model weights. This encourages the model to have smaller weights, which can help to prevent overfitting.

In the provided table, the decay parameter is set to 0.1 for the DNN model, indicating that L2 regularization is being used. However, it is not explicitly mentioned if decay is used in the other deep learning models (model 12 and model 19 with DeepConvLSTM architecture, and model 14 with CNN architecture).

Overall, the regularization methods used in the deep learning pipeline include dropout and decay (L2 regularization). Dropout is used to prevent overfitting by making the network more robust and less dependent on the training data, while decay is used to encourage the model to have smaller weights, which can help to prevent overfitting.
Answer:::
The hyperparameters used in the deep learning model include the learning rate, optimizer, batch size, and exponential decay rates of first and second-moment estimations. Specifically, the learning rate is set to 0.0001, and the Adam optimizer is used to fine-tune the model. The mini-batch gradient descent algorithm is used with a batch size of 32. The exponential decay rate of first-moment estimation is set to 0.9, and the exponential decay rate of second-moment estimation is set to 0.999. Additionally, a positive scalar value for epsilon is set to 1e-08. To avoid overfitting, fivefold cross-validation and early stopping are employed for efficient hyperparameter optimization. The early stopping criterion is based on the model performance when the model stops improving on precision, recall, and F1-score.

The Inception-ResNet-v2 model is trained on a dataset of endemic bird species, which is randomly split into 3,132 images for training and 760 images for testing. The training dataset is augmented 10 times, resulting in a total of 31,320 images. Then, 80% of the images are used for training, and 20% are used for validation.

It is worth noting that the choice of hyperparameters can significantly affect the performance of the deep learning model. The learning rate controls how much the weights are updated during training, and a learning rate that is too high or too low can lead to poor convergence or overshooting the optimal solution. The optimizer is used to update the weights based on the gradient of the loss function, and different optimizers have different convergence properties and computational requirements. The batch size determines the number of samples used to calculate the gradient at each iteration, and a larger batch size can lead to more stable gradients but may require more memory. The exponential decay rates of first and second-moment estimations control the moving averages of the gradient and squared gradient, respectively, and can affect the stability and convergence of the training process.

Therefore, selecting appropriate hyperparameters is crucial for achieving good performance in deep learning models. In this study, the authors have carefully chosen the hyperparameters based on their experience and the properties of the dataset. However, it is important to note that the optimal hyperparameters may vary for different datasets and tasks, and hyperparameter tuning is often an iterative process that requires careful experimentation and evaluation.
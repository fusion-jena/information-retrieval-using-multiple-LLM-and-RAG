The provided pieces of context do not directly discuss the regularization methods used to prevent overfitting in deep learning pipelines. However, I can provide a general explanation of such methods using information from related studies.

Regularization methods are essential in deep learning to prevent overfitting, which occurs when a model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization techniques discourage overly complex models by adding a penalty term to the loss function.

Two common regularization methods are:

1. L2 regularization: This method adds the squared magnitude of the coefficients to the loss function, which encourages smaller coefficient values. In other words, it penalizes large weights, preventing the model from fitting the noise in the training data.

2. Dropout: This technique randomly drops out, or deactivates, a proportion of neurons during training. By doing so, the model cannot rely on any single neuron, and the training process encourages the development of redundant representations across the network. As a result, the model becomes more robust and less prone to overfitting.

These regularization methods can be applied to deep learning pipelines, including those involving convolutional neural networks (CNNs), as mentioned in the context. Urech et al. (2020) and Puschmann (2019) used CNNs for landscape analysis, but the context does not explicitly mention the application of regularization techniques. Nonetheless, it is reasonable to assume that these methods were employed to ensure the generalizability and robustness of the models.
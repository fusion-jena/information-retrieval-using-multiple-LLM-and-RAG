Based on the provided context, there is no explicit description of a strategy implemented to monitor the model performance during training. However, there are some details about the training process and validation results that can give us some insights into how the model performance might have been informally monitored.

During training, the number of units in the third fully connected layer (fc3) was changed according to the number of classes of training data, and the training was stopped after 6900 iterations with a factor of 10 every 69 iterations. The batch size was set to 69, and the model used L2 weight decay with a penalty multiplier of 5 Ã— 10-4, dropout ratio of 0.5, and CPU mode. These settings suggest that the training process was systematically controlled, and the model's performance might have been observed after each factor increase or at the end of each epoch.

Moreover, the experiments were performed on a small-size dataset without data augmentation, and the model results demonstrate that it is feasible to use small datasets to train deep learning models as long as the parameters are chosen properly. The MG and MO models can both get good training results and perform well in their own validation set, proving that deep learning is a powerful method. These validation set results suggest that the model performance was formally monitored and evaluated using a separate set of data not used in the training process.

Therefore, while there is no explicit description of a strategy implemented to monitor the model performance during training, the training process settings and validation results suggest that the model's performance was systematically observed and evaluated during and after the training process.
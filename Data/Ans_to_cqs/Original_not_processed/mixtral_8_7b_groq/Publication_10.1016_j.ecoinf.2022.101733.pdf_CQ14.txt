Answer:::

Based on the provided context, there is no explicit information about how the hyperparameters of the models used in the studies are optimized. However, the studies do mention using Random Forest and Decision Tree algorithms, Support Vector Machine (SVM), and neural networks. These models have various hyperparameters that can be optimized using techniques such as grid search, random search, or Bayesian optimization.

For instance, in the study by Albayrak et al. (2021), Random Forest and Decision Tree algorithms were used. In Random Forest, hyperparameters like the number of trees, maximum depth, and minimum samples per leaf can be optimized. Similarly, in Decision Trees, the maximum depth, minimum samples per split, and minimum samples per leaf can be tuned. Grid search or random search can be employed to find the optimal hyperparameters for these models.

In the study by Gola et al. (2019), an SVM model was used for microstructure classification. SVM has hyperparameters such as the regularization parameter (C) and the kernel coefficient (Î³) that can be optimized using grid search or random search.

Lastly, in the study by Royimani et al. (2022), a neural network model was applied for feature selection and optimizing SVM parameters. Hyperparameters in neural networks include the number of layers, number of neurons, learning rate, and activation functions. Grid search, random search, or Bayesian optimization can be used for hyperparameter tuning in neural networks.

In summary, although the provided context does not explicitly mention the hyperparameter optimization techniques used, it can be inferred that grid search, random search, or other optimization techniques were likely employed for the Random Forest, Decision Tree, SVM, and neural network models used in the studies.
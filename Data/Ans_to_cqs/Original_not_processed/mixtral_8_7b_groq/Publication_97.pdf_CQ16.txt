Answer:::

The criteria used to determine when training is complete in the context provided is not explicitly stated. However, it can be inferred that the training process is considered complete when the average values of the loss function are less than 0.1 after 1500 epochs. This is based on the information that the authors tried to change the default learning rate parameter for the Adam algorithm, and its decrease did not lead to a significant reduction of loss function fluctuations but increased the number of epochs required for network training. 

The authors do mention that high-intensity fluctuations of the loss function are caused by the specificity of the algorithm used at the training stage, which suggests that the training process is not based on a traditional criteria such as a validation loss plateau. Instead, it appears that the training is completed after a predetermined number of epochs, regardless of the loss function behavior.

It's worth noting that the authors faced ambiguities in the objective function being minimized due to the fact that they generated batches of training images randomly, on the fly, from source images and never showed exactly the same subset of images to the network. This could have made it difficult to use a validation loss plateau as a stopping criterion, as the validation loss might not have been a reliable indicator of the model's ability to generalize to new data.

In summary, the criteria used to determine when training is complete in the provided context is not explicitly stated, but it can be inferred that it is based on the number of epochs and the average values of the loss function, rather than a validation loss plateau.
The provided context discusses a deep learning pipeline for object counting, but it does not explicitly mention strategies employed to handle randomness using a random seed value. However, there are some implicit strategies that can be inferred from the given information.

Firstly, in the context of data preparation, both datasets are split into training and testing images with a specific ratio. Although the exact method of splitting is not mentioned, it can be inferred that a fixed random seed value might have been used to ensure reproducibility and avoid bias in the data split.

Secondly, during the training of the models, a batch size of 8 is used. The selection of a batch size can introduce randomness in the training process. However, the context does not mention any specific strategy to handle this randomness.

Thirdly, the models are trained using the Stochastic Gradient Descent (SGD) optimizer and the Adam optimizer. Both of these optimizers introduce randomness through the use of random initialization of weights and stochastic updates. However, the context does not explicitly mention any strategy to handle this randomness.

Lastly, the early stopping technique is used to avoid overfitting based on the model's performance on the validation set. This technique can help in reducing the impact of randomness in the training process by stopping the training early if the model starts to overfit.

In summary, while the provided context does not explicitly mention strategies employed to handle randomness using a random seed value, it can be inferred that some implicit strategies such as fixed data split, batch size selection, and early stopping technique might have been used. However, the context does not provide enough information to confirm the use of a fixed random seed value.
Answer:::

The provided context does not give specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions that all architectures were trained with stochastic gradient descent, which suggests that some form of regularization was used during training. 

In general, there are several regularization techniques commonly used in deep learning to prevent overfitting, including dropout, L2 regularization, and data augmentation. Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent over-reliance on any single neuron and encourages the network to learn more robust features. L2 regularization, also known as weight decay, adds a penalty term to the loss function that encourages smaller weights, which can help to reduce overfitting by preventing the weights from becoming too large. Data augmentation is a technique where the training data is artificially expanded by applying random transformations, such as rotation, scaling, or flipping, which can help to improve the generalization of the model by providing more varied examples.

Without more specific information from the context, it is difficult to say which of these techniques were used in this particular case. However, it is mentioned that several methods were tried to balance the data sets, which suggests that data augmentation may have been used to address class imbalance. Additionally, the use of stochastic gradient descent suggests that some form of regularization was likely used, although the specific type is not specified.
The provided context discusses various sampling techniques used to address the problem of unbalanced training data in land use and land cover (LULC) mapping, but it does not directly mention regularization methods such as dropout or L2 regularization that are used to prevent overfitting in deep learning pipelines.

Regularization methods are used to prevent overfitting by adding a penalty term to the loss function during training, which discourages the model from learning overly complex patterns in the training data that may not generalize well to new data. Dropout is a regularization technique used in neural networks where randomly selected neurons are ignored during training, which helps to prevent over-reliance on any single neuron and promotes the development of more robust models. L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the model weights, which encourages the model to learn smaller weights and avoid overfitting.

While the provided context does not mention these specific regularization methods, it does discuss the importance of addressing overfitting in LULC mapping. Specifically, it notes that an incorrect sampling strategy may increase training complexity and lead to overfitting phenomena. This highlights the importance of using appropriate sampling techniques to ensure that the model is trained on a diverse and representative sample of the data, which can help to prevent overfitting.

In summary, while the provided context does not directly mention regularization methods such as dropout or L2 regularization, it does emphasize the importance of addressing overfitting in LULC mapping. This can be achieved through the use of appropriate sampling techniques, as well as other regularization methods that are commonly used in deep learning pipelines.
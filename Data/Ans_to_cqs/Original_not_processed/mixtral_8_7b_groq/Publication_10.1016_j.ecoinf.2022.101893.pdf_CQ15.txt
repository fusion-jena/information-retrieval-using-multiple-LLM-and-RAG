Based on the provided context, there is no explicit mention of any optimization techniques such as Stochastic Gradient Descent (SGD), Adam, RMSprop, etc. being used in the deep learning pipeline. However, it is mentioned that a dropout layer is added to the fully connected layer to reduce network parameters, which can be seen as a form of regularization to prevent overfitting.

Moreover, the context focuses on the description of the network architecture, dataset partitioning, and the comparison of the performance of various features and models, but it does not provide information about the optimization techniques used during the training of the models.

Therefore, it is not possible to provide a definitive answer to the query based on the given context. However, it is important to note that the choice of optimization technique can significantly impact the training of deep learning models, and it is a common practice to report the optimization method used in research papers.
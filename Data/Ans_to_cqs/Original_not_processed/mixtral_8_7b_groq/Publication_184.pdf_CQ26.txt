The provided context discusses a study that uses data augmentation as a technique to increase the amount of data and prevent overfitting during the training of a deep learning model. However, it does not explicitly mention the techniques used to address data bias during preprocessing. Therefore, based on the given information, I can only partially answer the query.

Partial Answer:::
In the context provided, the following technique is mentioned to be used during the preprocessing phase of the deep learning pipeline:

1. Data augmentation: This technique is used to extend the dataset by creating modified copies of existing data. The study describes applying random transformations such as horizontal flipping, grayscale conversion, blurring, adding noise, and cropping to the images. This approach increases the diversity of the dataset and helps the model generalize better, reducing overfitting.

However, the context does not mention other techniques commonly used to address data bias during preprocessing, such as:

1. Stratified splitting: This technique involves dividing the dataset into subgroups based on specific characteristics (e.g., class labels) and then ensuring that each subgroup is proportionally represented in both the training and testing sets.
2. Oversampling: This technique involves increasing the number of instances from underrepresented classes to balance the dataset.
3. Undersampling: This technique involves reducing the number of instances from overrepresented classes to balance the dataset.
4. Diverse data collection: This technique focuses on collecting data from various sources and ensuring that the dataset is representative of the target population to minimize bias.

In summary, based on the provided context, data augmentation is used as a preprocessing technique in the deep learning pipeline. However, other techniques commonly used to address data bias, such as stratified splitting, oversampling, undersampling, and diverse data collection, are not mentioned.

Partial Answer:::
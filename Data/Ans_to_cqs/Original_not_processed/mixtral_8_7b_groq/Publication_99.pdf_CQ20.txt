The context does not provide explicit information about the specific hardware resources used for training the deep learning models. However, it can be inferred that the training was likely performed on a machine equipped with Graphics Processing Units (GPUs) or similar accelerators, due to the use of deep learning techniques that can benefit from parallel processing capabilities offered by such hardware.

For instance, the PConv networks were trained using the Adam optimizer, which often benefits from GPU acceleration. Similarly, the Pix2Pix networks and the classification networks also used the Adam optimizer and cross-entropy loss, which can be computationally intensive and may have been accelerated using GPUs.

Moreover, the training of deep learning models often involves handling large datasets and performing numerous computations, which can be time-consuming and challenging on CPUs alone. Therefore, it is common practice to use GPUs or other accelerators to speed up the training process. However, without explicit information in the context, it is not possible to provide a definitive answer regarding the specific hardware resources used for training these models.
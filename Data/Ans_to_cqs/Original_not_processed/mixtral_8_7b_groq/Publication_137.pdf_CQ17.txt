Answer:::
The provided context discusses a deep learning pipeline for point cloud semantic segmentation, but it does not explicitly mention the regularization methods used to prevent overfitting, such as dropout or L2 regularization. However, it does mention a regularization strength parameter that influences the coarseness of the partition, which implies the use of some form of regularization.

Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. This penalty term discourages the model from learning overly complex patterns in the training data that may not generalize well to new data. There are several regularization methods commonly used in deep learning, including L1 and L2 regularization, dropout, and early stopping.

L1 and L2 regularization add a penalty term to the loss function proportional to the magnitude of the model's weights. L1 regularization, also known as Lasso regularization, adds a penalty term proportional to the absolute value of the weights, while L2 regularization, also known as weight decay or Ridge regularization, adds a penalty term proportional to the square of the weights. These regularization methods encourage the model to learn sparse weight vectors, which can help prevent overfitting.

Dropout is a regularization technique that randomly sets a fraction of the model's activations to zero during training. This helps prevent overfitting by reducing the co-adaptation of the model's weights and promoting the learning of more robust features.

Early stopping is a regularization technique that terminates the training process before the model starts overfitting. This is done by monitoring the model's performance on a validation set during training and stopping the training when the performance starts to degrade.

Based on the provided context, it is unclear which regularization method(s) were used in the deep learning pipeline for point cloud semantic segmentation. However, it is mentioned that the regularization strength parameter influences the coarseness of the partition, which suggests that some form of regularization was used. Without further information, it is not possible to provide a more specific answer.
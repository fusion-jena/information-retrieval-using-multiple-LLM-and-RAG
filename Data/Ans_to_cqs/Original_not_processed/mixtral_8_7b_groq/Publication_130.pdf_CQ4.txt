In the provided context, the specific data augmentation techniques used in the deep learning pipeline are not explicitly mentioned. However, it is stated that they used a data augmentation procedure to increase their training sample by artificially increasing the sample size from an existing set of images.

From general practices in deep learning, data augmentation techniques such as flipping, rotating, and scaling are commonly used. Flipping involves creating new images by flipping the original images horizontally or vertically. Rotating involves rotating the original images by a certain degree. Scaling involves changing the size of the original images. These techniques help increase the diversity of the training data, reduce overfitting, and improve the model's ability to generalize.

In the context, it is mentioned that they used images from different days in their training and validation datasets. This suggests that they may have used techniques such as temporal shifting or random cropping to increase the diversity of the training data.

Moreover, they replaced the VGG19 CNN network with layers with random weights that fit their particular task of interest and the corresponding number of classes. This could be seen as a form of data augmentation, as it introduces randomness and diversity into the model's architecture.

In summary, while the specific data augmentation techniques used in the deep learning pipeline are not explicitly mentioned in the provided context, it is likely that they used common techniques such as flipping, rotating, and scaling, as well as temporal shifting and random cropping to increase the diversity of the training data. Additionally, they introduced randomness into the model's architecture by replacing the VGG19 CNN network with layers with random weights.
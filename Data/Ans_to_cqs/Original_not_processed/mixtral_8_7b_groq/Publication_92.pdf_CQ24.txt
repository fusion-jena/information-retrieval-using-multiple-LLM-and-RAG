Based on the provided context, there is no explicit mention of strategies employed to handle randomness in the deep learning pipeline, such as the use of a random seed value. However, there are several strategies mentioned that are relevant to the deep learning process.

The text describes the use of a soft-max classifier in a deep neural network with an architecture similar to GoogLeNet. This network is trained on a dataset of cropped thumbnails and their corresponding labels. To improve localization accuracy, a new class called "part of fish" was introduced, and an existing class called "fish" was added to help the network recognize any fish, even if it is not in the learning database. The "fish" class contains fewer samples than the other classes to ensure that a fish is most likely labeled by its specific class rather than the generic class "fish."

Additionally, the text mentions the use of inception layers, which allow for dimensionality reduction and independence from the dimensional impact. The parameters of the network, such as the size of the strides and the first convolutions, were adapted to the size of the thumbnails, resulting in better performance than a classic architecture.

In the post-processing stage, a motion score is defined for each sliding window by computing the average absolute difference with the window at the same position in the previous frame. This score is used for the final detection decision based on the hypothesis that most fish are moving.

While these strategies are not explicitly related to handling randomness in the deep learning pipeline, they are important aspects of the deep learning process. The use of a random seed value can help ensure reproducibility in the training process, but it is not mentioned in the provided context.
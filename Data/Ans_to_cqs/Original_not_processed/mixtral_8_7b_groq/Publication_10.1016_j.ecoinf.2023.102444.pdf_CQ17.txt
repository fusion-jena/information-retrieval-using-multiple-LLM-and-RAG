The provided context discusses the use of Uniform Manifold Approximation and Projection (UMAP) for species distribution modeling (SDM) in ecology, but it does not mention any regularization methods used in deep learning pipelines such as dropout or L2 regularization.

However, I can provide some information on the regularization methods mentioned in the query.

Regularization methods are used in machine learning to prevent overfitting, which occurs when a model learns the training data too well and performs poorly on unseen data. Overfitting is a common problem in deep learning due to the complexity of the models and the large number of parameters.

Dropout is a regularization technique used in deep learning to prevent overfitting. It works by randomly dropping out, or setting to zero, a fraction of the hidden units in each layer during training. This prevents the co-adaptation of hidden units and helps the model generalize better to unseen data.

L2 regularization, also known as weight decay, is another regularization technique used in deep learning. It works by adding a penalty term to the loss function that is proportional to the sum of the squares of the model's weights. This encourages the model to learn smaller weights and prevents it from assigning too much importance to any single feature.

These regularization methods can be used in the deep learning pipeline to prevent overfitting and improve the model's performance on unseen data.
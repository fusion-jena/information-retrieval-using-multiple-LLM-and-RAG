Answer:::

The hyperparameters used in the deep learning model are the learning rate, optimizer, number of epochs, batch size, dropout rate, and the configuration of the convolutional layers.

The models were trained for 50 epochs using the Adam optimizer, with a learning rate of 0.001. The batch size was not explicitly mentioned, but it was used in conjunction with the Adam optimizer during training.

The models consisted of three convolutional layers, each with 32 filters and a kernel size of 4 × 4. After each convolutional layer, there was a dropout layer with a rate of 0.4, followed by a max pooling layer with a kernel size of 4 × 4. After the convolutional layers, there was a fully connected layer with 64 ReLU units, another dropout layer with a rate of 0.4, and a softmax function. The number of units in the softmax function depended on the number of species being classified (two units for CNN1 and three or four units for CNN2).

The customized CNN models were compared with a pre-trained ResNet152V2 architecture, which demonstrated good performance in animal sound classification tasks. However, the specific hyperparameters used for the ResNet152V2 model were not provided in the context.

The models were implemented using TensorFlow and Python 3, and the Ubuntu 20.04 operating system was used. The model training and prediction procedures were executed on Microsoft Azure using instance NV12s v3 with 12 vCPUs and 112 GB RAM.

In summary, the hyperparameters used in the deep learning model include the learning rate of 0.001, the Adam optimizer, 50 epochs, a batch size (not specified), dropout rates of 0.4, and the configuration of the convolutional layers.
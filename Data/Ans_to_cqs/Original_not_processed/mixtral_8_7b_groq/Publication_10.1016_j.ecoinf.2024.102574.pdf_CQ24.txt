The provided context does not directly discuss strategies employed to handle randomness in deep learning pipelines. However, it does mention the use of machine learning methods, such as random forests and neural networks, in the context of estimating above-ground biomass (AGB) in forest ecosystems. In these contexts, randomness can be handled through various strategies, including:

1. **Random seed value**: Deep learning models often involve random initializations, such as the initialization of model weights. By setting a fixed random seed value, researchers can ensure that these random initializations are reproducible, enabling consistent results across multiple runs of the same model.

2. **Data splitting**: When training and evaluating machine learning models, it is common practice to split the dataset into training, validation, and testing sets. This process can be randomized to ensure that each subset of the data is representative of the overall dataset.

3. **Data augmentation**: In cases where the available data is limited, data augmentation techniques, such as rotation, scaling, or flipping, can be used to artificially increase the size of the dataset. These techniques can introduce randomness into the dataset, helping the model generalize better.

4. **Regularization**: Techniques like dropout and L1/L2 regularization can be used to prevent overfitting by randomly dropping out neurons during training or penalizing large weights, respectively.

5. **Batch normalization**: This technique, which normalizes the inputs of each batch, can introduce randomness by shuffling the data within each batch.

6. **Early stopping**: This strategy involves monitoring the model's performance on a validation set during training and stopping the training process once the performance starts to degrade. This can help prevent overfitting and reduce the impact of randomness in the training process.

While these strategies are not explicitly mentioned in the provided context, they are commonly used in deep learning pipelines to handle randomness and improve model performance.
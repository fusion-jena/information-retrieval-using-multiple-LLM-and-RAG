Answer:::

The strategy implemented to monitor the model performance during training involved using a separate evaluation set, which consisted of 20% of the data. This set was used to assess the precision of the models after they had been trained on 80% of the data. Two metrics were employed to evaluate the accuracy of the models: the area under the curve (AUC) of the receiver operating characteristics (ROC) and the true skill statistics (TSS). The AUC of the ROC measures the model's ability to distinguish between positive and negative classes, while the TSS provides a single threshold-independent measure of the model's performance.

Furthermore, to ensure the robustness of the results, two ensemble approaches were applied: committee averaging and weighted mean. These methods were used to create separate ensemble models for each climate scenario and period. When selecting the final ensemble models, special attention was given to those with a TSS score of â‰¥0.7, indicating a high level of accuracy.

The use of a separate evaluation set, combined with the two accuracy metrics and ensemble methods, allowed for a rigorous monitoring of the model performance during training. This strategy ensured that the final ensemble models were robust, accurate, and well-suited for predicting the distribution of species under different climate scenarios.
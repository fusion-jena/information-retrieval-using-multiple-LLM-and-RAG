Answer:::

The data are split into training, validation, and testing sets for deep learning model training using specific criteria. The training and validation set is divided into ten parts using the 10-fold cross-validation method. In this method, nine parts are used as a training set, and one part is used for the validation set. This process is repeated ten times, with a different part used for validation in each iteration. The mean value of the ten results is used to estimate the accuracy of the model.

The percentage of the training, validation, and testing sets is 5.4:0.6:1. The training set is the data sample used for model fitting, and it consists of 5.4% of the total data. The validation set is a set of samples set aside separately during the model training and is used to adjust the hyperparameters of the model and to conduct a preliminary evaluation of the modelâ€™s capabilities. It consists of 0.6% of the total data. The testing set is used to evaluate the generalization ability of the final model and consists of 1% of the total data. However, it cannot be used as a basis for algorithm-related selection such as parameter tuning and selection of features.

The training set used in this study includes grid data with neighborhood features of six years (1986, 2005, 2008, 2010, and 2013). The features include the LU type, soil, topography, elevation, slope, aspect, distance to settlements, distance to roads, distance to rivers in this grid unit, and the four neighborhood grid units.

Additionally, constraint factors are used in the training set, which includes statistical data and the quantity simulated by the backtracking of the Markov model. The numbers of different LU types in 2000 after modification by the simulation numbers are used as constraint factors. Water and settlements are constraint factors and can be determined by manual interpretation, and thus, no simulation is needed.

Batch normalization is performed on the input data due to the large differences in the distribution between different batches of training data in the training stage and the difference between the training set and the test set. Batch normalization is a technique used to reparameterize and normalize the activations of the neurons in a layer of a neural network. It is used to reduce the internal covariate shift, which is the change in the distribution of the inputs to a layer caused by the change in the parameters of the previous layers. By reducing the internal covariate shift, batch normalization helps to improve the training speed, model convergence, and generalization performance of deep learning models.
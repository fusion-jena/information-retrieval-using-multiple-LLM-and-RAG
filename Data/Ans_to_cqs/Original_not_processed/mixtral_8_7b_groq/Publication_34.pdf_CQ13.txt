Answer:::
The hyperparameters used in the deep learning models for species distribution modeling include the initial learning rate, optimization algorithm (type of optimizer), momentum, batch size, and number of epochs. Specifically, in the study mentioned, the models were trained using a stochastic gradient descent algorithm with a momentum of 0.9, a batch size of 50, and an initial learning rate of 0.03. The learning rate was found to be critical for the optimization behavior, where a learning rate that is too big can lead to training loss divergence, and a learning rate that is too small can result in slow learning. The number of epochs varied for each model, taking approximately 5 minutes for the SNN, 10 minutes for the DNN, and 5 hours for the CNN. Additionally, the study employed a learning rate schedule called Batch Normalization, which consists of a parameterized centering and reduction of pre-activations. This technique facilitates optimization, enables the raising of the learning rate for quicker convergence, and has a regularization effect.

Moreover, the study suggests that future work could include a more suited stopping rule and a quick automated procedure for tuning optimization hyper-parameters, especially the initial learning rate. Furthermore, they propose minimizing the number of non-null connections in the network and introducing an L1-type penalty on the network parameters to make the model more interpretable.

Answer:::
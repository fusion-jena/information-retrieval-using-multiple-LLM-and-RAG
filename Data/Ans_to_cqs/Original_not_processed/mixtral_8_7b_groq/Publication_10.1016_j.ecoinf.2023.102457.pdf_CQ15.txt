The provided context discusses the use of deep learning models for individual acoustic discrimination of gibbons, but it does not explicitly mention the optimization techniques used in the deep learning pipeline. However, it is mentioned that pre-trained models such as VGGish, Wav2Vec 2.0, and BirdNET were used for comparison. These models were likely trained using optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam), which are commonly used in deep learning.

For instance, the BirdNET model, which was used for extracting embeddings, is a deep learning model that was originally trained on a large dataset of bird vocalizations. The training of such a model would have required the use of an optimization technique to minimize the loss function and update the model's parameters.

Moreover, the context highlights the potential of training a model specifically on gibbon vocalizations, which could result in even better performance. This implies that the training process would involve the use of an optimization technique to adjust the model's parameters and improve its performance.

Therefore, while the specific optimization techniques used in the deep learning pipeline are not mentioned in the provided context, it is reasonable to assume that optimization techniques such as SGD or Adam were used during the training of the pre-trained models that were compared.
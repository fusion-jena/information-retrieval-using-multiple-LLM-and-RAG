Answer:::

In the context provided, the data for deep learning model training is split using a specific approach. For each successive model iteration, 70% of the training data is used for training the model, while the remaining 30% is used for validating the model's performance. This train/validation split approach is chosen to minimize potential overfitting of the model on small sample sizes, as compared to a cross-validation approach. The F1 score is used as the validation metric, which is a standard performance metric that includes information on both model precision and recall.

The training data comprises 6363 hours of audio from ten independent sites, while the validation dataset consists of 30% of this training data. The audio data are stored as FLAC files at a sampling rate of 44.1 kHz for the solar-powered bioacoustic recorders and WAV files at a 32 kHz sampling rate for the Audiomoth recorders. All audio data are unlabelled at the commencement of the study.

A separate test dataset is used for final testing and evaluation of the model. However, the specific details of this test dataset are not provided in the context.

Additionally, it is mentioned that an active learning approach is applied to iteratively train and improve the CNN model. This approach involves training the model on a small initial dataset, using the model's predictions to select the most informative data points for labeling, and then re-training the model on the expanded labeled dataset. This process is repeated until the stopping criterion is reached, which is when successive model iterations do not reduce the F1 score. A final model is then trained using the combined training and validation data.

Therefore, the criteria used to split the data for deep learning model training include a train/validation split approach, where 70% of the training data is used for training and 30% is used for validation. The F1 score is used as the validation metric, and a separate test dataset is used for final testing and evaluation of the model.
The text provided discusses the use of deep learning techniques for mapping burned areas in the Cerrado biome in Brazil, but it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it is possible to infer from the literature referenced that dropout and L2 regularization are likely to have been used.

Deep learning models, such as the Deep Neural Network (DNN) used in this study, are prone to overfitting, especially when trained on small datasets. Overfitting occurs when the model learns the training data too well, including its noise and outliers, and performs poorly on new, unseen data. Regularization techniques are used to prevent overfitting by adding a penalty term to the loss function, which discourages the model from learning overly complex patterns in the training data.

Dropout is a regularization technique that randomly sets a fraction of the hidden units in each layer to zero during training. This prevents the co-adaptation of hidden units and forces the model to learn more robust features. Dropout has been shown to be an effective regularization method for deep neural networks, improving their generalization performance on various tasks.

L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the squared magnitude of the model's weights. This encourages the model to learn smaller weights, which reduces the capacity of the model and prevents overfitting. L2 regularization has been widely used in deep learning and has been shown to improve the performance of deep neural networks on various tasks.

Therefore, it is likely that the deep learning pipeline used in this study employed dropout and L2 regularization to prevent overfitting. However, the text does not provide enough information to confirm this, and further investigation would be needed to determine the specific regularization methods used.
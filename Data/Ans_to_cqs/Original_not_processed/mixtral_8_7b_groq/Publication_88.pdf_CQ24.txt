The provided context does not contain specific information about strategies employed to handle randomness in the deep learning pipeline. However, it does mention that the performance of the evaluated methods was assessed by running them multiple times with different choices of training samples for the class "no deforestation." This suggests that some form of randomness was introduced by selecting different training samples, and the variation in performance was measured to evaluate the models' stability.

In general, deep learning pipelines can employ various strategies to handle randomness:

1. **Random seed value**: A fixed seed value can be set for random number generators to ensure reproducibility of results. This includes random initialization of weights in neural networks, data shuffling, and dropout regularization.

2. **Cross-validation**: To reduce the impact of randomness in model training, k-fold cross-validation can be used. This technique divides the dataset into k subsets, trains the model k times, and evaluates it on the remaining k-1 subsets. The average performance metric is then calculated.

3. **Ensemble methods**: Multiple models can be trained with different random initializations or subsets of the data. The final prediction is obtained by combining the outputs of these models, which can help reduce the impact of randomness and improve overall performance.

4. **Data augmentation**: Random transformations can be applied to the training data, such as rotation, scaling, or flipping, to increase the dataset size and reduce overfitting.

While the provided context does not explicitly mention these strategies, they are common practices in deep learning to handle randomness and improve model performance.
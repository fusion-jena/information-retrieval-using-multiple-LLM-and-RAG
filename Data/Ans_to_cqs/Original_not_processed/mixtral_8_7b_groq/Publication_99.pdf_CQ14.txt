Based on the provided context, there is no explicit information about how the hyperparameters of the models were optimized. Hyperparameter optimization techniques such as grid search, random search, or Bayesian optimization are not mentioned in the text.

However, the context does provide some details about the hyperparameters used for each model. For instance, the PConv networks were trained with a learning rate of 2e-4, and the classification networks were trained with a learning rate of 0.03. The optimizer used for both types of networks is Adam. The loss function for the PConv networks is a combination of per-pixel loss, perceptual loss, style loss, and total variation loss, while the loss function for the classification networks is cross-entropy loss.

Additionally, the context mentions that the classification models were trained for 20 epochs, and the PConv networks were trained for 50 epochs. The batch size and input dimension vary for different models.

Therefore, while the specific method of hyperparameter optimization is not provided, the context does offer some insight into the chosen hyperparameters for each model.
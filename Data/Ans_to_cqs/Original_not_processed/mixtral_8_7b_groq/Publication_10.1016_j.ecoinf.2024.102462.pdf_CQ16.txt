Answer:::

The context provided does not give specific criteria used to determine when training of the deep learning (DL) networks is complete. However, it mentions that once the training process is finished, the network is evaluated using an independent testing data set. This suggests that one possible criterion for determining training completion could be when the validation loss plateaus or stops decreasing significantly.

In the context of DL, training a network involves adjusting the parameters using annotated data, also known as supervised learning. During training, the network learns patterns from the training data set. Once the network has learned these patterns, further training may not improve its performance, and could even lead to overfitting, where the network performs well on the training data but poorly on new, unseen data. Therefore, monitoring the validation loss and stopping training when it plateaus is a common strategy to prevent overfitting.

The context also provides a table showing the number of patches (#patch) and true positive/false positive rate (TPR/FPR) values for each DL network and species, both with and without data augmentation. Data augmentation is a technique used to increase the size of the training set by generating additional training examples through transformations such as rotation, scaling, or flipping. By comparing the TPR/FPR values between the models trained with and without data augmentation, one can observe the impact of data augmentation on the performance of the DL networks.

In summary, while the context does not provide specific criteria used to determine when training is complete, it suggests that monitoring the validation loss and stopping training when it plateaus could be a possible criterion. Additionally, the table provided allows for comparison of the performance of DL networks trained with and without data augmentation.
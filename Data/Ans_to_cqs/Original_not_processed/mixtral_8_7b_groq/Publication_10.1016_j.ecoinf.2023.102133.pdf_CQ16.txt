Answer:::
Based on the provided context, there are no specific details about the criteria used to determine when training is complete for the machine learning models. However, it is mentioned that cross-validation was used to assess the models' reliability and avoid overfitting. Cross-validation is a technique used during the training phase to estimate how accurately a predictive model will perform on an independent data set. It involves dividing the dataset into k subsets, or folds, and training and validating the model k times, with each fold serving as the validation set once. This can help to identify if the model is overfitting or underfitting the data.

In addition, the best models were selected based on their performance on the complete data subset for ETa, the output of the proposed data-driven models. The best model was chosen based on the root mean square error (RMSE) and the coefficient of determination (R2). The lowest RMSE and the highest R2 indicate a better fit of the model to the data.

It is also worth noting that the training process of the Random Forest model, which achieved the best performance, involves several parameters such as the number of trees, the maximum number of features, the depth of the tree, the minimum number of samples required to split a node, the minimum number of samples required at each leaf node, the method of selecting samples for training each tree, and the parameter for minimal cost-complexity pruning. These parameters could have an impact on the training process and the final performance of the model.

In summary, while the context does not provide specific information on the criteria used to determine when training is complete, it can be inferred that cross-validation and performance metrics such as RMSE and R2 were used to assess the models' reliability and select the best models.
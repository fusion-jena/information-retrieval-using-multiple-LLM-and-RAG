Answer:::

The pieces of context provided do not directly mention the specific metrics used to evaluate the performance of the deep learning model in the study by Mohedano et al., 2017. However, the study does present a table with accuracy values for different experiments.

In the context of machine learning and deep learning, several metrics are commonly used to evaluate the performance of models. Some of these metrics include accuracy, precision, recall, F1 score, and area under the curve (AUC).

Accuracy is the ratio of the number of correct predictions to the total number of input samples. It is a common and straightforward metric for evaluating the performance of a model. However, it may not always be the best metric, especially for imbalanced datasets.

Precision, also known as positive predictive value, is the ratio of true positive predictions to the sum of true positive and false positive predictions. It measures the proportion of correct positive predictions out of all positive predictions made by the model.

Recall, also known as sensitivity, is the ratio of true positive predictions to the sum of true positive and false negative predictions. It measures the proportion of correctly identified positive samples among all actual positive samples.

F1 score is the harmonic mean of precision and recall, providing a balance between the two metrics. It is a more robust metric than accuracy, especially for imbalanced datasets.

Area under the curve (AUC) is a metric used for binary classification problems. It measures the entire two-dimensional area underneath the entire ROC curve (receiver operating characteristic curve), which plots the true positive rate against the false positive rate. AUC provides an aggregate measure of performance across all possible classification thresholds.

Based on the information provided in the Mohedano et al., 2017 study, it can be inferred that they used accuracy as a metric to evaluate the performance of their deep learning model. However, other metrics like precision, recall, F1 score, and AUC could also have been used for a more comprehensive evaluation.
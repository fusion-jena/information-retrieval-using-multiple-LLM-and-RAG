Answer:::

The optimization technique applied in the deep learning pipeline is the Adam Solver type. This information is provided in the first piece of context, which states: "We used a learning rate of 10, a learning decay with a Gamma of 0.95, a dropout of 50% and an Adam Solver type as learning parameters." The Adam optimization algorithm is a popular choice for training deep learning models due to its efficiency and ability to adapt to different learning rates. It is an extension of the stochastic gradient descent (SGD) method, which is another optimization technique commonly used in deep learning. However, in this specific case, the Adam Solver type was chosen over the SGD method.
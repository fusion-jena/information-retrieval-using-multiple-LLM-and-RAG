The text provided does not give specific details about the hardware resources used for training the deep learning models. It mainly focuses on the comparison of different CNN architectures for species classification and their performance metrics. However, it is mentioned that deep learning networks with a large number of parameters, such as ResNetV50 and InceptionNetV3, showed overfitting, indicating the need for more training data or computational resources to handle such large models. Therefore, it can be inferred that high-end computational hardware, such as GPUs or TPUs, were likely used to train these deep learning models, given the large number of learnable parameters and the need to handle large datasets.

In the context of the chosen model with 2,197,578 learnable parameters and an F1-score of 92.75%, the focus is on the architecture's performance and parameter efficiency rather than the specific hardware resources used for training. It is also mentioned that the Adam optimizer was used, which is a computationally efficient optimization algorithm, suggesting that the training process was optimized for performance on available hardware resources.

In summary, while the text does not explicitly mention the hardware resources used for training the deep learning models, it implies the use of high-end computational hardware, such as GPUs or TPUs, due to the large number of learnable parameters and the need to handle large datasets.
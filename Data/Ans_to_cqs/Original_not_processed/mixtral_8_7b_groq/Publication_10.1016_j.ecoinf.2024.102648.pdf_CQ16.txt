Answer:::

In the context provided, there are two methods used to determine when training is complete: early stopping and the penalty method.

Early stopping is a process used in the bootstrap forest and boosted tree methods to stop growing additional trees or layers when further addition does not improve the validation statistic. In this case, the validation statistic is the R2 value for a continuous response. This method is used to prevent overfitting of the predicted percent CLCuD.

The penalty method is employed within the framework of artificial neural network (ANN) models to address the inclination of neural networks to overfit data.

In addition, the training of an artificial neural network model is complete when the validation loss plateaus or shows no significant improvement. This can be observed in Figure 4, where the actual by predicted plot of CLCuD (%) for training and validation data is shown. The residual by predicted plot of CLCuD (%) for training and validation data is also shown, which helps to visualize the accuracy of the model.

In Table 1, different activation functions and the number of hidden nodes/neurons and layers are tested for the artificial neural network model. The R2 and RMSE values for training and validation are provided, which can be used to determine when the training is complete based on the validation loss plateau.

Overall, the criteria used to determine when training is complete include early stopping, the penalty method, and observing the validation loss plateau. These methods are used to prevent overfitting and ensure the accuracy of the model.
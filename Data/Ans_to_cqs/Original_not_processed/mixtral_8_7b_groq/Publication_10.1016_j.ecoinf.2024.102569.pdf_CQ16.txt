Answer:::
The provided context does not give specific information about the criteria used to determine when training is complete in the described method. However, it does mention the use of cross-validation to calculate the Root Trimmed Mean Squared Prediction Error (RTMSPE) for tuning parameters. Cross-validation is a common technique used to assess model performance and prevent overfitting, but it is not specified if it is used as a stopping criterion for training.

In general, training of machine learning models can be stopped based on various criteria, such as:

1. Validation loss plateau: Training can be stopped when the performance on a validation set does not improve for a certain number of epochs or iterations. This helps to prevent overfitting and ensures that the model generalizes well to unseen data.
2. Early stopping: This is a regularization technique that stops training when the performance on a validation set starts to degrade. This can help to reduce overfitting and improve the model's ability to generalize.
3. Computational budget: Training can be stopped when the computational budget, such as time or resources, is exhausted.
4. Convergence: Training can be stopped when the model's parameters converge to a stable solution or when the change in the loss function is below a certain threshold.

In the context provided, the RTMSPE is used to select appropriate values for the tuning parameters τ and ϕ. However, it is not explicitly mentioned if this metric is used as a stopping criterion for training.
Answer:::

The optimization techniques applied in the deep learning pipeline include Stochastic Gradient Descent (SGD), Adam, and other variants.

In the context provided, the optimization of the Convolutional Neural Network (CNN) parameters is achieved during the training phase through a process called back-propagation. Back-propagation involves automatically changing the CNN parameters through the comparison between its output and the correct class of the training element to improve the final classification rate. However, the specific optimization technique used is not mentioned.

Stochastic Gradient Descent (SGD) is a popular optimization technique in deep learning. It updates the model parameters by computing the gradient of the loss function with respect to the parameters for a single training example or a small batch of training examples at each iteration. SGD is simple and efficient but can converge slowly and may get stuck in saddle points or local minima.

Adam is another optimization technique that has gained popularity in deep learning. It is an adaptive learning rate method that computes individual learning rates for different parameters based on the estimates of the first and second moments of the gradients. Adam combines the advantages of two other optimization techniques, AdaGrad and RMSProp, and has been shown to perform well in practice.

Other optimization techniques used in deep learning include AdaDelta, Rprop, and Adadelta. These techniques differ in how they compute the learning rates and update the parameters. The choice of the optimization technique depends on the specific problem and the characteristics of the data.

In summary, the optimization techniques applied in the deep learning pipeline include Stochastic Gradient Descent (SGD), Adam, and other variants. The specific optimization technique used depends on the problem and the data characteristics.
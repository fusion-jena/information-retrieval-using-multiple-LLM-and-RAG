Based on the provided context, there is no explicit mention of which optimization techniques were applied in the deep learning pipeline. However, it is mentioned that Guo et al. (2021) used a deep learning-based model to classify points of interest in 3D volumetric images. Common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adam, and RMSprop.

Stochastic Gradient Descent (SGD) is a simple optimization algorithm that seeks to minimize a function by iteratively moving in the direction of steepest descent. It is a stochastic version of the batch gradient descent algorithm, which uses the entire training dataset to compute the gradient of the loss function at each iteration. In contrast, SGD uses a single example from the training dataset at each iteration, making it more computationally efficient.

Adam is an optimization algorithm that combines the advantages of SGD and other optimization algorithms. It uses the adaptive learning rate of Adagrad, the momentum of RMSprop, and the ability to handle non-stationary objectives of Adadelta. Adam has been shown to perform well on a wide range of deep learning tasks.

RMSprop is an optimization algorithm that adjusts the learning rate based on the recent gradient history. It is similar to Adagrad, but it uses a moving average of the squared gradient instead of the sum of the squared gradient. This makes it more computationally efficient than Adagrad.

Without more information from the provided context, it is difficult to determine which optimization technique was used in the deep learning pipeline. However, it is likely that one of the aforementioned optimization techniques was used.
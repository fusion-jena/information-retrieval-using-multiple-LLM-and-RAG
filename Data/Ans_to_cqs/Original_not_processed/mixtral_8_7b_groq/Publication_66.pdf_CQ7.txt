Answer:::

The provided context does not explicitly state the criteria used to split the data for deep learning model training. However, it does mention that leave-one-out cross-validation was carried out during the study. This method involves removing a single bird from the dataset for each species, training the model on the other birds, and then validating the model on the single bird. This process is repeated for each bird in the dataset, providing a robust method for assessing the model's performance.

Additionally, the context states that the area under the receiving operating characteristic curve (AUC) for training and validation data was calculated. The AUC is a metric used to assess the performance of a binary classification model, and it provides an aggregate measure of the model's ability to distinguish between the two classes. The AUC is calculated using both the training and validation data, which suggests that the data was split into these two sets for model training and evaluation.

Furthermore, sensitivity and specificity for each model, as well as positive predicted value (PPV) and negative predicted value (NPV), were calculated. Sensitivity and specificity are metrics used to assess the performance of a binary classification model, and they provide insights into the model's ability to correctly identify positive and negative cases, respectively. PPV and NPV provide additional information about the model's performance, and they are calculated using the number of true predicted positives and negatives, respectively, divided by all predicted positives and negatives.

Overall, while the context does not explicitly state the criteria used to split the data for deep learning model training, it does provide evidence that leave-one-out cross-validation was carried out, and that the data was split into training and validation sets. Additionally, sensitivity, specificity, PPV, and NPV were calculated, which suggests that the model's performance was evaluated using these metrics.
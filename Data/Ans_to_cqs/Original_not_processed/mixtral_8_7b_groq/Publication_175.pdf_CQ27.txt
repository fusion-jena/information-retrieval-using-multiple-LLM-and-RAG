Based on the provided context, there is no explicit information about the process followed to deploy the trained deep learning model. The text mainly focuses on the training and evaluation of the model using different datasets.

However, we can make some educated guesses about the deployment process based on common practices in deep learning. 

First, once the model is trained and evaluated, it is likely that the model was saved or serialized to disk. This is a common practice in deep learning to ensure the trained model can be loaded and used for inference in the future without having to retrain the model. The serialized model can be saved in various formats depending on the deep learning framework used, such as a .h5 file in Keras or a .pth file in PyTorch.

Second, the deployed model would need to be integrated into a larger system or platform for use in a real-world application. The context does not provide information on what this platform might be, but it could be a web application, a mobile application, or an embedded system, among others. The choice of platform would depend on the specific requirements of the application.

Third, the deployed model would need to be served in a way that allows it to receive input data and return predicted output. This could be done using a variety of methods, such as setting up a REST API endpoint that accepts HTTP requests with input data and returns the predicted output, or integrating the model into a larger application that handles user interaction and data processing.

In summary, while the provided context does not give explicit information about the process used to deploy the trained deep learning model, we can make some educated guesses based on common practices in deep learning. The model was likely serialized to disk, integrated into a larger platform, and served in a way that allows it to receive input data and return predicted output.
The provided context does not give specific information about the hardware resources used for training the deep learning models in the studies by Halgamuge et al. (2020) and Charizanos and Demirhan (2023). However, it is mentioned that the model efficiency of the Halgamuge et al. (2020) study was 23.1 minutes on the same machine used in previous models, which implies that they likely used a high-performance computing system with a powerful CPU and possibly a GPU or TPU for training their deep learning models.

In general, deep learning models can be computationally intensive and require significant hardware resources for training. Graphics processing units (GPUs) and tensor processing units (TPUs) are commonly used for training deep learning models due to their ability to perform parallel computations, which significantly speeds up the training process compared to using a central processing unit (CPU) alone.

Therefore, while the specific hardware resources used for training the deep learning models in the studies are not provided, it is reasonable to assume that they used a high-performance computing system with a GPU or TPU for training their models.
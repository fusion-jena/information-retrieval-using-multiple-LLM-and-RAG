Answer:::

The text provided does not give specific details on how the data was split for training, validation, and testing of the deep learning models. However, it does mention that they used a sensitivity study to test the number of local training samples needed to achieve maximum performance. They selected different proportions of the total hand-annotations for training, specifically, 5%, 25%, 50%, and 75%. These selections were then compared to the full dataset for the within-site results for each site. The experiment was repeated 5 times to account for the random subsampling of annotations. Additionally, they ran the evaluation plots for the pretraining model only (i.e. 0% hand-annotated data) to assess whether the addition of hand-annotated data improved the within-site pretraining.

In terms of geographic generalization, they performed three types of experiments that used different combinations for hand-annotations and pretraining data. The first experiment is to use pretraining and hand-annotated data to predict the evaluation data from the same site (‘within-site’). The next setup is to use the pretraining data and hand-annotated from the same site to predict the evaluation data from a different site (‘cross-site’). To assess generalization without local pretraining data, they tested a model training using pretraining data from all other sites, but hand annotations from the same site as the evaluation data (‘transfer-learning’).

In summary, the text provides information on how the data was split for the sensitivity study and the geographic generalization experiments, but it does not explicitly mention the use of separate datasets for training, validation, and testing.
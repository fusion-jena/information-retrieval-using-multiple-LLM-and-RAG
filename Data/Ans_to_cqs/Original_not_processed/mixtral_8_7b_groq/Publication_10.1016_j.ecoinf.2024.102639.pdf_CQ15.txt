Based on the provided context, there is no direct information about the optimization techniques used in a deep learning pipeline such as Stochastic Gradient Descent (SGD) or Adam. However, the text does discuss the use of optimization techniques in the context of Support Vector Machines (SVM), another type of model used in machine learning.

In the SVM model, the regularization parameter 'C' and the kernel type (linear) are calibrated using a combination of grid search and cross-validation. This process aims to balance the complexity of the model and its ability to generalize to new data. In instances involving non-linear SVMs, the kernel coefficient 'gamma' is optimized using grid search to ensure the model is aligned with the data distribution.

In the context of deep learning, optimization techniques like SGD and Adam are commonly used to minimize the loss function and update the model's weights during training. These methods are used to find the optimal set of weights that can accurately predict the output for a given input.

In summary, while the provided context does not give specific information about the optimization techniques used in deep learning pipelines, it does discuss the use of optimization techniques in the context of SVMs. Additionally, it is important to note that optimization techniques like SGD and Adam are commonly used in deep learning for training models.
Based on the information provided, there is no explicit mention of how the hyperparameters of the model are optimized, such as through methods like grid search or random search. However, the text does provide some details on the hyperparameters used in the model's training process.

The batch size for the training network is set to 32, which is a common choice to balance memory constraints and computational resources. The optimizer used is Adam, which is a popular choice for its computational efficiency and ability to adapt to different problems. The learning rate is varied with group based on the cosine function's law, often ranging between 0.0 and 1.0. This is a form of learning rate scheduling, which can help improve model convergence and accuracy.

The model is trained for 100 epochs with an early stopping function, which halts the learning process if there is no improvement in accuracy after 20 epochs. This is a form of regularization that can help prevent overfitting. The loss function used is the cross-entropy loss function, which is commonly used for classification problems.

While these details provide some insight into the model's training process, they do not specify how the hyperparameters were optimized. It is possible that the authors used a method like grid search or random search, but this information is not provided in the context. Therefore, I cannot provide a definitive answer to this query.
Answer: In the study, the hyperparameters of the models were optimized using the Tree-structured Parzen Estimator (TPE) algorithm. The TPE algorithm is a probabilistic model that estimates the probability distribution of the hyperparameters and iteratively constructs a tree of Gaussian distributions to approximate the objective function. This probabilistic model then selects new hyperparameters for the subsequent rounds of model evaluation. The TPE algorithm is designed for efficient searching in high-dimensional spaces, making it suitable for optimizing the hyperparameters of complex models. The TPE algorithms were implemented in the CatBoost package (V.1.0.6), LightGBM package (V.3.2.1), XGBoost package (V.1.5.0), and Hyperopt package (V.0.2.7) for this study.

During the optimization process, each machine learning model was trained on 100 different subsets generated from the original training sets using the bootstrap method (sampling with replacement). The number of samples in each subset equaled the number of samples in the original training data. The predictions of the trained models on the testing dataset were averaged to obtain the main prediction. The average loss, average bias, and average variance were then derived for each model. The noise was assumed as 0 following previous studies (Domingos, 2000; Kohavi and Wolpert, 1996).

It is worth noting that the TPE algorithm was used to optimize the hyperparameters of the eleven machine learning models utilized in the study, including multilayer perceptron (MLP), logistic regression (LR), na√Øve Bayes (NB), CatBoost (CAT), random forest (RF), XGBoost (XGB), LightGBM (LGB), weighted soft voting (WSOFT), weighted hard voting (WHARD), blending, and stacking.

In summary, the hyperparameters of the models were optimized using the Tree-structured Parzen Estimator (TPE) algorithm, which is a probabilistic model that estimates the probability distribution of the hyperparameters and iteratively constructs a tree of Gaussian distributions to approximate the objective function. This algorithm was implemented in the CatBoost package (V.1.0.6), LightGBM package (V.3.2.1), XGBoost package (V.1.5.0), and Hyperopt package (V.0.2.7) for this study.
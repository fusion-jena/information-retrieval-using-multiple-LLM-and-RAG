Answer:::
In the given context, there is no explicit information provided about the specific criteria used to determine when training is complete. However, it can be inferred that the training process is automated and updates the model prototype continuously. The user can view the training metrics through the training tab.

The training cycle is demonstrated in Fig. 9, comparing active learning prioritizing low-certainty samples, active learning using high-certainty samples, and randomly presented samples. The results show that active learning using low-certainty sampling achieved 90% test accuracy with 20 training samples, while random sampling required 80 samples to reach the same accuracy. Active learning using high-certainty samples performed the worst and failed to converge within the first epoch.

Based on this information, it can be assumed that the training process might be using a form of early stopping based on the model's performance on the test set. This means that the training could be complete when the model's performance on the test set stops improving, even if the validation loss has not plateaued.

Additionally, the training process maintains an 80/20% training-validation split for each class, and the model performance is re-evaluated on the test set at 10 sample increments. This approach allows for monitoring the model's performance throughout the training process and could be used as a criterion for determining when training is complete.

In summary, while the specific criteria for determining when training is complete are not provided in the context, it can be inferred that the training process might be using early stopping based on the model's performance on the test set or monitoring the model's performance throughout the training process.
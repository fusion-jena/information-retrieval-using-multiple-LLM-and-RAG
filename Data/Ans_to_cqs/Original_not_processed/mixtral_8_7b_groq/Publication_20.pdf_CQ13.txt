Answer:::

The hyperparameters used in the deep learning model include:

1. Learning Rate: The learning rate used was 10.
2. Learning Decay: A learning rate decay with a Gamma of 0.95 was used.
3. Dropout: A dropout of 50% was used.
4. Solver Type: The Adam Solver type was used as the learning parameter.
5. Weight Initialization: A classic random Gaussian initialization was used for weight initialization.

These hyperparameters were chosen to prevent overfitting and to ensure fast convergence of the network. The learning rate decay with a Gamma of 0.95 was used to slowly reduce the learning rate as the training progressed, which helped the model to converge to a better minimum. The dropout of 50% was used to prevent overfitting by randomly dropping out 50% of the neurons during training. The Adam Solver type was used as the optimizer due to its efficiency and adaptability in learning rate. The weight initialization was done using a random Gaussian initialization, which is a common practice in deep learning.

Additionally, the model was trained for 70 epochs, with each image used only once in a complete scope of the dataset, to further prevent overfitting. The training was done on a computer with 64GB of RAM, an i7 3.50GHz CPU, and a Titan X GPU card for 900,000 images, which took 8 days to complete.

In summary, the hyperparameters used in the deep learning model include a learning rate of 10, a learning rate decay with a Gamma of 0.95, a dropout of 50%, the Adam Solver type as the optimizer, and a random Gaussian initialization for weight initialization. These hyperparameters were chosen to ensure fast convergence and prevent overfitting of the model.
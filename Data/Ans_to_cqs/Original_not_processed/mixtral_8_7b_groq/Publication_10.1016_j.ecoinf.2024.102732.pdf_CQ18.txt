Answer:::

The strategy implemented to monitor model performance during training involves using various evaluation metrics throughout the training process. For regression models, as mentioned in the context from "2018 4th International Conference on Computing Communication and Automation (ICCCA)" and the study by Su et al. (2020a), the performance can be evaluated using metrics such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R-squared. These metrics help assess how well the model predictions align with the actual values.

In the case of binary classification models, like the one compared by Strandberg and Låås (2019), evaluation metrics such as accuracy, precision, recall, and F1-score can be used. These metrics provide insights into the model's ability to correctly classify positive and negative instances.

Moreover, monitoring model performance during training can also include tracking the convergence of the model's loss function. This is particularly relevant for optimization algorithms like Gradient Descent, where the goal is to minimize the loss function. By tracking the loss function's value over training iterations, one can ensure that the model is not overfitting or underfitting the data.

Additionally, for models like XGBoost, as mentioned in the context, important indices such as "Gain" and "Frequency" can be used to assess variable importance. This can provide insights into the model's decision-making process and help identify any potential issues.

However, the context does not explicitly mention a strategy for monitoring model performance during training. The strategies mentioned are more focused on model selection, parameter tuning, and performance evaluation after training. Nonetheless, the mentioned metrics and techniques can be adapted to monitor model performance during training.
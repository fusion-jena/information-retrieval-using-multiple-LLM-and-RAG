Based on the provided context, there is no explicit information about the data augmentation techniques applied in the deep learning pipeline. The context focuses mainly on the description of the network architecture, its components, and the experimental design.

However, it is common practice in deep learning to use data augmentation techniques to increase the amount and diversity of training data, which can help improve model performance and generalization. Some common data augmentation techniques include:

1. Flipping: Horizontal or vertical flipping of images to increase the dataset size and help the model generalize better.
2. Rotating: Rotating images by a certain angle to simulate different perspectives and orientations.
3. Scaling: Changing the size of images, either uniformly or non-uniformly, to help the model handle objects at different scales.
4. Translation: Shifting images horizontally or vertically to simulate changes in object position.
5. Color jittering: Changing the brightness, contrast, saturation, or hue of images to account for variations in lighting conditions and colors.
6. Gaussian noise: Adding Gaussian noise to images to make the model more robust to sensor noise and variations.

While the provided context does not mention any specific data augmentation techniques, it is possible that some of these methods were applied during the training of the models described in the paper. Nonetheless, without explicit information, it is not possible to provide a definitive answer.
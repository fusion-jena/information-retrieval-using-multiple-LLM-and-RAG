The provided context discusses the use of machine learning models, including deep learning models like the Long-short-term-memory (LSTM) model, for processing sequential data. However, it does not explicitly mention the process followed to deploy the trained deep learning model.

In general, deploying a trained deep learning model involves several steps, which may include:

1. Model serialization: This involves saving the trained model in a format that can be easily loaded and used for making predictions. Common formats for model serialization include JSON, XML, or using specific libraries like TensorFlow's SavedModel or PyTorch's TorchScript.
2. Platform selection: This involves choosing the appropriate platform for deploying the model. The platform could be a local machine, a cloud-based server, or an edge device. The choice of platform depends on factors like the volume of data, the required latency, and the available resources.
3. Model optimization: This involves optimizing the model to make it more efficient for deployment. Model optimization techniques include quantization, pruning, and distillation.
4. Integration with the application: This involves integrating the model with the application that will use it for making predictions. This may involve creating a REST API, or embedding the model in a mobile or web application.

Based on the provided context, we can infer that the LSTM model was trained using a transformed time series data, and preselection of predictors was used to reduce computation time and the risk of overfitting. However, the context does not provide information on whether the model was serialized, the platform it was deployed on, or how it was integrated with the application. Therefore, we cannot provide a specific answer to the query.
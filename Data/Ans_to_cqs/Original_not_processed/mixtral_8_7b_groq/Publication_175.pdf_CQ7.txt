Based on the provided context, there is no explicit information about the criteria used to split the data for deep learning model training (e.g., train, test, validation). However, it does mention that they trained the same DL architecture with different datasets, including the biggest dataset AT0, a more limited dataset AT1, and subsampled datasets of 250 and 500 images per class from T0. It is likely that they used a separate subset of the data for testing and validation, but the specific criteria for splitting the data are not provided.

In general, when splitting data for deep learning model training, it is common to use a train-test split, where a portion of the data is used for training the model, and the remaining portion is used for testing the model's performance. Additionally, a validation set can be used during the training process to provide an unbiased evaluation of the model's performance on the training set. This can help to prevent overfitting and ensure that the model generalizes well to new data.

The specific criteria for splitting the data can vary depending on the size and nature of the dataset, as well as the specific goals of the modeling process. For example, a larger dataset may be split into larger training, testing, and validation sets, while a smaller dataset may require a smaller split to ensure that each set contains a sufficient number of examples.

In summary, while the specific criteria used to split the data for deep learning model training are not provided in the context, it is common to use a train-test split with an optional validation set to evaluate the model's performance and prevent overfitting.
Answer:::

The provided context discusses the training of a deep learning model for predicting species assemblages, but it does not explicitly mention preprocessing steps such as normalization, scaling, or cleaning. However, it does mention that a large environmental context is collected and provided to the model in the form of 64 Ã— 64 2D tensors sampled at the kilometre-scale resolution and centered on the observation. These predictors include WorldClim2 bioclimatic variables, Soilgrids pedological variables, human footprint rasters, terrestrial ecoregions of the world, and the observation location (longitude and latitude). Examples of input are shown in Fig. S5 and the full list of predictors is given in Table S2. Therefore, it can be inferred that some preprocessing of these predictors might have been done before providing them to the model.

Additionally, the context mentions that the model is a deep neural network with a cross-entropy loss function, and that the output softmax probabilities are thresholded to get the assemblage of predicted species. This implies that the softmax function, which is a type of normalization, is used during the training of the model.

In summary, while the context does not provide specific information on preprocessing steps such as normalization, scaling, or cleaning, it can be inferred that some preprocessing of the predictors might have been done, and that the softmax normalization is used during the training of the model.
Based on the provided context, there is no explicit information about the criteria used to split the data for deep learning model training (e.g., train, test, validation). The text focuses on the application of ecological-economic suitability areas (ESAs) and optimization algorithms for landslide susceptibility prediction.

However, it is important to note that when training deep learning models, data splitting is a crucial step. Typically, the dataset is divided into three subsets: training, validation, and testing.

1. Training set: This set is used to train the model, allowing it to learn the underlying patterns in the data.
2. Validation set: This set is used during the training process to tune hyperparameters and prevent overfitting. The model's performance on the validation set is monitored, and adjustments are made accordingly.
3. Testing set: This set is used to evaluate the final model's performance. It is the unbiased judge of how well the model generalizes to new, unseen data.

The criteria for splitting the data can vary depending on the specific problem and dataset. Common approaches include:

1. Random sampling: Data points are randomly assigned to each subset. This method ensures that the subsets are representative of the entire dataset.
2. Stratified sampling: When the dataset has imbalanced classes, this method ensures that each subset has a similar distribution of classes.
3. Time-based splitting: In time-series data, the dataset can be split based on time, using the earliest data for training, the middle for validation, and the latest for testing.

In summary, while the provided context does not mention the specific criteria used for splitting data in deep learning model training, it is a crucial step in the process. Common approaches include random sampling, stratified sampling, and time-based splitting.
Answer: In the context provided, the criteria used to determine when training is complete is not explicitly mentioned. However, it is mentioned that the performance of models is evaluated using metrics such as AUC (Area Under ROC Curve), TSS (True Skill Statistics), and kappa coefficient. These metrics are used to compare the accuracy or performance of probability distribution models with an in-situ validation dataset.

In the case of species probability distribution modeling for Rhododendron arboreum, the AUC value was used to compare the BIOCLIM and CNN-based models. The BIOCLIM model had a lower AUC value of 0.639, while the CNN-based model had a higher AUC value of 0.917. A higher AUC value indicates better performance, so the CNN-based model was considered to be very good compared to the BIOCLIM model.

Although not explicitly mentioned in the context, a common criterion used to determine when training is complete in machine learning is when the validation loss plateaus or stops decreasing significantly. This means that the model's performance on the validation set has stabilized, and further training is unlikely to improve its performance. Monitoring the validation loss during training can help prevent overfitting, which occurs when the model performs well on the training set but poorly on the validation set.

In addition to monitoring the validation loss, other techniques such as early stopping, learning rate schedules, and regularization methods can be used to prevent overfitting and improve the model's generalization performance.

In summary, while the specific criterion used to determine when training is complete is not mentioned in the context, monitoring the validation loss and using techniques to prevent overfitting are common practices in machine learning. In the context provided, the AUC value was used to compare the performance of probability distribution models for Rhododendron arboreum.
The context does not provide specific information on how the hyperparameters of the models were optimized, such as whether grid search, random search, or another method was used. However, it does mention that the Adam optimizer with a learning rate of 0.001 was used for optimizing all the parameters during the training of both Model-1 and Model-2. The early stopping technique was also applied based on the model's performance on the validation set to avoid overfitting. For Model-K, the Stochastic Gradient Descent (SGD) optimizer and an MSE loss function were used. It is possible that the hyperparameters for these optimizers were determined through experimentation or following best practices, but the specific method is not mentioned in the context.
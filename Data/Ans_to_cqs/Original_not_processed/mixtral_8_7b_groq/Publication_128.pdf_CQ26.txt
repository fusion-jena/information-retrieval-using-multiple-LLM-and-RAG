The provided context does not contain specific information about techniques used to address data bias during preprocessing of the deep learning pipeline. However, it does mention the use of data preprocessing techniques such as batch normalization and the use of Leaky ReLU activation function in the model training process.

Batch normalization is a technique used to normalize the inputs of each batch, which can help to reduce internal covariate shift and improve the stability of the neural network during training. Leaky ReLU is a variation of the rectified linear unit (ReLU) activation function, which allows for a small, non-zero gradient when the input is negative. This can help to mitigate the problem of dying ReLU units, where the gradient becomes stuck at zero, and improve the training of deep neural networks.

In terms of addressing data bias, the context does mention the use of diverse data collection in paper [10] where the authors used inner-distance shape classification, which is a shape descriptor that is robust to variations in pose, scale, and density. This method can be used to increase the diversity of the data and reduce bias by capturing the intrinsic shape properties of the data.

Additionally, in paper [6] the authors propose a regularization technique to enhance diversity in feature extraction, which can help to reduce overfitting and improve the generalization of the model. This technique can also help to reduce bias by ensuring that the model is not over-reliant on specific features of the training data.

In summary, while the provided context does not explicitly mention techniques such as stratified splitting, oversampling, and undersampling for addressing data bias during preprocessing, it does mention the use of batch normalization, Leaky ReLU, diverse data collection and feature extraction regularization which can help to reduce bias and improve the performance of deep learning models.
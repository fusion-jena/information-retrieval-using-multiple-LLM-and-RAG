Based on the provided context, there is no explicit information about the preprocessing steps taken before training a deep learning model. However, it does mention the use of machine learning and SVM (Support Vector Machine) models in the study of animal behavior. In the context of SVM model training, it is mentioned that the configuration was chosen to minimize cross-validation error and ensure the majority of the original data's variance was retained. This implies that some form of data preprocessing might have been done, but the specific steps are not detailed.

In general, when preparing data for deep learning models, several common preprocessing steps are often involved:

1. **Data Cleaning**: This involves handling missing or incorrect data points. Missing values can be imputed, or rows with missing data can be removed, depending on the severity and pattern of missingness.

2. **Data Scaling/Normalization**: Deep learning models, especially those using gradient descent for optimization, can be sensitive to the scale of the input features. Therefore, it's common to scale or normalize the data so that all features have a similar range. This can be done using techniques like min-max scaling or standardization.

3. **Data Transformation**: Depending on the nature of the data and the specific deep learning model, it might be beneficial to apply certain transformations to the data. For example, categorical variables might be one-hot encoded, or data might be log-transformed to reduce skewness.

4. **Feature Selection/Extraction**: This involves selecting the most relevant features for the model or creating new features from the existing ones. This can help reduce overfitting and improve model performance.

Given the focus of the provided context on SVM models and cross-validation error, it's likely that some form of feature selection or dimensionality reduction was performed. However, without explicit information, it's not possible to provide a definitive answer.
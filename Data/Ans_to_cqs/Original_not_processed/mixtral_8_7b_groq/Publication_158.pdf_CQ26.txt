Based on the provided context, there is no explicit mention of techniques such as stratified splitting, oversampling, undersampling, or diverse data collection being used to address data bias during the preprocessing of the deep learning pipeline. However, it is mentioned that data augmentation is applied to enhance sample variation, which includes rotation, reflectance bias of ±10%, and random noise of ±10% reflectance for 15% of the input image. This technique helps to increase the size of the training set and provides some degree of data augmentation, but it does not directly address data bias.

Additionally, it is mentioned that an ensemble of three models was generated to reduce variability in the results caused by weight initialization and random selection for batch training. This approach can help to improve the robustness of the model, but it does not directly address data bias either.

It is also worth noting that the alternative approach of using a random forest classifier was considered, which is an efficient algorithm that can be easily applied in a distributed processing framework. However, the text does not mention any techniques used to address data bias in this context.

In summary, while data augmentation and ensemble methods were used in the deep learning pipeline, there is no explicit mention of techniques such as stratified splitting, oversampling, undersampling, or diverse data collection being used to address data bias during the preprocessing of the deep learning pipeline.
Based on the provided context, there is no explicit mention of specific regularization methods such as dropout or L2 regularization that were used to prevent overfitting in the deep learning pipeline. However, it is mentioned that during training, square patches are randomly cropped from the images and flipped horizontally and vertically with a 50% probability. This can be considered as a form of data augmentation, which is a regularization technique used to increase the amount of training data, improve model performance, and prevent overfitting.

In addition, during the validation phase, images are divided into regularly-spaced, overlapping patches, which are then processed individually. The final global output is reconstructed by combining the predictions from the patches using non-maximum suppression. This approach can also be seen as a form of regularization, as it helps to reduce the impact of any noise or outliers in the data.

It is also worth noting that the deep learning models used in the study were trained and evaluated on a large dataset consisting of 16 images for training and eight images for testing. Having a large and diverse dataset can also help to prevent overfitting and improve the generalization ability of the model.

Therefore, while the context does not explicitly mention specific regularization methods such as dropout or L2 regularization, it does suggest that data augmentation, patch-based processing, and a large and diverse dataset were used to prevent overfitting in the deep learning pipeline.
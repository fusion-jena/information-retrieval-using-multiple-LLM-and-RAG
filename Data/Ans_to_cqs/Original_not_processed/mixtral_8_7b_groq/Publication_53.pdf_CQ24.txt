Answer:::

The text provided does not give specific details about the strategies used to handle randomness in the deep learning pipeline. However, it is mentioned that Python and the fastai library were used for developing the deep learning CNN, which implies that some measures to handle randomness were likely taken.

In general, deep learning models are sensitive to randomness due to various sources such as the initialization of weights, the order of training samples, and the selection of mini-batches. To address these issues, several strategies can be employed:

1. Initialization of weights: Deep learning models typically initialize the weights randomly. To ensure reproducibility, a fixed random seed value can be set. This ensures that the same set of random weights is used every time the model is trained.
2. Order of training samples: The order of training samples can affect the model's performance. To avoid this, the data can be shuffled before each epoch.
3. Selection of mini-batches: Deep learning models often use mini-batch gradient descent, where the weights are updated based on a subset of the training data. To ensure that the model is not biased towards any particular subset, the mini-batches can be selected randomly.
4. Regularization: Regularization techniques, such as dropout and weight decay, can help reduce overfitting and improve the model's generalization performance. These techniques introduce randomness by dropping out neurons or adding noise to the weights.
5. Ensemble methods: Ensemble methods, such as bagging and boosting, can help reduce the variance of the model by combining multiple models trained on different subsets of the data. These methods introduce randomness by using different subsets of the data and different initializations of the weights.

In summary, while the text provided does not give specific details about the strategies used to handle randomness in the deep learning pipeline, several strategies can be employed to address randomness in deep learning models, such as initializing the weights with a fixed random seed value, shuffling the data, selecting mini-batches randomly, using regularization techniques, and employing ensemble methods.
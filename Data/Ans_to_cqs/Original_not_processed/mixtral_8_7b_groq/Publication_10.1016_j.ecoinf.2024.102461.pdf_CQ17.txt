The provided context does not directly discuss the regularization methods used to prevent overfitting in deep learning pipelines. However, it does mention the use of deep learning in the creation of fire risk maps. Nevertheless, I will provide an answer based on general knowledge.

In deep learning, overfitting is a common issue where the model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization methods are employed to mitigate overfitting by adding a penalty term to the loss function, which discourages the learning of overly complex models.

Two common regularization techniques used in deep learning pipelines are:

1. L2 Regularization (Weight Decay): This method adds a penalty term proportional to the square of the model's weights to the loss function. This encourages smaller weights, reducing the impact of individual features and helping to prevent overfitting.

2. Dropout: During training, dropout randomly sets a fraction of the hidden units to zero at each update. This prevents the neural network from relying too much on specific connections, promoting the learning of more robust features and reducing overfitting.

These techniques can be applied together or separately, depending on the specific deep learning pipeline and the dataset being used. It is important to note that the optimal choice of regularization method(s) depends on the specific problem and dataset, and it may require experimentation to determine the best approach.
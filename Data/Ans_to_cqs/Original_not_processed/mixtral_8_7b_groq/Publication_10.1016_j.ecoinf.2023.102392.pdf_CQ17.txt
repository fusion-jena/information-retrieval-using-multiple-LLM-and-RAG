The provided context does not include specific information about the regularization methods used to prevent overfitting in the deep learning pipeline for the fire risk prediction model. However, it is mentioned that the study aims to enhance the use of deep learning models in overlay categorization utilizing images collected by unmanned aerial vehicles (UAVs) and SPOT satellite equipment with flexibility.

In general, deep learning models are prone to overfitting due to their complexity and the large number of parameters. Regularization methods are commonly used to prevent overfitting by adding a penalty term to the loss function, which discourages the model from learning overly complex patterns in the training data that may not generalize well to new data.

Two commonly used regularization methods in deep learning are dropout and L2 regularization. Dropout is a regularization technique that randomly sets a fraction of the hidden units in a layer to zero during training, which helps to prevent over-reliance on any single unit and encourages the model to learn more robust features. L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to learn smaller weights, which can help to reduce overfitting by preventing the model from learning overly complex patterns in the training data.

While the provided context does not mention the specific regularization methods used in the fire risk prediction model, it is likely that one or both of these techniques were used to prevent overfitting. However, without further information, this is only a speculation.
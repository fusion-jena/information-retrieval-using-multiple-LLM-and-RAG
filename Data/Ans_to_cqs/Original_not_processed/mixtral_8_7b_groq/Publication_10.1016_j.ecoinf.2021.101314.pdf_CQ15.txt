The provided context does not give information about the specific deep learning pipeline used or the optimization techniques applied in it, such as Stochastic Gradient Descent (SGD) or Adam. Therefore, I cannot provide an accurate answer to this query.

However, I can provide some general information about optimization techniques commonly used in deep learning.

Optimization techniques are crucial in deep learning to minimize the loss function and improve the model's performance. These techniques are used to update the model's parameters iteratively based on the gradient of the loss function.

Some of the most commonly used optimization techniques in deep learning are:

1. Stochastic Gradient Descent (SGD): It is a simple optimization algorithm that updates the model's parameters by computing the gradient of the loss function for a single training example at a time.
2. Mini-batch Gradient Descent: It is a variant of SGD that computes the gradient of the loss function for a small batch of training examples instead of a single example.
3. Momentum: It is an optimization technique that accumulates the gradient's momentum from previous iterations to speed up the learning process.
4. Adagrad: It is an optimization algorithm that adapts the learning rate for each parameter based on the historical gradient information.
5. Adadelta: It is an extension of Adagrad that uses a running average of squared gradients instead of the historical gradient information.
6. Adam: It is an optimization algorithm that combines the advantages of Momentum and Adagrad by using a running average of both the gradient and the squared gradient.

These optimization techniques can be applied in various deep learning pipelines, depending on the specific problem and the model's architecture. However, the provided context does not contain enough information to determine which optimization techniques were used in the deep learning pipeline in question.
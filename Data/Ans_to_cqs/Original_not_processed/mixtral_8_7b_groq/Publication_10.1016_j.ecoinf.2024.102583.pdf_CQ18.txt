Answer:

The strategy implemented to monitor the model performance during training is not explicitly mentioned in the provided context. However, it is mentioned that XGBoost, one of the models used in recent research for landslide susceptibility modeling (LSM), has built-in cross-validation support. This feature can be used to monitor the model's performance during training. Cross-validation involves dividing the dataset into k subsets or folds, where k-1 folds are used for training, and the remaining fold is used for testing. This process is repeated k times, and the average performance is calculated. This approach helps in evaluating the model's performance and tuning the hyperparameters, thereby preventing overfitting.

Additionally, the use of heterogeneous ensemble techniques, as mentioned in the context, can also aid in monitoring the model's performance during training. Heterogeneous ensemble learning combines predictions of multiple models that are different in terms of their underlying algorithms or architectures. This approach can address biases and improve the effectiveness of the models. By monitoring the performance of individual models in the ensemble, it is possible to identify any potential issues and make necessary adjustments during the training process.

It is also worth noting that XGBoost, LightGBM, and AdaBoost models have a regularization term in their loss function, as shown in equation (9) in the context. The regularization term, represented as 1/2 Î»||w||^2, helps in preventing overfitting by penalizing the complexity of the model. By monitoring the value of the regularization term during training, it is possible to ensure that the model is not becoming too complex and is generalizing well to unseen data.

In conclusion, while the specific strategy implemented to monitor the model performance during training is not mentioned in the provided context, there are several approaches that can be used, such as built-in cross-validation support, heterogeneous ensemble learning, and regularization techniques. These methods can help in evaluating the model's performance, tuning the hyperparameters, and preventing overfitting during the training process.
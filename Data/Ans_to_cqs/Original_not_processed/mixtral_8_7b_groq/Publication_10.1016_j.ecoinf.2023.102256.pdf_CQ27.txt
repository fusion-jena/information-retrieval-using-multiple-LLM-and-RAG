The text provided does not give detailed information about the process of deploying the trained deep learning model. However, it does mention that the proposed methodology can be adapted to other CNN architectures, which suggests that the model can be used in different contexts after training.

Based on common practices in deep learning model deployment, we can infer that the following steps were likely taken:

1. Model serialization: After training the model, the next step is to serialize the model so that it can be stored and loaded later. Model serialization involves saving the model's architecture, weights, and other relevant information to a file. This file can then be loaded into memory when it is time to make predictions.
2. Platform selection: Once the model is serialized, it can be deployed to a platform for prediction. The text does not specify which platform was used, but common options include local machines, cloud servers, and edge devices. The choice of platform depends on factors such as the volume of predictions needed, the available resources, and the latency requirements.
3. Integration with application: After deploying the model, it needs to be integrated with the application that will use it for prediction. This involves writing code to load the serialized model, prepare input data, and call the model's prediction method.

It is worth noting that the text provides some information about how the model was trained. Specifically, it mentions that the model was trained using a multi-branch CNN architecture and that the training was done using a small number of examples. It also mentions that the model was trained on spectrogram inputs and produced probabilities for each of the 22 species. However, the text does not provide information about how the model was evaluated or how its performance was measured.
Answer:::

The provided context discusses the use of ResNet-50 in image background classification and the comparison of its performance with an alternative method using softmax. However, it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline.

Based on general knowledge and common practices in deep learning, dropout and L2 regularization are two commonly used regularization methods to prevent overfitting.

Dropout is a regularization technique that randomly drops out a proportion of neurons during training, which helps to prevent over-reliance on any single neuron and encourages the model to learn more robust features.

L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to learn smaller weights, which can help to prevent overfitting by reducing the complexity of the model.

Therefore, it is reasonable to assume that these regularization methods were used in the deep learning pipeline described in the context, although it is not explicitly stated.
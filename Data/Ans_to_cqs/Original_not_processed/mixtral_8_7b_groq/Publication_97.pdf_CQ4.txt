Answer:::

In the context provided, the data augmentation techniques applied in the deep learning pipeline consist of random changes of RGB channels and random vertical and horizontal flips. The random changes for each RGB channel did not exceed 0.1 by absolute value and were applied simultaneously to all channels. This technique is implemented in the utility function "apply\_channel\_shift" from the Keras package. Random flips provided additional variability of images used for training and reduced overfitting. Although small random rotations were also considered, they did not improve the network's performance.

Answer:::

References:

- Yu, L.C.; Sung, W.K. Understanding geometry of encoder-decoder CNNs. arXiv 2019, arXiv:1901.07647v2.
- Ioﬀe, S.; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate
shift. arXiv 2015, arXiv:1502.03167v3.
- Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. Dropout: A simple way to prevent
neural networks from overﬁtting. J. Mach. Learn. Res. 2014, 15, 1929–1958.
- Evaluation of the CNN Design Choices Performance on ImageNet-2012. Available online: <https://github.com/ducha-aiki/caﬀenet-benchmark> (accessed on 24 March 2020).
- Abadi, M.; Agarwal, A.; Barham, P.; Brevdo, E.; Chen, Z.; Citro, C.; Corrado, G.S.; Davis, A.; Dean, J.;
Devin, M.; et al. TensorFlow: Large-scale machine learning on heterogeneous systems. arXiv 2016,
arXiv:1603.04467v2.
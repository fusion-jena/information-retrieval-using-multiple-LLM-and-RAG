Based on the provided context, it is not explicitly stated which measures were taken to ensure the generalizability of the deep learning model. However, some information can be inferred that might suggest some techniques were used.

Firstly, the authors mention that the model achieved F1 scores above 0.991 on all the training data and 0.934 on all validation data. This indicates that the model was trained and validated on different datasets, which is a common practice to prevent overfitting and improve generalizability.

Secondly, the authors report the F1 scores as well as the number of actual nests, true positives, false positives, and false negatives for each individual orthomosaic. This level of detail suggests that the data was split into individual orthomosaics, and the model was tested on each one of them. This approach is similar to cross-validation, where the model is trained and tested on different subsets of the data to ensure that it can generalize well to new, unseen data.

Finally, the authors mention that the model generalized exceptionally well over time at all locations for the testing data, and it obtained nearly perfect F1 scores at locations it had never seen before. This result indicates that the model was tested on a diverse set of data, including data from different locations and time periods, which is a good practice to ensure that the model can generalize well to new data.

In summary, while the context does not explicitly state which measures were taken to ensure the generalizability of the deep learning model, it suggests that the data was split into different subsets for training, validation, and testing, and that the model was tested on a diverse set of data from different locations and time periods. These practices are commonly used to ensure that deep learning models can generalize well to new, unseen data.
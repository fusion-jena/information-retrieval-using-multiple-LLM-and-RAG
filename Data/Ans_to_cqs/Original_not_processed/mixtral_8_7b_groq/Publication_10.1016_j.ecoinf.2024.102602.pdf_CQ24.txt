Answer:::

The provided context discusses the use of deep learning in monitoring invasive alien plants, but it does not explicitly mention strategies employed to handle randomness in the deep learning pipeline. However, it is possible to infer some relevant information regarding this topic.

In deep learning, randomness can be introduced in various stages of the pipeline, such as the initialization of weights, data shuffling, and dropout regularization. Handling randomness is crucial for ensuring the reproducibility of results, avoiding overfitting, and improving the generalization of models.

One common way to handle randomness is by setting a fixed random seed value. This ensures that the same sequence of random numbers is generated during different runs of the algorithm, making the training process reproducible. Although the context does not explicitly mention the use of a fixed random seed value, it is a standard practice in deep learning and could be employed in the studies discussed.

Moreover, the context highlights the use of data from online digital images and other forms of online data, such as textual information, to improve the generalization of deep learning tools for invasive alien species detection. Data shuffling is another way to introduce randomness in the deep learning pipeline, which can help improve the model's performance by ensuring that the training data is well-mixed and not biased towards specific patterns.

Dropout regularization is another strategy to handle randomness and avoid overfitting in deep learning models. During training, dropout randomly sets a fraction of the input units to zero, effectively preventing the co-adaptation of feature detectors. Although the context does not explicitly mention dropout regularization, it is a widely used technique in deep learning and could be employed in the studies discussed.

In summary, while the provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, it is possible to infer that techniques such as setting a fixed random seed value, data shuffling, and dropout regularization could be used in the studies discussed. These strategies are crucial for ensuring the reproducibility of results, avoiding overfitting, and improving the generalization of deep learning models.
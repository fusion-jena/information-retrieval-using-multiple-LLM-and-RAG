Based on the provided context, the hyperparameters of the model are optimized using the ADAM optimization algorithm with a learning rate of 0.0001. There is no mention of methods such as grid search or random search for hyperparameter tuning. The ADAM optimization algorithm is a popular method for optimizing neural network weights, and it adaptively adjusts the learning rate during training. This is done to minimize the cross entropy loss between the network's predictions and the pre-labeled data.

In addition to the ADAM optimizer, the model uses other techniques to improve performance and prevent overfitting. Dropout with a rate of 0.3 is used during training to make the model more stable. Dropout randomly sets a fraction rate of input units to 0 at each update during training time, which helps prevent over-dependence on any one input unit and thus improves generalization.

The model is trained for 150 epochs with a batch size of 128. Cross validation is performed by splitting the analyzed area into 10 sub-areas, which vary in terrain type. This helps validate the generalization behavior of the model to areas with slightly different characteristics. A padding of 27 pixels is added as a frame around the area used as input to counteract the size reduction that occurs within the CNN. This process creates 41,601 smaller segments that are handled independently by the model.

Overall, while the context does not explicitly mention grid search or random search for hyperparameter tuning, the model uses a combination of techniques including ADAM optimization, dropout, and cross validation to optimize hyperparameters and improve performance.
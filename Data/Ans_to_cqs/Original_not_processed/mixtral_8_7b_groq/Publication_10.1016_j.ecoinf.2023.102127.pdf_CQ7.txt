Answer:::

The criteria used to split the data for deep learning model training, such as creating separate sets for training, testing, and validation, can depend on various factors. However, the provided context suggests that in the specific case of species distribution modeling (SDM) using the Ibis.iSDM package, the standard practice of withholding parts of the training data for validating a model might not be the most appropriate approach. This is due to the fact that both training and testing data may suffer from the same spatial and environmental biases.

In SDM, if prior knowledge of the biology of a species is integrated into a model through a prior or offset, thus "nudging" or constraining response functions towards a more sensible outcome, the use of any (biased) withheld data would likely indicate a reduced predictive performance. Therefore, alternative methods for evaluating model performance may be more suitable.

Furthermore, the context highlights that not always are there multiple point occurrence datasets available for a given species. In many cases, expert information on habitat preferences or a broad delineation of a species range can also provide contextual information about a species. This expert knowledge can be integrated into the model as another type of integration, such as adding expert-delineated ranges or elevational limits as covariates to model objects.

Therefore, when it comes to splitting data for deep learning model training in SDM using the Ibis.iSDM package, it is important to consider the potential biases in the data and the availability of alternative sources of information, such as expert knowledge. Instead of the standard train-test-validation split, alternative methods such as cross-validation or using a separate dataset for external validation may be more appropriate. However, the specific criteria used for splitting the data would depend on the particular research question and the characteristics of the data and study system.
Answer:::

The hyperparameters used in the deep learning model include the optimization algorithm or optimizer, learning rate, and dropout layer. The optimizer used in the model is RMSProp, Adam, or SGD. The learning rate used in the model is 0.001 or 0.01. The dropout layer is either not used or used in the model.

The CNN model was trained using epochs and batch sizes of 10 and 32, respectively. The model was compiled using a categorical cross-entropy loss function, and the hyperparameters used in the CNN algorithm were tuned using a root mean square propagation (RMSProp) optimizer. The learning rate used in the RMSProp optimizer is 0.001.

In addition to RMSProp, the model also used the Adam and SGD optimizers. The learning rate used in the Adam optimizer is 0.001 or 0.01, while the learning rate used in the SGD optimizer is 0.01.

The model also used a dropout layer to prevent overfitting. The dropout layer was either not used or used in the model. When used, the dropout rate was 0.5.

The performance of the model was evaluated using accuracy, precision, recall, F1-Score, area under the curve (AUC) score, confusion matrix, and model training time for binary and multiclass classifications. The standard deviation values for each performance indicator were compared to determine the optimal configuration. Configuration 8 was determined as the ideal parameter, with a reduced standard deviation value. The feature extraction parameters that yielded the best performance were employed during the feature extraction phase in the optimization process of the CNN deep learning model.

In summary, the hyperparameters used in the deep learning model include the optimization algorithm or optimizer (RMSProp, Adam, or SGD), learning rate (0.001 or 0.01), and dropout layer (with or without). The RMSProp optimizer was used in the training process, and the learning rate used in the RMSProp optimizer is 0.001. The dropout layer was used in some configurations to prevent overfitting.
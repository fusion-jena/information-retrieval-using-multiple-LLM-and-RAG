Answer:::

Based on the provided context, the regularization methods used to prevent overfitting in the deep learning pipeline are L2 regularization and kernel regularization.

L2 regularization, also known as weight decay, is a method that adds a penalty to the loss function, which is proportional to the square of the magnitude of the model's weights. This penalty discourages large weight values, which can help to reduce overfitting. In the given context, the Regularization\_coefficient hyperparameter controls the strength of the L2 regularization.

Kernel regularization is another regularization method that is used in the context of convolutional neural networks (CNNs). This method adds a penalty to the loss function that is proportional to the square of the magnitude of the CNN's kernel weights. Like L2 regularization, kernel regularization discourages large weight values, which can help to prevent overfitting. The kernel\_parameter hyperparameter controls the strength of the kernel regularization.

It is worth noting that the context does not mention the use of dropout, which is another popular regularization method for deep learning models. Dropout is a technique that randomly sets a fraction of a model's activations to zero during training, which helps to prevent overfitting by reducing the co-adaptation of the model's weights.

In summary, the regularization methods used to prevent overfitting in the deep learning pipeline, as described in the given context, are L2 regularization and kernel regularization. These methods add a penalty to the loss function that discourages large weight values, which can help to reduce overfitting.
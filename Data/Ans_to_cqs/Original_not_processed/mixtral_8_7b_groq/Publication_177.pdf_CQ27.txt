Answer:::

The process followed to deploy the trained deep learning models is not explicitly mentioned in the provided context. However, based on common practices in deep learning model deployment, we can infer some steps that might have been taken.

Firstly, the models would have been serialized, i.e., the model architecture, weights, and other necessary parameters would have been saved to disk after training. This is usually done to avoid retraining the models from scratch, which can be time-consuming and computationally expensive. In Python, popular deep learning libraries like TensorFlow and PyTorch provide built-in functions for model serialization.

Secondly, the platform for deployment would have been selected based on the requirements and constraints of the application. For instance, if the application requires real-time inference, then a high-performance GPU server might be chosen. On the other hand, if the application is resource-constrained, then a lightweight edge device like a Raspberry Pi might be more suitable. The context mentions that the models were tested on 20% of the original dataset, which suggests that the models were deployed locally for testing.

Thirdly, the deployed models would have been integrated with the application's software stack. This could involve writing custom code to load the serialized models, preprocess input data, perform inference, and postprocess output data. The context mentions that the models were evaluated based on performance metrics like sensitivity, specificity, precision, and F1-score, which suggests that these metrics were computed as part of the deployment process.

Finally, the deployed models would have been monitored and maintained to ensure that they continue to perform as expected. This could involve tracking metrics like inference latency and accuracy, checking for hardware and software compatibility issues, and updating the models as new data becomes available.

In summary, while the exact process followed to deploy the trained deep learning models is not provided in the context, we can infer that the models were likely serialized, deployed on a suitable platform, integrated with the application's software stack, and monitored for performance and maintenance.
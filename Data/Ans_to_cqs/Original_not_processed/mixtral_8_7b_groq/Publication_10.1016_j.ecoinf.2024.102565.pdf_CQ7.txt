Answer:::

In the context provided, a K-fold cross-validation technique was utilized for splitting and validating the data during the training phase of the machine learning algorithms. Specifically, a four-fold cross-validation approach was implemented. This method involves dividing the initial dataset into four mutually exclusive folds through stratified sampling. Stratified sampling ensures that each fold maintains a similar distribution of data as the original dataset.

During the training process, each fold takes turns serving as the validation set, while the remaining three folds constitute the training set. The training set undergoes random under-sampling (RUS) to address any imbalances, while the validation set remains unchanged. The model is then trained using the balanced three-fold training dataset for a specified number of epochs and tested on the validation fold to generate predictions for that particular fold. This process iterates four times, using different folds as the validation set in each iteration, resulting in four distinct prediction arrays. These arrays are subsequently consolidated to provide predictions for the entire dataset.

It is important to note that this K-fold cross-validation technique integrates both on-site and remote sensing (RS) datasets comprehensively into both the training and validation phases. This approach maximizes the utilization of the available data, enhancing the robustness of the model training process.

In summary, the criteria used to split the data for deep learning model training, as described in the provided context, involve a four-fold cross-validation technique. This method entails dividing the dataset into four mutually exclusive folds, training the model on three folds while using the fourth fold as the validation set, and repeating this process for all four folds. This approach allows for more comprehensive use of the available data and improved model training robustness.
Answer:::

The provided context does not directly discuss strategies employed to handle randomness in the deep learning pipeline. However, it does mention the use of deep learning approaches, specifically convolutional neural networks (CNNs), in the creation of high-resolution CHMs of the Earth. 

In the context of deep learning, randomness can be handled in several ways:

1. **Random Initialization**: The weights and biases of the neural network are initialized with random values at the start of training. This helps to break symmetry and ensures that different neurons learn different features.

2. **Random Data Augmentation**: This involves creating new training examples by applying random transformations (like rotation, scaling, flipping, etc.) to the existing data. This increases the size of the training set and helps the model generalize better.

3. **Random Sampling**: In mini-batch gradient descent, a random subset of the training data is used for each update of the model weights. This helps to avoid local minima and provides a better estimate of the gradient.

4. **Dropout**: This is a regularization technique where randomly selected neurons are ignored during training. This helps to prevent overfitting and encourages the network to learn more robust features.

5. **Use of a Fixed Random Seed**: Using a fixed random seed ensures that the same random numbers are generated every time the model is trained. This makes the training process reproducible.

While the context does not explicitly mention these strategies, they are commonly used in deep learning to handle randomness.
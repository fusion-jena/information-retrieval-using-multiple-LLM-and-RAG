The pieces of context provided do not give specific information about the hardware resources used for training the deep learning models. However, it can be inferred that the models were likely trained using powerful computational resources such as Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs), given the complexity of deep learning models and the large datasets used in the studies.

For instance, Navares and Aznarte (2020) used a deep learning Long Short-Term Memory (LSTM) model to predict air quality, which requires significant computational power. Similarly, Chianese et al. (2019) used a multi-layer perceptron model for predicting ambient particulate matter concentration, which also requires substantial computational resources.

Moreover, the studies by Neckel et al. (2023) and Ogen (2020) involved processing large satellite images and datasets, which would have been computationally intensive. Therefore, it is highly likely that these studies also used powerful computational resources such as GPUs or TPUs for training their deep learning models.

However, the specific hardware resources used for training the models are not mentioned in the provided context.
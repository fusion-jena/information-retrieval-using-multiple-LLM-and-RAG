Based on the information provided, there is no explicit mention of the specific hardware resources used for training the deep learning model. The text focuses primarily on the details of the experiments, the datasets used, and the methodology for transfer learning.

However, it does mention several papers and tools that are used in the process, which can give us a general idea about the possible hardware resources used. For instance, the Caffe deep learning framework is mentioned, which is a popular tool for designing and training deep learning models. Caffe supports both CPU and GPU computation, but GPU is typically used for training deep learning models due to its superior computational power.

Additionally, the paper "Delving deep into rectifiers: Surpassing human-level performance on Imagenet classification" by He et al., which is cited in the text, mentions that they used two NVIDIA K40 GPUs for training their deep residual network. Therefore, it is possible that a similar setup was used in this study.

In summary, while there is no explicit mention of the hardware resources used for training the deep learning model, it is likely that GPUs were used due to their popularity in deep learning training. The specific type of GPU used is not mentioned in the text.
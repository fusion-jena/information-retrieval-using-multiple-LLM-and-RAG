Answer:::

The process followed to deploy the trained deep learning model involved several steps, including model serialization and platform selection. First, the model was trained using the cnn_learner function in the fast.ai library, which is a high-level library for deep learning in Python. The model architecture used was Resnet18, but a Resnet50 was used for the full results presented in the main text. The training was done for 2 epochs, but 20 epochs were used for the full results. The training process was performed on a Mac with a 2.4Ghz processor and 64Go memory, and it took approximately 1 minute per epoch. After each epoch, the model was saved for later use.

Once the model was trained, it was serialized for deployment. Model serialization is the process of saving the model architecture and weights to a file, so that it can be loaded and used for making predictions on new data. The serialized model can be used in different platforms, including CPU and GPU machines.

In this case, the serialized model was deployed on a GPU machine (Titan Xp nvidia) with 16Go of RAM. The model was loaded using the fast.ai library and used to make predictions on new data. The platform selection was based on the computational requirements of the model and the available resources.

In summary, the process followed to deploy the trained deep learning model involved training the model using the cnn_learner function in the fast.ai library, serializing the model using the same library, and deploying the serialized model on a GPU machine with 16Go of RAM. The training process was performed on a Mac with a 2.4Ghz processor and 64Go memory, and it took approximately 1 minute per epoch. The model was saved after each epoch for later use.
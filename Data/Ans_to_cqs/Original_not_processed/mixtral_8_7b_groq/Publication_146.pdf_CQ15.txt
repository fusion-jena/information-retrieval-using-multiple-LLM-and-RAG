Answer:::

In the deep learning pipeline, the optimization technique applied is the Adam optimizer. The choice of the Adam optimizer is based on the findings of several studies that proved its usefulness on relatively small datasets, which is the case in this study. The Adam optimizer was chosen over the static stochastic gradient descent (SGD) due to the higher model accuracies usually yielded by adaptive learning rate algorithms.

The Adam optimizer is a popular choice in neural network applications, and it is an adaptive learning rate optimization algorithm that combines the benefits of two other optimization algorithms: AdaGrad and RMSProp. The Adam optimizer adjusts the learning rate for each parameter separately, taking into account the historical gradient information of that parameter. This allows for a more precise and efficient optimization process compared to the static SGD, which uses the same learning rate for all parameters throughout the training process.

The hyperparameter setting for the Adam optimizer was completed based on studies that successfully implemented Yolo in remote sensing cases. The author of the Yolo-based "Yolt" model suggests implementing the same hyperparameters as the Yolo model. The default parameters of YoloV3, which are 0.001, 0.9, and 0.0005 for the learning rate, momentum, and weight decay, respectively, were used in this study.

In summary, the Adam optimizer is used in the deep learning pipeline of this study due to its proven usefulness on small datasets and its adaptive learning rate algorithm. The hyperparameters for the Adam optimizer were set based on successful implementations of Yolo in remote sensing cases.
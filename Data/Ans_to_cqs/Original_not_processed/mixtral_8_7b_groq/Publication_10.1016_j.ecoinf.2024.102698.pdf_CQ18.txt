Based on the provided context, there is no explicit information given about the strategy implemented to monitor the model performance during training. However, it is mentioned that machine learning and neural network-based models are used for fitting models to a wide range of data sets. These models typically have various methods for monitoring performance during training, such as validation and testing data sets, loss functions, and accuracy metrics.

For instance, in the context of machine learning, a common approach is to split the available data into training, validation, and testing sets. The model is trained on the training set, and its performance is evaluated on the validation set during training. This process helps to assess how well the model is generalizing to unseen data and prevents overfitting. Once the training is complete, the model's performance is evaluated on the testing set, which provides an unbiased estimate of the model's performance on new data.

In addition, loss functions are often used to monitor the model's performance during training. A loss function measures the difference between the model's predictions and the actual values. By minimizing the loss function, the model learns to make better predictions. Monitoring the loss function can provide insights into how well the model is learning and whether it is improving over time.

Accuracy metrics, such as precision, recall, and F1 score, are also commonly used to evaluate the model's performance during training. These metrics provide a more nuanced understanding of the model's performance than the loss function alone, as they take into account factors such as false positives and false negatives.

Therefore, while the specific strategy for monitoring the model's performance during training is not provided in the context, it is reasonable to assume that standard machine learning practices, such as using validation and testing data sets, loss functions, and accuracy metrics, are being employed.
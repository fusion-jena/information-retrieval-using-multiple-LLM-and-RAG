The provided context does not include information about the regularization methods used to prevent overfitting in deep learning pipelines. However, I can provide some general information about common regularization techniques in deep learning.

Regularization methods are used to prevent overfitting by adding a penalty term to the loss function during training. This penalty term discourages the model from learning overly complex patterns in the training data that may not generalize well to unseen data. Two common regularization techniques in deep learning are L2 regularization and dropout.

L2 regularization, also known as weight decay, adds a penalty term proportional to the sum of the squares of all model weights. This encourages the model to learn smaller weight values, reducing the impact of individual features on the model's output. L2 regularization can be applied to all model weights or only to certain layers, depending on the specific implementation.

Dropout is a regularization technique that randomly sets a fraction of the model's outputs to zero during training. This prevents the model from relying too heavily on any single neuron or feature, encouraging the model to learn more robust and generalizable features. Dropout can be applied to any layer in the model, but is most commonly used in fully connected layers.

These regularization techniques can be used in combination with other techniques, such as early stopping and data augmentation, to further reduce the risk of overfitting in deep learning models.

In summary, while the provided context does not include information about the specific regularization methods used in the deep learning pipeline for the simulations, L2 regularization and dropout are two common regularization techniques used in deep learning to prevent overfitting.
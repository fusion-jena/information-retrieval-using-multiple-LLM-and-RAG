The provided context discusses the use of deep learning for counting wildebeest in aerial images, but it does not explicitly mention strategies employed to handle randomness in the deep learning pipeline. However, it is possible that such strategies were used during the development of the deep learning method.

One common way to handle randomness in deep learning is by setting a random seed value. This ensures that the initial weights of the model and other random processes, such as data shuffling, are reproducible across different runs of the model. This can be useful for debugging and comparing the performance of different models.

In the context of the study, the deep learning method was developed using pretrained weights from the COCO dataset provided by Redmon & Farhadi (2018). The authors then passed the tiled survey images through a version of the YOLO DCNN using these pretrained weights. It is possible that they set a random seed value during the training of the YOLO DCNN to ensure reproducibility.

Furthermore, the authors mention that they created a training dataset by selecting 500 survey images at random and tiling them into subimages. While the randomness in the selection of the images may not directly affect the deep learning pipeline, it could introduce variability in the training data, which the deep learning method would need to handle.

Overall, while the provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, it is possible that such strategies were used during the development of the deep learning method. Strategies such as setting a random seed value and handling variability in the training data are common ways to handle randomness in deep learning.
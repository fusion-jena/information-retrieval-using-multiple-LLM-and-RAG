Answer: The provided context does not give specific information about the data augmentation techniques used in the deep learning pipelines for Unet, Pspnet, and Deeplabv3+ models. However, data augmentation techniques like flipping, rotating, and scaling are commonly used in deep learning to increase the size of the training dataset and improve model performance.

For instance, in the TernausNet model, which is based on the Unet architecture, the authors used elastic deformations as a data augmentation technique to improve the model's ability to generalize. Elastic deformations involve applying a random warping to the input image, which helps the model learn to handle variations in object shape and position.

In general, data augmentation techniques like flipping, rotating, and scaling can be easily implemented in deep learning frameworks like TensorFlow and PyTorch. These techniques involve applying random transformations to the input images, such as horizontal or vertical flipping, rotating the image by a certain angle, or scaling the image by a random factor. These transformed images are then added to the training dataset, allowing the model to learn to handle variations in object appearance and position.

Therefore, while the provided context does not give specific information about the data augmentation techniques used in the deep learning pipelines for Unet, Pspnet, and Deeplabv3+ models, it is reasonable to assume that data augmentation techniques like flipping, rotating, and scaling were used to improve model performance.
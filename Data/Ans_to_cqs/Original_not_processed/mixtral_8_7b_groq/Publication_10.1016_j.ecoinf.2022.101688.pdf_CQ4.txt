Answer:

The provided context does not give specific information about the data augmentation techniques applied in the deep learning pipeline. However, it does mention that they compared two main approaches for transfer learning: freezing the feature extractor and fine-tuning the output layer, and fine-tuning both the feature extractor and the output layer. 

In addition, it explains how they determined the fixed input size (segments) for the CNNs by studying the vocalisations within the presence class for each dataset. They chose an input size of 4 seconds for the Hainan gibbon calls, for example, to ensure that the smallest call would fit within the segment. 

However, it does not mention any data augmentation techniques such as flipping, rotating, or scaling which are common data augmentation techniques used in deep learning pipelines to increase the amount of training data and improve model performance. Flipping involves horizontally flipping images, rotating involves rotating images by a certain degree, and scaling involves changing the size of images. 

Therefore, I cannot provide information on the data augmentation techniques applied in this specific deep learning pipeline.
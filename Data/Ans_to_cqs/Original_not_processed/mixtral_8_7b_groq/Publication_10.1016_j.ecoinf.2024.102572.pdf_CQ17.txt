The provided context discusses the use of machine learning techniques for land use and land cover data analysis, specifically for fire-damaged monuments and quantifying carbon emissions from forest fires. However, it does not mention any regularization methods used to prevent overfitting in a deep learning pipeline.

Regularization methods are essential in deep learning to prevent overfitting, which occurs when a model learns the training data too well, including its noise, and performs poorly on unseen data. Common regularization methods include L1 and L2 regularization, dropout, and data augmentation.

L1 and L2 regularization techniques add a penalty term to the loss function to reduce the magnitude of the model's weights. L1 regularization, also known as Lasso regularization, adds the absolute value of the weights as a penalty, encouraging sparse weight matrices. L2 regularization, or weight decay, adds the squared magnitude of the weights as a penalty, preventing large weights.

Dropout is another regularization technique that randomly sets a fraction of the model's outputs to zero during training, effectively creating an ensemble of models. This helps prevent overfitting by reducing the co-adaptation of feature detectors.

Data augmentation is a regularization technique that generates new training samples by applying random transformations to the existing data, such as rotation, scaling, and flipping. This increases the size and diversity of the training set, reducing overfitting.

In summary, the provided context does not mention any regularization methods used to prevent overfitting in the deep learning pipeline. Common regularization techniques include L1 and L2 regularization, dropout, and data augmentation. These methods help prevent overfitting by adding a penalty term to the loss function, randomly setting outputs to zero, or generating new training samples.
Answer:::

The deep learning models used in the pipeline mentioned in the context are Convolutional Neural Networks (CNNs). Specifically, the models mentioned are ResNet152-V2, VGG16, VGGish, YAMNet, and BirdNet.

ResNet152-V2 and VGG16 are CNNs that were pre-trained on ImageNet, a dataset on the visual modality. On the other hand, VGGish and YAMNet are CNNs that were pre-trained on AudioSet, a dataset from the same target modality (audio) but a different domain (YouTube sound clips). Lastly, BirdNet was trained on data from the target modality (audio) and a related domain (bird vocalisations from focal recordings, also annotated at species level).

The models were chosen for their performance on different tasks. ResNet152-V2 and VGG16 were chosen for their performance on image classification tasks, while VGGish, YAMNet, and BirdNet were chosen for their performance on audio classification tasks.

The models were evaluated based on their performance on different layers. For VGG16, the last three layers before the final classification layer were evaluated. For ResNet152-V2, only the last embedding layer was evaluated. For VGGish, only the last layer was evaluated, while for YAMNet, the penultimate layer was evaluated. For BirdNet, the last three embedding layers were evaluated.

In summary, the deep learning models used in the pipeline are CNNs that were pre-trained on different datasets. The models were chosen for their performance on different tasks and were evaluated based on their performance on different layers.
The provided context discusses regularization techniques used in Species Distribution Models (SDMs) to prevent overfitting and improve model parsimony, rather than deep learning pipelines. However, I will explain the relevant regularization methods mentioned in the context.

Answer:::
The regularization methods used to prevent overfitting in the SDM algorithms mentioned in the provided context are:

1. Elastic-net regularization (GLM-EN): This method is used in the Generalized Linear Model (GLM) in Step B of the two-step "embedded" covariate selection procedure. Elastic-net regularization is a combination of L1 and L2 regularization, where L1 regularization (Lasso) encourages sparsity by setting coefficients to zero, and L2 regularization (Ridge) prevents overfitting by shrinking coefficients towards zero. Elastic-net regularization allows for a balance between L1 and L2 penalties, controlled by a mixing parameter.

2. Null-space penalization (GAM-NP): This method is used in the Generalized Additive Model (GAM) in Step B of the two-step "embedded" covariate selection procedure. Null-space penalization is a regularization technique that penalizes the null space of the design matrix, which corresponds to the space spanned by the columns associated with zero coefficients. This method helps to improve model parsimony and prevent overfitting.

3. Guided regularized random forest (RF-GR): This method is used in Step B of the two-step "embedded" covariate selection procedure. Guided regularized random forest is a regularized version of the random forest algorithm that incorporates a regularization term in the objective function. This term encourages the selection of fewer variables, thus improving model parsimony and preventing overfitting.

These regularization techniques are used in SDM algorithms to prevent overfitting and improve model performance. However, they differ from the regularization methods commonly used in deep learning pipelines, such as dropout and L2 regularization. Dropout is a regularization technique specific to neural networks, where randomly selected neurons are dropped during training to prevent overfitting. L2 regularization, also known as weight decay, is a regularization technique that adds a penalty term to the loss function, which discourages large weights and helps prevent overfitting.
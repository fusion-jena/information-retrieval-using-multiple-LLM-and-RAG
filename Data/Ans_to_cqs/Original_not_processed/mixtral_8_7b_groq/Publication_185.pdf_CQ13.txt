Answer:::

The hyperparameters used in the deep learning models mentioned in the context include the learning rate, optimizer, batch size, momentum, L2 weight decay, and dropout ratio. Specifically, for the experiments conducted on a small-size dataset without data augmentation, the learning rate was set to 10-2 for all layers initially. The optimizer used is stochastic gradient descent, and the backpropagation algorithm included in Caffe was used for training. The learning rate was decreased by a factor of 10 every 69 iterations, and training was stopped after 6900 iterations. The batch size was set to 69, and momentum was set to 0.9. L2 weight decay was applied with a penalty multiplier set to 5 Ã— 10-4, and a dropout ratio of 0.5 was used. The number of units in the third fully connected layer (fc3) was changed according to the number of classes of training data. The experiments were conducted in CPU mode.

It is worth noting that the specific hyperparameters used may vary depending on the architecture of the deep learning model and the dataset being used. Properly tuning these hyperparameters can significantly impact the performance of the model. In the context provided, the MG and MO models both achieved good training results and performed well in their validation sets, demonstrating the feasibility of using a small dataset to train deep learning models as long as the parameters are chosen properly.

Answer:::
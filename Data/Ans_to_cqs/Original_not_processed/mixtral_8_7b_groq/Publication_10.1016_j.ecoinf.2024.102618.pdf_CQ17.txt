Answer:::
The provided context discusses the performance metrics used to evaluate deep learning models, but it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, based on general deep learning practices, I can provide some common regularization techniques.

Regularization is an essential technique in deep learning to prevent overfitting, which occurs when a model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization methods work by adding a penalty term to the loss function, which discourages the learning of overly complex models.

Two common regularization techniques are:

1. L2 regularization (Weight decay):
L2 regularization adds a penalty term proportional to the square of the model's weights to the loss function. This term encourages smaller weights, reducing the impact of individual features and preventing overfitting.

2. Dropout:
Dropout is a regularization technique that randomly drops a specified fraction of neurons during training. By doing so, the model cannot rely on any single neuron or a small set of neurons, which helps to reduce overfitting and improves the model's generalization ability.

These techniques are widely used in deep learning pipelines, and the choice of regularization method depends on the specific problem and dataset. It is possible that the authors used one or both of these techniques in their deep learning models, but the context does not provide enough information to confirm this.
Answer:
The provided context discusses two regularization methods used to prevent overfitting in deep learning models: dropout and data augmentation.

Dropout is used in the classification model. It is implemented as a dropout layer with a dropout ratio of 0.5 in the adapted vgg16 network. Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent overfitting by reducing the co-adaptation of neurons (Srivastava et al., 2014).

Data augmentation is another regularization technique mentioned in the context. It is used during the training of the classification model. Various data augmentation techniques such as height and width shift, flipping, zooming, and brightness changes are applied to the input images. Data augmentation effectively increases the size of the training set and helps the model generalize better by creating new variations of the input data (Perez and Wang, 2017).

However, the context does not explicitly mention L2 regularization, which is another common method to prevent overfitting. L2 regularization adds a penalty term to the loss function, which shrinks the weights towards zero and helps to reduce overfitting (Ng, 2004).

In summary, the regularization methods used in the deep learning pipeline in the provided context are dropout and data augmentation. Dropout is implemented as a dropout layer in the classification model, and data augmentation is applied during the training of the classification model. L2 regularization, although a common method to prevent overfitting, is not mentioned in the context.

References:

- Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
- Liu, M.-Y., Tang, X., Osman, A., & Wang, X. (2018). Image inpainting for irregular holes using partial convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7605-7613).
- Ng, A. Y. (2004). Feature selection, l1 vs l2 regularization, and rotational invariance. In NIPS 2004 workshop on mathematical foundations of learning and computation (pp. 1-5).
- Perez, L., & Wang, Y. (2017). The importance of data augmentation in training deep learning models. arXiv preprint arXiv:1712.04621.
- Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1), 1929-1958.
- Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.
Based on the provided context, there is no explicit information about the measures taken to ensure the generalizability of the deep learning model, such as using a diverse dataset, cross-validation, or stratified splitting. However, there are some implicit indications that might suggest some efforts towards ensuring the model's generalizability.

First, the study used four different neural network architectures (Inception-V3, MobileNet-V2, ResNet-18, and DenseNet-121) for the animal classification task. This could imply that the researchers might have explored the generalizability of their model by comparing different architectures.

Second, the researchers considered the size of each model as an important factor since the models were intended for deployment on IoT edge devices. This consideration might suggest that the researchers were aware of the need for compact models that can generalize well to different edge devices.

Third, the researchers mentioned that the data was highly unbalanced and suggested using SMOTE or similar data balancing techniques to improve the results. Balancing the dataset might have helped to increase the model's generalizability by reducing the impact of the class imbalance on the model's performance.

However, the provided context does not explicitly mention any common techniques for improving the generalizability of deep learning models, such as:

1. Diverse dataset: Collecting and using a diverse dataset with various animal species, backgrounds, lighting conditions, and angles.
2. Cross-validation: Splitting the dataset into multiple folds, training the model on one fold, and testing it on another fold, repeating this process for all folds, and averaging the results.
3. Stratified splitting: Ensuring that the train and test sets have a similar distribution of classes when splitting the dataset.

Therefore, it is not possible to provide a definitive answer based on the given context.
Based on the provided context, there is no explicit mention of the specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adam used in the deep learning pipeline. The context focuses on the development of a fully automatic system for fish biomass estimation using a deep learning network, but it does not delve into the specifics of the training process.

However, it is important to note that the choice of optimization techniques can significantly impact the training of deep learning models. Commonly used optimization techniques include SGD, Adam, and RMSprop, among others. These methods are used to update the model's parameters during training, aiming to minimize the loss function and improve the model's performance.

In the context of fish biomass estimation, the deep learning network could have utilized any of these optimization techniques or a combination of them. The authors mention that they used a deep learning network for high-quality target detection, which suggests that they might have employed techniques like transfer learning or data augmentation to enhance the model's performance. These techniques can indirectly influence the optimization process by providing better initial weights or increasing the diversity of the training data.

In conclusion, while the provided context does not explicitly mention the optimization techniques used in the deep learning pipeline for fish biomass estimation, it is still essential to consider the role of optimization techniques in deep learning model training. The authors might have used SGD, Adam, or other optimization techniques, but this information is not available in the given context.
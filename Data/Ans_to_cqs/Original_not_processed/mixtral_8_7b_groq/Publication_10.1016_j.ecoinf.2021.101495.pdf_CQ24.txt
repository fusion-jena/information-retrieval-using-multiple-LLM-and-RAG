The provided context discusses the use of deep learning for fish species classification, but it does not explicitly mention strategies employed to handle randomness in the deep learning pipeline. However, it does mention some techniques that can contribute to managing randomness.

Firstly, the use of a residual neural network (ResNet50V2) model, which is a type of deep learning architecture, is mentioned. ResNet models are known for their ability to skip layers, which can help mitigate the vanishing gradient problem, a common issue in deep learning that can be influenced by randomness.

Secondly, the context mentions that two different approaches were used for training the neural networks: manual tuning and data augmentation. Manual tuning involves adjusting hyperparameters manually, which can help reduce the impact of randomness during the training process. Data augmentation, on the other hand, involves creating new training samples by applying random transformations to the existing data. This can help increase the size and diversity of the training set, reducing overfitting and making the model more robust to randomness.

Thirdly, the context mentions that two different methods were used for segmenting the data: manually and automatically. Manual segmentation can help reduce the impact of randomness by ensuring that the data is divided into meaningful segments. Automatic segmentation, on the other hand, can help increase the size of the training set and reduce the impact of human bias.

Lastly, the context mentions that the models were trained on a specific computing infrastructure, an Nvidia Tesla V100 (16GB) GPU. Using a consistent computing infrastructure can help reduce the impact of randomness by ensuring that the training process is repeatable and comparable across different experiments.

In summary, while the provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, it does mention techniques that can contribute to managing randomness, such as using a residual neural network architecture, manual tuning, data augmentation, manual and automatic segmentation, and a consistent computing infrastructure.
Answer:::
The specific hyperparameters used in the deep learning models mentioned in the provided texts are not explicitly stated. However, some of the papers discuss the use of certain techniques and tools that can be used to determine or optimize these hyperparameters.

For instance, Shorten and Khoshgoftaar (2019) discuss data augmentation techniques for deep learning, which can help improve model performance when the sample size is limited. Dufourq et al. (2022), Nanni et al. (2020), Nolasco et al. (2023), Padovese et al. (2023) are cited in the text as examples of new tools to enhance data or combine disparate datasets, which can help in determining the hyperparameters.

Stowell (2022) mentions the use of convolutional neural networks (CNNs) for computational bioacoustics with deep learning, which typically have hyperparameters such as learning rate, number of layers, batch size, and optimizer. The choice of optimizer can significantly affect the training of deep learning models. Common optimizers include stochastic gradient descent (SGD), Adam, and RMSprop.

Li et al. (2020a) used a CNN-based model for beaked whale species classification using echolocation signals. While they do not explicitly mention the hyperparameters used, they do mention using a validation set to tune the model's hyperparameters.

Frainer et al. (2023) and Frasier et al. (2017) are cited as examples of deep learning models used for marine mammal classification. However, the specific hyperparameters used in their models are not provided in the text.

In summary, while the specific hyperparameters used in the deep learning models mentioned in the provided texts are not explicitly stated, some of the papers discuss the use of techniques and tools that can be used to determine or optimize these hyperparameters. Common hyperparameters in deep learning models include learning rate, number of layers, batch size, and optimizer. The choice of optimizer can significantly affect the training of deep learning models.
Based on the provided context, the specific regularization methods used to prevent overfitting in the deep learning pipeline are not explicitly mentioned. However, it is stated that the model is regularized, and the k-fold cross-validation technique is used to avoid overfitting.

Regularization is a technique to prevent overfitting by adding a penalty term to the loss function, which discourages the learning of overly complex models. The most common regularization methods in deep learning are L1 and L2 regularization, dropout, and early stopping.

L1 and L2 regularization, also known as weight decay, add a penalty term proportional to the absolute value or square of the model's weights, respectively. This encourages the model to learn smaller weights, reducing the complexity of the model.

Dropout is a technique where randomly selected neurons are dropped out during training, effectively training an exponential number of different neural network architectures. This prevents overfitting by reducing the co-adaptation of neurons and improves the generalization of the model.

Early stopping is a method where training is halted as soon as the validation loss starts to increase, preventing the model from further overfitting to the training data.

Given that the model is regularized, it is likely that one or more of these regularization techniques were used. However, without explicit mention in the provided context, it is not possible to give a definitive answer.
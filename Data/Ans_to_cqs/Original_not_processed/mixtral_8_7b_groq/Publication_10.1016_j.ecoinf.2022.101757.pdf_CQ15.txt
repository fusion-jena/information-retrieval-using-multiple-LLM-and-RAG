Based on the provided context, there is no explicit mention of any specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adam used in the deep learning pipeline. The weights for the DeepLabV3+ model were obtained from an already trained implementation, so the optimization techniques used during the training of the original model are not specified in the context.

However, it is important to note that optimization techniques are crucial in deep learning pipelines as they help to minimize the loss function and improve the model's performance. Common optimization techniques include SGD, Adam, RMSprop, and Adagrad. The choice of the optimization technique depends on the specific problem, the nature of the data, and the requirements of the model.

In summary, while the context does not provide information on the optimization techniques used in the deep learning pipeline, it is an essential component of any deep learning project. The selection of the optimization technique depends on various factors, and the appropriate choice can significantly impact the model's performance.
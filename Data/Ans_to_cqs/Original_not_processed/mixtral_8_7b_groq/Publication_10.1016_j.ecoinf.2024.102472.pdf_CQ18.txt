Answer:

The strategy implemented to monitor the model performance during training involves the use of a technique called gradient boosting iteration, error back-propagation in artificial neural networks (ANNs), and validation data in the process of optimal predictor variable selection.

In gradient boosting iteration, as described in Huang et al. (2022), the model's performance is monitored through a repetition process of increasing or decreasing weights in the training datasets. This allows the model to optimize its performance by adjusting its predictions based on the gradient of the loss function.

In the context of ANNs, the performance of the model is monitored during the training phase through a process called error back-propagation. As explained in the text, initial arbitrary connection weights are assigned, and inputs are forward-fed from the input to the hidden layer. The hidden neurons multiply inputs by weights, sum products, and process sums through a transfer function. Results are then propagated to the output layer, with output values compared to expected values for error computation. The iterative error back-propagation adjusts connection weights until reaching a target minimal error. This process allows for the monitoring of the model's performance and the adjustment of weights to minimize the error.

Furthermore, in the process of optimal predictor variable selection, the model's performance is monitored using validation data. As mentioned, during this process, the network is trained using a portion of the data, and the performance is then evaluated using another portion of the data, known as validation data. This allows for the monitoring of the model's ability to generalize to new data, ensuring that the model's performance is not overfitting to the training data.

In summary, the strategy implemented to monitor the model performance during training involves the use of gradient boosting iteration, error back-propagation in ANNs, and validation data in the process of optimal predictor variable selection. These techniques allow for the monitoring of the model's performance and the adjustment of weights to minimize error and ensure the model's ability to generalize to new data.
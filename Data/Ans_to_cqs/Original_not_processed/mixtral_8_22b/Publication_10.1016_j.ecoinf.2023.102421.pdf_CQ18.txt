Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The training dataset, including 268 sample plots with 39 features in a 
tabulated format, was used as input during the training process (opti-
mizing  hyper-parameters  and  fitting  models  with  these  data).  The 
training data set was divided into two parts, in which 80% was used for 
training/validation, and the remaining 20% was kept as unseen data for 
testing. To eliminate bias, 10-fold cross-validation was used during the 
training  process,  and  mean  RMSE  was  used  as  the  objective  function 
(lost function). Table 4 shows RMSE, MAE, and R2  using different op-
timizers. In addition, we tested the differences between the RMSEs in 
Table 4 using the Wilcoxon Signed-Rank test with paired samples. The 
differences between (XGBoost-BO vs. XGBoost-TDO, XGBoost-BOHB vs. 
XGBoost-TDO) or (LightGBM-BO vs. LightGBM-TDO, LightGBM-BOHB 
vs.  LightGBM-TDO)  are  significant.  However,  the  performance  of 
XGBoost-TDO  versus  LightGBM-TDO  appears  to  be  the  same (Fig.  3),

2.4. Hyperparameter optimisation 

Hyperparameter  optimization  searches  for  the  best  set  of  hyper-
parameters  for  a  machine  learning  model  that  can  perform  best  on  a 
given  task.  In  many  packages,  random  or  grid  search  is  the  default 
method  for  selecting  optimal  hyperparameters  (Snoek  et  al.,  2012). 
There are many studies on the selections of optimization algorithms for 
theoretical  problems  or  engineering  applications,  in  which  various 
versions  of  Bayesian  optimizers  and  Swarm-based  algorithms  were 
compared (Stenger et al., 2019; Stenger and Dirk, 2022).

information  gain  estimation  and  tree  growth.  This  algorithm  is 
controlled by several parameter groups, including (1) Boosting param-
eters such as Max_depth, Learning_rate, and gamma, and (2) Learning 
task  parameters  such  as  loss  function  type,  evaluation  metric,  and 
number of iterations. As a result, the model becomes more complex as 
the tree grows, but more loss is reduced than level-wise, and the algo-
rithm learns faster. 

EcologicalInformatics79(2024)1024214Q.-T. Bui et al.

Parameter to control 

minimum loss reduction 
Amount of weight required 
for a tree to produce a 
child 

Parameter to control the 

step size at each iteration 

Maximum-depth trees can 

grow 

number of boosting 

iterations 

Minimal number of data in 

one leaf.  

‘reg_alpha’: (0.0, 0.1), 

‘lambda_l1’: (0.0, 0.1), 

‘reg_lambda’: (0.0, 0.1), 

‘lambda_l2’: (0.0, 0.1), 

‘num_leaves’: (25, 5000), 

“gamma”: (0,10)  

‘min_child_weight’: 
(1,10), 

‘min_child_samples’: (50, 
10,000), 

Learning_rate: (0, 0.1) 

Learning_rate: (0, 0.1) 

‘max_depth’: (5, 50), 

‘max_depth’: (5, 50), 

‘num_boost_round’: 
(100,2000), 

‘num_boost_round’: (100, 
2000), 
‘min_data_in_leaf’: (100, 
2000), 

Random seed for bagging 

“seed”: (0,200),   

performance comparisons. 

3. Results and discussions 

3.1. AGB estimation

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To monitor the model performance during training, the authors employed a 10-fold cross-validation strategy. They divided their training dataset into two parts, where 80% was utilized for training and validation purposes, while the remaining 20% was reserved as unseen data for testing. During this process, they aimed to minimize any potential biases. Additionally, they used mean Root Mean Square Error (RMSE) as the objective function (loss function) to evaluate the performance of the models.
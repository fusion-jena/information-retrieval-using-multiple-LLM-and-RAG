Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Other  ML  algorithms,  in  general,  and  deep  learning  algorithms,in 
particular,  can  be  applied  to  hyperspectral  data  to  predict  the  water 
quality parameters with better accuracy and performance using orbital 
hyperspectral data. The number and spatial distribution of the samples, 
and influence of different atmospheric correction algorithms (Vanhel-
lemont and Ruddick, 2021) are among essential parameters that should 
be noticed in  all future studies  to create  a representative dataset and 
make  a  ML-based  model  with  acceptable  generalisation  (Menezes  de 

EcologicalInformatics75(2023)1020589M. Saberioon et al.

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Biophysical properties 
Hyperspectral imagery 
Machine learning 
Remote sensing 
Satellite imagery 
Water quality

Maier, P.M., Keller, S., Hinz, S., 2021. Deep learning with wasi simulation data for 

estimating chlorophyll a concentration of inland water bodies. Remote Sens. 13, 718. 
https://doi.org/10.3390/rs13040718. 

Novo, E.M.L.M., Steffen, C.A., Braga, C.Z.F., 1991. Results of a laboratory experiment 
relating spectral reflectance to total suspended solids. Remote Sens. Environ. 36, 
67–72. https://doi.org/10.1016/0034-4257(91)90031-Z. 

Olmanson, L.G., Brezonik, P.L., Bauer, M.E., 2013. Airborne hyperspectral remote 

sensing to assess spatial distribution of water quality characteristics in large rivers: 
The mississippi river and its tributaries in minnesota. Remote Sens. Environ. 130, 
254–265. https://doi.org/10.1016/j.rse.2012.11.023.

5. Conclusions 

This study evaluates the ability of EnMAP simulated data coupled 
with machine learning algorithms to predict two critical water quality 
properties  for  inland  waters  (i.e.,  Chl-a  and  TSS).  To  this  end,  water 
samples were collected from several lakes and ponds in the south of the 
Czech  Republic  concurrent  to  the  hyperspectral  AISA  Eagle  airborne 
imaging campaign. EnMAP simulation data also was generated based on 
the  AISA  Eagle  airborne  data  using  EeteS.  As  demonstrated,  the 
enhanced  spectral  resolution  of  EnMAP  permitted  the  prediction  of 
biochemical properties of small inland waters with acceptable accuracy. 
Additionally, results indicate that Vis/NIR hyperspectral imaging from 
orbit combined with machine learning algorithms has the potential to be 
used as  a  rapid and accurate method  for predicting and  mapping the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, various preprocessing steps may be required depending on the nature of the input data. Some common preprocessing techniques include:

1. Normalization: This involves rescaling the input features so they fall within a specific range or have zero mean and unit variance. Common methods include Min-Max scaling, which scales values between 0 and 1, and z-score normalization, which standardizes the data to have a mean of 0 and a standard deviation of 1.

2. Scaling: Similar to normalization, scaling adjusts the magnitude of input features to prevent any single feature from dominating the others during training. Techniques such as logarithmic scaling or square root scaling can help reduce skewness in the data distribution.

3. Cleaning: Data cleaning is crucial to remove noise, outliers, missing values, or inconsistent entries in the dataset. This step ensures that only relevant information is fed into the model, improving overall performance and reducing overfitting.

4. Feature engineering: In some cases, it might be necessary to extract new features from raw data or transform existing ones to improve their representation. For example, polynomial features could be created by multiplying original features together, or principal component analysis (PCA) could be used to identify underlying patterns in high-dimensional datasets.

5. Augmentation: Data augmentation techniques like rotation, flipping, zooming, or cropping can artificially increase the size of the dataset, helping the model learn more robust representations and preventing overfitting.

6. Encoding categorical variables: When dealing with non-numeric data, encoding schemes like one-hot encoding or label encoding can convert categorical variables into numerical format suitable for deep learning models.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2023; Yuan et al., 2021; Zhao et al., 2023c). The model’s initial learning 
rate was set to 0.01, further refined via iterative testing. The number of 
iterations was 150, with a batch size of 4, downsampling multiplier set 
at 8, and utilized SGD optimizer, ReLu activation function, and Dice loss 
function.

to augment the feature map size, aligning with the encoder’s pooling 
layer.  Each  decoder  module  concatenates  its  output  with  the  corre-
sponding encoder output. The final decoder tensor undergoes convolu-
tion and then traverses through a softmax layer, normalizing the output 
probabilities for two classes at each pixel (Gao et al., 2022; He et al., 
2022;  Qiu  et  al.,  2023).  The  model’s  initial  learning  rate  was  set  to 
0.0001, subsequently fine-tuned through iterative testing. The number 
of iterations was 150, the batch size was configured as 2, utilizing the 
Adam optimizer, ReLu activation function, and Dice loss function. 

(2)  Pspnet.  To  facilitate  the  integration  of  multi-scale  data  and

(3) Deeplabv3þ. In the encoding stage of Deeplabv3+ model, the 
backbone  network  is  replaced  with  Mobilenetv2  featuring  expanded 
convolution (Fig. 5). Mobilenetv2 network ensures higher accuracy and 
enhances  the  model’s  lightweight  nature  compared  to  the  originally 
used  Xception+ network  (Fu  et  al.,  2022),  significantly  reducing  the 
model weights file size and accelerating model prediction speed. Addi-
tionally,  the  semantic  segmentation  task  involves  incorporating  ASPP 
module into the encoder-decoder network (Chen et al., 2018b). Five sets 
of feature maps are obtained by utilizing four atrous convolution blocks 
with varied dilation rates (including convolution, batch normalization, 
and activation layers) and a global average pooling block. These feature 
maps are concatenated, then passed through a 1 × 1 convolution block, 
and  ultimately  inputted  into  the  Decoder  module.  ASPP  method  in-

2.3.1. Model architectures 

(1)  Unet.  By  incorporating  an  encoder-decoder  structure  that  es-
tablishes shortcut connections, Unet model extended upon FCN’s end-to- 
end concept (Ronneberger et al., 2015). The input to the network is a 
512  × 512  pixels  image  (Fig.  3).  The  encoder  (left  dashed  square) 
downsamples the input tensor and encodes it into a vector with reduced 
dimensions. It consists of multiple convolutional blocks, each including 
a  3  × 3  convolutional  kernel  layer,  batch  normalization,  and  an 

Fig. 1. (a) Overview of Tarim River Basin. (b) Remote sensing images of the study area.  

EcologicalInformatics79(2024)1024093H. Wang et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about any specific preprocessing steps involved before training the mentioned deep learning models (Unet, Pspnet, and Deeplabv3+). However, typically, some common preprocessing steps used in deep learning include:

1. Normalization: This step scales the input features to have zero mean and unit variance or between 0 and 1. Common methods include Min-Max scaling, z-score normalization, and log transformation.

2. Data Cleaning: Removing missing values, outliers, or irrelevant data from the dataset can improve the performance of the model.

3. Image Augmentation: For computer vision tasks, techniques like rotation, flipping, zooming, cropping, and shifting can be applied to increase the diversity of the training dataset.

4. Encoding Categorical Variables: Converting categorical variables into numerical form using one-hot encoding, label encoding, or binary encoding.

5. Feature Scaling: Standardizing the range of independent variables so that they contribute proportionately during training.

6. Dimensionality Reduction: Techniques such as Principal Component Analysis (PCA) can reduce the dimensionality of the dataset while retaining most of the relevant information.
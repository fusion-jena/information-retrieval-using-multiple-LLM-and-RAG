Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Ruder, S., 2016. An overview of gradient descent optimization algorithms. arXiv preprint 

arXiv:1609.04747.  

Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.-C., 2018. Mobilenetv2: 

Inverted residuals and linear bottlenecks, pp. 4510–4520. 

Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., Vaswani, A., 2021. Bottleneck 
transformers for visual recognition. Proceedings of the IEEE/CVF Conference on 
Computer Vision and Pattern Recognition, pp. 16519–16529. 

Tan, M., Le, Q., 2019. Efficientnet: rethinking model scaling for convolutional neural 
networks. In: International Conference on Machine Learning, pp. 6105–6114. 
Tan, M., Le, Q., 2021. Efficientnetv2: smaller models and faster training. In: International 

Conference on Machine Learning, pp. 10096–10106. 

The State of World Fisheries and Aquaculture, 2022, 2022. FAO. https://doi.org/ 

10.4060/cc0461en. 

Wang, C.-Y., Bochkovskiy, A., Liao, H.-Y.M., 2022a. YOLOv7: Trainable Bag-of-Freebies

A  bottleneck  transformer  is  a  simple  but  powerful  transformer 
structure. Self-attention models in the field of vision can be classified 
into  Pure  Attention  Models  and  Convolution  Attention  Models.  The 
advantage of convolution-based models is their parameter sharing and 
efficient local information aggregation capabilities. To integrate global 
information, convolution-based models require the stacking of multiple 
convolutional layers. Self-attention, on the other hand, has the ability to 
obtain  global  information.  Therefore,  combining  self-attention  and 
convolution  can  reduce  the  network  depth  and  improve  the  model’s 
performance. BoTNet replaces the 3 × 3 spatial convolutions in ResNet’s 
last  three  bottleneck  blocks  with  multi-head  self-attention  (MHSA) 
(Srinivas et al., 2021). The memory and computational requirements for 
self-attention  are  quadratic  in  the  spatial  dimension,  and  these  re-

Chollet, F., 2017. Xception: Deep learning with depthwise separable convolutions. In: 
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 
pp. 1251–1258. 

Claudet, J., Fraschetti, S., 2010. Human-driven impacts on marine habitats: a regional 
meta-analysis in the Mediterranean Sea. Biol. Conserv. 143, 2195–2206. https://doi. 
org/10.1016/j.biocon.2010.06.004. 
ˆ
A., 2020. Ecological and Economic Importance of Benthic 

Crespo, D., Pardal, M.

Communities, pp. 1–11. https://doi.org/10.1007/978-3-319-71064-8_5-1. 
Dai, J., Li, Y., He, K., Sun, J., 2016. R-fcn: Object detection via region-based fully 

convolutional networks. Adv. Neural Inf. Proces. Syst. 29. 

Duan, Y., Li, D., Li, Z., Fu, Z., 2015. Review on visual characteristic measurement 

research of aquatic animals based on computer vision. Nongye Gongcheng Xuebao/ 
Trans. Chin. Soc. Agric. Eng. https://doi.org/10.11975/j.issn.1002- 
6819.2015.15.001.

EcologicalInformatics80(2024)1024672L. Zhang et al.                                                                                                                                                                                                                                   

animals, even with a small number of samples. 

2.1.2. Training data preparation

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To address data bias during preprocessing of the deep learning pipeline, several techniques can be employed. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into subsets or strata, ensuring each subset maintains the same distribution of classes as the original dataset. This technique helps prevent class imbalance issues when partitioning datasets into training and testing sets.

Oversampling is a method used to increase the representation of minority classes by duplicating examples from those classes. This approach aims to balance the dataset and mitigate the impact of class imbalances on model performance. Synthetic Minority Over-sampling Technique (SMOTE) is one popular oversampling algorithm that generates new synthetic instances rather than simply copying existing ones.

Undersampling, conversely, reduces the size of majority classes by randomly removing examples from them. By doing so, this technique attempts to create a more balanced dataset without introducing artificial instances. However, it may lead to loss of valuable information if not applied carefully.

Lastly, diverse data collection focuses on gathering a wide range of representative samples to ensure the dataset accurately reflects real-world scenarios. Collecting diverse data can help minimize biases introduced by limited or unrepresentative datasets.

These techniques should be chosen based on the specific characteristics and challenges present in the dataset being used for deep learning tasks.
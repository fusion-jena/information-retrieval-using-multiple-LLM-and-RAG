Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

[32] C. Szegedy, S. Ioffe, V. Vanhoucke, A.A. Alemi, Inception-v4,
inception-ResNet and the impact of residual connections on
learning, Proceedings of the Thirty-First AAAI Conference on
Artiﬁcial Intelligence, AAAI Press, San Francisco, California,
USA, 2017, pp. 4278–4284.

[33] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image
in: Proceedings of the IEEE Computer Society
recognition,
Conference on Computer Vision and Pattern Recognition, 2016,
pp. 770–778.

[34] G. Huang, Z. Liu, L. Van Der Maaten, K.Q. Weinberger,
Densely connected convolutional networks, Proceedings - 30th
IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2017, Institute of Electrical and Electronics Engineers
Inc., 2017, pp. 2261–2269.

[35] I. Vasilev, Advanced Deep Learning with Python: Design and
solutions using

implement advanced next-generation AI
TensorFlow and PyTorch, Packt Publishing, 2019.

[36] J.M. Wargo, Apache Cordova 4 Programming, Pearson

Education, 2015.

[34]. Second,

from the input image (see Fig. 2). It can be a pre-trained
CNN architecture such as Inception [32], Residual Neural Net-
work (Resnet) [33], and Dense Convolutional Network (Den-
senet)
the RPN module proposes object
locations of the feature maps. Third, a regressor and classiﬁer
are trained using the loss function L in (1) for the CNN detec-
tion network to adjust these proposed locations and to predict
single or multi-object classes with the corresponding bounding
box area in the resulted image, as shown in Fig. 2.

Lð pif g;

tif gÞ ¼

1
Ncls

X

i

Lclsðpi

i Þ þ k 1
; p(cid:3)
Nreg

X

i

i Lregðti; t(cid:3)
p(cid:3)

i Þ: ð1Þ

Although this study is limited to use only Faster R-CNN
InceptionV2 as a proposed deep learning module of our devel-
oped recognition system, it can be easily extended to include
other pre-trained deep feature extractors such as Resnet and
Densenet models. In cloud computing environment, some
types of faults can be occurred such as network fault of inter-
net connection, physical faults of hardware resources, faults in
the processing of developed software. Therefore, the fault tol-
erance is a critical requirement to keep the functionality and
expected performance of cloud computing services and mobile
applications regardless of these possible failures [47–49]. Two
reactive fault tolerance mechanisms have been applied in this
study as follows. First, the recognition task of insect pests is

A new mobile application of agricultural pests recognition

4431

[18] H. Chen, A. Chen, L. Xu, H. Xie, H. Qiao, Q. Lin, K. Cai, A
deep learning CNN architecture applied in smart near-infrared
analysis of water pollution for agricultural irrigation resources,
Agric. Water Manag. 240 (2020) 106303.

[19] X. Wu, C. Zhan, Y. Lai, M. Cheng, J. Yang, IP102: A Large-
Scale Benchmark Dataset for Insect Pest Recognition, IEEE/
CVF Conference on Computer Vision and Pattern Recognition
(CVPR) 2019 (2019) 8779–8788.

[20] W. Li, P. Chen, B. Wang, C. Xie, Automatic Localization and
Count of Agricultural Crop Pests Based on an Improved Deep
Learning Pipeline, Sci. Rep. 9 (2019) 7024.

[21] A. Gutierrez, A. Ansuategi, L. Susperregi, C. Tubı´ o, I. Rankic´ ,
L. Lenzˇ a, A Benchmarking of Learning Strategies for Pest
Detection and Identiﬁcation on Tomato Plants for Autonomous
Scouting Robots Using Internal Databases, J. Sens. 2019 (2019)
5219471.

4432

M.E. Karar et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper does not provide explicit information about the specific regularization methods used to prevent overfitting in their deep learning pipeline. However, common techniques used in deep learning architectures to address overfitting include dropout, L1 and L2 regularization, early stopping, data augmentation, and batch normalization. Dropout randomly sets a fraction rate of input units to 0 during training time, which helps reduce co-adaptation between neurons. L1 and L2 regularization add penalty terms to the loss function to discourage large weights, thus preventing overfitting. Early stopping monitors validation error while training and stops when the error starts increasing, indicating overfitting. Data augmentation generates additional training samples by applying transformations like rotation, scaling, and flipping to existing images. Lastly, batch normalization standardizes inputs within each mini-batch, reducing internal covariate shift and improving model generalization.
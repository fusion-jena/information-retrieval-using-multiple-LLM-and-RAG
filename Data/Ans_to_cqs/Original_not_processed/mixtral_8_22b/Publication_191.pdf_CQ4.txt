Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

deeper, its optimization capability can be further improved. How- 
ever, deep CNN networks require very large amounts of training 
data. Table 6 shows examples of existing well-known datasets and 
their size as quantity of images. The biggest plant database that 
we  have  found  is  the  PlantClef2015  dataset  [6]  which  has  only 
around 113,205 number of images. This is still far from matching 
the scale and variety of existing general major datasets for images 
[51,52,55] , videos [53] or languages [54] . In addition, we can see 
that the PlantClef2015 dataset [6] has one of the largest number 
of object categories but the least number of images. For example, 
compared to the ILSVRC 2010 dataset [55] , it has less than 10% of 
their total images but the same number of categories. Hence, to

Feature 

Classiﬁer 

From Deep CNN (D1) 
From Deep CNN (D1) 
From Deep CNN (D2) 
From Deep CNN (D2) 
LeafSnap [7] 
LeafSnap [7] 
HCF [8] 
HCF-ScaleRobust [8] 
Combine [8] 
SIFT [37] 

MLP 
SVM (linear) 
MLP 
SVM (linear) 
SVM (RBF) 
NN 
SVM (RBF) 
SVM (RBF) 
Sum rule (SVM (linear)) 
SVM (linear) 

Acc 

0.977 
0.981 
0.995 
0.993 
0.420 
0.589 
0.716 
0.665 
0.951 
0.588

In Section 4 , using the V1 strategy on Malayakew dataset, we 
analysed the global response of ﬁlters in each convolution layer. 
In this section, in order to gain insights into CNN, we further ex- 
plore the local responses of individual ﬁlters in each convolution 
layer. We randomly subsample some of the feature maps/channels 
in each layer and reconstruct them back to image pixels to reveal 
the structures within each patch that stimulated a particular fea- 
ture map using the DN approach [36] . We also run through all the 
training samples, and subsequently discover which portions of the 
training images caused the ﬁring of neurons. By doing this, we can 
improve our understanding of the transformation of the features 
learned in each layer and realise the characteristic of each layer in 
the CNN. Fig. 4 shows the feature visualisation of layer 1. We can 
see that some of the ﬁlters learned are similar to a set of Gabor-

2. Deep learning 

Deep learning is a class of techniques in machine learning tech- 
nology, consisting of multiple processing layers that allow repre- 
sentation  learning  of  multiple  level  data  abstraction.  The  gist  of 
DL is its capacity to create and extrapolate new features from raw 
representations of input data without having to be told explicitly 
which features to use and how to extract them. 

In the plant identiﬁcation domain, numerous studies have fo- 
cused on procedures or algorithms that maximize the use of leaf 
databases, and this always leads to a norm that leaf features are 
liable  to  change  with  different  leaf  data  and  feature  extraction 
techniques. Heretofore, we have been engaged with ambiguity sur- 
rounding the subset of features that best represent the leaf data. 
Hence, in the present study, instead of delving into the creation of

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information about specific data augmentation techniques used in the deep learning pipeline. However, based on common practices in deep learning, some popular data augmentation techniques include flipping, rotation, scaling, cropping, zooming, and color space transformations. These techniques help increase the amount of available training data by creating modified versions of the original images, thereby improving the model's ability to generalize and reducing overfitting.
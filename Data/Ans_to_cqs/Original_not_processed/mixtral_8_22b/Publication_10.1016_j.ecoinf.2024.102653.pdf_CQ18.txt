Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.6. Performance metrics 

Conventional  metrics  were  used  to  evaluate  the  model’s  perfor-
mance, such as confusion matrices, accuracy, recall, precision, F1-score, 
Cohen  Kappa,  and  the  area under  the  ROC curve,  some of  which  are 
defined below (Johnson and Khoshgoftaar, 2019).  

Table 1 
List of hyperparameters utilized for training the Recurrent Neural Network 
model.  

Hyperparameter 

Value 

Optimizer 
Learning rate 
Batch size 
Epochs 
Dropout rate 
Loss function 

ADAM (Kingma and Ba, 2017) 
0.003 
32 
100 
0.5 
Binary Cross Entropy  

●  Confusion Matrix

2.4. Model training and hyperparameters 

The  model’s  weights  were  randomly  initialized  using  a  specific 
integer seed to guarantee consistent reproducibility of results. Further-
more, meticulous manual tuning of hyperparameters was conducted to 
achieve  optimal  model  performance.  The  selected  hyperparameters, 
which yielded the best performance metrics, are presented in Table 1. 

2.5. Model validation 

Two distinct validation methodologies were implemented. The first 
method  involved  partitioning  the dataset into  training and  validation 
sets, with 70% of the data designated for training and 30% for valida-
tion.  This  division  was  performed  through  random  selection  (Dobbin 
and Simon, 2011).

This study used the train_test_split function from the scikit-learn library 
(version 1.2.2), a function that facilitates randomized shuffling and di-
vision of datasets into distinct training and testing subsets. Specifically, 
70%  of  the  data  was  designated for  training  the  classification  model, 
while the remaining 30% was set aside for testing (Dobbin and Simon, 
2011). Individuals from the control and pesticide-exposed groups were 
evenly  distributed  between  these  subsets,  guaranteeing  a  balanced 
representation of each class. The recurrent neural network (RNN) model 
was designed to incorporate three primary input parameters: the bee’s 
age, the daily number of flights, and the total time spent outside daily. 
Following  data  preparation,  labels  for  both  training  and  testing 
datasets  underwent  a  transformation  known  as  one-hot  encoding, 
facilitated  by  the  OneHotEncoder  function  from  the  scikit-learn  library

EcologicalInformatics81(2024)1026534U. Olivares-Pinto et al.                                                                                                                                                                                                                        

Fig. 4. Accuracy of the training (red) and validation (blue) datasets. A) Accuracy of models for each previously published dataset. B) Accuracy of models trained with 
1–4 data sets combined. C)Accuracy of models trained with a maximum of 5, 10, 15, 20, and 25 days of records per bee (all four datasets combined). Values at the 
bottom of the graphs indicate the final validation accuracy. (For interpretation of the references to colour in this figure legend, the reader is referred to the web 
version of this article.)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Answer: The strategy implemented to monitor the model performance during training involves partitioning the dataset into training and validation sets. In this case, 70% of the data was designated for training, while the remaining 30% was set aside for validation. This division was performed through random selection using the train_test_split function from the scikit-learn library. During the training process, the accuracy of both the training and validation datasets was monitored. The accuracy of the models for each previously published dataset was evaluated separately, as well as the accuracy of models trained with multiple datasets combined. Additionally, the accuracy of models trained with varying numbers of days of records per bee was also assessed. The values at the bottom of the graph in Figure 4 indicate the final validation accuracy.
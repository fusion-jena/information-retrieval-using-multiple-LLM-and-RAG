Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics82(2024)1026984Connectivity and coral coverZoningHeat stressWave stressDepthFilter sites hot, too subject towave damage and without enough space for coral.Weight layers according to importance for intervention of interest and decision scenario.Rank sites from most to least suitable for implementing the intervention using aggregate criteria values and their weightings.1234R. Crocker et al.                                                                                                                                                                                                                                 

convolutional  neural  networks  (CNN),  auto  encoders  and  generative 
adversarial  networks  (GAN),  and  are  better  at  learning  sophisticated 
patterns in data but can be more computationally expensive and require 
more data to train (Endres et al., 2022).

National Oceanic and Atmospheric Association, 2023. Satellites & Bleaching. Retrieved 
from NOAA satellite and information service. https://coralreefwatch.noaa.gov/pr 
oduct/5km/tutorial/crw10a_dhw_product.php. 

Nikolenko, S.I., 2021. Synthetic Data for Deep Learning. eBook. Springer. https://doi. 

org/10.1007/978-3-030-75178-4. 

Patki, N., Wedge, R., Veeramachaneni, K., 2016, October. The Synthetic Data Vault. In: 
IEEE International Conference on Data Science and Advanced Analytics (DSAA), 
pp. 399–410. https://doi.org/10.1109/DSAA.2016.49. 

Poisot, T., Gravel, D., Leroux, S., Wood, S.A., Fortin, M.-J., Baiser, B., Stouffer, D.B., 
2016. Synthetic datasets and community tools for the rapid testing of ecological 
hypotheses. Ecography 39, 402–408. https://doi.org/10.1111/ecog.01941. 

Quintana, D., 2020. A synthetic dataset primer for the behavioural science to promote 
reproducability and hypothesis generation. Elife 9, e5327. https://doi.org/10.7554/ 
eLife.5327.

5.2. Initial coral cover data 

The  initial  coral  cover  model  was  developed  using  a  TVAE  model 
from SDV, which is based on a Variational Autoencoder model. In this 
model,  an  encoder  maps  the  original  data  to  distributions  in  a  lower 
dimensional latent space. Data is then sampled from the latent space and 
transformed back to the original space using a decoder. In the learning 
phase the decoder learns by finding a transformation with minimal loss 
between the original and final data distributions. This loss is quantified 
using the evidence lower bound (ELBO), which transforms intractable 
inference  problems  into  optimisation  problems  which  can  be  solved 
using gradient methods (Xu et al., 2019). 

The model learns the spatially dependent distribution of cover for 
each of the six species of coral modelled in ADRIA. The radially sampled 
latitudes and longitudes from the synthetic site data set are then used to

Despite the drawbacks discussed, a data pipeline for producing on- 
demand  synthetic  data  packages  for  methods  testing,  validation  and 
publishing demonstrations for the reef decision support tool ADRIA is 
developed. Using the accessible and flexible synthetic data libraries SDV 
and y-data-synthetic, we demonstrate that synthetic data for a reef de-
cision support tool can be developed for decision support tool datasets 
without requiring building models from the ground up for each dataset. 
With the push for embracing big data in ecological and environmental 
modelling, synthetic data for reef decision support tools offers a means 
of fully capitalising on big data models and building a rigorous testing, 
development, and publication pipeline where data is scarce or sensitive.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't any explicit mention of the specific code repository for the deep learning pipeline in the given context. Therefore, I cannot provide the exact location of the code repository such as GitHub, GitLab, or BitBucket. However, it might be helpful to look for related publications or resources from the authors mentioned in the context, like R. Crocker et al., Nikolenko, S.I., Patki, N., etc., who may have shared their work on these platforms.
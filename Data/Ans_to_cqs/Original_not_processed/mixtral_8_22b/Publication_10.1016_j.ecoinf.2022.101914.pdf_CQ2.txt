Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.8. Training individual models 

To train the component models for the ultimate ensemble models, we 
used  six  modeling  approaches,  categorized  into  regression-based  and 
machine  learning models. Regression-based  models included General-
ized Linear Models (GLMs), Generalized Additive Models (GAMs), and 
Multivariate  Adaptive  Regression  Splines  (MARS).  Machine  learning 
models included Random Forest (RF), Artificial Neural Networks (ANN), 
and Maximum Entropy software (MaxEnt). We built all models using the 
biomod2 package in R (Thuiller et al., 2021).

To separate data into training and testing datasets, we used a spatial 
blocking technique, in which we divided our study area into 25 equally 
sized  blocks,  each  approximately  41  km  × 41  km  in  size,  using  the 
blockCV package in the R programming language (Valavi et al., 2019). 
We determined the size of our spatial blocks using the spatialAutoRange 
function in the blockCV package, which returns a block size equal to the 
median  range  of  spatial  autocorrelation  in  independent  covariates 
(Valavi  et  al.,  2019).  This  attempts  to  ensure  data  within  a  block  is 
relatively independent of data outside of that block. We then divided 
these blocks among five folds in which the numbers of presences and 
background points were roughly equal. We used five folds because, due 
to the spatial arrangement of our presence data, using more would cause 
folds to have drastically differing numbers of presences and background

points. Then, to obtain presences and background points to use during 
external evaluation, we set aside one fold. We then used the remaining 
four folds for internal cross-validation during model training.

(h) Ensemble models using an averaging approach, each time using all six component models. For the optimized ensemble model, for each component model include 
the one trained with the number of background points with the highest AUC. For the 1×, 10×, and 10 K ensemble models, include all component models built with 
the 1×, 10×, and 10 K background points, respectively. (i) Evaluate ensemble models using the evaluation dataset (using AUC and TSS here). (j) Compare ensemble 
model AUCs, TSSs, sensitivities, and specificities and choose the model that performs the best for the data. (For interpretation of the references to colour in this figure 
legend, the reader is referred to the web version of this article.)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information on the type of data format used in the deep learning pipeline. However, it mentions the use of biomod2 package in R for building models, which suggests that the input data might be in a tabular or matrix format compatible with R. The context also discusses the division of the study area into blocks, implying that spatial data could be involved. But without explicit mention of the data format, it cannot be definitively stated whether the data used in the deep learning pipeline is in the form of images, audio, video, CSV, or any other format.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Further  discussions  analyze  IFTL  and  FTL  for  their  abilities  to 
improve learning by restricting overconfidence (controlling hesitancy) 
during TL by considering GRNN and SVR as the TUR. Overconfidence in 
ELM,  GRNN,  or  SVR  arises  when  they  make  predictions  on  a  dataset 
(target  domain) that has a huge data distribution difference from the 
source domain data on which they are trained. In this scenario, they just 
use their training experience to make predictions during testing without 
considering the distribution divergence in the testing dataset from the 
training  dataset.  We  conclude  this  section  with  the  execution  time 
analysis of the approaches.  

a)  GDP prediction using only CO2 emission data2

The industrialization has been the primary cause of the economic boom in almost all countries. However, this 
happened  at  the  cost  of  the  environment,  as  industrialization  also  caused  carbon emissions  to  increase  expo-
nentially. According to the established literature, Gross Domestic Product (GDP) is related to carbon emissions 
(CO2) which could be optimally employed to precisely estimate a country’s GDP. However, the scarcity of data is 
a  significant  bottleneck  that  could  be  handled  using  transfer  learning  (TL)  which  uses  previously  learned  in-
formation to resolve new tasks, more specifically, related tasks. Notably, TL is highly vulnerable to performance 
degradation due to the deficiency of suitable information and hesitancy in decision-making. Therefore, this paper 
proposes ‘Intuitionistic Fuzzy Transfer Learning (IFTL)’, which is trained to use CO2 emission data of developed

We have found that the proposed approach efficiently captured the 
uncertainty produced by extreme variations in data distribution or the 
predictive tasks across the source and target data. It suitably restricted 
overconfidence  for  better  learning  while  transferring  the  learned 
knowledge. IFTL outperformed FTL, where the training and testing do-
mains (including labels) have huge data distribution differences. This 
validated  the  effectiveness  of  hesitancy  in  restricting  overconfidence 
during TL. IFTL is also thoroughly analyzed for its asymptotic compu-
tational complexity and execution time. The limitation of IFTL is that it 
adds  additional  computation  by  calculating  membership,  non- 
membership,  hesitancy  margin  of  each  feature,  and  the  distance 
metric for refining the output labels predicted by other ML approaches. 
This  work  has  successfully  demonstrated  the  utility  of  ecological

b)  IFTL Refinement over GRNN and SVR 

In previous results, ELM is chosen as TUR, and IFTL is implemented 
on the ELM predictions to improve its knowledge transfer. Here, GRNN 
and SVR are chosen one by one as a TUR model in the process described 
in Fig. 2. Then, IFTL with the same parameter settings (as was in the case 
of ELM in the previous subsection, where γ = 0.1) is applied over their 
prediction. The improvement of IFTL over these approaches are depicted 
in Fig. 6, where ‘GRNN_IF’  denotes the error in IFTL prediction when 
IFTL  is  implemented,  choosing  GRNN  as  TUR.  Similarly,  ‘SVR_IF’  de-
notes the IFTL error when IFTL is implemented, choosing SVR as TUR. 

EcologicalInformatics77(2023)1022069S. Kumar et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't any explicit mention of a specific strategy used to monitor the model performance during training. However, we can infer that the authors might have monitored the performance of models like Extreme Learning Machine (ELM), General Regression Neural Network (GRNN), and Support Vector Regression (SVR) through various evaluation metrics such as accuracy, precision, recall, or mean squared error. These metrics help assess how well the models perform during both training and testing phases. Additionally, the authors discuss the issue of overconfidence in these models when dealing with datasets having large data distribution differences between the source and target domains. To address this problem, they propose Intuitionistic Fuzzy Transfer Learning (IFTL). By implementing IFTL, the authors aim to capture uncertainty resulting from extreme variations in data distribution or predictive tasks across different datasets. Furthermore, IFTL helps restrict overconfidence for improved learning while transferring learned knowledge. Although not directly mentioned, monitoring the performance of IFTL would likely involve evaluating its ability to reduce overconfidence and enhance learning compared to traditional methods like Fuzzy Transfer Learning (FTL).
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

with a batch size of 32. The initial learning rate of 0.001 was
exponentially decayed every 3 epochs with a factor of 0.94.

95.17
85.16
88.33
87.52
83.73
81.91
92.74
89.01
94.23
87.89
89.81
88.96
96.02
95.95
89.97
94.34

0.95
0.87
0.89
0.88
0.84
0.82
0.93
0.89
0.95
0.89
0.89
0.90
0.96
0.96
0.91
0.95

Higher values of per-channel accuracy and F-measure indicate better
performance and vice versa.

New
Phytologist

database for training of deep learning models. Comparable stud-
ies used 1060 (Daood et al., 2016), 13 617 images (Pedersen
et al., 2017) or 19 500 images (Sevillano et al., 2020), while in
our study the CNN classiﬁer was trained with 426 876 images.

We have previously suggested that a minimum of 200 images
per species are required for accurate species identiﬁcation of phy-
toplankton (Dunker et al., 2018). By contrast, in the present
study on pollen, ﬁve out of six species with fewer than 200 pollen
grains were classiﬁed with an accuracy higher than 90%. Based
on these ﬁndings, we concluded that between 50 and 100 pollen
grains represented a reasonable amount of pollen images on a sin-
gle channel to be included in the reference library.

Deep learning

Dataset Our dataset consisted of 35 573 images for each of the
12 channels covering 35 species and resulting in a total of 426
876 images. This dataset was split into three sets for training, val-
idation, and testing in the proportions 80 : 10 : 10. All images
were centre cropped to an equally sized rectangle. In order to gain
a more robust and better generalising classiﬁer, the images in the

New Phytologist (2021) 229: 593–606
www.newphytologist.com

training set were additionally augmented in the following ways:
ﬂipped horizontal, ﬂipped vertically,
adjusted in bright-
ness (cid:3) 12.5%, adjusted in saturation (cid:3) 50.0%, adjusted by
contrast (cid:3) 12.5%, and adjusted in hue (cid:3) 20%.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The performance of the deep learning model is evaluated using two main metrics: per-channel accuracy and F-measure. Higher values of both metrics indicate better performance, while lower values suggest poorer performance. Per-channel accuracy refers to the proportion of correctly classified samples for each individual channel, while the F-measure is a measure of a test's accuracy that considers both the precision and recall of the test to compute the score. Precision measures the number of true positive results divided by the number of all positive results returned by the classifier, including those not belonging to the target category. Recall, also known as sensitivity, measures the number of true positives divided by the sum of true positives and false negatives.
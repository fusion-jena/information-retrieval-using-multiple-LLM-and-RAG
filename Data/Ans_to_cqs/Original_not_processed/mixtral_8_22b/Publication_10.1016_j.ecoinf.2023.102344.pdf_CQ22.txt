Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

the  shortcut  connections  within  the  network  that  makes  the  training 
faster; bottleneck architecture which consists of a 1 × 1 convolutional 
layer  for  dimension  reduction,  3  × 3  convolution  layer  for  feature 
extraction, and another 1 × 1 convolutional layer for dimension resto-
ration;  shortcut  connections  which  allows  gradients  to  flow  directly 
through  the  network  due  to  the  input  added  directly  to  the  output; 
global  average  pooling  (GAP)  which  computes  spatial  average  of  the 
feature  maps  at  the  end  section  of  the  convolutional  layer;  and  skip 
connections which bypass multiple residual blocks. On the other hand, 
InceptionV3 is a 48-layer 24 CNN containing 24 × 106 parameters with 
enhancement using label smoothing and factorized convolutional layers 
in order to easily propagate the information from the input to the output 
section. For all image-based pre-trained networks, the purpose of CNN

Approach 2. This is done to avoid bias in comparing their metrics af-
terwards.  All  pre-trained  models  under  Approach  2  converged  the 
training after 1000 epochs with SGDM as the optimization algorithm. 
Adaptive  Moment  Estimation  (ADAM)  was  preliminarily  used  but 
appeared  to  converge  on  a  sharp  minimum  resulting  in  a  negligible 
accuracy, and so, not further considered in this study. 

2.6. Approach 3: Traceability modelling using hybrid deep neural network

Table 1 
Metric evaluations of the developed traceability models using Approaches 1 to 3.  

Approach 

Feature 
Selection 

Model 

Training 

Testing 

Default 
Model 

Optimized Model 

1 

2 

PCA-NCA 

– 

3 

PCA-NCA 

EfficientNet-B0 
ResNet101 
MobileNetV2 
InceptionV3 
EfficientNet-B0 
ResNet101 
MobileNetV2 
InceptionV3 
LSTM-GRU- 
BiLSTM 
LSTM-BiLSTM- 
GRU 
GRU-LSTM- 
BiLSTM 
(BivalveNet) 
GRU-BiLSTM- 
LSTM 
BiLSTM-LSTM- 
GRU 
BiLSTM-GRU- 
LSTM 

Accuracy 

Accuracy 

Fall-out 

Precision 

Specificity 

Recall 

86.16% 
92.50% 
94.91% 
98.27% 
94.23% 
86.66% 
91.80% 
95.54% 

78.33% 
91.58% 
91.26% 
89.33% 
88.89% 
81.76% 
86.61% 
90.13% 

89.61% 
6.21% 
11.54% 
10.00% 
10.33% 
13.33% 
19.44% 
12.55% 

12.63% 
93.79% 
88.46% 
90.00% 
89.67% 
86.67% 
80.56% 
87.45% 

87.37% 
93.79% 
88.46% 
90.00% 
89.67% 
86.67% 
80.56% 
87.45% 

87.37% 
90.91% 
95.78% 
93.33% 
89.39% 
83.50% 
98.00% 
94.98% 

F1- 
score 

93.33% 
91.58% 
91.26% 
89.33% 
88.89% 
81.76% 
86.61% 
90.13%

means the images where the extracted numerical features were used in 
Approach 1 during training were the exact images used in training in

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The table presented in the given context provides the evaluation metrics used to assess the performance of various deep learning models. These metrics include Accuracy, Fall-out, Precision, Specificity, Recall, and F1-score. Each metric offers insights into different aspects of the model's performance.

Accuracy measures the proportion of correct predictions out of the total number of instances. It gives a general idea about how well the model performs overall. However, it may not be reliable when dealing with imbalanced datasets.

Fall-out represents the ratio of false positives to the sum of true negatives and false positives. A lower fall-out value indicates better performance.

Precision, also known as positive predictive value, calculates the fraction of relevant instances among the retrieved instances. In other words, it shows the percentage of correctly predicted positive observations out of the total predicted positives.

Specificity, or True Negative Rate, determines the proportion of actual negative cases that are correctly identified. It helps understand the model's ability to identify negative instances accurately.

Recall, or sensitivity, measures the proportion of actual positive cases that are correctly identified. It reflects the model's capacity to find all positive instances.

The F1-score combines both precision and recall into a single metric, providing a more balanced view of the model's performance. It is particularly useful when dealing with imbalanced classes.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

the Adam (Kingma and Ba, 2015) variant of stochastic gradient descent
(SGD), which adaptively scales the magnitude of the parameter updates
based on the statistics of previous updates to speed up convergence.
During training, we periodically evaluate the prediction error of the
model (i.e., the current set of parameters) on a held-out validation set
and keep the configuration 𝜽∗ with the lowest error as the final model.

4.5. Implementation details

We have implemented our models in PyTorch (Paszke et al., 2017).
We trained 𝑀 = 5 models with batch size 𝐵 = 64 and a base learning
rate 𝛼 = 10−4. The learning rate is automatically reduced by a factor
of 0.1 when the validation loss has not improved for 15 consecutive
epochs. We apply weight decay to control the strength of the unit
Gaussian prior, with an empirically chosen magnitude of 10−3 that
is inversely proportional to the hyperparameter 𝜆 from Eq. (1). We
chose 𝛽1 = 0.9, 𝛽2 = 0.999 and 𝜖 = 10−8 as hyper-parameters for the
Adam optimizer. Each neural network was trained on a single Nvidia
RTX2080Ti GPU for ∼14 days.

5. Experimental results and discussion

As commonly done in deep learning, we iteratively learn the model
parameters with stochastic gradient descend, starting from a random
initialization. In each iteration, we randomly sample a batch of 𝐵 = 64
reference data patches of size 15 × 15 pixels, where a patch is only
considered for training if the center pixel is forested. We consider a
pixel forested if and only if it contains vegetation points (points with
𝐷𝑧 > 1.3 m, see Section 3.1) and also is considered forested based on
NIBIO’s Norway-wide timber volume map (Astrup et al., 2019). We use
the latter as an additional precautionary measure to avoid unnecessary
noise from non-forested areas, as we are interested in learning forest
characteristics only. For every reference data patch, we randomly pick
an optical image from the correct year and two SAR images (one as-
cending and one descending orbit) with acquisition dates near the one
of the optical image. Using SAR with both ascending and descending

particular when trained on large data sets.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning pipeline employs the Adam variant of Stochastic Gradient Descent (SGD) as its optimization technique. This particular version of SGD, developed by Kingma and Ba in 2015, accelerates convergence by adaptively scaling the magnitude of parameter updates based on past update statistics. During the training process, the model's prediction error is frequently assessed using a separate validation dataset. The configuration yielding the least error is then selected as the final model.

In terms of implementation specifics, the models were built utilizing PyTorch. Five models were trained with a batch size of 64 and a base learning rate of 10^-4. The learning rate is programmed to decrease by a factor of 0.1 whenever there is no improvement in the validation loss over 15 successive epochs. Weight decay is used to manage the Gaussian prior's intensity, with a value of 10^-3 that is inversely related to the hyperparameter lambda from Equation (1). Hyperparameters for the Adam optimizer include beta1=0.9, beta2=0.999, and epsilon=10^-8. Training took place on a single Nvidia RTX2080Ti GPU for approximately 14 days per neural network.
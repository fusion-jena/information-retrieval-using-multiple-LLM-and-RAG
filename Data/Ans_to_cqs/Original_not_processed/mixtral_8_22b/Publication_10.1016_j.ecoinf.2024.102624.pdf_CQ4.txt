Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Staiano, 2019). Training data augmentation, which extends the existing 
training  data,  enhances  the  generalization  capabilities  (Lik´o  et  al., 
2023). Post-classification, generally segmentation, and filtering gener-
ally  provide  better  maps  with  a  lower  rate  of  salt-and-pepper  errors 
(Ahmed et al., 2017). We can also deal with the characteristics of the 
geometry of the training data, excluding outliers, refining the training 
samples, or changing the proportion of the training and test data. These 
strategies collectively improve classification accuracy, enabling better 
modelling of the environment.

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Image classification 
Spectral index 
Texture index 
DSM 
Data fusion 
UAS 
Recursive feature elimination 
Aquatic vegetation

Table 2 
The different combinations of input datasets used for the classifications (Near Infrared [NIR], Normalized Difference Vegetation Index [NDVI], Normalized Difference 
RedEdge Index [NDRE] and visible NDVI [vNDVI], Digital Surface Model [DSM], kernel [k] and offset [o] values applied during the Run Percentage calculation).  

Code 

Description 

Input data 

a 
b 
si 
t 
bsi 
rfe 

tsi 
tb 
db 
dsi 

all type 
spectral bands 
spectral indices 
texture indices 
bands and spectral indices 
variable selection with Recursive Feature Elimination of the 7 class model 
variable selection with Recursive Feature Elimination of the 13 class model 
texture index and spectral indices 
texture index and spectral bands 
DSM and spectral bands 
DSM and spectral indices

Altogether, we had 17 variables (spectral bands: green, red, red edge, 
NIR; spectral indices – NDVI, vNDVI, NDRE, DSM, and nine variations of 
the  texture  index, Run  Percentage, with three  kernel  and three  offset 
options in all combinations). We applied different input datasets using 
combinations of possible variables (Fig. 2 and Table 2). We also deter-
mined the most important variables based on the largest OA with the 
Recursive Feature Elimination (RFE) method. RFE is a variable selection 
method that can be combined with machine learning algorithms and we 
used the RF. RFE removes the variable with the lowest contribution from 
the input set to achieve the highest overall accuracy stepwise: first, the 
weakest variable is removed and continues until only one variable (with 
the  highest  contribution)  remains.  We  applied  the  RFE  in  two  steps 
(Chen et al., 2018; Rasel et al., 2021): (i) first we determined the best-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there is no explicit mention of specific data augmentation techniques such as flipping, rotation, scaling, etc., being applied in the deep learning pipeline. However, it does discuss the use of training data augmentation to enhance the generalization capabilities of the model. This could potentially include various data augmentation techniques like those mentioned above, but without further information, it cannot be confirmed.
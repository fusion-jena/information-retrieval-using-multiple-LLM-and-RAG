Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Supplementary Materials: The following are available online at http://www.mdpi.com/2072-4292/12/3/343/s1,
Figure S1: Number of results in the Google scholar academic search engine. Date of queries, 28 December 2019.
[keyword + deep learning]. Keywords: AlexNet, DenseNet, GoogLeNet, Inception, MobileNet, Resnet, Resnext,
VGG, Xception. Figure S2: Illustration of the diﬀerence between the sampling design strategies, (A) Continuous
and (B) Discrete, including examples of images (0.5 ha) from each class (Three Tree-Cover levels plus the Non-Forest
class) of the training dataset. Image data: Google, Maxar, and NWPU-RESISC45 dataset. Archive S1: Metadata of
FAO’s GDA photointerpreted very high resolution plots (71,135) with zoom 19 in Google Maps. The CVS ﬁle
contains Id, UpperLeft and Downright coordinates, Zoom, Region, Aridity level, Class (Forest/Non-forest), and
Tree cover. Archive S2: Metadata of training dataset of continuous larger sample CNN-based model with very

For this, we obtained ﬁve models by training Inception v.3 on the ﬁve created datasets. Several
studies have shown that increasing the size of the dataset using data augmentation improves the
performance of the CNN-based models [49,50]. These techniques have been proposed to reduce the
requirement of a large dataset for model training [43–45]. We conﬁgured the model parameters by
training the last two fully connected network layers in our dataset using a learning rate of 0.001 and a
decay factor of 16 every 30 epochs. As an optimization algorithm, we used RMSProp with a momentum
of 0.9 and epsilon of 0.1.

67. Neelakantan, A.; Vilnis, L.; Le, Q.V.; Sutskever, I.; Kaiser, L.; Kurach, K.; Martens, J. Adding gradient noise

improves learning for very deep networks. arXiv 2015, arXiv:1511.06807.

68. Ganguly, S.; Kalia, S.; Li, S.; Michaelis, A.; Nemani, R.R.; Saatchi, S. Very High Resolution Tree Cover Mapping
for Continental United States using Deep Convolutional Neural Networks. In Proceedings of the AGU Fall
Meeting Abstracts, New Orlean, LA, USA, 11–15 December 2017.
Suzuki, K.; Rin, U.; Maeda, Y.; Takeda, H. Forest cover classiﬁcation using geospatial multimodal data.
Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2018, 42, 1091–1096. [CrossRef]

69.

70. Marshall, M.; Thenkabail, P. Advantage of hyperspectral EO-1 Hyperion over multispectral IKONOS,
GeoEye-1, WorldView-2, Landsat ETM+, and MODIS vegetation indices in crop biomass estimation. ISPRS J.
Photogramm. 2015, 108, 205–218. [CrossRef]

Remote Sens. 2020, 12, 343

15 of 15

39.
40.

41.

Cover Classiﬁcation Using Convolutional Neural Networks. Remote Sens. 2019, 11, 274. [CrossRef]
Schmidhuber, J. Deep learning in neural networks: An overview. Neural Netw. 2015, 61, 85–117. [CrossRef]
Szegedy, C.; Vanhoucke, V.; Ioﬀe, S.; Shlens, J.; Wojna, Z. Rethinking the inception architecture for computer
vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV,
USA, 27–30 June 2016; pp. 2818–2826.
Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A.
Going Deeper with Convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), Boston, MA, USA, 7–12 June 2015.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The study does not explicitly mention any specific regularization methods used to prevent overfitting in their deep learning pipeline. However, they do use several techniques that can help mitigate overfitting. For instance, they employ data augmentation, which is known to improve the performance of convolutional neural network (CNN)-based models and reduce the requirement of a large dataset for model training. Data augmentation involves creating new training samples by applying transformations such as rotation, scaling, or flipping to existing samples, thereby artificially expanding the dataset and making it more diverse. This helps prevent the model from memorizing the training set and encourages it to learn generalizable features. Additionally, the authors use transfer learning by fine-tuning pretrained Inception v.3 models on their custom datasets. Transfer learning allows them to leverage the knowledge gained by these models during their initial training on large-scale image classification tasks, reducing the risk of overfitting when adapting them to smaller, specialized datasets. Lastly, the authors configure their model parameters using a decay factor, which gradually reduces the learning rate during training, helping the model converge towards a better solution without overshooting. While these techniques contribute to preventing overfitting indirectly, the study does not discuss direct regularization methods like dropout or L2 regularization.
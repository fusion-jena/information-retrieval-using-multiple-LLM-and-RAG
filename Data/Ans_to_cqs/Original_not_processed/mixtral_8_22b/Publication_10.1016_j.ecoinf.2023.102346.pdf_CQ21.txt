Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

To verify the model’s validity, 30% of each taxon’s data records were 
utilized as testing data, while the remaining 70% were used as training 
data.  After  10  K-fold  cross-validation  and  10-fold  bootstrapping,  all 
models  were  assessed  based  on  the  mean  values  for  Area  Under  the 
Curve AUC (Hanley and McNeil, 1982), True Skill Statistics TSS (Mon-
serud  and  Leemans,  1992),  Cohen’s  KAPPA  (Allouche  et  al.,  2006), 
point-biserial correlation COR (Elith et al., 2010), and Receiver Oper-
ator Characteristics ROC (Hanley and McNeil, 1982). Bootstrapping was 
used  to  provide  unbiased  predictive  accuracy  with  a  sufficiently  low 
variance  (Harrell  et  al.,  2005;  Lima  et  al.,  2019).  For  the  model 
threshold,  the  sensitivity-specificity  sum  maximization  technique  was 
employed  (Liu  et  al.,  2005).  According  to  this  evaluation  criterion, 
species distribution can be accurately predicted (Liu et al., 2005). For

◦

◦

QGIS Development Team, 2022. QGIS Geographic Information System. Open Source 

Geospatial Foundation. http://qgis.osgeo.org. 

Thompson, J., Johansen, R., Dunbar, J., Munsky, B., 2019. Machine learning to predict 
microbial community functions: an analysis of dissolved organic carbon from litter 
decomposition. PLoS One 14 (7), e0215502. https://doi.org/10.1371/journal. 
pone.0215502. 

Thuiller, W., Lafourcade, B., Engler, R., Araújo, M.B., 2009. BIOMOD - a platform for 
ensemble forecasting of species distributions. Ecography 32 (3), 369–373. https:// 
doi.org/10.1111/j.1600-0587.2008.05742.x. 

Topçuo˘glu, B.D., Lesniak, N.A., Ruffin, M.T., Wiens, J., Schloss, P.D., 2020. A framework 
for effective application of machine learning to microbiome-based classification 
problems. mBio 11 (3). https://doi.org/10.1128/mBio.00434-20 e00434–20.

3. Results 

3.1. Model performance 

All tested model algorithms had high sensitivity and high AUC values 
for accuracy for all selected taxa of denitrifiers (AUC = 0.95–1.00). In 
addition, RF and SVM performed very well for each taxon with AUC = 1, 
and with high sensitivity indicating correctly predicted presences, and 
high  specificity  indicating  correctly  predicted  absence.  ROC  curves 
generated after 10-fold bootstrapping indicated that all tested models 
were able to correctly predict presence and absence using the training 
data, and the average of the true skill statistics (TSS) for each model was 
near to one indicating superior performance (Table S6 and Fig. S3). 

3.2. Assessment for relative variable importance

Jiang, Y., Luo, J., Huang, D., Liu, Y., Li, D., 2022. Machine learning advances in 

microbiology: A review of methods and applications. Front. Microbiol. 13, 925454 
https://doi.org/10.3389/fmicb.2022.925454. 

Jim´enez-Valverde, A., 2012. Insights into the area under the receiver operating 

characteristic curve (AUC) as a discrimination measure in species distribution 
modelling: insights into the AUC. Glob. Ecol. Biogeogr. 21 (4), 498–507. https://doi. 
org/10.1111/j.1466-8238.2011.00683.x. 

Jones, C.M., Hallin, S., 2010. Ecological and evolutionary factors underlying global and 

local assembly of denitrifier communities. ISME J. 4 (5) https://doi.org/10.1038/ 
ismej.2009.152. Article 5.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training process, several postprocessing steps can be taken to evaluate the model's performance and interpret its results. Some common postprocessing steps include:

1. Calculating metrics: Various metrics such as accuracy, precision, recall, F1 score, and AUC-ROC can be calculated to quantify the model's performance. These metrics help determine how well the model is performing and whether it needs further tuning or optimization.

2. Generating confusion matrices: A confusion matrix provides a visual representation of the model's predictions compared to actual outcomes. It helps identify where the model is making mistakes and which classes are being confused with others.

3. Creating saliency maps: Saliency maps highlight areas of input data that contribute most significantly to the model's output. They can be useful for understanding what features the model is focusing on when making predictions.

In the given context, the authors have employed various techniques to validate their model's performance. They utilized 30% of each taxon's data records as testing data and the remaining 70% as training data. The models were then evaluated based on multiple criteria, including AUC, TSS, Cohen's Kappa, point-biserial correlation, and ROC. Additionally, they applied 10-fold cross-validation and 10-fold bootstrapping to ensure robustness and reduce variance.

The authors also mentioned that the sensitivity-specificity sum maximization technique was used for determining the model threshold. This method aims to find the optimal balance between false positives and false negatives by maximizing both sensitivity and specificity simultaneously.

However, there is no explicit mention of generating confusion matrices or creating saliency maps in the provided context. Nevertheless, these techniques could still be applicable depending on the specific requirements and objectives of the study.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1 
An example training instance for different weakly-labeled settings. Here yj indicates the targets of an instance. Red indicates an 
error in the training data (deviation from ground truth) – it is not known to the user where errors occur. Note that in the weak 
j=1yj = 1 and yj⩾0). In our setting 
label setting, only 0s may be errors. Shading represents a constraint (in this example, that 
(structurally incomplete), the constraint may not be met, but errors are neither random deviations. Mixtures of these settings 
are possible, e.g., we may have structurally incomplete with missing labels, etc. 

∑
5

EcologicalInformatics72(2022)1018492R. Beigait˙e et al.

4.2. Performance of predictive models 

The performance of compared predictive models is summarized in 
Tables 2–4. When using only complete targets for training, the MAE of 
complete  observations  of  the  test  set  is  the  lowest.  However,  experi-
mental  results  on  evaluating  the  prediction  accuracy  of  dominant 
vegetation  cover  types  show  that  using  this  approach,  the  prediction 
accuracy of incomplete observations is more than 10% less than using 
other approaches. This suggests that complete observations do not carry 
full information about the distribution of the natural vegetation cover 
worldwide.  It  confirms  existing  findings  (Nikoloski  et  al.,  2021)  that 
better performance can be achieved if incompletely labeled data is used 
in model training instead of discarding it.

1. Introduction 

Target variables are usually fully labeled in the classical supervised 
machine learning setting. In real-world predictive tasks, however, labels 
are often scarce and/or noisy. Various definitions and terms are used in 
the  literature  to  describe  variants  of  noise  and  scarceness  of  labels 
(Allison, 2001; Xie and Huang, 2018; Nikoloski et al., 2021; Sun et al., 
2010; Gao et al., 2017; Alarc´on and Destercke, 2021; Van Engelen and 
Hoos, 2020), and each setting requires tailored approaches for exploit-
ing such target labels. In this study, we formulate a new computational 
setting for regression, where target labels are structurally incomplete. We 
computationally study this task via a case study in predictive modeling 
of global vegetation cover. 

1.1. The vegetation modelling task

Zanelli, D., 2021. Predicting human activities patterns based on climate and related data. 

Master’s thesis. University of Padua. 

Zhou, Z.H., 2018. A brief introduction to weakly supervised learning. Natl. Sci. Rev. 5 

Kostopoulos, G., Karlos, S., Kotsiantis, S., Ragos, O., 2018. Semi-supervised regression: a 

(1), 44–53. 

recent review. J. Intell. Fuzz. Syst. 35 (2), 1483–1500. 

Zhu, X.J., 2005. Semi-supervised learning literature survey. University of Wisconsin- 

Madison Department of Computer Sciences. 

EcologicalInformatics72(2022)10184910

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't any explicit mention or discussion regarding the specific strategy implemented to monitor the model performance during training. Therefore, I cannot provide a direct answer to your query. However, it is common practice in machine learning to use various strategies like cross-validation, early stopping, or tracking metrics like loss or accuracy on validation sets to monitor model performance during training.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We  utilize  different  data  augmentation  techniques  during  the 
training process to prevent overfitting and to generalize better. We use 
the python library imgaug (Jung et al. (2020)) for the data augmentation 
of  our  images  and  videos.  For  Mask  R-CNN  and  FGFA  the  same 
augmentation  technique  with  identical  parameters  is  applied  to  each 
frame of the video. We use the augmentation techniques horizontal flip, 

add / subtract intensity, Guassian blur and additive Gaussian noise. All the 
mentioned augmentation techniques are used simultaneously. A further 
special augmentation function from imgaug we use, is Fog. This complex 
technique simulates fog in the video. Since it is a very strong augmen-
tation,  we  combine  it  only  with  the  random  horizontal  flip.  The  Fog 
augmentation is performed with a probability of 10%. The other com-
bined  augmentation  techniques  described  above  are  applied  with 
probability of 90%.

Stochastic Gradient Descent (SGD) is used as optimization technique. 
We start with a learning rate of 0.0005, a momentum of 0.9 and weight 
decay of 0.0005. Different settings of momentum and weight decay are 
checked but show no improvements. In the training process, we decrease 
the learning rate every 10 epochs by multiplying it with γ = 0.1. In the 
first  epoch  we  use  warmup  iterations  for  the  learning  rate  with  a 
warmup factor of 1/1000. These parameters gave the best results in our 
evaluations. We train our models for 30 epochs.

Feichtenhofer et al. (2019). This parameter combination achieved the 
best  results  in  our  experiments.  The  Slow  and  Fast  pathways  are 
implemented by a ResNet-101. 

We  split  the  annotated  dataset  AnnotationsActions  (cf.  Section  2.2) 
randomly (but fixed for all experiments and all networks) into a train set, 
validation set and test set. The test set includes 20% of the video clips of 
each class. Of the remaining clips, again 20% of the clips in each class 
form the validation set. The remaining clips are the training set. 

4.4.1. Data augmentation for action recognition

4.4. Action recognition 

The three ResNet variants are trained for 40 epochs starting with an 
initial learning rate of 0.001. The learning rate is decreased every 10 
epochs by multiplying it with a γ = 0.1. For optimization we utilize the 
Stochastic  Gradient  Descent  (SGD)  with  a  momentum  of  0.9  and  a 

EcologicalInformatics61(2021)1012158F. Schindler and V. Steinhage

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To prevent overfitting in their deep learning pipeline, the authors utilized various data augmentation techniques during the training process. They employed the Python library imgaug (Jung et al. (2020)) for image and video data augmentation. Specifically, they used horizontal flipping, adding or subtracting intensity, Gaussian blurring, and additive Gaussian noise on each frame of the video for both Mask R-CNN and FGFA. Additionally, they implemented a more complex augmentation function called 'Fog,' which simulated fog in the video. However, due to its strength, this augmentation was combined only with random horizontal flipping and applied with a probability of 10%. The other combined augmentation techniques were applied with a probability of 90%.

As for optimization techniques, Stochastic Gradient Descent (SGD) was used with a learning rate of 0.0005, a momentum of 0.9, and a weight decay of 0.0005. During the training process, the learning rate was reduced every 10 epochs by multiplying it with γ = 0.1. Warmup iterations were also used for the learning rate with a warmup factor of 1/1000 in the first epoch. Models were trained for 30 epochs.

However, there is no explicit mention of using specific regularization methods such as dropout or L2 regularization in the given context. Therefore, based on the information provided, it can be concluded that the primary method used to prevent overfitting in the deep learning pipeline was data augmentation techniques.
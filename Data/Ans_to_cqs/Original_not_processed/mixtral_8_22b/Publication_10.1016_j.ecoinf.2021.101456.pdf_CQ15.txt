Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

All models were implemented in PyTorch (Paszke et al., 2019) and 
trained for 5000 epochs using the Adam optimizer (Kingma et al., 2015) 
(cid:0) 3. The models were trained on Nvidia GTX 
with a learning rate of 10
1050 graphics processing unit with 4 GB memory with a batch size of 
256. A decision to adapt β was taken every fifth epoch to avoid random 
fluctuations. The scaling of the clustering loss, γ, in the semi-supervised 
mode was cycled between 0.01 and 0.2 every 100 epochs. 

4.4. Results 

The clustering performance on the labelled test set for the unsuper-
vised and semi-supervised instances of the dynamic β-VAE is presented 
in Table 2. 

The dynamic β-VAE performs better than the baselines in the ARI- 
and AMI-scores which quantifies the intra-class cohesion and inter-class 
separability. While HCA have been successfully used to identify groups

ℒβ(cid:0) VAE = ℒrec + βℒreg.

(3)  

Here the role of β ≥ 0 is to balance the reconstruction- and regulariza-
tion losses. Typically, lower β values yield better reconstructions but a 
less regularized latent space and less disentangled features. On the other 
hand, higher β may lead to posterior collapse, where all reconstructions 
are reduced to the average input and the KLD approaches zero. Various 
methods have been proposed to overcome this instability in achieving a 
reasonable trade-off between the loss terms. A common implementation 
is β-annealing, where β is gradually increased from a very low value up 
to a fixed point. While this solves the initial stability problems, the task 
of finding the optimal value of β remains. Recently, it has been shown 
that repeating the process with a cyclic β can lead to better performance 
(Fu et al., 2019). However, when unchecked both implementations face 
the risk of posterior collapse (vanishing KLD) once β enters a stationary

dataset  recorded  at  a  different  location  and  was  gradually  expanded 
until  reconstructions  were  sufficiently  good.  In  order  to  visualize  the 
latent  representation,  the  size  of  the  bottleneck  of  the  model  (latent 
dimension) was limited to two. Details of the network architecture are 
reported in Table 1.

2.2. VAEs and β-annealing 

Variational  autoencoders  (VAEs)  consist  of  a  regularized  probabi-
listic  encoder-decoder  pair  and  are  some  of  the  most  powerful  repre-
sentation learning methods (Bengio et al., 2013; Kingma and Welling, 
2014). They have seen broad applications in generative modelling and 
unsupervised learning tasks.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, the models were trained using the Adam optimizer (Kingma et al., 2015), which is a popular optimization technique used in deep learning pipelines. Adam combines the advantages of two other extensions of stochastic gradient descent (SGD): Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). It computes adaptive learning rates for different parameters by performing smaller updates for parameters associated with frequently occurring features and larger updates for infrequently occurring ones. This makes Adam well suited for handling sparse gradients and noisy data, often encountered in deep learning tasks.
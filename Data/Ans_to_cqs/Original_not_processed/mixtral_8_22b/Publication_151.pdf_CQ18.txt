Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

by a running average of its recent magnitude to speed
up training (Tieleman and Hinton 2012). The batch size
is ﬁxed to 32. We check the training loss every 50 epochs
and stop the training when the value of the loss function
fails to decrease in two consecutive checks. The model
with the optimal objective function value is chosen.

{

=

=

n
i

n
i

Di}
{

DR
i }

1 and

Evaluation on Simulated Data Sets
DEPP training and parameter sensitivity.—We start by
evaluating DEPP on simulated data sets, testing the
ability to train the CNN model in reasonable times. As
the training epochs advance, the loss function (1) drops
rapidly and stabilizes after around 500 epochs in a typical
case (Fig. S1 of the Supplementary material available on
Dryad). Here, training, which is a one-time process for
each reference tree, ﬁnished in around 20 min for the 200-
taxon data set and 260 minutes for 10,000-taxon data set,
on a machine with one 2080Ti NVIDIA GPU and 8 CPU
cores. Placement of 1000 queries took 4 seconds for the
200-taxon and 30 s for the 10,000-taxon data sets using a
single CPU core. On the small 200-taxon data set, EPA-ng
has an advantage in terms of running time. However, in
the larger HGT data set (10,000-taxon), DEPP placements
are faster than the alternatives with half the running time
of EPA-ng. In terms of the memory usage, APPLES+JC

DEPP is mostly robust to weighing schema, with all
four schemes tested resulting in statistically indistin-
guishable performance (Table S2 of the Supplementary
material available on Dryad). Models with more para-
meters, that is, deeper network or larger embeddding
size, tend to have better performance but also longer
training time. For example, for the 200-taxon tree, the
time for training a model with one residual block is
around 15 min while this number goes up to 20 min
when the model has ﬁve residual blocks. Reducing
the number of residual blocks from ﬁve to one or

is placed independently, and the result is compared
against the full reference tree before pruning the query
(i.e., the true tree for simulations and the ASTRAL tree
for WoL). The error metric we report is the number of

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The strategy implemented to monitor the model performance during training involves checking the training loss every 50 epochs. Training continues until the value of the loss function fails to decrease in two consecutive checks. At this point, the training stops, and the model with the optimal objective function value is selected. This approach ensures that the model does not overfit the training data and maintains good generalization performance.
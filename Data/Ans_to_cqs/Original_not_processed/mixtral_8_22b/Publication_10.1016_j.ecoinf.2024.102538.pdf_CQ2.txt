Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

their  convolutional  kernels  and  pooling  layers.  Regarding  this  issue, 
Zhang  et  al.  (2019)  incorporated  a  long  short-term  memory  (LSTM) 
network to develop a 3DCNN-LSTM model as a classifier, making the 
network more sensitive to the temporal changes in birdsong informa-
tion. It is important to note that the use of RNNs such as the CRNN model 
requires  more  computing  resources  for  training,  and  performance 
improvement is not always guaranteed. Another common approach to 
addressing  the  limitations  of  CNNs  is  to  introduce  attention  mecha-
nisms.  For  example,  Soundception  (Sevilla  and  Glotin,  2017)  was 
developed  by  introducing  time  and  time-frequency  attention  mecha-
nisms to Inception V4; the resulting model achieved first place in the 
BirdCLEF  2017  Competition.  Fu  et  al.  (2023)  proposed  an  improved 
ACGAN model named DR-ACGAN based on the residual structure and an

from data, thereby reducing the need for manual feature selection; such 
networks  have  demonstrated  considerable  potential.  The  mainstream 
deep  learning  approach  for  sound  recognition  involves  mapping  the 
sound amplitude onto a 2-D mel-scale spectrogram and using a modified 
network  architecture  adapted  from  advanced  image  recognition  for 
automatic  feature  learning.  The  effectiveness  of  convolutional  neural 
networks  with  residual  connections  in  recognizing  bird  sounds  was 
demonstrated  in  the  annual  LifeCLEF  Bird  Identification  Challenge 
(BirdCLEF) competition (Kahl et al., 2021a, 2022, 2023).

Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D., 2017. Grad- 
cam: visual explanations from deep networks via gradient-based localization. In: 
Proceedings of the IEEE International Conference on Computer Vision, pp. 618–626. 
Sevilla, A., Glotin, H., 2017. Audio bird classification with inception-v4 extended with 
time and time-frequency attention mechanisms. CLEF (Working Notes) 1866, 1–8. 
S´olymos, P., Matsuoka, S.M., Stralberg, D., Barker, N.K., Bayne, E.M., 2018. Phylogeny 

and species traits predict bird detectability. Ecography 41, 1595–1603. 

Kingma, D.P., Ba, J.. Adam: a method for stochastic optimization. https://doi.org/10.48 

Stastny, J., Munk, M., Juranek, L., 2018. Automatic bird species recognition based on 

550/arXiv.1412.6980. 

Laiolo, P., Rolando, A., 2003a. Comparative analysis of the rattle calls in corvus and 
nucifraga: the effect of body size, bill size, and phylogeny. Condor 105, 139–144.

In  the  LifeCLEF  2014  Bird  Sound  Recognition  task,  Go¨eau  et  al. 
(2014) brought the application of deep learning in bird acoustics to the 
public’s  attention  for  the  first  time,  marking  the  entry  of  bird  sound 
recognition into the DL era. In practice, convolutional neural networks 
(CNNs) have been widely used in recent studies and have shown supe-
rior performance compared to traditional methods (LeBien et al., 2020; 
Zhong et al., 2021). Following this approach, a number of classic models 
emerged. Kahl et al. (2021b) took advantage of the residual block and 
proposed the widely-known BirdNet program. Another model, Bird@-
Edge (H¨ochst et al., 2022), implemented an Edge AI system for real-time 
biodiversity  monitoring  based  on  the  EfficientNet-B3  architecture. 
However,  CNNs  have  limitations  in  capturing  long-term  continuous 
dynamic  changes  in  birdsong over  time due  to  the  size limitations  of

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The main data format used in the deep learning pipeline for sound recognition is audio. However, it is often transformed into a different format before being fed into the network. Specifically, the sound amplitude is mapped onto a 2-D mel-scale spectrogram. This transformation allows the use of advanced image recognition techniques for automatic feature learning. Therefore, while the original input is audio, the actual data format used in the deep learning pipeline is a 2-D image representation of the audio signal.
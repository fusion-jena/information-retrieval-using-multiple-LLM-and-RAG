Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

configuration of optimizer and learning rate which has been considered 
for the training of the framework. The EMViT-Net framework achieves 
prominent results as compared to other deep learning models and per-
forms better with an augmented set of EMDS-6 datasets.

for  different  deep 

While test accuracy for all the considered networks DenseNet 121, Alex 
Net, ViT, ResNet50, Xception Net, VGG19, InceptionV3 and VGG16 are 
33.77%,  35.87%,  37.65%,  41.58%,  44.76%,  48.57%,  48.88%,  and 
50.15% respectively. It has been observed that on the test set the per-
formance of every model was lower than the validation set performance, 
While the performance of the model on the test set demonstrates how 
well the model can generalize. After seeing these results, it would be 
worth  noting  that  the  proposed  transformer-based  neural  network 
EMViT-Net achieves the highest classification accuracy with a validation 
accuracy of 71.17% and test accuracy of 67.62% while requiring lesser 
training time and performing faster than the other models. 

3.5.2. Performance comparison of EMViT with other models after 
augmentation

To further validate the performance of the proposed model EMViT- 
Net,  the  acquired  findings  were  compared  with  other  state-of-the-art 
models. The performance measures used to compare the performance 
of  EMViT-Net  with  other  deep  learning  models  are  evaluated  on  the 
validation set and test set of the considered dataset EMDS-6. The com-
parison of the classification results achieved by other pre-trained deep 
learning models is given in Table 4 and Table 5 which illustrates the 
finding on each set validation set and test set respectively by calculating 
the  average  value  of  precision,  recall  and  f1-score  for  all  categories 
which are named as Avg_Precision, Avg_Recall and Avg_F1-score. The 
detailed analysis of the proposed network EMViT-Net and other deep 
learning models with specific configurations on unseen test sets is out-
lined  here  which  shows  that  the  EMViT-Net  outperforms  precision,

capable of learning weights for a generalized trained model on the un-
seen test set of EMDS-6. In model optimization learning rate was set to 
0.002 and the batch size was set to 32. The proposed model is trained for 
100 epochs with optimizer AdamW and the activation function GELU 
which  are  chosen  very  carefully  due  to  their  effectiveness  with  the 
proposed  model  for  the  considered  dataset.  Finally,  the  default  loss 
function  cross-entropy  is  used  to  calculate  the  loss  of  the  proposed 
network. By considering all the results of this section taken into account 
it was found that the proposed model EMViT-Net may be considered as 
fast and more reliable due to its low computational cost and efficiency. 
The loss and accuracy curves of the reported network are given in 
Fig.  5.  which  represents  training  and  validation  accuracy  with  their 
corresponding loss for the proposed model EMViT-Net with a specific

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the provided context, there isn't explicit information about the criteria used to split the data for deep learning model training. However, typically, the data is divided into three subsets: training, testing, and validation. The training set is used to fit the model, the validation set is used to tune hyperparameters and prevent overfitting, and the test set is used to evaluate the final model's performance. The division ratio can vary depending on the total amount of available data but commonly ranges from 60-80% for training, 10-20% for testing, and 10-20% for validation.
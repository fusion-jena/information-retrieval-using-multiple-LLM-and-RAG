Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The architecture of VGG16, as depicted in Fig. 5, consists of five sets 
of  convolutional  layers  alternating  with  five  max-pooling  layers,  fol-
lowed by two fully connected layers and an output layer. The initial two 
blocks comprise two convolutional layers each, employing respectively 
64  and  128  filters.  The  subsequent  two  blocks  encompass  three  con-
volutional  layers  with  respectively  256,  512  and  512  filters.  All  con-
volutional filters have a size of 3 × 3 and the max-pooling layers utilize a 
2 × 2 pool size with a stride of 2. During the SNN training process, we 
fine-tuned VGG16 by unfreezing the last two blocks (block 4 and block 
5), ensuring that their weights are updated in each epoch. Finally, ad-
justments were made to the final dense layer in each CNN to account for 
the specific number of classes representing beluga individuals in each 
dataset.  Fine-tuning  of  these  deep  models  was  conducted  using  Sto-

6.4. Biodiversity challenges 

In a recent article, Villon and collaborators (Villon et al., 2022) have 
delved  into  challenges that extend  beyond dataset limitations,  with a 
particular focus on biodiversity and its impact on deep learning-based 
automated  monitoring  of  marine  mammals  and  fish  habitats  through 
computer  vision.  More  specifically,  they  explored  the  implications  of 
three  fundamental  rules  of  biodiversity,  namely  the  distribution  of 
species  abundance,  species  rarity  and  ecosystem  openness.  They  also 
proposed promising solutions to these problems,  some of which  were 
previously briefly discussed in Section 2.

• No-overfitting. In this setting, the optimization algorithm of CNNs is 
Stochastic Gradient Descent (SGD), with a learning rate of 0.0001 
and a weight decay of 0.5. The batch size is set to 32, the number of 
training epochs to 200 and finally the batch-norm and dropout (0.5) 
are used to reduce the overfitting level.  

• Overfitting.  We  use  the  same  hyperparameters  setting  as  the  no- 
overfitting  but  we  remove  the  use  of  batch-norm,  weight  decay 
and dropout techniques to ensure that the model overfits.

Image Recognition. CoRR abs/1409.1556. 

Skalski, P., 2019. Make Sense. https://github.com/SkalskiP/make-sense/. 
Song, L., Shokri, R., Mittal, P., 2019. Membership inference attacks against adversarially 
robust deep learning models. In: 2019 IEEE Security and Privacy Workshops (SPW), 
pp. 50–56. https://doi.org/10.1109/SPW.2019.00021. 

Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014. Deepface: closing the gap to human- 
level performance in face verification. In: Proceedings of the IEEE Conference on 
Computer Vision and Pattern Recognition, pp. 1701–1708. 

Tayama, I., Shimdate, M., Kubuta, N., Nomura, Y., 1982. Application for optical sensor to 

fish sorting. Reito (Tokyo). Refrigeration 57, 1146–1150. 

Tuia, D., Kellenberger, B., Beery, S., Costelloe, B.R., Zuffi, S., Risse, B., Mathis, A., 
Mathis, M.W., van Langevelde, F., Burghardt, T., et al., 2022. Perspectives in 
machine learning for wildlife conservation. Nat. Commun. 13, 792.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given information, there isn't any explicit mention of where the code repository of the deep learning pipeline is available. It could be hosted on platforms such as GitHub, GitLab, or BitBucket, among others, but without further details, it's impossible to provide a definitive answer.

However, one reference mentioned in the context is Skalski, P., 2019. Make Sense. <https://github.com/SkalskiP/make-sense/>. This suggests that the author might have shared some related work on GitHub, although it doesn't confirm if it includes the exact deep learning pipeline referred to in the context. To find out more about the availability of the code repository, additional research would be required.
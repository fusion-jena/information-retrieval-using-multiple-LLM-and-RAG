Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(cid:0) 4,  and  steep  value  of  1.3e

4). 

(cid:0)

4.3.3. Model evaluation 

In this section, the results of the deep learning model developed are 
presented. The model was trained to make day-ahead predictions of the 
three pollutants, but first, an appropriate evaluation metric had to be 
selected.  The  top  metrics  for  regression  problems  are  mean  squared 
error/root  mean  squared  error  (MSE/RMSE),  mean  absolute  error 

Fig. 12. A plot showing the model's training and validation losses against the 
number of epochs. It is worth noting that there was a gradual decrease in both 
losses  as  the  training  epochs  increased  which  indicates  that  the  model  was 
learning. Further training beyond 20,000 epochs would have either resulted in 
overfitting or no further drop in both losses.

3. A proposed deep learning framework for highway AQ 
monitoring and prediction 

The proposed framework is a four-layered architecture composed of 
the hardware layer, data storage layer, integration layer and analytics 
layer as depicted in Fig. 2. This section introduces these layers and their 
functionalities. 

3.1. Hardware layer

Fig. 13. An illustration of captured NO2 pollutant readings (blue highlight) and the deep learning model predictions (red highlight). These results were derived from 
an evaluation using the validation dataset. It should be pointed out that the model's predictions are not too far off the actual readings. (For interpretation of the 
references to colour in this figure legend, the reader is referred to the web version of this article.) 

Table 4 
Hardware specifications of the two oracle cloud instances used to test scalablity.  

Instance 
Type 

Bare 
Metal 

Name 

Compute – 
Ampere 
A1 – 
OCPU 

Processor 

OCPU 

GPU 
type 

– 

CPU 
cores 

CPU 
memory 

GPU 
memory 

6 

32GB 

– 

VM. 

GPU 

Pascal 

GPU2.1 

1 
NVIDIA 
P100 

12 

72GB 

16GB  

4.4. Evaluating the scalability performance of the REVIS system

this layer is MLOps - model maintenance in the production environment. 
MLOps encompasses automation and monitoring steps such as contin-
uous  integration,  deployment  and  training  on  data  collected  in 
production.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Unfortunately, based on the given context, it does not provide explicit information about the process followed to deploy the trained deep learning model, including details like model serialization or platform selection. However, we can infer some related aspects from the available text.

From the context, we understand that the deep learning model was designed for making day-ahead predictions of three pollutants. To evaluate its performance, appropriate evaluation metrics were selected, such as Mean Squared Error (MSE)/Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). Additionally, Figure 12 shows the model's training and validation losses over time, indicating that the model learned effectively without signs of overfitting after 20,000 epochs.

Furthermore, the context mentions a four-layered architecture consisting of a hardware layer, data storage layer, integration layer, and analytics layer. Although the description of each layer focuses more on functionality rather than implementation details, one could assume that the analytics layer might involve deploying the trained models.

Lastly, the context discusses MLOps—model maintenance in the production environment—which includes continuous integration, deployment, and training on data collected in production. While this doesn't directly explain the deployment process, it suggests that the team likely employed best practices for maintaining and updating deployed machine learning models.

However, without additional context, it remains unclear what specific processes were followed to deploy the trained deep learning model, such as model serialization or platform selection.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

four skilled counters c.	3–6	weeks	(Torney	et	al.,	2016).	Automating	

Rosi,  &  Post,  2017),  as  well  as  providing  vital  ecosystem  services 

this aspect of the survey would have two major advantages. First, it 

to human communities around the park (Sinclair, Metzger, Mduma, 

would remove a bottleneck in running the survey. Beyond the actual 

(a)

(b)

(c)

more importantly, reduce the number of empty images that citizen 

account for the large amounts of empty space in the training images. 

counters needed to process.

We achieved this by increasing the weighting (from 0.5 to 2) given to 

Following	initial	trials	on	the	website	in	August	2015,	it	was	de-

the no- object component of the multipart loss function described in 

termined  that  volunteers  would  struggle  to  count  entire  aerial  im-

Redmon et al. (2016). For training, we used transfer learning, again 

ages due to their large size and high resolution. Our solution was to 

using  the  pretrained  general  purpose  YOLO  object  detector  as  a 

split each aerial image into 12 equal- sized tiles. The images were up-

starting point with initial weights created by training on the COCO 

loaded to the Serengeti Wildebeest Count project on the Zooniverse 

dataset (Redmon & Farhadi, 2018). During training, we first froze all

count wildebeest, and each image was counted by 15 different vol-

tomated	count	of	the	training	images	and	the	expert	count.	All	code	

unteers.  Once  an  image  was  counted  15  times,  it  was  retired  and 

is available from http://dx.doi.org/10.5281/zenodo.2562058.

the overall project progress was displayed on a statistics bar on the 

For the final stage, we counted 1,000 survey images selected at 

home page. Once all images were retired the classification data was 

random,  but  excluding  the  500  training  images.  Counting  the  test 

downloaded. The data included the number of wildebeest counted 

images took 2 hr using the same GPU as for the training.

by each user, their username (unregistered users were given a ran-

dom username) and the pixel location of each of their identifications. 

Prior  to  analysis,  any  count  data  made  using  early  versions  of  the 

2.4 | Expert count

counting interface or collected by either developers or citizen scien-

science counts of the survey and comparison to expert counts.

tee  that  the  approach  is  transferable  and  how  to  appropriately 

4 |  D I S CU S S I O N

filter the data may be affected by the wording of the guidelines, 

the image resolution and sizes used, or the set of volunteers that 

participate  in  the  project.  Other  more  sophisticated  approaches 

to processing citizen science data have been proposed (Swanson 

From our results, we see that both citizen science and deep learn-

et al., 2016); however, given the range of counts provided by the 

ing methods are capable of producing highly accurate image counts. 

volunteers  and  the  large  errors  we  observe  in  the  baseline  met-

Counting the wildebeest within the survey images is a difficult and 

rics  (c.  11%  and c.  9%  undercount  for  the  mean  and  median,  re-

time- consuming task. When collecting the census images, there are

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific criteria used to determine when training is completed. However, based on common practices in machine learning, some possible criteria could include reaching a certain level of accuracy, achieving a stable validation loss over several epochs, or meeting a predefined threshold for performance metrics such as precision, recall, or F1 score. In this case, since the authors mention using transfer learning and freezing layers during training, they might have relied on these techniques to optimize the model's performance and decide when to stop training. Nevertheless, without further details, it is impossible to pinpoint the exact criteria used in this study.
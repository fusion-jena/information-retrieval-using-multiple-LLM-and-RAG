Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

After training the MLP models, we computed SHAP values using the
model-agnostic Kernel SHAP method to understand which features are
most important in predicting the start and (height of the) peak of the
greening season. We used the implementation in the Python SHAP
package for this analysis Lundberg et al. (2017).

Table 2
Overview of the explored ranges of hyperparameters used in the Optuna grid
search. The optimal values for the three different regression tasks are displayed
in the right-most three columns.

Description

Range

Number of neurons in first

layer

Number of neurons in

second layer
Strength of the L2

regularization term
the solver for weight

optimization

initial learning rate

learning rate schedule for

weight updates

maximum number of

iterations

maximum number of
iterations with no
improvement

int: 10, 20, …,
100
int: 0, 10, …,
100
float: 1e-4 —
1e-1 logscale

SOS

100

0

POS

PEAK

70

0

30

100

0.0290

0.0010

0.0606

adam, lbfgs

adam

adam

adam

To evaluate how well the models performed, we used three standard
metrics: MSE, mean average error (MAE), and the coefficient of deter-
mination (r2). For the grid search, we focused on minimizing the MSE to
identify the optimal set of hyperparameters. Prior to conducting the grid
search, we divided our data into a training set (80% of the data) and a
test set (20% of the data). This split ensures that the models are trained
on one portion of the data and tested on a separate, previously unseen
portion, allowing us to assess their ability to generalize to new, unseen
data accurately.

2.3.3. SHAP values

Abbreviations: NDVI, Normalized Difference Vegetation Index; SOS, start of the season; PEAK, maximum annual NDVI value; POS, peak of the season; ML,
machine learning; ANN, artificial neural network; MLP, multilayer perceptron; MSE, mean squared error; MAE, mean average error; CV, cross validation; SHAP,
SHapley Additive exPlanations; LIME, Local Interpretable Model-Agnostic Explanations; xAI, explainable artificial intelligence.

* Corresponding author.

E-mail address: steven.mortier@uantwerpen.be (S. Mortier).

https://doi.org/10.1016/j.ecoinf.2024.102730
Received 30 November 2023; Received in revised form 15 July 2024; Accepted 16 July 2024

EcologicalInformatics82(2024)102730Availableonline20July20241574-9541/©2024TheAuthors.PublishedbyElsevierB.V.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).S. Mortier et al.

the last decades, different

2.3.2. Machine learning

To better understand the inter-annual variability in our results, we
used ML techniques to create models predicting different vegetation
phenology characteristics. Specifically, we trained three separate MLPs:
one to predict the start of the season, another to predict the peak of the
season, and a third to predict the height of the peak season. An MLP is a
type of ANN designed to mimic the way the human brain processes in-
formation. It consists of multiple layers of nodes (neurons): an input
layer, one or more hidden layers, and an output layer which is used to
provide the final predictions. Each node in a layer connects to every
node in the next layer, with each connection having a specific weight.
During training, the MLP adjusts these weights to minimize the differ-
ence between its predictions and the actual outcomes using an algorithm
called backpropagation, allowing it to learn complex patterns in the data
(Werbos, 1990).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, the authors use two criteria to determine when training is complete: the maximum number of iterations and the maximum number of iterations with no improvement. These criteria are part of the hyperparameter tuning process conducted through a grid search using Optuna. The goal of this process is to find the best combination of hyperparameters that result in the lowest Mean Squared Error (MSE) for the model. During this process, the authors divide their dataset into a training set (80%) and a test set (20%). They then train the Multilayer Perceptron (MLP) models on the training set while monitoring the performance on the test set. Training continues until either the maximum number of iterations has been reached or there have been no improvements in the MSE after a certain number of iterations. In other words, if the model stops improving during training, the authors stop the training process even before reaching the maximum number of iterations. However, the exact threshold for determining when there have been no improvements is not specified in the provided text.
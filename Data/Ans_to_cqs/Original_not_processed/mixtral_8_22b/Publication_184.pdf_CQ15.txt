Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In  order  to  train  the  model,  it  is important to provide not only 
the  class  of  the  object  but  also  the  bounding  box  data  of  the 
object  as  correct  answer.  In  this  context,  we  parsed  the  x,  y 
coordinates, width and height of the fish in the annotation of the 
image’s  dataset.  In  addition,  we  used  data  augmentation  to 
extend the amount of data. 

The  model  has  better  performance  as  number  of  epochs 
increases.  However,  when  number  of  epochs  exceeds  200,  the 
network  seems  to  be  overfitted  to  training  data.  As  a  result, 
Table  I  shows  the  hyper-parameters  used  to  train  and  validate 
the Yolo model. 

Number 
of 
iterations 
4000 

Number 
of 
epochs 
200 

Learning 
rate 

Batch 
size 

Subdivisions 

0.001 

16 

64 

Table 1. Hyper-parameters to train the Yolo model.

Furthermore,  YOLOv3  has  been  made  with  another 
improvement,  the  new  CNN  feature  extractor  named  Darknet-
53. It is a 53 layered CNN that uses 3x3 and 1x1 convolutional 
layers. It has demonstrated the advanced accuracy but with less 
floating-point  tasks  and  better  speed.  For  example,  it  has  less 
the  same 
floating-point  operations 
performance at a double speed. It utilizes also skip connections 
network inspired from ResNet. 

than  ResNet-152  but

The graph illustrates the loss while training the neural network 
and the average loss is reduced to 0.54%. That means the model 
is  affected  by  training  data.  A  total  of  4000  iteration  were  run 
and  it  took  14  hours  to  complete  the  training.  The  model 
improved swiftly in terms of precision, recall and mean average 
precision  before  plateauing  after  about  1000  iterations,  and 
around 1000 iterations the loss showed a rapid decline. 

For  each  epoch,  64  images  are  randomly  selected  and  used  to 
train  the  model.  Each  image  is  used  multiple  times  due  to  the 
limited  number  of  samples.  Figure  8,  shows  some  training 
samples, and the number zero indicates the fish class. 

Figure 8. Training samples.

Amanullah, M. A., Habeeb, R. A. A., Nasaruddin, F. H., Gani, 
A.,  Ahmed,  E.,  Nainar,  A.  S.  M.,  Akim,  N.  M.,  &  Imran,  M. 
(2020).  Deep  learning  and  big  data  technologies  for  IoT 
security.  Computer  Communications,  151(December  2019), 
495–517. https://doi.org/10.1016/j.comcom.2020.01.016 
Ayaz,  A.,  Acarli,  D.,  Altinagac,  U.,  Ozekinci,  U.,  Kara,  A.,  & 
Ozen,  O.  (2006).  Ghost  fishing  by  monofilament  and 
multifilament gillnets in Izmir Bay, Turkey. Fisheries Research, 
79(3), 267–271. https://doi.org/10.1016/j.fishres.2006.03.029 

Beyan,  C.,  &  Fisher,  R.  B.  (2012).  A  filtering  mechanism  for 
normal fish trajectories. Proceedings - International Conference 
on Pattern Recognition, 2286–2289. 

Brodie,  G.,  Brodie,  J.,  Maata,  M.,  Peter,  M.,  Otiawa,  T.,  & 
Devlin,  M.  J.  (2020).  Seagrass  habitat  in  Tarawa  Lagoon,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, there isn't any explicit mention of specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being used in the deep learning pipeline. However, the context does indicate that the model was trained using a learning rate of 0.001, which suggests that some form of gradient descent optimization technique might have been employed. Additionally, the context mentions that the model improved swiftly in terms of precision, recall, and mean average precision before plateauing after around 1000 iterations, indicating that the learning process may have involved some sort of adaptive learning rate adjustment strategy. Nonetheless, without further information regarding the specific optimization algorithms used during the training phase, it would be speculative to definitively state whether SGD, Adam, or other optimization techniques were utilized in the deep learning pipeline.
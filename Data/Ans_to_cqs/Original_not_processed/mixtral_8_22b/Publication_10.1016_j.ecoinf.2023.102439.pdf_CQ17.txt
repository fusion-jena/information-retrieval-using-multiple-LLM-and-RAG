Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(3)  

λ = wavelength of emitted radiance (10.6 μm) 
p = h.c/ σ (1.438.10
σ = (1.38.10–23 JK
h = Planck’s constant (6.626.10
c = Velocity of light (2.998.108 ms

(cid:0) 2 m. K) 
(cid:0) 1) 

(cid:0) 34 J.s) 
(cid:0) 1) 

2.3.3. Modelling 

Artificial  neural  networks  are  a  successful  method  in  modelling 
studies and are frequently discussed in the literature. Essentially, neural 
networks learn from past and present data, uncover hidden relationships 
among  the  data,  and  utilise  them  to  make  predictions  for  future  sce-
narios. Among the various types of artificial neural network models, the 
NARX model is widely employed. In this study, the NARX feedback ANN 
model  was  utilised  to  define  vegetation  biomass  density  for  2030.  A 
hybrid  model  was  developed  by  combining  wavelet  analysis  and  the 
NARX ANN algorithm to enhance the performance of the NARX ANN 
model.

Al-Dousari, A.E., Mishra, A., Singh, S., 2023. Land use land cover change detection and 

urban sprawl prediction for Kuwait metropolitan region, using multi-layer 
perceptron neural networks (MLPNN). Egypt. J. Remote Sens. Space Sci. 26 (2), 
381–392. 

Alshari, E.A., Abdulkareem, M.B., Gawali, B.W., 2023. Classification of land use/land 
cover using artificial intelligence (ANN-RF) —frontiers. Artif. Intell. 5, 964279. 
Alves, E.D.L., Lopes, A., 2017. The urban Heat Island effect and the role of vegetation to 
address the negative impacts climate changes in a small Brazilian City. Atmosphere 8 
(2), 18. 

Anshuka, A., van Ogtrop, F.F., Vervoort, R.W., 2019. Drought forecasting through 

statistical models using standardized precipitation index: a systematic review and 
Meta-regression analysis. Nat. Hazards 97, 955–977. 

Artis, D.A., Carnahan, W.H., 1982. Survey of emissivity variability in thermography of 

urban areas. Remote Sens. Environ. 12 (4), 313–329.

By employing the W-ANN model, the training values reached 96% 
for Iznik and 91% for Alanya, while the estimated percentages for all 
data increased to 95% for Iznik and 88% for Alanya, respectively. The 
developed hybrid model improved learning success, with an increase of 
18.7%  in  Iznik  and  7.3%  in  Alanya,  as  determined  by  the  network’s 
performance.  These  findings  highlight  the  hybrid  model’s  enhanced 
predictive capabilities and effectiveness in both study areas.

3. Results 

As indicated before, the ANN model utilised in this study comprises 
an  input,  hidden,  and  output  layer.  The  input  layer  comprises  three 
processing elements (neurons) corresponding to the LST, temperature, 
and precipitation parameters. The EVI data was used as the output layer. 
The number of neurons in the hidden layer is a crucial factor influencing 
the learning capability of the network. Insufficient neurons may hinder 
successful learning, while excessive neurons may lead to over fitting and 
compromise the network’s performance (Çakır, 2018). In this study, the 
number of neurons in the hidden layer was incrementally to determine 
the optimal configuration that yields the best results.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Regularization methods are techniques used in machine learning and deep learning pipelines to prevent overfitting. Overfitting occurs when a model performs well on training data but poorly on new, unseen data due to its complexity or high variance. Regularization helps reduce this issue by adding a penalty term to the loss function during optimization, which discourages large weights and encourages simpler models. Two common regularization methods are Dropout and L2 regularization.

Dropout is a technique where randomly selected neurons are temporarily removed ("dropped out") from the network during each iteration of training. This forces the remaining neurons to learn more robust features and prevents any single neuron from becoming too influential. As a result, the network becomes more resilient to noise and generalizes better to new data. Dropout rates typically range between 0.2 and 0.5, depending on the size and complexity of the network.

L2 regularization, also known as weight decay, adds a penalty term proportional to the square of the magnitude of the weights to the loss function. This encourages smaller weights and thus simpler models, reducing the risk of overfitting. The strength of the penalty can be controlled by adjusting the regularization parameter lambda. Higher values of lambda will result in stronger penalties and simpler models, while lower values will allow for more complex models.

In summary, regularization methods such as Dropout and L2 regularization help prevent overfitting in deep learning pipelines by introducing penalties to the loss function during optimization. Dropout achieves this by randomly dropping out neurons during training, while L2 regularization penalizes large weights, encouraging simpler models. Both techniques aim to improve the model's ability to generalize to new, unseen data.
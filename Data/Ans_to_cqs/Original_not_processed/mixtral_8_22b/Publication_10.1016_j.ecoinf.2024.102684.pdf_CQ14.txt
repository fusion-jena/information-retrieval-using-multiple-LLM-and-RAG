Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We applied the  standard machine learning hyperparameter  search 
over  two  sites  because  of  computational  cost.  In  our  research,  we 
considered the activation function, the optimizer, and the learning rate. 
In addition, we performed an exhaustive search for the fully connected 
architecture for all mono-site and multi-site models. In all these cases, 
we compare the training, validation, and testing performance to avoid 
overfitting.  We  could  have  achieved  the  same  results  using  hyper-
parameter search software such as Optuna and Sklearn. However, even 
though they could save coding time, we prefer to control every aspect of 
the process to avoid problems. The hyperparameter search involved a 
small grid search selecting the following hyperparameters and values:  

1.  Hidden activation function: ReLU, sigmoid, or elu,  
2.  Optimizer: RMSprop or adam  
3.  Learning rate: 0.1, 0.001, 0.0001, and 0.00001.

The dataset was divided into training (60%), validation (20%), and 
testing (20%) subsets for model evaluation and hyperparameter search 
(Fig. 3 (b)). This approach was chosen to obtain an unbiased estimation 
of  error.  Data  augmentation  was  only  applied  to  the  training  data  to 
avoid overlapping the same data in different partitions. To evaluate the 
models we used the coefficients of determination (R2  - calculated as the 
squared  Pearson's  correlation  coefficient),  which  measures  the  per-
centage of variation in the response variable explained by the model. 
Additionally,  we  used  the  root  mean  squared  error  (RMSE)  metric, 
where  RMSE =
,  with  Yi = the  actual  values  pine 
̂
Y i = the predicted pine cove values. In our work, RMSE 
cover values and 
values ranged from 0.0 to 1.0, with values closer to 0 indicating better 
model  predictions.  The  overfitting  was  assessed  by  comparing  model

It is crucial to emphasize that the practical applicability of the modelt 
search  is  constrained.  Often,  many  applications  need  labeled  data  in 
regions  targeted  for  information  transfer,  particularly  if  they  need  to 
consider the test dataset as transfer learning. Moreover, when such data 
is available, it is generally more prudent to incorporate it into model 
calibration and testing processes. 

4.4. Spatial analysis of the modelv

Predictive performance of machine learning model with varying sampling designs, 
sample sizes, and spatial extents. Ecol. Inform. 102294. 

Cao, J., Leng, W., Liu, K., Liu, L., He, Z., Zhu, Y., 2018. Object-based mangrove species 
classification using unmanned aerial vehicle hyperspectral images and digital 
surface models. Remote Sens. 10, 89. 

Chadwick, A.J., Coops, N.C., Bater, C.W., Martens, L.A., White, B., 2024. Transferability 
of a mask r–CNN model for the delineation and classification of two species of 
regenerating tree crowns to untrained sites. Sci. Remote Sens. 9, 100109. 

Chollet, F., 2017. Xception: deep learning with depthwise separable convolutions arXiv: 

1610.02357 abs/1610.02357.  

Chollet, F., et al., 2015. Keras. https://keras.io. 
da Silva, S.D.P., Eugenio, F.C., Fantinel, R.A., de Paula Amaral, L., dos Santos, A.R.,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The hyperparameters of the model were optimized through a small grid search. The specific hyperparameters considered in this search included the hidden activation function, the optimizer, and the learning rate. For each of these parameters, a set of possible values was selected. The hidden activation function options were ReLU, sigmoid, or elu; the optimizer choices were RMSprop or adam; and the learning rates tested were 0.1, 0.001, 0.0001, and 0.00001. By conducting a grid search across these combinations, the researchers aimed to find the optimal configuration for their model. It should be noted that while automated tools like Optuna and Sklearn could have been employed to expedite this process, the researchers opted to manually code their own solution to maintain full control over the optimization procedure.
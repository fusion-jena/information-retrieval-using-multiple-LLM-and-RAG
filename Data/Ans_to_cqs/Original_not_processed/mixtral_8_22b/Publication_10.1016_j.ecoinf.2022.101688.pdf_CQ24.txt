Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Since CNNs require a fixed input size, we studied the vocalisations 
within the presence class for each dataset to determine the character-
istics of the calls which would allow us to create fixed input, which we 
refer to as segments. For example, the Hainan gibbon calls vary from 2 to 
9 s (Dufourq et al., 2021), and thus a suitable input size was 4 s to ensure 
that the smallest call would fit within the segment. A longer input (>4 
seconds) would result in CNNs with more network parameters, an un-
desirable consequence as this would increase the chances to overfit. A 
shorter input (<4 s) would not contain enough information, especially 
in cases where the individual pulses that make up a call are long. A short 
input could omit parts of the call. Preliminary experiments were con-
ducted on the different datasets to minimise the input length as much as 
possible,  thus  minimising  network  parameters.  The  characteristics  of

Table 4 
Comparison of the average F1 score across the different network architectures and dataset configurations. The exponent approach was used for the spectrogram input. 
The feature extracted was frozen. The results are averaged across 13 unique executions. The results are ordered (highest to lowed) based on the average of each 
network architecture across all configurations. The best three performing network architectures on a particular dataset configuration is highlighted in bold.  

Method 

ResNet101V2 
ResNet152V2 
InceptionResNetV2 
ResNet50V2 
DenseNet169 
DenseNet201 
VGG16 
DenseNet121 
InceptionV3 
ResNet101 
Xception 
MobileNetV2 

G 25 

95.30 
95.18 
94.70 
94.97 
94.92 
94.84 
97.26 
94.58 
92.22 
96.17 
93.88 
94.62 

G 50 

97.40 
96.92 
96.75 
97.04 
96.95 
96.72 
98.09 
96.69 
95.42 
97.49 
95.79 
96.65 

G 100 

96.27 
96.58 
96.57 
95.13 
95.69 
95.86 
94.99 
95.00 
95.40 
94.23 
95.50 
91.65 

L 25

Using  a  pre-trained  CNN  feature  extractor  and  adding  a  softmax 
output layer is less complex than having to optimise a CNN from scratch 
and requires less network design decisions and also less time on hyper- 
parameter tuning. We thus argue that this approach renders the use of 
deep learning much more accessible to practitioners. Extensive hyper- 
parameter tuning also requires expensive GPU hardware which might 
not  be  accessible  to  practitioners.  Our  findings  revealed  that  results 
could be obtained on limited hardware within 9 h (10 epochs of fine- 
tuning  the  feature  extractor)  which  would  cost  2USD,  at  the  time  of 
writing, if that was executed on a Microsoft Azure virtual machine – thus 
rendering this approach affordable and accessible. 

It is well accepted that no single machine learning algorithm – or in

execution the weights in the output layer of the CNN were randomly 
initialised. We split each dataset into training (60%) and testing (40%) 
by  randomly  selecting  entire  audio  files  –  similar  to  other  machine 
learning studies. To ensure a fair evaluation, we split the data in such a 
way that the training audio files were mutually exclusive to the testing 
audio  files.  The  testing  files  were  generally  recordings  over  different 
days. For testing, the CNNs predicted two softmax outputs on each entire 
testing file. The final class was assigned based on the softmax output 
which  had  a  value  greater  than  0.5.  This  decision  threshold  was  not 
optimised to keep the resulting models as accessible and easy-to-use as 
possible. This process was done using a sliding window of constant time 
duration.  The  window  is  shifted  by  1  second  until  the  network  had 
predicted on the entire file. The testing files were manually annotated

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The text does not provide explicit details about handling randomness in the deep learning pipeline, such as managing random seed values. However, it mentions that the weights in the output layer of the Convolutional Neural Network (CNN) were randomly initialized. Additionally, the data splitting process involved randomly selecting entire audio files for training (60%) and testing (40%). These steps suggest that some level of randomness was introduced during model initialization and data partitioning.

To manage randomness in the deep learning pipeline, one common strategy is setting a specific seed value before initializing any part of the model or processing the data. By doing so, researchers can reproduce their results consistently across multiple runs. Another strategy involves conducting multiple trials with different seeds and reporting the aggregated performance metrics, such as mean and standard deviation, to account for variability due to randomness.

Although the given context does not explicitly discuss these strategies, they are commonly employed in deep learning research to address randomness concerns.
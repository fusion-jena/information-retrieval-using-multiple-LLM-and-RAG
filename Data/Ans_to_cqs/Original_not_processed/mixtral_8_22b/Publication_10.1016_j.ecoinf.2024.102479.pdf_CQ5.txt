Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Friedman, J.H., 2002. Stochastic gradient boosting. Comput. Stat. Data Anal. 38, 

367–378. https://doi.org/10.1016/S0167-9473(01)00065-2. 

Fritsch, S., Guenther, F., Guenther, M.F., 2019. Package ‘neuralnet’. In: Training of 

Neural Networks, 2, p. 30. 

Gamble, J.S., Fischer, C.E.C., 1915–1935. Flora of the Presidency of Madras, Vol. 1–3, 

21, Hart Street, W.C.  

Gascon, F., Ramoino, F., Deanos, Y., 2017. Sentinel-2 Data Exploitation with ESA’s 

Sentinel-2 Toolbox, 19. EGU Gen. Assem, p. 19548 [Google Scholar].  

Ghasemi, N., Sahebi, M.R., Mohammadzadeh, A., 2011. A review on biomass estimation 
methods using synthetic aperture radar data. Int. J. Geomat. Geosci. 1 (4), 776–788 
[Google Scholar].  

Gholamy, A., Kreinovich, V., Kosheleva, O., 2018. Why 70/30 or 80/20 relation between 

training and testing sets: a pedagogical explanation [Google Scholar].

(7150), 188–190. 

Hengl, T., Mendes de Jesus, J., Heuvelink, G.B., Ruiperez Gonzalez, M., Kilibarda, M., 
Blagoti´c, A., Shangguan, W., Wright, M.N., Geng, X., Bauer-Marschallinger, B., 
Guevara, M.A., 2017. SoilGrids250m: global gridded soil information based on 
machine learning. PLoS One 12 (2), e0169748. https://doi.org/10.1371/journal. 
pone.0169748. 

Herold, M., Carter, S., Avitabile, V., Espejo, A.B., Jonckheere, I., Lucas, R., McRoberts, R. 
E., Næsset, E., Nightingale, J., Petersen, R., Reiche, J., 2019. The role and need for 
space-based forest biomass-related measurements in environmental management 
and policy. Surv. Geophys. 40, 757–778. https://doi.org/10.1007/s10712-019- 
09510-6. 

Herraiz, A.D., Salazar-Zarzosa, P.C., Mesas, F.J., Arenas-Castro, S., Ruiz-Benito, P., 

Villar, R., 2023. Modelling aboveground biomass and productivity and the impact of 
climate change in Mediterranean forests of South Spain. Agric. For. Meteorol. 337, 
109498.

To assess the performance of the models, k-fold cross-validation was 
employed,  a  widely  used  evaluation  method  in  machine  learning 
(Anguita et al., 2012; Singh et al., 2022; Yadav and Shukla, 2016). This 
technique has proven to be valuable in machine learning for estimation 
and aids in preventing overfitting by dividing the dataset into k subsets. 
Some of these subsets are used for training the model, while the others 
are utilized to evaluate its performance. In the present study, a 10-fold 
cross-validation  design  was  employed  as  per  Tyralis  et  al.  (2019b). 
The  dataset,  consisting  of  138  ground-based  AGB  estimates  (after 
eliminating outliers), was split into two distinct sets: 70% of the dataset 
was designated for model training while the remaining 30% was used as 
testing dataset. This approach ensured that the model was trained on a 
sufficient amount of data while also providing an independent dataset

hyperparameters  in  a  model.  It  identifies  the  most  effective  set  of 
hyperparameters  that  enhance  the  model's  performance.  During 
training,  GSCV  generates  a  series  of  models,  each  characterized  by  a 
different  set  of  hyperparameters.  The  purpose  of  this  technique  is  to 
systematically train and assess the performance of these various models 
through cross-validation. The process concludes with the selection of the 
model that demonstrates the most superior performance, determined by 
its optimal hyperparameter combination (Adnan et al., 2022). Details of 
the primary tuning parameters, their range and the optimal combination 
of  hyperparameters  utilized  for  each  algorithm  for  each  model  are 
shown in Table 1.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, there is no explicit mention of specific datasets such as MNIST, CIFAR, or ImageNet being used in the deep learning pipeline. However, it does state that the dataset consists of 138 ground-based AGB (aboveground biomass) estimates after eliminating outliers. This dataset was then divided into two parts - 70% for model training and 30% for testing purposes. Therefore, the dataset used in this case appears to be related to estimating aboveground biomass rather than common image classification tasks like those found in MNIST, CIFAR, or ImageNet datasets.
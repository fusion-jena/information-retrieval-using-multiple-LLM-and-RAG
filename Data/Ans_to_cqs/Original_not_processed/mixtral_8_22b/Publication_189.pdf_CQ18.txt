Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

0.0000e+00; 

The  loss  values  for  training  and  validation  dataset  during 
Mask  R-CNN  training  for  epoch  20  were  obtained  as-  loss: 
0.8241;  rpn_class_loss:  0.3679; 
  rpn_bbox_loss:  0.4562; 
mrcnn_class_loss: 3.1710e-06;  mrcnn_bbox_loss: 0.0000e+00; 
mrcnn_mask_loss: 
0.6051; 
  val_rpn_bbox_loss:  0.1993; 
val_rpn_class_loss:  0.4057; 
val_mrcnn_bbox_loss: 
val_mrcnn_class_loss: 
0.0000e+00; 
0.0000e+00,  where 
training  loss  is  the  sum  of  rpn_class_loss  =  RPN  anchor 
classifier loss, rpn_bbox_loss = RPN bounding box loss graph, 
mrcnn_class_loss = loss for the classifier head of Mask R-CNN, 
mrcnn_bbox_loss  =  loss  for  Mask  R-CNN  bounding  box 
refinement, mrcnn_mask_loss = mask binary cross-entropy loss  

3.2186e-06; 
val_mrcnn_mask_loss: 

val_loss: 

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on September 08,2024 at 12:02:05 UTC from IEEE Xplore.  Restrictions apply. 

37

[26]  Chang, Chih-Chung and Chih-Jen Lin. “LIBSVM: A library for support 
vector machines.” ACM Trans. Intell. Syst. Technol. 2 (2011): 27:1-27:27. 
[27]  Poojary, Ramaprasad & Raina, Roma & Mondal, Amit Kumar. (2020). 
Effect  of  data-augmentation  on  fine-tuned  CNN  model  performance. 
IAES  International  Journal  of  Artificial  Intelligence  (IJ-AI).  10. 
10.11591/ijai.v10.i1.pp84-92. 

[28]  IKKAKU Project-https://ikkaku.lne.st/en/vision/ 

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on September 08,2024 at 12:02:05 UTC from IEEE Xplore.  Restrictions apply. 

38

This  system  is  most  compatible  with  tensorflow  version 
1.14.0 and keras version 2.2.4 along with python 3.6. This model 
has been implemented on a machine with 1 GPU which uses 1 
Image per GPU and batch size for mrcnn training is  taken as a 
product of GPU and Images per GPU. The system used resnet 
50 and resnet 101 as backbone structures and used anchor boxes 
of ratio 32, 64, 128, 256, 512. The width and height of images 
in  this  research  is  fixed,  therefore  [a,640,640,3]  becomes  the 
image  shape  here  and  [a,640,640,1]  becomes  the  mask  shape 
where  a  is  the  number  of  training  images  and  true  masked 
images. For training on Mask R-CNN, 20 epochs with 500 steps 
each  are  being  used  for  training  images  and  100  steps  for 
validation images while evaluating the model. The learning rate, 
 used for heads layer training  has been  taken as 0.001 while 
during  all  layers  training    decayed  by  10.  For  weight

[15]  Tian,  Zhi  &  Shen,  Chunhua  &  Wang,  Xinlong  &  Chen,  Hao.  (2020). 
BoxInst: High-Performance Instance Segmentation with Box Annotations. 
[16]  Gao, S., Cheng, M., Zhao, K., Zhang, X., Yang, M., & Torr, P.H. (2021). 
Res2Net: A New Multi-Scale Backbone Architecture. IEEE Transactions 
on Pattern Analysis and Machine Intelligence, 43, 652-662. 

[17]  Bolya,  Daniel  &  Zhou,  Chong  &  Xiao,  Fanyi  &  Lee,  Yong.  (2019). 

YOLACT: Real-time Instance Segmentation. 

[18]  Bolya, D., Zhou, C., Xiao, F., & Lee, Y.J. (2020). YOLACT++: Better 
Real-time Instance Segmentation. IEEE transactions on pattern analysis 
and machine intelligence, PP. 

[19]  Long,  Jonathan  &  Shelhamer,  Evan  &  Darrell,  Trevor.  (2015).  Fully 
segmentation.  3431-3440. 

semantic 

convolutional  networks 
for 
10.1109/CVPR.2015.7298965

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To monitor the model performance during training, the authors have employed several strategies. They have calculated various types of losses such as rpn\_class\_loss, rpn\_bbox\_loss, mrcnn\_class\_loss, mrcnn\_bbox\_loss, and mrcnn\_mask\_loss. These losses provide insights into different aspects of the model's performance. For instance, rpn\_class\_loss measures the error in predicting whether an anchor is a foreground or background object, while rpn\_bbox\_loss calculates the regression error between the predicted and ground truth bounding boxes. Similarly, mrcnn\_class\_loss, mrcnn\_bbox\_loss, and mrcnn\_mask\_loss measure the classification, localization, and segmentation errors respectively. By monitoring these losses, one can understand how well the model is performing and identify areas for improvement. Additionally, the authors have also evaluated their model using validation datasets after every few epochs, providing further insight into its generalizability and robustness.
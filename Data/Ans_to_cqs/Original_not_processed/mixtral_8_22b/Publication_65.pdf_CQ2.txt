Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The most signiﬁcant variation of performance is concerning the number of parameters
and with training and inference time. Despite being the best architecture in performance,
According to Table 2, DeepLabv3+ needed more parameters than the other architectures,
about 2.75 times more parameters than the U-Net, the least requiring one. The need for a
more signiﬁcant number of parameters often implies a higher demand for training samples
that our dataset or another dataset may not have met that the methods present in this
research paper may be applied, possibly causing the DeepLabV3+ architecture to perform
below its potential.

Remote Sens. 2021, 13, 3054

11 of 19

Figure 11. Convergence of the evaluated networks.

Table 2. Number of Parameters and Processing Time of the proposed approaches. The training time
represents the results for the test set of each method. The inference time stands for the time taken by
each model to make predictions for each image.

Method

FCN

U-Net

SegNet

We adopted the same training/trial protocol for all CNNs. All networks used in
this research have been trained from scratch (i.e., without pre-trained weights from other
datasets, like ImageNet, for example). We used 1938 input patch sizes of 256 × 256 pixels,
with 388 patches for the test, 1162 for train, and 388 for validation, a distribution of approx-
imately 20%, 60%, and 20% respectively. It is important to note that additional input patch
sizes were tested in an experimental phase, but the results did not change considerably
and only increased the training time. All approaches used the same set of hyperparam-
eters during training, which was deﬁned based on previous analyses. Speciﬁcally, the
learning rate, weight decay, momentum, and iterations were 0.01, 0.005, 0.9, and 200,000,
respectively. The model is optimized using stochastic gradient descent (SGD). To assist
the convergence process and prevent overﬁtting (Overﬁtting is a concept used to refer to a

Remote Sens. 2021, 13, 3054

8 of 19

Figure 9. Schematic diagram of the evaluation process.

All deep learning-based models exploited in this work were implemented using the
TensorFlow [51], a Python framework conceived to allow efﬁcient analysis and implemen-
tation of deep learning with Graphics Processing Units (GPUs). All experiments conducted
here were performed on a 64-bit Intel i7-8700K@3.70GHz CPU workstation, 64 GB memory,
and NVIDIA® GTX 1080 GPU with 12Gb of memory, under a 10.0 CUDA version. Debian
4.195.98-1 version was used as the operating system.

2.2.2. Evaluation Metrics

DeepLabV3+: The DeepLabV3+ [48] starts with three blocks composed of two convo-
lutions and one polling layer that performs the feature extraction and an initial prediction
map. These features are then processed by a particular layer, called Atrous Spatial Pyramid
Pooling (ASPP) introduced in [49]. This technique involves employing atrous convolution
in parallel to extract features at multiple scales and alleviate the loss of spatial information
due to prior pooling or convolutions with striding operations. The data is then processed
with features extracted from the ﬁrst pooling layer and reﬁned by one extra convolutional
layer. Then three convolutional layers process the concatenated segments upsampled
by a bilinear interpolation producing the ﬁnal prediction map. For more details, see
Figure 7 [48].

Figure 7. DeepLabV3+ architecture. Adapted from [43,48].

Remote Sens. 2021, 13, 3054

7 of 19

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it can be inferred that the data format used in the deep learning pipeline is images. Specifically, the study uses input patch sizes of 256 x 256 pixels, indicating that the data format is indeed images. Furthermore, the use of TensorFlow, a popular deep learning library known for processing images, also supports this conclusion. However, there is no mention of any other data formats such as audio, video, or CSV files being used in the deep learning pipeline. Therefore, we can conclude that the data format used in the deep learning pipeline is images.
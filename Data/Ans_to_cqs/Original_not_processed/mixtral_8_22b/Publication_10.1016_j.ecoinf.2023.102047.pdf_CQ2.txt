Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Cspnet: A new backbone that can enhance learning capability of cnn. In: Proceedings 
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 
Workshops, pp. 390–391. 

Waycott, M., Duarte, C.M., Carruthers, T.J.B., Orth, R.J., Dennison, W.C., Olyarnik, S., 

Calladine, A., Fourqurean, J.W., Heck, K.L., Hughes, A.R., Kendrick, G.A., 
Kenworthy, W.J., Short, F.T., Williams, S.L., 2009. Accelerating loss of seagrasses 
across the globe threatens coastal ecosystems. Proc. Natl. Acad. Sci. 106 (30), 
12377–12381. 

Weidmann, F., Jager, J., Reus, G., Schultz, S.T., Kruschel, C., Wolff, V., Fricke- 

Neuderth, K., 2019. A closer look at seagrass meadows: Semantic segmentation for 
visual coverage estimation. In: OCEANS 2019 Marseille, 1–6. 

Xu, R., Lin, H., Lu, K., Cao, L., Liu, Y., 2021. A forest fire detection system based on 

ensemble learning. Forests 12 (2), 217. 

EcologicalInformatics76(2023)10204713

Our proposed detector-2 was based on EfficientDet-B7. EfficientDet 
frameworks are well optimised with the input image sizes of 512, 640, 
768,  896,  1024,  1280,  1280,  and  1536  for  the  family  network 
EfficientDet-D0 to EfficientDetD7, respectively. These optimised image 
sizes achieved the highest accuracy on the ImageNet dataset. We set up 
the model config file with a class number 1, image size of 1536 × 1536, 
batch size of 16, initial learning rate of 2e3, momentum optimiser value 
of 0.5 and step number of 400,000. We also monitored the validation 
loss  using  early  stopping  during  training  for  every  epoch.  We 
consciously monitored the validation accuracy to stop overfitting and 
overtraining  with  a  patience  of  20.  We  have  used  TensorFlow  object 
detection  API.  First,  we  downloaded  the  pre-trained  EfficientDet-D7 
model weight from the TensorFlow-2 object detection model zoo page.

0.3464  on  the  ECUHO-1  and  ECUHO-2  datasets  respectively.  More 
recently, Noman et al. (Noman et al., 2021b) proposed using Faster R- 
CNN with the NASNet model and achieved the highest mAP of 0.291 and 
0.412 on these H. ovalis datasets respectively.

Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C., 2016. Ssd: 
Single shot multibox detector. In: European Conference on Computer Vision. 
Springer, pp. 21–37. 

Long, J., Shelhamer, E., Darrell, T., June 2015. Fully convolutional networks for 

Redmon, J., 2016. Darknet: Open Source Neural Networks in c. 2013–2016. 
Redmon, J., Farhadi, A., 2017. Yolo9000: better, faster, stronger. In: Proceedings of the 
IEEE Conference on Computer Vision and Pattern Recognition, pp. 7263–7271. 
Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You only look once: Unified, 

semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision 
and Pattern Recognition (CVPR). 

real-time object detection. In: Proceedings of the IEEE Conference on Computer 
Vision and Pattern Recognition, pp. 779–788.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not explicitly mention the specific data format used in the deep learning pipeline. However, it is implied that images were used as inputs for the models mentioned. For instance, the text mentions setting up the model configuration file with an image size of 1536 x 1536, which suggests that the input data consists of images. Additionally, references to various computer vision conferences and papers indicate that the focus is primarily on visual data. Therefore, while other data formats such as audio or video may be compatible with some of the mentioned models, the context provided here pertains mainly to image data.
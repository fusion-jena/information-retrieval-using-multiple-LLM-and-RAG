Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Generally, the proposed network system has 
20  layers:  12  convolutional  layers,  five 
pooling  layers,  one  Fully  Connected  layer, 
one LSTM layer, and one output layer with 
the  softmax  function  for  classification.  The 
convolutional  layer  with  a  size  of  3  ×  3 
kernels is used for extracting feature and it is 
activated  by  the  ReLU  function.  The  max-
pooling layer with a size of 2 × 2 kernels is 
applied to reduce the dimensions of an input 
image. In the last part of the architecture, the 
function map is shifted to the LSTM layer to 
for 
information 
extract 
classification  purpose.  Using  the  reshape 
method,  the  input  size  of  the  LSTM  layer 
becomes (196,512). After analyzing the time 
features,  the  model  sorts  the  plant  species 
images  through  a  fully  connected  layer  to 

required 

time 

predict whether they belong under any of the 
100 categories of the plant species. 

Experimental set up

Where f(s) is the output condition 
probability 
training example Si                

 for some 

This probability function for softmax 
activation is given in equation 2 

RESULTS 

The training process took around 10 hours on 
the  CPU.  The  accuracy  obtained  reaches 
95%  on  140  epochs.  We  first  measure  the 
performance  of  the  proposed  model  fig  6. 
and then compared it with the state-of-the-art 
approaches to justify its performance. 

100 

 
 
 
 
Tanzania Journal of Forestry and Nature Conservation, Vol 90, No. 3 (2021) Special Issue: 
Embracing Science and Technology in Nature Conservation. pp 93-103 

Experiment  show  that  the  proposed  CNN-
LSTM  performs  better  in  classifying  plant 
species  than  the  convectional  CNN  as  it 
attains  the  accuracy  of  95.06%  while  the 
literatures (Zhang et al. 2019) have reported 
only 91% accuracy. 

Performance Evaluation

Experimental set up 

We  perform  a  complete  experiment  to 
demonstrate the performance of the proposed 
approach.  The  CNN-LSTM  Model  was 
implemented  with  the  python  library,  using 
Theano and Keras. The model parameters are 
optimized  using  gradient  descent  Adam 
algorithm  with  all  parameters  set  to  default 
values.  The system specification details are 
presented in  table 1,  while the detail  of the 
the  proposed 
tensor 
architecture 
the  summary 
presented in Table 2. 

layer  of 
in 

is  shown 

in  each 

Table 1: System requirement for plant specifies identification. 

System requirement 
Central processing unit (CPU) 
Operating system (OS) 
Random access memory (RAM) 
System architecture 

Specifications 
Intel(R)Core i7-4790@ 3.6GHz 
Microsoft window 
16GB 
64bit 

99 

 
 
 
 
 
 
 
 
Tanzania Journal of Forestry and Nature Conservation, Vol 90, No. 3 (2021) Special Issue: 
Embracing Science and Technology in Nature Conservation. pp 93-103

Input Size 
224 × 224 × 3 
224 × 224 × 3 
112 × 112 × 32 
112 × 112 × 64 
112 × 112 × 64 
112 × 112 × 64 
56 × 56 × 64 
56 × 56 × 128 
56 × 56 × 128 
28 × 28 × 128 
28 × 28 × 256 
28 × 28 × 256 
28 × 28 × 256 
14 × 14 × 256 
14 × 14 × 512 
14 × 14 × 512 
14 × 14 × 512 
196 × 512 
100,352 
32 

Data preprocessing 

Before  the  training  stage,  we  first  need  to 
pre-process  the  raw  data  from  the  dataset.  
The input of the pose estimator is the cropped 
image,  but  the  original  ground  truth  of  the 
image  is  the  absolute  position  in  the  entire 
raw image. Therefore, there is a need to first 
transform  the  ground  truth  into  a  relative 
position  with  respect  to  the  center  of  the 
hand. Finally, the images are sized to 96x96 
and  in  grey-scale  with  values  normalized 
between 0 and 1. 

Training

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training (e.g., train, test, validation). However, typically, datasets are divided into three subsets: training, testing, and validation sets. The most common approach is to use a random stratified sampling method to maintain the same distribution of classes in each subset. A typical ratio for splitting the data could be 70% for training, 20% for testing, and 10% for validation. This allows the model to learn patterns from the majority of the data during training, evaluate its generalization capabilities on unseen data during testing, and fine-tune hyperparameters or prevent overfitting during validation.
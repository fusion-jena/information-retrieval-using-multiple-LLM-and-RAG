Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Owing to the insufficient clarity of underwater optical images, gen-
eral deep learning networks have difficulty capturing accurate feature 
information and cannot detect objects correctly. The original YOLOv8 
network  uses  the  C2f  module  for  feature  extraction.  However,  this 
module employs an split  operation on features after convolution. This 
operation divides the input data into two non-overlapping parts, making 
it  difficult  to  obtain  comprehensive  and  rich  feature  information 
through half of the channel’s features. Specifically, the shape of features 
F obtained after convolution is ℝN×H×W×C, where N represents the batch 
size, C represents the channel number, and H and W denote the height 
and width, respectively. The split  operation evenly divides F  into two 
parts on the channel dimension with shapes of ℝN×H×W×0.5C, and these 
two parts do not overlap. The original YOLOv8 convolves only half of

However,  using  large  kernel  convolutions  usually  leads  to  over-
whelming computational complexity, resulting in a sharp increase in the 
number  of  network  parameters  and  network  computational  load.  To 
overcome this issue, we used dilated convolution (Yu and Koltun, 2016) 
and depthwise separable convolution to reduce computations. Dilated 
convolution can increase the receptive field of the convolution kernel by 
inserting “holes”  without adding parameters or increasing the compu-
tational load. The equation for calculating the receptive field of dilated 
convolution  is  shown  in  Eq.  (4),  where  n  represents  the  size  of  the 
receptive field and d represents the dilation rate. 

EcologicalInformatics82(2024)1026806H. Zhou et al.

Fig.  5(a)  shows  the  flowchart  of  the  CSMB  module  for  processing 
underwater optical image feature maps, where the split operation of the 
C2f module has been removed and replaced by two independent con-
volutions.  Thus,  two  different  convolutional  computations  are  per-
formed  on  a  feature  map.  Then,  the  results  of  these  convolutions  are 
concatenated  with  the  results  processed  by  DarknetBottleneck  (DB). 
This method can comprehensively learn the shape, texture, and color 
features of underwater objects and extract background information from 
images. The final result consists of three non-interference feature maps. 
The CSMB module enables more effective feature extraction and analysis 
of features from poor-quality underwater optical images. By performing 
convolution operations on three different channels of a complete feature 
map,  we  can  significantly  enhance  the  performance  of  the  model  in

enriching  the  semantic  representation  of  underwater  objects.  DP- 
FishNet  (Liu  et  al.,  2024b)  combines  the  strengths  of  pyramid  vision 
transformers  and  CNNs  in a  dual-path  approach to  effectively extract 
both global and local features from underwater images, which signifi-
cantly  improves  the  network  performance.  However,  these  methods 
generally  require  a  significant  amount  of  underwater  image  data  for 
training and have high computational costs, thus demanding substantial 
computational resources.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it appears that the deep learning pipeline primarily deals with underwater optical images. These images serve as the primary data format used in the described deep learning network. While other data formats such as audio, video, or CSV files may be relevant in various deep learning pipelines, they are not explicitly mentioned within the provided context. Therefore, based solely on the available information, one could infer that the main data format utilized in this specific deep learning pipeline is underwater optical images.
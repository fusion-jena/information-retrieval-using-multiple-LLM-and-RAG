Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2. Model training: We process data with a deep neural
network and adjust the training epoch, batch size, and
volume size to output a classiﬁer for prediction.
3. Result output: The point cloud test set is segmented,
and we can obtain the ﬁnal segmentation results. The
output ﬁles include point cloud geometric partition
graphs, SPGs, and segmentation results.

Results and Discussion

overall accuracy (OA) as metrics. We adjusted three
parameters (voxel size is the voxelization subsampling
size, the regularization strength inﬂuences the coarseness
of the minimal partition, and batch size means the batch
number of point cloud samples in training). We used the
hold-out method to distinguish the test set and the training
set at a ratio of 3:7 for preprocessing the original data. We
further adjust the parameters of networks to explore the

inﬂuence of parameters on experimental results, and we
discuss the selection of the frame parameters.

Selection of the Frame Parameters

We investigated the effects of several parameters in our
framework, such as the regularization strength, voxel size
for the minimal partition, and training batch size. In

123

Journal of the Indian Society of Remote Sensing (September 2021) 49(9):2163–2172

2169

Fig. 5 Part of segmentation experimental results. a Geometric features, b Geometric partition, c SPG, and d Segmentation result

123

123

2170

Journal of the Indian Society of Remote Sensing (September 2021) 49(9):2163–2172

training, we set the learning decay rate to 0.7 so that the
learning rate gradually decays as the training progresses.
Table 1 shows the experimental results in which we ﬁxed
the voxel size and batch size and adjusted the regulariza-
tion strength, from which we can be told that result is better
while regularization strength is 0.2.

2165

Fig. 1 The pipeline of the SPG framework. a Represents the original
input data (taking a tree and a pole as examples). b Is a hand-crafted
point cloud partition, c each partition is converted into a superpoint of

the SPG, and d is the network for superpoint embedding and
segmentation based on PointNet and GRU. The black lines represent
the superedges

with K ¼ P3
adjacency:

i¼1 ki,

E ¼ (cid:3)

3
X

i¼1

ki
K

log

(cid:2) (cid:3)
ki
K

in accordance with the optimal

ð1Þ

The linearity, planarity, and scattering are derived by the

following terms:

linearity ¼

planrity ¼

scattering ¼

k1 (cid:3) k2
k1
k2 (cid:3) k3
k1

k3
k1

ð2Þ

The linearity describes how elongated the adjacency is,
the planarity assesses how plane the adjacency is, the high-
scattering values are used to represent an isotropic and
spherical adjacency. We describe these three point features
as geometric features.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To ensure the generalizability of their deep learning model, the authors employed the hold-out method to separate the test set and the training set at a ratio of 3:7 for preprocessing the original data. This approach helps prevent overfitting and allows the model to perform well on unseen data. Additionally, they further adjusted the parameters of the networks to investigate the influence of parameters on experimental results and discuss the selection of frame parameters. However, it should be noted that while this method does help improve the model's performance, other techniques like cross-validation or stratified splitting could have been used to further enhance its robustness and reliability.

Answer:::

Based on the given information, the authors took steps to ensure the generalizability of their deep learning model by using the hold-out method to split the dataset into a training set and a test set at a ratio of 3:7. This technique helps prevent overfitting and enables the model to perform well on new, unseen data. Furthermore, they explored different parameter settings within their framework, including the regularization strength, voxel size for minimal partition, and training batch size, to understand their impact on the experimental outcomes. They also discussed the selection of frame parameters. While these methods contribute to improving the model's performance, additional techniques such as cross-validation or stratified splitting could have been utilized to further strengthen the model's robustness and dependability.
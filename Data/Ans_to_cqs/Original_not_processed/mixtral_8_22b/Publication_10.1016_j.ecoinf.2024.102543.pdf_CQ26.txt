Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During  the  preprocessing  phase,  techniques  like  auto-orientation, 
resizing,  tiling,  and  filtering  were  utilized  to  standardize  and  enrich 
the dataset. These steps ensured that the images were consistently ori-
ented,  had  a  uniform  size  of  640  × 640  pixels,  and  contained  a  sub-
stantial  amount  of  annotated  data,  meeting  the  90%  annotation 

threshold. Augmentation played a crucial role in further diversifying the 
dataset. By introducing random saturation adjustments to each training 
example, the number of outputs per example was increased to two. This 
augmentation  strategy 
intensity, 
contributing to a more comprehensive and robust training dataset. 

introduced  variations 

in  color

introduced  variations 

in  color 

The  training  set,  comprising  the  majority  of  the  data  (82%),  con-
taining 4000 images, is used to train the model and adjust its parame-
ters, allowing it to learn from a diverse range of examples and patterns in 
the data. The validation set (13%), consisting of 607 images, is utilized 
during  training  to  fine-tune  hyperparameters  and  assess  the  model's 
performance on unseen data, helping to prevent overfitting and ensuring 
generalization. Lastly, the testing set (5%), which included 260 images, 
serves  as  an  independent  evaluation  of  the  model's  performance  on 
completely unseen data, providing a reliable measure of its real-world 
effectiveness and ability to generalize. 

2.3. Performance evaluation 

To evaluate the ablated model results, we use five metrics, namely 
Precision (Eq. (1)), Recall (Eq. (2)), mAP0.5 and mAP0.5:0.95, related to 
Eq. (3), Params(M), Flops(G), Inference(ms) and Time(h). 

Precision =

It is worth noting, however, that increasing the network depth also 
comes with some potential downsides, such as increased computational 
complexity and a greater risk of overfitting the model to the training 
data. So, adding the P2 layer to the YOLOv8 architecture is a tradeoff 
that needs to be carefully balanced with other considerations, such as 
model size and performance requirements. In this sense, we proposed in 
this work a modification in the C2f layer due to its importance in the 
general architecture of YOLOv8, in the sense of making it lighter and 
counterbalancing  the  increase  obtained  with  the  addition  of  the  P2 
feature level layer. In our work, we call this new layer C2f2.

mAP =

1
k

∑k

i=1

APi

(3) 

We  also  consider  Params  and  FLOPs  to  evaluate  the  model.  The 
“Params”  metric  in  a  neural  network  model  refers  to  the  number  of 
learned variables used for making predictions. It is an essential metric 
for  evaluating  the  model's  complexity  and  computational  efficiency. 
Models  with  more  parameters  generally  require  more  resources  for

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In the given context, several techniques are mentioned for preprocessing the dataset during the deep learning pipeline. However, none of these techniques directly addresses data bias. Instead, they focus on standardizing and enriching the dataset.

Techniques like auto-orientation, resizing, tiling, and filtering are employed to ensure consistent orientation, uniform size, and substantial annotated data in the images. Auto-orientation ensures that all images have the same orientation, while resizing scales them to a uniform size of 640 x 640 pixels. Tiling and filtering help in improving the quality of the images and removing any irrelevant information.

Augmentation plays a significant role in diversifying the dataset by introducing random saturation adjustments to each training example. This increases the number of outputs per example to two, contributing to a more comprehensive and robust training dataset.

However, there is no mention of specific techniques to address data bias, such as stratified splitting, oversampling, undersampling, or diverse data collection. Therefore, based on the provided context, I cannot provide a helpful answer regarding the techniques used to address data bias during preprocessing of the deep learning pipeline.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

24.36 
0.885 

15.25 
0.656 

25.59 
0.893  

EcologicalInformatics81(2024)1026319A.B. Bakht et al.                                                                                                                                                                                                                                

Table 2 
Quantitative  comparison  on  C-60  dataset  using  UIQM,  UCIQE,  and  NIQE 
metrics.   

UIQM↑ 

UCIQE↑ 

NIQE↓ 

Raw_Images 
Fusion 
IBLA 
U-Transformer 
Funie-GAN 
Water-Net 
UWCNN 
Ours 

1.99 
2.78 
1.81 
2.65 
3.10 
2.57 
2.25 
3.12 

0.478 
0.512 
0.574 
0.534 
0.572 
0.578 
0.466 
0.591 

5.18 
4.74 
5.02 
4.94 
4.73 
4.88 
4.89 
4.67  

Table 3 
Quantitative comparison on U45 dataset using UIQM, UCIQE, and NIQE metrics.   

UIQM↑ 

UCIQE↑ 

Raw_Images 
Fusion 
IBLA 
U-Transformer 
Funie-GAN 
Water-Net 
UWCNN 
Ours 

2.44 
3.14 
1.60 
3.10 
3.21 
3.18 
2.82 
3.23 

0.481 
0.532 
0.579 
0.553 
0.602 
0.587 
0.471 
0.612 

NIQE↓

Table 4 
Quantitative  comparison  on  UCCS  dataset  using  UIQM,  UCIQE,  and  NIQE 
metrics.   

UIQM↑ 

UCIQE↑ 

NIQE↓ 

Raw_Images 
Fusion 
IBLA 
U-Transformer 
Funie-GAN 
Water-Net 
UWCNN 
Ours 

2.29 
2.99 
2.36 
3.02 
3.05 
3.13 
2.78 
3.17 

0.410 
0.476 
0.480 
0.539 
0.558 
0.550 
0.455 
0.568 

4.57 
4.38 
4.29 
3.96 
4.37 
4.07 
4.12 
4.03  

A quantitative analysis of this dataset is given in Table 5, confirming 
our  model's  effectiveness.  Our  method  has  achieved  a  UIQM  score  of 
3.26 and an NIQE score of 5.39, followed by the Funie-GAN in terms of 
UIQM with a score of 3.21 and Fusion in terms of NIQE with a score of 
5.40. While measuring the UCIQE score, IBLA achieved a score of 0.626, 
although their resultant images are just color deviations from the orig-
inal images. However, since UCIQE measures a linear combination of 
chroma, saturation, and contrast, a higher value can be expected just by 
having a change in colors (even if they are not desirable).

lationships among features. Unlike channel attention, which emphasizes 
‘what’ is meaningful within an input, spatial attention is concerned with 
‘where’ the informative regions are located, offering a complementary 
perspective. To create the spatial attention map, the SA branch begins by 
separately applying global average pooling and max pooling operations 
along the channel dimensions of the feature map F as give by eq.2 and 
eq.3. The global average pooling is expressed as:

3.3. Loss Functions 

In this work, we used a linear combination of the adversarial loss 

function Ladv, L1  loss function, and the perceptual loss function Lper. 

Ltotal = Ladv + λ1Lper + λ2L1

(4)  

(2) 

(3) 

where λ1  and λ2  are scaling factors set to 7 and 3, respectively. 

Adversarial Loss: We used adversarial loss to facilitate the training 
of the generator network and the discriminator network in a competitive 
manner. This loss function encourages the generator to minimize the log 
probability  that  the  discriminator  assigns  to  the  generated  samples, 

Fig. 4. PatchGAN Discriminator: It processes the image patch-wise, and each block in the output label map matrix shows whether the corresponding image patch is 
real or fake.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The strategy implemented to monitor the model performance during training involves using a linear combination of three different types of loss functions - the adversarial loss function, the L1 loss function, and the perceptual loss function. These loss functions help in facilitating the training of both the generator and discriminator networks in a competitive manner. The total loss function is calculated as a sum of these individual losses, where each loss term is scaled by its respective weight factor. The adversarial loss function encourages the generator to minimize the probability assigned by the discriminator to the generated samples. The L1 loss function helps in minimizing the absolute difference between the predicted and actual values, while the perceptual loss function focuses on preserving high-level features such as edges and textures in the generated images. By monitoring the overall loss during training, one can assess the performance of the model and adjust the learning rate or other hyperparameters accordingly.
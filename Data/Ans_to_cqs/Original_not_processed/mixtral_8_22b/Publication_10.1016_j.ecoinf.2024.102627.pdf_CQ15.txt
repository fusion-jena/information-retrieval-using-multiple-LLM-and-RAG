Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(cid:0)

̂ηs(x) = exp
exp

∑

j

)

θ(x)
f s
(
f j
θ(x)

)

where θ is the set of parameters of the neural network to be optimised by 
minimizing the loss function of Eq. (2). 

Using this very common deep learning framework, it is possible to 
show that the species assemblage predictor Sλ(x) of Eq. (1) is consistent 
λ(x) when  the 
(Lorieul,  2020),  i.e.  it  tends  towards  the  optimal  set  S*
number  of  training  samples  increases.  In  other  words,  our  species 
assemblage predictor is as simple as training a deep neural network with 
a cross-entropy loss function on the presence-only samples and thresh-
olding  the  output  softmax  probabilities  to  get  the  assemblage  of  pre-
dicted species. 

◦

Finally,  we  can  chose  λ  so  as  to  minimize  the  average  species 
assemblage size E[|Sλ(X) |] - which is equivalent to maximize λ  - while 
guarantying that the average error rate is lower than an ϵ objective: 

argmin
λ∈[0,1]

E[|Sλ(X) |]

max
λ∈[0,1]

(λ)

⇔

s.t. ℰ(Sλ) ≤ ϵ

s.t. ℰ(Sλ) ≤ ϵ

(3) 

This is equivalent to what is called conformal prediction in machine 
learning (Fontana et al., 2023) and guarantees that the actual species is 
contained within the set with probability 1 (cid:0) ϵ. 

2.3. Conservation indices for species assemblages 

2.3.1. Definition of the indices 

In addition to the classical Shannon index ℐH , we define two novel 
indices characterizing the extinction risk of a predicted species assem-
blage, ℐc and ℐO . They respectively render the proportion of threat-
ened species in the assemblage and the most critical IUCN status in the 
assemblage. Let’s break down their construction. 

2.3.2.

IUCN status notations

(IDRIS). A full description of the model architecture, dataset spatial split 
and training procedure can be found in supplementary information (SI) 
Box A. Finally, the settings of our species assemblage model are sum-
marised in the Fig. 1.

2.2.1. Definition 

The objective is to optimise a model returning likely species assem-
blages worldwide while being learned on a set of presence-only obser-
vations. To do so, we optimise a deep species distribution model (Botella 
et  al.,  2018b)  and  further  calibrate  it  to  return  species  assemblages 
including the initial species observed with very high confidence. This 
method is derived from what is called set-valued prediction (or set-valued 
classification) in the machine learning community (Chzhen et al., 2021; 
Mortier et al., 2021). The model is trained on presence-only data, all 
species combined (multi-species SDM), and is then used to predict a set 
of  labels  by  thresholding  the  SDM  output  categorical  probabilities 
associated to species.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, it appears that the authors use a deep learning framework to train a species distribution model (SDM) for multi-species predictions. However, the specific optimization technique employed during the training process is not explicitly mentioned. Commonly used optimization techniques in deep learning include Stochastic Gradient Descent (SGD) and its variants like Adaptive Moment Estimation (Adam), but without explicit mention in the text, it cannot be definitively stated which one was used in this case. Therefore, based solely on the provided context, the exact optimization technique remains unknown.
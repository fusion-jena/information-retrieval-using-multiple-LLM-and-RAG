Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

identified by both textual and visual analysis, we could achieve almost 
perfect  precision  (0.992),  at  the  cost  of  lower  recall.  Thirdly,  by 
combining the two approaches, we increased the extracted data volume 
by almost 14%, while still downsamping the original dataset by around 
99.5% and with a precision 0.636. Recall for the workflow overall is not 
known, since this would require manual analysis of over 600,000 posts. 
Our results thus go further than previous work by demonstrating how 
images  retrieved  using  text  and  image  metadata  can  be  combined  to 
achieve  very  high  precision,  or  merged  to  increase  recall  while  still 
filtering initial datasets very effectively. To better understand our results 
and their transferability to other species, we looked more closely at data 
quality.  Flickr  users  often  used  relevant  textual  descriptions  to  label 
their  red  kite  sightings  with  their  captured  images.  This  high  textual

950 (4%) 
1110 (4%) 

Total 

24,675 
32,986  

passes over the entire training set) and a batch size of 64, with an ADAM 
(cid:0) 5. Image augmentation was applied 
optimiser and a learning rate of 10
in the form of horizontal flip, 0.2 degree counter clock wise shear and a 
random zoom between 0 and 0.2 - all leading to 224 Ã— 244 pixel RGB 
input tensors. Data was normalised to ImageNet mean values, and the 
pixels values were rescaled in the range of [0, 1]. Model training took 
roughly 4 days. The best model was selected based on minimal valida-
tion loss that occurred at epoch 448. This model showed a training loss 
of 0.256, a training accuracy of 0.899, a validation loss of 0.298 and 
validation accuracy of 0.891. We evaluated the red kite model perfor-
mance based on an independent test set of 2060 images (as described in 
3.3). 950 of these images were true positive red kites images and the

quality  of  extracted  information  in  isolation  through  metrics  such  as 
precision and recall, but do not explore the added value of integrating 
data. These gaps lead to three research questions, which we address in 
what follows. The first two questions relate to the development of our 
workflow:  

1.  How can a generic workflow be developed which leverages expert 
contributions from citizen science data to extract content from social 
media posts related to a given species, which can easily be used in 
practical applications?  

2.  How can multiple modalities (e.g. text and images) be used to extract 
relevant information and does such a combination result in higher 
recall and/or precision? 

The third research question relates to the added value of our work-

flow in integrating data from different sources. 

3. What added value with respect to coverage, type, overlap and vol-
ume can we identify using a combination of citizen science and social 
media data?

images.  We  found  that  if  we  included  only  Flickr  posts  identified  by 
textual and image-based information, that with the retained candidate 
records we can achieve an almost perfect precision of 0.992.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

To evaluate the performance of the deep learning model, several metrics are used. In this case, the authors use training loss, training accuracy, validation loss, and validation accuracy. Training loss refers to the error made by the model during its training phase, while training accuracy measures the proportion of correct predictions out of total predictions made during training. Similarly, validation loss indicates the error made by the model when validated against unseen data, and validation accuracy shows the percentage of accurate predictions made during validation.

In addition to these metrics, the authors also mention precision and recall. Precision is calculated as the ratio of correctly predicted positive observations to the total predicted positives, whereas recall is computed as the ratio of correctly predicted positive observations to the actual number of positives. However, it should be noted that the recall for the workflow overall is not known due to the large amount of data required for manual analysis.

Lastly, the authors discuss the concept of data quality, emphasizing the importance of relevant textual descriptions and labels accompanying images uploaded by Flickr users. High textual quality contributes to improved extraction of information and better integration of data from various sources.
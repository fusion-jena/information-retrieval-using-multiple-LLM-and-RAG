Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The batch size was 32, and the early stopping procedure was used to 
avoid over-fitting. The patience parameter, which controls the number 
of epochs without improvements in the validation loss, was set to 10. 
Each network, with a particular architecture and considering a specific 
substratum characterization, was trained and executed three times, each 
time with a different (random) initialization of the trainable parameters 
and with a different data fold. As already mentioned, the results shown 
in  the  next  section  are  averages  of  those  three  executions.  Data 
augmentation was applied to all extracted patches: a 90∘  rotation and 
vertical and horizontal flips. 

4.4. Performance metrics

The  experimental  results  suggest  that  there  might  be  room  to 
improve the generalization capacity of the deep learning classifiers. That 
could  be  achieved  by  better  exploiting  the  training  data,  e.g.,  with 
additional data augmentation techniques, using recent advances in un-
supervised learning techniques, such as self-supervised methods, or by 
just simplifying the classifiers in terms of the number of parameters, thus 
reducing the risk of overfitting. 

Another direction for continuing this research is to exploit the clas-
sification uncertainty further in an active learning context. The uncer-
tainty measure could be used in interactively training the deep learning 
models,  selecting  high-uncertainty  samples  in  the  datasets,  and 
increasing their importance in the computation of the loss function. 

CRediT authorship contribution statement

Variants  of  the  VGG  network  have  demonstrated  that  increasing 
network  depth  can  improve  classification  accuracy  by  enabling  the 
learning  of  semantically  enriched  features.  In  the  present  work,  we 
adopted a particular architecture inspired by VGG that has 14 layers (13 
convolutional  and  one  dense  layer).  A  detailed  description  of  the  ar-
chitecture implemented in this work can be found in Section 7. 

3.2. ResNet

3.1. VGG 

To  date,  the  VGG  (Visual  Geometry  Group)  network  is  one  of  the 
most  popular  image  classification  architectures,  and  pre-trained  VGG 
models  are  commonly  used  in  transfer  learning  (e.g.,  fine-tuning) 
schemes.  It  was  proposed  in  Simonyan  and  Zisserman  (2015),  which 
aimed at investigating the effects of increasing convolutional network 
depth.  Evaluated  on  the  ImageNet  Large  Scale  Visual  Recognition 
Challenge  (ILSVRC)  (Russakovsky  et  al.,  2015)  in  2015,  the  VGG16 
model outperformed all previous participants, which comprised several 
state-of-the-art architectures.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Data augmentation techniques were indeed applied in the deep learning pipeline. Specifically, the text mentions that a 90° rotation and vertical and horizontal flips were used. These techniques help increase the diversity of the dataset and prevent overfitting by creating new instances from existing ones through various transformations. However, it does not explicitly mention scaling as a technique employed in this case.
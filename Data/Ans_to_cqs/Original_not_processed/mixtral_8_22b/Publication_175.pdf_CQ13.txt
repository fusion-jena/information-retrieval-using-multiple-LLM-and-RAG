Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

1)  We  trained  a  classic  DL  algorithms  architecture  with  our  biggest 

dataset AT0 as a baseline for the DL accuracy;  

2)  We  trained  the  same  DL  architecture  with  the  same  hyper- 
parameters (e.g. model architecture and training process) but on a 
much  more  limited  dataset  (AT1).  Hyper-parameters  are  the  pa-
rameters  defining  the  architecture (number of  layers, number and 
size of convolutions, connections between layers) and the training 
process  of  a  Deep  Model  (learning  rate,  neurone  activation,  back- 
propagation compotation).;  

3)  We trained the same DL architecture with limited datasets obtained 
by  subsampling  T0  to  250  and  500  images  per  class  (here  after 
“species” when we are referring to our experiments), corresponding 
to 2500 and 5000 thumbnails in AT0;

2)  adapt  this  model  to  a  new  task  with  few  examples,  while  not 
forgetting  the  concepts  learned  previously  (Gidaris  et  al.,  2018;  Har-
iharan et al., 2017). Finally, optimization-based methods are designed to 
adapt quickly to new tasks, hence able to learn a classification task with 
few examples (Finn et al., 2017; Nichol and Schulman, 2018; Sun and 
Chua, 2018). Optimization-based algorithms showed promising results 
in  deep  learning  few-shot  classification  (Finn  et  al.,  2017;  Jamal  and 
Cloud, 2019; Wang et al., 2019). Such methods propose to pre-train (or 
“meta-train”)  a  model  with  existing  databases  (e.g.  MiniImageNet 
(Russakovsky et al., 2015), Ominglot (Lake et al., 2019)) on different 
tasks so it can adapt easily to a new one. For object identification, a task 
is defined by the classes the model has to discriminate. Once this model, 
called “meta-model” has been trained, it can then be tuned to operate on

Comput. Vis. 211–252. 

Salman, A., et al., 2016. Oceanography : Methods Fish Species Classification in 

Unconstrained Underwater Environments Based on Deep Learning, pp. 570–585. 

Shiu, Y., et al., 2020. Deep Neural Networks For Automated Detection of Marine 

Mammal Species, pp. 1–12. 

Stuart-smith, R.D., et al., 2013. Integrating abundance and functional traits reveals new 

global hotspots of fish diversity. Nature 501 (7468), 539–542. 

Sun, Q., Chua, Y.L.T., 2018. Meta-transfer learning for few-shot learning. Conf. Comput. 

Vis. Pattern Recognit. 403–412.

a new task with a very limited dataset, usually only 1–5 thumbnails per 
class.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The hyperparameters used in the deep learning model include parameters defining the architecture such as the number of layers, number and size of convolutions, and connections between layers. They also include parameters related to the training process like the learning rate, neuron activation, and back-propagation computation. However, specific values or details about these hyperparameters were not mentioned in the given context.
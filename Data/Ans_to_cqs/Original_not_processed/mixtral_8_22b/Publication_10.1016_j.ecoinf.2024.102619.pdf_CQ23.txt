Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 5 
Classifier  training  performance:  with  tuned  hyperparameters,  mean  cross- 
validation (CV) and final training accuracy.      

illumination patterns. 

3.2. Classification performance 

Set 

Images 

Classifier 

Hyperparameters 

Mean CV 

Train 

Accuracy 

1 

6682 

2 

992 

3 

459 

SVM:Lineard 
SVM:Linear 
SVM:RBFd 
SVM:RBF 
CNN 
SVM:Lineard 
SVM:Linear 
SVM:RBFd 
SVM:RBF 
CNN 
SVM:Lineard 
SVM:Linear 
SVM:RBFd 
SVM:RBF 
CNN 

C = 23.0 

(cid:0)

– 

(cid:0) 15.0 

C = 23.0, γ = 2
lr = 0.001 
– 

(cid:0) 15.0 

C = 23.0 

– 

C = 23.75, γ = 2
lr = 0.001 
– 

C = 23.0 

– 

C = 25.25, γ = 2
lr = 0.001 

(cid:0) 15.0 

0.93 (± 0.003) 
0.93 (± 0.003) 
0.95 (± 0.003) 
0.96 (± 0.003) 
0.92 (± 0.023) 
0.87 (± 0.010) 
0.87 (± 0.010) 
0.87 (± 0.018) 
0.91 (± 0.016) 
0.90 (± 0.015) 
0.83 (± 0.021) 
0.83 (± 0.021) 
0.78 (± 0.016) 
0.86 (± 0.020) 
0.82 (± 0.020) 

1.00 
1.00 
0.97 
0.98 
0.98 
1.00 
1.00 
0.91 
0.97 
0.97 
1.00 
1.00 
0.84 
0.95 
0.95

2.2.5. Classification 

Each  ML  approach  requires  hyperparameters  to  classify  imagery, 
which when optimized during training can increase model performance, 
see  Table  4  for  a  hyperparameter  glossary.  Given  the  computational 
efficiency of the SVMs and the few hyperparameters required, each of 
these can be optimized simply and relatively quickly (subject to dataset 
size)  during  a  k-fold  (k = 5) cross-validated  fine  grid-search  on  the 
training  data.  For  our  CNN  þ SVM  method,  we  followed  hyper-
parameter  recommendations  by  (Hsu  et  al.,  2016),  authors  of  the 
LIBSVM library (Chang and Lin, 2011). For our non-linear RBF SVM we 
searched  hyperparameters  C = 23, 23.25, …, 27  and  γ = 2
(cid:0) 13  & 
(cid:0) 11. For the linear SVM, we used the same hyperparameter search for 
2
its sole parameter C. We also looked at the RBF and linear SVM with 

(cid:0) 15, 2

Fig. 3. A diagram of various support vector machines.

Learning 
rate 

Optimizer 

SVM: 
C 

γ 

The number of images you send to the model in each iteration. 
Model parameters are updated after each batch during training. 
How many times you pass the full image dataset through the model. 
The error metric that you wish to minimize. 
e.g. Cross entropy loss for multi-class classification. 
A small number (0, 1] that determines the amount to alter parameters 
during training with respect to the loss. 
Also known as the step size. 
An algorithm that modifies CNN parameters according to a particular 
strategy to minimize the loss. 
e.g. the Adam optimizer sets the learning rate adaptively for faster and 
more efficient training.

2.2.3. Model preparation 

The VGG16 network was sourced from the torch library and all layers 
frozen, preventing any further training (updates to model parameters), 
see Table 3. We then duplicated this network to provide a foundation for 
each modelling approach. For our CNN þ SVM modelling approach we 
kept the architecture up to the first FC layer (FC1), creating a feature 
extractor (Fig. 2). We then paired it with an SVM, sourced from the scikit- 
learn library. We evaluate two types of SVM: a linear SVM and a non- 
linear SVM, known as a Radial Basis Function (RBF) (Fig. 3). An RBF 
SVM is a good default choice, as it can find both a linear and non-linear 
hyperplanes at high dimensions. 

For  the  CNN  approach  however,  we  use  the  full  VGG16  network, 
leaving the feature extractor and classifier intact. In its frozen state the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not explicitly mention specific measures taken to ensure the generalizability of the deep learning model. However, there are some indications that suggest certain practices were employed.

Firstly, the table shows different sets of images used for training, which could imply that a diverse dataset was utilized. This diversity might have been achieved through variations in illumination patterns or other factors not mentioned in the given context. Using a diverse dataset helps improve the model's ability to generalize well to unseen data.

Secondly, the text mentions that each machine learning approach requires hyperparameters to classify imagery, which when optimized during training can increase model performance. Hyperparameters are crucial in ensuring that the model performs well on new, unseen data. By optimizing these hyperparameters using techniques like cross-validation, one can prevent overfitting and enhance the model's generalization capabilities.

Lastly, the text discusses the use of Support Vector Machines (SVMs) and Convolutional Neural Networks (CNNs) for modeling approaches. Both methods are known for their robustness and ability to handle complex datasets, contributing to better generalization. Furthermore, the use of a pre-trained VGG16 network suggests transfer learning, which allows models to leverage knowledge gained from solving similar tasks, thereby improving their capacity to generalize.

In conclusion, while the text does not directly address the measures taken to ensure the generalizability of the deep learning model, it provides hints about potential strategies such as employing a diverse dataset, optimizing hyperparameters, and utilizing robust modeling approaches like SVMs and CNNs.
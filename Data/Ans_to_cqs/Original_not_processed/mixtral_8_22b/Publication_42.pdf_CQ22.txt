Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Sensors 2021, 21, 343

11 of 18

7,047,754 learnable parameters gave a F1-score of 84.93% which is even lower. CNN
architectures with many parameters (more than 20,000,000) such as ResNetV50 [38] and
InceptionNetV3 [39] gave a high training accuracy, but a lower validation F1-score of 69.1%
and 81.7%, respectively. This result indicates overﬁtting and that more training data are
needed when such large deep learning networks are used. A very high F1-score of 96.6%
was ﬁnally achieved by transfer learning on ResNetV50 using pretrained weights and only
training the output layers. This indicates that the state-of-the-art was able to outperform
our proposed model, but requires pretrained weights with many more parameters.

2.2.4. Summary Statistics

The chosen model shown in Figure 5 had an F1-score of 92.75%, which indicated that
the trained CNN was very accurate in its predictions. This ﬁnal architecture was chosen
because it achieved average precision, recall, and an F1-score of 93%, which indicated a
suitable model classiﬁcation.

Sensors 2021, 21, 343

10 of 18

Table 2. Ranking of the CNN architectures with highest and lowest F1 classiﬁcation scores. Rank 1 to
32 were trained using the Adam optimizer. Rank 33 to 64 were trained using the SGD optimizer. The
hyperparameters column shows values of {kernel size layer 1, kernel size last layer, convolutional
depth layer 1, convolutional depth last layer, fully connected size}.

Rating

Hyperparameters

Learnable
Parameters

F1/-Score

1.
2.
3.
4.
5.
...
31.
32.

33.
34.
...
62.
63.
64.

3, 3, 32, 128, 512
5, 1, 32, 128, 512
5, 3, 32, 64, 512
3, 3, 32, 64, 512
5, 3, 32, 128, 512

5, 1, 64, 64, 512
5, 3, 32, 32, 512

5, 3, 64, 128, 512
5, 3, 32, 128, 512

36. Tan, M.; Le, Q.V. EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks. arXiv 2019, arXiv:1905.11946,
37. Huang, G.; Liu, Z.; Weinberger, K.Q. Densely Connected Convolutional Networks. arXiv 2016, arXiv:1608.06993,
38. Wu, S.; Zhong, S.; Liu, Y. ResNet. Multimed. Tools Appl. 2017. [CrossRef]
39.

Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; Wojna, Z. Rethinking the Inception Architecture for Computer Vision.
In
Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 26 June–1
July 2016; pp. 2818–2826.

40. R Core Team. R: A Language and Environment for Statistical Computing; R Core Team: Geneva, Switzerland, 2020. Available online:

http://softlibre.unizar.es/manuales/aplicaciones/r/fullrefman.pdf (accessed on 9 November 2020).

Finally, the customized CNN architectures were compared with selected state-of-
the-art CNN optimized architectures. EfﬁcientNetB0 [36] is scaled to work with a small
image input size of 224 × 224 pixel and has 4,030,358 learnable parameters. Using the
moths dataset with the same data augmentation, the EfﬁcientNetB0 achieved a F1-score
of 88.62%, which is lower than our top ﬁve best architectures. DenceNet121 [37] with

Sensors 2021, 21, 343

11 of 18

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The performance of the deep learning models in this study is evaluated primarily using the F1-score metric. The F1-score is a measure of a test's accuracy that considers both the precision and recall of the test to compute the score. Precision is the number of true positive results divided by the number of all positive results returned by the classifier, while recall is the number of true positive results divided by the number of positive samples in the dataset. Additionally, other metrics like average precision, recall, and F1-scores are also mentioned to indicate suitable model classification. However, no specific mention of accuracy as a standalone metric is made in the given context.
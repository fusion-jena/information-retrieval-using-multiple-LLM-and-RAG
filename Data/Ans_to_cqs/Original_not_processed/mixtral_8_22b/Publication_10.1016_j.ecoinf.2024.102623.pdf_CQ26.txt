Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

diverse datasets. This includes addressing issues such as class imbalance 
and sampling bias.

vancements in deep learning to enhance performance. Specifically, our 
MLP architecture consists of four layers, each containing 512 neurons 
and connected with residual connections (He et al., 2016). We employ 
batch normalization (Ioffe and Szegedy, 2015) and the Rectified Linear 
Unit (ReLU) activation function in all layers except the final one, where 
instead a sigmoid function is used to enable multi-label classification. 
The model is trained with a batch size of 256 for 30 epochs using the 
AdamW optimizer (Loshchilov and Hutter, 2017). Both the weight decay 
and learning rate are set to 0.0001. Additionally, we employ a learning 
rate scheduler with exponential decay of 0.95, and introduce dropout

only data often exhibits biases towards specific areas, and we need to 
assess the modelâ€™s ability to counter them.

2.2.1. Limitations of L full 

Primarily, this loss function fails to adequately address the issue of 
class  imbalance  within  datasets.  Two  key  aspects  of  class  imbalance 
need to be tackled here: 

EcologicalInformatics81(2024)1026233R. Zbinden et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text mentions several techniques used to address data bias during preprocessing of the deep learning pipeline. These include addressing issues such as class imbalance and sampling bias. To tackle class imbalance, two key aspects need to be addressed: ecological fallacy and Simpson's paradox. However, the text does not provide explicit details on how these issues are specifically addressed. Nevertheless, common methods to handle class imbalance include stratified splitting, oversampling, undersampling, and diverse data collection. Stratified splitting involves dividing the dataset into training and testing sets while maintaining the same distribution of classes in both sets. Oversampling increases the number of instances from minority classes by duplicating existing samples or generating synthetic ones. Undersampling reduces the number of instances from majority classes by randomly removing some samples. Lastly, diverse data collection aims to gather more representative data from underrepresented classes. In addition to these techniques, the text also mentions the use of advanced deep learning architectures like MLP with residual connections, batch normalization, ReLU activation functions, and sigmoid functions for multi-label classification. Furthermore, it discusses the use of AdamW optimizers, setting appropriate weight decays, learning rates, and introducing dropouts to improve model performance.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The performance of machine learning models was evaluated by four 
criteria:  coefficient  of  determination  (R2),  NSE  (Nash  and  Sutcliffe, 
1970), Kling–Gupta efficiency (KGE) (Gupta et al., 2009), and correla-
tion coefficient (COR) (Sardoei et al., 2024).

Table 5 
Model  performance statistics  in  predicting groundwater  (GW)  salinity  hazard 
maps for the training and testing datasets by statistical (frequency ratio (FR) and 
statistical index (SI)) and machine learning (Random Forest (RF) and Classifi-
cation and Regression Trees (CART)) models.  

Model 

Statistical models 

Machine learning 

models 

Performance 
statistics 

NSE 
Accuracy 
Sensitivity 
Kappa 
AUC 

NSE 
COR 
KGE 
R2 

Training 

Testing 

FR 

SI 

FR 

SI 

0.85 
0.897 
0.794 
0.794 
0.923 
RF 
0.88 
0.95 
0.79 
0.96 

0.77 
0.867 
0.735 
0.735 
0.905 
CART 
0.69 
0.83 
0.70 
0.89 

0.73 
0.897 
0.794 
0.794 
0.920 
RF 
0.70 
0.85 
0.75 
0.91 

0.68 
0.882 
0.764 
0.764 
0.920   
CART 
0.63 
0.82 
0.75 
0.89  

necessary  to  make  a  fair  judgment  about  how  models  perform  under 
similar conditions. 

3.4. GW salinity vulnerability maps 

3.4.1. Development of standard maps

2.4. Feature selection

Five performance criteria, including Nash–Sutcliffe efficiency (NSE) 
(Nash and Sutcliffe, 1970), accuracy, sensitivity, Kappa (K), and Area 
under  the  Receiver  Operating  Characteristic  (ROC)  Curve  (hereafter 
AUC), were applied for the validation assessment of statistical models (i. 
e., FR and SI models). An AUC value of <0.6 indicates weak accuracy, 
between 0.6 and 0.7 medium accuracies, 0.7 and 0.8 high accuracy, and 
>0.8 very high (Evans et al., 2005). The AUC values of ROC indicate the 
accuracy  of  both  classification  models  (i.e.,  statistical  and  machine 
learning models). The model’s accuracy would be acceptable whenever 
the  under-curve  area exceeds  70%. The  pROC  package of R  Software 
(Ver. 3.5.6) was applied to indicate model accuracy based on ROC in this 
research (Robin et al., 2011).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, it appears that the performance of the machine learning models was monitored using several evaluation metrics such as coefficient of determination (R2), NSE (Nash and Sutcliffe, 1970), Kling–Gupta efficiency (KGE) (Gupta et al., 2009), and correlation coefficient (COR) (Sardoei et al., 2024). These metrics are used to assess the quality of predictions made by the models during the training phase.

In Table 5, we can see that the performance statistics for the training dataset include NSE, COR, KGE, and R2 for both Random Forest (RF) and Classification and Regression Trees (CART) models. This suggests that these metrics were employed to evaluate the performance of the models during training. However, the context does not provide explicit details regarding the specific strategy or methodology used to implement these metrics for monitoring model performance during training. Therefore, I cannot provide a more detailed explanation of the exact strategy used for monitoring model performance during training.
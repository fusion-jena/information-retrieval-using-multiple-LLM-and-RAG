Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

hyperparameter optimisation process, the performance metrics detailed 
in Section 2.5 are meticulously evaluated using the designated test set. 
Subsequently, based on the performance metrics obtained, we discern 
and  select  the  most  effective  models  for  each  target  variable.  For  an 
illustrative overview of this model selection process, kindly refer to the 
visual representation provided in Fig. 4.

The regression hyperparameter tuning has been performed for all the 
nine prediction models (XGB, RF, SVR, GP, EN, KNN, MLP, Tree and LR), 
and  based  on  that,  the  best  regression  parameter  has  been  used  for 
training the model. 

As illustrated in Fig. 8, the x-axis denotes the sequence of observed 
values, and the y-axis represents the differences between the predicted 
and  observed values. As  can be seen, there  is a  constant trend  in the 
distribution of differences between predicted and observed values as the 
observed values increase on the X-axis. Particularly, the values on the Y- 
axis remain consistently close to 0, both above and below the reference 
line,  indicating  relative  stability  in  the  discrepancies  between  pre-
dictions and actual values across the range of observed values.

Variable  Model  MAE 

MSE 

RMSE  MAPE 

MASE 

Table 4 includes details such as the name of the prediction model, its 
hyperparameters,  the  parameters  selected  during  the  5-fold  cross- 
validation,  and  the  hyperparameters  adjusted  according  to  the  selec-
tion  criteria  to  determine  the  most  appropriate  configuration  for  the 
prediction model. 

3.2. Performance evaluation of H, LE and CO2  prediction models

EN 

KNN 
GP 

TREE 

RF 

XGB 

SVR 

MLP 

alpha 
l1_ratio 
neighbours 
weights 
p 
alpha 
max_depth 
min_samples_split 
min_samples_leaf 
max_features 
n_estimators 
max_depth 
min_samples_split 
min_samples_leaf 
max_features 
n_estimators 
learning_rate 
max_depth 
subsample 
colsample_bytree 
reg_alpha 
gamma 
min_child_weight 
C 
gamma 
epsilon 
hidden_layer_sizes 
activation 
alpha 
learning_rate 
max_iter 

* BP denotes Best Parameter. 

(cid:0) 1] 

(cid:0) 5, 10

(cid:0) 2, 10

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not explicitly state which method was used for hyperparameter optimization, but it mentions that the hyperparameter tuning has been performed for all nine prediction models. This suggests that some form of hyperparameter optimization was indeed carried out. However, without further information, it cannot be definitively stated whether methods like grid search or random search were employed.

Based on the given context, it appears that the hyperparameters were optimized through a process involving the use of performance metrics, a test set, and possibly cross-validation. The performance metrics mentioned in Section 2.5 were likely used to evaluate the effectiveness of different hyperparameter configurations. After evaluating these metrics, the most effective models for each target variable were chosen. It is also noted that a visual representation of this model selection process is available in Figure 4.

Additionally, Table 4 provides more specific details about the hyperparameters used for each prediction model, including those selected during the 5-fold cross-validation and any adjustments made according to the selection criteria. While this table offers valuable insights into the final hyperparameters used for each model, it does not provide explicit information regarding the optimization method itself.

In conclusion, while the text indicates that hyperparameter optimization took place, it does not specify the exact method used, such as grid search or random search.
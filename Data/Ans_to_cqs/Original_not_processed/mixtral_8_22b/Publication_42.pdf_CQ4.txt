Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Sensors 2021, 21, 343

11 of 18

7,047,754 learnable parameters gave a F1-score of 84.93% which is even lower. CNN
architectures with many parameters (more than 20,000,000) such as ResNetV50 [38] and
InceptionNetV3 [39] gave a high training accuracy, but a lower validation F1-score of 69.1%
and 81.7%, respectively. This result indicates overﬁtting and that more training data are
needed when such large deep learning networks are used. A very high F1-score of 96.6%
was ﬁnally achieved by transfer learning on ResNetV50 using pretrained weights and only
training the output layers. This indicates that the state-of-the-art was able to outperform
our proposed model, but requires pretrained weights with many more parameters.

2.2.4. Summary Statistics

Finally, the customized CNN architectures were compared with selected state-of-
the-art CNN optimized architectures. EfﬁcientNetB0 [36] is scaled to work with a small
image input size of 224 × 224 pixel and has 4,030,358 learnable parameters. Using the
moths dataset with the same data augmentation, the EfﬁcientNetB0 achieved a F1-score
of 88.62%, which is lower than our top ﬁve best architectures. DenceNet121 [37] with

Sensors 2021, 21, 343

11 of 18

Total

250
250
250
250
250
250
250
250
250

2250

It was a challenge to obtain a sufﬁcient number of images. Especially, Agrotis puta
and Mythimna pallens had fewer occurrences than the other species. That was the main
reason for the limited number of images (250) for each species. Data augmentation was
therefore applied to all images with a ﬂip vertical, horizontal, zoom, different illumination
intensity and rotation of different degrees. This operation provided more training data and
was used to create a uniform distribution of species. The dataset was scaled with a factor
of 32 times, resulting in 72,000 images, where each class contained 8000 data points after
augmentation. From this dataset, 80% was used for training and 20% for validation of the
CNN model.

36. Tan, M.; Le, Q.V. EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks. arXiv 2019, arXiv:1905.11946,
37. Huang, G.; Liu, Z.; Weinberger, K.Q. Densely Connected Convolutional Networks. arXiv 2016, arXiv:1608.06993,
38. Wu, S.; Zhong, S.; Liu, Y. ResNet. Multimed. Tools Appl. 2017. [CrossRef]
39.

Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; Wojna, Z. Rethinking the Inception Architecture for Computer Vision.
In
Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 26 June–1
July 2016; pp. 2818–2826.

40. R Core Team. R: A Language and Environment for Statistical Computing; R Core Team: Geneva, Switzerland, 2020. Available online:

http://softlibre.unizar.es/manuales/aplicaciones/r/fullrefman.pdf (accessed on 9 November 2020).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The data augmentation techniques applied in the deep learning pipeline include flip vertical, flip horizontal, zoom, different illumination intensity, and rotation of different degrees. These operations were performed to provide more training data and create a uniform distribution of species. As a result, the original dataset was scaled with a factor of 32 times, leading to 72,000 images, where each class contained 8000 data points after augmentation.
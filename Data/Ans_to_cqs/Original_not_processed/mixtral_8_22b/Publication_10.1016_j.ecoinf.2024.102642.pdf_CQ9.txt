Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

directly  to  datasets  where  the  relevance  of  annotations  has  not  been 
evaluated.

To  evaluate  the  performance  of  the  CNNs  trained  on  different 
annotation sets, datasets containing other SEIO PBW vocalizations and 
blue whale’s Dcall were chosen. Only a small portion of those datasets 
were used to reduce computational time, as hundreds of networks were 
trained and compared. 

For the evaluation of the SEIO PBW vocalizations, 7 h of audio sig-
nals recorded during the OHASISBIO program were used [Royer, 2009; 
Torterotot et al., 2020]. These recordings were collected at the SWAMS 
site, located in the Indian Ocean’s oceanic zone, between Kerguelen and 
Amsterdam Island. This dataset is called SWAMS in this paper. The re-
cordings were made in March 2015 using a hydrophone deployed at a 
depth of 1000 m. The sampling rate was 240 Hz. A total of 102 vocali-
zations were manually annotated by a second expert.

During  the  campaign,  methods  are  proposed  to  assess  inter- 
annotator  variability  and  aggregate  annotation  sets.  Those  methods 
will help to define the minimal number of annotators needed [Dubus 
et al., 2023; Walter et al., 2022]. Increasing the number of annotators 
per sample could be helpful if the variability is too high. Finally, the two 
grouping  methods:  majority  voting  (Section  2.2.3)  and  soft  labeling 
(Section 2.3.2) can also be used to produce relevant annotation sets for 
deep learning. 

6. Conclusion 

In this study, new approaches were explored to improve the accuracy 
and  efficiency  of  cetacean  vocalization  detection  using  deep  learning 
models. Leveraging multi-annotation campaigns involving both expert 
and novice annotators aimed to address the challenges associated with 
the scarcity of ground truth data in underwater PAM studies.

platform  is  not  user-friendly,  annotators  will  become  disengaged  and 
cease  annotating  the  files.  As  annotation  campaigns  generally  aim  to 
annotate large datasets, online access to the data is recommended. It is 
necessary for the procedure to be automatic, from the first observation 
of data by the annotators to the gathering of all annotations. Moreover, 
the platform must unify the data representation for all users, in order to 
reduce  the  variability.  To  reduce  the  over-representation  of  the  first 
annotations, it is also important to keep all the annotations independent. 
We also strongly believe that the temporal context is an important factor 
for many underwater sound events: song context, periodic emission of 
vocalization  [Madhusudhana  et  al.,  2021].  Thus,  the  spectrogram 
should be proposed to the annotator in a chronological order, or be long 
enough  to  capture  the  long-term  temporal  context  of  targeted  sound

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The information regarding the availability of the deep learning pipeline's data repository is not explicitly mentioned in the given context. Therefore, I cannot provide specific details about its location on platforms such as Zenodo, Figshare, Dryad, or GBIF. However, the context does mention that the study uses 7 hours of audio signals recorded during the OHASISBIO program, which were collected at the SWAMS site in the Indian Ocean's oceanic zone between Kerguelen and Amsterdam Island. Additionally, the study mentions that only a small portion of these datasets were used due to computational constraints.
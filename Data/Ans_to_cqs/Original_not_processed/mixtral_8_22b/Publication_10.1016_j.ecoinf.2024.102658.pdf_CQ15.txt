Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Following this, forward feature selection (FFS) was performed. FFS is 
a  common  stepwise  selection  procedure  (Hastie  et  al.,  2020),  which 
begins with an empty model, and iteratively selects the next feature from 
the initial set which most improves model performance. Candidate lo-
gistic  regression  and  random  forest  models  were  evaluated  via  (1) 
random cross-validation (CV) and (2) spatial CV (Roberts et al., 2017), 
in which the full training dataset was partitioned into spatial blocks like 
those shown in Fig. 2. 

Because model performance estimated via spatial CV is dependent on 
the spatial configuration of the CV folds, the procedure was repeated 
over 100 random blocking configurations; those features selected most 

Fig.  2. One  spatial-CV  fold  pattern  used  in  feature  selection.  This  particular 
fold was also utilized when calculating numerical scores for the models.

4.2.1. Baseline model: random forest 

Random forest (Breiman, 2001) utilizes an ensemble of classification 
or regression trees; each tree is grown from a bootstrap sample of the 
training dataset, and represents a series of sequential decisions, in which 
each node of the tree is a binary split made on a predictive feature (e.g., 
whether  the  mean  annual  temperature  is  above  25 
C).  Further,  a 
random subset of features is considered when finding the optimal split at 
each node (we use a value of sqrt(n), n being the number of potential 
predictors, for the size of this subset). When used for classification, the 
outputs from the component trees are put through a majority vote to 
create a single output. This is known to be a simple but powerful method 
of retaining the complex non-linearity of decision trees while avoiding 
overfitting. 

◦

Table 1 
The list of features. We utilize two feature sets selected through different CV 
processes  in  a  previous  study  (Mechenich  et  al.,  2024),  which  we  name  the 
‘random-CV’ feature set and the ‘spatial-CV’ feature set, respectively. The two 
feature sets share six common features.  

Feature set 

Feature 
Name 

Source 

Definition 

BIO03 

WorldClim 

random-CV 

TN10P 

ETCCDI 

GSL 

TNX 

ETCCDI 

ETCCDI 

BIO08 

WorldClim 

TXX 

ETCCDI 

spatial-CV 

BIO02 

WorldClim 

TN90P 

ETCCDI 

ID 
BIO14 
BIO18 
CWD 

ETCCDI 
WorldClim 
WorldClim 
ETCCDI 

RX1DAY 

ETCCDI 

WSDI 

ETCCDI 

common for 
both sets

In  our  case,  the  possible  configurations  we  explored  were  these 
modeling  options:  two  sets  of  features  selected  from  a  larger  pool  of 
available features (method in Section 5.2.1), whether or not to scale the 
features, choice of prior strength, and for Bayesian GAM only, the basis 
dimension. For the priors, we were partially able to utilize the priorsense 
package (Kallioinen et al., 2021) to optimize the exploration of different 
configurations.  For  the  rest  of  the  settings  however,  we  kept  to  the 
repeated iterations of the Bayesian workflow. Our workflow results are 
the visualizations and scores presented in Figs. 3, 4, and B.11, and the 
observations are the discussions and analysis in this text: what we can 
interpret from the scores and visualizations and diagnoses of problems, 
if any. 

5.2. Modeling options 

5.2.1. Feature selection

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it does not appear that any specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) are mentioned in relation to a deep learning pipeline. Instead, the context discusses feature selection methods, specifically focusing on Forward Feature Selection (FFS) and its application in conjunction with logistic regression and random forest models. Additionally, the context mentions using the priorsense package to optimize the exploration of different configurations, but this seems unrelated to deep learning optimization techniques. Therefore, based solely on the provided information, I cannot provide details about the optimization techniques applied in a deep learning pipeline.
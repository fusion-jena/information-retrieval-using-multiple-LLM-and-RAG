Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.3.2. Vision transformer (ViT) 

The second model of choice in this paper is the Vision Transformer 
(ViT).  The  original  Transformer  architecture  has  been  widely  used  in 
natural  language  processing  tasks.  This  was  until  Dosovitskiy  et  al. 
(2020) turned it into an accurate model in computer vision applications. 
As shown in Fig. 5b, the Transformer’s attention mechanism is applied 
directly  to  the  sequences  of  image  patches  in  ViT  (Jahanbakht  et  al., 
2022a). This is shown to perform very well on image classification tasks 
(Dosovitskiy et al., 2020) and therefore has been used in our study. 

2.3.3. Transfer learning and weak supervision

Labeling Time 
Training Time 
Turnaround 

Time 
Accuracy 

Very Short 
1.0 h 

4 h 

89.4% 

Short 
6.9 h 

22 h 

94.0% 

Very Long 
~3.5 h 

94 h 

98%  

4. Discussions 

The  development  of  deep  learning  methodologies  continues  to 
advance at an astonishing rate and be applied to various applications 
ranging from biomedical (Azghadi et al., 2020), hydrological processes 
in river channels (Talukdar et al., 2023) and agricultural (Olsen et al., 
2019) systems, to marine (Laradji et al., 2021; Saleh et al., 2022b), and 
environmental (Jahanbakht et al., 2022a) sciences. The application of 
deep learning technologies has been also used in profiling the ecosystem 
services of estuarine habitats by community members (Yee et al., 2023). 
In this paper, we extend the application of deep learning methodologies 
to advance state-of-the-art underwater fish video processing techniques 
applied to turbid waters.

2.3.1. EfficientNet 

The first model selected is EfficientNet-B7, which is a Convolutional 
Neural Network (CNN) with efficient scaling factors (Tan and Le, 2019). 
To elaborate, all CNNs are a cascade of convolutional layers that can 
scale  the  input  image’s  resolution  (width  and  height)  and  channel 
(number  of  color  channels).  CNN  architectures  can  also  scale  by  the 
number of consecutive convolutional layers (depth).

We collected the FishInTurbidWater dataset and quickly and weakly 
labeled  it  to  contribute  to  the  first  weakly-supervised  fish  dataset  in 
turbid  waters.  We,  then  used  this  dataset  to  develop  two  novel  deep 
learning networks, one using semi-supervised contrastive learning for 
significantly  accelerated  model  deployment  time,  and  one  weakly- 
supervised  model  to  shorten  deployment  time,  while  providing  high 
accuracy. 

For  semi-supervised  contrastive  learning,  we  first  trained  a  self- 
supervised  contrastive  learning  model  and  then  fine-tuned  it  on  our

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't any information available regarding the specific data augmentation techniques applied in the deep learning pipeline mentioned in the given context. Therefore, I cannot provide a helpful answer to this query based on the provided information.
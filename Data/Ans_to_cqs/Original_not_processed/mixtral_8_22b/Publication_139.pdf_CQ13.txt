Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(cid:0) 4 and a decay of 1×10

The  optimizer  Adam  was  selected  with  an  initial  learning  rate  of 
(cid:0) 7 and early stopping was used to prevent 
1×10
overfitting  (Kingma  and  Ba,  2014).  Typically,  the  model  trained  be-
tween 10 and 15 epochs before training ceased, where each epoch took 
roughly 1 min on an NVIDIA GeForce RTX 2080 Ti GPU. In addition to 
the conventional accuracy metric, the area under the curve, as computed 
by  a  Riemann  sum,  was  also  used  for  training.  From  the  assembled 
database of annotated clips, in each of the five cross-validation folds, 
10% of the data was reserved for comparing the training and validation 
loss/accuracy  after  each  epoch,  and  a  further  10%  was  reserved  for 
performance testing after all training had ceased. All results reported are 
the average after five-fold cross-validation.

Rosenblatt, F., 1962. Multi-layer perceptrons with fixed preterminal networks. In: 

Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. 
Spartan Books, pp. 313–345. Section: 15.  

Ruff, Z.J., Lesmeister, D.B., Duchac, L.S., Padmaraju, B.K., Sullivan, C.M., 2020. 

Automated identification of avian vocalizations with deep convolutional neural 
networks, 6, pp. 79–92. URL. https://onlinelibrary.wiley.com/doi/abs/10.1002/ 
rse2.125. https://doi.org/10.1002/rse2.125. 

Saunders, D.S., 2014. Insect photoperiodism: effects of temperature on the induction of 
insect diapause and diverse roles for the circadian system in the photoperiodic 
response. Entomol. Sci. 17, 25–40. URL. https://onlinelibrary.wiley.com/doi/abs/ 
10.1111/ens.12059. https://doi.org/10.1111/ens.12059. 

Kaufman, K., 2001. Lives of North American Birds, 1st edition ed. Houghton Mifflin 

Sevilla, A., Glotin, H., 2017. Audio bird classification with inception-v4 extended with 

Harcourt.

Pre-training on the both ResNet50 (He et al., 2016) and Inception 
(Szegedy et al., 2014) neural networks did not meaningfully improve 
classification performance, therefore this simpler architecture, similar to 
those implemented by (Ruff et al., 2020; Sprengel et al., 2016) was used 
instead. 

2.6. Generating prediction record

2.4.1. Training database assembly 

To assemble a training database, three randomly selected days from 
each month of the calendar year (see Table 1) were selected for anno-
tation. Select spectrogram images were manually labeled via visual in-
spection by two trained graduate students into a set of sound categories 
that evolved as the year progressed. On average, around 60 images could 
be  annotated  per  minute,  which  corresponds  to  8  min  of  audio  data. 
Therefore,  annotating  every  single  spectrogram  from  a  single  day 
(10,800 images) would take 3 h. Assembling the training database took 
less time in practice because only 10–15% of the spectrograms from each 
of the selected days were actually annotated (many spectrograms which 
contained only background noise were simply not needed). 

In total, this database consists of over 40 k images of bio-, anthro- 
and  geophonies.  The  full  contents  of  the  database  are  presented  in 
Table 2.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The deep learning model used the optimizer Adam with an initial learning rate of 7. Early stopping was employed to avoid overfitting. The model typically trained between 10 and 15 epochs before training stopped. Each epoch took approximately 1 minute on an NVIDIA GeForce RTX 2080 Ti GPU. Apart from the standard accuracy metric, the area under the curve, calculated using a Riemann sum, was also utilized during training. The model was trained using five-fold cross-validation, reserving 10% of the data for comparing training and validation loss/accuracy after each epoch, and another 10% for performance testing post-training completion.
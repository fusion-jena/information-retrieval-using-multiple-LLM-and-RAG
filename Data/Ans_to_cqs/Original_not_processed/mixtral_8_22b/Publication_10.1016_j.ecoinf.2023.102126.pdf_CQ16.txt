Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

6. Conclusions 

addressing  data  quality.  For  this  reason,  it  is  necessary  to  continue 
developing metrics such as the one proposed by F-UJI and methodolo-
gies  such  as  the  one  proposed  in  this  study  that  favour  not  only  the 
availability of data but also that they are really useful to use, share, and 
reuse. 

Regarding F-UJI, as mentioned by its creators, it is a continuously 
developing tool. Automatic testing of research data objects is based on 
the FAIR ecosystem, and its success depends on automatic testing with 
clear criteria that can be evaluated using machines.

In line with the requirements highlighted by Kinkade and Shepherd 
(2021), several tools have been developed based on a series of metrics 
that can be used to assess the compliance of a dataset with FAIR prin-
ciples. These tools pursue the same objective of evaluating the degree of 
FAIRness of a dataset but employ different methodologies. In general, 
these tools can be divided into three groups as automatic, manual, and 
hybrid  tools  (Peters-Von  Gehlen  et  al.,  2022;  Sun  et  al.,  2022).  Ac-
cording to Peters-Von Gehlen et al. (2022), none of the three models are 
perfect;  however,  each  offers  certain  advantages.  Manual  approxima-
tions  capture  contextual  approximations  that  are  more  subjective, 
whereas automatic approximations are stricter in terms of the aspects of 
the  analysis  conducted  by  the  machine.  More  difficulties  are  encoun-
tered in the context of automatic evaluation, and the search for alter-

considered.  

1.  The overall repository score considers the results obtained from the 

datasets for all principles.  

2.  Scores obtained by each repository with respect to the four principles 

separately. 

3. Results 

3.1. Results obtained from the DCI 

Of  the  6288  valid  records  included  in  the  sample,  5962  (94.81%) 
corresponded to the period from 2013 to 2022, with 2014 being the year 
with the highest productivity. Regarding data sources, 30 different re-
positories were observed (Fig. 2).

Fig. 1 shows an extract from a JSON file that includes the identifier 
number, metric name with results (output), evaluation score, debugging 
messages, and a  summary of  all metrics in  the evaluation. Finally, to 
visualize the scores after compiling all the PIDs, we created a report by 
running a computational notebook (. ipynb document) provided by the 
FAIRsFAIR team (Devaraju and Huber, 2021a). This notebook provides 
an overall analysis and visualisation of all PID responses assessed using 
the F-UJI. 

In summary, a) the report reads JSONâ€™s responses, b) collects all the 
total scores for the FAIR metric within a data frame, and c) visualises the 
metrics  and  summarises  the  evaluations  (in  our  case,  the  datasets 
associated with each repository). The characteristics of the report are 
discussed in the Section 3. 

For further analysis, two aspects of the findings of this study were 

considered.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not contain information about the specific criteria used to determine when training is complete, such as a validation loss plateau or other stopping conditions. Therefore, I cannot provide a helpful answer based solely on the given context.
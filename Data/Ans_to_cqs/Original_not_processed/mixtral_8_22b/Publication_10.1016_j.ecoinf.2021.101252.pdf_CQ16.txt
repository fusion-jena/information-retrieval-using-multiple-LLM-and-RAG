Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

To identify an ‘optimal’ number of training epochs, we examined the 
progression of validation performance (Bv). Models can be trained for an 
infinite number of epochs, so here we stopped training if no increase in 
validation performance was observed after 25 epochs (other thresholds 
could be considered, according to time resources available). Finally, the 
model trained with the number of epochs showing highest AUC in pre-
dicting Bv was used to classify the test data (data set T), with perfor-
mance measured using AUC. 

We recorded processing time of all models from the onset of training 
of candidate models to the last training epoch evaluated for the selected 
model. This was done on two distinct systems: a ‘desktop PC’ with an

model.  A  model  trained  too  few  epochs  will  not  capture  all  relevant 
patterns in the data, reducing predictive performance. A model trained 
for an excessive number of epochs might overfit, reducing its generality 
and ability to classify new data. There is no definitive way to identify an 
optimal  number  of  training  epochs,  but  one  practical  approach  is 
through  monitoring  the  model’s  validation  performance  (i.e.,  using 
holdout  data  partition  Bv;  Fig.  2).  The  ‘optimal’  number  of  training 
epochs is the one that provides the best validation performance. Finally, 
the performance of the model having an ‘optimal’  number of training 
epochs is evaluated using a ‘final’ test data set (T; Fig. 2), providing the 
best estimate of the predictive performance of the model.

time, from the onset of candidate model training to the 72nd training 
epoch of the selected model, took 26 min on a desktop PC. On the high- 
end workstation, a distinct modelling event took 3 min.

For the three case studies below, we used the same model generation 
and selection strategy. We had mcfly generate 20 candidate models, five 
for each architecture type. These models were trained during 4 epochs 
(using At). The candidate model achieving highest performance in pre-
dicting the classes of the validation data (Av) was then trained on the full 
training  data  set  (Bt).  For  each  epoch  we  measured  training  perfor-
mance, as provided by mcfly (which uses the accuracy metric i.e., ‘the 
proportion of cases correctly classified’). The classification performance 
on  the  validation  data  (Bv)  was  measured  using  the  area  under  the 
receiver  operating  characteristic  curve  (AUC),  a  metric  that  is  not 
affected by differences in the prevalence of classes and is widely used in 
ecology (e.g. Dyderski et al., 2018).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The criteria used to determine when training is complete is based on monitoring the model's validation performance. Training continues until there is no further improvement in validation performance after a certain number of epochs. In this case, the threshold was set to stop training if no increase in validation performance was observed after 25 epochs. However, it is noted that other thresholds could also be considered depending on the time resources available. The 'optimal' number of training epochs is determined as the one that provides the best validation performance. Once the optimal number of epochs has been identified, the final performance of the model is evaluated using a separate test dataset, which gives the best estimate of the predictive performance of the model.
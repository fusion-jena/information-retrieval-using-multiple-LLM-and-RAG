Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Cross-validation is an iterative method for calculating the expected
value of a particular measure. It splits the dataset into K equally-sized
parts that are called folds. In each iteration, diﬀerent (K − 1)-folds are
used for training and the remaining fold for testing. The overall mea-
sure of its performance is the average of the measures of the individual
iterations. The above method guarantees that every instance will be
used both for training and testing. There is an alternative of cross-va-
lidation method called stratiﬁed cross-validation, where in each fold a
balanced number of instances for each class is selected. For our ex-
perimentation, we used stratiﬁed cross-validation with K = 10 folds.

4. Experiments

In this section, we perform two experiments. First, we are trying to
predict the existence of seagrass, and then its family. We are also in-
terested on which variables aﬀecting those predictions most.

4.1. Detecting seagrass

When computing the total F-measure, we use the macro-average.
Macro-averaging takes the average of all individual class F-measures,
treating small and large classes equally, in contrast to micro-averaging
which aggregates the TP/TN/FP/FN counts from all classes and com-
putes a total measure which is biased to the large classes.

We also experimented with using subsets of best features from top-
50% down to top-5% with a step of 5%, and the eﬀectiveness max-
imized at using top-10% of features. When all features are used, the best
performing classiﬁers are the tree-based Decision Tree and Random
Forest. For the top-10% of the features, which were calculated using
variable importance with Random Forest, all algorithms except the tree-
based ones give lower results. However, the best F-measure score (37.1
of Decision Tree) has been improved by 2.6% in comparison with the
all-features experiment and is now achieved by Random Forest with
39.7%.

The ﬁnal step is to train and test the machine learning algorithms in
this dataset for 5-class classiﬁcation. Stratiﬁed cross-validation was
used, and evaluated with Precision, Recall and F-measure. This dataset
is not balanced as can be seen in Table 1. Cymodocea is the dominant
class that constitutes the 75% of the dataset. Thus, Accuracy may not be
a suitable measure, because it measures how many correct predictions
were made overall, and if we predict all the test examples as cymo-
docea, then Accuracy would be close to 75% without even predicting
another class. This is clearly a problem because many machine learning
algorithms are designed to maximize overall Accuracy, with the ex-
ception of the tree-based algorithms. So, we resort to the F-measure; the
algorithms are ranked based on it.

1116
598
598
582
597
597
1357
825

By determining which variables are most important we can simplify
the analysis of a dataset, better understand the physical concepts of it,
and exclude the ones that confuse the model achieving better accuracy
and learning/testing speed.

There exist several ways to determine variable importance, e.g.
Decision Trees, Random Forests, Chi-square, and Regression. We deal
with tree-based variable importance, like (Arthur et al., 2010) did. It

63.01%
33.76%
33.76%
32.86%
33.71%
33.71%
76.62%
46.58%

162

D. Eﬀrosynidis et al.

Ecological Informatics 48 (2018) 158–170

Fig. 4. Presence (green) and absence (red) points of seagrass in Mediterranean (above) and Sicily (below).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To ensure the generalizability of their deep learning models, the authors employed various techniques such as stratified cross-validation and using diverse datasets. Specifically, they utilized stratiﬁed cross-validation with K = 10 folds, ensuring that every instance was used both for training and testing. Additionally, they performed experiments using subsets of best features from top-50% down to top-5% with a step of 5%, ultimately finding that the effectiveness maximized when using top-10% of features. They also mentioned that the dataset was not balanced, indicating that they likely took steps to address potential issues related to imbalance during model development. Overall, these methods helped improve the robustness and applicability of their models across different scenarios.
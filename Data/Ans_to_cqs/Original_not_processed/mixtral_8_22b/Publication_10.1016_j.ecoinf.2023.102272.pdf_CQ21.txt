Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.5. Model selection procedure 

The model selection procedure was performed as follows (Capinha 
et  al.,  2021;  Van  Kuppevelt  et  al.,  2020):  we  randomly  generated  5 
models for each of the four available deep-ANN architecture types (20 
models in total) and trained each one with a small subset of the training 
data  (data  partition  At)  for  4  epochs  (an  “epoch”  corresponds  to  the 
complete training dataset being passed forward and backward across the 
network  one  time;  Capinha  et  al.,  2021).  The  accuracy  of  candidate 
models,  as  provided  by  mcfly  (i.e.,  the  “proportion  of  cases  correctly 
classified”), was then compared using a left-out validation data set (data 
partition Av) and the model with the highest performance was selected 
for training on the full training data (data partition Bt; Bt = At + Av) for 
up to 30 epochs.

The data used for model training and selection were randomly par-
titioned into: data for training candidate models (25% of the data; At); 

data for validating candidate models (50%; Av); data for training the 
selected  candidate  model  (75%;  Bt  = At  + Av);  validation  data  to 
determine the optimal number of epochs to train the selected candidate 
model (25%; Bv). Data partition was performed in R with package dismo 
(Hijmans et al., 2017; R Core Team, 2022). 

2.5. Model selection procedure

2.4. Data partition 

The ‘autoML’  workflow in  mcfly requires using multiple partitions 
from the full data set of response and predictor variables. Here we fol-
lowed the partition scheme of Capinha et al. (2021), consisting of four 
partitions  that  are  used  at  the  various  stages  of  model  selection  and 
training, and a fifth partition that is used to assess the predictive per-
formance of the fully trained (‘final’) model. To ensure independence 
between the data used for model training and the data used for model 
testing, we used the data of each year (i.e., 2013 to 2019) separately for 
model testing (partition ‘T'), and the data for the remaining years for 
model training. The aim of this procedure is to mimic an operational 
setting where data available for previous years is used to train a model 
that  will  be  employed  in  the  real-time  forecasting  of  changes  in  the 
number of eggs for the coming week.

model  complexity,  construction  complexity,  iteration  complexity,  or 
complexity of incorporating new data.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: Based on the given context, there isn't any explicit information about the specific postprocessing steps involved after the model training such as saliency maps, metrics calculation, confusion matrix, etc. However, it can be inferred from the context that some form of evaluation and comparison of the candidate models is done based on their accuracy, which is defined as the proportion of cases correctly classified. This evaluation is carried out using a separate validation dataset (Av) not used during the initial training phase. After selecting the best performing model, it is further trained on the complete training dataset (Bt) for up to 30 epochs. Additionally, another validation dataset (Bv) is used to determine the optimal number of epochs for training the final model. Therefore, while the exact postprocessing steps aren't mentioned, it can be understood that they involve evaluating the model's performance using appropriate metrics and datasets.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Step 3: Evaluate the performance of the trained model by validating 
it against the test images. This involves computing metrics such as ac-
curacy,  recall,  F1  score,  and  other  relevant  measures.  Adjust  the  pa-
rameters and structure of the model based on the evaluation results to 
improve the detection performance. Iterate the training and evaluation 
process until a satisfactory level of performance is achieved.

Attention module 

mAp(%) 

mAp50–95(%) 

P(%) 

R(%) 

F1(%) 

– 

SE 
CA 
CBAM 

70.9 
71.1 
72.3 
72.3 

36.9 
36.7 
37.4 
37.5 

83.5 
81.6 
81.4 
83.9 

64.7 
64.9 
66.4 
65.2 

72.9 
72.3 
73.1 
73.4  

4.5. Analysis of different attention modules

}

After processing by the backbone network, the input of the feature 
{
, Pin
Pin
.  At  the  output  of  the  feature 
fusion  network is  obtained 
{
}
5
3
, Pout
fusion network 
. The CRFPN builds a feature branch from 
5
the  corresponding  input  combined  with  attention.  The  calculated 
expression is as follows: 
(cid:0)
(cid:0)

, Pout
4

, Pin
4

Pout
3

) )

Pout
3 = Conv

Ptd
3 + CBAM

Pin
3

(cid:0)

(cid:0)

Pout
4 = Conv

Pout
5 = Conv

Ptd
4 + CBAM

Ptd
5 + CBAM

) )

) )

(cid:0)

(cid:0)

Pin
4

Pin
5

(5)  

(6)  

(7)  

Fig. 5. C3 module (left) SRC3 module (right).  

3.3. The head of YWnet 

convolution  procedures  and  fed  into  the  bottleneck  block  in  parallel. 
Simultaneously,  a  branch  is  introduced  to  connect  with  the  final 
dimension.  Finally,  a  convolution  is  employed  to  output  the  fused 
features.

EcologicalInformatics79(2024)1024018P. Liu et al.                                                                                                                                                                                                                                       

Fig. 8. Visualization of the attention module.  

Table 4 
Comparison of different feature fusion networks.  

Neck 

PANet 
BiFPN 
CRFPN 

mAp 
(%) 

72.8 
72.6 
73 

mAp50–95 
(%) 

38.2 
37.4 
38.7 

P 
(%) 

87.2 
86.4 
85.6 

R 
(%) 

65.6 
66.4 
65.9 

F1 
(%) 

74.9 
75.1 
74.5 

Parm 
(M) 

11.6 
11.6 
12.0 

GLOPS 
(G) 

28.9 
29.1 
29.6  

Table 5 
Comparison of different loss functions.  

loss functions 

mAp(%) 

mAp50–95(%) 

P(%) 

R(%) 

F1(%) 

NWD 
Alpha-IoU 
CIoU 
SIoU 
EIoU 

69.6 
71.1 
73 
72.7 
73.2 

36.0 
38.4 
38.7 
38.2 
38.8 

82.9 
81.8 
85.6 
88.5 
79.8 

62.5 
64.9 
65.9 
64.7 
68.5 

71.3 
72.4 
74.5 
74.8 
73.7

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The postprocessing steps involved after the model training include evaluating the performance of the trained model by validating it against the test images. This process involves computing various metrics such as accuracy, recall, F1 score, and others. Based on these evaluation results, adjustments can be made to the parameters and structure of the model to improve its detection performance. This iterative process continues until a satisfactory level of performance is achieved. Additionally, visualizations like saliency maps may also be used for further analysis or interpretation of the model's behavior.
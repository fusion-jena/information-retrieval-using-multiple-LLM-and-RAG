Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Data-driven models depend on a number of parameters, known as 
hyperparameters,  which  are  employed  to  enhance  and  regulate  the 
learning  procedure.  Optimal  hyperparameter  selection 
leads  to 
improved model accuracy and enhanced prediction performance (Aze-
dou et al., 2023). To fine-tune hyperparameters in data-driven models, 
the development dataset (comprising all data records except the evalu-
ation data records) is divided into two categories: the training dataset 
and the tuning dataset. Given the size constrains of the dataset used in 
the  analysis  and  the  need  to  utilize  all  development  data  records  in 
training  to  enhance  the  model’s  generalization  ability,  k-fold  cross- 
validation  was  chosen  as  the  preferred  approach  for  hyperparameter 
tuning (Saha et al., 2022). In this technique, the development dataset is 
evenly divided into k groups. During each iteration, one group called the

tuning dataset is excluded from the training process. The model is then 
developed using the remaining dataset (i.e. the training dataset), and its 
performance is evaluated using the tuning dataset. The average perfor-
mance of the k models developed on the validation groups represents the 
performance of the machine learning technique for the selected hyper-
parameters (Velasco Hererra et al., 2022). For this study, a value of 5 is 
assumed  for  k.  Grid  search  is  employed  to  evaluate  the  data-driven 
model’s  performance  using  each  combination  of  predefined  hyper-
parameters and identify the best hyperparameters. The maximum depth 
of the tree, the number of trees in the ensemble model, and the learning 
rate (which shows how fast the model learns) are tuned in this study as 
the influential hyperparameters (Cakiroglu et al., 2022).

The suitability of data-driven model predictions depends largely on 
both the quality and quantity of the data used to develop the models 
(Arashpour, 2023). A small training dataset often will lead to overfitting 
issues when developing machine learning models (Jiang et al., 2022). 
There can be several reasons for this, but typically this occurs because 
the small dataset do not satisfactorily represent all possible input data 
combinations. Lack of data will also force the model to learn from the 
noise  and  describe  it,  rather  than  capture  the  data  trends  (Peng  and 
Nagata,  2020).  In  the  context  of  modelling  fire  behaviour,  obtaining 
sufficient  experimental  or  wildfire  spread  data  is  challenging  due  to 
operational, safety, and legal constraints (Anderson et al., 2015); with 
the field of study considered to be data poor (Gould et al. 2024). Syn-
thetic data generation provides a method to enhance real-world datasets

The results showed the issue of overfitting in ML modelling, namely 
when the learning dataset used is relatively small. Some recent studies 
have  evaluated  model  performance  against  the  data  used  in  model 
fitting  and  tuning  (e.g.  Kucuk  and  Sevinc  (2023).  This  may  fail  to 
identify the true fit and behaviour of ML models, because ML models aim 
to  maximize  accuracy  against  the  model  development  (training  and 
tuning) dataset. Our model evaluation against an independent dataset 
revealed  true  model  fit,  which  in  our  case  was  10%  lower  than  that 
obtained against the model development dataset) in the best performing 
models and about 30% lower in the worst performing ones. Analysis of 
model behaviour also showed flawed behaviour in some models, such as 
non-monotonic trends that result from overfitting. The overfitting (i.e. 
where the model captures not only the trend but also the noise in the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, the data is split into three parts for deep learning model training: the training dataset, the tuning dataset, and the evaluation dataset. The training dataset is used to train the model, while the tuning dataset is used to adjust the hyperparameters of the model during the training process. The evaluation dataset is used to assess the final performance of the model after training and tuning.

To fine-tune hyperparameters in data-driven models, the development dataset (comprising all data records except the evaluation data records) is divided into two categories: the training dataset and the tuning dataset. Due to the size constraints of the dataset used in the analysis and the need to utilize all development data records in training to enhance the model’s generalization ability, k-fold cross-validation is chosen as the preferred approach for hyperparameter tuning. In this technique, the development dataset is evenly divided into k groups. During each iteration, one group called the tuning dataset is excluded from the training process. The model is then developed using the remaining dataset (i.e., the training dataset), and its performance is evaluated using the tuning dataset. The average performance of the k models developed on the validation groups represents the performance of the machine learning technique for the selected hyperparameters. For this study, a value of 5 is assumed for k. Grid search is employed to evaluate the data-driven model’s performance using each combination of predefined hyperparameters and identify the best hyperparameters. The maximum depth of the tree, the number of trees in the ensemble model, and the learning rate (which shows how fast the model learns) are tuned in this study as the influential hyperparameters.
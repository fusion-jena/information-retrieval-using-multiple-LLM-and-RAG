Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2023; Yuan et al., 2021; Zhao et al., 2023c). The model’s initial learning 
rate was set to 0.01, further refined via iterative testing. The number of 
iterations was 150, with a batch size of 4, downsampling multiplier set 
at 8, and utilized SGD optimizer, ReLu activation function, and Dice loss 
function.

2.3.1. Model architectures 

(1)  Unet.  By  incorporating  an  encoder-decoder  structure  that  es-
tablishes shortcut connections, Unet model extended upon FCN’s end-to- 
end concept (Ronneberger et al., 2015). The input to the network is a 
512  × 512  pixels  image  (Fig.  3).  The  encoder  (left  dashed  square) 
downsamples the input tensor and encodes it into a vector with reduced 
dimensions. It consists of multiple convolutional blocks, each including 
a  3  × 3  convolutional  kernel  layer,  batch  normalization,  and  an 

Fig. 1. (a) Overview of Tarim River Basin. (b) Remote sensing images of the study area.  

EcologicalInformatics79(2024)1024093H. Wang et al.

(3) Deeplabv3þ. In the encoding stage of Deeplabv3+ model, the 
backbone  network  is  replaced  with  Mobilenetv2  featuring  expanded 
convolution (Fig. 5). Mobilenetv2 network ensures higher accuracy and 
enhances  the  model’s  lightweight  nature  compared  to  the  originally 
used  Xception+ network  (Fu  et  al.,  2022),  significantly  reducing  the 
model weights file size and accelerating model prediction speed. Addi-
tionally,  the  semantic  segmentation  task  involves  incorporating  ASPP 
module into the encoder-decoder network (Chen et al., 2018b). Five sets 
of feature maps are obtained by utilizing four atrous convolution blocks 
with varied dilation rates (including convolution, batch normalization, 
and activation layers) and a global average pooling block. These feature 
maps are concatenated, then passed through a 1 × 1 convolution block, 
and  ultimately  inputted  into  the  Decoder  module.  ASPP  method  in-

to augment the feature map size, aligning with the encoder’s pooling 
layer.  Each  decoder  module  concatenates  its  output  with  the  corre-
sponding encoder output. The final decoder tensor undergoes convolu-
tion and then traverses through a softmax layer, normalizing the output 
probabilities for two classes at each pixel (Gao et al., 2022; He et al., 
2022;  Qiu  et  al.,  2023).  The  model’s  initial  learning  rate  was  set  to 
0.0001, subsequently fine-tuned through iterative testing. The number 
of iterations was 150, the batch size was configured as 2, utilizing the 
Adam optimizer, ReLu activation function, and Dice loss function. 

(2)  Pspnet.  To  facilitate  the  integration  of  multi-scale  data  and

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning models mentioned in the given context are Unet, DeepLabV3+, and PSPNet. Here are the hyperparameters used for these models:

Unet:
- Initial Learning Rate: 0.01
- Optimizer: Stochastic Gradient Descent (SGD)
- Activation Function: ReLu
- Loss Function: Dice Loss
- Number of Iterations: 150
- Batch Size: 4
- Downsampling Multiplier: 8

DeepLabV3+:
- Initial Learning Rate: 0.0001
- Optimizer: Adam
- Activation Function: ReLu
- Loss Function: Dice Loss
- Number of Iterations: 150
- Batch Size: 2

PSPNet:
Unfortunately, the information about the hyperparameters used for PSPNet is not explicitly stated in the provided context. Therefore, I cannot provide this information.
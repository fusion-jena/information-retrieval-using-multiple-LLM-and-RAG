Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

‖ps‖2 + ‖qt‖2 + b2

s + b2

t

)

(4) 

Consequently, the unknown parameters bs, bt, ps, qt can be estimated 
optimizing  the  regularized  cost  function  using  stochastic  gradient 
descent: 

ps, qt, bs, bt = argmin
ps,qt ,bs,bt

L .

2.2. Embedding layers 

(5)  

Embedding  layers  are  modules  of  deep  neural  networks  that  are 
generally  implemented  in  natural  language  processing  problems 
(Sharma  et  al.,  2020)  and  collaborative  filtering  (He  et  al.,  2017) 
because they can solve the one-hot encoding problem (Yu et al., 2022), 
where the latent representations in the classic models are composed of 
sparse representations generally using vectors mainly composed of zero 
layers  replace  such  dispersed  vectors  with 
values.  Embedding

ei, Θ(1)

(cid:0)

(cid:0)

)

, ..Θ(h)

)

, Θ(h+1)

)

(13)  

where W denotes the matrix of weights in each layer; bi, the bias vector; 
g(.),  the  sigmoid  activation  function;  Θ(j),  the  hyperparameters  of  the 
network, i.e., Θ(j) = {W(j), bi
(j)), j = 1, 2, . . , h +
1; and h, the number of hidden layers in the network. 

(j)} ,g(j)(t, Θ(j)) =g(j)(W(j), bi

Lastly, according to (Fan and Cheng, 2018), the model presented in 
Eq.(13) is finally known as DMF. In this case, it will be called DMF1 and 
used to reconstruct the non-observed representations n in matrix M. 

2.4. Variations of the proposed DMF model 

This  section  presents  the  design  of  three  DMF  models  that  can 
complement the DMF1 model (see Fig. 2) by including spatial-temporal 
features,  which  vary  the  dimension  d  of  the  embedding  layers  in 
accordance with the information to be represented. In this sense, the aim 
of this proposal is to consolidate an approach to reconstruct signals that

3.3. Comparison of DMF and state of the art 

In this experiment, the period analyzed, the configuration of DMF4, 
and its training are the same as those described in subsection 3.2. The 
MF model (Rivera-Mu˜noz et al., 2021) used here for comparison was 
tuned by employing a grid search. As a result of the tuning process, λ was 
set at 0.1, k = 360, and the number of iterations to 100. The imputation 
by EM algorithm is carried out using the function em of Python package 
impyute and eps parameter is set 0.1. For MICE, we used the function 
IterativeImputer  of  Python  package  sklearn  (Pedregosa  et  al.,  2011b) 
with  the  recommended  parameters  (BayesianRidge  as  estimator  and  a 
max_iter = 10).

Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., 
Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E., 2011b. Scikit-learn: 
machine learning in Python. J. Mach. Learn. Res. 12, 2825–2830. 

Piech, C., Bassen, J., Huang, J., Ganguli, S., Sahami, M., Guibas, L.J., Sohl-Dickstein, J., 
2015. Deep knowledge tracing. In: Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., 
Garnett, R. (Eds.), Advances in Neural Information Processing Systems, vol. 28. 
Curran Associates, Inc, pp. 505–513. 

In: WHO Regional Office for Europe. Tech. Rep. 

Xie, K., Ning, X., Wang, X., Xie, D., Cao, J., Xie, G., Wen, J., 2017. Recover corrupted data 
in sensor networks: a matrix completion solution. IEEE Trans. Mob. Comput. 16 (5), 
1434–1448. 

Xue, H.-J., Dai, X., Zhang, J., Huang, S., Chen, J., 2017a. Deep matrix factorization

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information on how the hyperparameters of the model were optimized. However, there is mention of a grid search being used to tune the MF model for comparison purposes. This suggests that the authors may have also employed similar methods like grid search or random search to optimize the hyperparameters of their proposed DMF models. Unfortunately, without more specific details from the given context, it cannot be definitively stated which method was used to optimize the hyperparameters of the DMF models.
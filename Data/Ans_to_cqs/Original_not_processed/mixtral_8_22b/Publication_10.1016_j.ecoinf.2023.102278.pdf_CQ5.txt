Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Krizhevsky, A., Nair, V., Hinton, G., 2009. CIFAR-10 and CIFAR-100 datasets. 
La Grassa, R., Gallo, I., Landro, N., 2021. Learn class hierarchy using convolutional 

neural networks. Appl. Intell. 51 (10) https://doi.org/10.1007/s10489-020-02103- 
6. 

Lima, M.C.F., Leandro, M.E.D.d.A., Valero, C., Coronel, L.C.P., Bazzo, C.O.G., 2020. 
Automatic detection and monitoring of insect pests - A review. Agriculture 
(Switzerland) 10 (5). https://doi.org/10.3390/agriculture10050161. 

Maurer, A., Pontil, M., Romera-Paredes, B., 2016. The benefit of multitask representation 

learning. J. Mach. Learn. Res. 17. 

Ong, S.Q., Hamid, S.A., 2022. Next generation insect taxonomic classification by 

comparing different deep learning algorithms. PloS one 17 (12), e0279094. https:// 
doi.org/10.1371/journal.pone.0279094. 

Pang, G., Shen, C., Cao, L., Hengel, A.V.D., 2021. Deep learning for anomaly detection: a 

review. ACM Comput. Surv. 54 (2) https://doi.org/10.1145/3439950.

Table 4 
Average performance (Avg) and standard deviation (SD) for five trained models. Average precision, recall and F1-score for trained ResNet50 and EfficientNetB3 
(EffNetB3) models modified for multitask learning (MTL) with transfer learning using pre-trained weights from ImageNet. The models are trained and validated on the 
TLm  dataset. The models ResNet50, EfficientNetB3 are trained without MTL.  

Model 

Level 

ResNet50MTL 
EffNetB3MTL 

ResNet50MTL 
EffNetB3MTL 

ResNet50MTL 
EffNetB3MTL 

ResNet50 
EffNetB3 

L1 Order 
L1 Order 

L2 Family 
L2 Family 

L3 Species 
L3 Species 

Species 
Species 

Avg 

0.990 
0.986 

0.987 
0.984 

0.955 
0.948 

0.955 
0.953 

Precision 

SD (10

(cid:0) 3) 

(1.0) 
(4.4) 

(0.8) 
(3.1) 

(4.3) 
(5.2) 

(3.3) 
(2.5) 

Avg 

0.991 
0.993 

0.986 
0.988 

0.961 
0.966 

0.957 
0.966 

Recall 

SD (10

(cid:0) 3) 

(1.1) 
(0.5) 

(0.9) 
(0.7) 

(9.8) 
(5.1) 

(7.3) 
(2.5) 

Avg 

0.991 
0.989 

0.987 
0.986 

0.957 
0.956 

0.955 
0.959

We simplified the network architecture and hierarchical dependency 
loss  proposed  by  Gao  (2020)  and  rewrote  the  code  from  Ugenteraan 
(2020), which will be explained in the following sections. 

3.1. A multitask CNN for hierarchical classification 

Our proposed network architecture is shown in Fig. 2. The first part 
of the architecture is a CNN such, as ResNet (He et al., 2016) or Effi-
cientNet (Tan and Le, 2019). It is given an input image X transformed by 
the network NCNN(θCNN), where θCNN  means the trainable parameters of 
the CNN network. The network output is viewed as a root representation 
R0, 

R0 = NCNN (X, θCNN )

(1) 

For each level l we add two FC layers and an activation function to 
perform a non-linear transformation of the input R0  to the level output 
Rl. 

Rl = (Wl2*σrelu(Wl1*R0 + bl1)) + bl2)

(2)

(cid:0) 4 was finally 
chosen after comparing with stochastic gradient descent (SGD) with the 
(cid:0) 4  and learning rate of 
(cid:0) 4, weight decay of 1.0⋅10
momentum of 1.0⋅10
(cid:0) 3. SGD was tested with the parameters as recommended by L. 
1.0⋅10
Smith  (Smith,  2018),  but  achieved  a  1%–2%  decrease  in  accuracy 
compared to using the Adam optimizer. 

The  ResNet50  architecture  (He  et  al.,  2016)  was  modified  and 
trained with transfer learning using pre-trained weights from ImageNet 
(Smith,  2018).  Models  trained  with  transfer  learning  outperformed 
models trained with random weights with an increase of 4% in the F1-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The paper mentions the use of the TLm dataset for training and validation of the models. However, it does not provide any further information about this dataset. Additionally, the authors mention using pre-trained weights from ImageNet for transfer learning. Therefore, it can be inferred that the ImageNet dataset was also utilized in their study. Unfortunately, there is no explicit mention of other commonly used datasets like MNIST or CIFAR in the provided context.
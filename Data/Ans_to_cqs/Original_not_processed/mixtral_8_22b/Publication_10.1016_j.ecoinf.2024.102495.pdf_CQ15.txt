Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

10 

20/40/60/80 

20/40/60 

11 

20/40/60/80 

20/40/60 

128 
128 
128 
128 
128 
128 
64 
256 
512 
64/256/ 
512 
64/256/ 
512 

1024 
1024 
1024 
1024 
1024 
1024 
1024 
1024 
1024 

512 

2048  

Table 3 
Combination  of  the  convolutional  neural  network  deep  learning  model 
parameters.  

No 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 

Model 

Optimizer 

Learning Rate 

Without Dropout 

With Dropout 

RMSProp 

Adam 

SGD 

RMSProp 

Adam 

SGD 

0.001 
0.01 
0.001 
0.01 
0.001 
0.01 
0.001 
0.01 
0.001 
0.01 
0.001 
0.01  

EcologicalInformatics80(2024)1024958E. Joelianto et al.                                                                                                                                                                                                                               

Fig. 8. Performance test of each configuration.  

Table 4 
The standard deviation values for each performance metric.  

Configurations 

Accuracy 
(%) 

Precision 
(%)

The standard deviation values for each performance indicator were 
compared  to  determine  the  optimal  configuration  (Table  4).  Configu-
ration 8 was determined as the ideal parameter (number of bands: 60, 
number of frames: 40, hop length: 256, and length of the FFT window: 
1024)  owing  to  its  reduced  standard  deviation  value.  The  feature 
extraction parameters that yielded the best performance were employed 
during the feature extraction phase in the optimization process of the 
CNN  deep  learning  model.  The  model  optimization  outcomes  were 
derived  using  performance  criteria  identical  to  those  employed  to 
optimize  feature  extraction  parameters.  Fig.  9  illustrates  the  perfor-
mance evaluation of each model. Additionally, Model 1 exhibited the 
most favorable average performance outcome, with an 89.43% accuracy 
rate, 91.38% precision, 95.52% recall, and 0.816 F1-Score. The obtained

The model parameters were tuned to increase model accuracy. Some 
model  parameters  or  hyperparameters  were  varied  to  evaluate  the 
performance of the given model. The model with the best performance 
was selected from several other models with different parameter vari-
ations.  However,  parameter  tuning  was  limited  to  a  few  hyper-
parameters  because  of  the  large  number  of  parameters  in  the  CNN 
model. The model parameters that varied were the optimization algo-
rithm or optimizer, learning rate, and dropout layer. The following are 

Fig. 3. Schematic of the process that occurs in the convolutional neural network algorithm.  

EcologicalInformatics80(2024)1024955E. Joelianto et al.

results  demonstrated  an  improved  optimality  level  compared  to  the 
alternative models. Consequently, Model 1 that lacked dropout, used the 
RMSprop optimizer, and had a 0.001 learning rate is the most optimal 
deep learning model.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Three optimization techniques are applied in the deep learning pipeline: RMSProp, Adam, and Stochastic Gradient Descent (SGD). These techniques are used to update the weights of the neurons in the neural network during training. Each technique has its own strengths and weaknesses, and they can be chosen based on the specific requirements of the problem being solved.

RMSProp is an adaptive learning rate method that uses the root mean square of recent gradients to normalize the gradient updates. This helps prevent oscillations and improves convergence speed. It is particularly useful when dealing with sparse data or noisy gradients.

Adam is another adaptive learning rate method that combines the advantages of both RMSProp and momentum methods. It maintains a moving average of past squared gradients and past gradients, which allows it to adaptively adjust the learning rate for each weight. Adam is often considered a good default choice for many problems due to its robustness and efficiency.

SGD is a simple yet effective optimization technique that updates the weights by taking small steps proportional to the negative gradient of the loss function. Although it may converge slowly, it is computationally efficient and works well for large datasets.

In this study, the authors experimented with these three optimization techniques to find the most optimal deep learning model. They found that Model 1, which did not use dropout, used the RMSprop optimizer, and had a learning rate of 0.001, performed the best among all the tested configurations.
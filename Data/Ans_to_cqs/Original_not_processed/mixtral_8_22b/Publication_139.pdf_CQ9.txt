Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

However, deep learning approaches are not without limitations, such 
as a lack of generalizablity to different recording sites with a different set 
of  target  species.  However,  even  with  state-of-the-art  approaches, 
location-specific  training  data  is  still  often  necessary  to  yield  crucial 
performance improvements (Kahl et al., 2019; Lasseck, 2019). For this 
reason, the neural network architecture implemented in this work was 
designed  to  be  retrained  in  under  20  min,  making  inevitable  dataset 
modifications for new recording sites practical.

2.4.1. Training database assembly 

To assemble a training database, three randomly selected days from 
each month of the calendar year (see Table 1) were selected for anno-
tation. Select spectrogram images were manually labeled via visual in-
spection by two trained graduate students into a set of sound categories 
that evolved as the year progressed. On average, around 60 images could 
be  annotated  per  minute,  which  corresponds  to  8  min  of  audio  data. 
Therefore,  annotating  every  single  spectrogram  from  a  single  day 
(10,800 images) would take 3 h. Assembling the training database took 
less time in practice because only 10â€“15% of the spectrograms from each 
of the selected days were actually annotated (many spectrograms which 
contained only background noise were simply not needed). 

In total, this database consists of over 40 k images of bio-, anthro- 
and  geophonies.  The  full  contents  of  the  database  are  presented  in 
Table 2.

Transfer learning, where a model pre-trained on one dataset is re- 
trained  to  classify  a  similar  datset,  is  one  convenient  approach  to 
effectively  utilize  the  power  of  CNNs.  Well-known  pre-trained  archi-
tectures such as Inception, MobileNet, and Resnet50 have recently been 
used with great success for the classification of birds in soundscape re-
cordings (Florentin et al., 2020; He et al., 2016; Howard et al., 2017; 
Incze et al., 2018; LeBien et al., 2020; Sevilla and Glotin, 2017; Szegedy 
et  al.,  2014).  Such  networks  are  typically  pre-trained  on  large  image 
databases such as ImageNet, which contains over one million samples 
distributed across 1000 classes (Deng et al., 2009). However, in cases 
where  targeting  foreground  species  is  the  main  task,  or  with  more 
acoustically sparse soundscapes, CNN architectures with far fewer (<10) 
convolutional layers have proven more than sufficient, and indeed, may

This paper seeks to bridge a  similar gap between theoretical deep 
learning-based  bioacoustic  classification  research  and  applied  studies 
that are either limited by time-consuming data analysis techniques, or 
constrained to report only on general soundscape trends, without access 
to species-specific activity. Rather than applying deep learning tools to 
an  existing,  de-contextualized  bioacoustic  dataset  in  order  to  make 
minute  classification  performance  improvements,  this  work  proposes 
and applies a methodological framework for automatic analysis of an 
ongoing terrestrial bioacoustics research project.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide information about the availability of the data repository for the deep learning pipeline used in this study. It discusses the process of creating a training database, including the number of images collected and the use of transfer learning with well-known pre-trained architectures like Inception, MobileNet, and Resnet50. However, it does not mention any specific details regarding the public availability of the dataset or its storage location in a data repository such as Zenodo, Figshare, Dryad, or GBIF.
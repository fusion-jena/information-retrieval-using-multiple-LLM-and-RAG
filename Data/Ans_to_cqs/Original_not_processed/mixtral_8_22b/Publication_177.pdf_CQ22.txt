Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

TABLE I.  

PERFORMANCE METRICS FOR UBD_45 DATASET 

Model 

Performance Metrics 

Sensitivity 

Specificity 

Precision 

F1-Score 

InceptionResnet2 

95.93 

Resnet50 

Xception 

Inceptionv3 

Mobilenetv2 

Googlenet 

95.93 

94.44 

91.85 

92.04 

85.00 

99.92 

99.90 

99.87 

99.85 

99.83 

99.62 

97.65 

97.20 

95.90 

94.73 

95.16 

86.15 

96.78 

96.56 

95.16 

93.27 

93.57 

85.57 

TABLE II.  

PERFORMANCE METRICS FOR VP_200 DATASET 

Model 

Performance Metrics 

Sensitivity 

Specificity 

Precision 

F1-Score 

InceptionResnet2 

91.20 

Resnet50 

Xception 

Inceptionv3 

Mobilenetv2 

Googlenet 

88.68 

91.80 

88.65 

85.58 

83.28 

99.96 

99.94 

99.96 

99.94 

99.93 

99.92 

91.92 

89.46 

92.83 

90.47 

87.20 

84.90 

91.56 

89.07 

92.31 

89.55 

86.38 

84.08

E.  Evaluation Metrics 

Since one of the datasets used was imbalanced, measuring 
the performance of the proposed method using accuracy alone 
would  not  be  a  sufficient  metric.  Therefore,  other  metrics 
including recall, precision, specificity, and F1-score were also 
used in the evaluation. These metrics are based on a number 
of true positive (TP) or true negative (TN) where the predicted 
species matches the ground truth class and false positive (FP) 
or false negative (FN) where  the predicted  species does not 
match  with  the  ground  truth  class.  In  summary,  these 
evaluation metrics are represented in Equations 1 – 5. 

𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 =  

𝑇𝑃 + 𝑇𝑁
𝑇𝑃 + 𝑇𝑁 + 𝐹𝑃 + 𝐹𝑁

          (1) 

𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦/𝑅𝑒𝑐𝑎𝑙𝑙  =  

𝑇𝑃
𝑇𝑃 + 𝐹𝑁

            (2) 

𝑇𝑃
𝑇𝑃 + 𝐹𝑃
𝑇𝑁
𝑇𝑁 + 𝐹𝑃
2. 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 . 𝑟𝑒𝑐𝑎𝑙𝑙
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + 𝑟𝑒𝑐𝑎𝑙𝑙

𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐𝑖𝑡𝑦 =  

                            (4) 

𝐹1 − 𝑠𝑐𝑜𝑟𝑒 =  

        (5)

The  selected  network  models  were  initialized  with  pre-
trained ImageNet weights and then fine-tuned to our datasets. 
The performance of a deep neural network for a dataset highly 
depends on network hyperparameters. The selection and fine-
tuning of  optimized hyperparameters  is  generally  a  difficult 
and  time-consuming  task.  Instead  of  manually  selecting  the 
hyperparameters, we employed Bayesian optimization to find 
their optimal values for each of the six models.  A Gaussian 
process  model  of  the  objective  function  is  used  by  the 
Bayesian optimization technique.  Different variables can be 
optimized using this technique such as network section depth, 
batch size, initial learning rate, momentum, and regularization 
strength. For this study, we optimized the network for batch 
size  (between  1  and  32)  and  initial  learning  rate  (between 
1×10-4  and  1×10-2).  The  optimization  was  performed  by

Fig. 4.  Classification accuracy for validation and test datasets for UBD_45 

Fig. 5.  Classification accuracy for validation and test datasets for VP_200 

The  other  performance  metrics  including  sensitivity, 
specificity,  precision,  and  F1-measure  for  all  models  for 
UBD_45 and VP_200 datasets are shown in Table I and Table 
II, respectively. The specificity values for all models for both 
datasets  were  found  to  be  above  99%  which  indicates  that 
these  models  were  able  to  highly  correctly  identify  those 

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on September 08,2024 at 16:24:56 UTC from IEEE Xplore.  Restrictions apply.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: The performance of the deep learning models in this study was evaluated using several metrics due to the presence of an imbalanced dataset. Accuracy alone would not have been a sufficient metric for evaluating the performance of the proposed method. Therefore, additional metrics such as recall, precision, specificity, and F1-score were also utilized. These metrics are based on the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). True positives refer to cases where the predicted species match the ground truth class, while false positives occur when the predicted species do not match the ground truth class. Similarly, true negatives represent cases where the absence of a particular species is accurately identified, whereas false negatives occur when the absence of a species is incorrectly classified as its presence. The equations representing these evaluation metrics are presented in the paper.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The training is done from scratch using GPU (Tesla T4) and CUDA 
Version-12.0 in Google Colab with the parameters (i) Input image size =
416, (ii) Batch size  = 16,  (iii) Epochs = 100, 150, 200  and 250, (iv) 
Weights  = Default  to yolov5.pt., (v) Data = custom data set  and (vi) 
Cache = True. 

EcologicalInformatics79(2024)10239811B. Bhagabati et al.                                                                                                                                                                                                                              

Fig. 11. Training Plots for the proposed model.

attention  mechanism  is  embedded  in  the  backbone  of  the  YOLOv5 
model because it is more convenient and drives the overall performance 
of  the  system.  Moreover,  the  main  feature  extraction  is  done  in  the 
backbone.  Therefore,  if  SENet  is  embedded  in  the  backbone,  it  can 
extract more features and enhance the performance of the model. This is 
the key aspect of the present work. 

The  SENet  attention  mechanism  comprises  squeeze  and  excitation 
operations.  The  architecture  of  the  SENet  attention  mechanism  (Hu 
et al., 2019) is shown in Fig. 4. In the mechanism, the residual block is a 
stack of layers that takes a convolutional block as input and adds the 
output of the convolutional block to a deeper layer (He et al., 2016). The 
global pooling block squeezes each channel into a single numeric value

Fig.  4. SENet  attention  mechanism.  Different  blocks  of  the  attention  mecha-
nism with sizes are shown. 

Table 2 
Hyper  parameters and their values used in  training the  proposed model. The 
hyper parameters are based on the optimized values for YOLOv5 COCO training 
from scratch.  

Parameter Name 

lr0 
Lrf 
momentum 
weight_decay 
warmup_epochs 
warmup_momentum 
warmup_bias_lr 
box 
cls 
cls_pw 
obj 
obj_pw 
iou_t 
anchor_t 

Value 

0.01 
0.01 
0.937 
0.0005 
3.0 
0.8 
0.1 
0.05 
0.5 
1.0 
1.0 
1.0 
0.2 
4.0 

Parameter Name 

fl_gamma 
hsv_h 
hsv_s 
hsv_v 
degrees 
translate 
scale 
shear 
Perspective 
flipud 
fliplr 
mosaic 
mixup 
copy_paste 

Value 

0.0 
0.015 
0.7 
0.4 
0.0 
0.1 
0.5 
0.0 
0.0 
0.0 
0.5 
1.0 
0.0 
0.0

In  order  to  determine  more  accurate  training  results  and  also  to 
explore the effect of epoch upon training result, apart from 150 epochs, 
the model with attention layer is trained with epoch values 100, 200, 
and  250  under  a  uniform  training  environment  and  with  the  same 
dataset.  The  training  summary  for  each  of  these  epochs  is  shown  in 
Tables 5, 6 and 7 for epochs 100, 200, and 250, respectively. The trends 
of mAP values with increasing epochs are shown in Figs. 14 and Fig. 15. 
The size of the dataset used for custom training is sufficiently large. 
Further, the DL model used in this work is tuned with optimized hyper 
parameters  for  which  overfitting  and  under  fitting  situations  are  pre-
vented from occurring. As mentioned above, the effort to over-train the 
model has been avoided and graded MSE convergence (Figs. 7 to 11) has 
been  adopted  to  fine-tune  the  model.  Results  obtained  for  different

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide information about the specific framework used to build the deep learning model. However, since the authors mention using CUDA version 12.0, we can infer that they likely used either TensorFlow or PyTorch, as both support CUDA acceleration for deep learning tasks. Without further details, it is impossible to definitively state which framework was used.
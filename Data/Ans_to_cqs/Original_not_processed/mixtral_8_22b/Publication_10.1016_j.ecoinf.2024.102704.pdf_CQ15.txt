Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

several  times  higher  than  those  of  other  operations  (Li  et  al.,  2018). 
Therefore,  reducing  memory  access  is  the  key  to  further  enhancing 
model  performance.  By  reconfiguring  the  computation  sequence  of 
‘convolutional layer + batch normalisation + activation layer on NVI-
DIA TESLA V100 GPUs, Wang et al. (2019) reduced memory access by 
33%,  22%,  and  31%  for  the  ResNet-50,  Inception  V3,  and  DenseNet 
models,  respectively,  leading  to  increases  in  the  computational  effi-
ciency  of  20.5%,  18.5%,  and  18.1%.  Lowering  the  memory  access  in 
models can accelerate operations across the three main layers of CNNs, 
thus fully harnessing the model's strengths and more effectively utilising 
the computational resources provided by the embedded devices (Gilan 
et al., 2019; Zhang et al., 2015). Therefore, by reducing the number of 
layers  in  the  network  model,  the  number  of  convolutions  and  data

Zhang, Q.L., Yang, Y.B., 2021, June. Sa-net: Shuffle attention for deep convolutional 

neural networks. In: ICASSP 2021–2021 IEEE International Conference on Acoustics, 
Speech and Signal Processing (ICASSP). IEEE, pp. 2235–2239. https://doi.org/ 
10.1109/ICASSP39728.2021.9414568. 

Zhang, C., Li, P., Sun, G., Guan, Y., Xiao, B., Cong, J., 2015, February. Optimizing FPGA- 
based accelerator design for deep convolutional neural networks. In: Proceedings of 
the 2015 ACM/SIGDA International Symposium on Field-programmable Gate 
Arrays, pp. 161–170. https://doi.org/10.1145/2684746.2689060. 

Zhang, T., Zhang, X., Shi, J., Wei, S., 2019. Depthwise separable convolution neural 

network for high-speed SAR ship detection. Remote Sens. 11 (21), 2483. 

Zhang, T., Yang, Y., Liu, Y., Liu, C., Zhao, R., Li, D., Shi, C., 2024. Fully automatic system 
for fish biomass estimation based on deep neural network. Eco. Inform. 79, 102399 
https://doi.org/10.1016/j.ecoinf.2023.102399.

(n), small (s), medium (m), large (l), and extra-large (x). These scales 
meet the requirements of various scenarios and tasks. As scale increases, 
so does the network depth and width of the model; therefore, ‘n’  rep-
resents the smallest and fastest scale, whereas ‘x’ is the largest and offers 
the  highest  accuracy.  Although  the  accuracy  improves  as  the  model 
depth increases, this also leads to changes in the number of parameters, 
amount  of  computation,  and  detection  speed,  with  correspondingly 
higher demands on the hardware configuration. To delve deeper into the 
underlying causes of enhancing model lightweight and inference speed, 
we utilised LigObNet, YOLOv5, YOLOv6, YOLOv7, and YOLOv8 to train 
across  four  scales:  n,  s,  m,  and  l.  As  Fig.  9  illustrates,  reducing  the 
number  of  parameters  in  the  various  models  does  not  enhance  the 
processing speed. The total parameter count, representing the combined

Hu, J., Shen, L., Sun, G., 2018. Squeeze-and-excitation networks. In: Proceedings of the 
IEEE Conference on Computer Vision and Pattern Recognition, pp. 7132–7141. 
https://doi.org/10.1109/CVPR.2018.00745. 

Hu, G., Guo, Y., Wei, G., Abualigah, L., 2023. Genghis Khan shark optimizer: a novel 
nature-inspired algorithm for engineering optimization. Adv. Eng. Inform. 58, 
102210 https://doi.org/10.1016/j.aei.2023.102210. 

Shao, H., Bai, J., Wu, R., Jiang, J., Liang, H., 2023. FGPNet: a weakly supervised fine- 
grained 3D point clouds classification network. Pattern Recogn. 139, 109509 
https://doi.org/10.1016/j.patcog.2023.109509. 

Song, M.Y., Chen, L.R., Liang, J.A., Li, J.P., Niu, Z.Z., Wang, Z., Bai, L.L., 2022. Real-time 
Fiber end-face defect detection model with lightweight improved network. Progress 
Lasers Optoelectron. 24, 201–211. 

Soom, J., Pattanaik, V., Leier, M., Tuhtan, J.A., 2022. Environmentally adaptive fish or

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide specific information about optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, it discusses methods to improve the efficiency of Convolutional Neural Networks (CNNs) by reducing memory access and increasing computational efficiency. This includes reconfiguring the computation sequence of 'convolutional layer + batch normalization + activation layer', which can lead to significant reductions in memory access and improvements in computational efficiency. Additionally, lowering the memory access in models can accelerate operations across the three main layers of CNNs, thus fully utilizing the model's strengths and more effectively using the computational resources provided by embedded devices. Reducing the number of layers in the network model can also help to reduce memory access and increase computational efficiency.
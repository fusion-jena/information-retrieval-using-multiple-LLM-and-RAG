Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

5.1.4. Total model parameters 

Total  parameters  in  a  neural  network encompass  the  aggregate of 
trainable weights and biases, serving as a direct measure of the model’s 
architectural complexity. This parameter count is pivotal in defining the 
model’s capacity for feature extraction and pattern recognition. High- 
parameter  models  are  adept  at  discerning  intricate  data  patterns  but 
are  susceptible  to  overfitting,  particularly  in  scenarios  with  limited 
training samples. Furthermore, such models escalate computational and 
memory  requirements,  potentially  impeding  deployment  in  resource- 
constrained environments. 

5.1.5. Total layers in the model

3.2.4. ResNet-50 

ResNet-50 is a widely used deep residual neural network architecture 
that addresses the challenges of training very deep networks. It employs 
residual blocks with skip connections to enable the flow of gradients and 
facilitate the training of deeper models. ResNet-50 has been pre-trained 
on  large-scale  datasets,  allowing  it  to  capture  complex  patterns  and 
achieve strong performance. 

3.2.5.

InceptionV3 

InceptionV3 is an advanced CNN architecture that incorporates the 
concept of inception modules. These modules utilize parallel convolu-
tions  of  different  kernel  sizes  to  capture  multi-scale  features.  Incep-
tionV3  has  been  pre-trained  on 
large-scale  datasets  and  has 
demonstrated  excellent  performance  in  various  image  classification 
tasks. 

3.2.6.

InceptionResNetV2

3.2.3. EfficientNet 

EfficientNet comes under the family of pre-trained CNN that delivers 
cutting-edge performance with improved efficiency. These models use a 
compound scaling method to balance network depth, width, and reso-
lution to achieve optimal performance. EfficientNet models have been 
pre-trained  on  large-scale  datasets,  enabling  them  to  learn  highly 
discriminative  features.  Its  ability  to  efficiently  scale  up  in  capacity 
makes it a compelling choice for accurately diagnosing YVMV, ensuring 
high reliability in various diagnostic scenarios. 

3.2.4. ResNet-50

3.2.2. VGG-19 

VGG-19 is also a CNN architecture with 19 layers. It is known for its 
simplicity and uniform structure, consisting of multiple convolutional 
and pooling layers followed by fully connected layers. VGG-19 has been 
pre-trained on the ImageNet dataset and has demonstrated strong per-
formance  in  image  classification  tasks,  especially  in  intricate  pattern 
recognition, making it a valuable model for our study. 

3.2.3. EfficientNet

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The pipeline uses several types of deep learning models, all of which are Convolutional Neural Networks (CNN). Specifically, the models employed include VGG-19, ResNet-50, InceptionV3, and EfficientNet. Each of these models has unique characteristics and strengths. For instance, VGG-19 is known for its simplicity and uniform structure, while ResNet-50 addresses the challenges of training very deep networks using residual blocks with skip connections. On the other hand, InceptionV3 utilizes parallel convolutions of different kernel sizes to capture multi-scale features, and EfficientNet delivers cutting-edge performance with improved efficiency through a compound scaling method. All these models have been pre-trained on large-scale datasets, contributing to their robustness and effectiveness in various image classification tasks.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Fig. 7. Left movement.  

search  to  ascertain  the  configuration  that  minimizes  cross- 
validation error, thereby enhancing model precision.  

• The  selection  of  the  distance  metric,  typically  Euclidean,  was 
predicated on the data’s characteristics and the specific analytical 
prerequisites.  

was dictated by the explained variance ratio, to ensure the distilled 
dataset retained the majority of the original data’s variance.  
• The  SVM  model  was  thereafter  trained  on  this  dimensionally 
reduced  dataset,  with  hyperparameters  refined  as  delineated 
above. 

2.  For SVM  

• The  regularization  parameter  (‘C’)  and  the  kernel  type  (linear) 
were  calibrated  through  a  synergistic  application  of  grid  search 
and  cross-validation,  aiming  to  mediate  the  balance  between 
model complexity and its generalization prowess.

Precision 
% 

92.32 
95.32 

96.25 
96.07 

97.29 
98.07 

Recall 
% 

97.16 
87.74 

97.67 
93.72 

98.67 
94.59 

F-measure 
% 

Accuracy 
% 

94.68 
91.37  

96.95 
94.88  

98.63 
97.22   

93.42 

96.18 

98.15 

5.3.3. Results analysis and comparison 

The integration of K-Nearest Neighbors (KNN) and Support Vector 
Machine  (SVM)  classification  algorithms  (see  (Bansal  et  al.,  2022; 
Boateng  et  al.,  2020)),  along  with  a  Principal  Component  Analysis- 
Support  Vector  Machine  (PCA-SVM)  hybrid  approach  (Hai  and  An, 
2016), has significantly enhanced our understanding of zebrafish and rat 
movements within T-maze environments. This comprehensive analysis, 
as  detailed  in  Tables  3  and  4,  showcases  the  detailed  insights  gained 
from these methodologies. For zebrafish, the KNN model demonstrated 
high precision and recall in identifying left movements, with impressive

MSC (2020): 
92D50 
00A71 
68U07 
91B06 
47H10 

Keywords: 
Animal behavior 
Decision-making 
T-mazes 
Computational modeling 
Solution 
Machine learning methods 

1. Introduction

4.3. Analytical solution 

In  this  subsection,  we  delve  into  the  analytical  resolution  of  the 
decision-making model as delineated in eq. (4.4). The ensuing theorems 
and  corollaries  are  instrumental  in  establishing  the  existence  and 
uniqueness  of  solutions  within  the  confines  of  our  model  framework. 
These results hinge on the intricate interplay between the mappings V 1 
and V 2, and the probabilistic function f(x), which collectively orches-
trate the decision-making process in our T-maze setup. 

Theorem 4.2. Consider the model (4.4). Let 

|V 1(μ) (cid:0) V 2(ν) | ≤ ℓ3|μ (cid:0) ν|,

(4.5)  

where ℓ3 ∈ [0, 1), ∀μ, ν ∈ X with μ ∕= ν, such that Ξ1 := [(1 + ℓ4)ℓ1 + ℓ2 
+ℓ3 ] < 1, and there exists a subset ℰ ∕= 0 of S := {P ∈ C |P(1) ≤ 1 } with 
the structure (ℰ, ‖ ⋅ ‖) of a Banach space (given in (4.3)). Then, a singular 
solution exists for the model (4.4). Additionally, the sequence {Pn} converges 
to a unique solution of (4.4), where P0 ∈ ℰ and

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

To monitor the model performance during training, a combination of grid search and cross-validation techniques were employed. Specifically, the regularization parameter ('C') and the kernel type (linear) of the Support Vector Machine (SVM) model were calibrated using these techniques. Grid search was used to systematically explore different combinations of hyperparameter values, while cross-validation was applied to estimate how well the model would perform on unseen data. By combining these two strategies, it was possible to find the optimal set of hyperparameters that balanced model complexity and generalization ability, thus ensuring good performance both during training and when making predictions on new data.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

As it has been proven, ELMs are an important approach for handling and analysing Big
Data as they require the minimum training time relative to the corresponding engineering
learning algorithms. Moreover ELMs do not require ﬁne manipulations to determine their
operating parameters and ﬁnally they can determine the appropriate output weights
towards the most eﬀective resolution of a problem. What is most important, they have
in contrast to corresponding methods which adjust their
the potential to generalize,

508

K. DEMERTZIS ET AL.

performance based solely on their training data set. It is obvious that the emerging use of
ELM in Big Data analysis as well as DELE creates serious prerequisites for complex systems’
development by low-cost machines.

8. Future research

pleted. The LA has no prior knowledge as to how many training observations will be
presented. Unlike other sequential
learning algorithms which have many control
parameters to be tuned, OSELM only requires the number of hidden nodes to be
speciﬁed (Huang et al., 2005; Liang et al., 2006).

ELMs use the SLFFNN’s general methodology, with the speciﬁcity that the Hidden Layer
(feature mapping) is not required to work in a coordinated fashion. All hidden-layer par-
ameters are independent from the activation functions and from the training data.

ELMs can randomly create hidden nodes or hidden level parameters, before seeing the
training data, while it is remarkable that they can handle non-diﬀerential activation
equations and they do not address known NN problems such as stopping criterion, learn-
ing rate and learning epochs (Cambria & Guang-Bin, 2013; Huang, 2014, 2015).

A mathematical basis has been provided for the understanding of ELM (Equations

(1)–(9)) (Cambria & Guang-Bin, 2013; Huang, 2014, 2015).

For an ELM using SLFFNN and random representation of hidden neurons, input data is
mapped to a random L-dimensional space with a discrete training set N, where
(xi,ti), i [ 1, N with xi [ Rd and ti [ Rc. The speciﬁcation output of the network is the
following:

The Online Sequential ELM (OSELM) (Huang, Liang, Rong, Saratchandran, & Sundar-
arajan, 2005; Liang, Huang, Saratchandran, & Sundararajan, 2006) is an alternative
technique for
large scale computing and ML which is employed when data
becomes available in a sequential manner to determine mapping to data set corre-
sponding labels. The main diﬀerence between the Online Learning (ONL) and the
the
Batch Learning (BL)
arrival of every new data point in a scale fashion, whereas BL techniques are
used when one has access to the entire training data set at once. It is a versatile
sequential Learning Algorithm (LA) because training observations are introduced
sequentially one-by-one, or chunk-by-chunk, with varying or ﬁxed chunk length.
At any moment, only the newly arrived single record or chunk are used and
learned. A single record or a chunk of training observations is discarded as soon
as the learning procedure for that particular (single or chunk) observation(s) is com-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't enough information to provide a specific strategy for monitoring the model performance during training. However, generally speaking, strategies for monitoring model performance during training include tracking metrics like accuracy, precision, recall, F1 score, ROC curves, etc. Additionally, cross-validation techniques can also be used to evaluate the model's performance on unseen data. In the case of Extreme Learning Machines (ELMs), since they do not require fine manipulation of operating parameters and can determine the appropriate output weights towards effective problem resolution, monitoring could involve checking if the output weights converge to optimal values within a reasonable amount of iterations. Furthermore, since ELMs have the potential to generalize based solely on their training dataset, evaluating the model's ability to perform well on unseen data would also be crucial.
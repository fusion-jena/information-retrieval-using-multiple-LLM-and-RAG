Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4. Open Science methodology and tools 

Our  workflow  implements  a  FAIR  approach  that  tests  FAIR  data 
principles’  practicability.  It  is  open-source  (Supplementary  material) 
and was integrated with the DataMiner Cloud computing platform of the 
D4Science e-Infrastructure (Coro et al., 2017), which allows accessing 
the mentioned knowledge sources on-the-fly during processing (Candela 
et al., 2016; Coro et al., 2015a). Data FAIRness is facilitated through the 
indexing of these resources in the D4Science catalogue (Assante et al., 
2019b), which can be accessed by all processes via the Catalogue Services 
for the Web (CSW) standard of the Open Geospatial Consortium (OGC, 
2020). Geospatial data are offered as standarised NetCDF files available 
on  a  distributed  ISO/OGC  compliant  Spatial  Data  Infrastructure

workflow  can  compensate  for  some  of  these  biases  –  through  data 
classification  and  spatio-temporal  aggregation  –  mainly  when  large 
input datasets are used. Generally, it is worth noting that all big data 
processing methods are approximate, but they can discover general and 
valuable knowledge if the approximation is tolerated within the appli-
cation context (Coro, 2020b).

DataMiner  offers  15  machines  with  Ubuntu  18.04.5  LTS  x86  64 
operating system, 16 virtual cores, 32 GB of RAM, and 100 GB of disk, to 
run  executions  in  parallel/distributed  and  multi-tenancy  modes. 
Furthermore,  this  platform  enables  the  repeatability,  reproducibility, 
reusability, and interoperability of the processes, within a collaborative 
online environment (Assante et al., 2019b; Coro et al., 2021). To this 
aim, it offers a script-to-service transformation tool and a provenance 
tracking feature (i.e., it records all input and output data, parameters, 
and metadata) (Coro et al., 2016c). The hosted services are published 
under the Web Processing Service standard (WPS, Schut and Whiteside 
(2007))  of  the  Open  Geospatial  Consortium  (OGC)  to  maximise  their 
reuse from other software. Moreover, a Web graphic interface is auto-
matically generated based on the input/output definitions. Among the

At the time of writing, global-scale GFW data were downloadable for 
scientific purposes for the 2012–2016 period only. GFW distributes 
◦
vessel  data  aggregated  at  0.01
resolutions  with  fishing 
activity cells already classified through a machine learning model.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about where the data repository of the deep learning pipeline is available. However, it mentions that the workflow is integrated with the DataMiner Cloud computing platform of the D4Science e-Infrastructure, which allows accessing knowledge sources on-the-fly during processing. Furthermore, it states that geospatial data are offered as standardized NetCDF files available on a distributed ISO/OGC compliant Spatial Data Infrastructure. Therefore, it can be inferred that the data repository might be accessible through the DataMiner Cloud computing platform or the D4Science catalogue, although the specific platforms such as Zenodo, Figshare, Dryad, or GBIF are not explicitly mentioned.
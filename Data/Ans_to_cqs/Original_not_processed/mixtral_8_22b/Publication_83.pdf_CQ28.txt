Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The duration of audio ﬁles in CLO-43DS data is different,
which cannot be directly used as the input to the CNN. The
ﬁrst method for dealing with the multi-variate varying length
audio data is that the signal is repeated from the beginning to
force the ﬁxed duration of 2s, which has been used in [30].
The second method is to directly resize the audio image to a
ﬁxed size.

D. DEEP LEARNING ARCHITECTURE
The feature learning part of our proposed model follows a
VGG style network [24], which has been previously used
for classifying acoustic scenes [5]. The overall architec-
ture is illustrated in Table 1. This network is trained using
Adam optimizer with a learning rate of 10−4. The cate-
gorical cross entropy is utilized as the loss function. The
batch size is 64 samples and the network is trained with
200 epochs.

TABLE 3. Classification performance of different methods using single
CNN-based model. Here, Mel-CNN, Harm-CNN, and Perc-CNN denote that
the input to those CNNs are Mel-spectrogram, harmonic-component
based spectrogram, and percussive-component based spectrogram.
Subnet-CNN denotes that a SubSpectralNet architecture is used with the
Mel-spectrogram as the input.

[13] Á. Incze, H. Jancsó, Z. Szilágyi, A. Farkas, and C. Sulyok, ‘‘Bird sound
recognition using a convolutional neural network,’’ in Proc. IEEE 16th Int.
Symp. Intell. Syst. Inform. (SISY), Sep. 2018, pp. 000295–000300.
[14] P. Jancovic and M. Köküer, ‘‘Bird species recognition using unsu-
pervised modeling of individual vocalization elements,’’ IEEE/ACM
Trans. Audio, Speech, Language Process., vol. 27, no. 5, pp. 932–947,
May 2019.

[15] E. C. Knight, S. P. Hernandez, E. M. Bayne, V. Bulitko, and B. V. Tucker,
‘‘Pre-processing spectrogram parameters improve the accuracy of bioa-
coustic classiﬁcation using convolutional neural networks,’’ Bioacoustics,
pp. 1–19, 2019, doi: 10.1080/09524622.2019.1606734.

[16] Q. Kong, Y. Xu, and M. D. Plumbley, ‘‘Joint detection and classiﬁca-
tion convolutional neural network on weakly labelled bird audio detec-
tion,’’ in Proc. 25th Eur. Signal Process. Conf. (EUSIPCO), Aug. 2017,
pp. 1749–1753.

175354

VOLUME 7, 2019

J. Xie et al.: Investigation of Different CNN-Based Models for Improved Bird Sound Classification

FIGURE 2. [Color online] The number of instance for all bird species in the CLO-43DS dataset. Here, x-axis denotes the abbreviations of common names
of those 43 bird species, which can be found in [27].

FIGURE 3. [Color online] The time-frequency representation generation procedure. Here, Mel-spectrogram is divided into harmonic and percussive
components using Harmonic-percussive source separation. DFT denotes discrete Fourier transform.

directional microphones and omni-directional microphones.
In addition, the signal-to-noise ratio varies among different
recordings. The detailed information of those wood-warbler
species involved in this dataset can be found in [23].

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which platform was used to deploy the deep learning model (e.g., AWS, Azure, Google Cloud platform)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After reviewing the given context, it appears there is no explicit mention of the specific platform used to deploy the deep learning model. Therefore, it is not possible to determine whether the model was deployed on platforms such as AWS, Azure, or Google Cloud Platform based solely on the provided information.
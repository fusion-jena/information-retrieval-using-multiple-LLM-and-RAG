Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Yang et al. (cid:129) An Adaptive Approach for Empty Images Filtering

233

 23285540, 2021, 2, Downloaded from https://wildlife.onlinelibrary.wiley.com/doi/10.1002/wsb.1176 by Vamsi Krishna Kommineni - Friedrich-Schiller-Universität , Wiley Online Library on [29/08/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License3.6 GHz central processing unit (CPU) and 32 GB of
memory, we used the incremental training mode to train
and test the DCNN model. On another Dell PowerEdge
C4130 rack server with two Tesla K80 GPUs and 256 GB
of memory, we used the zero‐start and the incremental
training modes to train and test models, respectively.

RESULTS
The incremental training mode performed consistently with
the zero‐start training mode when the training sample size

At present, few researchers focus on time costs of model
training and the required computing platform. Yousif et al.
(2019) stated that they optimized their model performance,
but model training still failed to complete on a common PC.
In terms of recognition rate, their rate on a 2.6 GHz CPU
computer was 100 frames/minute, which is lower than our
recognition rate (260 images/minute). One possible reason
is that their preprocessing was more complex.

MANAGEMENT IMPLICATIONS
Our method can greatly reduce time and personnel costs in
3 ways. First, using our method to automatically ﬁlter empty
images can greatly save labor and time, and the percentage
of the labor and time savings is the same as the proportion

newly‐labeled images,
model on the extended training set
accuracy.

then automatically retrain the
to improve its

The DCNN model of our study was the AlexNet model,
which included ﬁve convolutional
layers and three fully
connected layers (Krizhevsky et al. 2012). Except for the last
fully connected layer, each layer performed Local Response
Normalization (LRN) processing and Rectiﬁed Linear Unit
(ReLU) activation function processing. Meanwhile, the
ﬁrst, second, and ﬁfth convolutional
layers performed a
downsampling operation (here using the mean pooling
method).

STUDY AREA
Our study area (28°14′‐28°22′N, 99°4′‐99°17′E) was located
at the Baimaxueshan Mountains (BMXSM), which were
part of the Hengduan Mountains in China (Fig. 1). The
Hengduan Mountains region, also known as the Mountains
of Southwest China, was identiﬁed as a global biodiversity
hotspot (Myers 1988, 1990, Myers et al. 2000). The altitude
changes from about 2000 m to higher than 4600 m above
sea level.

METHODS

Adaptive Incremental Training Method
To reduce the demand for computing resources for
model training, we proposed an adaptive incremental
training method (Fig. 2). In our method, we carried the
training process of the DCNN model in stages. The
model training starts with a smaller training set, and
then the trained model can be applied to the practical
ecological
In practical applications,
users can expand the previous training set by adding

survey project.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning model used in this study is a Convolutional Neural Network (CNN). Specifically, it uses the AlexNet model, which includes five convolutional layers and three fully connected layers. Each layer performs Local Response Normalization (LRN) processing and Rectified Linear Unit (ReLU) activation function processing, except for the last fully connected layer. Additionally, the first, second, and fifth convolutional layers perform a downsampling operation using the mean pooling method.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The mixed audio clips are then converted into spectrograms as input 
images for model training. The clips are transformed by applying the 
short-time Fourier transform (STFT) with a window size of 1600 sam-
ples,  hop  size  of  400  samples  (75%  overlap),  and  240  mel-scale  fre-
quency bins.  Each spectrogram thus  has a height  of 438 pixels  and a 
width of 436 pixels (Appendix S1: Fig. S4a).

2.4.1. Data transformation 

The labeled sounds in a recording are clipped and transformed into 
fixed-sized input images for model training. There are four data trans-
including  audio  standardization,  segmentation, 
formation  steps, 
augmentation, and spectrogram conversion. 

The  recordings  are  first  standardized  because  they  are  from  the 
community-contributed  collections  and  thus  may  be  recorded  with 
various recording equipment, settings and weather conditions. Because 
the frequency range of the labeled sounds in our database is between 
100 Hz and 16 kHz, we use a sampling rate of 32 kHz with a 100 Hz high- 
pass filter as the parameters for audio standardization.

The convolutional neural network (CNN) algorithm has been used as 
a successful alternative in recent years to improve the performance of 
identifying animal sounds in soundscape recordings. CNN is a state-of- 
the-art deep learning algorithm that is very effective in image recogni-
tion  and  classification  (Pak  and  Kim,  2017;  Rawat  and  Wang,  2017). 
CNN-based classifiers can also be used on audio data by converting the

3.4. Model training and evaluation 

3.4.1. Model training 

We  used  YOLOv5s,  the  smallest  weight  of  YOLOv5,  to  build  the 
sound identification model. The model was trained for 100 epochs with 
32 batch size and the input image dimension 640 × 640. We used default 
data  augmentation  including  scaling,  color  space  adjustments,  and 
Mosaic  augmentation  during  the  training  process.  Under  this  setting, 
four  original  spectrograms  were  randomly  selected,  resized,  cropped, 
color jittered (i.e., changes in hue, saturation and exposure) and then 
merged into one big input image for model training. The model training 
was performed using a workstation equipped with the following speci-
fications: Intel Xeon E5–2660 V4 CPU * 2, DDR4 2400 ECC 16GB RAM * 
8, NVIDIA Titan RTX GPU * 2, and WD black AN1500 4 TB NVMePCIe 
SSD * 2. 

3.4.2. Model inference and test datasets

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the described deep learning pipeline, two main types of data formats are utilized: audio and image. Initially, the raw audio data is collected from community-contributed sources, which may vary in terms of recording equipment, settings, and environmental conditions. These audio files are then processed through several stages, such as standardization, segmentation, augmentation, and spectrogram conversion.

Audio standardization involves setting a consistent sampling rate of 32 kHz and applying a 100 Hz high-pass filter to ensure uniformity across all recordings. Segmentation entails extracting specific segments or clips containing the labeled sounds from each recording. Augmentation techniques like scaling, color space adjustments, and Mosaic augmentation are applied to enhance the diversity of the dataset.

Finally, the mixed audio clips are converted into spectrograms, which serve as input images for model training. This conversion is achieved by employing the short-time Fourier transform (STFT) with a window size of 1600 samples, a hop size of 400 samples (75% overlap), and 240 mel-scale frequency bins. As a result, each spectrogram has dimensions of 438 x 436 pixels.

For model training, the Convolutional Neural Network (CNN) algorithm is employed due to its effectiveness in image recognition and classification tasks. Specifically, YOLOv5s, the smallest weight of YOLOv5, is chosen to build the sound identification model. During training, the input image dimension is set to 640 x 640 pixels, and default data augmentation methods are applied, including scaling, color space adjustments, and Mosaic augmentation.

To summarize, the deep learning pipeline primarily uses audio and image data formats. Audio files are initially gathered, preprocessed, and eventually transformed into spectrograms, which function as input images for model training.
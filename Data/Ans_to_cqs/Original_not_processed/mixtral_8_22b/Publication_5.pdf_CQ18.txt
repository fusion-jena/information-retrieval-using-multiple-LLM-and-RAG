Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Korea for model training and testing. TensorFlow (https://tensorflow.org (accessed on 24 June 2021)), which is an end-to-end open source platform for DL, was used to develop our species richness estimation model. In developing DL models, there are several hyperparameters that need to be tuned prior to training the model, but there are common sets of rules or heuristics governing parameter tuning. After iterative grid search parameter tuning, using a small subset of our data, 5 hidden layers (ùêáùëó,ùëó=1,2,‚ãØ,5) were used, and the number of neurons (n) in Remote Sens. 2021, 13, 2490

The training process was performed for a number of iterations in which all the training
data were exposed to the network until the loss function reached its minimum value. The
model score reached its maximum after approximately 5000 iterations with a NVIDIA
Titan X GPU (3584 CUDA cores). The number of trainable parameters was 85,569 and the
computational run-time was approximately 4 h with a training batch size of 1024.

Deep learning approaches have shown promising results in many scientiÔ¨Åc applica-
tions [67,68] but do not always guarantee better outcomes [69,70]. Therefore, we compared
the performance of the proposed DL model with a RF regression model as a baseline, since
RF showed reasonable results and is popular in various machine learning applications [71].
To determine the best values of the hyperparameters (number of trees, maximum tree
depths, and the maximum number of features) in the RF model, a grid search was used.

10 of 20

Table 1. Comparison of statistical accuracy metrics between the random forest and deep learning
models.

Model Types

MAE (Mean
Absolute Error)

RMSE (Root Mean
Square Error)

Bias

Correlation

Random Forest
Deep Learning

61.1028
28.8105

78.9512
38.5759

0.5656
10.2055

0.8843
0.9752

Due to the unique characteristics of neural networks, which solve problems by ex-
ploiting the hidden relationships inherent in multiple input variables, it was difÔ¨Åcult to
physically quantify the importance of the input variables. As an alternative, we performed
a statistical feature importance test (SFIT) to explain which feature had the greatest sig-
niÔ¨Åcance in the species richness retrievals and to determine the optimized features in
an operational retrieval system. For the SFIT, a single feature was randomly shufÔ¨Çed,
while all the other features were kept constant. We iterated this process by changing the
test variable. The feature importance shows the extent to which the model performance
decreased with random shufÔ¨Çing. In this study, we used the root mean square error (RMSE)
as the performance metric.

2.5. Independent Validation of Species Richness

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The strategy implemented to monitor the model performance during training involved exposing all the training data to the network for a number of iterations until the loss function reached its minimum value. This process was carried out using a NVIDIA Titan X GPU (3584 CUDA cores), and the model score reached its maximum after approximately 5000 iterations. The number of trainable parameters was 85,569 and the computational runtime was around 4 hours with a training batch size of 1024. Additionally, the performance of the proposed Deep Learning model was compared with a Random Forest regression model as a baseline to ensure optimal performance.
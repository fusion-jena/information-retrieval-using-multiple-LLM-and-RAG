Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.2. Indications for policy-making authorities 

4.3.2. Data limitations

4.3. Advantages and drawbacks 

4.3.1. Model prediction beyond the training ranges

in the basin, based on 180 resident speciesâ€™ data. As species observation 
data,  we  use  data  from  a  public  authoritative  repository  of  big  data 
(GBIF)  improved  with  reliable  records  we  accurately  and  directly 
observed in the field.

After  the application of  the data pre-processing algorithm, we ob-
tained 17 datasets: one for ET, one for WS, one for C, 7 for T, and 7 for P. 
We  used  these  data  as  proxy  environmental  conditions  for  annual 
environmental snapshots in the years 1950, 1999, 2016, 2050, and 2100 
(under the RCP4.5 and 8.5 scenarios, for the last two). 

2.3. Species observation data

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific format of the data used in the deep learning pipeline. However, it mentions using data from a public authoritative repository of big data (GBIF), which is likely to contain various types of data such as images or CSV files. Additionally, the mention of obtaining 17 datasets suggests that structured data formats like CSV might be used. The text also refers to'species observation data,' implying that visual data like images could potentially be part of the dataset. Nevertheless, without more specific details, it cannot be definitively stated what exact data formats are used in the deep learning pipeline.
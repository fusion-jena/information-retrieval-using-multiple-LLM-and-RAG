Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Another considerable experimental result is that Elastic Net can improve the perfor-
mance of the proposed model. A good deep learning model usually requires abundant
data to train and analyze, while the limitations of obtaining DNA barcode sequences of ﬁsh
species from different families and the problem of overﬁtting in small datasets are more
and more serious. To solve the overﬁtting problem in training process on small datasets
is of great importance. In our study, Elastic Net is used to solve overﬁtting problem and
improve the generalization ability of the ESK model. Moreover, genetic characteristics
of species belong to high-dimensional data, which are time consuming during training.
However, directly combining a set of fully connected EN-SAE is often has little effect for
extracting useful information. Elastic Net provides sparse connection, which can also save
training time. Therefore, Elastic Net can improve the performance of proposed model.

36.

Random Forest supervised learning model. BMC Genet. 2019, 20, 2. [CrossRef] [PubMed]
Jin, S.; Zeng, X.; Xia, F.; Huang, W.; Liu, X. Application of deep learning methods in biological networks. Brief. Bioinform. 2021, 22,
1902–1917. [CrossRef]

37. Kumar, S.; Stecher, G.; Li, M.; Knyaz, C.; Tamura, K. MEGA X: Molecular Evolutionary Genetics Analysis across Computing

Platforms. Mol. Biol. Evol. 2018, 35, 1547–1549. [CrossRef]

38. Chu, Z.; Yu, J. An end-to-end model for rice yield prediction using deep learning fusion. Comput. Electron. Agric. 2020, 174,

105471. [CrossRef]

39. Chen, J.; Sathe, S.; Aggarwal, C.; Turaga, D. Outlier Detection with Autoencoder Ensembles. In Proceedings of the 2017 SIAM

International Conference on Data Mining (SDM), Houston, TX, USA, 27–29 April 2017; pp. 90–98. [CrossRef]

Symmetry 2021, 13, 1599

16 of 16

40. Homoliak, I. Convergence Optimization of Backpropagation Artiﬁcial Neural Network Used for Dichotomous Classiﬁcation of

Intrusion Detection Dataset. J. Comput. 2017, 143–155. [CrossRef]

41. Vincent, P.; Larochelle, H.; Lajoie, I.; Bengio, Y.; Manzagol, P.A. Stacked Denoising Autoencoders: Learning Useful Representations

in a Deep Network with a Local Denoising Criterion. J. Mach. Learn. Res. 2010, 11, 3371–3408.

42. Taaffe, K.; Pearce, B.; Ritchie, G. Using kernel density estimation to model surgical procedure duration. Int. Trans. Oper. Res. 2018,

28, 401–418. [CrossRef]

43. Liu, F.T.; Ting, K.M.; Zhou, Z.H. Isolation-Based Anomaly Detection. ACM Trans. Knowl. Discov. Data 2012, 6, 1–39. [CrossRef]
44. Gou, J.; Liu, G.; Zuo, Y.; Wu, J. An Anomaly Detection Framework Based on Autoencoder and Nearest Neighbor. In Proceedings
of the 2018 15th International Conference on Service Systems and Service Management (ICSSSM), Hangzhou, China, 21–22 July
2018; IEEE: Piscataway, NJ, USA, 2018; pp. 1–6. [CrossRef]

h = σe(Wx + b)

(3)

where x denotes the input data vector, W is the weight matrix connecting the input and
hidden layers, b is the bias vector belonging to the latent layer nodes, and σe represents the
activation function, such as Sigmoid, Relu, Tanh, etc.

(2) Decoder: in this step, the hidden representation h is mapped into reconstruction

vector y, the typical form as follows:

y = σd(W(cid:48)h + b(cid:48))

(4)

where W’ is the weight matrix connecting the latent and output layers, b’ is the bias vector,
and σd represents the activation function.

Loss function is deﬁned to measure the reliability of SAE. SAE is trained to reconstruct
the features of input and adjust the weights of the encoder and decoder to minimize
the error between the output and the input. Thus, loss function is introduced, which is
expressed in terms of mean square error as follows:

L(W, b) = ∑(cid:107)y − x(cid:107)2

(5)

Symmetry 2021, 13, 1599

6 of 16

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps may be required depending on the nature of the dataset. These steps help ensure that the data is clean, consistent, and suitable for use by the machine learning algorithm. Some common preprocessing techniques include:

1. Normalization or standardization: This involves rescaling the values in each feature column so they have zero mean and unit variance. This helps prevent any one feature from dominating the others due to its scale. Common techniques include min-max normalization, z-score normalization, and decimal scaling.

2. Scaling: Similar to normalization, scaling involves transforming the range of independent variables or predictors to a certain range, typically between 0 and 1. This ensures that all features contribute equally to the final outcome.

3. Cleaning: This involves removing missing values, outliers, and irrelevant observations from the dataset. Missing values can be replaced with mean, median, mode, or other imputation strategies. Outliers can be detected using statistical measures like the interquartile range (IQR). Irrelevant observations can be removed based on domain knowledge or correlation analysis.

4. Encoding categorical variables: Machine learning algorithms cannot handle non-numeric inputs. Hence, categorical variables need to be converted into numerical representations. Techniques like label encoding, one-hot encoding, and binary encoding can be employed.

5. Feature engineering: This involves creating new features from existing ones to better capture patterns in the data. For example, polynomial features can be created from continuous variables, interaction terms can be generated from two or more variables, and binning can be applied to convert continuous variables into discrete categories.

These preprocessing steps not only enhance the quality of the data but also improve the accuracy and efficiency of the deep learning models. However, it is important to note that these steps should be performed judiciously, keeping in mind the specific requirements and constraints of the task at hand.
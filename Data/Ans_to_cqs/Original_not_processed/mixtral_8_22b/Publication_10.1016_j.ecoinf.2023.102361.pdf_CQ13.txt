Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.2. Split the datasets 

In this research, the datasets were randomly divided into three parts 
for training, validation and testing. Here, 60% of the data was used to 
train the deep learning models, and 20% of them was kept to validate the 
models. The remaining 20% data was used to evaluate the performance 
of the models. Table 1 show the number of data in the dataset and how 
they are separated for training, testing and validation. 

2.3. Deep learning models

Resize 

GAN 

Change 

Resize 

GAN 

Change 

MobileNetV2 

VGG16 

VGG19 

ResNet-50 

Inception-V3 

Inception-ResNetV2 

Xception 

DenseNet121 

DenseNet169 

DenseNet201 

Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 

93.84 
93.84 
93.76 
89.23 
88.97 
88.72 
87.88 
87.92 
87.61 
91.71 
91.65 
91.47 
93.37 
93.39 
93.32 
92.51 
92.53 
92.38 
93.91 
93.90 
93.85 
94.63 
94.61 
94.58 
94.36 
94.39 
94.35 
96.38 
96.38 
96.37 

95.29 
95.30 
95.26 
93.70 
93.70 
93.63 
93.02 
92.96 
92.83 
95.71 
95.73 
95.70 
93.09 
93.07 
93.01 
96.24 
96.21 
96.19 
96.18 
96.15 
96.14 
96.69 
96.69 
96.68 
97.11 
97.12 
97.11 
98.49 
98.49 
98.49

VGG16 and VGG19 are classical architectures that are well known 
for  simplicity  and  uniformity.  These  models  are  suitable  for  smaller 
datasets and can provide better accuracy by fine-tuning the pre-trained 
network (Sukegawa et al., 2020). The models have several drawbacks, 
such  as  vanishing  gradient  problems  and  loss  of  fine-grained  spatial 
information (Pan et al., 2020). On the other hand, ResNet-50 contains 
residual  connections,  which  can  overcome  the  vanishing  gradient 
problem and enable training very deep networks. The model performs 
well on both large and small datasets (Al-Masni et al., 2020).

2.3. Deep learning models 

The  selection  of  the  deep  learning  models  for  image  classification 
depends  on  available  computational  resources  and  the  trade-offs  be-
tween the model complexity and performance (Druzhkov and Kustikova, 
2016;  Li  et  al.,  2018).  To  test  the  performance  of  our  technique,  we 
selected  the  following  deep  learning  models:  VGG16  (Simonyan  and 
Zisserman, 2014), VGG19 (Simonyan and Zisserman, 2014), ResNet-50 
(He  et  al.,  2016),  Inception-V3  (Szegedy  et  al.,  2016),  InceptionRes-
NetV2  (Szegedy et  al., 2017), Xception  (Chollet,  2017), DenseNet121 
(Huang et al., 2017), DenseNet169 (Huang et al., 2017), DenseNet201 
(Huang et al., 2017) and MobileNetV2 (Sandler et al., 2018). For the 
sake  of  brevity,  we  briefly  summarise  the  main  attributes  of  these 
techniques.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: Unfortunately, there is no information provided about the specific hyperparameters used in the deep learning models such as learning rate or optimizer. Therefore, I cannot provide a helpful response to this query based on the given context.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In  the  proposed  work,  the  down-sampling  (contract-
ing)  path  repeatedly  applies  a  block  comprised  of  two 
3 × 3  convolutions,  followed  by  batch-normalization, 
a  Rectified  Linear  Unit  (ReLU)  activation  and  a  2 × 2 
max-pooling  layer  of  stride  2.  The  number  of  feature 
map  channels  in  the  contracting  path  is  doubled  at 
each  down-sampling  block.  Similarly,  the  up-sampling 
(expansive)  path  replaces  the  max-pooling  layers  with 
up-sampling layers that apply nearest-neighbor interpo-
lation.  Analogous  to  the  contracting  path,  the  number 
of  feature  map  channels  is  halved  at  each  up-sampling 
block.  The  feature  maps  of  the  up-sampling  path  are 
concatenated  with  the  feature  maps  of  the  contracting 
path. Finally, the output layer results by applying a 1 × 1 
convolution.

Loss function

The  Model-K  architecture  is  a  regression  model  based 
on  VGG16  without  the  feature  extractor  on  top.  The 
output layer was flattened and given as input to 2 fully 
connected  (FC)  layers  with  linear  output.  The  regres-
sion  model  was  designed  to  predict  classwise  (five 
categories)  count.  To  compare  it  with  the  proposed 
solution,  we  modify  the  model  by  connecting  the  out-
put  layer  with  a  fully  connected  one  output  neuron. 
Model-K  was  initialized  with  pre-trained  Imagenet 
weights  and  then  trained  using  our  training  data  set 
with  a  Stochastic  Gradient  Descent  (SGD)  optimizer 
and  an  MSE  loss  function.  The  proposed  Model-2 
with  EfficientNet  feature  extractor  reached  an  RMSE 
value  of  1.88  and  0.60  for  the  SLL  and  elephants’  data 
sets,  respectively,  performing  better  than  the  Model-K 
with  an  RMSE  of  2.17  and  0.81  for  SSL  and  elephants’

Page 6 of 10

EfficientNet-B5  feature  extractor  [27].  EfficientNet  is  a 
CNN  developed  by  Google,  characterized  by  high  accu-
racy  and  computational  efficiency.  Model-2  was  initial-
ized  by  pre-trained  weights  based  on  the  Imagenet  data 
set  [28].  All  the  parameters  were  optimized  using  the 
Adam optimizer with a learning rate of 0.001.

Training

An  Nvidia  GeForce  RTX  2060  GPU  was  used  for  train-
ing,  with  a  batch  size  of  8.  Model-1  (without  feature 
extractor)  was  trained  for  7  h  and  Model-2  (with  a  fea-
ture  extractor)  for  17  h.  Based  on  the  model’s  perfor-
mance on the validation set, the early stopping technique 
was  applied  to  avoid  over-fitting.  Model-2,  which  used 

pre-trained  weights  and  thus  some  prior  relevant  infor-
mation, converged faster than Model-1 (Fig. 3).

Model evaluation: testing

2018. CoRR arXiv: 1807. 11809

 16.  Kamilaris A., van den Brink C., Karatsiolis S. (2019) Training Deep Learning 
Models via Synthetic Data: Application in Unmanned Aerial Vehicles. 
In: Vento M. et al. (eds) Computer Analysis of Images and Patterns. CAIP 
2019. Communications in Computer and Information Science, vol 1089. 
Springer, Cham. https:// doi. org/ 10. 1007/ 978-3- 030- 29930-9_8

 17.  Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, 
Courville A, Bengio Y. Generative adversarial nets. In: Proceedings of the 
27th international conference on neural information processing systems. 
NIPS’14. MIT Press, Cambridge, MA, USA; 2014, vol. 2, p. 2672–2680.
 18.  Young-Chul Yoon K-JY. Animal detection in huge air-view images using 

cnn-based sliding window. In: International workshop on frontiers of 
computer vision (IWFCV); 2018.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Unfortunately, there isn't any explicit mention of specific techniques used to address data bias during preprocessing of the deep learning pipeline in the provided context. However, generally speaking, several common techniques can be employed to mitigate data bias issues. These include stratified sampling, where the dataset is divided into subgroups or strata, ensuring equal representation from all groups when creating training and test datasets. Oversampling involves increasing the frequency of underrepresented classes in the dataset, while undersampling reduces the frequency of overrepresented classes. Another approach is diverse data collection, which aims to gather more representative samples across different categories, reducing potential biases caused by limited or skewed data sources.
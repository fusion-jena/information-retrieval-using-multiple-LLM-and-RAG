Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

windows of 100-m, 500-m, or 1-km radii), (ii) the category level (e.g.: 
within thematic covariate categories), and (iii) using all remainders.

algorithms character vector with the name(s) of the 
algorithms(s) used for the embedding procedure; 
ncov value for the target number of covariates to 
include in the final set; maxncov value for the 
maximum possible number of covariates to include in 
the final set; nthreads value for the number of cores to 
be used during parallel operations    

corcut value of the correlation coefficient threshold 
used for identifying collinearity; categories character 
vector with category-level covariate names; variables 
character vector with variable-level names

2. We introduce a two-step “embedded” covariate selection procedure aimed at optimizing the pre-
dictive ability and parsimony of species distribution models fitted in a context of high-dimensional 
candidate covariate space. The procedure combines a collinearity-filtering algorithm (Step A) with 
three  model-specific  embedded  regularization  techniques  (Step  B),  including  generalized  linear 
model with elastic net regularization, generalized additive model with null-space penalization, and 
guided regularized random forest.

Table 2 
Percentage of species (n = 50) for which a given covariate selection procedure (Em: “embedded”, Fi: “filter”, and Rd: “random”) led to the top Score value (average 
value of Somers’ D, maximum True Skill Statistic, and continuous Boyce index) for each modelling algorithm (GLM: Generalized Linear Model, GAM: Generalized 
Additive Model, and RF: Random Forest). Results are shown for all species, species with the highest number of records (3rd tercile: “More records”), species with the 
lowest number of records (1st tercile: “Less records”), species with the widest spatial coverage in Switzerland (3rd tercile: “High coverage”), and species with the lowest 
spatial coverage (1st tercile: “Low coverage”). See Supplementary material 1: Table S1 for details on species data.   

All species 

More records (3rd tercile) 

Less records (1st tercile) 

High coverage (3rd tercile) 

Low coverage (1st tercile) 

Procedure 

Em 

GLM 
GAM 
RF 

76 
82 
92 

Fi 

22 
18 
8 

Rd 

Em

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the criteria used to split the data for deep learning model training into train, test, and validation sets. However, it mentions several factors that could potentially be used for splitting the data. These factors include the size of the windows (100-m, 500-m, or 1-km radii), the category level within thematic covariate categories, and using all remainders. Additionally, the text discusses the use of three different algorithms for the embedding procedure, namely generalized linear model with elastic net regularization, generalized additive model with null-space penalization, and guided regularized random forest. It is possible that these algorithms could also influence how the data is split for training purposes. Nevertheless, without more specific information, it is difficult to determine exactly what criteria were used for splitting the data.
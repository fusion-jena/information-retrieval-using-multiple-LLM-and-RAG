Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(cid:0)

̂ηs(x) = exp
exp

∑

j

)

θ(x)
f s
(
f j
θ(x)

)

where θ is the set of parameters of the neural network to be optimised by 
minimizing the loss function of Eq. (2). 

Using this very common deep learning framework, it is possible to 
show that the species assemblage predictor Sλ(x) of Eq. (1) is consistent 
λ(x) when  the 
(Lorieul,  2020),  i.e.  it  tends  towards  the  optimal  set  S*
number  of  training  samples  increases.  In  other  words,  our  species 
assemblage predictor is as simple as training a deep neural network with 
a cross-entropy loss function on the presence-only samples and thresh-
olding  the  output  softmax  probabilities  to  get  the  assemblage  of  pre-
dicted species. 

◦

(IDRIS). A full description of the model architecture, dataset spatial split 
and training procedure can be found in supplementary information (SI) 
Box A. Finally, the settings of our species assemblage model are sum-
marised in the Fig. 1.

Our backbone model is an adaptation of the Inception v3 (Szegedy 
et al., 2016). This convolutional neural network learn spatial patterns 
from  two-dimensional  predictors  (Botella  et  al.,  2018a;  Deneu  et  al., 
2021).  A  spatial  block  hold-out  strategy  is  used  to  limit  the  effect  of 
spatial autocorrelation in the data when evaluating the model (Roberts 
et al., 2017). Blocks are defined in the spherical coordinate system ac-
cording to a 0.025
grid (2.8 km square blocks at the equator). The split 
of the training/validation/test spatial blocks (90%/5%/5%) is stratified 
by region to ensure that all regions are represented within each set, see 
Table 1. We use the regions defined by the World geographical scheme 
for  recording  plant  distribution  (WGSRPD)  level  2  (Brummitt  et  al., 
2001). Training is done on Jean Zay, a supercomputer from the Institute 
for  Development  and  Resources  in  Intensive  Scientific  Computing

Zizka, A., Andermann, T., Silvestro, D., 2022. Iucnn–deep learning approaches to 

approximate species’ extinction risk. Divers. Distrib. 28, 227–241. 

EcologicalInformatics81(2024)10262715

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide specific details about the data augmentation techniques applied in the deep learning pipeline. However, based on general practices in deep learning, some commonly used data augmentation techniques include flipping, rotating, scaling, cropping, and color shifting. These techniques help increase the diversity of the training data without collecting new images, which can improve the model's ability to generalize to unseen data. It is important to note that the choice of data augmentation techniques depends on the nature of the problem and the characteristics of the input data. Therefore, without further context or information, it is not possible to determine which specific data augmentation techniques were applied in this case.
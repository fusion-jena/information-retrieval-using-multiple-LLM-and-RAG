Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

74.57 
75.13 
76.20 
73.41 
77.39 
77.25 

70.67 
70.11 
71.30 
69.53 
73.20 
72.74 

66.48 
67.32 
67.46 
65.07 
69.05 
68.71 

68.09 
68.41 
68.68 
66.87 
70.61 
70.22  

Table 3 
Classification performance (%) of the proposed methods and other works on the 
IP102 dataset.  

Name 

Method 

Accuracy 

Precision 

Recall 

F1- 
score 

40.1 

/ 

/ 

/ 

Wu et al. 
(2019) 
Yang et al. 
(2021) 

Liu et al. 
(2022) 
Peng and 
Wang 
(2022) 
An et al. 
(2023) 

Zheng 
et al. 
(2023) 

Wang 

et al. 
(2023) 
Lin et al. 
(2023) 
Ayan et al. 
(2020) 
Ung et al. 
(2021) 

Su et al. 
(2023) 

Ours 

ResNet-50 

CNN + Spatial and 
channel attention 
mechanism 
ViT-B/16 + FRCF +
LSMAE 
CNN and Transformer 
hybrid architecture 

49.4 

73.29 

74.69 

74.58 

/ 

/ 

/ 

/ 

/ 

/ 

/ 

/ 

Feature fusion network 

65.60 

60.9 

59.7 

60.3 

PCNet 

AA-Trans (Backbone: 
ViT-B/16)

Table 1 
General transfer training hyper-parameters for each basic model.  

Hyper-parameter / Model 

ResNet-50 

ViT-S/16 

Volo-d1 

ViP-Small/7 

Learning rate 
Minimum learning rate 
Optimizer 
Scheduler 
Batch size 
Weight decay 
Input size 
Epochs 

1e-4 
2e-3 
1e-5 
1e-5 
Adamw(0.9, 0.999) 
Cosine 
128 
5e-4 
224 Ã— 224 
210  

64 
5e-4 

8e-6 
4e-6 

64 
1e-8 

2e-3 
1e-5 

64 
5e-2 

EcologicalInformatics82(2024)1026936M. Chen et al.                                                                                                                                                                                                                                   

Table 2 
Classification performance (%) of basic models and proposed methods on the 
IP102 dataset.  

which they are applicable. 

5.1. Ablation experiments 

Model 

Accuracy 

Precision 

Recall 

F1-score 

Resnet-50 
ViT-S/16 
Volo-d1 
ViP-Small/7 
VecEnsemble (Ours) 
MatEnsemble (Ours)

Yuan et al. (2022) claimed that the superior performance of a ViT can 
primarily be attributed to extensive pre-training data. If a ViT is trained 
from  scratch  on  a  medium-sized  dataset,  its  performance  lags  behind 
that of a CNN model with comparable parameter sizes. To address this 
limitation, they proposed Volo with the novel Outlook Attention as the 
token mixer module, which enabled fine-grained token representation 
and the aggregation of global information. 

2.3. MLP-like model

We compared the performance of our method with those of previous 
studies using the IP102 dataset, as shown in Table 3. This table presentes 
the  results  of  both  the  single  models  and  ensemble  methods.  By 
comparing the four basic models in Table 2 with the single models in 
Table  3,  it  is  evident  that  our  trained  basic  models  demonstrated 
impressive performances on the IP102 dataset. For example, our trained 
ResNet-50 exhibited a remarkable improvement of approximately 20% 
in accuracy compared to the results reported by Wu et al. (2019), and 
our  trained  ViT-S/16  outperformed  the  larger-scale  AA-Trans  (Wang 
et al., 2023). This notable enhancement can be primarily attributed to 
the implementation of advanced deep transfer-learning strategies and 
the incorporation of multiple data augmentation techniques. Moreover, 
the performance of Volo-d1 exceeded that of some larger-scale single 
models  and  ensemble  methods.  This  observation  highlights  the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The classification performance of the deep learning models is evaluated using several metrics such as Accuracy, Precision, Recall, and F1-score. These metrics provide insights into different aspects of the model's performance. Accuracy measures the overall correctness of the model predictions. Precision indicates the proportion of true positive predictions among all positive predictions made by the model. Recall shows the ratio of correctly predicted positive instances out of all actual positive instances. Lastly, the F1-score combines Precision and Recall into a single metric, providing a balanced measure of the model's performance.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

To  evaluate  the  performance  of  the  CNNs  trained  on  different 
annotation sets, datasets containing other SEIO PBW vocalizations and 
blue whale’s Dcall were chosen. Only a small portion of those datasets 
were used to reduce computational time, as hundreds of networks were 
trained and compared. 

For the evaluation of the SEIO PBW vocalizations, 7 h of audio sig-
nals recorded during the OHASISBIO program were used [Royer, 2009; 
Torterotot et al., 2020]. These recordings were collected at the SWAMS 
site, located in the Indian Ocean’s oceanic zone, between Kerguelen and 
Amsterdam Island. This dataset is called SWAMS in this paper. The re-
cordings were made in March 2015 using a hydrophone deployed at a 
depth of 1000 m. The sampling rate was 240 Hz. A total of 102 vocali-
zations were manually annotated by a second expert.

In this kind of project, volunteer citizen scientists give free time to 
explore  and  annotate  large  datasets.  Contact  with  the  annotator  is 
extremely important: (i) proposing a clear explanation about the aim of 
the  annotation  campaign,  (ii)  answering  questions,  (iii)  organizing 
meetings throughout the campaign and share a report after. Those steps 
motivate the annotators and increase the educational potential of the 
campaign.

directly  to  datasets  where  the  relevance  of  annotations  has  not  been 
evaluated.

During  the  campaign,  methods  are  proposed  to  assess  inter- 
annotator  variability  and  aggregate  annotation  sets.  Those  methods 
will help to define the minimal number of annotators needed [Dubus 
et al., 2023; Walter et al., 2022]. Increasing the number of annotators 
per sample could be helpful if the variability is too high. Finally, the two 
grouping  methods:  majority  voting  (Section  2.2.3)  and  soft  labeling 
(Section 2.3.2) can also be used to produce relevant annotation sets for 
deep learning. 

6. Conclusion 

In this study, new approaches were explored to improve the accuracy 
and  efficiency  of  cetacean  vocalization  detection  using  deep  learning 
models. Leveraging multi-annotation campaigns involving both expert 
and novice annotators aimed to address the challenges associated with 
the scarcity of ground truth data in underwater PAM studies.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: Unfortunately, based on the given information, there is no mention of the specific location or platform where the code repository of the deep learning pipeline is available. Therefore, I cannot provide the exact link or name of the repository. However, it is common practice to publish such resources alongside research papers or projects, so it might be worth checking any related publications or websites for more details.
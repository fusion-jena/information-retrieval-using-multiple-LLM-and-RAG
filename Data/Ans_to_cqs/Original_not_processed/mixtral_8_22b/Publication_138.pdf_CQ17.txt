Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(4) VGG is proposed by Simonyan et al. in 2014, in which the convolution filter is a 3 × 3 filter 
and  the  stride  is  2.  VGG-11,  VGG-16  and  VGG-19  respectively  include  11,  16  and  19  layers.  The 
Softmax layer is the final layer for classification [24]. 

(5) ResNet is proposed by Kaiming et al. in 2015 to address to the vanishing gradient problem and 
shows the excellent ability of classification [25]. The popular ResNet consists of 49 convolution layers. 
All  the  algorithms  in  our  experiments  are  coded  in  Matlab  R2019a.  The  values  of  initial 
parameters  are  randomly  generated  for  networks.  The  parameters  are  set  as  follows:  the  number  of 
epochs, learning rate drop factor, learning rate drop period, batch size are 200, 0.0004, 0.6, 30 and 128, 
respectively. The operating system of PC is Window 7 and the GPU is a NVIDIA with 8 GB memory. 

3.4. 

Results and discussions

The pooling operator is connected after this layer for extracting the main features of a certain area, 
reducing the number of parameters, and preventing the model from over fitting. The maximum pooling 
with a 2 × 2 window and 2 strides is used in this layer. The pooling result is the data with the size of 56 
× 56 × 64.  

After pooling, the LRN (Local Response Normalization) is used to normalize the local response. 
The LRN enhances the larger response value and reduce the smaller response value. Thus, the LRN 
also can improve the generalization of the model. The output result of the LRN is the data with the size 
of 56 × 56 × 64.

(1) LeNet is firstly proposed by LeCun et al. in the 1990s, which includes three convolution layers, 
two  sampling  layers,  two  fully  connected  layers.  At  that  time,  it  is  difficult  to  run  LeNet  due  to  the 
limit of computation and memory capacity [21]. 

(2) AlexNet is proposed by Alex et al. and won the ILSVRC 2012 [22]. AlexNet achieves higher 
identifying accuracy than all the traditional machine learning algorithms. It is significant breakthrough 
for machine learning for classification. 

(3) GoogLeNet is proposed by Christian of Google and is the winner of ILSVRC 2014 [23], in 
which inception layers, including different receptive areas with different kernel sizes capturing sparse 
correlation patterns, are integrated into CNN.

networks, Commun. ACM, 60 (2017), 84–90. 

23.  C.  Szegedy,  W.  Liu,  Y.  Jia,  P.  Sermanet,  S.  Reed,  D.  Anguelov,  et  al.,  Going  Deeper  with 
Convolutions,  2015  IEEE  Conference  on  Computer  Vision  and  Pattern  Recognition  (CVPR), 
Boston, MA, 2015. 

24.  K.  Simonyan,  A.  Zisserman,  Very  deep  convolutional  networks  for  large-scale  image 

recognition, Comput. Sci., 2014 (2014), 21–30. 

25.  K.  He,  X.  Zhang,  S.  Ren,  J.  Sun,  Deep  residual  learning  for  image  recognition,  2016  IEEE 
Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, 2016. 

©2021 the Author(s), licensee AIMS Press. This is an open access article 
distributed  under  the  terms  of  the  Creative  Commons  Attribution 
License (http://creativecommons.org/licenses/by/4.0) 

Mathematical Biosciences and Engineering 

Volume 18, Issue 2, 1121–1135.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, there is no explicit mention of specific regularization methods such as dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. However, one method mentioned that could potentially help prevent overfitting is Local Response Normalization (LRN).

LRN is a technique used in Convolutional Neural Networks (CNNs) where the input neurons compete against each other, resulting in the enhancement of larger responses and suppression of smaller ones. By doing so, LRN helps to improve the generalization of the model, making it more robust and less prone to overfitting.

While not explicitly stated, it can be inferred that techniques like dropout and L2 regularization may also have been employed in the deep learning pipelines discussed in the context. Dropout is a widely used regularization technique in neural networks where some neurons are randomly ignored during training, helping to prevent overfitting. On the other hand, L2 regularization adds a penalty term to the loss function, encouraging smaller weights and thus reducing the complexity of the model.

In summary, while the context does not provide explicit information about the use of regularization methods like dropout or L2 regularization, it mentions the use of Local Response Normalization (LRN) as a potential means to prevent overfitting. Additionally, based on common practices in deep learning, it can be assumed that techniques like dropout and L2 regularization might also have been utilized.
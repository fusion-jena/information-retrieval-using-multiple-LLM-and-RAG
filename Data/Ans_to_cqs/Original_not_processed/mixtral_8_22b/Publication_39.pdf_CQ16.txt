Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Feichtenhofer et al. (2019). This parameter combination achieved the 
best  results  in  our  experiments.  The  Slow  and  Fast  pathways  are 
implemented by a ResNet-101. 

We  split  the  annotated  dataset  AnnotationsActions  (cf.  Section  2.2) 
randomly (but fixed for all experiments and all networks) into a train set, 
validation set and test set. The test set includes 20% of the video clips of 
each class. Of the remaining clips, again 20% of the clips in each class 
form the validation set. The remaining clips are the training set. 

4.4.1. Data augmentation for action recognition

Stochastic Gradient Descent (SGD) is used as optimization technique. 
We start with a learning rate of 0.0005, a momentum of 0.9 and weight 
decay of 0.0005. Different settings of momentum and weight decay are 
checked but show no improvements. In the training process, we decrease 
the learning rate every 10 epochs by multiplying it with γ = 0.1. In the 
first  epoch  we  use  warmup  iterations  for  the  learning  rate  with  a 
warmup factor of 1/1000. These parameters gave the best results in our 
evaluations. We train our models for 30 epochs.

The SlowFast architecture is trained in 40 epochs with an initial base 
learning rate of 0.1. We use 5 warmup epochs with a warmup learning 
rate of 0.001. Momentum is set to 0.9 and the weight decay parameter is 
(cid:0) 4.  The  learning  rate  is  decreased  with  a  cosine  strategy.  The 
1  ⋅  10
optimization technique is SGD and the loss function is the cross entropy 
loss. In this parameter selection we follow Feichtenhofer et al. (2019). 
We set the temporal length of the input to 16 frames. We set the sam-
pling rate to 4 and α = 2. The parameter β is set to 1/8 as proposed by 

EcologicalInformatics61(2021)1012159F. Schindler and V. Steinhage

AP0.50 + AP0.55 + … + AP0.95
10

(6) 

We  also  consider  AP0.50  and  AP0.75,  which  are  part  of  the  above 
calculation as an evaluation metric. AP0.50  is the AP for IoU threshold 
0.50,  also  known  as  Pascal  VOC  metric  and  AP0.75  is  the  AP  for  IoU 
threshold 0.75, also called strict metric in the sense of COCO. 

4.1.2. Average recall 

The average recall (AR) is averaged over different IoU thresholds. 
For  COCO  datasets  we  consider  the  10  IoU  thresholds  0.50,  0.55,  … 
0.95.  They  are  identical  to  (6).  Therefore,  the  AR  for  class  i  at  IoU 
threshold th is computed as 
r0.50 + r0.55 + … + r0.95
10

ARi,th =

(7) 

The AR is also averaged over all K classes. This average can also be 
called the mean average recall (mAR), but in COCO the mAR is denoted 
as AR. The ARth at IoU threshold th is calculated as 

ARth =

∑K

i=1ARi,th
K

(8) 

We  consider  two  different  ARs,  ARmax=1  and  ARmax=10.  They  are

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, the model is trained for a specific number of epochs rather than using a criterion such as a validation loss plateau to determine when training is complete. Specifically, the SlowFast architecture is trained for 40 epochs, while other models are trained for 30 epochs. During the training process, the learning rate is adjusted according to predefined schedules. For example, in the case of the SlowFast architecture, the learning rate is decreased with a cosine strategy, whereas for other models, the learning rate is decreased every 10 epochs by multiplying it with a factor of 0.1. Additionally, warmup iterations are used for the learning rate during the first epoch, with a warmup factor of 1/1000.
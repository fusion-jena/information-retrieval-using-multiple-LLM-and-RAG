Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

To train the EfficientNet, we prepared 4000 images for each category 
using the data augmentation method proposed above. We used 80% of 
the 4000 images for training and the remaining 20% for validation. The 
EfficientNet was trained by the Adagrad in 100 epochs. The dropout rate 
for each task was set at 0.5. The batch size was set at 20. 

All evaluations were performed on the NVIDIA GeForce RTX 2080Ti 
GPU with 11G memory, and the software was Windows 10, Python 3.7, 
CUDA 10.2. Keras 2.4.3 were used to build all models. 

To compare model results with the test video, we used IoU as the 
threshold  for  positives  or  negatives  based  on  the  object  confidence 
scores estimated by the object detector. IoU was defined as the inter-
section  over  the  union  of  the  two  bounding  boxes,  which  were  the 
ground-truth  bounding  box  and  the  predicted  bounding  box.  In  this 
paper, we set the threshold of IoU at 0.3 to consider correct or incorrect 
detection results.

these methods. By applying random erasing to the training dataset of the 
EfficientNet, performance was also moderately improved (Table 6).

To re-identify the candidate regions, we inputted outputs from the 
YOLOv4  into  the  EfficientNet.  As  mentioned  above,  there  were  eight 
types of the EfficientNet. Starting from the baseline EfficientNet-B0, the 
baseline network was scaled up from EfficientNet-B1 to B7 based on the

The  past  development  of  CNN  was  synonymous  to  the  increased 
depth of the network. By increasing the depth and widening the channel 
size, a more complex network achieved a high resolution of the image 
data. In addition, it resulted in more fine-grained characteristics. This 
development in the network size improved the classification accuracy of 
the network, but also led to the problem of the high computational cost 
of the gradient explosion parameter. ResNet (He et al., 2015) proposed 
that skip connection could avoid gradient explosion skillfully. Mobile-
Net (Howard et al., 2017) used pointwise and depthwise convolutions to 
reduce network parameters and improve training efficiency. The SENet 
(Hu et al., 2017) weighted various features by loss of network training in 
order to achieve better results in model training.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, the optimization technique used to train the EfficientNet is Adagrad. Adagrad is an algorithm for gradient-based optimization that adapts the learning rate to the parameters, performing smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequent features. It does so by dividing the learning rate for a weight by the square root of the sum of the squares of its previous gradients. This adaptive learning rate helps to prevent overfitting and improves convergence speed. Other popular optimization techniques include Stochastic Gradient Descent (SGD), Adam, and RMSprop, among others. However, the context only mentions the use of Adagrad.
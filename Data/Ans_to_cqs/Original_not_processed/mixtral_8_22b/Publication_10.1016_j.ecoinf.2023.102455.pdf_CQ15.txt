Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiGRU Layers = 1, BiGRU neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 43,batch_size = 32, 
learning_rate = 0.000455, Dense = 100 
CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiGRU Layers = 1, BiGRU neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 1000,batch_size = 32, 
learning_rate = 0.0015, Dense = 100 
num_nodes: 20, regular_para = 0.2, 
weight_random_range = [(cid:0) 1,1], 
bias_random_range = [0,1], 
num_nodes: 50, regular_para = 0.9, 
weight_random_range = [(cid:0) 1, 1], 
bias_random_range = [0, 1], 
Regularization_coefficient = 3E+05, 
kernel_parameter =7000 
Regularization_coefficient = 9E+06, 
kernel_parameter =9E+05 
CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiRNN Layers = 1, BiRNN neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 200,batch_size = 32, 
learning_rate = 0.0002, Dense = 100 
CNN Layers = 2, filters = 64, kernel_size =

CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiRNN Layers = 1, BiRNN neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 200,batch_size = 32, 
learning_rate = 0.0002, Dense = 100 
CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiGRU Layers = 1, BiGRU neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 40,batch_size = 32, 
learning_rate = 0.000455, Dense = 100 
CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiGRU Layers = 1, BiGRU neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 1000,batch_size = 32, 
learning_rate = 0.0015, Dense = 100 
num_nodes: 20, regular_para = 0.05, 
weight_random_range = [(cid:0) 1, 1], 
bias_random_range = [0, 1], 
num_nodes: 30, regular_para = 2, 
weight_random_range = [(cid:0) 1, 1], 
bias_random_range = [0, 1], 
Regularization_coefficient = 9E+05, 
kernel_parameter =9E+04 
Regularization_coefficient = 1E+06, 
kernel_parameter =9E+04 
CNN Layers = 2, filters = 64, kernel_size =

CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiRNN Layers = 1, BiRNN neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 200,batch_size = 32, 
learning_rate = 0.0001, Dense = 100 
CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiRNN Layers = 1, BiRNN neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 200,batch_size = 32, 
learning_rate = 0.0001, Dense = 100

et al., 2023). Table 3 represents the optimal hyperparameters related to 
the complementary models for -one and -three days ahead forecasting 
the streamflow in Bear and Winter Rivers of PEI. The important hyper-
parameters  for  CNN-BiGRU  are  listed  in  Table  3,  including  the  CNN 
Layers,  BiGRU  Layers,  kernel  parameter,  Adam,  activation  function, 
CNN  filters,  epochs,  and  learning_rate.  It  is  worth  mentioning that  to 
prevent cumulative errors caused by the multiscale decomposition of the 

decomposed signals and convergence improvement, all inputs and target 
parameters are normalized between 0 and 1 by the following formula-
tion (Jamei et al., 2022a): 

Xnorm =

X (cid:0) Xmin
Xmax (cid:0) Xmin

(37)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The optimization technique used in the deep learning pipeline mentioned in the given context is Adam. Adam is a popular optimization algorithm used in training neural networks due to its ability to adaptively adjust the learning rate based on the first and second moments of gradients. This makes it more efficient and effective compared to other optimization algorithms such as Stochastic Gradient Descent (SGD), which uses a fixed learning rate throughout the training process. In this case, Adam was chosen as the optimizer with different learning rates depending on the specific model being trained. For example, one model had a learning rate of 0.000455 while another had a learning rate of 0.0015. Overall, using Adam as the optimization technique helped improve the performance of the deep learning pipeline in predicting streamflow in Bear and Winter Rivers of PEI.
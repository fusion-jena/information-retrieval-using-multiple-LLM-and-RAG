Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

for model validation, either in addition to internal validation strategies
(n = 22), or as the sole validation method (n = 17). Notably, a consid-
erable number of studies did not use an explicit validation method to
measure model performance or did not clearly state this in the text (n =
94, 32%) (category “not provided” in Fig. 10).

(TSS, Cohen's Kappa, Sensitivity, Specificity, Accuracy and Precision). In
contrast, there appears to be comparatively less emphasis on calibration
metrics like Brier score. Of course, when choosing metrics, it is essential
to consider the type of response variable being modelled, whether
qualitative, quantitative, or semi-quantitative (as illustrated in 15.1 of
Guisan et al., 2017). While Table S7 is not exhaustive in terms of all
utilised metrics in research for cetaceans and sea turtles, it can provide
valuable assistance in understanding most commonly used metrics.

In our review of model validation metrics, half of the examined
studies relied on a single metric, and some even omitted the specifica-
tion of the metric used, posing a challenge to achieving a comprehensive
overview of metrics utilised. This indicates a need for improvement in
metric selection and clarity. Aligning with the recommendation of
Guisan et al. (2017), best practice involves incorporating multiple
validation metrics when assessing model performance to ensure the
integration of different aspects of the model's predictive capabilities: i.e.,
the extent to which a model correctly predicts the conditional proba-
bility of presence (calibration) and the ability to distinguish between
occupied and unoccupied sites (discrimination). It is interesting to note
that our research has shown a greater emphasis on discrimination
metrics, such as AUC-ROC, or those derived from two-way contingency
tables comparing presence-absence observation to primary predictions

A diverse set of validation metrics emerged from the analysis of
studies performing model predictions (n = 260), with the most common
displayed in Fig. S2. The Area Under the Receiver Operating Charac-
teristic Curve (AUC-ROC curve) was the predominant choice (n = 144,
55%). However, in many cases, the metrics used were not clearly stated
or provided in the text (n = 59, 23%). Approximately half of the studies
(n = 104, 40%) considered more than one validation metric simulta-
neously to assess different aspects of model performance. Multiple
software tools were employed for modelling, with a focus on R and its
associated packages, and listed in Table S3.

3.6. Model results

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The study primarily focuses on species distribution models rather than deep learning models specifically. However, several metrics used to evaluate these models overlap with those used for evaluating deep learning models. The most frequently used metric in the reviewed studies was the Area Under the Receiver Operating Characteristic Curve (AUC-ROC curve), appearing in 55% of the studies. Other popular metrics included TSS, Cohen's Kappa, Sensitivity, Specificity, Accuracy, and Precision. These metrics help measure various aspects of model performance, including the ability to discriminate between occupied and unoccupied sites and the correct prediction of the conditional probability of presence. Interestingly, there seems to be less emphasis on calibration metrics like the Brier score. Half of the examined studies relied on a single metric, while others didn't specify the metric used. To achieve a comprehensive overview of model performance, it is recommended to incorporate multiple validation metrics, aligning with the suggestion by Guisan et al. (2017).
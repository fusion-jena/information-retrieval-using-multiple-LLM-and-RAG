Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

split  data  with  the  highest  mAP50  (0.987)  for  the  prediction  of  our 
model.

When  testing  Model  4  on  a  test  dataset  of  preprocessed  original 
images from our dataset, it can be seen that on average, only 51.5% of 
the images in the test batch were classified correctly (Table 8). Espe-
cially among examples of the digits 0, 2, and 5, low values (below 40%) 

are observed, whereas the digits 1 and 4 (with values above 70%) are 
predicted correctly more often (for exact values, see tables in the pub-
lished dataset). Furthermore, there were multiple misclassifications of 
clusters that showed streaks or other irregularities. 

However,  when  applying  the  model  to  images  of  the  extracted 
clusters from the color recognition, the numbers can be recognized with 
an accuracy of 97.3%, given the ideal train-test split.

Using this ideal split, a test was conducted using unprocessed data. 
This indicates that the test dataset appeared to be similar to the training 
data  because  it  consisted  of  images  of  the  extracted  clusters  without 
further processing. 

The  dataset  used  in  this  study  included  single-tree  point  clouds, 
images of single-trees, and *. csv containing the attributes used to pro-
duce the images, reference data, clusters of numbers (in two different 
point-thickness settings), labeled numbers, preprocessed images of the 
numbers, numbers from the best split, and tables containing the results 
of  our  digit  recognition  are  freely  available  under:  Doi:  https://doi. 
org/10.5281/zenodo.10663114. 

3. Results 

3.1. Classification accuracy

Image preprocessing was conducted using Python with the OpenCV 
library (Python Software Foundation, 2023b). The last step was image 
inversion, in which white pixels (representing markings) were switched 
to black pixels, and the black pixels (representing the background) were 
switched to white pixels. 

The  YOLOv8-framework,  an  end-to-end  neural  network  (Jocher 
et al., 2023), was used for model-building training, and a Python (Py-
thon Software Foundation, 2023a) script was used to convert all labels 
into the required format. These labels were annotated to the original via 
Labelbox (Labelbox, 2023) and consisted of the numbers that each image 
was  supposed  to  show.  A  separate  script  downloaded  all  scatterplot 
images,  including  labels  created  through  Labelbox.  Moreover,  the 
required data structure was automatically created, and images and la-
bels were moved to the corresponding folders for training, testing, and 
validation.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the provided context, no specific technique is mentioned to address data bias during preprocessing of the deep learning pipeline. However, some general practices can be identified. For instance, the use of stratified splitting is implied by mentioning the 'ideal train-test split'. This method ensures that both the training and test datasets have roughly the same distribution of classes, thus reducing potential biases caused by class imbalance. Additionally, the context mentions the availability of diverse data, such as single-tree point clouds, images of single trees, and CSV files containing various attributes. This suggests that diverse data collection could also help mitigate data bias. Nevertheless, explicit methods like oversampling, undersampling, or SMOTE are not discussed in the provided context.
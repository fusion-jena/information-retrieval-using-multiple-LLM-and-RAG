Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.2.5. Classification 

Each  ML  approach  requires  hyperparameters  to  classify  imagery, 
which when optimized during training can increase model performance, 
see  Table  4  for  a  hyperparameter  glossary.  Given  the  computational 
efficiency of the SVMs and the few hyperparameters required, each of 
these can be optimized simply and relatively quickly (subject to dataset 
size)  during  a  k-fold  (k = 5) cross-validated  fine  grid-search  on  the 
training  data.  For  our  CNN  þ SVM  method,  we  followed  hyper-
parameter  recommendations  by  (Hsu  et  al.,  2016),  authors  of  the 
LIBSVM library (Chang and Lin, 2011). For our non-linear RBF SVM we 
searched  hyperparameters  C = 23, 23.25, …, 27  and  γ = 2
(cid:0) 13  & 
(cid:0) 11. For the linear SVM, we used the same hyperparameter search for 
2
its sole parameter C. We also looked at the RBF and linear SVM with 

(cid:0) 15, 2

Fig. 3. A diagram of various support vector machines.

test accuracy are near equivalent, to minimize over- and under- fitting. 
In addition, with each application, time-consuming tuning and explo-
ration of hyperparameters is essential to maximise their performance.

A regularization parameter that offers a trade-off between the 
maximum-margin and misclassification rate. 
e.g. A large C enforces a small margin hyper-plane maximizing 
classification accuracy. 
A value to determine the distance over which support vectors 
influence the hyperplane. 
e.g. A high γ considers only points that are close to each other and causes 
the decision-boundary to be highly curved. 

N.B.  This  list  of  hyperparameters  is  not  exhaustive.  It  simply  covers  hyper-
parameters relevant to our approaches.

Learning 
rate 

Optimizer 

SVM: 
C 

γ 

The number of images you send to the model in each iteration. 
Model parameters are updated after each batch during training. 
How many times you pass the full image dataset through the model. 
The error metric that you wish to minimize. 
e.g. Cross entropy loss for multi-class classification. 
A small number (0, 1] that determines the amount to alter parameters 
during training with respect to the loss. 
Also known as the step size. 
An algorithm that modifies CNN parameters according to a particular 
strategy to minimize the loss. 
e.g. the Adam optimizer sets the learning rate adaptively for faster and 
more efficient training.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The hyperparameters of the model are optimized using a k-fold (k=5) cross-validated fine grid-search on the training data. This process involves searching through a range of values for each hyperparameter and evaluating the performance of the model for each combination of hyperparameters. In this case, the hyperparameters searched for the non-linear RBF SVM were C = 2^(-3), 2^(-2.75),..., 2^(7) and gamma = 2^-13, 2^-11. Similarly, the same hyperparameter search was performed for the single parameter C of the linear SVM. Additionally, the RBF and linear SVM were also evaluated with different combinations of hyperparameters.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Vision Transformers (ViT) (Dosovitskiy et al., 2021) are based on an 
alternative attention-based model design rather than CNN. The trans-
former  architecture was first applied on Natural Language Processing 
(NLP)  problems.  The  recent  ground-breaking  research  outputs  of 
Generative  Pre-trained  Transformer  (GPT)  and  Bidirectional  Encoder 
Representations from Transformers (BERT) are all based on the trans-
former architecture. The concepts of attention were later applied to the 
image classification problems as Vision Transformers (Dosovitskiy et al., 
2021). The transformers use the relationship between words, termed as 
attention, whereas the vision transformers determine relationships be-
tween patches of images. ViT have since achieved performance similar 
to convolution-based architectures. There are many vision transformer 
architectures for image classification but we chose MobileViT (Mehta 
and Rastegari, 2022) models proposed as a small model for embedded

Neural architecture search (NAS) was used for developing the Effi-
cientNetV2B0 model, and for the optimization of parameter efficiency 
and  training  speed  (Tan  and  Le,  2021).  The  model  performance  was 
improved  using  progressive  learning  to  adapt  regularization  to  the 
image size (Tan and Le, 2021). EfficientNetV2 is a family of models (B0 
to B7), and B0 is the smallest variant in the family (Keras, n.d), with the 
other  models  progressively  increasing  in  performance  going  up  to 
EfficientNetV2B7.

utilized, whereas the later layers (to be fine-tuned) learn the high-level 
features of the new dataset during the model training.

3.2. Process flow 

This study has two perspectives, model training and development on 
a high-end computer, and the model deployment to an edge device, as 
shown in Fig. 3. The training dataset could be pre-processed, such as 
denoising,  contrast  enhancement,  and  image  resizing.  We  only  used 
image re-sizing to make the images suitable for the input image reso-
lution required for the selected models. 

After a model has been trained, it needs to be validated and tested to 
confirm that it has achieved the desired performance. The performance 
might  be  measured  using  different  metrics,  such  as  classification 

Table 2 
The class distribution in the Extinction dataset.  

Class 

Number of images

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there is no explicit mention of specific data augmentation techniques applied in the deep learning pipeline. However, common data augmentation techniques include flipping, rotating, scaling, cropping, and color space transformation. These techniques help increase the diversity of the training set and improve the generalizability of the model. In this case, the only mentioned preprocessing technique is image resizing, which is done to fit the input resolution requirements of the chosen models. Therefore, without further details, it cannot be confirmed if any other data augmentation techniques were utilized in this particular study.
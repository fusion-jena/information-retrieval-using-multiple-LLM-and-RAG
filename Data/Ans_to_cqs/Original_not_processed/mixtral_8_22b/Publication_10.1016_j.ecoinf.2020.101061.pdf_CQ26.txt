Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Collecting a suﬃcient number of training samples will often be a
bottleneck in developing supervised methods in airborne imagery. It is
therefore useful to test the number of local training samples needed to
achieve maximum performance. We performed a sensitivity study by
training models using diﬀerent proportions of training data. We se-
lected 5%, 25%, 50% and 75% of the total hand-annotations to com-
pare to the full dataset for the within-site results for each site. We reran
this experiment ﬁve times to account for the random subsampling of
annotations. In addition, we ran the evaluation plots for the pretraining
model only (i.e. 0% hand-annotated data) to assess whether the addi-
tion of hand-annotated data improved the within-site pretraining.

3. Results

To assess generalization among sites, we performed three types of
experiments that used diﬀerent combinations for hand-annotations and
pretraining data (Fig. 2). The ﬁrst experiment is to use pretraining and
hand-annotated data to predict the evaluation data from the same site
(‘within-site’). The next setup is to use the pretraining data and hand-
annotated from the same site to predict the evaluation data from a
diﬀerent site (‘cross-site’). For example, using each of the within-site
models, we can test the ability for a model to predict tree conditions in
each of the other geographic sites, creating a matrix of cross-site pre-
dictions. To assess generalization without local pretraining data, we
tested a model training using pretraining data from all other sites, but
hand annotations from the same site as the evaluation data (‘transfer-
learning’). For example, the transfer learning model for Oak Woodland
used the hand-annotations from Oak Woodland, but the pretraining

Tesla K80 GPU for 5 epochs. To align these unsupervised classiﬁcations
with the ImageNet pretraining weights, we normalized the RGB chan-
nels by subtracting the ImageNet mean from each channel. We then
retrained the network using the hand-annotated data for 40 epochs. For
more details of this approach see Weinstein et al. (2019). Data aug-
mentation of random ﬂips and translations was tested and found to
have little eﬀect on the ﬁnal score.

Fig. 1. Conceptual workﬂow of proposed approach for airborne de-
tection of individual tree crowns. Pretraining data is generated by
overlaying predicted trees from a LiDAR-based unsupervised algorithm
on to RGB imagery. These RGB images are used to pretrain a deep
learning neural network. The resulting model is retrained based on
RGB hand-annotations.

2

B.G. Weinstein, et al.

Ecological Informatics 56 (2020) 101061

Table 1
The number of tree annotations used for pretraining, retraining and evaluation.
Pretraining annotations are generated automatically using a LiDAR-based un-
supervised algorithm. Training and evaluation annotations were hand-drawn.

Forest type

Pretraining
annotations

Training
annotations

Evaluation
annotations

Oak Woodland
Mixed Pine
Alpine
Eastern Deciduous

550,905
2,522,855
3,121,036
3,131,283

2533
3405
9730
1231

293
747
1699
489

Alpine forest, and completely connected crowns in the Eastern
Deciduous forest.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it appears that the authors did not explicitly discuss or employ specific techniques such as stratified splitting, oversampling, undersampling, or diverse data collection to address data bias during preprocessing of their deep learning pipeline. However, they did perform a sensitivity study by training models using different proportions of training data (5%, 25%, 50%, and 75%) to compare with the full dataset. This could potentially help mitigate any biases introduced due to limited training data. Additionally, they repeated this experiment five times to account for the random subsampling of annotations. They also evaluated the performance of the pretraining model alone (i.e., 0% hand-annotated data) to determine if adding hand-annotated data improved the within-site pretraining. While these approaches do not directly target data bias, they contribute to understanding the impact of varying amounts of training data on model performance.
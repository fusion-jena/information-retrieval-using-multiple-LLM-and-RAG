Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In  the  development  stage,  the  preprocessed  data  was  divided  into 
training (80%), validation (10%), and test sets (10%) for model training 
and testing purposes (Shah et al., 2018), followed by the construction of 
predictive  models  using  both  deep  learning  algorithms  (e.g.,  Deep 
Neural  Networks)  and  traditional  machine  learning  approaches  (e.g., 
AdaBoost  Regressor,  Support  Vector  Regressor,  etc.),  with  hyper-
parameter tuning performed via GridSearchCV with cross-validation (cv 
= 10).  While some studies  might  use different proportions, such as  a 
70–30 split between training and testing/validation (Khan et al., 2022), 
the  80–10-10  split  employed  in  this  study  ensured  a  more  extensive 
training set, potentially leading to a better-generalized model. The test 
set,  which  the  model  has  not  previously  seen  during  the  training  or 
validation phases indicates the model’s performance in real-world sce-
narios (Shah et al., 2018).

In the context of predictive model performance, it is imperative to 
consider the impact of the limited number of observations for each plot. 
A constrained dataset size can significantly influence model accuracy, as 
it  restricts  the  model’s  ability  to  learn  and  generalize  from  the  data 
effectively. This limitation is particularly pertinent in complex agricul-
tural  systems  like  oil  palm  yield  prediction,  where  the  diversity  and 
interplay  of  numerous  variables  demand  a  robust  dataset  for  optimal 
model  training  and  validation.  Therefore,  the  relatively  lower  perfor-
mance  of  some  models  in  this  study  could  be  partly  attributed  to 
insufficient  observations,  which  hampers  the  models’  capability  to 
capture and predict the intricate patterns and interactions inherent in 
the agricultural data. This factor stresses the need for more extensive 
and comprehensive datasets to enhance the predictive accuracy of ma-

The  choice  and  tuning  of  parameters  played  a  critical  role  in  the 
performance of these models (Yang and Shami, 2020). Parameters like 
‘max_depth’ in tree-based models and ‘learning_rate’ in boosting models 
impacted the ability of the model to learn the data without overfitting. 

Similarly, the number of estimators or iterations controlled the extent of 
learning, making it a crucial parameter for model optimization.

Examining the other models in the study offers a broad view of their 
relative  performances.  Models  like  the  Random  Forest  Regressor, 
Histogram-Based Gradient Boosting Regressor, and Bagging Regressor 
exhibited  moderate  performances.  Their  MSE  values  ranged  from 
1001.27 to 1025.74, and R2 values hovered around the 0.6 mark. On the 
other end of the spectrum, models such as the Deep Neural Network, 
XGBoost  Regressor,  and  Gradient  Boosting  Regressor  showed  lower 
effectiveness, with MSE values exceeding 1100 and R2  values dipping 
below  0.55.  These  variations  emphasize  the  differences  in  how  each 
model architecture interacts with the dataset and the complexity of oil 
palm yield prediction.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
During the training phase, the model performance was monitored through a validation set, which comprised 10% of the total preprocessed data. This approach allowed for continuous evaluation of the model's performance on unseen data while adjusting its parameters. Additionally, hyper-parameter tuning was carried out using GridSearchCV with cross-validation (cv = 10). This method systematically explores various combinations of hyperparameters to identify the best configuration for optimizing the model's performance. By employing this strategy, the researchers aimed to ensure that the final model would have strong predictive capabilities and avoid overfitting to the training data.
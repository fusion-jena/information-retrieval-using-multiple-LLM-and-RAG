Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

24. Keras. 2015. Available online: https://keras.io (accessed on 10 September 2021).
25. Abadi, M.; Agarwal, A.; Barham, P.; Brevdo, E.; Chen, Z.; Citro, C.; Corrado, G.S.; Davis, A.; Dean, J.; Devin, M.; et al. TensorFlow:
Large-Scale Machine Learning on Heterogeneous Systems. 2015. Available online: tensorﬂow.org (accessed on 10 September
2021).

26. Deng, J.; Dong, W.; Socher, R.; Li, L.J.; Li, K.; Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proceedings of
the 2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL, USA, 20–25 June 2009; pp. 248–255.

27. Kingma, D.P.; Ba, J. Adam: A method for stochastic optimization. arXiv 2014, arXiv:1412.6980.
28. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You Only Look Once: Uniﬁed, Real-Time Object Detection. In Proceedings of the
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 27–30 June 2016; pp. 779–788.
[CrossRef]

the number of images when training the ResNet50V2 network for classifying IAPS. The
highest recall for a given IAPS and input image size was generally achieved at annotation
sizes between 25% and 65%. Generally, the two image sizes 768 × 1024 and 1536 × 2048
performed similarly on the individual species and outperformed the other three image

Figure 8. Recall of the ResNet50V2 models on the validation set as a function of the annotation size relative to image size.
The validation images were binned according to the relative size of their respective annotations with respect to the image
size (“annotation size”). Each line shows the recall (left axis) of a ResNet50V2 model with a given input image size across
the binned annotation sizes. The recall lines were “smoothed”, using kernel density estimation. The number of images in
each bin is shown as vertical bars (right axis).

A preliminary test showed that the processing platform could evaluate images with
a resolution of 224 × 224 pixels in real time, using ResNet50-v2 [22]. MobileNet-v2 [23]
is a network structure designed with mobile devices in mind. In other words, it aims
for good performance with a low computational load. Both network structures are avail-
able in Keras [24] that is a high-level API based on Tensorﬂow [25], and they include
weights pre-trained on ImageNet [26], making them easy to evaluate in the case at hand.
Therefore, the two network structures ResNet50-v2 and MobileNet-v2 were explored for
image classiﬁcation.

5.1.1. Network Training Procedure

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't enough information available from the provided context to determine the exact code repository link of the deep learning pipeline used in this study. However, we can infer some details about the tools and libraries used in the research process.

From the context, we learn that the researchers utilized Keras [24] which is a high-level neural networks API written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, or PlaidML. This suggests that the deep learning pipeline might have been built using one of these frameworks.

Additionally, the authors mention using both ResNet50-v2 [22] and MobileNet-v2 [23] architectures for image classification tasks. These architectures are widely popular and readily available within the Keras library. Therefore, it is possible that the researchers directly imported these architectures into their pipeline without any significant modifications.

Lastly, the paper mentions that both network structures include weights pre-trained on ImageNet [26]. This implies that the researchers may have leveraged transfer learning techniques during the development of their deep learning pipeline.

In conclusion, while we cannot provide the specific code repository link for the deep learning pipeline mentioned in the study, we can confirm that the researchers made use of popular deep learning libraries such as Keras and possibly TensorFlow, along with well-known architectures like ResNet50-v2 and MobileNet-v2. They also likely employed transfer learning by utilizing pre-trained weights from ImageNet.
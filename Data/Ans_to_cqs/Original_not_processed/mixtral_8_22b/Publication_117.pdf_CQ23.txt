Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

science counts of the survey and comparison to expert counts.

tee  that  the  approach  is  transferable  and  how  to  appropriately 

4 |  D I S CU S S I O N

filter the data may be affected by the wording of the guidelines, 

the image resolution and sizes used, or the set of volunteers that 

participate  in  the  project.  Other  more  sophisticated  approaches 

to processing citizen science data have been proposed (Swanson 

From our results, we see that both citizen science and deep learn-

et al., 2016); however, given the range of counts provided by the 

ing methods are capable of producing highly accurate image counts. 

volunteers  and  the  large  errors  we  observe  in  the  baseline  met-

Counting the wildebeest within the survey images is a difficult and 

rics  (c.  11%  and c.  9%  undercount  for  the  mean  and  median,  re-

time- consuming task. When collecting the census images, there are

within the image. While there remains the potential for bias in the 

three main steps.

expert count, we take this count to be the gold standard. Hence, our 

Firstly, we generated a training dataset by selecting 500 of the 

results are a comparison between the two novel methods employed 

survey images at random to be used exclusively for training. Images 

and a count by a single experienced expert, which could in principle 

were tiled into 864 × 864 subimages and then passed though a ver-

deviate from the unknown true count.

sion of the YOLO DCNN using pretrained weights from the COCO 

dataset provided by (Redmon & Farhadi, 2018). This process created 

a	list	 of	the	locations	of	potential	objects	in	each	image.	As	a	first	

3 |  R E S U LT S

pass, these results were filtered by discarding any object detections 

that did not correspond to an identification from the Zooniverse data. 

We  compared  the  accuracy  of  the  methods  by  calculating  the

3.  Through  the  use  of  the  online  platform  Zooniverse,  we  collected  multiple  non-

expert counts by citizen scientists and used three different aggregation methods 

to  obtain  a  single  count  for  the  survey  images.  We  also  counted  the  images  by 

developing a bespoke deep learning method via the use of a convolutional neural 

network. The results of both approaches were then compared.

4.	 After	filtering	of	the	citizen	science	counts,	both	approaches	provided	highly	ac-

curate total estimates. The deep learning method was far faster and appears to be 

a more reliable and predictable approach; however, we note that citizen science 

volunteers played an important role when creating training data for the algorithm. 

Notably, our results show that accurate, species-specific, automated counting of 

aerial wildlife images is now possible.

K E Y W O R D S

citizen science, conservation, deep learning, monitoring, population ecology, surveys

tures directly from the training data. For image classification tasks, 

It has long been noted that multiple non- expert individuals can be 

DCNNs achieved accuracy levels that match the ability of humans 

as accurate as a single expert for certain tasks if their estimates are 

a number of years ago (Szegedy et al., 2015). Computationally ef-

appropriately aggregated (Condorcet, 1976; Galton, 1907). This phe-

ficient object detection is a more difficult task as it effectively in-

nomenon represents collective intelligence in its purest form, or as 

volves multiple classifications of different regions within an image. 

it's commonly known ‘the wisdom of crowds’ (Surowiecki, 2005) and 

Recently, a number of specialised object detection networks have 

in effect means that as more individuals estimate some quantity of 

been developed that either use a two- stage process of proposing 

interest,  then  an  appropriate  aggregate  quantity  (Kao  et al.,  2018)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information on the specific measures taken to ensure the generalizability of the deep learning model. However, it mentions that a training dataset was generated by randomly selecting 500 survey images. These images were then tiled into 864 x 864 subimages and processed through a version of the YOLO Deep Convolutional Neural Network (DCNN) using pretrained weights from the COCO dataset provided by Redmon & Farhadi (2018). This suggests that the model was trained on diverse data, as the COCO dataset contains images with various objects and scenes. Additionally, the mention of filtering the citizen science counts implies that some form of validation might have been performed to improve the reliability and robustness of the model. Nevertheless, without further details, it cannot be confirmed whether techniques such as cross-validation or stratified splitting were specifically applied to enhance the model's generalizability.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics82(2024)1027346or mel-frequency cepstral coefficients (MFCC) can be represented as a
single-channel images. We evaluate a commonly applied pre-trained
ResNet-50 architecture (He et al., 2016). Dufourq et al. (Dufourq
et al., 2022) also demonstrates strong classification performance and
low data requirements of ResNet-50 when comparing alternative pre-
trained CNN-based models. With the rapid development of transformer-
based models we also evaluate two transformer models Audio Spectro-
gram Transformer (AST) (Gong et al., 2021) which, like ResNet, oper-
ates on the spectrogram. We also evaluate HuBERT (Hsu et al., 2021)
which operates directly on the 1-D waveform. We evaluate the models
using the same feature extractor fθ and user model Uϕ pipeline presented
in Fig. 1 using a two-class binary output. Hence Uϕ is a two layer {100,2}
fully connected network.

methodology and discuss key design considerations.

4.1. Feature extraction

Convolutional Neural Network (CNN) based audio classification re-
mains a common approach. Audio representations such as spectrograms

Fig. 8. Example of 5-min field recording with 5-s automatically generated segments.

EcologicalInformatics82(2024)1027347B. McEwen et al.

We compare three classification models - ResNet-50 (He et al., 2016),
Audio Spectrogram Transformer (AST) (Gong et al., 2021) and HuBERT
(Hsu et al., 2021). The classification models compared in Section 4.1
were trained on a dataset containing 3000 5 s segments automatically
generated by the annotator. The dataset includes 1130 possum vocal-
isations and 1870 noise sources. The dataset has an 80/20% training-
validation split which is used to evaluate model performance during
training. Model hyperparameters are implemented based on the
recommendation outlined by the model authors. Each segment has been
manually reviewed by a human. 500 segments are separated into a test
set containing 187 possum vocalisations and 313 noise samples. The
distribution of target features to non-target features is representative of
the true distribution post-segmentation. Both validation and test dataset
performance are reported. Using the test dataset we evaluate the model

2.1. State of the art

Machine learning tasks designed for low-data contexts are referred to
as few-shot learning tasks. Within the context of bioacoustic applications,

EcologicalInformatics82(2024)1027342B. McEwen et al.

the field of computational bioacoustics (Stowell, 2021) has experienced
significant development over the last few years, in part due to public
challenges such as the Detection and Classification of Acoustic Scenes
and Events (DCASE) (Nolasco et al., 2023).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning pipeline uses single-channel images derived from Mel-Frequency Cepstral Coefficients (MFCC) as input data format. These images are then processed through various models including ResNet-50, Audio Spectrogram Transformer (AST), and HuBERT. The ResNet-50 and AST operate on the spectrogram, while HuBERT works directly on the 1-D waveform.
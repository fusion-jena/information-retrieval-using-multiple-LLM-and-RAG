Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We applied the  standard machine learning hyperparameter  search 
over  two  sites  because  of  computational  cost.  In  our  research,  we 
considered the activation function, the optimizer, and the learning rate. 
In addition, we performed an exhaustive search for the fully connected 
architecture for all mono-site and multi-site models. In all these cases, 
we compare the training, validation, and testing performance to avoid 
overfitting.  We  could  have  achieved  the  same  results  using  hyper-
parameter search software such as Optuna and Sklearn. However, even 
though they could save coding time, we prefer to control every aspect of 
the process to avoid problems. The hyperparameter search involved a 
small grid search selecting the following hyperparameters and values:  

1.  Hidden activation function: ReLU, sigmoid, or elu,  
2.  Optimizer: RMSprop or adam  
3.  Learning rate: 0.1, 0.001, 0.0001, and 0.00001.

performance on the training, validation, and test datasets. All models 
were implemented using the Keras library in Python 3.8.3 (Chollet et al.,

of the models. For instance, Mil`a et al. (2022) used the Nearest Neighbor 
Distance Matching (NNDM) LOO CV method to delineate a geographic 
space  in  which  predictions  can  be  made.  Many  of  these  approaches 
accurately  demonstrate  the  model's  transfer  learning  capabilities,  yet 
they  impose  limitations  on  the  training  domain,  leading  to  reduced 
training  heterogeneity  (Meyer  and  Pebesma,  2021).  However,  con-
ducting large CV experiments with deep learning-based models is often 
infeasible due to  their high computational costs. Deep  learning-based 
models  typically  employ  simple  data  partitioning  strategies,  such  as 
training, validation, and testing (Ocer et al., 2020). This simple scheme 
can help to add heterogeneous information to the training procedure, 
but may also overlook the diversity of heterogeneous spatial information 
in remotely sensed data that is aligned with different learning domains

2.  Second,  we  evaluated  the  predictions  from  all  generated  models, 
spanning various fully connected architectures. These models differ in 
the number of layers and neurons because we explored up to three 
layers with 8, 16, 32, and 64 neurons per layer. Thus, we explored up 
to  12  models  per  experiment,  as  illustrated  in  Fig.  1(e).  Here,  we 
selected the model with the highest R2  value based on the transfer 
learning evaluation dataset. This model is denoted as modelt  since its 
selection  is  anchored on  the  transfer  learning data.  This  approach 
might  not  always  be  feasible  in  real-world  applications,  given  the 
necessity of labels in the new domain. However, we employed this 
‘optimized model’ evaluation to discern the performance differences 
between modelv  and modelt. 

2.7. Model spatial autocorrelation

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning models were built using the Keras library in Python 3.8.3. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.
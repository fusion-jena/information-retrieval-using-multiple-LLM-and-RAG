Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.2. Data pre-processing

The complete dataset of 42 WQPs (Appendix A) for the 13 moni-
toring sites was subjected to an outlier detection and artificial data
replacement process, as Casillas-Garc√≠a et al. (2021) reported. Processed
data were used subsequently to train machine learning models, first with

EcologicalInformatics82(2024)1026552A.F. del Castillo et al.

Fig. 1. Study area and geographical distribution of sampling points along the Santiago-Guadalajara River.

the full dataset of 13 monitoring sites and secondly with the best training
subset (BTS) composed of sites selected by CA and TSA. Predictions from
all models were evaluated by comparing the root-mean-square-error
(RMSE), mean squared error (MSE), mean absolute error (MAE) and
coefficient of determination (R2) and finally, the BTS-ANFIS was tested
against new data and individual data from each monitoring site. The
general procedure for the training data selection and model develop-
ment is described in Fig. 2.

4.2. Machine learning models

A R T I C L E I N F O

A B S T R A C T

Keywords:
Water Quality Index
Highly polluted river
Time series analysis
Cluster analysis
Monitoring network
Data Science

2.6. Machine learning models

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given information, there is no mention or reference to a specific data repository link for the deep learning pipeline. Therefore, it cannot be determined where the data can be accessed.
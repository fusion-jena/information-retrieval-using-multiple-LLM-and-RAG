Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Baselines 

Prototypical 

Mel +PCEN 

Systems submitted 
to the public 
challenge 

Template matching 

Yang et al. (2021) 

Lin 

Mel 

Tang et al. (2021) 

Lin + PCEN 

Du_NERCSLIP 

Mel +PCEN 

Liu_Surrey 

Mel +PCEN & 
delta-MFCC 

CNN 

n/a 

CNN 

CNN 

CNN 
framewise 

CNN 

n/a 

x-ent 

Proto 

x-ent 

Proto 
(modifed) 

Wu_SHNU (+Wu 
2023 ICASSP) DFSL 
Moummad_IMT 

Other 

Wolters 2021 arxiv 
Perceiver 

You et al. (2023) 
(ICASSP 2023) 

Mel 

Mel 

Mel 

Mel +PCEN 

CNN (ResNet) 

x-ent 

DFSL attentive 

No 

Pseudo-pos 

– 

Proto 

Dist:Proto 

TI, Retrain 

5 

Between-the-5 +
Pseudo-neg 
(SpecSim) 
Pseudo-neg 

CNN (ResNet) 

SCL 

CNN + CRNN 
+Perceiver 

Proto +RPN 
(R-CRNN) 

Posterior 

Finetune 

Between-the-5 

Dist:Proto 

No 

n/a 

5 

5 

AST 

Proto 

Proto 

Dist:Proto 

Finetune, TI 

Between-the-5 

5 + aug 

New 
templates 
Retrain (new 
pos + neg) 
Proto 

Finetune last 
layer 

DFSL 
attentive 
Finetune last 
layer 
Proto

• Transductive  few-shot  learning  -  Meta  learning  methods  aim  to 
learn on scarce data in order to generalise to unseen tasks,  which 
makes the problem fundamentally difficult. In order to mitigate the 
difficulty, transductive based methods utilise the information present 
in the unlabeled examples from the query set to adapt the model and 
improve its predictions. In Liu et al. (2018), the samples in support 
and query set are jointly modelled as nodes of a graph and the pre-
diction on query set is conducted by label-propagation algorithm. In 
Hou  et  al.  (2019),  a  cross-attention  based  map  is  learnt  between 
support set and query set in order to make predictions on individual 
query examples.

Such fragmentation hinders the practical usability of deep learning 
in bioacoustics, and thus in the life sciences at large. Indeed to date, the 
success of deep neural networks in the supervised regime depends on the 
availability of a massive corpus of audio examples for the sound events 
of interest, paired with human annotations. Yet, temporally-precise and 
fine-grained annotation of audio demands expertise, and is thus costly 
and time-consuming. In many cases, the obstacle is not only to acquire 
annotations, but also the audio examples themselves: e.g. for rare spe-
cies,  remote  locations,  or  costly  equipment.  Furthermore,  these 
numerous small-data scenarios remain outside the scope of digital bio-
acoustic archives, such as Xeno-Canto and the Macaulay Library.

The  (deep)  feature  extraction  procedure  itself  can  place  limits  on 
event durations. Firstly the resolution of spectrograms, with a typical 
granularity  around  10  ms  per  ‘frame’,  often  predetermines  the  finest 
scale that can be resolved. Datasets QU and MGE contained very short 
sounds  at  around  this  scale;  Du_NERCSLIP(23)’s  framewise  CNN 
excelled on these datasets. Secondly, CNNs (used by all submitted sys-
tems) have relatively small “receptive fields”  and do not consider the 
whole  spectrogram  but  local  feature  patterns.  Many  of  the  strongest 
systems adapted themselves at query-time to the expected event length 
inferred from the 5 examples, in particular Martinsson_RISE who trained 
a  set  of  embedding  functions,  each  designed  for  a  different  duration.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, it appears that two specific datasets were mentioned: QU and MGE. However, there is no explicit mention of other commonly known datasets like MNIST, CIFAR, or ImageNet. Therefore, we cannot definitively state what datasets were used in the deep learning pipelines described here. It should be noted that the context primarily discusses challenges related to applying deep learning techniques in bioacoustics due to limited availability of labeled audio examples and fine-grained annotations.
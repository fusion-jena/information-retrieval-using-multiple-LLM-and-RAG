Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

hyperparameters  (i.e.,  ‘AutoML’;  He  et  al.,  2021).  This  represents  an 
important  advantage  for  non-experts  in  deep  learning,  as  it  does  not 
require  the  manual  assembly  of  the  models  and  definition  of  their 
hyperparameters. The AutoML procedure starts by generating a set of 
candidate models with architectures and hyperparameters (e.g. number 
of layers; learning rate) selected at random from a prespecified range of 
values (see Fig. 2). Each candidate model is trained using a small subset 
of the data (data partition At; Fig. 2) during a small number of epochs. 
After  training,  the  performance  of  the  candidate  models  is  compared 
using a left-out validation data set (Av; Fig. 2). The selected candidate 
model (usually the best performing among candidates) is then trained on 
the full training data (Bt; Fig. 2). In this step it is required to identify an 
optimal number of training epochs, to avoid under- or overfitting of the

model.  A  model  trained  too  few  epochs  will  not  capture  all  relevant 
patterns in the data, reducing predictive performance. A model trained 
for an excessive number of epochs might overfit, reducing its generality 
and ability to classify new data. There is no definitive way to identify an 
optimal  number  of  training  epochs,  but  one  practical  approach  is 
through  monitoring  the  model’s  validation  performance  (i.e.,  using 
holdout  data  partition  Bv;  Fig.  2).  The  ‘optimal’  number  of  training 
epochs is the one that provides the best validation performance. Finally, 
the performance of the model having an ‘optimal’  number of training 
epochs is evaluated using a ‘final’ test data set (T; Fig. 2), providing the 
best estimate of the predictive performance of the model.

Table 1 
Type of architecture and accuracy of candidate models and predictive performance of selected models (bold). The accuracy of candidate models was measured using 
the proportion of correctly classified cases. The accuracy of selected models was measured using the area under the receiver operating characteristic curve (AUC).  

Candidate 
model 

Architecture 

Accuracy of 
candidate 
models (% 
correct) 

AUC of 
selected 
model 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 

Case study 1 

ResNet 
InceptionTime 
LSTM 
CNN 
ResNet 
LSTM 
InceptionTime 
CNN 
CNN 
ResNet 
InceptionTime 
LSTM 
ResNet 
CNN 
InceptionTime 
LSTM 
CNN 
InceptionTime 
LSTM 
ResNet 

0.32  
0.66  
0.31  
0.7  
0.46  
0.32  
0.57  
0.68  
0.39  
0.32  
0.32  
0.39  
0.32  
0.49  
0.85 
0.39  
0.6  
0.79  
0.36  
0.42  

0.96 

AUC of 
selected 
model  

0.91 

Architecture 

Case study 2

To identify an ‘optimal’ number of training epochs, we examined the 
progression of validation performance (Bv). Models can be trained for an 
infinite number of epochs, so here we stopped training if no increase in 
validation performance was observed after 25 epochs (other thresholds 
could be considered, according to time resources available). Finally, the 
model trained with the number of epochs showing highest AUC in pre-
dicting Bv was used to classify the test data (data set T), with perfor-
mance measured using AUC. 

We recorded processing time of all models from the onset of training 
of candidate models to the last training epoch evaluated for the selected 
model. This was done on two distinct systems: a ‘desktop PC’ with an

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The data is typically split into three subsets for deep learning model training: training data (At and Bt), validation data (Av), and testing data (T). The training data is used to fit the parameters of the model, while the validation data is used to tune the hyperparameters of the model. The testing data is used to evaluate the final performance of the model. The splitting ratio between these datasets may vary depending on the specific application and dataset size, but common ratios include 80/10/10 or 70/15/15 for training/validation/testing respectively. It is important to ensure that the splits are representative of the overall distribution of the data and that they do not introduce any bias into the model. Additionally, cross-validation techniques such as k-fold cross-validation can also be employed to further improve the robustness and generalizability of the model.
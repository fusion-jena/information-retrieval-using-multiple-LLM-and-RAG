Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

On the other hand, since the powerful feature extraction 
capabilities, the deep learning methods have succeed in many 
fields,  such  as  computer  vision  and  natural  language 
processing. In recently, the deep learning based methods have 
also  been  used  to  solve  the  HSI  classification  task  and 
achieved remarkable achievements. Among the deep learning 
methods,  the  Convolutional  Neural  Networks  (CNNs)  are 
most widely explored to solve the HSI classification task. Hu 
et  al.  [4]  designed  a  one-dimensional  convolutional  neural 
network(1-D-CNN) for classification according to the spectral 
feature  of  HSI.  Due  to  the  insufficient  training  data,  the 
designed  1-D-CNN  just  includes  a  convolution  operation,  a 
pooling operation and a fully connected operation. Although 
the structure of  designed 1-D-CNN is simple, it achieved a 
satisfactory  classification  performance.  The  classification

614 

586 

5529 

5277 

1810 

16288 

1525 

13721 

18863 

169770 

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on August 29,2024 at 07:36:45 UTC from IEEE Xplore.  Restrictions apply. 
7128

What needs illustration is that before classifying the pixel 
f  unknown  categories  in  the  image,  it  is  necessary  to  first 
optimize  the  connections  (weights)  of  the  network  by using 
the labeled pixels. In this paper, the cross-entropy is selected 
as  the  cost  function  and  the  stochastic  gradient  descent 
algorithm is used to optimize model weights. 

The cross-entropy cost function is defined as(cid:726) 

J

(cid:32) (cid:16)

1
m

m

(cid:166)

i

1
(cid:32)

y
i

log(

(cid:16)
y

i

(cid:170)
(cid:171)
(cid:172)

)

(1
(cid:14) (cid:16)

y
i

) log(1

(cid:16)

(cid:16)
y

)

i

(cid:186)
(cid:187)
(cid:188)

 (5) 

(a)

(b)

IV. RESULTS AND DISCUSSION

Keywords—forest species classification, convolutional neural 
network  (CNN),  unmanned  aerial  vehicle(UAV),  hyperspectral 
image(HSI) 

I.  INTRODUCTION

k

1
(cid:32)

l

(cid:32)

0

m

(cid:32)

0

,
l m
w v
,
i j k
,

(
x l
(cid:14)
( 1),
i
(cid:16)

),(
k

y m
(cid:14)

)

)

 (4) 

where bi,j is the bias,  Ki-1 is the number of the feature ma 
in (i-1)th layer,  Li and  Mi are the height and width of 2 –D 
f (cid:152) is the 
convolutional kernel in the ith layer respectively,  
ReLU activation function as shown in Eq. (2). 

( )

The pooling layer is mainly applied to compress the input 
data,  which  often  follows  convolutional  layer.  The  max-
pooling strategy is used in SSPCNN. 

Table I Ground object classes and numbers of labeled pixels in 

UAV hyperspectral image 

Class 

Samples 

No. 

Name 

Train 

Test 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

Osmanthus 

1251 

11262 

Holly 

1070 

9629 

Platanus orientalis 

5378 

48402 

Cyclobalanopsis oak 

420 

3781 

Willow 

1434 

12908 

Camphor tree 

4775 

42973 

Camellia2 

Soil 

Road 

Water 

  Toal 

614 

586 

5529 

5277 

1810 

16288 

1525

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, the optimization technique applied in the deep learning pipeline is the Stochastic Gradient Descent (SGD) algorithm. This algorithm is used to optimize the model weights during the training process. Specifically, the cross-entropy cost function is chosen as the objective function to be minimized. The SGD algorithm iteratively updates the model parameters in the direction that reduces the value of the cost function. It does so by computing the gradient of the cost function with respect to the model parameters and then updating the parameters in the opposite direction of the gradient. This process continues until convergence or a maximum number of iterations is reached. Other popular optimization algorithms used in deep learning include Adam, RMSprop, and Adagrad, among others. However, these are not mentioned in the provided context.
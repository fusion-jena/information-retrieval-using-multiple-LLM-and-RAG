Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 4 
The metric area is divided into three distinct ranges: 
APs, APm, and APl, which correspond to varying pixel 
sizes of the bird instance and denote average preci-
sion for small, medium, large size target.  

Metric 

APs 
APm 
APl 

Area (pixel size) 

Area < 322 
322 < Area < 962 
Area > 962  

choice for object detection on the proposed dataset.

The  results  indicate  that  the  Cascade  RCNN  outperformed  other 
models at each training stage, while the FCOS model and Sparse CNN 
model consistently had lower mAP scores than others. This is consistent 
with  the  findings  of  Section  3.1.  Although  12  epochs  were  a  typical 
number of training times, almost all models reached the maximum mAP 
when the number of epochs reached 10. Therefore, for bird recognition, 
not  all  models  are  better  with  more  training  times,  and  choosing 

appropriate  training  times  for  different  models  is  beneficial  to  save 
computing resources. Specifically, the Cascade RCNN model had almost 
no  increase  in  the  mAP  at  9  epochs,  and  the  Faster  CNN  model  also 
showed a similar trend. However, the ATSS and RepPoint growth curves 
continued to 12 epochs, suggesting they require more training data or a 
longer training schedule to perform better.

The  Faster  RCNN  model  initially  achieved  a  relatively  high  mAP 
score of 0.568 in epoch 7; however, its performance plateaued in sub-
sequent  epochs,  suggesting  that  further  improvements  may  require 
modifications  to  its  architecture  or  hyperparameters.  In  contrast,  the 
Sparse RCNN model showed a slow start, with its mAP score increasing 
from 0 in epoch 1 to 0.324 in epoch 4, but achieved a steady increase in 
performance after that. 

Overall, these results suggest that Cascade RCNN and ATSS are the 
most effective models for object detection on camera trap images, while 
Faster  RCNN,  RetinaNet  and  RepPoints  are  also  viable  options.  How-
ever, FCOS and Sparse RCNN may not be suitable for this task, given 
their slower and less stable convergence and lower mAP scores. 

3.4. Baseline detectors with different backbones

# Instances 

190 
146 
2885 
1427 
300 
1953 
1447 
527 
262 
127 
230 
478 
1632 
440 
568 
12,612  

Table 3 
The  number  of  images  for  training  and 
validation.  

Category 

Training 
Validation 
Total 

Number 

7773 
1943 
9717  

their performance in complex, real-world ecological settings. In recent 
years, deep learning-based approaches, such as YOLO (Redmon et al., 
2016) (one-stage) and Faster RCNN (two-stage) (Ren et al., 2015), have 
been shown to achieve state-of-the-art performance in object detection 
tasks.  Two-stage  and  one-stage  object  detection  are  two  popular  ap-
proaches for detecting objects in images (Fig. 2).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The primary metric used to evaluate the performance of the deep learning models in this context is mean Average Precision (mAP). The mAP is calculated by taking the average precision value for recall values between 0 and 1. Additionally, the metric area is divided into three distinct ranges: APs, APm, and APl, corresponding to varying pixel sizes of the bird instances. These ranges denote average precision for small, medium, and large size targets, respectively. The specific pixel size ranges for each category are defined as follows: Area < 322 pixels for APs, 322 < Area < 962 pixels for APm, and Area > 962 pixels for APl.
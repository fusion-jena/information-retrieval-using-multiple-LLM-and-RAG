Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

minimizing  the  loss  on  the  validation  dataset  to  perform 
objective function evaluations. The models were trained for a 
maximum of 10 epochs using the stochastic gradient descent 
with  a  momentum  (â€˜sgdmâ€™)  optimizer.  Thus,  a  total  of  six

TABLE I.  

PERFORMANCE METRICS FOR UBD_45 DATASET 

Model 

Performance Metrics 

Sensitivity 

Specificity 

Precision 

F1-Score 

InceptionResnet2 

95.93 

Resnet50 

Xception 

Inceptionv3 

Mobilenetv2 

Googlenet 

95.93 

94.44 

91.85 

92.04 

85.00 

99.92 

99.90 

99.87 

99.85 

99.83 

99.62 

97.65 

97.20 

95.90 

94.73 

95.16 

86.15 

96.78 

96.56 

95.16 

93.27 

93.57 

85.57 

TABLE II.  

PERFORMANCE METRICS FOR VP_200 DATASET 

Model 

Performance Metrics 

Sensitivity 

Specificity 

Precision 

F1-Score 

InceptionResnet2 

91.20 

Resnet50 

Xception 

Inceptionv3 

Mobilenetv2 

Googlenet 

88.68 

91.80 

88.65 

85.58 

83.28 

99.96 

99.94 

99.96 

99.94 

99.93 

99.92 

91.92 

89.46 

92.83 

90.47 

87.20 

84.90 

91.56 

89.07 

92.31 

89.55 

86.38 

84.08

E.  Evaluation Metrics 

Since one of the datasets used was imbalanced, measuring 
the performance of the proposed method using accuracy alone 
would  not  be  a  sufficient  metric.  Therefore,  other  metrics 
including recall, precision, specificity, and F1-score were also 
used in the evaluation. These metrics are based on a number 
of true positive (TP) or true negative (TN) where the predicted 
species matches the ground truth class and false positive (FP) 
or false negative (FN) where  the predicted  species does not 
match  with  the  ground  truth  class.  In  summary,  these 
evaluation metrics are represented in Equations 1 â€“ 5. 

ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ =  

ğ‘‡ğ‘ƒ + ğ‘‡ğ‘
ğ‘‡ğ‘ƒ + ğ‘‡ğ‘ + ğ¹ğ‘ƒ + ğ¹ğ‘

          (1) 

ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦/ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™  =  

ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ + ğ¹ğ‘

            (2) 

ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ
ğ‘‡ğ‘
ğ‘‡ğ‘ + ğ¹ğ‘ƒ
2. ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› . ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™
ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› + ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™

ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦ =  

                            (4) 

ğ¹1 âˆ’ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ =  

        (5)

Fig. 4.  Classification accuracy for validation and test datasets for UBD_45 

Fig. 5.  Classification accuracy for validation and test datasets for VP_200 

The  other  performance  metrics  including  sensitivity, 
specificity,  precision,  and  F1-measure  for  all  models  for 
UBD_45 and VP_200 datasets are shown in Table I and Table 
II, respectively. The specificity values for all models for both 
datasets  were  found  to  be  above  99%  which  indicates  that 
these  models  were  able  to  highly  correctly  identify  those 

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on September 08,2024 at 16:24:56 UTC from IEEE Xplore.  Restrictions apply.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Answer: Training completion is determined by several criteria, depending on the model and optimization algorithm being used. One common criterion is reaching a validation loss plateau, where further training no longer significantly improves the model's performance on the validation set. This can indicate that the model has learned as much as it can from the data without overfitting. Another criterion could be setting a maximum number of epochs, such as the mentioned 10 epochs in this case, after which training will stop regardless of whether the validation loss has plateaued or not. Additionally, early stopping can be employed, where training stops if the validation loss starts increasing instead of decreasing, indicating potential overfitting. However, the exact criteria may vary between different applications and models.
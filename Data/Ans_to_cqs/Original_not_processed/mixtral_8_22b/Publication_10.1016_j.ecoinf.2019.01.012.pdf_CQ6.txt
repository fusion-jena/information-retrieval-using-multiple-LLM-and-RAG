Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Although the use of pre-trained models has proven to be advanta-
geous for many image recognition tasks (Weiss et al., 2016), we decided
to train the model from scratch. We recognized that our speciﬁc clas-
siﬁcation problem was too diﬀerent from the pre-trained networks.
Moreover, we considered the number of labeled samples available
(nearly 30,000 per class) as large enough for training our own network.

We trained the CNN using ReLU activation for the convolutional layers
and an exponential linear unit (ELU) activation on the fully connected
layer (Clevert et al., 2015). As optimizer we chose the adaptive ADAM
(Adaptive Moment Estimation) with a learning rate of lr = 0.0001.
Training was performed with a batch size of 256 over 16,290 iterations
(= 90 epochs).

layers, pooling layers and fully connected layers (Voulodimos et al.,
2018). The ﬁrst two components are responsible for automatic feature
extraction by applying a large number of diﬀerent ﬁlters on the input
data. This process of feature extraction is performed on multiple levels,
whereby the output of each level is the input to the following. From
level to level, the extracted features increase in complexity - from rather
simple features (e.g. edges) on the lowest level to more complex fea-
tures on the highest level (Gu et al., 2018). By passing large quantities
of labeled training data through the network, the model successively
learns to recognize the relevant features, which are necessary to dis-
tinguish between classes.

Machine learning, and particularly deep learning, has evolved into
the most commonly used approaches for the automatic classiﬁcation of
digital images (LeCun et al., 2015). The major advantage of deep
learning is that the time-consuming and complex step of previous fea-
ture extraction becomes unnecessary. Instead, the model learns and
extracts the relevant features itself during the training process. The
major drawback of deep learning is that large amounts of labeled
training data are required (Kamilaris and Prenafeta-Boldú, 2018).
Among the deep learning architectures, convolutional neural networks
(CNN) are particularly suitable for image analysis due to their ability to
extract spatial features. CNNs have proven to be quite powerful in
performing diﬀerent tasks such as object detection (Everingham et al.,
2010; Tompson et al., 2014), classiﬁcation (Traore et al., 2018, Xu
et al., 2017, Amara et al., 2017, Lu et al., 2017, Han et al., 2018) and

Ecological Informatics 50 (2019) 220–233

Fig. 5. Learning process for loss (left) and model accuracy (right) over 90 epochs. The gap between training and validation accuracy is caused by using dropout on
the training data only.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about any specific preprocessing steps taken before training the deep learning model. However, it can be inferred that some form of labeling was done on the dataset since the authors mention having nearly 30,000 labeled samples per class. Labeled datasets are crucial for supervised learning tasks like classification.

Preprocessing steps often depend on the nature of the dataset and the task at hand. Common preprocessing techniques include normalization or standardization, where the data is scaled to a certain range or made to follow a particular distribution. Data cleaning might also be necessary if there are missing values, outliers, or noise in the dataset. Image data may require additional preprocessing steps such as resizing, cropping, rotation, flipping, or augmentation to improve the robustness of the model.

In this case, the authors did not explicitly state whether they applied these preprocessing techniques. They directly mentioned training the Convolutional Neural Network (CNN) using ReLU activation for the convolutional layers and ELU activation on the fully connected layer. Therefore, without further context, one cannot definitively determine what preprocessing steps were involved before training the deep learning model.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

is training and storing models using an interface provided by the site and 
its computational resources.

4.  A modular classification scheme. A taxonomic classification system 
requires  training data  that includes  at  least  hundreds  of  manually 
labeled individuals per class. Thus, a general purpose, yet high res-
olution, system for taxonomic classification is probably not feasible. 
Instead we envision a (gradually growing) set of custom-made AI- 
based classification models. Labeling of individual insects by experts 
is  already  implemented  in  STARdbi,  but  the  other  aspects  of  this 
vision - training, storage, retrieval and application of classification 
models  -  are  not  yet  operational.  We  are  already  creating  task- 
specific  classifiers  on  usersâ€™  requests.  The  performance  of  such  a 
classifier is presented in the Results section as case study 1.

2.5. Automatic insect classification 

Automatic,  AI-based,  insect  classification  is  not  yet  part  of  the 
STARdbi pipeline. Users may download images and their box annota-
tions (as Comma Separated Values, csv, files), train models locally, and 
apply them to unlabeled data. The programs that we applied to this end 
(case study 1 in the Results section) are available in the STARdbi web- 
site as a Google Colab notebook. We also welcome collaborations with 
other researchers, in which our team will take the role of designing and 
trouble-shooting classification models to meet specific needs. 

The available programs for the generation of classification models 

include:  

1.  STARdbi_split_images 

EcologicalInformatics80(2024)1025213T. Keasar et al.

insects are accessible to users with the relevant authorizations (Fig. 1B). 
Current user operations include: (I) viewing and downloading of images 
and bounding box information (position, annotation, and insect-covered 
area), (II) manual editing of the bounding boxes, and/or taxonomically 
annotating them, and (III) deriving abundance and diversity metrics. We 
aim to implement three additional important operations: (IV) training of 
high resolution (e.g. species-level) classification models, (V) storing the 
models  in  the  database,  and  (VI)  applying  them  to  the  database.  A 
detailed user view of the pipeline as well as best practices for its use can 
be found in Appendix 1. 

2.3. The database 

STARdbi  uses  a  mySQL  database 

(https://www.oracle.com 

/mysql/what-is-mysql/), with the following tables

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there is no explicit mention of a strategy implemented to monitor the model performance during training. However, it does state that users can create task-specific classifiers upon request, and the performance of such a classifier is presented in the results section as case study 1. Additionally, users have access to tools like STARdbi\_split\_images and STARdbi\_train\_model to generate and train their own classification models. It can be inferred that users might employ common strategies to monitor model performance during training, such as tracking loss or accuracy over epochs, cross-validation, or early stopping. But without more specific details about the methods used, it cannot be definitively stated what exact strategy is employed to monitor model performance during training.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

studies which used transfer learning (King et al., 2018; Mahmood et al.,
2017, 2016), we trained our CNNs from scratch. Different ResNet18s
were trained with batch normalization and ReLU activation taking
place before the convolution layer, as this has been proven to improve
generalization (He et al., 2016b). We trained these CNNs using Adam
optimiser, with a learning rate of 10−3 and other parameters set to their
defaults, in accordance with the original study (Kingma and Ba, 2014).
Data was fed iteratively in batches of 512, for patches of dimensions
64 × 64, 96 × 96 and 128 × 128 pixels. Batches of 200 were used for
patch size 224 × 224 due to computational limitations. A ResNet50
was also trained under the same conditions, with patch size 128 × 128
and batch size 128, once again due to computational limitations. In
order to better compare the networks, a ResNet18 was also trained with
same conditions as the ResNet50. The ensemble network was built on

from underwater video using neural networks. Opt. Express 13, 8766. https://doi.
org/10.1364/OPEX.13.008766.

Masters, D., Luschi, C., 2018. Revisiting Small Batch Training for Deep Neural Networks.
McGill, B.J., Dornelas, M., Gotelli, N.J., Magurran, A.E., 2015. Fifteen forms of biodi-

versity trend in the Anthropocene. Trends Ecol. Evol. 30, 104–113. https://doi.org/
10.1016/j.tree.2014.11.006.

Mehdipour Ghazi, M., Yanikoglu, B., Aptoula, E., 2016. Open-Set Plant Identification

Using an Ensemble of Deep Convolutional Neural Networks. Working Notes of CLEF.
Mishkin, D., Sergievskiy, N., Matas, J., 2016. Systematic evaluation of CNN advances on
the ImageNet. Comput. Vis. Image Underst. 161, 11–19. https://doi.org/10.1016/j.
cviu.2017.05.007.

Niculescu-Mizil, A., Caruana, R., 2005. Predicting good probabilities with supervised

60.64
63.82
65.77
66.30

60.17
63.57
65.54
65.94

Table 3
Performances of different ResNet architectures on validation and test sets. ResNetX-Y is written so that X indicates the network's depth and Y the input size. In bold
the best value for each metric.

Network -patch Size

Batch size

Validation set

Test set

Macro-F1

Top-1 accuracy

Micro-F1

Macro-F1

Top-1 accuracy

Micro-F1

ResNet152–224
ResNet50–128
ResNet18–128
ResNet18–224
Ensemble

16
128
128
200
128

37.45
52.04
51.40
54.93
60.56

62.38
64.07
63.90
66.70
70.60

60.46
63.85
63.88
66.44
70.35

38.26
52.27
51.62
53.93
60.38

61.71
64.35
63.60
66.30
70.54

60.09
63.89
63.44
65.94
70.37

A ResNet152 was used as the baseline for classification perfor-
mance. This network was initially programmed using pre-trained
weights lifted from the ImageNet challenge. The top fully-connected
layer was replaced with a layer using ReLU activation, followed by a
dropout layer and a softmax layer, with outputs corresponding to the
number of classes (here 61). Firstly, the top layer was trained alone;
using features extracted from the convolutional layers, a Stochastic
Gradient Descent (SGD) with a learning rate of 10−3, and weight decay
rate set to 5 × 10−4. The whole network was then trained with SGD;

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors use two main optimization techniques in their deep learning pipeline: Stochastic Gradient Descent (SGD) and Adam optimizer. They first train the top fully connected layer of a ResNet152 model using SGD with a learning rate of 10^-3 and a weight decay rate of 5 x 10^-4. Then they train the entire network using SGD. For training the ResNet18 models, they use the Adam optimizer with a learning rate of 10^-3 and default settings according to the original study by Kingma and Ba (2014).
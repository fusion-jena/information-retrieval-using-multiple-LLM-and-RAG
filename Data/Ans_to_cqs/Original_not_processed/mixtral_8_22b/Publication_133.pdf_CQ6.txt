Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Model Training
Deep learning models
iteratively: generating
learn data
predictions of the training set samples, evaluating them by

3https://github.com/ultralytics/yolov5/tree/v4.0
4https://cocodataset.org/#home

the loss function, and modifying their parameters according
to the loss function’s gradient. Thus,
the loss declines in
each training round (aka epoch), and the model’s annotation
accuracy increases (Figure 2 and Supplementary Figure 1).
As the number of parameters of a typical deep learning
model
is large, overﬁtting is a major concern. A common
(partial) remedy is the use of validation data, which are set-
aside while training, yet their annotations by the models are
the training
monitored. The validation data help to limit
duration, thereby reducing the risk of overﬁtting. In this study
we used 30 epochs per training task, based on our validation
runs (Figure 2).
The data of

FIGURE 2 | The training process of the YOLOv5 model. The left panel depicts the loss function that drives the learning process. This function penalizes incorrect
predictions of the three focal species as well as missed ones. The learning consists of iterative reduction of this penalty for the training set images (blue) by a
stochastic gradient algorithm (Ruder, 2016). The validation set images (red) do not participate in the optimization and the gradual reduction in their loss indicates that
the training has not reached an overﬁtting point. The validation loss stabilizes after 30 epochs, suggesting that further training might lead to overﬁtting. Thus, the
deep learning model generated by 30 training epochs was used for tests. A different perspective on the gradually improving performance of the model is provided in
the right panel. The model’s average precision (AP) improves with the number of training epochs. The plot depicts the model’s mean AP (mAP) over the three insect
classes.

1

May 2021 | Volume 9 | Article 600931

fevo-09-600931

May 17, 2021

Time: 21:42

# 2

Gerovichev et al.

Deep Learning for Insect Ecoinformatics

INTRODUCTION

Machine learning methods are increasingly applied to
biological problems that involve classiﬁcation of images and
extracting information from them. The current leading approach
to such tasks is supervised learning using deep neural networks

1https://www.gbif.org

(DNNs), and particularly convolutional neural networks (CNNs),
which are able to extract abstract high level features from images.
Identifying objects within the image and classifying them may
be treated as separate tasks. Yet, more recent methods, such as
“Faster R-CNN” (Ren et al., 2015), and YOLO (Redmon et al.,
2016) consider both problems at the same time.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps can be taken to improve the quality of the input data and enhance the model's performance. These steps include:

1. Data Cleaning: Removing irrelevant or redundant data points, handling missing values, and correcting inconsistencies in the dataset.

2. Normalization: Scaling numerical inputs to a specific range, typically between 0 and 1, or -1 and 1. This helps prevent any single feature from dominating the others due to its scale. Common techniques include Min-Max scaling and Z-score normalization.

3. Image Augmentation: For image datasets, applying transformations like rotation, flipping, zooming, cropping, and changing brightness or contrast to increase the size and diversity of the dataset. This helps reduce overfitting and makes the model more robust.

4. Encoding Categorical Variables: Converting categorical variables into numerical representations, such as one-hot encoding or label encoding, so they can be processed by the model.

5. Splitting Datasets: Dividing the dataset into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune hyperparameters and monitor the model's performance during training, and the test set is used to evaluate the final model's performance.

These preprocessing steps are crucial for ensuring that the deep learning model receives high-quality input data, which ultimately leads to better performance and generalizability.
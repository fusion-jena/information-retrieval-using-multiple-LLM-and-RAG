Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

∑C

j=1,j∕=yi

es(cosθj +m/2)

(3) 

In the above formula, the number of samples in a batch, the number 
of sample classes, the cosine factor, the cosine interval, the angle be-
tween the feature and the target weight, the angle between the feature, 
and the non-target weight are all denoted by the letters n, C, s, m, θyi , 
and θj, respectively. The network can concentrate its expressive power to 

Fig. 3. DenseNet structure used in module 1.  

EcologicalInformatics78(2023)1023345H. Gong et al.                                                                                                                                                                                                                                    

Fig. 4. Module 2 model diagram.

First, the acquired image data were randomly generated in 8:2 ratio 
with  the  corresponding  training  and  test  sets,  which  were  uniformly 
adjusted to a size of 224 × 224 for network model training and accuracy 
testing,  respectively.  The  training  set  was  4674  and  the  test  set  was 
1169.  Normally,  enough  samples  are  required  to  train  the  model. 
However, the data set was enlarged to avoid experimental overfitting. In 
this  work,  the  training  dataset  was  increased  to  6360  photos  using 
random cropping, vertical flipping, and horizontal flipping. Each input 
was set to 1 in the experiments in this work since the problem is a multi- 
class  problem,  and  each  image  comprises  example  values  from  many 
classes (e.g. prime points). 

13. Analysis of the experimental results 

Ablation and comparison experiments were used to demonstrate the 
validity  of  the  validation  in  this  work  and  the  results  are  shown  in 
Table 2 and Fig. 8.

6.1. Vision transformer 

Transformer (Vaswani et al., 2017), which is a deep neural network 
first created for natural language processing (NLP), was built on a self- 
attention mechanism that makes it possible for noticeably huge recep-
tive  fields.  The Transformer  has become  a  de facto industry  standard 
procedure in NLP because of its remarkable efficiency, and it has also 
motivated  researchers  in  the  field  of  computer  vision  to  investigate 
applications  by  utilizing  the  long-term  relationships  between  pixels 
(Khan et al., 2022).

nism (Dosovitskiy et al., 2021). In contrast to CNN, ViT's Self Attention 
was  able  to  combine  the  data  from  the  entire  image,  and  accurately 
model  lengthy  sequence  information.  In  the  standard  ViT  model,  the 
relationship between distinct tokens was crucial since all images were 
turned into fixed-size tokens. By reusing the relationship matrix of the 
image  features,  the  lower  level  ViT  can  perform  more  accurate  rela-
tionship modelling. The images were converted into tokens in a process 
that  is  equivalent  to  downsampling.  A  larger  downsampling  multiple 
indicates that fewer tokens are obtained and more information is lost. 
When  more  tokens  are  acquired  and  the  downsampling  multiple  is 
lower, the lost information is less. When converting a picture to a token, 
the image's content is not taken into account. Such downsampling is too 
fine-grained for complicated pictures, which results in a loss of accuracy.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, some strategies can be inferred based on common practices in machine learning and deep learning. One approach is setting a specific random seed value before running any code involving random processes, ensuring consistency across different runs or sessions. Another strategy involves using techniques like cross-validation, where the dataset is divided into several subsets, with one subset serving as the testing set while others act as training sets. This helps mitigate the impact of randomness on performance evaluation. Additionally, employing ensemble methods, combining predictions from various models trained with different initializations or hyperparameters, can further reduce the effects of randomness.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We  classified  all  the  input  dataset combinations  (10  variable  sets; 
Table  2)  with  each  classifier  (RF,  SVM,  MARS)  and  with  two  sets  of 
classes (7C and 13C); thus, we had 60 models as a result of the classi-
fications. Classifications and hyperparameter tuning were conducted in 
R  4.2  (R  Core  Team,  2023)  with  the  caret  package  (Kuhnaut  et  al., 
2022). 

2.4. Accuracy assessment

EcologicalInformatics81(2024)1026244L. Szab´o et al.                                                                                                                                                                                                                                   

Table 1 
Hyperparameters in the grid search.  

Algorithm 

Hyperparameter ranges 

RF 
SVM 

MARS 

mtry: 1–20 with increments of 1 
gamma: 0.01–0.1 with increments of 0.01 
C: 2–16 with increments of 2 
degree: 1–3 
nprune: 2 to 100 with increments of 10  

(Conrad et al., 2015). The optimal segment size was chosen based on the 
extent of the patches, without losing any elements of the species pattern. 
The mean values of the segments were used for classification. Segments 
are  optimal  to  avoid  spatial  autocorrelation  when  using  k-fold  cross- 
validation  (KCV)  in  the  model-building  and/or  accuracy  assessment 
phase.

and Dr˘agut¸, 2016; Breiman, 2000; Phinzi et al., 2020); and Multivariate 
Adaptive Regression Splines (MARS) as a non-parametric extension of 
regressions, which is insensitive to nonlinearity (Friedman, 1991; Garosi 
et al., 2019; Rotigliano et al., 2018). All classification models were run 
with hyperparameter tuning using the grid search method to determine 
the  best  sets  of  function  parameters  (Table  1).  RF  has  one  important 
hyperparameter, mtry, which controls the input variables for the deci-
sion trees. SVM has two parameters to fine-tune: gamma (how large the 
influence of the training data is: small values define large distance, large 
values define small distance in the hyperspace defined by the variables), 
and C (small values increase, large values decrease misclassifications). 
In  the  case  of  MARS,  the  two  parameters  are  the  degree  (maximum 
degree of interactions between input variables) and nprune (controls the

GREEN, RED, REDEDGE, NIR, NDRE, NDVI, vNDVI, DSM, Run Percentage (k5o1) 
GREEN, RED, REDEDGE, NIR 
NDVI, NDRE, vNDVI 
Run Percentage (k2o1, k2o3, k2o5, k3o1, k3o3, k3o5, k5o1, k5o3, k5o5) 
GREEN, RED, REDEDGE, NIR, NDRE, NDVI, vNDVI 
DSM, REDEDGE, NIR, vNDVI, GREEN, NDVI 
REDEDGE, vNDVI, DSM, NIR, NDRE, NDVI 
Run Percentage (k5o1), NDVI, NDRE, vNDVI 
Run Percentage (k5o1), GREEN, RED, REDEDGE, NIR 
DSM, GREEN, RED, REDEDGE, NIR 
DSM, NDRE, NDVI, vNDVI  

EcologicalInformatics81(2024)1026245L. Szab´o et al.                                                                                                                                                                                                                                   

JM values were squared to achieve the common JM value range of 0–2 as 
the varSel used a different function (with the square root of JM). 

3.3. Class level evaluation 

3. Results 

3.1. Jeffries-Matusita separability analysis

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, it appears that the study does not involve a deep learning model but rather uses three machine learning algorithms: Random Forest (RF), Support Vector Machine (SVM), and Multivariate Adaptive Regression Splines (MARS). Each algorithm has its own set of hyperparameters that need to be tuned during the modeling process. For RF, the main hyperparameter is'mtry,' which determines the number of input variables considered for decision tree construction. In contrast, SVM has two primary hyperparameters: 'gamma' and 'C.' The former defines the influence radius of the training data, while the latter regulates the trade-off between achieving low error rates and minimizing misclassification penalties. Lastly, MARS employs 'degree' and 'nprune' as its key hyperparameters, controlling the maximum degree of interaction among input variables and pruning unnecessary terms from the final regression equation, respectively. These hyperparameters were optimized through a grid search approach, as outlined in Table 1. However, no specific details regarding learning rates or optimizers were mentioned since these concepts are typically associated with neural networks and deep learning architectures, which do not appear to have been utilized in this particular study.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

FIGURE 2 | The training process of the YOLOv5 model. The left panel depicts the loss function that drives the learning process. This function penalizes incorrect
predictions of the three focal species as well as missed ones. The learning consists of iterative reduction of this penalty for the training set images (blue) by a
stochastic gradient algorithm (Ruder, 2016). The validation set images (red) do not participate in the optimization and the gradual reduction in their loss indicates that
the training has not reached an overﬁtting point. The validation loss stabilizes after 30 epochs, suggesting that further training might lead to overﬁtting. Thus, the
deep learning model generated by 30 training epochs was used for tests. A different perspective on the gradually improving performance of the model is provided in
the right panel. The model’s average precision (AP) improves with the number of training epochs. The plot depicts the model’s mean AP (mAP) over the three insect
classes.

Model Training
Deep learning models
iteratively: generating
learn data
predictions of the training set samples, evaluating them by

3https://github.com/ultralytics/yolov5/tree/v4.0
4https://cocodataset.org/#home

the loss function, and modifying their parameters according
to the loss function’s gradient. Thus,
the loss declines in
each training round (aka epoch), and the model’s annotation
accuracy increases (Figure 2 and Supplementary Figure 1).
As the number of parameters of a typical deep learning
model
is large, overﬁtting is a major concern. A common
(partial) remedy is the use of validation data, which are set-
aside while training, yet their annotations by the models are
the training
monitored. The validation data help to limit
duration, thereby reducing the risk of overﬁtting. In this study
we used 30 epochs per training task, based on our validation
runs (Figure 2).
The data of

34.56588

July, 2020

46

10

10

10

8

10

10

and to label them correctly. For the training and testing of the
method we used labeled images, which were divided to three
subsets: “training,” “validation,” and “test.” In the training phase
the algorithm uses the “training” and “validation” sets to create
a statistical model. In the test phase we apply the model to
the “test” set images, ignoring their labels. Each predicted label
is associated with a conﬁdence score that considers both the
probability that the label is correct, and the expected overlap
between the ground truth bounding box and the predicted one.
This overlap is expressed as the Intersection over union (IoU),
indicating the agreement between a predicted bounding box and
a ground truth box (Eq. 1).

Model Evaluation
We evaluated the quality of
the model by comparing the
predictions with the ground truth, which is a set of boxes drawn
and labeled by a human expert (magenta boxes in Figure 3).
The metrics of model quality are based on the class labels and
their associated conﬁdence scores. We calculated three out of the
four elements of a confusion matrix: false positives (FP), false
negatives (FN) and true positives (TP). To do so, we counted
boxes labeled either by the model, the expert, or both of them. We
deﬁned two thresholds: TIoU – the lowest IoU that we consider
meaningful object detection, and Tcs, the lowest conﬁdence score
that we consider reliable. Here we adopt a TIoU of 0.5 which is
commonly used in the image processing world (Everingham et al.,
2010). Varying Tcs over the 0 to 1 range results in diﬀerent TP, FP,
and FN values, depending on how restrictive our prediction is.
A prediction is considered TP if it shares a label with an observed

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: After the model training, several postprocessing steps can be performed to evaluate the model's performance and interpret its predictions. Some examples include:

1. Model evaluation: Comparing the model's predictions with the ground truth using various metrics such as accuracy, precision, recall, and F1-score. These metrics provide insights into the model's ability to accurately detect objects and avoid false detections.

2. Confusion matrix: Constructing a confusion matrix to visualize the model's performance across all classes. It helps identify patterns of misclassification and provides insights into the strengths and weaknesses of the model.

3. Saliency maps: Generating saliency maps to understand the regions of input images that contribute most significantly to the model's output. They highlight areas where the model focuses its attention during decision making.

In the given context, the authors have followed these postprocessing steps:

1. Model evaluation: The authors compared the model's predictions with the ground truth using metrics like false positives (FP), false negatives (FN), and true positives (TP). They also adopted a commonly used threshold value of 0.5 for intersection over union (IoU) to determine meaningful object detection.

2. Confusion matrix: Although not explicitly mentioned, the authors calculated three elements of a confusion matrix - FP, FN, and TP. However, they did not mention calculating true negatives (TN), which would complete the confusion matrix.

3. Saliency maps: There is no information regarding the generation of saliency maps in the given context. Therefore, it cannot be confirmed whether the authors employed this technique as part of their postprocessing steps.
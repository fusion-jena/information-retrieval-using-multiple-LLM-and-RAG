Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

has a frame size of 16 milliseconds and frame shift of 1 millisecond that 
are averaged over the labelled sound to obtain a prediction. In the same 
way we averaged the outputs of the penultimate layer of the model to 
obtain embeddings for each labelled sound that have 4096 dimensions. 
The ResNet152-architecture is based on the ResNet152 model (He 
et al., 2015) developed for image classification, but we did not use pre- 
trained weights. We relied on Keras (Chollet et al., 2015) implementa-
tion of ResNet152 to obtain the model architecture, added a dense layer 
and trained the models from scratch with randomly initialised weights. 
The input was 3 s sound frames converted into mel spectrograms using a 
fast Fourier transform window size of 1024, hop length of 896 and 128 
mel filterbanks. The resulting models had around 58 million trainable 
parameters.  We  trained  two  types  of  models  using  extracts  from  the

2.3. Models 

To  generate  embeddings  we  used  two  publicly  available  models: 
VGGish (Hershey et al., 2017) and BirdNET (Kahl et al., 2021); as well as 
models that we trained using the datasets described above. 

VGGish is a 128-dimensional embeddings model originally trained 
on  AudioSet,  an  event  dataset  with  >2  million  10  s  video  clips  from 
YouTube  (Gemmeke  et  al.,  2017).  The  dataset  has  527  classes  domi-
nated by music and human speech, and although it includes some bio-
acoustic events such as “bird”, “whale song”, “fowl” or “wild animals”, it 
is not directly comparable to the bioacoustic tasks addressed here that 
try to differentiate species. We used the TensorFlow Hub implementa-
tion (TensorFlow Hub, 2023) that relies on an enhanced dataset called 
YouTube-8  M  (Abu-El-Haija  et  al.,  2016)  with  3826  classes,  178  of 
which are within the top level class named “Pets & Animals”.

labelled NIPS4Bplus dataset. We reduced the dimensionality of BirdNET 
embeddings from 420 to 2 dimensions using t-SNE (Fig. 2). Note that 
BirdNET requires a 3-s sound input which we obtained from the sound 
file but, as indicated in methods, it was not always possible to extract a 
whole 3-s extract from the sound file by commencing at the label start. 
The maximum length of the sound files in the dataset is about 5 s, so in 
many cases a 3-s extract from the label start would extend beyond the

Neural information processing scaled for bioacoustics-from neurons to Big Data. In: 
Proceedings of Neural Information Processing Scaled for Bioacoustics: From Neurons 
to Big Data, 2013. http://sabiod.univ-tln.fr/NIPS4B2013_book.pdf. 

Gupta, Gaurav, Kshirsagar, Meghana, Zhong, Ming, Gholami, Shahrzad, Ferres, Juan 

Lavista, 2021. Comparing recurrent convolutional neural networks for large scale 
bird species classification. Sci. Rep. 11 (1), 17085. https://doi.org/10.1038/s41598- 
021-96446-w. 

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, Sun, Jian, 2015. ‘Deep residual learning 

for image recognition’. arXiv. http://arxiv.org/abs/1512.03385. 

Hershey, Shawn, Chaudhuri, Sourish, Ellis, Daniel P.W., Gemmeke, Jort F., Jansen, Aren, 
Moore, Channing, Plakal, Manoj, et al., 2017. CNN Architectures for Large-Scale 
Audio Classification. In: International Conference on Acoustics, Speech and Signal 
Processing (ICASSP).. https://arxiv.org/abs/1609.09430.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In this case, the deep learning pipeline primarily uses audio data. Specifically, the models mentioned - VGGish, BirdNET, and the custom models - all process sound inputs. These sounds are converted into mel spectrograms using fast Fourier transform, indicating that the raw format of the data is indeed audio. While the models themselves are based on architectures initially designed for image classification (ResNet152), they are adapted for audio analysis in this context. Therefore, the primary data format used in this deep learning pipeline is audio.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

|  159

because eventually both methods have sufficient constraints to learn 

active  learning  pipeline  on  the  crops  produced  from  running  the 

a good embedding.

3.3.2 | Active learning strategies

MegaDetector  model  over  the  NACTI  dataset.  We  employed  mar-

gin-based active learning. After the first 30,000 active queries, the 

classifier  achieves  93.2%  overall  accuracy  which  further  confirms 

the usefulness of the suggested pipeline. More detailed results are 

available in Table S3.

Different strategies can be employed to select samples to be labelled 

by the oracle. The most naive strategy is selecting queries at random. 

Here  we  try  five  different  query  selection  strategies  and  compare 

4 |  D I S CU S S I O N

them against a control of selecting samples at random. In particular, 

we try model uncertainty criteria (confidence, margin, entropy; Lewis 

This  paper  demonstrates  the  potential  to  significantly  reduce

Xu, Z., Yu, K., Tresp, V., Xu, X., & Wang, J. (2003). Representative sampling 
for  text  classification  using  support  vector  machines.  In  European 
conference on information retrieval (pp. 393–407). Springer.

Wiley & Sons.

Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learn-
ing with neural networks. In Advances in neural information processing 
systems (pp. 3104–3112).

Swanson, A., Kosmala, M., Lintott, C., Simpson, R., Smith, A., & Packer, 
C.  (2015).  Snapshot  serengeti,  high-frequency  annotated  camera 
trap images of 40 mammalian species in an african savanna. Scientific 
Data, 2, 150026.

1:

2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

Run a pre-trained object detection model on the images

Run a pre-trained embedding model on the crops produced 

by the objection detection model

Select 1,000 random images and request labels from the 

human oracle

Run the embedding model on the labelled set to produce 

feature vectors

Train the classification model on the labelled feature vectors

while Termination condition has not reached do

Select 100 images using the active learning selection 
strategy, pass these to the human oracle for labelling

Fine-tune the classification model on the entire labelled 

set of the target dataset

if number of active queries % 2,000 == 0 then

Fine-tune the embedding model on the entire labelled 

set of the target dataset

end if

end while

Our proposed method allows us to evaluate all data points in order 

to  find  the  most  significant  examples  to  ask  humans  to  label,  while 

3 |  R E S U LT S

ing model, such as a neural network, and try to improve that model by 

alization algorithms like t-SNE (van der Maaten & Hinton, 2008), can 

selecting training samples. Active learning algorithms typically start 

be regarded as embedding functions.

training the underlying model on a small, randomly selected labelled 

Deep  neural  networks  are  frequently  used  for  dimension-

set of data samples. After training the initial model, various criteria 

ality  reduction:  the  input  to  a  deep  network  often  has  many 

can  be  employed  to  select  the  most  informative  unlabelled  sam-

values, but layers typically get smaller throughout the network, 

ples to be passed to the oracle for labelling (Settles, 2009). Among 

and the output of a layer can be viewed as a reduced represen-

the  most  popular  query  selection  strategies  for  active  learning  are 

tation  of  the  network's  input.  In  this  paper,  we  use  two  com-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The strategy implemented to monitor the model performance during training involves employing margin-based active learning. This approach was applied after the first 30,000 active queries, resulting in the classifier achieving 93.2% overall accuracy. This process confirms the effectiveness of the suggested pipeline. Additionally, the authors compared five different query selection strategies against a control of selecting samples at random. These strategies include model uncertainty criteria such as confidence, margin, entropy, and representative sampling for text classification using support vector machines. Detailed results are available in Table S3. However, it should be noted that the specific implementation details of how the model performance is monitored during training are not explicitly stated in the given context.
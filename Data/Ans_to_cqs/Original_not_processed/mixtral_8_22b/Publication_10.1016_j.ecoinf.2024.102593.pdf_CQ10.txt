Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

12 

Detailed 
Detailed (5 s 
frames) 

960 

72,285  Weak 

5481 

2188 

– 

Length 
(m) 

48 

120 

54,599 

-Brazil extract 

111 

5857 

-Complementary 
train extract 

684 

53,110 

-Barn owl extract 

2 

425 

Detailed 
(generated) 
Detailed 
(generated) 
Detailed 
(generated) 

9274 

4160 

188,304 

45,472 

977 

616  

EcologicalInformatics81(2024)1025933F.J. Bravo Sanchez et al.

Kahl, Stefan, Wood, Connor M., Eibl, Maximilian, Klinck, Holger, 2021. BirdNET: a deep 
learning solution for avian diversity monitoring. Eco. Inform. 61 (March), 101236 
https://doi.org/10.1016/j.ecoinf.2021.101236. 

Karpathy, Andrej, 2014. T-SNE Visualization of CNN Codes, 2014. T-SNE Visualization of 

Sainburg, Tim, Thielk, Marvin, Gentner, Timothy Q., 2020. ‘Finding, visualizing, and 
quantifying latent structure across diverse animal vocal repertoires’. Edited by 
Fr´ed´eric E. Theunissen. PLoS Comput. Biol. 16 (10) https://doi.org/10.1371/ 
journal.pcbi.1008228 e1008228.  

CNN Codes. https://cs.stanford.edu/people/karpathy/cnnembed/. 

McDonald, Kyle, Tan, Manny, Mann, Yotam, 2016. Google Creative Lab/AI Experiments 
Bird Sounds, 2016. https://github.com/googlecreativelab/aiexperiments-bird-sou 
nds. 

McGinn, Kate, Stefan Kahl, M., Peery, Zachariah, Klinck, Holger, Wood, Connor M.,

outputs.  The  embeddings  obtained  using  the  BirdNET-Analyzer  v2.1 
models have 420 dimensions.

2.3. Models 

To  generate  embeddings  we  used  two  publicly  available  models: 
VGGish (Hershey et al., 2017) and BirdNET (Kahl et al., 2021); as well as 
models that we trained using the datasets described above. 

VGGish is a 128-dimensional embeddings model originally trained 
on  AudioSet,  an  event  dataset  with  >2  million  10  s  video  clips  from 
YouTube  (Gemmeke  et  al.,  2017).  The  dataset  has  527  classes  domi-
nated by music and human speech, and although it includes some bio-
acoustic events such as “bird”, “whale song”, “fowl” or “wild animals”, it 
is not directly comparable to the bioacoustic tasks addressed here that 
try to differentiate species. We used the TensorFlow Hub implementa-
tion (TensorFlow Hub, 2023) that relies on an enhanced dataset called 
YouTube-8  M  (Abu-El-Haija  et  al.,  2016)  with  3826  classes,  178  of 
which are within the top level class named “Pets & Animals”.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The code repository link of the deep learning pipeline mentioned in the given context is not explicitly stated. However, there is a reference to a GitHub repository related to bird sounds, which might be relevant. Here is the link: <https://github.com/googlecreativelab/aiexperiments-bird-sounds>. Please note that this repository may not contain the exact deep learning pipeline used in the research but could provide insights into similar work.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

59.

van den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, et al. Wavenet: A generative
model for raw audio. arXiv preprint arXiv:160903499. 2016.

60. Hochreiter S, Schmidhuber J. Long short-term memory. Neural computation. 1997; 9(8):1735–80.

PMID: 9377276

61. Kosmala M, Wiggins A, Swanson A, Simmons B. Assessing data quality in citizen science. Frontiers in

Ecology and the Environment. 2016; 14(10):551–60. https://doi.org/10.1002/fee.1436

62. Welinder P, Branson S, Perona P, Belongie SJ, editors. The multidimensional wisdom of crowds.

Advances in neural information processing systems; 2010.

63. Swanson A, Kosmala M, Lintott C, Packer C. A generalized approach for producing, quantifying, and

validating citizen science data from wildlife images. Conservation Biology. 2016; 30(3):520–31. https://
doi.org/10.1111/cobi.12695 PMID: 27111678

and semantic segmentation. Proceedings of the IEEE conference on computer vision and pattern rec-
ognition; 2014.

34. Piczak KJ, Environmental sound classification with convolutional neural networks. 2015 IEEE 25th

International Workshop on Machine Learning for Signal Processing (MLSP); 2015: IEEE.

35. Salamon J, Bello JP. Deep convolutional neural networks and data augmentation for environmental

sound classification. arXiv preprint arXiv:160804363. 2016.

36. Hershey S, Chaudhuri S, Ellis DP, Gemmeke JF, Jansen A, Moore RC, et al. CNN Architectures for

Large-Scale Audio Classification. arXiv preprint arXiv:160909430. 2016.

37. Hinton G, Deng L, Yu D, Dahl GE, Mohamed A-r, Jaitly N, et al. Deep neural networks for acoustic

modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing
Magazine. 2012; 29(6):82–97.

38. Hannun A, Case C, Casper J, Catanzaro B, Diamos G, Elsen E, et al. Deep speech: Scaling up end-to-

Acoustic training datasets

We trained our BatDetect CNNs using a subset of full-spectrum time-expanded (TE) ultra-
sonic acoustic data recorded between 2005–2011 along road-transects by citizen scientists as
part of the Indicator Bats Programme (iBats) [7] (see Supplementary Information Methods for
detailed data collection protocols). During surveys, acoustic devices (Tranquility Transect,
Courtplan Design Ltd, UK) were set to record using a TE factor of 10, a sampling time of
320ms, and sensitivity set on maximum, giving a continuous sequence of ‘snapshots’, consist-
ing of 320ms of silence (sensor listening) and 3.2s of TE audio (sensor playing back x 10). As
sensitivity was set at maximum, and no minimum amplitude trigger mechanism was used on
the recording devices, our recorded audio data contained many instances of low amplitude
and faint bat calls, as well as other night-time ‘background’ noises such as other biotic, abiotic,
and anthropogenic sounds.

binary classification problem. Our CNNFULL consisted of three convolution and max pooling
layers, followed by one fully connected layer (see Supplementary Information Methods for fur-
ther details). We halved the size of the input spectrogram to reduce the input dimensionality
to the CNN which resulted in an input array of size of 130 frequency bins by 20 time steps, cor-
responding to a fixed length, detection window size of 23ms. We applied the CNN in a sliding
window fashion, to predict the presence of a search-phase bat call at every instance of time in
the spectrogram (Fig 1D). As passive acoustic monitoring can generate large quantities of data,
we required a detection algorithm that would run faster than real time. While CNNs produce
state of the art results for many tasks, naïve application of them for detection problems at test
time can be extremely computationally inefficient [33]. So, to increase the speed of our system

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The method utilized for collecting raw data in the deep learning pipeline is through full-spectrum time-expanded (TE) ultrasonic acoustic data recorded between 2005-2011 along road-transects by citizen scientists as part of the Indicator Bats Programme (iBats). Acoustic devices (Tranquility Transect, Courtplan Design Ltd, UK) were set to record using a TE factor of 10, a sampling time of 320ms, and sensitivity set on maximum, resulting in a continuous sequence of'snapshots', consisting of 320ms of silence (sensor listening) and 3.2s of TE audio (sensor playing back x 10). With sensitivity set at maximum and no minimum amplitude trigger mechanism used on the recording devices, the recorded audio data contained many instances of low amplitude and faint bat calls, as well as other night-time 'background' noises such as other biotic, abiotic, and anthropogenic sounds.
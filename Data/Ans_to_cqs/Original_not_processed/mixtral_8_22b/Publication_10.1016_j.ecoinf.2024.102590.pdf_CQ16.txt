Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The supervised learning workflow adheres to the steps outlined in 
Sections 2.3.1–2.3.5. However, as detailed in Section 2.3.2, we deviated 
by not generating 400 random points over the entire boundary. Instead, 
we used the vegetation types described by Gertenbach (1983) to guide 
the placement of our sampling plots, from which we then collected the 
training and validation points (Fig. A.2). The creation of several small 
sampling plots within each vegetation type, instead of using one large 
area, also helped speed up the sampling process. Performing the data 
collection  within  the  sampling  plots  ensured  that  we  had  training/ 
validation  points  that  were  representative  of  the  diverse  vegetation 
within each boundary. To determine the size of the sampling plots, we 
first calculated 5% of the total area covered by the boundary, divided 
that area by the number of vegetation types intersecting the boundary

Table 3 
Statistics include percentage of Non-woody (NW) and Woody (W) cover (%), Overall accuracy, Producer’s accuracy (%), and User’s accuracy (%) per photojob.  

Photojob 

Non-woody cover (%) 

Woody cover (%) 

Overall accuracy (%) 

Class 

Producer’s accuracy (%) 

User’s accuracy (%) 

165A (1942) 

165B (1942) 

56p (1944) 

155 (1940) 

150 (1939) 

81 

71 

72 

77 

78 

19 

29 

28 

23 

22 

92 

87 

89 

91 

83

2.3.1. Data upload and script preparation 

To perform a land cover (LC) classification, GEE requires two initial 
inputs:  an  aerial  image  and  the  boundary  of  the  area  of  interest.  We 
uploaded the subplot boundary and the aerial images of each subplot as 
GEE  assets.  Once  the  assets  were  imported  into  our  JavaScript-based 
program,  the  training/validation  datasets  could  be  created.  Since  we 
were interested in extracting the woody cover, we divided our training 
point features into two cover classes: ‘non-woody’ (LC property = 1) and 
‘woody’  (LC  property  = 2).  Woody  cover  was  considered  to  be  all 
vegetation  with  woody  stems  (i.e.,  excluding  grasses  and  forbs), 
including  trees  that  were  shrubby  (i.e.,  multi-stemmed)  and  short  in

based Landsat 8 data classification in Google earth engine using random Forest: the 
case study of Maiella National Park. Remote Sens. 13, 2299. https://doi.org/ 
10.3390/rs13122299. 

Trabucco, A., Zomer, R., 2014. Influence of Aridity on Vegetation [WWW Document]. 

https://doi.org/10.2499/9780896298460_23. 

United Nations Convention to Combat Desertification, 2022. Global Land Outlook, 2nd 

edition. UNCCD, Bonn.  

Urban, M., Heckel, K., Berger, C., Schratz, P., Smit, I.P.J., Strydom, T., Baade, J., 

Schmullius, C., 2020. Woody cover mapping in the savanna ecosystem of the Kruger 
National Park using Sentinel-1 C-band time series data. Koedoe 62, 1–6. https://doi. 
org/10.4102/koedoe.v62i1.1621. 

Verburg, P.H., Neumann, K., Nol, L., 2011. Challenges in using land use and land cover 
data for global change studies. Glob. Chang. Biol. 17, 974–989. https://doi.org/ 
10.1111/j.1365-2486.2010.02307.x.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't explicit information about the specific criteria used to determine when training is complete. However, it can be inferred that the overall accuracy, producer's accuracy, and user's accuracy might play a role in evaluating the performance of the model during training. These metrics are mentioned in Table 3, but no threshold or target values are specified.

In general, common criteria used to determine when training is complete include reaching a certain number of epochs, achieving a desired level of accuracy, observing a plateau in validation loss, or meeting other predefined stopping conditions. Without more specific details regarding the training process, it is difficult to provide a definitive answer based solely on the given context.
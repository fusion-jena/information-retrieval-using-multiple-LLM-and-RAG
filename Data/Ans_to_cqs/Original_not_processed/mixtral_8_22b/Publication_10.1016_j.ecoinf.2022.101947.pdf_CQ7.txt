Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Further algorithm validation was undertaken using holdback trials, 
to establish the minimum number of trails required for testing. Although 
the predicted mean for each test:training set size was close to the actual 
test size up to the 32:12 combination, after a ratio of 20:24 the amount 
of  variation  in  the  sequential  trials  increased,  potentially  resulting  in 
reduced algorithm accuracy. 

Fig. 4. Holdback partitioning trials using test and training sets. The test set size 
was plotted against itself (black circle), against the predicted value for the test 
size  (black  X)  and  against  the  mean  predicted  value  for  each  test  size 
(blue square).

mean of the predicted test set size diverges from the actual test set size. 
Optimal classification accuracy was obtained when the test set size 
was smallest relative to the training set. However, even when the test to 
training set  ratio  was  32:12, the  predicted value  was  close  to the  ex-
pected value, demonstrating the robustness of the model.

Data validation was performed on the captive Amur tiger dataset to 
determine the optimal number of footprint trails required for the algo-
rithm training set and test sets that could provide an accurate population 
estimate. Fig. 4 shows the result of a holdback trial partitioning test on 
training sets for 44 captive Amur tigers. Using the algorithm generated 
in FIT, the analysis was iterated 10 times for each combination of test/ 
training  size,  with  randomly  selected  trails,  to  examine  how  the  pre-
dicted outcome compared with the known test set size. The figure shows, 
for example, that then the test set size (y axis) comprises trails from 4 
tigers, and the test/training set size comprises trails from 04/40 tigers, 
the predicted test set sizes are very similar across a range of partitioning 
trials. However, when the test set size is 36 tigers, and the test/training 
ratio 36/08 tigers, there is a wide range of predicted test set sizes and the

Trails  were  collected  from  free-ranging  Amur  tiger,  of  unknown 
identity, at the study sites. Table 2 reports the trail ID collected for each 
trail. As described above (Section 3.1) sub-trails were formed where >10 
footprints existed in any one trail. For example, the table indicates that 
using the 25 left hind footprints from trail MW09, we were able to derive 
four sub-trails. Section 3.5 below explains how the sex classification for 
images and the resulting designations for each trail were made. 

3.3. Data validation to determine the optimal size for algorithm training 
and test sets

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text provides information about the process of determining the optimal size for algorithm training and test sets for a deep learning model applied to classify footprints of captive Amur tigers. The authors conducted holdback partitioning trials using different combinations of test and training set sizes to find the minimum number of trails required for testing. They found that the optimal classification accuracy was achieved when the test set size was smallest relative to the training set. Specifically, they observed that the predicted value was close to the expected value even when the test to training set ratio was 32:12, which demonstrated the robustness of the model.

To perform this analysis, the authors used the algorithm generated in FIT and iterated it ten times for each combination of test/training size, with randomly selected trails. This allowed them to compare the predicted outcome with the known test set size. The results showed that when the test set size comprised trails from four tigers, and the test/training set size comprised trails from 04/40 tigers, the predicted test set sizes were very similar across a range of partitioning trials. However, when the test set size was 36 tigers, and the test/training ratio was 36/08 tigers, there was a wide range of predicted test set sizes.

Therefore, based on the given context, the criteria used to split the data for deep learning model training were not explicitly stated. However, it can be inferred that the authors used random selection of trails to create different combinations of test and training sets and evaluated their performance by comparing the predicted outcomes with the known test set sizes. Additionally, they aimed to achieve optimal classification accuracy while minimizing the number of trails required for testing.
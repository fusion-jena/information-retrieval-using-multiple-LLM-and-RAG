Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

For the CNN, The large number of hyperparameters (Table 4) and 
high  computational  demand,  mean  that  an  exhaustive  grid-search  is 
inappropriate.  Instead  our  preliminary  work  showed  that  common 
default parameters, were suitable for our data. These include a batch- 
size of 32 images and a cross-entropy loss function. We also used the 
Adam learning rate optimizer (Kingma and Ba, 2015), which automat-
ically adjusted our initial learning rate of 1e-03 during training in a way 
that  improved  performance.  Adam  is  computationally  efficient  and 
straight-forward to use. In preliminary work, each model was set to train 
for  100  epochs  maximum.  However  for  later  time-saving  and  better 
automation, we enabled early stopping if the validation error (loss) did 
not reduce for 10 epochs. This identified a suitable number of epochs for 
each dataset: 14, 14 and 23 epochs for Datasets 1, 2 and 3, respectively.

Learning 
rate 

Optimizer 

SVM: 
C 

γ 

The number of images you send to the model in each iteration. 
Model parameters are updated after each batch during training. 
How many times you pass the full image dataset through the model. 
The error metric that you wish to minimize. 
e.g. Cross entropy loss for multi-class classification. 
A small number (0, 1] that determines the amount to alter parameters 
during training with respect to the loss. 
Also known as the step size. 
An algorithm that modifies CNN parameters according to a particular 
strategy to minimize the loss. 
e.g. the Adam optimizer sets the learning rate adaptively for faster and 
more efficient training.

2.2.5. Classification 

Each  ML  approach  requires  hyperparameters  to  classify  imagery, 
which when optimized during training can increase model performance, 
see  Table  4  for  a  hyperparameter  glossary.  Given  the  computational 
efficiency of the SVMs and the few hyperparameters required, each of 
these can be optimized simply and relatively quickly (subject to dataset 
size)  during  a  k-fold  (k = 5) cross-validated  fine  grid-search  on  the 
training  data.  For  our  CNN  þ SVM  method,  we  followed  hyper-
parameter  recommendations  by  (Hsu  et  al.,  2016),  authors  of  the 
LIBSVM library (Chang and Lin, 2011). For our non-linear RBF SVM we 
searched  hyperparameters  C = 23, 23.25, …, 27  and  γ = 2
(cid:0) 13  & 
(cid:0) 11. For the linear SVM, we used the same hyperparameter search for 
2
its sole parameter C. We also looked at the RBF and linear SVM with 

(cid:0) 15, 2

Fig. 3. A diagram of various support vector machines.

From a coding perspective, training an SVM is extremely straight-
forward, with training, optimisation and testing executed within only a 
few simple lines of code. The relative complexity with the CNN þ SVM 
approach,  and  by  extension  the  CNN  approach,  rather  lies  in  data 
management and feature extraction. The complexity of these steps is not 
so  much  related to the  actions required, but navigating  the extensive 
literature and knowing “where to start”. As one of the contributions of 
this paper we hope to better guide the user with clear and detailed de-
scriptions  of  these  steps,  that  are  applicable  across  ML  frameworks 
(Table 3). Aside from these preparations, training the CNN classifier is 
further complicated since an optimal is not automatically found. Instead 
performance  metrics  must  be  monitored  across  epochs  and  decisions 
made on when to stop training - typically the point at which training and

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In this study, the researchers utilized the Adam learning rate optimizer for their Convolutional Neural Network (CNN) model. The Adam optimizer is a popular choice due to its efficiency and ease of use. It dynamically adjusts the learning rate during training, improving overall performance. By using the Adam optimizer, the researchers could avoid manually tuning the learning rate, making it more convenient and effective. Additionally, they employed early stopping as another optimization technique. Early stopping involves halting the training process once the validation error does not decrease for a certain number of epochs. In this case, the researchers stopped training if there was no improvement in validation error for 10 consecutive epochs. This strategy helped identify the appropriate number of epochs needed for each dataset, resulting in 14, 14, and 23 epochs for datasets 1, 2, and 3, respectively.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics75(2023)1020324S. Steinbach et al.                                                                                                                                                                                                                               

3. Methodology 

3.1. Wetland Use Intensity calculation

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Wetlands 
Wetland use intensity 
East Africa 
Ecosystem health 
Earth observation 
Sentinel-2

URL. https://medium.com/sentinel-hub/improving-cloud-detection-with-machine- 
learning-c09dc5d7cf13 (accessed 4.14.22).  

EcologicalInformatics75(2023)10203216

the  Sentinel-2  image  database  grows,  further research  should  explore 
the comparison of WUI between time steps to measure trends and shifts 
as  well  as  across  regions,  other  wetland  management  categories,  and 
between specific agricultural use practices.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information on the metrics used to evaluate the performance of a deep learning model for calculating Wetland Use Intensity using Earth Observation data from Sentinel-2 satellites. However, common metrics used to evaluate the performance of deep learning models include accuracy, precision, recall, and F1 score. Accuracy measures the proportion of correct predictions out of total predictions made by the model. Precision is the ratio of true positive predictions to all positive predictions made by the model. Recall is the ratio of true positive predictions to all actual positives in the dataset. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both precision and recall. It is important to note that the choice of evaluation metrics depends on the problem domain and the desired trade-off between false positives and false negatives.
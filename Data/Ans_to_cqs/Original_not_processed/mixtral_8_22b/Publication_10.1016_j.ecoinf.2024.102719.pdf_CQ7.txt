Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

learning problem: the CoIL challenge 2000. Mach. Learn. 57, 177–195. https://doi.
org/10.1023/B:MACH.0000035476.95130.99.

Van Der Valk, D., Picek, S., 2019. Bias-variance decomposition in machine learning-

based side-channel analysis. Cryptol. ePrint Arch. 1–27.

Wohl, E., Angermeier, P.L., Bledsoe, B., Kondolf, G.M., MacDonnell, L., Merritt, D.M.,

Palmer, M.A., Poff, N.L.R., Tarboton, D., 2005. River restoration. Water Resour. Res.
41, 1–12. https://doi.org/10.1029/2005WR003985.

Wolpert, D.H., 1992. Stacked generalization. Neural Netw. 5, 241–259. https://doi.org/

10.1016/S0893-6080(05)80023-1.

Woo, S.Y., Jung, C.G., Lee, J.W., Kim, S.J., 2019. Evaluation of watershed scale aquatic
ecosystem health by SWAT modeling and random forest technique. Sustain 11.
https://doi.org/10.3390/SU11123397.

During this process the hyperparameters of the heterogeneous
ensemble models were set to those optimized by the TPE algorithms.
Each ML model was trained on 100 different subsets generated from the
original training sets using the bootstrap method (sampling with
replacement). The number of samples in each subset equaled the num-
ber of samples in the original training data. The predictions of the
trained models on the testing dataset were averaged to obtain the main
prediction. The average loss, average bias, and average variance were
then derived for each model. The noise was assumed as 0 following
previous studies (Domingos, 2000; Kohavi and Wolpert, 1996).

3.3. Model interpretation

Predictive models that relate river health to gradients of environ-
mental factors can identify potential causes of impairment in areas of
ecological concern, supporting strategic decisions on prioritizing resto-
ration activities and new monitoring initiatives. Such models should be
generalizable to unseen data for predicting the river health in unmoni-
tored areas. Bias–variance decomposition is widely adopted for gener-
alizability evaluations of ML models (Guan and Burton, 2022; Van Der
Putten and Van Someren, 2004; Van Der Valk and Picek, 2019).
Although deep learning and other ML models have been widely
employed in river health predictions (Gazendam et al., 2016; Kwon
et al., 2024; Lee et al., 2021), they have not been supplemented with
evaluation techniques that would confirm their generalizability to un-
seen datasets. In the present study, the generalizability of various ML

hyperparameters for the base model in terms of accuracy, which in-
dicates the proportion of correctly classified samples among all samples
in the validation set. The four heterogeneous ensemble models were
founded on these optimized base models. Although the ensembles were
constructed from the optimized base models, the meta-models in
stacking and blending are critically important and require special
attention. Therefore, an additional layer of optimization was introduced
for fine-tuning the meta model parameters of stacking and blending.

The entire dataset was randomly divided into a training set (60%), a
validation set (20%), and a testing set (20%). Each ML model was
trained on the training set, optimized on the validation set, and evalu-
ated on the testing set (Fig. 3).

The RF, MLP, LR, NB, and stacking ensemble were implemented in
the scikit-learn package (V.1.0.2) in Python 3.7. The CAT, LGB, XGB,

EcologicalInformatics82(2024)1027195T. Park et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The entire dataset was randomly divided into three parts: a training set (60%), a validation set (20%), and a testing set (20%). Each Machine Learning (ML) model was then trained on the training set, optimized on the validation set, and evaluated on the testing set. This approach allows for better tuning of the model's hyperparameters during the training phase and provides a more accurate assessment of its performance when exposed to previously unseen data.

The training set is used to fit the model, i.e., adjust the weights and biases of the neural network based on the input features and corresponding target values. The validation set serves two purposes: firstly, it helps prevent overfitting by allowing early stopping when the model starts performing poorly on this set; secondly, it enables hyperparameter tuning through cross-validation or grid search methods. Lastly, the testing set remains completely separate from both the training and validation processes, providing an independent evaluation of the final model's performance.
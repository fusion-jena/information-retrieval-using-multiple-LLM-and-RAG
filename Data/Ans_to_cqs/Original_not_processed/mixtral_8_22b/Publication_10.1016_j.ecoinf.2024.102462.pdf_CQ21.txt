Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

training sets are presented as confusion matrices in Fig. 6 (bottom row). 
Furthermore, the lower half of Table 4 shows the number of patches for 
each class after data augmentation.

the  experts.  Then  the  network  is  fed  this  annotated  data  and  the  pa-
rameters of the model are adjusted following an optimization process. 
Once this training process is finished, the model is ready to process new 
unseen  data.  DL  networks  are,  thus,  able  to  capture  statistics  and  re-
lationships directly from the data following human expertise. If the data 
used  for  training  the  model  is  representative  of  the  problem  and  the 
annotations are carefully generated, the resulting network can perform 
at a similar level with any input data set. This capacity to translate the 
training to other independent datasets is called the generalization power 
of the network. In practice, differences in performance between the re-
sults with the training data (or data that is similar to it, i.e., from the 
same  site  and  acquired  at  the  same  time)  and  those  of  independent 
datasets (often in research papers referred to as the testing data set) can

In order to solve this problem, we used a DL approach. DL networks 
can be seen as non-linear functions with a large number of parameters. 
The parameters of each DL are adjusted using annotated data (usually 
referred  to  as  a  “supervised  approach”).  Once  this  process  (called 
training the network) is finished, the evaluation of the network on the real 
problem  is  performed  by  feeding  it  an  independent  data  set  not  seen 
during training (the testing data set). DL techniques are currently a hot 
topic in machine learning with applications in a variety of fields, and are 
considered the current state of the art for CV applications. In fact, in 
some disciplines, this technology is considered to have exceeded human 
capabilities (Buetti-Dinh et al., 2019; He et al., 2015).

4.1. Observer study 

As detailed in Section 3.2, a group of observers were asked to classify 
457  patches  in  the  testing  set  into  6  possible  classes  to  determine  a 
baseline of human performance. The patches comprised an area of 100 
× 100 pixels and typically depicted one single shrub. The only excep-
tions were patches depicting very small shrubs that included small parts 
of nearby vegetation. The overall classification results for each observer 
are presented using confusion matrices in Fig. 5.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: After the model training, several postprocessing steps can be taken to evaluate the performance of the deep learning (DL) network. These include generating saliency maps, calculating metrics, and creating confusion matrices.

Saliency maps help visualize which areas of the image contribute most significantly to the final prediction made by the model. They provide insights into how the model interprets different features within the images.

Calculating various metrics such as accuracy, precision, recall, F1 score, etc., helps quantitatively assess the performance of the model. These metrics offer valuable information about the strengths and weaknesses of the model when dealing with specific classes or types of inputs.

Confusion matrices are another useful tool for evaluating the performance of the trained model. A confusion matrix provides a summary of prediction results on a classification task. It displays the count of true positives, false negatives, false positives, and true negatives for each class. By analyzing the confusion matrix, researchers can identify patterns of misclassification and understand where the model struggles.

In the given context, confusion matrices are mentioned as part of the postprocessing steps after model training. Furthermore, the text discusses the use of annotated data during the supervised training process and the subsequent evaluation of the network on an independent dataset (testing data set). However, the exact postprocessing steps involving saliency maps and metric calculations are not explicitly stated but are common practices in evaluating deep learning models.
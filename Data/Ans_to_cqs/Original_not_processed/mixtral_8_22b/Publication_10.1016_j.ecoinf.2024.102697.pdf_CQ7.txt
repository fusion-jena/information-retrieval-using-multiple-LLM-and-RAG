Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

the  primary  hyperparameters  required  for  application  of  the  random 
forest method. Here, hyperparameter optimization was performed using 
Bayesian  Optimization  to  minimize  a  5  k-fold  cross-validation  loss 
during model development (Section 2.7).

2.7. Machine learning training methodology 

To ensure accuracy and reliability of results, a repeated nested k-fold 
cross-validation approach was employed for all ML models developed 
here. An outer 10 k-fold cross-validation provided an initial division of 
the data into holdout data and data for model development (Molinaro 
et al., 2005). An inner 5 k-fold cross-validation was then used to mini-
mize the bias caused by tuning of hyperparameters on training results, as 
demonstrated in previous studies (Cawley and Talbot, 2010; Yates et al., 
2022).  The  training  performance  was  calculated  based  on  the  inner 
cross-validated  ensemble,  while  the  testing  performance  was  deter-
mined by selecting the optimal set of hyperparameters from the entire 
training set to predict on the holdout data. This process was repeated 15 
times for each unique model to reduce the potential bias and variance 
related to random sampling (Kim, 2009).

2.5. Machine learning methods

In  the  subsequent  sections,  we  assess  model  performance  in  the 
context of model complexity and explainability, crop specific training 
against  generalized  training,  and  contrast  remote  sensing  and  meteo-
rological measurements to provide predictive performance for several 
potential use cases. 

3.1. Model complexity 

The management of model complexity is an important consideration 
in  data-driven  model  development (Jolliffe,  1986; Wang  et  al.,  2017; 
Yates  et  al.,  2021).  In  this  study  we  define  model  complexity  as  the 
number of variables included in the predictor set. Proper selection of 
variables, i.e. feature selection, can minimize training costs, measure-
ment  requirements,  and  decrease  the  likelihood  of  overfitting  (Banks 
and Fienberg, 2003; Cawley and Talbot, 2010).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
A repeated nested k-fold cross-validation approach was employed for all machine learning models developed here. An outer 10 k-fold cross-validation provided an initial division of the data into holdout data and data for model development. An inner 5 k-fold cross-validation was then used to minimize the bias caused by tuning of hyperparameters on training results. The training performance was calculated based on the inner cross-validated ensemble, while the testing performance was determined by selecting the optimal set of hyperparameters from the entire training set to predict on the holdout data. This process was repeated 15 times for each unique model to reduce the potential bias and variance related to random sampling. Therefore, the criteria used to split the data for deep learning model training were a combination of outer 10 k-fold cross-validation and inner 5 k-fold cross-validation.
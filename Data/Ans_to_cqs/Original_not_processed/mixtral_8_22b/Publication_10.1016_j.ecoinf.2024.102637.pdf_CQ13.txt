Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Aʹ(x) = (1 (cid:0) λ)A(x) + λ(x)

(10)  

where,  λ  is  a  hyperparameter  used  to  adjust  the  balance  relationship 
between the nonlinear factors of the activation function and the identity 
mapping of Aʹ(x) based on the number of training iterations, which can 
be expressed as λ = e
E. Here, e represents the current number of rounds of 
training and E represents the total number of rounds of model training. 
→0, and Aʹ(x)→A(x). At this point, a strong 
nonlinear  factor  exists  between  the  two  convolutional  layers  of  the 
network, which can achieve better training of the model. When e→E, 
λ→1, and Aʹ(x)→x. Nonlinear factors are gradually integrated into the 

Starting training, λ = e
E

generalised  model  by  adding  convolution  modules  or  introducing  re-
sidual  connection  techniques.  However,  more  complex  and  deeper 
models  often  incur  higher  computational  costs  and  require  longer 
training  time.  To  solve  these  problems,  the  AlexNet  network  was 
designed to be lightweight and optimised to achieve deep training and 
short-term testing (Zhang et al., 2022). Specifically, the two convolution 
layers are merged in the model deployment mode while ensuring that 
the nonlinear factors caused by the activation function are not reduced. 
By reducing the number of layers in the model, the computational effi-
ciency is improved (Ding et al., 2021; Zhao et al., 2017) and the infer-
ence time of the model is significantly shortened.

can  further  improve  the  efficiency  and  performance  of  deep  learning 
models and promote the development of related fields.

In view of this discovery, we re-examined the application strategies 
of deep learning, no longer pursuing model complexity but exploring the 
potential  of  shallow  networks.  Through  meticulous  algorithm  optimi-
sation  and  structural  design,  we  strive  to  maintain  the  model  perfor-
mance while reducing the computational complexity  and deployment 
costs,  achieving  a  balance  between  speed  and  accuracy.  The  specific 
contributions of this study are as follows:  

• We propose a lightweight SIAlex model that utilises AlexNet as the 
backbone,  fully  exploiting  the  performance  of  minimalist  models. 
Ensuring a good balance between speed and accuracy. 

• The  method  of  cascading  multiple  activation  functions  fully  in-
troduces  nonlinear  factors  such  that  the  model  approximates  the 
nonlinear  expression  function  of  the  learning  features  while  also 
improving the gradient propagation.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does mention one hyperparameter, specifically for adjusting the balance relationship between the nonlinear factors of the activation function and the identity mapping of A'(x), which is denoted as λ. This hyperparameter is calculated using the formula λ = e^(E/e), where 'e' represents the current number of rounds of training and 'E' represents the total number of rounds of model training. As the value of 'e' approaches 'E', the value of λ tends towards 1, causing A'(x) to approach x. This indicates that nonlinear factors are gradually integrated into the generalized model. However, it should be noted that the context does not provide information about other common hyperparameters like learning rate or optimizers.
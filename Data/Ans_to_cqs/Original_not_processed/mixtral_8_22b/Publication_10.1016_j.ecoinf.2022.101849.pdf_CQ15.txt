Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

1.  A feed-forward neural network. We chose a fixed architecture of a 
feed-forward neural network, of three layers. The first layer consists 
of 47 input neurons for each climatic feature. The hidden layer has 
40 neurons with sigmoid activation functions. The third layer con-
sists of 13 output neurons for each land cover type, with a softmax 
function, to satisfy the constraint of the outputs summing up to unity. 
The  neural  network is  trained with  Adam  optimizer  and  the  mean 
absolute error loss function using the Keras (Chollet, 2015) library. 
This loss function was chosen as it is more robust to outliers.  

2.  Multivariate random forests. The model is trained using the Scikit- 
learn (Pedregosa et al., 2011) library. We chose 50 trees in the forest, 
a  squared  error  function  for  measuring  the  quality  of  a  split  and 
unlimited tree depth.

A related research question in ecology and biogeography is how to 
predict the potential natural vegetation (PNV). PNV is the expected state 
of mature vegetation, given a particular set of environmental constraints 
in the absence of human intervention (Chiarucci et al., 2010). At first, 
PNV models were constructed based only on expert knowledge, whereas 
nowadays, various statistical techniques and machine learning methods 
are more widely employed (Hemsing and Bryn, 2012). In Hengl et al. 
(2018), authors evaluate different machine learning methods, such as 
neural  networks,  random  forests,  gradient  boosting,  and  k-nearest 
neighbours,  for  PNV  mapping  in  a  classification  setting.  The  latter 
example  describes  global  PNV  mapping.  However,  most  PNV  studies 
focus on specific areas or regions (Raja et al., 2019; Vaca et al., 2011; 
Hemsing and Bryn, 2012). 

1.2. Why the task is difficult

1. Introduction 

Target variables are usually fully labeled in the classical supervised 
machine learning setting. In real-world predictive tasks, however, labels 
are often scarce and/or noisy. Various definitions and terms are used in 
the  literature  to  describe  variants  of  noise  and  scarceness  of  labels 
(Allison, 2001; Xie and Huang, 2018; Nikoloski et al., 2021; Sun et al., 
2010; Gao et al., 2017; Alarc´on and Destercke, 2021; Van Engelen and 
Hoos, 2020), and each setting requires tailored approaches for exploit-
ing such target labels. In this study, we formulate a new computational 
setting for regression, where target labels are structurally incomplete. We 
computationally study this task via a case study in predictive modeling 
of global vegetation cover. 

1.1. The vegetation modelling task

Table 1 
An example training instance for different weakly-labeled settings. Here yj indicates the targets of an instance. Red indicates an 
error in the training data (deviation from ground truth) – it is not known to the user where errors occur. Note that in the weak 
j=1yj = 1 and yj⩾0). In our setting 
label setting, only 0s may be errors. Shading represents a constraint (in this example, that 
(structurally incomplete), the constraint may not be met, but errors are neither random deviations. Mixtures of these settings 
are possible, e.g., we may have structurally incomplete with missing labels, etc. 

∑
5

EcologicalInformatics72(2022)1018492R. Beigait˙e et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In the provided piece of context, one optimization technique is mentioned in relation to the deep learning pipeline. Specifically, the feed-forward neural network is trained with the Adam optimizer. This choice of optimizer is likely due to its ability to adaptively adjust the learning rate during training, which can lead to faster convergence and improved performance compared to other optimization algorithms like stochastic gradient descent (SGD). Additionally, the use of the mean absolute error loss function suggests that the goal of the model is to minimize the average magnitude of the prediction errors, rather than focusing solely on minimizing the overall error rate. Overall, these choices reflect a thoughtful approach to designing and implementing a deep learning pipeline for the given task.
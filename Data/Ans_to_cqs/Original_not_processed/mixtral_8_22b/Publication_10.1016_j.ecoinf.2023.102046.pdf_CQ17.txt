Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

ResNet-50 is a convolutional neural network model that uses skip 
connections,  which  made  it  possible  to  get  good  performance  with 
deeper models than was previously possible (He et al., 2015). Training 
deep neural networks can result in exploding gradients, and skip con-
nections  were  introduced  to  ameliorate  that  problem.  ResNet-50  was 
chosen  for  comparison  as  it  is  a  proven  CV  classification  model.  It  is 
commonly used and provides a well-performing baseline. 

Inception-V3  is  a  convolutional  neural  network  used  for  image 
classification, that has an auxiliary classifier that acts as a regularizer 
(Szegedy et al., 2015b). The Inception-V3 architecture is built on pre-
vious Inception models, with the aim of making the V3 computationally

given by the L2-norm. We solve: 

min
w

‖ Xw (cid:0) y‖2

2 + α ‖ w‖2

2, α ≥ 0  

where X is the input matrix and y the corresponding label vector. The 
regularization parameter α controls the strength of the L2 term. 

For  SVC  and  ridge  regression,  we  use  the  implementation  from 

Scikit-learn (Pedregosa et al., 2011). 

2.2.2. Fine-tuning models 

We compare fine-tuning of three models. 
Vision Transformer (ViT) is a deep learning model for vision pro-
cessing  tasks  that  extends  the  original  transformer  architecture  (Vas-
wani et al., 2017) designed for natural language processing applications 
(Dosovitskiy et al., 2023). For ViT, an image is split into patches which 
are fed into the transformer encoder, using attention to learn what parts 
of the image input are relevant.

The Inception-V3 model accepts images resized to (299, 299) pixels 
but ViT and ResNet resize to (224, 224) pixels. Dataset imbalance was 
dealt with by including class weights in the cross-entropy loss function. 
For the training we used AdamW optimizer for Hugging Face models, 
which is the default optimizer in the transformers library by Hugging-
Face. For Inception-V3, we used Adam since that was the optimizer used 
in  previous  work  (and  AdamW  is  not  available  in  Keras).  We  used  a 
(cid:0) 4  for 
batch  size  of  16  examples  and  the  learning  rate  was  set  to  10
(cid:0) 5  for  Vanilla  Inception-V3  and  ViT.  For  Deep  Otolith 
ResNet  but  10
(cid:0) 4 as suggested by Politikos 
Inception we used a learning rate of 4 × 10
et  al.  (2021).  For  every  fine-tuning  experiment,  the  model  with  the 

smallest validation loss was used for evaluation on the test set. 

2.3. Performance

For fine-tuning, we used the HuggingFace implementations of ViT 
(“google/vit-hugepatch14–224-in21k”) and ResNet (“microsoft/resnet- 
50”)  but  the  Keras  implementation  of  Inception-V3.  All  fine-tuning 
models  had  been  pre-trained  on  the  ImageNet  (Deng  et  al.,  2009) 
dataset. 

We replaced the classifier head on all models so that it received the 
output of the models along with the additional parameters, quarters and 
length. We also tried adding additional dropout and dense layers at the 
end of the Inception model as for the DeepOtolith model Politikos et al. 
(2021). To differentiate we use Deep Otolith Inception-V3 for the model 

EcologicalInformatics76(2023)1020463A.R. Sigurðard´ottir et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Regularization methods are employed in deep learning pipelines to prevent overfitting and improve generalization. Two common regularization techniques mentioned in the given context are dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps reduce interdependent learning among them. This method is not explicitly mentioned in the provided text, but it is commonly used in deep learning architectures like those discussed here. On the other hand, L2 regularization is directly referenced in the context. Also known as weight decay, this method adds a penalty term to the loss function based on the square of the magnitude of coefficients. By doing so, it encourages smaller weights and prevents any single feature from dominating the decision boundary. In the given context, L2 regularization is applied in the Ridge Regression model, where the regularization parameter alpha controls the strength of the L2 term.
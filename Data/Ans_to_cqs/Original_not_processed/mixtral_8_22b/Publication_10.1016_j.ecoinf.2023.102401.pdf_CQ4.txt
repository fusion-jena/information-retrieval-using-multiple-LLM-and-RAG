Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.4. Data augmentation 

Data augmentation techniques play a crucial role in augmenting the 
training dataset to enhance the learning effect and generalization per-
formance of the network. This experiment uses various data augmen-
tation methods, including HSV adjustment, rotation, scaling, cropping, 
flipping, and mosaicking, to expand the dataset. Random probabilities 
are used to determine whether each image should undergo augmenta-
tion. HSV  adjustment involves modifying  the image's  hue, saturation, 
and value components. And rotation randomly rotates the image within 
a specific range. Moreover, scaling changes the size of the image while 
maintaining its aspect ratio. Cropping randomly crops a portion of the 
image.  Additionally,  Flipping  horizontally  flips  the  image.  Mosaic 
combines  multiple  randomly  cropped,  resized,  and  rotated  images  to 

Table 1 
Detailed information of selected datasets.  

Datasets 

Images 

Category

compared  to  the  original  YOLOv5.  Krishnan  et  al.  (2022)  focused  on 
data  augmentation  techniques  to  increase  the  size  of  training  under-
water datasets, leading to improved detection performance and average 
precision.

Datasets 

Images 

Category 

Aquarium 
Trash-ICRA19 
VisDrone 
NWPU VHR-10 
HRSID 

638 
1144 
1610 
800 
5604 

7 
3 
12 
10 
1 

Train 

510 
915 
1288 
640 
4483 

Test 

128 
229 
322 
160 
1121  

EcologicalInformatics79(2024)1024017P. Liu et al.                                                                                                                                                                                                                                       

create a single image, enhancing the background of the identified object. 
Table  2  presents  the  hyperparameters  used  for  data  augmentation, 
specifying  the  ranges  and  probabilities  for  each  augmentation  tech-
nique.  These  augmentation  techniques  introduce  diversity  into  the 
training  dataset,  enabling  the  model  to  learn  robust  features  and 
improve its performance in real-world scenarios. 

Table 3 
Comparison of networks using different attention mechanism modules.

their ability to automatically learn and extract low- and high-level visual 
features  directly  from  the  data.  Traditional  methods  often  require 
manual  feature  engineering,  where  domain-specific  knowledge  and 
expertise are used to design handcrafted features. Deep learning algo-
rithms,  on  the  other  hand,  can  learn  feature  representations  directly 
from the raw input data, eliminating the need for manual feature engi-
neering.  Efficiently  handling  these  multiscale  features  is  crucial  for 
network performance, given their diverse resolutions. Feature pyramid 
networks  (FPNs)  have  made  significant  advancements  by  integrating 
multiscale  features  in  a  top-down  manner.  The  path  aggregation 
network  (PANet)  (Liu  et  al.,  2016)  further  extends  the  FPN  by  intro-
ducing a bottom-up path. Another notable network learning mechanism, 
the bidirectional feature pyramid network (BiFPN) (Tan et al., 2020),

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The experiment applies several data augmentation techniques to improve the learning effect and generalization performance of the network. These techniques include HSV adjustment, rotation, scaling, cropping, flipping, and mosaicking. HSV adjustment modifies the image's hue, saturation, and value components. Rotation randomly rotates the image within a certain range. Scaling changes the size of the image while maintaining its aspect ratio. Cropping randomly crops a part of the image. Horizontal flipping flips the image. Lastly, mosaicking combines multiple randomly cropped, resized, and rotated images to create a single image, enhancing the background of the identified object. Table 2 provides detailed information about the hyperparameters used for data augmentation, specifying the ranges and probabilities for each technique.
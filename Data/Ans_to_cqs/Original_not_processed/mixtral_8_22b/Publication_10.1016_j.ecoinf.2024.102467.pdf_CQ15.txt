Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Ruder, S., 2016. An overview of gradient descent optimization algorithms. arXiv preprint 

arXiv:1609.04747.  

Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.-C., 2018. Mobilenetv2: 

Inverted residuals and linear bottlenecks, pp. 4510–4520. 

Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., Vaswani, A., 2021. Bottleneck 
transformers for visual recognition. Proceedings of the IEEE/CVF Conference on 
Computer Vision and Pattern Recognition, pp. 16519–16529. 

Tan, M., Le, Q., 2019. Efficientnet: rethinking model scaling for convolutional neural 
networks. In: International Conference on Machine Learning, pp. 6105–6114. 
Tan, M., Le, Q., 2021. Efficientnetv2: smaller models and faster training. In: International 

Conference on Machine Learning, pp. 10096–10106. 

The State of World Fisheries and Aquaculture, 2022, 2022. FAO. https://doi.org/ 

10.4060/cc0461en. 

Wang, C.-Y., Bochkovskiy, A., Liao, H.-Y.M., 2022a. YOLOv7: Trainable Bag-of-Freebies

EfficientnetV2 is an efficient, lightweight feature extraction network. 
The  EfficientNet  series  comprises  a  set  of  lightweight  convolutional 
neural networks. In the EfficientNetV1 series, Google’s focus was pre-
dominantly on optimizing the accuracy, parameter count, and compu-
tational  requirements  of  the  model  (Tan  and  Le,  2019).  The 
EfficientNetV2  series  further  reduces  the  number  of  parameters  and 
computations  while  preserving  the  accuracy  of  the  model,  leading  to 
significant  decreases  in  model  training  and  inference  duration. 
Furthermore, the EfficientNetV2 network uses an improved progressive 
learning approach that dynamically adjusts the regularization method 
based on the training image size. This method can improve the training 
speed  and  accuracy  of  the  network.  Compared  with  some  previous 
networks, the experimental results of this method show that the training

EfficientNetV2 uses the MBConv block from EfficientNetV1. Fig. 2(a) 
shows that the MBConv block is a linear bottleneck layer (Sandler et al., 
2018) with inverted residuals and depth-wise separable convolutions. A 
basic  MBConv  block  consists  of  a  1  × 1  Conv  for  dimensionality 
expansion,  Depthwise  Convolution  (Chollet,  2017),  which  performs 
deep convolution operations on the feature map after expansion with 
either a 3 × 3 or 5 × 5 filter size, SENet (Hu et al., 2018), which en-
hances important channel features and weakens non-important channel 
features, and a 1 × 1 Conv for dimensionality reduction and dropout. 
This block enables the feature extraction network to focus on important 
information  features  and  distinguish  them  from  the  background.  The 
inverted residual structure of this block not only improves the efficiency 
of feature extraction but also reduces the memory required for inference.

speed is increased by 11 times, and the number of parameters is reduced 
by  1/6.8  (Tan  and  Le,  2021).  Based  on  this  network,  we  designed  a 
backbone  to  improve  the  training  speed  of  the  model  while  ensuring 
detection accuracy and speed.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The papers referenced do not directly discuss specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) used within the deep learning pipeline. However, these optimization techniques are commonly employed in deep learning frameworks to minimize loss functions during the training process.

For instance, Ruder (2016) provides an overview of various gradient descent optimization algorithms, including SGD and Adam. These methods help update the weights of the neural network based on the calculated gradients, aiming to find the optimal solution that minimizes the error between predicted and actual values.

While the mentioned papers primarily focus on architectures like MobileNetV2, Bottleneck Transformers, EfficientNet, and YOLOv7, it should be noted that these architectures would still rely on optimization techniques like SGD or Adam during their respective training processes.
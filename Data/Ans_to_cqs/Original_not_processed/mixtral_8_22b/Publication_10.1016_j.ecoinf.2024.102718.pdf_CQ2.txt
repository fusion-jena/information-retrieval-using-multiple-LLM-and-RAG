Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

large-scale image datasets. Various methods have been explored for
vision transformers including contrastive learning and self-distillation

Gaussian Noise
Speckle Noise
To Tensor
Normalize

Parameters

Size: (224, 224)
None
None
Degree: 30
Brightness: 0.4
Contrast: 0.4
Saturation: 0.4
Hue: 0.1
Standard Deviation: 25
Variance: 25
None
Mean: [0.485, 0.456, 0.406]
Std: [0.229, 0.224, 0.225]

transformer architecture in vision tasks, and Xception (Chollet, 2017)
for its depthwise separable convolutions that enhance model perfor-
mance and efficiency. Therefore, we employed our BEiT model to
leverage transformer capabilities with these existing models. This
combination ensures effective and complementary strengths for our
purpose. Furthermore, we add the LIME explainable technique to
enhance the interpretability of our model’s decision-making process
which mainly
broader
transparency
understanding.

promotes

ensures

and

3.1. Data acquisition

dataset is being used to obtain the pretraining weights for our proposed
model. After augmenting the dataset, the number of images increased to
185,431 with 38 classes.

3.3. Dhan-Shomadhan dataset

The proposed BEiT model exhibits considerable performance im-
provements in the classification task compared to various conventional
deep learning models, as summarized in Table 16. Achieving an accu-
racy of 98.1%, the BEiT model exceeds all other State-of-the-art models,
including the previously top-performing model by Farian et al. (96.1%).
This improvement in accuracy is statistically significant, with signifi-
cance denoted by the α markers, indicating p < 0.05 using a two-sided
McNemar’s test against the BEiT model. It achieves a precision of
98.2%, a recall of 97.7%, and an F1-score of 96.5%, which are higher
than those of other models such as the ViT-30, ViT-20, and LAMP-LFD
models. Vivek et al.’s model, which shows high precision (94.5%) and
recall (95.0%), still falls short of the BEiT model’s comprehensive

Table 11
Ablation study evaluating BEiT model performance without added noise.

Model

Acc

Loss

AUC

CK

Precision

Recall

F1

98.33
99.33
97.33

0.092
0.082
0.121

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The given context primarily discusses large-scale image datasets and their use in training deep learning models like the BEiT model. Specifically, it mentions the use of the ImageNet dataset for obtaining pretraining weights. Additionally, the context refers to various techniques applied to images, such as Gaussian Noise, Speckle Noise, ToTensor, Normalization, and several augmentations like rotation, brightness adjustment, contrast change, saturation alteration, and hue modification. These details suggest that the primary data format used in this deep learning pipeline is images. However, there is no explicit mention of other data formats like audio, video, or CSV files.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Fig. 5. Structure of VAE.  

Stochastic  gradient  descent  (SGD)  on  BP  is  managing  stochastic 
input,  then  not  stochastic  unit  within  the  networks.  The  solution  is 
named as “reparameterization trick”, which is to transfer the sampling 
to input layer. It is easy from N(μ(x), θ(x)) by sampling ∈ ~ N(0, I), af-
terward calculating pmodelz = μ(x) + θ1/2(x) * e. Where μ(x) and θ(x) are 
the mean and covariance of (z| x). So, Eq. (13) is calculated as: 

L(q) = Ee∼N(0,I)pmodel

(cid:0)

x|z = μ(x) + θ1/2(x) × ∈

)

(cid:0) DKL(q(z|x)‖pmodel(z) )

(14) 

In VAE is comprised of input layer, various AEs, and output layer. 
Then, an unsupervised pre-training step, the supervised fine-tuning step 
is implemented for learning the entire network parameters by employ-
ing the BP technique. This technique is comprised of 1 input layer, 5 
hidden layers, and 1 output layer. 

4. Performance validation

satellite image classification using deep learning approach. In: Machine Learning 
and Data Mining in Aerospace Technology. Springer, Cham, pp. 165–186. 
Li, L., Zhou, Y., Xie, J., 2014. A free search krill herd algorithm for functions 

optimization. Math. Probl. Eng. 2014. 

Li, Y., Chen, R., Zhang, Y., Zhang, M., Chen, L., 2020. Multi-label remote sensing image 
scene classification by combining a convolutional neural network and a graph neural 
network. Remote Sens. 12 (23), 4003. 

Liu, Q., Basu, S., Ganguly, S., Mukhopadhyay, S., DiBiano, R., Karki, M., Nemani, R., 

2020. Deepsat v2: feature augmented convolutional neural nets for satellite image 
classification. Remote Sens. Lett. 11 (2), 156–165. 

Lunga, D., Gerrand, J., Yang, L., Layton, C., Stewart, R., 2020. Apache spark accelerated 
deep learning inference for large scale satellite image analytics. IEEE J. Select. 
Topics Appl. Earth Observ. Remote Sens. 13, 271–283.

cientNet utilizes inverted bottleneck convolutional that is initial estab-
lished in the MobileNetV2 method that has a layer which initial expands 
the  networks  and  next  compresses  the  channel.  This  framework 
decreased calculation by the factor of 2 as related to normal convolu-
tional, in which f implies the filter size. The researchers in Hassan et al. 
(2021) outperformed that EfficientNetB0 is the easiest of every 8 tech-
niques  and  utilizes  less  parameters.  Therefore,  it  is  directly  utilized 
EfficientNetB0  for  evaluating  efficiency.  Fig.  3  illustrates  the  layered 
architecture of EfficientNet.

A  detailed  comparative  outcomes  analysis  of  the  PTDLEN-VAE 
manner  takes  place  with  other  techniques  on  the  AIM  dataset  in 
Table 4 and Fig. 9 (Li et al., 2020). From the outcomes, it can be obvious 
that  the  CNN-RBFNN,  CA-CNN–BiLSTM,  and  CNN  manners have  out-
performed  least  performance  over  the  other  techniques.  In  line  with, 
MLRSSC-CGNN-SGAT,  MLRSSC-CGNN-MLGAT,  and  AL-RCNN  algo-
rithms have accomplished reasonably closer results. Besides, the DLEN- 
LSTM,  DLEN-DNN,  and  DLEN-VAE  approaches  have  resulted  in 
competitive  outcomes  with  respect  to  various  measures.  But,  the 

Table 4 
Performances of proposed PTDLEN-VAE with existing methods on the AID multi- 
label dataset.  

Table 5 
Performances  of  proposed  PTDLEN-VAE  with  existing  methods  in  terms  of 
computation time (min).  

Methods 

Precision 

Recall 

F1-Score 

F2-Score 

Methods 

UCM Dataset 

AID Dataset

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about the specific optimization techniques used in the deep learning pipeline. However, there is a mention of Stochastic Gradient Descent (SGD) being used in conjunction with Backpropagation (BP). Additionally, the concept of reparameterization trick is mentioned, which is often used in Variational Autoencoders (VAEs) to optimize the loss function during training. Other common optimization techniques used in deep learning include Adam, RMSProp, and Momentum. Without more specific details, it cannot be definitively stated which optimization techniques were used in this particular case.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.3.  Model training 

2.3.1.  Training data preprocessing 

The  CNN  model  used  in  this  study  requires  equally  sized  input 
images. We chose a time-frequency input window size of 2 s as it is near 
the mean and median template duration across target call types (Fig. 2). 
Most  call  types  have  a  duration  below  1  s,  and  for  those  above  2  s,

The manual effort in training data creation was reduced to template 
creation and validation of template-based detections in a graphical user 
interface (Fig. 3). This addresses an important need for more accessible 
training  data  from  study  sites  to  leverage  deep  learning  for  acoustic 
monitoring.  Our  evaluation  demonstrates  that  strong  classification 
performance  can  be  achieved  using  data  collected  from  the  study  re-
gion,  without  relying  on  crowd-sourced  public  datasets.  The  pipeline 
thus  increases  the  potential  for  region-optimized  acoustic  monitoring 
systems.  Furthermore,  the  training  data  collection  pipeline  could  ac-
celerate the collection of data for rare species. 

Our  training  scheme  allowed  for  multi-label  learning  from  single- 
label  training  data  by  defining  a  custom  training  loss  and  including

To  expand  this  approach  to  the  broader  community,  we  have 
identified three important challenges for future research. First, future 
developments  should  account  for  the  large  variability  in  the  size  of 
target  calls  (i.e.  templates).  Introducing  recurrent  connections  in  the 
CNN, or other architecture modifications could potentially reduce the 
negative  effects  of  window  size.  Second,  previous  studies  have  found 
data  augmentation  to  significantly  improve  performance  (Kahl  et  al., 
2019).  In  these  cases,  training  data  was  mainly  based  on  monodirec-
tional  recordings  of  single  species,  and  data  augmentation  (i.e.  noise 
addition)  apparently  helped  to  emulate  the  conditions  of  soundscape 
recordings.  The  effect  may  be  reduced  for  training  data  collected  di-
rectly from soundscapes, as in this study. Still, data augmentation may

important features can still be captured within 2 s. For the case of input 
frequency bandwidth, we chose to use the entire range of 24 kHz. This 
was chosen over a smaller, more focused bandwidth for several reasons. 
Firstly, we approach training as a multi-label classification problem. In 
other  words,  for  each  input  audio  segment,  the  model  is  trained  to 
predict  the  set  of  all  species  present,  rather  than  a  single  foreground 
class.  This  eliminates  the  need  to  focus  on  single  target  calls  in  the 
input. Also, many species' calls highly overlap in frequency (Fig. 2). So, 
while  a  focused  bandwidth  could  separate  several  call  types  by  fre-
quency and avoid their presence together in the same input, many calls 
would still potentially be present together. A large bandwidth also re-
duces the number of predictions required to cover a 1-min recording. 
Furthermore,  it  increases  model  generalizability  because  the  optimal

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not provide information about the specific hardware resources used for training the deep learning model, such as GPUs or TPUs. Therefore, I cannot provide a helpful answer to this query based on the available context.
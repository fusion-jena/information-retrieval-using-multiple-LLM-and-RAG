Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.2. Split the datasets 

In this research, the datasets were randomly divided into three parts 
for training, validation and testing. Here, 60% of the data was used to 
train the deep learning models, and 20% of them was kept to validate the 
models. The remaining 20% data was used to evaluate the performance 
of the models. Table 1 show the number of data in the dataset and how 
they are separated for training, testing and validation. 

2.3. Deep learning models

2.6.4. Model training with selected patches 

We trained the same deep learning models as mentioned in Section 
2.5. For training, we resized all the patches of images to a resolution of 
256 × 256 pixels. Although after dividing the image into patches, there 
were images with lower resolution, i.e., 64 × 64 and 128 × 128 pixels, 
they were converted into a uniform size. 

2.6.5. Evaluation of the models 

An image was divided into patches first to predict its class label. Then 
the important patches were selected using the approach mentioned in 
Section 2.6.3. After that, the model predicted the class label for each 
patch. The weighted majority voting technique was used to predict the 
class label of the image from the predicted labels. 

2.7. Experimental setup

datasets. A global average pooling layer followed by two dense layers 
with 1024 neurons and ReLU (Rectified Linear Unit) had replaced the 
fully connected layer of the original model. The final output layer was 
another dense layer with a softmax activation function, and the number 
of  neurons  varied  depending  on  the  number  of  classes.  Although  the 
maximum number of epochs for training was set to 100, the training was 
completed  before  that  because  of  the  early  stopping  strategy  by 
inspecting validation accuracy. The initial learning rate was set to 1 ×
(cid:0) 6  by monitoring the validation 
10
loss  after  every  epoch.  We  used  “Adam”  optimiser  and  “Categorical 
Cross Entropy” loss for training all deep learning models. The input size 
for all the DL models was 256 × 256 × 3 and due to capability of the 
computing device the batch size was set to 32. 

(cid:0) 4  and was randomly decreased to 10

3. Results

Our  proposed  technique  (98.49%)  outperformed  the  graph-based 
model (98.1%). DenseNet201 model achieved 97% and 99% accuracy 
for  chinee  apple  and  parkinsonia  weed,  respectively.  Although  the 
performance  was  not  improved  for  snake  weed  species,  the  model 
yielded 100% accuracy for classifying native plants. This will ensure not 
to kill the off-target plants, reducing the waste of herbicide and saving 
the native ecosystem (Olsen et al., 2019). 

3.5. Performance improvement on class imbalanced dataset

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The strategy implemented to monitor the model performance during training is the early stopping strategy. This strategy inspects the validation accuracy after every epoch and stops the training process if it detects no further improvements. In this case, the maximum number of epochs for training was set to 100, but the training was completed before reaching that limit due to the early stopping strategy. Additionally, the initial learning rate was set to 1 x 10^-6 and was randomly decreased to 10^-6 by monitoring the validation loss after every epoch.
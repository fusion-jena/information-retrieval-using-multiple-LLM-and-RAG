Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

10.3.5 Evaluation Metrics

Predictions are made for every species of E50 and several model performance
metrics are calculated for each species and for two disjoints and randomly sampled
subsets of sites: A train set (4781 sites) which is used for ﬁtting all models and a
test set (400 sites) which aims at testing models generalization capacities. Then,
train and test metrics are averaged over the 50 species. The performance metrics are
described in the following.

10.3.5.1 Mean Loss

Mean loss, just named loss in the following, is an important metric to consider
because it is relevant regarding our ecological model and it is the objective function
that is minimized during model training. The Mean loss of model m on species i
and on sites 1, . . . , K is:

Loss(m, i,

1, . . . , K

{

)
}

=

1
K

K

k

1

=

λm,θi (xk)

−

yi
k log(λm,θi (xk))

shift and help to regularize our model as well. It consists of a parameterized
centering and reduction of pre-activations. This facilitates optimization and enables
to raise the learning rate leading to a quicker convergence. At the same time, it has
a regularization effect because the centering and reduction of a neuron activation
is linked to the mini-batch statistics. The mini-batch selection being stochastic at
every iteration, a neuron activation is stochastic itself, and the model will not rely
on it when it has no good effect on prediction.

m (x, θ )

j )T al

:=

1,.

Learning of model parameters is done through optimization (minimization
by convention) of an objective function that depends on the prediction goal.
Optimization method for NN parameters θ is based on stochastic gradient descent
algorithms, however, the loss function gradient is approximated by the back-
propagation algorithm [18].

Counts of Festuca cinerea Vill. with DNN50

D)

Counts of Festuca cinerea Vill. with DNN1000

50

48

46

44

42

e
d
u
t
i
t
a
L

50

48

46

44

42

−5

0

5

−5

0

5

Longitude

Longitude

E)

Counts of Festuca cinerea Vill. with MAXENT

F)

Counts of Festuca cinerea Vill. with LGL

50

48

46

44

42

−5

0
Longitude

5

e
d
u
t
i
t
a
L

50

48

46

44

42

−5

0
Longitude

5

Fig. 10.4 Real count of Festuca cinerea Vill. and prediction for ﬁve different models. Test sites
are framed into green squares. (a) Number of observations in INPN dataset, and geographic
distribution predicted with (b) CNN1000, (c) DNN50, (d) DNN1000, (e) Maxent, (f) LGL

10.5 Discussion

The performance increase with multi-responses models shows that multi-responses
architecture are an efﬁcient regularization scheme for NNs in SDM. It could be
interesting to evaluate the performance impact of going multi-response on rare
species where data rare limited. We have systematically noticed false predicted

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Answer: Based on the given context, the criterion used to determine when training is complete is not explicitly mentioned. However, we can infer from the information provided that the mean loss or simply 'loss' is an important metric considered during the training process. The mean loss is also the objective function that is minimized during model training. Additionally, the text mentions that learning of model parameters is done through optimization (minimization by convention) of an objective function that depends on the prediction goal. Optimization methods for neural network parameters are based on stochastic gradient descent algorithms, while the loss function gradient is approximated using the back-propagation algorithm. Therefore, one possible criterion for determining when training is complete might be reaching a certain threshold or stability in the mean loss value.
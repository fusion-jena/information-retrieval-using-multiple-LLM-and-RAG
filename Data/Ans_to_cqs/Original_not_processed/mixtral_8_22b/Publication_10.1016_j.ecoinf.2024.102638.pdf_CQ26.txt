Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The dataset is then divided into training sets, which comprise 80% of 
the  data,  and  a  test  set  representing  the  remaining  20%,  in  order  to 
maximise model training while maintaining a sufficient amount of data 
for robust model validation (Adjuik and Davis, 2022; Fern´andez-L´opez 
et al., 2020). This division is done by random sampling to ensure that 
both subsets are representative of the overall dataset and minimise the 
risk  of  bias.  Randomisation  helps  to  preserve  the  distribution  of  key 
features  and  target  variables  across  both  training  and  test  sets, 
enhancing the generalisability of the model. We leverage the scikit-learn 
function GridSearchCV to meticulously fine-tune the hyperparameters of 
each model within the confines of the training set, aiming to enhance 
their predictive performance. A five-fold cross-validation is used and the 
negative mean square error is set as the target measure. Following this

EN 

KNN 
GP 

TREE 

RF 

XGB 

SVR 

MLP 

alpha 
l1_ratio 
neighbours 
weights 
p 
alpha 
max_depth 
min_samples_split 
min_samples_leaf 
max_features 
n_estimators 
max_depth 
min_samples_split 
min_samples_leaf 
max_features 
n_estimators 
learning_rate 
max_depth 
subsample 
colsample_bytree 
reg_alpha 
gamma 
min_child_weight 
C 
gamma 
epsilon 
hidden_layer_sizes 
activation 
alpha 
learning_rate 
max_iter 

* BP denotes Best Parameter. 

(cid:0) 1] 

(cid:0) 5, 10

(cid:0) 2, 10

Table 2 presents a summary of instances before and after the filtering 
process, along with the percentage of valid data for each variable. The 
pre-filtered instances represent the total number of data points before 
any  filtering  procedures  were  applied.  After  filtering,  post-filtered in-
stances  depict  the  remaining  data  points  that  passed  the  filtering 
criteria. The ‘Significant data (%)’  column indicates the proportion of 
instances  deemed  valid  after  filtering,  providing  insights  into  the 
effectiveness of the data cleaning process. As discussed in more detail in 
subsection 2.3, data filtering consists of manually removing outliers and 
smoothing them with a moving average.

Reichstein, M., Camps-Valls, G., Stevens, B., Jung, M., Denzler, J., Carvalhais, N., 
Prabhat, 2019. Deep learning and process understanding for data-driven earth 
system science. Nature 566, 195–204. https://doi.org/10.1038/s41586-019-0912-1. 

Ryu, Y., Baldocchi, D.D., Black, T.A., Detto, M., Law, B.E., Leuning, R., Miyata, A., 
Reichstein, M., Vargas, R., Ammann, C., Beringer, J., Flanagan, L.B., Gu, L., 
Hutley, L.B., Kim, J., McCaughey, H., Moors, E.J., Rambal, S., Vesala, T., 2012. On 
the temporal upscaling of evapotranspiration from instantaneous remote sensing 
measurements to 8-day mean daily-sums. Agric. For. Meteorol. 152, 212–222. 
https://doi.org/10.1016/j.agrformet.2011.09.010. 

Schwank, M., Wigneron, J.P., Lopez-Baeza, E., Volksch, I., Matzler, C., Kerr, Y.H., 2012. 
L-band radiative properties of vine vegetation at the MELBEX III SMOS cal/val site. 
IEEE Trans. Geosci. Remote Sens. 50 (5), 1587–1601. https://doi.org/10.1109/ 
TGRS.2012.2184126.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To address data bias during preprocessing of the deep learning pipeline, several techniques can be employed. In the given context, one technique mentioned is random sampling to divide the dataset into training and testing sets. This method ensures that both subsets are representative of the overall dataset and minimises the risk of bias. By preserving the distribution of key features and target variables across both training and test sets, it enhances the generalisability of the model.

Stratified splitting is another technique that could have been used but was not explicitly mentioned in the context. It involves dividing the dataset such that the proportions of classes or categories remain consistent between the training and testing sets. This approach is particularly useful when dealing with imbalanced datasets.

Oversampling and undersampling are also common methods to handle class imbalance. Oversampling increases the frequency of minority classes, while undersampling reduces the frequency of majority classes. These techniques help to balance the dataset and prevent models from being biased towards the majority class. However, they were not specifically mentioned in the context.

Lastly, diverse data collection is a proactive approach to addressing data bias. Collecting data from various sources and ensuring diversity in the dataset can help mitigate potential biases. While not directly related to preprocessing, it plays a crucial role in building unbiased machine learning models.

In conclusion, based on the provided context, random sampling is used to minimise data bias during preprocessing. Other techniques like stratified splitting, oversampling, undersampling, and diverse data collection, though not explicitly mentioned, are also relevant approaches to handling data bias.
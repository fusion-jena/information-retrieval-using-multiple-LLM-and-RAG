Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

the parameter setting in training, we employ ﬁxed learning policy. 
We set the learning rate to 10 −3 , and then decrease it by a fac- 
tor of 10 when the validation set accuracy stops improving. The 
momentum is set to 0.9 and weight decay to 10 −4 . In this experi- 
ment, we compute the top-1 classiﬁcation accuracy as described in 
Section 4.3 . 

6.2.1. Results and discussion

We  train  our  model  using  Caffe  [35]  framework.  For  the  pa- 
rameter  setting  in  training,  we  employ  step  learning  policy.  The 
learning rate was initially set to 10 −3  for all layers to accept the 
newly deﬁned last fully connected layer set to 10 −2 . It is higher 
than other layers due to the weights being trained starting from 
random. The learning rate was then decreased by a factor of 10 ev- 
ery 20K iteration and was stopped after 100K iterations. The units 
of the third fully connected layer (fc8) were changed according to 
the number of classes of training data. We set the batch size to 50 
and momentum to 0.9. We applied L 
2 weight decay with penalty 
multiplier set to 5 × 10 −4 and dropout ratio set to 0.5, respectively. 

4.2. Deconvolutional network

Dataset 

Quantity of images 

Number of 
categories 

MS COCO [51] 
Places2 [52] 
Sport-1M [53] 
Visual Genome QA [54] 
ILSVRC 2010 [55] 
PlantClef2015 dataset [6]  113,205 

328k (2.5 million labeled instances) 
8.3 million 
1 million 
1.7 million questions/answer pairs 
1.4 million 

91 
365 
487 
–
10 0 0 
10 0 0 

pre-train each network using its corresponding leaf data. During 
the validation phase, we combine both softmax outputs and com- 
pute the ﬁnal class scores using fusion methods: average (ave) or 
max voting (mav).

170 , conv 4 

373 , conv 4 

In Fig. 8 , we visualise the response of the selected ﬁlter units 
( conv 4 
365 ) in layer 4. In layer 4, 
148  and conv 4 
we  observe  mid-level  semantic  partial  abstraction  of  leaf  struc- 
tures, where the features extracted have almost similar complexity 
levels to layer 3. For example: venation-like features are observed 
in the channel conv 4 
373 ( Fig. 5 (i)) based on the deconvolution re- 
sult of the validation set; the neurons are not only activated on 
the divergent structures (secondary veins) but on the central veins 
(primary veins) as well. For the channel conv 4 
170 , the selected ﬁl- 
ters are activated by the curvature of the lobed leaves, as shown 
in the deconvolution of the top two image patches ( Fig. 5 (j)). This 
can be interpreted as extraction of conjunctions of curvature fea- 
tures  in  certain  orientations.  On  the  other  hand,  for  the  chan- 

8 

S.H. Lee et al. / Pattern Recognition 71 (2017) 1–13

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The completion of training is determined by monitoring the performance of the model on a separate validation set. Specifically, the authors use the validation set accuracy as an indicator of whether the model has learned useful representations. When the validation set accuracy stops improving, they consider the training process to be complete. Additionally, they also adjust the learning rate during training. Initially, the learning rate is set to 10^-3, and then it is decreased by a factor of 10 when the validation set accuracy stops improving. This approach allows them to fine-tune the model and prevent overfitting. Furthermore, they apply regularization techniques such as weight decay and dropout to further improve generalization. Overall, these strategies help ensure that the model learns robust and meaningful representations without overfitting to the training data.
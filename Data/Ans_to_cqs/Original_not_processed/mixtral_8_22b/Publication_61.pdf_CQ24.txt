Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Training a neural network requires to set several hyper-parameters
such as the learning rate, number of epochs and batch size. The learning
rate is a special one since it deﬁnes how much the weights are ‘moved’
to decrease the loss. A bigger one could cause the network not to learn,
on the other hand, a smaller one could require much more steps for
learning. To decrease this issue, we train the network with Adam op-
timizer (Kingma and Ba, 2015), a variant of the stochastic gradient
optimization where the learning rate is adjusted automatically. The rest
of the parameters are set empirically, more details are presented in the
experiments section.

5. Experiments

In this section, we present the experimental characterization of the
proposed approach. The implemented network was trained with Adam
optimizer (Kingma and Ba, 2015) on a Inter Core i7 machine with
NVIDIA GeForce 1080 GPU. The hyperparameters were set as follows:
learning rate 0.01, number of epochs 150, batch size 2500.

max pooling operation is performed, using a kernel of 2 × 2 and stride
two. Next, the features are ﬂattened to a one dimension vector of size
400. Later, three fully connected layers are applied with 120, 84, and 2
nodes respectively. Until this point, the output of the CNN is a vector of
real numbers called logits. Therefore, a LogSoftMax function is applied
to convert the logits into a normalized probability distribution:

LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature 521 (7553), 436.
Lee, S.H., Chan, C.S., Wilkin, P., Remagnino, P., 2015. Deep-plant: plant identiﬁcation
with convolutional neural networks. IEEE Int. Conf. Image Process. (ICIP) 2015,
452–456.

Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick,
C.L., 2014. Microsoft coco: common objects in context. In: European Conference on
Computer Vision. Springer, pp. 740–755.

Munisami, T., Ramsurn, M., Kishnah, S., Pudaruth, S., 2015. Plant leaf recognition using
shape features and colour histogram with k-nearest neighbour classiﬁers. Proc.

Valiente-Banuet, A., Casas, A., Alcántara, A., Dávila, P., Flores-Hernández, N., del Coro
Arizmendi, M., Villaseñor, J.L., Ramrez, J.O., 2000. The vegetation of the valley of
tehuacan-cuicatlan. Bot. Sci. (67), 25–74.

Vasquez-Gomez, J.I., 2019. Cactus Aerial Photos. https://www.kaggle.com/

E. López-Jiménez, et al.

Ecological Informatics 52 (2019) 131–138

batch size, and data augmentation) aﬀect the ﬁne-tuning of pre-trained
models. DeepPlant is another example of a system for plant identiﬁ-
cation based on a CNN that uses a pre-training approach (Lee et al.,
2015). Background textures it is always an element that complicates the
classiﬁcation task. Xiao et al. proposed a segmentation and crop
method, based on Fourier transformations and a K-means algorithm, to
generate a region of interest in the images; the cropped images are then
used to train a CNN for diﬀerent plant species classiﬁcation (Xiao et al.,
2018). CNN have not been used only for taxonomic classiﬁcation of
plants, but also for the detection of diseases that can aﬀect them
(Sladojevic et al., 2016).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, some implicit strategies can be inferred from the context. For instance, the use of the Adam optimizer (Kingma & Ba, 2015) adjusts the learning rate automatically, which might help mitigate the effects of random initialization or random weight updates during training. Additionally, setting hyperparameters like the learning rate, number of epochs, and batch size empirically suggests that these values may have been chosen through experimentation and fine-tuning rather than being randomly assigned. This process could potentially reduce the impact of randomness in the model performance. Furthermore, techniques like data augmentation mentioned in the context can introduce controlled variability in the dataset, helping the model generalize better and reducing its sensitivity to random factors. Nonetheless, without specific information regarding random seeds or their management, it cannot be definitively stated what strategies are explicitly employed to handle randomness in the deep learning pipeline.
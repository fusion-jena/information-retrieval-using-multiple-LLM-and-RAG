Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

training datasets.

•  In the first phase, a classification model is built by training a CNN on a given database T0 (Fig. 2a)
•  Then, the second phase consists of tuning a risk threshold τi specific to each class (i.e. each species in our 

case), noted i, with i ∈ {1, ..., n} , using a second and independent database noted T1 (Fig. 2b).

In terms of classification, it means we transform the 2 classification options (correct, wrong) in 3 options 

(Fig. 3) by applying Eqs. (15, 16).

Computing the confidence thresholds.  After the phase 1 (model training phase), for an image X of the 
threshold tuning dataset processed by the classifier, we obtain an output {C(X), S(X)} , where C(X) is the class 
(i.e. species, belonging to the trained set of species) with the highest classification score S(X). For this image, we 
know the ground truth Y in {1, .., n} belonging to the same set of species classes.

0.85

1

0.87

0.75

0.9

0.98

1

0.97

0.8

0.91

0.94

0.98

0.8

0.93

0.07

Table 3.   Accuracy of the model without post-processing, and with post processing according to our goals, on 
the cross-validation, with thresholds tuned on T1 and applied on T2. Each line shows the result for a species, 
with: the species name, the accuracy of the model without post processing, and the accuracy of the model with 
post processing according to the 3 goals defined earlier.

Whatever the goal, our framework is highly flexible and can be adapted by tuning the species thresholds 
regulating the trade-off between classification robustness and coverage in an attempt to monitor biodiversity 
through big datasets where species are unidentified. To unclog the bottleneck of information extraction about 
organism forms, behaviors and sounds from massive digital data, machine learning algorithms, and particularly 
the last generation of deep learning algorithms, offer immense promises. Here we propose to help the users to 
control their error rates in ecology. This is a valuable addition to the ecologist’s toolkit towards a routine and 
robust analysis of big data and real-time biodiversity monitoring from remote sensors. With this control of error 
rate in the hands of users, Deep Learning Algorithms can be used for real applications, with acceptable and 

Scientific RepoRtS  |        (2020) 10:10972  | 

https://doi.org/10.1038/s41598-020-67573-7

11

Mar. Freshw. Res. 68(10), 1950–1954 (2017).

 27.  Gaston, K. J. What is rarity? In Rarity 1–21. (Springer, Dordrecht, 1994).
 28.  Chow, C. On optimum recognition error and reject tradeoff. IEEE Trans. Inf. Theory 16(1), 41–46 (1970).
 29.  Corbière, C., Thome, N., Bar-Hen, A., Cord, M., Pérez, P. Addressing Failure Prediction by Learning Model Confidence. arXiv 

e-prints https ://arXiv .org//arXiv :1910.04851  (2019).

 30.  Cortes, C., DeSalvo, G. & Mohri, M. Boosting with abstention. In Advances in Neural Information Processing Systems (eds Diet-

terich, T. G. et al.) 1660–1668 (A Bradford Book, Cambridge, 2016).

 31.  Geifman, Y. & El-Yaniv, R. Selective classification for deep neural networks. In Advances in Neural Information Processing Systems 

(eds Dietterich, T. G. et al.) 4878–4887 (A Bradford Book, Cambridge, 2017).

 32.  De Stefano, C., Sansone, C. & Vento, M. To reject or not to reject: That is the question—An answer in case of neural classifiers.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The strategy implemented to monitor the model performance during training involves two phases. In the first phase, a classification model is built by training a Convolutional Neural Network (CNN) on a given database T0. Once the initial model is trained, the second phase begins, which focuses on tuning a risk threshold specific to each class (species in this case) using a second and independent database called T1. This process allows for adjustments to ensure optimal performance and accuracy when dealing with large datasets containing unidentified species.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

threshold during training for 3000 iterations. At the end of the training process, the best
performing model based on the validation loss was saved and used as the Faster R-CNN
model with a NMS threshold of 0.7 during testing.

as a feature [13]. As the study showed, they require a large training sample to maintain a
good performance of their approach.

is an improvement in the previous version by adding an effec-tive decoder module to improve the boundaries of the segmentation results [41]. Further-more, apart from ResNet-101, an Xception model can be used as a feature extractor while applying a depth-wise separable convolution to both ASPP and the decoder module, hence improving the speed and robustness of the encoder-decoder network.  Figure 2. A schematic diagram of a fully convolution neural network for semantic segmentation. The network consists of an encoder part where the model extracts potential useful features and the decoder part, which up-samples the extracted feature map to produce the final segmentation results. 3.2. Phase 2: Leaf Extraction Using Connected Component A classic connected component algorithm was first introduced by Azriel and John in 1966 [42]. Since then, numerous different implementations have been proposed for im-proving existing ones [43]. In image processing, connected components analysis helps to

9 of 23

Table 1. Dataset summary used for training the single-leaf classiﬁer.

Datasets

UBDH dataset
Flavia dataset

Total

Single Leaves
(Positive Samples)

Non-Single Leaves
(Negative Samples)

798
83

881

1015
0

1015

Figure 4. Negative samples manually extracted from the segmentation model results for training the single-leaf classiﬁer.
The dataset consisted of 881 intact individual leaves as positive training samples and 1015 negative samples.

4.2. Pre-Processing and Training of Semantic Segmentation Model

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it is not explicitly mentioned how the data was split for deep learning model training into train, test, and validation sets. However, it is common practice in machine learning to randomly divide the available data into three subsets: training set, validation set, and test set. Typically, around 60-80% of the data is allocated for training, 10-20% for validation, and the remaining 10-20% for testing. This division allows the model to learn patterns from the training data, tune hyperparameters using the validation data, and evaluate its generalization performance on unseen data using the test data.

In this case, we do see that the best performing model was selected based on the validation loss after training for 3000 iterations. This suggests that there was indeed a separate validation set used during the training process. However, without further information, we cannot determine the exact proportions or method used to split the data.
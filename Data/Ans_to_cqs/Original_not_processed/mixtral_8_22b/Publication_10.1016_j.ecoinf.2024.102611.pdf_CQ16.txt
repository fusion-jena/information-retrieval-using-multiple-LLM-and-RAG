Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.4. Level-wise training, validation, and testing strategies with Random 
Forest classifier 

The  data  collection  is  split  into  eleven  groups  based  on  visual 
morphological  characteristics,  as  discussed  in  Section  2.  Each  group 
consists of several plant species varieties in the range of 7–14 plants per 
group covering  100  medicinal  plant  species with  training,  validation, 
and testing proportion as 70:20:10 in both the levels, apart from GSL100 
datasets,  a  completely  new  datasets  RTL80  and  RTP40  comprising 
random collections captured in real-time is used to assess the efficiency 
of the proposed hierarchical classification model. The main evaluation 
objective using random groups is to analyze the sensitivity of classifier 
samples collected in various real-time scenarios.

Table  3  gives  insights  into  the  distribution  of  trained,  tested,  and 
validated samples across  the groups. The data suggest that consistent 
ratios across groups are maintained in the sampling process to avoid any 
potential bias during classification. Different samples are present across 
groups to establish the reliability and generalizability of the model. For 
evaluation,  both  validation  and  test  sets  are  considered  separately  to 
fine-tune the model’s hyperparameters, leading to generalizability and 
scalability towards unseen datasets. 

4.5. Classifier specifications 

The Random Forest (RF) classifier is used in both levels of the hier-
archical classification model for performing group and plant-level pre-
dictions. Let several estimators N and Fi(x) be the function of predicting 
̂
the ith  decision tree for the input X, then the prediction 
C by RF is given 
by (9). 

̂C =

1
N

∑N

i=1

Fi(X)

(9)

validation sets (VS). The inference from Table 4 indicates the less false 
positive  rate  by  achieving  the  highest  precision,  recall,  and  F1-score 
towards each group using a fusion of convolution features. Seven out 
of 11 groups always exhibit precision of more than 90% and recall of 
more than 90% for six groups, whereas in the case of test sets, the pre-
cision and recall rate of more than 90% for eight groups, and the same is 
accurate concerning the F1-score metric.

CF-TS- 
Precision 

Non-CF-TS- 
Precision 

CF-TS- 
Recall 

Non-CF- 
TS- 
Recall 

CF-TS- 
F1 
Score 

Non-CF- 
TS-F1 
Score 

G1 
G2 
G3 
G4 
G5 
G6 
G7 
G8 
G9 
G10 
G11 

0.91 
0.86 
0.8 
0.84 
0.7 
0.91 
0.92 
0.95 
0.96 
0.94 
0.96 

0.78 
0.8 
0.7 
0.68 
0.38 
0.74 
0.62 
0.89 
0.92 
0.76 
0.86 

0.83 
0.87 
0.88 
0.88 
0.92 
0.93 
0.92 
0.89 
0.96 
0.89 
0.98 

0.66 
0.73 
0.72 
0.74 
0.71 
0.78 
0.73 
0.86 
0.89 
0.69 
0.89 

0.87 
0.86 
0.84 
0.86 
0.8 
0.92 
0.92 
0.92 
0.96 
0.92 
0.97 

0.71 
0.76 
0.71 
0.71 
0.5 
0.76 
0.67 
0.87 
0.9 
0.72 
0.87  

5.4. Performance of level one - hierarchical classification model-GSL100 
datasets

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the provided context, it appears that the authors use a combination of training, validation, and testing sets with a ratio of 70:20:10 respectively. However, there is no explicit mention of the criteria used to determine when training is complete. Typically, common criteria include reaching a certain number of epochs or iterations, early stopping when performance on a validation set stops improving, or when a desired level of accuracy or loss is achieved. Given the information available, we cannot definitively state what criteria were used in this study.
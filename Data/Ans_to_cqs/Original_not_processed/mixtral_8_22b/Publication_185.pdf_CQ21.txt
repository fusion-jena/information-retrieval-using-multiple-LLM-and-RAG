Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

factor  of  10  every  69  iterations  and  training  was  stopped  after  6900  iterations.  The  number  of 
units in the third fully connected layer (fc3) was changed according to the number of classes of 
training data. We set the batch size to 69 and momentum to 0.9 and applied L2 weight decay with 
penalty multiplier set to 5 Ã— 10-4, dropout ratio set to 0.5, CPU mode

I
D 

M
O 
M
G 
M
O 
M
G 
M
O 
M
G 
T
O 

T
G 

Lycae
nidae 

Nympha
Nympha
lidae 

Pieri
dae 

Papilion
idae 

Saty
ridae 

Hesper
iidae 

Total 

pre-
processing 
processing

823 

828 

92 

92 

5 

5 

1147 

406 

1152 

413 

128 

129 

5 

5 

46 

46 

5 

5 

347 

351 

39 

40 

5 

5 

702 

560 

3985 

none 

706 

565 

4015 

78 

79 

5 

5 

63 

63 

5 

5 

446 

449 

30 

30 

Green 
Green 
background 
background

none 

Green 
Green 
background 
background

none 

Green 
Green 
background 
background

202 

100 

56 

100 

30 

89 

577 

none 

202 

100 

56 

100 

30 

89 

577 

Green 
Green 
background 
background

ima
ge 
sour
ce 

(Zh
ou, 
199
4)   

(Ho
ng 
et 
al., 
200
0) 

Figure 1. The left is original image, the right is pre

processed image used for training set 
The left is original image, the right is pre-processed image used for training set

. 

3

The experiments were performed on a small
size dataset without data augmentation. The model 
The experiments were performed on a small-size dataset without data augmentation. The model 
results demonstrate that it is feasible to use small dataset to train deep learning models as long as 
results demonstrate that it is feasible to use small dataset to train deep learning models as long as 
results demonstrate that it is feasible to use small dataset to train deep learning models as long as 
properly. The MG and MO models can both get good training results 
the parameters are chosen properly. The MG and MO models can both get good training results 
properly. The MG and MO models can both get good training results 
and they perform well in their own validation set, proving that deep learning is a powerful method 
and they perform well in their own validation set, proving that deep learning is a powerful method

3.2.Test Accuracy 

removed  model  have  almost  the 
Table  2  shows  that  the  original  image  model  and  background
Table  2  shows  that  the  original  image  model  and  background-removed  model  have  almost  the 
same  validation  performance  (to  two  decimal  places),  reaching  98%  accuracy,  while  the  MG 
same  validation  performance  (to  two  decimal  places),  reaching  98%  accuracy,  while  the  MG 
same  validation  performance  (to  two  decimal  places),  reaching  98%  accuracy,  while  the  MG 
the MG dataset leads to 
model performed better on the test sets, which means that training with 
model performed better on the test sets, which means that training with the MG dataset leads to 
better ability to generalize to these test sets.
better ability to generalize to these test sets. 

3.3. Figures and Tables 

Table 1. Butterfly datasets 

number of images 

Dataset 

Trainin
g set 

Validat
ion set 

Test set 

I
D 

M
O 
M
G 
M
O 
M
G 
M
O 
M
G 
T
O

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there isn't any explicit mention of specific postprocessing steps taken after the model training. However, we do see some indicators of certain processes such as validation performance evaluation and testing accuracy calculations.

In Table 2, it is shown that the original image model and background-removed model had almost the same validation performance, reaching 98% accuracy. This suggests that some form of metrics calculation was done during or after the training process to evaluate the model's performance.

Furthermore, in section 3.2, it mentions that the MG dataset led to better ability to generalize to the test sets. This implies that testing accuracy was calculated using different test sets to assess how well the trained models could predict unseen data.

However, there is no direct reference to other common postprocessing techniques like saliency maps or confusion matrices. Therefore, based solely on the provided context, it cannot be determined if these methods were employed.
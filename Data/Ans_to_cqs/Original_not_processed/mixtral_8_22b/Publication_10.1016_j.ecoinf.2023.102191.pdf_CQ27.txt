Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

sponses  and  open-ended  queries  to  get  insight  into  the  best  way  the 
monitoring program can bring value to the dive community. Responses 
were compiled  and data was used to calculate the average SUS score 
(Kumar, 2020).

Matej Buzinkai: Conceptualisation, Writing – original draft, Writing 
–  review  &  editing,  Investigation,  Formal  analysis,  Visualisation,  Su-
pervision. Marko Radeta: Conceptualisation, Methodology, Software, 
Data curation,  Resources, Supervision.  Claudio  Rodrigues: Software, 
Data curation. Francisco Silva: Software, Data curation. Ruben Frei-
tas: Software, Data curation. Sahar Chebaane: Methodology, Investi-
gation,  Writing  –  review  &  editing.  Paola  Parretti:  Methodology, 
Investigation, Visualisation, Data curation, Writing – review & editing. 
Susanne  Sch¨afer:  Methodology,  Conceptualisation,  Investigation, 
Writing – review & editing. Rodrigo Silva: Methodology, Investigation. 
Francesca  Gizzi:  Methodology,  Writing  –  review  &  editing.  Silvia 
Almeida: Formal analysis, Resources. Sonia K.M. Gueroun: Method-
ology,  Investigation,  Visualisation,  Writing  –  review  &  editing.  Jo˜ao

curator while using the app as a tool for post-dive debriefing. Combined, 
these two steps ensure that there is a specific checklist to cross out and 
that the interview is embedded in regular operations of the dive centre. 
Another aspect that could influence data quality is the possibility of 
misidentifying  species  that  are  less  known  to  the  citizen  scientists 
(Austen  et  al.,  2016;  Freiwald  et  al.,  2018).  This  is  especially  true 
because the app does not require submission of photographic evidence, 
which  could  be  validated  by  experts  (Kosmala  et  al.,  2016)  or  using 
machine learning (Saoud et  al., 2020). Once again, by relying on the 
dive guide to conduct the interview, there is some level of validation of 
the species sighted and reported, as they are often knowledgeable and 
experienced in local fauna and flora. This approach has also been used in 
previous studies as a safeguard for data quality (Branchini et al., 2015;

Overall  results  (Fig.  1)  show  that  respondents  largely  agree  with 
positive  statements  about  the  monitoring  program  and  the  app  (odd- 
numbered  questions)  and  largely  disagree  with  negative  ones  (even- 
numbered  questions).  The  most  positive  feedback  was  received  on 
questions regarding app simplicity, recognition of the monitored species 
and  the  program  bringing  value  to  the  diving  community.  The  least 
positive feedback was regarding the timing and effort spent on post-dive 
interviews and data collection (Fig. 1). Additionally, we compiled and 
ranked common responses to the final open question regarding how the 
monitoring program could best contribute to the dive centre (Table S5 in 
Supplementary Material). Most commonly mentioned suggestions were: 
to include additional and/or more emblematic species on the taxa list 
(40%); to complement the monitoring program and app with a website

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information on the deployment process of the trained deep learning model. Therefore, it is not possible to determine what process was followed to deploy the trained deep learning model based on the provided context.
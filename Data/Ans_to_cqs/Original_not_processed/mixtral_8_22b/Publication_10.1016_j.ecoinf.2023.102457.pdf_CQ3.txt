Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The second approach we used to extract embeddings was from the 
pre-trained  BirdNET  model.  The  original  version  of  the  model  was

We expected higher performance of the embeddings, but a possible 
limitation to our study may be the use of pre-trained CNNs. This may 
have limited the potential performance of using embedding from neural 
networks.  Interestingly,  higher  classification  accuracy was  seen  when 
using  BirdNET,  a  CNN  trained  on  audio  data  of  bird  vocalizations  in 
comparison  with  VGGish  and  Wav2Vec  2.0,  two  models  trained 
respectively on a vast Youtube audio dataset and a human speech audio 
dataset. This suggests that training a model with data input of gibbon 
vocalizations might result in an even better performance of neural net-
works for individual gibbon discrimination. However, such data training 
requires a large dataset and is very computationally costly, hence why 
we chose to test pre-trained CNNs.

contribute  to  larger  data  input  for  future  training  of  deep  learning 
models (Best  et  al., 2023).  Accordingly, the  rapid growth of  artificial 
intelligence  and  deep  learning  methods  could  bring  about  important 
innovations related to feature extraction and classification with the aim 
of overcoming the current challenges halting the use of PAM for indi-
vidual acoustic discrimination and succeeding in a sustained and non- 
invasive monitoring of animal populations.

Future  studies  that  will  be  particularly  useful  for  accelerating  the 
applicability of PAM as a conservation tool concern identifying the most 
compatible feature extraction techniques and classification methods for 
identifying  individuals  from  PAM  data.  Future  work  that  combines 
acoustic localization and acoustic discrimination of individuals will be 
informative.  Similarly,  more  efforts  should  be  put  towards  creating 
larger  datasets  of  animal  vocalizations  and  towards  building  more 
domain-specific embeddings using deep learning. In a recent study, self- 
supervised auto-encoders were shown to efficiently classify unlabeled 
data on 7 different data sets of vocalizing species and provide results 
closely matching the observations and annotations of bioacoustics ex-
perts.  This  method  will  enable  a  less  laborious  and  more  efficient 
annotation  of  large  data  sets  of  vocalizations,  which  in  turn  will

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention any specific data annotation techniques used in the deep learning pipeline, such as bounding box annotation or instance segmentation. However, it does discuss the use of pre-trained convolutional neural networks (CNNs), including BirdNET, VGGish, and Wav2Vec 2.0, for feature extraction and classification tasks. These models have been trained on various audio datasets, including bird vocalizations, YouTube audio, and human speech audio.

In terms of data annotation, the text briefly mentions that self-supervised auto-encoders have been used to classify unlabeled data on seven different datasets of vocalizing species, providing results closely matching expert annotations. While this technique is not directly comparable to traditional image annotation methods like bounding boxes or instance segmentation, it demonstrates the potential for automated labeling and annotation of large datasets of vocalizations.

Overall, while the text does not focus specifically on data annotation techniques within the deep learning pipeline, it highlights the importance of leveraging pre-trained models and developing domain-specific embeddings for improved feature extraction and classification in the field of passive acoustic monitoring (PAM) for wildlife conservation.
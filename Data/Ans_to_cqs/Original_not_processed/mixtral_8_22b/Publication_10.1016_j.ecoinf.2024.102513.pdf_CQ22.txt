Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.7. Training configuration 

The calculations were performed on an ASUS laptop with an Intel i7- 
11370H CPU (4 cores, 3.30 GHz), 16 GB DDR4 memory, and an NVIDIA 
GeForce RTX 3050 graphics card (4 GB). The dataset for each period was 
divided into training, testing, and validation sets in a ratio of 70:20:10, 
respectively. 

2.8. Classification accuracy 

The accuracy calculation was performed for each model trained on 
the corresponding period and was computed using the sklearn library for 

Table 2 
List of spectral indices used in the study.  

Name 

Normalized Difference Vegetation 

Index (NDVI) 

Normalized Difference Built-up 

Index (NDBI) 

Green-Red Vegetation Index (GRVI) 

Built-up Index (BU) 

Normalized Difference Water Index 

(NDWI) 

Calculation 
Formula 

Nir (cid:0) Red
Nir + Red 
SWIR (cid:0) Nir
SWIR (cid:0) Nir 
Green (cid:0) Red
Green + Red 
NDBI (cid:0) NDVI 

Green (cid:0) Nir
Green + Nir  

Reference 

(Huang et al., 2021) 

(Zha et al., 2003)

Ncorrect
Ntotal

× 100%

(5)  

where Ncorrect  is the number of pixels correctly classified by the model, 
and Ntotal  is the total number of pixels. 

The Kappa Coefficient takes into account the chance agreement be-
tween  classification  and  observations.  It  determines  how  much  the 
classification accuracy exceeds random agreement. Mathematically, the 
Kappa Coefficient is calculated using the formula (Wan et al., 2015): 

Kappa =

Po (cid:0) Pe
1 (cid:0) Pe

(6)  

where Po is the ratio of agreed pixels to the total number of pixels, and Pe 
is the expected probability of agreement, calculated as the product of the 
sum of class proportions by rows and columns. 

2.7. Training configuration

2.6. Model accuracy assessment 

An important stage in the analysis of land cover change on the Kerch 
Peninsula is the evaluation of the accuracy of land use and land cover 
classification. This assessment allows determining the degree of corre-
spondence between the classification results and the actual character-
istics of the area, as well as evaluating the quality of the conducted work. 
For assessing the accuracy of LULC classification on the Kerch Peninsula, 
a standard approach was used, which includes comparing the classified 
data with ground observations. 

The  metrics  of  Overall  Accuracy  and  Kappa  Coefficient  were 

employed for accuracy assessment. 

The  percentage  of  correctness  (Overall  Accuracy)  measures  the 
proportion of pixels correctly classified out of the total number of pixels 
in  the  image.  Mathematically,  it  is  calculated  as  follows  (Story  and 
Congalton, 1986): 

OverallAccuracy =

Ncorrect
Ntotal

× 100%

(5)

of deep learning, a neural network was meticulously trained using these 
datasets.  This  training  process  was  pivotal  in  developing  a  classifier 
capable of accurately identifying various land cover forms based on their 
spectral signatures. 

2.2. Conducting LULC analysis using deep neural network

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Two main metrics are employed to evaluate the performance of the deep learning model used for Land Use/Land Cover (LULC) classification on the Kerch Peninsula: Overall Accuracy and Kappa Coefficient.

The Overall Accuracy metric calculates the proportion of pixels correctly classified out of the total number of pixels in the image. It is mathematically represented as the ratio of the number of correctly classified pixels (Ncorrect) to the total number of pixels (Ntotal) multiplied by 100%.

On the other hand, the Kappa Coefficient considers the chance agreement between classification and observations. It quantifies how much the classification accuracy surpasses random agreement. The Kappa Coefficient is calculated using the formula where Po is the ratio of agreed pixels to the total number of pixels, and Pe is the expected probability of agreement, determined as the product of the sum of class proportions by rows and columns.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

according to a 60%–40% ratio. Table 6 presents the details about the 
total number of samples, the training, and the validation samples. The 
distribution of samples is represented in Fig. 8. It is worthy to note that 
after selecting the samples, we checked all the samples carefully and as 
we see in Fig. 8 (as an example); surprisingly, no mis-collected samples 
were detected in the automatically collected samples.

• Numerical  evaluation:  The  validation  samples  allow  us  to  measure 
numerous metrics such as overall accuracy (OA), kappa coefficient 
(KC), producer’s accuracy (PA), user’s accuracy (UA), commission 
error (CE), and omission error (OE) and to report them in confusion 
matrix format. The above metrics are defined based on the values of 
true positive (TP), true negative (TN), false positive (FP), and false 
negative (FN). Let Table 5 be a sample for the confusion matrix, then 
the OA and KC are defined as Eqs. (26) and (27), respectively. 

(

)

TP + TN
∑

× 100

OA =

r,c

∑
∑

r,c

r,c

KC =

OA (cid:0)
1 (cid:0)

(26)  

(27)   

• Visual assessment: A variety of image subsets are obtained from cor-
responding  GE  images  and  the  final  citrus  map  to  have  a  visual 
evaluation  of  the  classification  process.  The  coordinates  of  the 
cropped subsets are also surveyed in the field operation.

matic  selection  + human  post-revision).  As  citrus  orchards  have  a 
relatively regular geometric shape and spatial patterns, this information 
can be exerted to the classification by the human supervisor as well. A 
few points (about 40 samples) are also collected in field surveying which 
is used for visual assessment of classification performance. Afterward, 
each category of samples (citrus and non-citrus) is randomly split up into 
two groups of training and validation in a 60%–40% ratio, respectively. 
Totally,  we  have  “citrus-training”,  “citrus-validation”,  “non-citrus- 
training”,  and  “non-citrus-validation”  collections.  Of  the  two  defined 
classes, the “citrus” class includes oranges and mandarin orchards (the 
appearance of both is similar). The “non-citrus”  class encompasses all

sample’s class proportions are typical of the landscape’s real class pro-
portions. The analyst must make methodological decisions that present 
tradeoffs  in  data  quality  (i.e.,  class  representativeness)  and  quantity 
while  creating  training  data  for  RS  image  classification.  The  ideal

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information regarding the specific criteria used to determine when training is complete. However, it mentions that the samples are divided into training and validation sets according to a 60%-40% ratio. This suggests that the authors might use some form of cross-validation during their model training process. In machine learning, common stopping criteria include reaching a predefined number of epochs, early stopping when the validation loss stops improving, or achieving a target level of performance on the validation set. Given these possibilities, one could infer that the authors may have used similar criteria to decide when their training was complete. Nevertheless, without more specific information, it remains uncertain what exact criteria were employed.
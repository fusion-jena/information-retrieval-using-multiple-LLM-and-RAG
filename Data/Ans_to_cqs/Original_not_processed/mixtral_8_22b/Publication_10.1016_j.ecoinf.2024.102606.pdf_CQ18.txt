Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4. Training protocols

The models were all trained with the same parameters: 500 epochs, 
an initial learning rate of 0.01 with a weight decay of 0.005, the opti-
mizer Adam was chosen, with a beta1 of 0.937, and data augmentation 
transformations  were  applied  while  training.  Data  augmentation  is  a 
technique used in machine learning to artificially increase the size of a 
dataset  by  creating  new  samples  from  the  existing  ones.  Augmented 
samples are used to train the model more effectively by increasing its 
ability to generalize and its accuracy on the test dataset. The advantage 
of this technique when dealing with a low amount of data such as in our 
case, is the reduced risk of overfitting, since models are exposed to more 
variations, they will tend  to less memorize the dataset. This  becomes 
essential when you have limited data to train models with, which is a 
common problem in deep-learning applications. Another advantage is

Because of the low amount of data at our disposal, cross validation 
was used to ensure the statistical results, and that the model was not 
over-fitting. Over-fitting happens when a model is too complex for the 
amount of data given and loses the ability to learn a solution that gen-
eralizes well because it fits the training data too much, resulting in poor 
performance  on  unseen  data.  The  cross-validation  technique  we  used 
works as follows: K crosses are created by randomly shuffling the dataset 
and dividing it into a training and an evaluation set K times, then for 
each cross, a new model is trained. This allows us to train K different 
models,  each  one  on  a  different  version  of  the  dataset.  Once  all  the 
models are trained, we evaluate them and average all of their perfor-
mances to better understand the model's capability on this dataset. The 
similarity  of  each  model's  evaluation  results  would  indicate  that  the

of  the  model to be  correct when  predicting a  species. The higher  the 
precision for a species, the more confident the model is to be correct 
when  predicting  it.  From  these  two  metrics,  we  calculated  a  more 
advanced  metric,  the  precision-recall  curve,  which  is  a  graph  repre-
senting the balance between the recall and the precision, depending on

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To monitor the model performance during training, the authors employed several strategies due to the limited amount of data available. Firstly, they utilized data augmentation techniques to artificially increase the size of the dataset. By applying various transformations to the existing samples, they aimed to improve the model's ability to generalize and enhance its accuracy on the test dataset. This approach also helped reduce the risk of overfitting, as the models were exposed to more variations, making them less likely to simply memorize the dataset.

Secondly, considering the scarcity of data, the authors adopted cross-validation to validate the statistical results and prevent overfitting. Cross-validation involves splitting the dataset into multiple subsets or 'crosses,' where each subset consists of a training set and an evaluation set. For each cross, a new model is trained, allowing for the creation of multiple models based on different versions of the dataset. After training all the models, their performances are evaluated and averaged to provide a comprehensive understanding of the model's capabilities on the dataset.

Lastly, the authors calculated advanced metrics like precision and recall to assess the model's confidence in predicting specific species. Precision measures the proportion of true positive predictions out of all positive predictions made by the model, while recall indicates the ratio of true positives identified correctly by the model out of all actual positives in the dataset. These metrics help determine the balance between recall and precision, represented through a precision-recall curve.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Model evaluation: testing

Table  1  shows  the  model’s  performance  on  the  test-
ing  images  (see  “Data  preparation”  section).  Model-2, 
with 37M parameters, outperformed Model-1 with 15M 
parameters in terms of the counting prediction, for both 
the  SSL  and  elephants’  data  sets.  Figure  4  shows  the 
actual vs. predicted number of animals. The diagonal red 
line represents the case of no errors (i.e., perfect predic-
tion). The closer the points are to the line, the better the 
prediction.

Fig. 3  Training loss curve: training loss function gradient vs. iteration curve for Basic UNet (Model-1) and UNet with EfficientNet-B5 feature extractor 
architecture (Model-2)

Table 1  Performance comparison of Model-1, Model-2 on test data set

Model

Feature extractor

Model-2

Model-1

Eff.Net-B5

No

SSL

RMSE

1.88

5.57

MAE

1.09

3.54

Elephant

RMSE

0.60

1.01

MAE

0.34

0.53

Parameters

≈37M
≈14M

The  Model-K  architecture  is  a  regression  model  based 
on  VGG16  without  the  feature  extractor  on  top.  The 
output layer was flattened and given as input to 2 fully 
connected  (FC)  layers  with  linear  output.  The  regres-
sion  model  was  designed  to  predict  classwise  (five 
categories)  count.  To  compare  it  with  the  proposed 
solution,  we  modify  the  model  by  connecting  the  out-
put  layer  with  a  fully  connected  one  output  neuron. 
Model-K  was  initialized  with  pre-trained  Imagenet 
weights  and  then  trained  using  our  training  data  set 
with  a  Stochastic  Gradient  Descent  (SGD)  optimizer 
and  an  MSE  loss  function.  The  proposed  Model-2 
with  EfficientNet  feature  extractor  reached  an  RMSE 
value  of  1.88  and  0.60  for  the  SLL  and  elephants’  data 
sets,  respectively,  performing  better  than  the  Model-K 
with  an  RMSE  of  2.17  and  0.81  for  SSL  and  elephants’

zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.

Loss function

In  accordance  with  the  performance  metrics  used  in 
similar  previous  studies,  the  Root  Mean  Square  Error 
ˆD ) and 
(RMSE) loss between the predicted density map ( 
the true density map (D) is employed in this paper. RMSE 
is  the  square  root  of  the  average  of  squared  differences 
between predicted and actual count, defined as

L = (cid:31)
(cid:30)
(cid:30)
(cid:29)

1

N

N

(cid:28)
n=1

( ˆD − D)2

Counting from density map

Dx =

N (p,

)

(cid:30)

(cid:31)
p∈Ax

(2)

(cid:31)

(cid:31)=

 .  Covariance  is  modeled  as

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the provided context, there isn't explicit information about the specific criteria used to determine when training is complete. However, common practices in machine learning include monitoring the validation loss during training and stopping the process once the validation loss reaches a plateau or starts increasing. This indicates that the model has learned all it can from the training data and further training may lead to overfitting. In some cases, early stopping is also implemented, where training stops if the validation loss does not improve after a certain number of epochs. These methods help prevent overfitting and ensure optimal generalization performance on unseen data.
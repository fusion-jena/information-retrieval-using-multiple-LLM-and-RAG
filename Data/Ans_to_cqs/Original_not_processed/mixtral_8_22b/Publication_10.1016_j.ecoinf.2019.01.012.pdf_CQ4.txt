Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

layers, pooling layers and fully connected layers (Voulodimos et al.,
2018). The ﬁrst two components are responsible for automatic feature
extraction by applying a large number of diﬀerent ﬁlters on the input
data. This process of feature extraction is performed on multiple levels,
whereby the output of each level is the input to the following. From
level to level, the extracted features increase in complexity - from rather
simple features (e.g. edges) on the lowest level to more complex fea-
tures on the highest level (Gu et al., 2018). By passing large quantities
of labeled training data through the network, the model successively
learns to recognize the relevant features, which are necessary to dis-
tinguish between classes.

Machine learning, and particularly deep learning, has evolved into
the most commonly used approaches for the automatic classiﬁcation of
digital images (LeCun et al., 2015). The major advantage of deep
learning is that the time-consuming and complex step of previous fea-
ture extraction becomes unnecessary. Instead, the model learns and
extracts the relevant features itself during the training process. The
major drawback of deep learning is that large amounts of labeled
training data are required (Kamilaris and Prenafeta-Boldú, 2018).
Among the deep learning architectures, convolutional neural networks
(CNN) are particularly suitable for image analysis due to their ability to
extract spatial features. CNNs have proven to be quite powerful in
performing diﬀerent tasks such as object detection (Everingham et al.,
2010; Tompson et al., 2014), classiﬁcation (Traore et al., 2018, Xu
et al., 2017, Amara et al., 2017, Lu et al., 2017, Han et al., 2018) and

The second set of photographs was used for testing the model per-
formance on new and more realistic data. When whole images needs to
be classiﬁed with CNN, there arise two problems. First, classical CNNs
assign only a single category on the input data and do not provide a
classiﬁcation with distinct boundaries. Second, the CNN takes only a

Fig. 3. Examples of the 50 × 50 pixel sample tiles for training. Woody vege-
tation is used as positive class (a), the negative class (b) contains grass, open
soil, buildings, human, animals, sky, water, stone and asphalt. Note that many
tiles of grassy and herbaceous plants look very similar to tiles of woody plants.

ﬁxed input size. To overcome these limitations, we split each photo-
graph into 50 × 50 pixel tiles to match the size of the training samples.
Before splitting, all photographs were resized to a maximum width of
2500 pixels to reduce computing time.

2.4. Convolutional neural network setup and training

CNNs are composed of three main components: convolutional

223

U. Bayr and O. Puschmann

Ecological Informatics 50 (2019) 220–233

Fig. 4. Illustration of the CNN architecture and the classiﬁcation process. The network was trained with manually labeled samples with a tile size of 50 × 50 pixel.
The trained classiﬁer was then applied on whole repeat photographs, which were also split into 50 × 50 pixel tiles. The classiﬁer predicts the output for each single
tile and reassembles them to the original image size.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, it does not explicitly mention any specific data augmentation techniques applied in the deep learning pipeline, such as flipping, rotating, or scaling. However, it discusses the use of convolutional neural networks (CNNs) for image classification tasks, specifically focusing on the classification of digital images using machine learning and deep learning methods.

In general, data augmentation techniques like flipping, rotation, and scaling can help improve the performance of deep learning models by increasing the amount of available training data and introducing variations in the dataset. These techniques can be especially useful when working with limited datasets or when dealing with imbalanced classes.

While the context does not directly address the application of data augmentation techniques, it mentions that the model requires large amounts of labeled training data. Therefore, one could infer that data augmentation might have been employed to enhance the training dataset and improve the overall performance of the deep learning model. Nonetheless, without explicit information regarding the use of specific data augmentation techniques, this remains speculative.
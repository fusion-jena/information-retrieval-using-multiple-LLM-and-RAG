Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Detection rates in the laboratory or at test data sets
of more than 90% can generally not be achieved in the
field under real, constantly changing conditions. For
this reason, a feedback mechanism is provided, which
involves the user in the optimization of the system, but
at  the  same  time  minimizes  the  labeling  effort  to  be
made  manually.  This  is  known  as  active  learning.  In
the actual application, the system will, at certain inter-
vals, forward images to humans, and after the annota-
tion these images will be used to train a classifier iter-
atively. Evaluation is carried out with a retained test set
of images annotated manually.

Monitoring  devices  will  likely  also  detect  species
that are unknown for the recognition system. There-
fore, novelty detection is relevant [2, 3] which allows

AP50 and AP75 denote the average precision at an inter-
section  over  union  (IoU)  threshold  of  50  and  75%,
respectively. AP denotes the average precision over IoU
thresholds from 50 to 95% in 5% increments. The out-
lined metrics are summarized for bounding box detec-
tion and segmentation in Table 2.

AUTOMATIZED LONG-TERM 
WILDLIFE  CLASSIFICATION

Since  the  AMMOD  stations  should  operate  for  a
longer period of time (several years), it is important to
use recognition systems that can improve their perfor-
mance  over  time  by  exploiting  data  that  is  recorded
during  operation.  Hence,  we  aim  at  applying  and
improving lifelong learning algorithms for the identifi-
cation  of  wildlife  animal  species  that  occur  in  the
images from the camera traps. Lifelong learning, often
also called continuous learning, continual learning, or
incremental  learning,  denotes  a  process  that  enables
updates of the classification models once new data is

20. D. Korsch and J. Denzler, “In defense of active part se-
lection for fine-grained classification,” Pattern Recog-
nit. Image Anal. 28 (4), 658–663 (2018).

21. D. Korsch, P. Bodesheim, and J. Denzler, “Classifica-
tion-specific  parts  for  improving  fine-grained  visual
categorization,” in German Conference on Pattern Rec-
ognition (GCPR) (2019), pp. 62–75.

22. A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Ima-
geNet  classification  with  deep  convolutional  neural
networks,” in Advances in Neural Information Processing
Systems (NIPS) (2012), pp. 1097–1105.

23. T. Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan,
and S. Belongie, “Feature pyramid networks for object
detection,”  in  Proceedings  of  the  IEEE  Conference  on
Computer  Vision  and  Pattern  Recognition  (2017),
pp. 2117–2125.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention any specific criteria for determining when training is complete. However, based on common practices in machine learning, some possible criteria could include reaching a predefined number of epochs, achieving a desired level of performance on a validation dataset, observing no further improvement in validation accuracy or loss, or meeting other custom stopping rules defined by the researchers. It would be best to consult the original research paper or documentation for precise details regarding the completion of training.
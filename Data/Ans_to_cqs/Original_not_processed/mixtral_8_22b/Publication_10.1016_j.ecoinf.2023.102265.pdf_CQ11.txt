Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

ResNet-50 is made up of five convolutional blocks stacked on top of 
one  another  (Fig.  5).  The  image  features  are  extracted  from  the  fully 
connected layers pre-trained with the ImageNet database (https://imag 
e-net.org/index.php). The input to the network is an image of 224x224 

EcologicalInformatics77(2023)1022654S. Singha Roy et al.

EcologicalInformatics77(2023)1022657S. Singha Roy et al.                                                                                                                                                                                                                            

Fig. 7. Visualisation of extracted features using a pre-trained ResNet-50 CNN model showing that the background microhabitat classes are well structured. Green 
triangles represent ‘natural’ microhabitats and the light brown circles represent ‘anthropogenic’ mirohabitats. (For interpretation of the references to colour in this 
figure legend, the reader is referred to the web version of this article.) 

Table 1 
Comparison  of  training  time  for  pre-trained  ResNet-18/50/101  models  for 
extracting features of backgrounds of 500 honey bee images.   

ResNet-18 

ResNet-50 

ResNet-101 

Training time 

1.25 s 

1.2 s 

1.22 s

easy to train with reduced complexity, even though they have deeper 
layers than CNN models, because of the presence of skip connections 
between the input and output of each block (He et al., 2016). ResNet has 
different variants with a variety of convolution layers. We trialled three 
depths, ResNet-18, (cid:0) 50 and (cid:0) 101, and compared their training times 
and validation accuracies. They behaved similarly but ResNet-50 had a 
marginally  higher  validation  accuracy  (Section  3.1.1)  leading  us  to 
choose it for feature extraction.

(<i>eristalis tenax</i>) as a crop pollinator in New Zealand. NZ Plant Protect., 72: 
221–230. URL https://nzpps.org/_journal/index.php/nzpp/article/view/304. 
Jena, B. et al., 2021. Artificial intelligence-based hybrid deep learning models for image 
classification: The first narrative review. Comput. Biol. Med., 137: 104803. ISSN 
0010–4825. https://www.sciencedirect.com/science/article/pii/ 
S0010482521005977. 

Joly, A., et al., 2019. Overview of lifeclef 2019: Identification of amazonian plants, south 

& north american birds, and niche prediction. In: Experimental IR Meets 
Multilinguality, Multimodality, and Interaction. Springer International Publishing, 
Cham, ISBN 978-3-030-28577-7, pp. 387–401. https://doi.org/10.1007/978-3-030- 
28577-7_29. 

Jones, L.E., Leather, R.S., 2012. Invertebrates in urban areas: a review. EJE. ISSN:

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information about the data repository link of the deep learning pipeline. Therefore, I cannot provide an accurate answer based on the available context.
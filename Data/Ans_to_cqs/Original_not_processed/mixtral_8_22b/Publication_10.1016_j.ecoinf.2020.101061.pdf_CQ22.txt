Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Using the evaluation plots, we chose two metrics to assess model
performance. For comparison with the existing LiDAR-only im-
plementations, we used precision and recall statistics with a bounding
box marked as true positive if it had an intersection-over-union (IoU) of
greater than 0.5. Intersection-over-union is the ratio of the area of
bounding box overlap to the area of bounding box union between the
predicted tree crown and the visually annotated crowns in the eva-
luation data. For each bounding box prediction, the deep learning
model reports a conﬁdence score between 0 and 1. To transform these
scores into precision and recall statistics, we need to deﬁne a threshold
of box scores to accept. As we lower the threshold for acceptance, a
greater number of trees will be captured, but at the expense of de-
creased precision. To highlight this relationship, we showed the per-
formance of the deep learning approach across all bounding box

probability thresholds between 0 and 1 with an interval of 0.1. IoU
precision and recall are reported separately and do not capture diﬀer-
ences in bounding box conﬁdence scores. When comparing the diﬀerent
generalization approaches, it is useful to have a single metric to com-
pare. We used the Average Precision (AP) metric commonly used for
object detection tasks in computer vision, which is the area under the
precision-recall curve computed at the 11 ﬁxed 0.1 intervals between 0
and 1 (Lin et al., 2017).

Collecting a suﬃcient number of training samples will often be a
bottleneck in developing supervised methods in airborne imagery. It is
therefore useful to test the number of local training samples needed to
achieve maximum performance. We performed a sensitivity study by
training models using diﬀerent proportions of training data. We se-
lected 5%, 25%, 50% and 75% of the total hand-annotations to com-
pare to the full dataset for the within-site results for each site. We reran
this experiment ﬁve times to account for the random subsampling of
annotations. In addition, we ran the evaluation plots for the pretraining
model only (i.e. 0% hand-annotated data) to assess whether the addi-
tion of hand-annotated data improved the within-site pretraining.

3. Results

2.3. Model evaluation

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The performance of the deep learning model was evaluated using precision and recall statistics. These were calculated based on the intersection-over-union (IoU), which measures the degree of overlap between the predicted tree crowns and the manually annotated ones. Each bounding box prediction came with a confidence score between 0 and 1. A threshold was set to determine which predictions to accept, balancing the trade-off between capturing more trees and maintaining high precision. The average precision (AP) metric was also used, which is the area under the precision-recall curve computed at fixed intervals from 0 to 1. This allowed for a single metric to compare different generalization approaches.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics81(2024)1025817S. Ghosh et al.                                                                                                                                                                                                                                   

Table 4 
Search space of CNN hyperparameters to be explored.  

Hyperparameters 

Range 

Convolution Layers 
Filters 
Filter Size 
Number of Neuron 
Batch Size 
Epochs 
CNN Model Optimizer 

Lower limit = 1 and Upper limit = 10 
Lower limit = 1 and Upper limit = 64 
Lower limit = 1 and Upper limit = 10 
Lower limit = 32 and Upper limit = 1024 
Lower limit = 8 and Upper limit = 512 
Lower limit = 1 and Upper limit = 25 
ADAM, SGD, RMSProp, Adadelta, Adagrad, Adamax

hyperparameters.  The  hyperparameters  with  the  current  results  are 
saved until better accuracy is achieved. The termination criterion of the 
proposed approach is the maximum number of iterations to be executed. 
After the completion of the iterations, the proposed approach provides 
us with the best hyperparameters, thus evolving the near-optimal CNN. 
The  overall  complexity  is  dominated  by  the  iterations,  “TC”,  and 
within  each  iteration,  the  operations  depend  on  the  population  size 
“NP”.  Therefore,  the  total  complexity  can  be  approximated  as 
O(TC*NP).

sample  the  feature  maps  to  reduce  dimensionality.  By  leveraging  the 
hierarchical  representations  learned  through  these  layers,  CNNs  can 
automatically  detect  and  recognize  complex  patterns  and  objects  in 
images. The training process of a CNN involves optimizing the network 
parameters using large-scale labeled training data through techniques 
such as backpropagation and gradient descent (Chouhan et al., 2020). 
Understanding the significant parameters related to CNNs is essential 
because they can drastically affect model performance. The convolution 
operation is meant to apply filters to the input image. It allows CNNs to 
extract local features and capture spatial relationships between pixels. 
The convolution operation is calculated with the help of eq. (3):

ResNet50 
VGG16 
InceptionV3 
MobileNetV2 
DenseNet121 
NASNetMobile 
Xception 
EfficientNetB0 
ResNeXt50 
InceptionResNetV2 
Proposed Approach 

23,534,592 
14,714,688 
23,583,872 
3,538,304 
8,062,464 
4,939,648 
22,910,480 
5,330,571 
25,028,032 
55,873,736 
2,86,348 

2,56,000 
2,257,984 
14,714,688 
2,257,088 
21,802,784 
2,727,160 
2,283,084 
404,956 
2,48,523 
2,85,245 
640  

Table 9 
Performance  comparison  of  the  proposed  HPB3C-3PGA-based  approach  with 
existing transfer learning approaches.  

Model 

Dataset 

Accuracy 

Precision 

Recall 

VGG16 

Inception V3 

MobileNet V2 

ResNet 50 

DenseNet 121 

Xception 

PCA Based VGG16 

Proposed 

Approach 

Mendeley 
CVIP100 
Mendeley 
CVIP100 
Mendeley 
CVIP100 
Mendeley 
CVIP100 
Mendeley 
CVIP100 
Mendeley 
CVIP100 
Mendeley 
CVIP100 
Mendeley 
CVIP100 

76.00% 
78.80% 
82.50% 
85.20% 
87.92% 
90.52% 
88.00% 
92.80% 
88.00% 
92.72% 
88.26% 
90.56% 
95.25% 
96.28% 
98.96% 
98.97%

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps may be required depending on the nature of the dataset. These steps aim to improve the quality of the data and enhance the model's ability to learn from it effectively. Some common preprocessing steps include:

1. Data Cleaning: This step involves removing or correcting any inconsistencies, errors, or missing values within the dataset. This ensures that the model trains on accurate and reliable information.

2. Normalization/Standardization: In this step, the range of independent variables is rescaled so that all variables have similar ranges. Common methods for normalization include Min-Max Scaling, which scales the data to lie between 0 and 1, and Z-score standardization, where the mean is subtracted from each value and then divided by the standard deviation.

3. Feature Selection/Extraction: This step aims to identify the most relevant features or attributes that contribute significantly to the target variable. Techniques such as Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), or manual selection based on domain knowledge can be used for feature selection or extraction.

4. Data Augmentation: To increase the amount of available training data, synthetic samples can be generated by applying transformations like rotation, flipping, zooming, cropping, etc., especially when working with images.

5. Encoding Categorical Variables: For categorical variables, one-hot encoding or label encoding can be applied to convert them into numerical format suitable for machine learning algorithms.

These preprocessing steps ensure that the deep learning models receive high-quality inputs, leading to improved performance and generalizability. However, not all datasets require every single preprocessing technique mentioned above; the choice depends on the specific characteristics and requirements of the given problem.
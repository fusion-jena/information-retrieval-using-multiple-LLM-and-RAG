Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

we selected the set of hyperparameter values that resulted in the mini-
mum  MAE  and  RMSE,  as  well  as  the  highest  R2,  for  each  machine 
learning model. Table 1 presents the resulting hyperparameter values 
for the four machine learning models.

training and test data set to evaluate the performance of the machine 
learning algorithms. Of the four machine learning algorithms, RF (S-2 
MSI: R2 = 0.61, MAE% = 6.56, RMSE = 12.51 μg/L, L-8 OLI: R2 = 0.56, 
MAE = 8.44%, RMSE = 16.01 μg/L) yielded the most accurate CHL-a 
predictions from the Sentinel-2 MSI and Landsat-8 OLI data in the test 
set, outperforming the KNN (S-2 MSI: R2 = 0.60, MAE = 6.49%, RMSE =
12.64 μg/L, L-8 OLI: R2  = 0.55, MAE = 8.24%, RMSE = 16.22 μg/L), 
AdaBoost (S-2 MSI: R2 = 0.58, MAE = 7.39%, RMSE = 12.97 μg/L, L-8 
OLI: R2 = 0.44, MAE = 9.81%, RMSE = 18.11 μg/L), and ANN (S-2 MSI: 
R2 = 0.59, MAE = 7.31%, RMSE = 12.76 μg/L, L-8 OLI: R2 = 0.14, MAE 
= 11.27%, RMSE = 22.43 μg/L) models (Fig. 4). Furthermore, the RF 
algorithm also exhibited better SD predictions (S-2 MSI: R2 = 0.45, MAE 
= 0.76%, RMSE = 1.03 m, L-8 OLI: R2 = 0.35, MAE = 0.77%, RMSE =
1.09 m) and TSS (S-2 MSI: R2 = 0.46, MAE = 2.95%, RMSE = 5.61 mg/L,

2.4.4. Artificial neural network 

2.5.2. MAE 

MAE  is  a  popular  statistic  accuracy  metric  that  can  measure  the 
discrepancy  between  a  model’s  predicted  and  observed  values.  The 
formula for calculating MAE is as follows: MAE = (1/n) * Σ|yᵢ - ˆyᵢ|. MAE 
is  favored  over  mean  squared  error  (MSE)  when  outliers  in  the  data 
significantly affect model performance. MAE is less sensitive to outliers 
and gives a more reliable estimate of mode accuracy than the MSE (Li 
et al., 2021b). 

2.5.3. RMSE 

RMSE is a popular statistic used in machine learning to compare a 
model’s predictions to the actual values. RMSE is the average squared 
deviation from the expected value. The formula for calculating RMSE is 
as follows: RMSE = sqrt((1/n) * Σ(yᵢ - ˆyᵢ)2). Models with higher R2 values 
and lower RMSE and MAE values are considered more appropriate for 
estimating water quality parameters (Kuhn et al., 2019; Ma et al., 2021; 
Song et al., 2020). 

3. Results

3.3. Hyperparameter tuning and performance evaluation of machine 
learning models 

We developed machine learning models to predict CHL-a, SD, and 
TSS using surface reflectance values from Sentinel-2 MSI and Landsat-8 
OLI  imagery.  To  optimize  the  performance  of  these  models,  we  con-
ducted hyperparameter tuning. The optimal hyperparameter values for 
each machine learning model were determined using a randomized grid 
search  (Bergstra and  Bengio, 2012; Lee et  al., 2022). It is  a  powerful 
optimization  technique  that  differs  from  traditional  grid  search  by 

EcologicalInformatics81(2024)1026085M. Mamun et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The study uses three main metrics to evaluate the performance of the machine learning models: Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and coefficient of determination (R^2). These metrics help assess the discrepancies between the model's predicted and observed values, with lower MAE and RMSE values indicating better performance. Additionally, higher R^2 values suggest a stronger correlation between the predicted and observed values. In this case, Random Forest (RF) was found to be the best performing algorithm based on its superior results across all three metrics compared to other models such as KNN, AdaBoost, and ANN.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

hyperparameter

In the proposed model,

the optimal model hyper-
parameters are obtained utilizing a distributed asynchro-
[47].
nous
+erefore, for parameter ﬁnding and optimization, we used
the Tree Parzen Estimator (TPE) [47] method in the
Hyperopt package (available at http://hyperopt.github.io/
hyperopt/). Table 1 shows
the proposed GRU-DNN
model hyperparameters and their search spaces for deter-
mining the best model hyperparameter values.

approach

5.5. GRU-DNN Hyperparameter Analysis. GRU-DNN
model is trained in a supervised learning fashion using lag
features (i.e., using K previous observations), where K
denotes the number of previous observations used in the
training and forecasting task. Typically, K is considered
as a hyperparameter that needs to be optimized. +ere-
fore, we performed a grid search method to obtain the
optimal K value. Figure 6 depicts the grid search for
diﬀerent values of K hyperparameter over search space
ranges from 1 to 15. Speciﬁcally, Figure 6(a) presents the
model performance for water temperature forecasting
using various K values, where K � 6 achieves the lowest
MAE error. Similarly, K � 4 is the optimal value for
signiﬁcant wave height
shown in
Figure 6(b). It is noteworthy that the experiments pre-
sented in Figure 7 are for the ﬁrst dataset of each fore-
casting problem, assuming that the rest of the datasets
have similar behaviour.

forecasting

as

the optimal conﬁguration of hyperparameter values for a
predictive model has a direct eﬀect on the models’ per-
formance and the tested dataset. While hyperparameter
tuning is a crucial step in the model training process to
ensure a successful ML application, it is a compute-in-
tensive procedure. +is is because of the large number of
possible combinations to test and the computational re-
sources required [18, 19].

4.2. Machine Learning Models. One of the main obstacles
for designing machine learning models is tuning the
hyperparameters of the model. +is is because diﬀerent
hyperparameter values can lead to diﬀerent accuracy levels.
In the proposed system, four machine learning models are
employed, which are Linear Regression, Support Vector
Regression (SVR), Decision Tree (DT), and Random Forest
(RF). Each learning model is subjected to a grid search in
order to achieve the optimal parameter tuning. +e
hyperparameters are essential for being one of the primary
sources that inﬂuence the behaviour of a machine learning
model
in general. Hence, determining the optimal
hyperparameters combination is a critical goal which re-
duces a predeﬁned loss function and produces better
outcomes. For instance, the degree, kernel, epsilon, and
gamma are all adjusted in SVR to reach the highest ac-
curacy. In RF, however, grid search is applied to determine

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide specific information about the hyperparameters used in the deep learning model such as learning rate or optimizer. However, it mentions that the optimal model hyperparameters were found using the Tree Parzen Estimator (TPE) method in the Hyperopt package. Additionally, it states that the GRU-DNN model was trained with lag features using K previous observations, where K is considered a hyperparameter that needs to be optimized. A grid search method was employed to find the optimal K value. Therefore, while the exact hyperparameters used in the deep learning model are not specified, it is clear that hyperparameter optimization played a significant role in developing the model.
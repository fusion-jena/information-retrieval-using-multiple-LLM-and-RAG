Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The baseline model comprised a simple CNN architecture that had 
two  convolutional layers (8  filters of  size 16 × 16,  with ReLU activa-
tions), followed by max pooling (4 × 4), a flattening operation, and two 
fully-connected layers (32 ReLU and 22 softmax units respectively) (see 
Fig. 2). Given only a spectrogram input, the baseline model produced a 
probability for each of the 22 species. 

3.3.2. Case II: One-hot encoding 

Different methods were explored and tested to enhance the neural 
network with contextual information. One straightforward approach to 
incorporate additional information into a deep learning classifier, while 
maintaining  model  simplicity,  is  to  utilize  a  multi-branch  CNN.  As  a 
result, we investigate this approach as the initial method to integrate

The objective of this study was to assess whether deep learning al-
gorithms  can  utilize  contextual  information  for  improved  animal  call 
detection  and  classification. The investigation of  contextual metadata 
incorporation  in  deep  learning  classifiers  for  bioacoustics  has  been 
limited  in  prior  literature,  despite  the  potential  for  significant 
improvement. Our study demonstrated that techniques commonly used 
in  image  processing  can  be  adapted  and  effectively  applied  in 
bioacoustics for this purpose. Based on a dataset built specifically for this 
purpose,  we  found  that  augmenting  spatial  information  with  spectro-
grams  improved  the  accuracy  of  bird  song  classification,  with  higher 
sensitivity  and  precision  indices  than  the  baseline  model  (Table  2). 
Specifically, we achieved a significant improvement in the F1-score of 
bird song classification of 22 species from 61.02% (baseline) to 87.78%

The predominant paradigm for enhancing deep learning applications 
in bioacoustics has centered on refining neural network architectures, in 
conjunction  with  optimizing  pre-  and  post-processing  methodologies 
but few have considered adding contextual data. Despite the availability 
of metadata in BirdCLEF, utilization of this contextual information to 
enhance  bird  sound  classification  has  been  infrequently  used  by 
competing teams (Joly et al., 2021; Joly et al., 2022). However, it stands 
to reason that an experienced human practitioner in sound recognition 

Fig.  1. Example  of  spectrograms  recorded  for  two  species,  European  serin 
(Serinus 
in 
different countries. 

(Serinus  canicollis),  occurring 

serinus)  and  Cape  Canary

He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition. In: 
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). doi: 
10.1109/CVPR.2016.90. 

Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A.R., Jaitly, N., Kingsbury, B., 2012. 
Deep neural networks for acoustic modeling in speech recognition: The shared views 
of four research groups. IEEE Signal Process. Mag. 29 (6), 82–97. https://doi.org/ 
10.1109/MSP.2012.2205597. 

Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Sifre, L., 
2022. An empirical analysis of compute-optimal large language model training. In: 
Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (Eds.), Advances 
in neural information processing systems, vol. 35. Curran Associates, Inc., 
pp. 30016–30030 

Hu, Y., Cardoso, G.C., 2009. Are bird species that vocalize at higher frequencies

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, the primary data format used in the deep learning pipeline is audio, specifically bird songs or calls. These sounds are converted into spectrograms, which are visual representations of the spectrum of frequencies of sound as they vary with time. Spectrograms are essentially images, so one could argue that both audio and image data formats are utilized in this pipeline. There is no mention of using other data formats such as video, CSV, or others.
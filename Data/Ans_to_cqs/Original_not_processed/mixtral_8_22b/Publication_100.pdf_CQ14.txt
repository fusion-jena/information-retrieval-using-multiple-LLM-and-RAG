Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.5. Implementation details

We have implemented our models in PyTorch (Paszke et al., 2017).
We trained ğ‘€ = 5 models with batch size ğµ = 64 and a base learning
rate ğ›¼ = 10âˆ’4. The learning rate is automatically reduced by a factor
of 0.1 when the validation loss has not improved for 15 consecutive
epochs. We apply weight decay to control the strength of the unit
Gaussian prior, with an empirically chosen magnitude of 10âˆ’3 that
is inversely proportional to the hyperparameter ğœ† from Eq. (1). We
chose ğ›½1 = 0.9, ğ›½2 = 0.999 and ğœ– = 10âˆ’8 as hyper-parameters for the
Adam optimizer. Each neural network was trained on a single Nvidia
RTX2080Ti GPU for âˆ¼14 days.

5. Experimental results and discussion

In addition, we conduct a residual structure analysis of each variable
with respect to all others, i.e., we investigate how the residuals in one
variable are distributed across other variablesâ€™ value ranges. Note that
to get more stable estimates, we remove data points exceeding the 99th
percentile of the respective variable before binning. For this analysis,
we used ğ‘ = 10 bins. In Fig. 7, the column indicates the query variable,
whose ground truth values (clipped to the 99th percentile for stability)
are discretized into ğ‘ bins and define the ordering along the ğ‘¥-axis.
The row indicates the target variable whose residual distribution is
plotted (so, for instance, the fourth graph in the top row are the P95
residuals ordered by the corresponding pixelsâ€™ Gini coefficient). The
solid green line denotes the mean residual, the shaded area the resid-
ualsâ€™ standard deviation. Mean residuals close to zero (gray line) mean

training and testing the model is provided, too.

Fig. 1. High-level overview of our method. Top panel: During training, the parameters of a neural network model are optimized to reduce the deviation between predicted and
ALS-derived forest structure variables. Bottom panel: During inference, an ensemble of multiple neural networks predicts a distribution over forest structure variables, given only
optical and SAR satellite images with 10 m GSD as input. Satellite icons from FU Berlin (2019).

revisit time of ALS campaigns and is a bottleneck for country-scale
applications.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Answer: The text does not provide explicit information about the specific optimization technique used for selecting the hyperparameters of the model. However, it mentions that the learning rate is automatically reduced by a factor of 0.1 when the validation loss has not improved for 15 consecutive epochs. This suggests that some form of early stopping or learning rate scheduling might be employed during the training process. Additionally, the Adam optimizer is used with specific hyperparameters (Î²1=0.9, Î²2=0.999, and Ïµ=10^-8), but no further details regarding their selection are mentioned. Therefore, based on the available information, it cannot be determined whether techniques such as grid search or random search were utilized for hyperparameter optimization.
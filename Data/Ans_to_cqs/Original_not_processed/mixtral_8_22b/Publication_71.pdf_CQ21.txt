Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1. Training parameters used in DeepLabV3+.

Training Parameter

Value

Learning policy
Base learning rate
Learning rate decay factor
Learning rate decay step
Learning power
Training number of steps
Momentum
Train batch size
Weight decay
Train crop size
Last layer gradient multiplier
Upsample logits
Drop path keep prob
tf_initial_checkpoint
initialize_last_layer
last_layers_contain_logits_only
slow_start_step
slow_start_learning_rate
ﬁne_tune_batch_norm
min_scale_factor
max_scale_factor
scale_factor_step_size
atrous_rates
output_stride

Poly
0.0001
0.1
2000
0.9
≥100,000
0.9
2
0.00004
‘513,513’
1
True
1
deeplabv3_pascal_train_aug
False
True
0
1 × 10
False
0.5
2
0.25
[6,12,18]
16

−4

work can be seen in Table 1. Table 1. Training parameters used in DeepLabV3+. Training Parameter Value Learning policy Poly Base learning rate 0.0001 Learning rate decay factor 0.1 Learning rate decay step 2000 Learning power 0.9 Training number of steps ≥100,000 Momentum 0.9 Train batch size 2 Weight decay 0.00004 Train crop size ‘513,513′ Last layer gradient multiplier 1 Upsample logits True Drop path keep prob 1 tf_initial_checkpoint deeplabv3_pascal_train_aug initialize_last_layer False last_layers_contain_logits_only True slow_start_step 0 slow_start_learning_rate 1e-4 fine_tune_batch_norm False min_scale_factor 0.5 max_scale_factor 2 scale_factor_step_size 0.25 atrous_rates [6,12,18] output_stride 16   Remote Sens. 2020, 12, 2502

2.1.1. Vasiliko Site Data Used for Training

the classification accuracy. Due to memory issues, we could not feed the whole Vasiliko image for training. Instead, we had to divide the training image into four quadrants. The performance metrics were generated by sequentially training the model on each of the four quadrants of the Vasiliko dataset. In order to do this, we trained an initial model on the first quadrant and then updated the weights using the following quadrant. This step was repeated for the final three quadrants of the image. For training the model sequentially, we used a patch size of 7, the learning rate of 0.01, and used all the possible training samples in the quadrant. Table 8 summarizes the results for our sequential approach. We only generated results using the Kimisala-20 test image because Kimisala-10 is very large in size. The average overall accuracy for the CNN model reached 0.8298, which was better than the earlier results in Table 7 in which only 30,000 samples were used to train Remote Sens. 2020,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training process, there are several postprocessing steps that can be performed depending on the specific requirements or goals of the project. Some common postprocessing steps include:

1. Model evaluation: Evaluating the performance of the trained model using various metrics such as accuracy, precision, recall, F1 score, etc. This helps to determine how well the model has learned from the training data and its ability to generalize to new unseen data.
2. Visualization: Creating visualizations such as saliency maps, activation maps, or heatmaps to understand what parts of the input data are most important for making predictions. These visualizations can help identify any biases or errors in the model's decision-making process.
3. Confusion matrix: Generating a confusion matrix to evaluate the performance of the classifier. A confusion matrix shows the number of correct and incorrect predictions made by the model for each class. It provides insights into the types of errors made by the model and can help improve its performance.
4. Ensemble methods: Combining multiple models to improve the overall performance of the system. Ensemble methods involve training multiple models with different architectures or hyperparameters and combining their outputs to produce a more accurate prediction.
5. Transfer learning: Using pretrained models to extract features from new datasets. Transfer learning involves taking a pretrained model and fine-tuning it on a smaller dataset to adapt it to a new task. This can save time and resources compared to training a new model from scratch.
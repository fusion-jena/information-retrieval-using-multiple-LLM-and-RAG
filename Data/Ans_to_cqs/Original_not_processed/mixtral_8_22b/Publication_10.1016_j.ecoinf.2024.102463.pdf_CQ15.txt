Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The key idea behind Hyperband is to explore several configurations 
in parallel while allocating more training resources to promising con-
figurations. This enables a more efficient search of the hyperparameter 
space, particularly when computational resources are limited. In sum-
mary, Hyperband combines an initial random search with a ‘successive 
halving’  strategy  to  quickly  identify  promising  configurations  while 
eliminating those that show inferior performance. 

The  detail  of  the  grid  of  possible  combinations  is  presented.  The 
number of convolution layers is lower than that given in the article, the 
network has two convolution layers. Their size and the associated max- 
pooling will be determined in the following interval for each:

that  do  not  contribute  significantly  to  the  final  prediction,  which 
facilitates training by allowing  gradients to propagate more easily 
through the layers. More specifically, the RHN uses gates to control 
the flow of data between the layers of the network. These gates are 
non-linear functions that take as inputs the outputs of the previous 
layer and the outputs of the current layer, and determine how much 
data should be passed to the next layer. By using these gates, the RHN 
can  learn  to  “ignore”  some  layers  that  are  not  useful  for  the  final 
prediction. The transformation gate acts as a selection and control of 
information from history, and a transport gate can transport infor-
mation  between  hidden  states  without  any  activation  function 
(Schmidhuber, 1992).  
• Attention mechanism:

• CNN  window  size  (dim-filter-cnn)  ∈ [3, 4, 5, 6, 7].  The  convolution 
window size determines the size of the region over which convolu-
tion  is  applied  at  each  time  step.  It  is  important  to  choose  an 
appropriate window size to capture the relevant temporal patterns in 
the data.  

• the  number  of  filters  (nbr-filters-cnn)  ∈ [8, 16, 24, 32, 40, 48, 56, 64,
72, 80, 88, 96, 104, 112, 120, 128].The  number  of  filters  determines 
how many different patterns the network can learn. The higher the 
number of filters, the more complex the network can be, but this can 
also make training more difficult.

hyperparameters for model architectures.  

2.  Partial training: Models are partially trained (on a small fraction of 
the data) to quickly eliminate underperforming configurations.  
3.  Successive Halving: The remaining configurations are grouped into 
sets of different sizes, and the associated models are trained further. 
The best-performing configurations in each set are promoted to the 
next  stage,  while 
the  under-performing  configurations  are 
eliminated. 

4. Repeat: Steps 2 and 3 are repeated until only one configuration re-
mains, which is then considered the best configuration found.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Hyperband does not explicitly mention the use of specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) in its process. However, it is common practice in deep learning pipelines to use these optimization techniques during the training phase of neural networks.

Hyperband focuses on efficiently searching through the space of hyperparameters for model architectures. It achieves this by combining an initial random search with a'successive halving' strategy. This involves partial training of models on a small fraction of the data to quickly eliminate underperforming configurations. The remaining configurations are then grouped into sets of different sizes, and their corresponding models are trained further. The best performing configurations in each set are promoted to the next stage, while underperforming ones are eliminated. This process is repeated until only one configuration remains, which is then considered the best configuration found.

While Hyperband itself does not specify the use of optimization techniques like SGD or Adam, they would typically be employed during the actual training of the neural networks within this framework. These optimization techniques help minimize the loss function during backpropagation, thereby improving the accuracy of the model.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 3 
Parameters tested and values for each classical machine learning model used and each test year. xgbTree = extreme gradient boosting tree, RF = random forest, NNET =
neural network, DNN = deep neural network.  

Model 

xgbTree 

RF 
NNET 

DNN 

Parameter 

2013 

2014 

2015 

2016 

2017 

2018 

2019 

Nrounds 
max_depth 
Eta 
Gamma 
colsample_bytree 
min_child_weight 
Subsample 
Mtry 
Size 
Decay 
layer1 
layer2 
layer3 
hidden_dropout 
visible_dropout 

150 
3 
0.4 
0 
0.8 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.8 
1 
1 
2 
3 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150  
3  
0.3  
0  
0.6  
1  
1 
2 
5  
0.1 
1  
0  
0  
0  
0

10.3 
1.6 
0.6 
74.9 
0.8 
0.5 
11.4 

8.4 
1.0 
0.7 
79.4 
0.5 
0.3 
9.6 

10.4 
0.9 
0.8 
75.3 
0.7 
0.5 
11.4  

represented  correspond  to  the  lower  decreases  or  increases  in  the 
number of eggs (i.e., 1 to 25% and > 25 to 50%; Table 1). 

3.2. Deep learning model accuracy, predictive performance, and predictor 
importance 

Concerning the performance of the candidate models, the best vali-
dation accuracies were achieved by model 12 in all the years (accuracy 
= 0.833 ± 0.005; mean of years ± sd) (Fig. 2), a model having a Deep 
Convolutional Long Short-Term Memory (DeepConvLSTM) architecture. 
Model  19,  also  with  a  DeepConvLSTM  architecture  had  high  vali-
dation accuracies too (accuracy = 0.829 ± 0.003; mean of years ± sd), 
followed  by model 14, a Convolutional Neural Network  (CNN) archi-
tecture reaching a validation accuracy of 0.821 ± 0.005 (mean of years 
± sd).

2.5. Model selection procedure 

The model selection procedure was performed as follows (Capinha 
et  al.,  2021;  Van  Kuppevelt  et  al.,  2020):  we  randomly  generated  5 
models for each of the four available deep-ANN architecture types (20 
models in total) and trained each one with a small subset of the training 
data  (data  partition  At)  for  4  epochs  (an  “epoch”  corresponds  to  the 
complete training dataset being passed forward and backward across the 
network  one  time;  Capinha  et  al.,  2021).  The  accuracy  of  candidate 
models,  as  provided  by  mcfly  (i.e.,  the  “proportion  of  cases  correctly 
classified”), was then compared using a left-out validation data set (data 
partition Av) and the model with the highest performance was selected 
for training on the full training data (data partition Bt; Bt = At + Av) for 
up to 30 epochs.

Also, deep learning approaches should be compared with the more 
classical  statistical  and  machine  learning  approaches  to  assess  how 
model  performances  differ  between  methods.  In  this  sense,  we  com-
plemented our study design with a comparison with classical machine

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the information provided, it appears that the table does not explicitly list the hyperparameters typically associated with deep learning models such as learning rate, optimizer, batch size, etc. However, there are some parameters listed that could potentially be considered hyperparameters for the deep learning models used. These include 'Nrounds','max_depth', 'Eta', 'Gamma', 'colsample_bytree','min_child_weight', 'Subsample', 'Mtry', 'Size', 'Decay', 'layer1', 'layer2', 'layer3', 'hidden_dropout', and 'visible_dropout'. It is important to note that without additional context or clarification, it cannot be definitively stated which of these parameters are specific to the deep learning models and what their exact roles are within those models. Therefore, while I can provide a list of potential hyperparameters based on the given information, further details would be needed to confirm their relevance and function within the deep learning models used.
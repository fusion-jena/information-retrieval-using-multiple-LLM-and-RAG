Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The prediction power with machine learning algorithms was tested 
with the package “sdm”  (Naimi and Araújo, 2016), “dismo”  (Hijmans 
et al., 2015), “bioclim” (Booth et al., 2014), and the ensemble modeling 
approach  implemented  in  “biomod2”  (Thuiller  et  al.,  2009;  2014). 
Model mean performances for AUC, ROC, TSS, KAPPA, and COR values 
were extracted using “shiny” packages (Chang et al., 2023) in RStudio 
(RStudio Team, 2020). 

2.10. Statistical analysis

EcologicalInformatics78(2023)1023465M. Abdelgadir et al.                                                                                                                                                                                                                            

importance,  and  regularized  training  gain  for  each  variable.  Here,  a 
variable  with  highest  regularized  training  gain  is  indicating  that  this 
variable  has  the  most  useful  information  on  its  own  when  used  in 
isolation. Model sensitivity and accuracy for habitat suitability were also 
checked  with  the  AUC  values.  Graphical  representation  of  MAXENT 
models were produced in QGIS Desktop v 3.24.2 (QGIS Development 
Team, 2022).

To verify the model’s validity, 30% of each taxon’s data records were 
utilized as testing data, while the remaining 70% were used as training 
data.  After  10  K-fold  cross-validation  and  10-fold  bootstrapping,  all 
models  were  assessed  based  on  the  mean  values  for  Area  Under  the 
Curve AUC (Hanley and McNeil, 1982), True Skill Statistics TSS (Mon-
serud  and  Leemans,  1992),  Cohen’s  KAPPA  (Allouche  et  al.,  2006), 
point-biserial correlation COR (Elith et al., 2010), and Receiver Oper-
ator Characteristics ROC (Hanley and McNeil, 1982). Bootstrapping was 
used  to  provide  unbiased  predictive  accuracy  with  a  sufficiently  low 
variance  (Harrell  et  al.,  2005;  Lima  et  al.,  2019).  For  the  model 
threshold,  the  sensitivity-specificity  sum  maximization  technique  was 
employed  (Liu  et  al.,  2005).  According  to  this  evaluation  criterion, 
species distribution can be accurately predicted (Liu et al., 2005). For

2.6. Model selection 

Since  performance,  accuracy,  key  parameters,  and  sensitivity  for 
prediction vary depending on which method that is used (Diniz-Filho 
et al., 2009; Elith et al., 2006; Qiao et al., 2015), we tested eight different 
machine  learning  algorithms  to  generate  our  predictive  models.  The 
included  algorithms  were:  Generalized  Linear  Model  (GLM),  Support 
Vector  Machine  (SVM),  Multivariate  Adaptive  Regression  Spline 
(MARS),  Random  Forest  (RF),  Flexible  Discriminant  Analysis  (FDA), 
Classification  and  Regression  Trees  (CART),  Generalized  Boosting 
regression Model (GBM) and Maximum-Entropy learning (MAXENT) for 
habitat suitability prediction (Table S5). 

2.7. Ensemble modeling approach

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it appears that the authors did not explicitly mention any specific criteria for determining when training is completed. However, they mentioned using 30% of each taxon's data records as testing data and the remaining 70% as training data. They also performed 10 K-fold cross-validation and 10-fold bootstrapping. These methods are commonly used to evaluate the performance of machine learning models during training and help prevent overfitting.

Regarding the criteria for stopping training, one common approach is to monitor the validation loss or error rate during training and stop when it reaches a plateau or starts increasing. This indicates that the model may have started to overfit the training data and will likely perform poorly on new, unseen data. Another approach is to use early stopping, where training is stopped after a certain number of epochs without improvement in the validation loss.

However, since the authors did not specifically state their criteria for stopping training, I cannot definitively answer what criteria they used. It would require further clarification from the authors or additional context to confirm.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

CRediT authorship contribution statement 

Zixuan Yin: Data curation, Methodology, Software, Writing – orig-
inal  draft,  Writing  –  review  &  editing.  Yaqin  Zhao:  Methodology, 
Writing  –  review  &  editing.  Zhihao  Xu:  Data  curation.  Qiuping  Yu: 
Software. 

Data availability 

Data will be made available on request. 

Acknowledgements 

Supported  by  National  Natural  Science  Foundation  of  China 

(32371583). 

References 

Aarts, R.M., Irwan, R., Janssen, A.J.E.M., 2002. Efficient tracking of the cross-correlation 
coefficient. IEEE Trans. Speech Audio Proc. 10 (6), 391–402. https://doi.org/ 
10.1109/TSA.2002.803447. 

Babenko, B., Yang, Ming-Hsuan, Belongie, S., 2011. Robust object tracking with online 

multiple instance learning. IEEE Trans. Pattern Anal. Mach. Intell. 33 (8), 
1619–1632. https://doi.org/10.1109/TPAMI.2010.226. 

Beery, S., Wu, G., Rathod, V., Votel, R., Huang, J., 2020. Context R-CNN: long term

The Siamese relation network(Cheng et al., 2021) used a deep re-
sidual neural network (ResNet-50) as its backbone to extract multi-scale 
features from images of animals. The cascade structure of a region-based 
convolutional neural network was used to classify the proposed location 
boxes. The Siamese relation network introduced two primary modules 
into  the network structure, namely, a relation detector module and a 
refinement  module.  The  relation  detector  module  was  equipped  with 
three different nonlinear detectors, including the global, local, and patch 
detectors. These detectors were used to compare the relationships be-
tween the features generated by the target and multi-scale regression 
features,  including  pixel-wise,  patch-wise,  and  global  features.  The 
refinement module converted the output of the relation detector into a 
matching  score  followed  by  an  element-wise  replication  between  the

Clune, J., 2018. Automatically identifying, counting, and describing wild animals in 
camera-trap images with deep learning. Proc. Natl. Acad. Sci. 115 (25), 
e5716–e5725. https://doi.org/10.1073/pnas.1719367115. 

Okafor, E., Pawara, P., Karaaba, F., Surinta, O., Codreanu, V., Schomaker, L., 

Wiering, M., 2016. Comparative study between deep learning and bag of visual 
words for wild-animal recognition. In: 2016 IEEE Symposium Series on 
Computational Intelligence (SSCI), pp. 1–8. https://doi.org/10.1109/ 
SSCI.2016.7850111. 

Riffenburgh, R.H., 2012. Epidemiology. In: Statistics in Medicine. Elsevier, pp. 535–549. 

https://doi.org/10.1016/B978-0-12-384864-2.00025-1. 

Rose, P.E., Nash, S.M., Riley, L.M., 2017. To pace or not to pace? A review of what 

abnormal repetitive behavior tells us about zoo animal management. J. Vet. Behav. 
20, 11–21. https://doi.org/10.1016/j.jveb.2017.02.007. 

Schindler, F., Steinhage, V., 2021. Identification of animals and recognition of their

matching score and cross-correlation feature map of the classification 
branch. Finally, a refined classification confidence score was generated 
using a convolution operation. The refinement module effectively inte-
grated  the  relation  detector  into  the  Siamese  framework  to  suppress 
false-positive detections and filter background interference.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
According to the given information, it appears that the data used in this research will be made available upon request. However, there is no specific mention of where the data will be stored or shared publicly, such as through platforms like Zenodo, Figshare, Dryad, or GBIF. Therefore, if one wants access to the data, they should contact the authors directly.
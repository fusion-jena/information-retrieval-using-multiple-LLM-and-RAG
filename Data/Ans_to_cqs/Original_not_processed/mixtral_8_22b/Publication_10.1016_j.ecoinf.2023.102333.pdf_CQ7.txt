Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The  points  were  randomized  and  divided  into  training  (80%)  and 
validation  (20%)  sets.  A  15  m  buffer  was  applied  around  all  sample 
points  to  ensure  independence  between  the  datasets  (Fig.  4).  The 
reference points were then exported to the cloud for model training and 
validation. 

2.4. Indexes calculation

A search space of the primary hyperparameters was defined. Opti-
mization  algorithms  were executed to  find  the  highest  accuracy from 
1,788,160  hyperparameter  combinations.  Our  model  was  trained  for 
200 epochs for each trial to find three DNN architectures that were used 
to  build  three  models  and  train  them  for  the  entire  dataset  for  1000 
epochs (Table 2).

adjustable parameters that control network architecture, optimization 
procedure,  and  regularization.  Common  hyperparameters  include 
number of layers and nodes, learning rate, dropout rate, etc. However, 
optimizing  model hyperparameters  remains imperative  for leveraging

This  study  adopted  an  automated  optimization  architecture  by 
comparing three architecture optimization algorithms: Random Search 
(RS), Hyperband, and Bayesian optimization. Hyperparameters of our 
DNN model were automatically optimized according to a search grid of 
several hyperparameters listed in Table 1. A dropout layer was added to 
prevent overfitting and to reduce statistical noise. The dropout regula-
rization  randomly  removed  different  sets  of  neurons  using  a  pre-
determined  dropout  rate,  which  resulted  in  training  different  neural 
networks and reduced overfitting (Srivastava et al., 2014). 

2.6.1. Random search

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In this study, the data was split into two parts: training and validation sets. The division was done randomly, with 80% of the data allocated for training and the remaining 20% for validation. To maintain independence between these datasets, a 15-meter buffer was applied around all sample points. This means that no point within the 15-meter radius of another point was included in both the training and validation sets. After splitting, the reference points were exported to the cloud for model training and validation.
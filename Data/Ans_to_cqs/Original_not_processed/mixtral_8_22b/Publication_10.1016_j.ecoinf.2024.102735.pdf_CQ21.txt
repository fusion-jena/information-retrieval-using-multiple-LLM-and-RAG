Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

C and 27

◦

◦

3. Methodology

To provide clarity, this section begins with a brief overview of the
methodology which includes each objective and the sequential steps
undertaken to achieve them, followed by a detailed explanation of the
methods employed for each objective.

3.1. Overview of methodology

Objective 1
This objective involves the development of multiple machine-
learning models using randomized subsets of training data to estimate

Fig. 2. Flowchart of adopted methodology.

effect of measurement uncertainties on chlorophyll-a predictions.

While the overall methodology is shown in the flowchart in Fig. 2,
this section provides the general outline of the methodology adopted to
address objective 1.

I. Data Acquisition

i. High frequency (e.g., 15 min) continuous surface water
quality data (e.g., pH, turbidity) were acquired from water
quality sensors-YSI profiler deployed in Boyd Millpond by the
Land Development Division of Greenville County, SC.

Data-driven models have evolved as effective decision-support tools
for HABs monitoring and prediction. These models are essential for
using patterns between water quality parameters and ecosystem vari-
ables to predict algal-related parameters such as chlorophyll-a, phyco-
cyanin, algal cells, and toxins (Busari et al., 2023; Kimambo et al.,
2019). Machine learning (ML) models such as random forest (RF),
Multilayer perceptron (MLP), long-short term memory (LSTM), and
support vector regression (SVR) have been used successfully to predict
different algal-related parameters and help identify drivers of HABs (Liu
et al., 2019; Qian et al., 2023; Zhang et al., 2015, Busari et al., 2023).
Model predictions are, however, influenced by varying degrees of un-
certainty (Gawlikowski et al., 2023). According to Guzman et al. (2015),
uncertainty refers to the variation between the actual state of a variable
and its theoretical assessment at a particular time. These prediction

III. Model Development

i. All the datasets were divided randomly into two: 80 % for
training the RF and SVR models, and 20 % for testing the
model, although the training set was further randomly
sampled to select training and validation set to obtain opti-
mized model hyperparameters.

EcologicalInformatics82(2024)1027355I. Busari et al.

Table 3
Description of models developed in the study.

Model abbreviation

Description

RF_with_Sensor

RF_w/o_Sensor

SVR_with_Sensor

SVR_w/oSensor

RF_with_Sensor_cont

RF_w/o_Sensor_cont

SVR_with_Sensor_cont

SVR_w/o_Sensor_cont

included during the model training phase when introducing high-
frequency datasets. This is unsurprisingly as dataset size is often line-
arly related to number of regression trees required to build the models
(Biau and Scornet, 2016). This will enable the RF model to learn more
patterns
in the water quality datasets efficiently and improve
chlorophyll-a predictions. The most occurring max_depth when using
the manually sampled dataset was 28, which increased to 37 when more
datasets were used to train the model. Deeper trees in the continuous
high frequency data allows the model to learn intrinsic water quality
patterns. Less deep trees in the manually sampled datasets helps the
model to learn effectively with the few datasets without overfitting. The
inclusion of the chlorophyll-a sensor values as a predictor variable made
little impact on the parameter distribution for both RF_with_Sensor and
RF_w/o_Sensor (Fig. 5). This suggests that increasing predictor numbers

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not explicitly mention any specific postprocessing steps involved after the model training. However, based on common practices in machine learning projects, some possible postprocessing steps could include evaluating the performance of the trained models using metrics like accuracy, precision, recall, F1 score, or area under the ROC curve. Additionally, confusion matrices can be generated to visualize the performance of the classifiers. Other techniques like saliency maps might also be used to understand how the models arrived at their decisions. But these are speculations based on typical ML workflows, as the provided context does not directly address the query.
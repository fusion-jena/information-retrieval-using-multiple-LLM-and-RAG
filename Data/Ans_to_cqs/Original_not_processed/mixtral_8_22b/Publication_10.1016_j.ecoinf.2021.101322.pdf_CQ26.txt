Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2. Methods 

2.1. CNN training data

Data availability 

Salmon scale 9056 image dataset: 
doi:10.21335/NMDC-1050865887 
Expert  reading  of  extra  150 

335/NMDC-1462728994 

image  dataset:  doi:10.21 

CNN based architecture: EfficientNet: https://github.com/qubvel/ 

efficientnet 

Pre-trained  network  weights  (EfficientNet–B4):  https://github. 

com/qubvel/efficientnet/blob/master/efficientnet/weights.py 

Declaration of Competing Interest 

The authors declare that they have no known competing financial 
interests or personal relationship that could have appeared to influence 
the work reported in this paper. 

Acknowledgements 

We gratefully acknowledge valuable advice on the CNN architecture 
provided by Hyeongji Kim from the Institute of Marine Research, Ber-
gen, Norway. This project was funded in part by Research Council of 
Norway  projects  270966/O70  (COGMAR  -  Ubiquitous  cognitive com-
puter vision for marine services). 

References

We found that the use of transfer learning when training our CNNs 
was crucial for their performance. Transfer learning is a common tech-
nique within deep learning, where networks trained to perform well on 
one  task  can  be  adapted  with  some  training  towards  similar  tasks 
(Yosinski et al., 2014). This is especially useful when datasets are sparse 
and more narrowly distributed. Often, a significant benefit can be gained 
by starting from a pre-trained network where the first and intermediate 
layers in the network already have been shaped by training on a large 
and  diverse  dataset.  EfficientNet  is  available  pretrained  on  the  large 
benchmark dataset, ImageNet, containing around 1.4 million images in 
1000 classes (Deng et al., 2009), more than 100 times the number of 
labelled salmon scale images used in this study. The network's ability to 
abstract lower level but more generally useful features has then already

readers.  This  is  based  on  the  statistical  phenomena  that  the  group 
average of several estimators –  without systematic bias –  of some un-
known quantity tends to be closer to the truth than most of the single 
estimates.  Using  such  a  labelling  approach  would  smoothen  single 
reader  biases  and  the  trained  CNN  should  tend  to  predict  the  group 
average.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In addressing data bias during preprocessing of the deep learning pipeline, various techniques were employed. One technique mentioned is the use of transfer learning, which involves adapting networks trained on one task to perform similar tasks. This method proved particularly beneficial when dealing with sparse and narrowly distributed datasets. In this case, the researchers utilized EfficientNet, a pre-trained network that had been trained on the large benchmark dataset, ImageNet. By leveraging these pre-existing models, the researchers were able to take advantage of the network's ability to abstract lower-level features, which had already been honed through extensive training on a vast and varied dataset.

Additionally, the researchers also addressed potential reader bias in the labeling process. They did so by employing a group average approach, wherein multiple readers labeled each sample independently. This strategy aimed to mitigate individual reader biases and ensure that the resulting labels reflected a more balanced perspective. Consequently, the trained convolutional neural network (CNN) tended to predict the group average rather than being influenced by any particular reader's interpretation.

However, it appears that specific techniques like stratified splitting, oversampling, undersampling, or diverse data collection were not explicitly discussed in the given context. These methods are commonly used to manage class imbalance and improve model generalization, but there is no direct mention of them in relation to this research. Therefore, while we cannot definitively state whether these techniques were applied, we can confirm that transfer learning and a group averaging approach were implemented to tackle data bias during preprocessing.
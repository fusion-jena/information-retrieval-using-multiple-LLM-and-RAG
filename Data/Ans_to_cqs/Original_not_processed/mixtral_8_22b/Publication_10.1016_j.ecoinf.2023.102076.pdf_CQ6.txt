Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Obtaining a sufficient volume of ‘gold-standard’ training data is in-
tegral to achieving high performance in machine learning applications: 
this was also reflected in our experimental results. However, although 
the burden of manual annotation has often been cited as a barrier to the 
use of custom text classifiers in online data collection (Kulkarni and Di 
Minin,  2021),  our  study  indicates  that  the  fine-tuning  of  pre-trained 
language  models  can  achieve  classification  accuracy  of  >85%  with 
<500  training  examples  (Fig.  4).  As  also  noted  by  Stringham  et  al. 
(2021a),  beyond  the  ‘peak’  number  of  training  examples,  model  per-
formance is likely determined by the nature of the textual data and the 
model, rather than the volume of training data. Again, this emphasises 
that, for clearly defined classification tasks, transformer-based models 
provide the opportunity to rapidly develop high-performing text clas-

Beltagy, I., Lo, K., Cohan, A., 2019. SciBERT: a pretrained language model for scientific 
text. In: Proceedings of the 2019 Conference on Empirical Methods in Natural 
Language Processing and the 9th International Joint Conference on Natural 
Language Processing (EMNLP-IJCNLP), pp. 3615–3620. 

Benıtez-L´opez, A., Santini, L., Schipper, A.M., Busana, M., Huijbregts, M.A., 2019. Intact 
but empty forests? Patterns of hunting-induced mammal defaunation in the tropics. 
PLoS Biol. 17 (5), e3000247. 

Blasi, D., Anastasopoulos, A., Neubig, G., 2022. Systematic inequalities in language 
technology performance across the world’s languages. In: Proceedings of the 60th 
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 
pp. 5486–5505. https://doi.org/10.18653/v1/2022.acl-long.376. 

Bojanowski, P., Grave, E., Joulin, A., Mikolov, T., 2017. Enriching word vectors with 

subword information. Trans. Assoc. Comput. Linguist. 5, 135–146.

computational resources required to train them, the practical benefits of 
using  transformer  models  for  text  classification  may  be  limited  when 
applied to simpler classification tasks.

Since BERT can only accept a maximum of 512 tokens as input, we 
used smaller subsections of the longer web articles as classifier input, an 
approach previously applied to long document classification (Fiok et al., 
2021; Sun et al., 2019). To identify the sections of text most likely to be 
relevant to the classification decision, documents were split into chunks 
of 300 tokens using the NLTK word-tokenizer. Chunks that contained 
any  of  our  original  query  terms  (i.e.  any  bat-related  or  exploitation- 
related  term)  were  retained.  If  more  than  one  chunk  contained  key-
words,  the  first  such  chunk  of  the  document  was  retained  for  use  in 
model training and evaluation. Before being fed into the BERT model, 
texts are tokenized using the BERT tokenizer. Here, we set the maximum 
number of tokens as 400 for social media posts, as most were very short, 
and 512 for web articles, meaning some texts would have been further 
truncated at this step.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The provided context does not explicitly mention all possible preprocessing steps typically used before training a deep learning model, but it does describe specific preprocessing steps taken for a particular task involving text classification using BERT. These include:

1. Tokenizing the text using the NLTK word-tokenizer to divide the documents into smaller chunks of 300 tokens each. This helps manage computational resources since BERT can only handle a maximum of 512 tokens as input.

2. Retaining only those chunks containing relevant keywords related to the classification task. For instance, if the task involves identifying bats and their exploitation, then only those chunks containing bat-related or exploitation-related terms are kept.

3. Selecting the first keyword-containing chunk from each document for use in model training and evaluation.

4. Further tokenizing the selected chunks using the BERT tokenizer before feeding them into the BERT model.

5. Setting a maximum limit on the number of tokens allowed for different types of inputs – 400 tokens for social media posts and 512 tokens for web articles. Any excess tokens will be truncated during this stage.

These preprocessing steps help prepare the raw text data for efficient processing and analysis by the deep learning model. Other common preprocessing techniques like normalization, scaling, and cleaning might also be necessary depending on the dataset and the specific requirements of the deep learning model.
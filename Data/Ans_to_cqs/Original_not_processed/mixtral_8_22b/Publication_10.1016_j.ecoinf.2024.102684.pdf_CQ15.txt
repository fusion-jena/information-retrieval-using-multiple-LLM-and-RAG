Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We applied the  standard machine learning hyperparameter  search 
over  two  sites  because  of  computational  cost.  In  our  research,  we 
considered the activation function, the optimizer, and the learning rate. 
In addition, we performed an exhaustive search for the fully connected 
architecture for all mono-site and multi-site models. In all these cases, 
we compare the training, validation, and testing performance to avoid 
overfitting.  We  could  have  achieved  the  same  results  using  hyper-
parameter search software such as Optuna and Sklearn. However, even 
though they could save coding time, we prefer to control every aspect of 
the process to avoid problems. The hyperparameter search involved a 
small grid search selecting the following hyperparameters and values:  

1.  Hidden activation function: ReLU, sigmoid, or elu,  
2.  Optimizer: RMSprop or adam  
3.  Learning rate: 0.1, 0.001, 0.0001, and 0.00001.

of the models. For instance, Mil`a et al. (2022) used the Nearest Neighbor 
Distance Matching (NNDM) LOO CV method to delineate a geographic 
space  in  which  predictions  can  be  made.  Many  of  these  approaches 
accurately  demonstrate  the  model's  transfer  learning  capabilities,  yet 
they  impose  limitations  on  the  training  domain,  leading  to  reduced 
training  heterogeneity  (Meyer  and  Pebesma,  2021).  However,  con-
ducting large CV experiments with deep learning-based models is often 
infeasible due to  their high computational costs. Deep  learning-based 
models  typically  employ  simple  data  partitioning  strategies,  such  as 
training, validation, and testing (Ocer et al., 2020). This simple scheme 
can help to add heterogeneous information to the training procedure, 
but may also overlook the diversity of heterogeneous spatial information 
in remotely sensed data that is aligned with different learning domains

CNNs  have  demonstrated  a  solid  ability  to  solve  computer  vision 
problems, outperforming even human performance in image classifica-
tion  (He et  al., 2016). CNNs can be used to solve different problems, 
including classification, regression, object detection, and semantic- and 
instance-based  segmentation.  The  differences  between  them  mainly 
depend on the architectures, the output function, and the labeling pro-
cess. For example, the regression approach based on CNN architectures 
receives an image, and then it passes through a series of convolutional 
layers connected to a fully connected network, with a linear or sigmoid 
output function (Lathuiliere et al., 2020). This approach is often prac-
tical and easy to interpret, as it allows direct estimation of continuous 
transitions and  patterns in the landscape, such as plant covers.  Alter-
natively, semantic segmentation, e.g. using the UNet model, has been

Szegedy, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, Rabinovich, 2015. Going deeper 

Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, 
pp. 659–666. 

with convolutions. In: IEEE Conference on Computer Vision and Pattern 
Recognition, pp. 1–9. 

Karystinos, G.N., Pados, D.A., 2000. On overfitting, generalization, and randomly 

Telford, R.J., Birks, H.J.B., 2009. Evaluation of transfer functions in spatially structured 

expanded training sets. IEEE Trans. Neural Netw. 11, 1050–1057. 

environments. Quat. Sci. 28, 1309–1316. 

Kattenborn, T., Eichel, J., Fassnacht, F., 2019a. Convolutional neural networks enable 
efficient, accurate and fine-grained segmentation of plant species and communities 
from high-resolution UAV imagery. Sci. Rep. 9, 17656.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text mentions that the researchers considered the activation function, the optimizer, and the learning rate as part of their standard machine learning hyperparameter search. Specifically, they looked at the hidden activation function, choosing from options like ReLU, sigmoid, or elu. As for the optimizer, they selected either RMSprop or adam. Lastly, they experimented with various learning rates, including 0.1, 0.001, 0.0001, and 0.00001. Therefore, the optimization techniques applied in this deep learning pipeline include RMSprop and Adam as optimizers, along with adjusting the learning rate to find the best value.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.2. Performance of the models and forecast distribution 

The  mean  accuracies  and  standard  deviations  of  the  5  × 10-fold 
cross-validations  for  each  method  and  species  are  shown  in  Table  4. 
Random Forest is the method that achieves the highest accuracy for all 
the analysed species (0.718 for SKJ, 0.728 for YFT, 0.589 for BET and 
0.842 for FAL). Consequently, this is the method that has been used to 
build the prediction models. 

A  model  has  been  trained  for  each  species,  by  using  the  selected 
variables in Table 3. Then, for each model, the probabilities of finding 
high and low catches and absences of each species in the study area have 
been predicted by using the corresponding predictors given in Table 3. 
The results of the validation of the trained models are shown in Table 5,

EcologicalInformatics81(2024)1025773N. Goikoetxea et al.                                                                                                                                                                                                                            

Fig. 1. Diagram of the model building process.  

Table 2 
Number of instances of each species and class.   

HIGH 
LOW 
ABS 

SKJ 

6307 
6533 
6300 

YFT 

5234 
5831 
5200  

BET 

3395 
3447 

FAL 

8929 

6704  

kept.  To  apply  both  the  CFS  cross-validation  and  SUS,  their  imple-
mentation in Weka (Frank et al., 2016) has been used. 

In the third step, a supervised classification model is trained with the 
selected variables. Four different methods, widely used in species dis-
tribution modelling, have been tried to build the classification model:  

• Random Forest (Breiman, 2001): Classifier that combines the output 

of multiple decision trees to reach a single result.

YFT 

BET 

FAL 

F 

SUS 

Predictor 

F 

SUS 

Predictor 

F 

SUS 

Predictor 

F 

SUS 

SKJ 

Predictor 

SST 

Temperature at 50 m 
Month 
Temperature gradient at 

200 m 

Chlorophyll concentration 

integrated at 20 m 

Nitrate at 50 m 

Phosphate at 50 m 

10 

10 
10 

10 

10 

10 

10 

0.0078 

0.0078 
0.0075 

Oxygen concentration 
integrated at 10 m 
Temperature at 50 m 
Nitrate at 50 m 

0.0071 

Surface phosphate 

0.0069  Month 

0.0067 

Nitrate at 20 m 

0.0064 

SST 

10 

10 
10 

10 

10 

10 

10 

0.0080 

Silicate at 150 m 

0.0079 
0.0078 

Salinity at 50 m 
Temperature at 50 m 

0.0075 

SST 

0.0069 

Nitrate at 100 m 

0.0056 

0.0055 

Salinity at 200 m 
Oxygen concentration 
integrated at 50 m 
Oxygen concentration 
integrated at 175 m 

Nitrate at 20 m 

10 

0.0064 

Temperature at 150 m 

10 

0.0051 

Temperature gradient at 

150 m 

10 

0.0061 

Phosphate at 175 m 

10 

0.0050 

Phosphate at 10 m 

Sea level anomaly 

10 

0.0061 

Longitude

mitigate  climate  change  caused  by  the  fishing  industry  and  reduce 
operating  costs  (Granado  et  al.,  2021).  Nevertheless,  it  must  be 
considered that this kind of models learn from historical catch data, and 
if the data used to train them is not significant enough, the models will 
not be able to make good predictions. Considering this fact, performing 
periodic updates of the models with new catch data is desired to improve 
their performance.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide explicit information on the specific strategy implemented to monitor the model performance during training. However, it mentions that a 5 x 10-fold cross-validation was performed for each method and species, which suggests that the authors might have used cross-validation as a technique to evaluate the model performance during training. Cross-validation involves splitting the dataset into several subsets or folds, where one fold is used for testing while the remaining ones are used for training. This process is repeated until every fold serves as the test set once, allowing for a more robust evaluation of the model's performance.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The utilization of an imbalanced dataset for training a deep learning 
model may introduce inherent biases and limitations (Dong et al., 2018). 
It  is  important  to  acknowledge  that  the  dataset  used  may  not  fully 
capture  the  diverse  range  of  bird  populations  present  in  the  wild 
(Gregory and van Strien, 2010). This can result in a biased representa-
tion, potentially impacting the accuracy and generalization ability of the 
model. Factors such as geographical bias, limited sample sizes, or un-
derrepresentation of certain bird species can all contribute to this issue 
(Devenish-Nelson et al., 2019). Therefore, it is crucial to consider the 
limitations and potential biases of the training dataset when interpreting 
the results and making inferences about real-world bird species recog-
nition.  Additionally,  efforts  should  be  made  to  collect  more  compre-
hensive  and  diverse  datasets  that  better  reflect  the  true  variability  of

Yang, L., Shami, A., 2020. On hyperparameter optimization of machine learning 

algorithms: theory and practice. Neurocomputing 415, 295–316. 

Shorten, C., Khoshgoftaar, T.M., 2019. A survey on image data augmentation for deep 

Yang, C.-L., Harjoseputro, Y., Hu, Y.-C., Chen, Y.-Y., 2022a. An improved transfer- 

learning. J. Big Data 6 (1), 1–48. 

Sitepu, A.C., Liu, C.-M., Sigiro, M., Panjaitan, J., Copa, V., 2022. A convolutional neural 
network bird’s classification using north american bird images. J. Health Sci. 6 (S2), 
15067–15080. 

Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., Patwary, M., 

Prabhat, M., Adams, R., 2015. Scalable bayesian optimization using deep neural 
networks. In: International Conference On Machine Learning. PMLR, pp. 2171–2180. 

learning for image-based species classification of protected indonesians birds. CMC 
Comp. Mater. Continua 73 (3), 4577–4593.

networks  experienced  overfitting,  failing  to  generalize  effectively. 
Several  data  augmentation  techniques  were  employed  to  address  this 
issue.  Data  augmentation  expands  a  dataset  by  applying  trans-
formations, such as flipping, translation, and shear, to the existing data 
(Shorten and Khoshgoftaar, 2019). Data augmentation helps maintain 
dataset balance, reduce overfitting, and improve the generalization of 
deep learning models (Mungloo-Dilmohamud et al., 2022). In this study, 
horizontal and vertical translations were done within a range of (cid:0) 30 to 
30 pixels, allowing random shifts. Shear distortion was applied within a 
range of (cid:0) 5 to 5, representing maximum distortion to the left and right. 
Table 3 details the transformations used for data augmentation on the 
selected  dataset.  After  augmentation,  the  training  dataset  size  was 
increased from 11,788 to 40,000 images. After the data augmentation,

The  average  accuracy  of  pre-trained  models  was  assessed  across 
three train test partitions, i.e., 60:40,80:20, and 90:20 before and after 
augmentation, as illustrated in Table 11. The result showed significant 
improvement in accuracy after augmentation. EfficientNetB0 achieved 
the highest accuracy of 96.25%, respectively. These results demonstrate 
the substantial impact of data augmentation on model performance. 

The performance of the pre-trained models employed in the present 
study was compared to other existing methods shows that (Wei et al., 
2018),  (Kondaveeti  et  al.,  2023;  Kumar  et  al.,  2022a;  Kumar  and 

Table 11 
The average performance metrics of pre trained models were calculated from 
three  partitioned  tests  conducted  before  data  augmentation  and  after  data 
augmentation.  

Model name 

Accuracy 

Before data augmentation 

After data augmentation

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To address data bias during preprocessing of the deep learning pipeline, several techniques can be utilized. One common approach is stratified splitting, which involves dividing the dataset into training and testing sets while maintaining the same distribution of classes in both sets. This ensures that the model is exposed to a representative sample of each class during training, reducing the risk of bias towards any particular class.

Oversampling and undersampling are also popular techniques used to handle class imbalance in datasets. Oversampling involves duplicating examples from the minority class to match the number of examples in the majority class. Undersampling, on the other hand, reduces the number of examples in the majority class to match the number of examples in the minority class. Both techniques aim to create a balanced dataset, improving the model's ability to learn from underrepresented classes.

Collecting diverse data is another essential technique to mitigate data bias. As mentioned earlier, utilizing an imbalanced dataset for training a deep learning model may introduce inherent biases and limitations. To overcome this, efforts should be made to gather comprehensive and diverse datasets that accurately reflect the true variability of the target population. For instance, in the case of bird species recognition, collecting data from various geographic locations and habitats would help capture the diversity of bird populations present in the wild.

In addition to these techniques, data augmentation plays a vital role in addressing data bias. It involves expanding the dataset by applying transformations to the existing data, such as flipping, translation, and shear. Data augmentation helps maintain dataset balance, reduce overfitting, and improve the generalization of deep learning models. By employing these techniques, researchers can significantly enhance the performance and reliability of their deep learning models.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4. Model training 

The  experiments  were  conducted  on  a  Windows  10-based  mobile 
laptop  equipped  with  an  Intel(R)  i7-11800H  CPU  and  a  GeForce 
RTX3080  GPU  with  16GB  of  video  memory.  The  PyCharm  software 
platform was utilised to carry out the experiments, employing Python 
3.8 and the Pytorch 1.9 deep learning framework for training purposes. 
The  details  of  the  specific  environmental  configurations  and  model 
training parameters are listed in Table 1. To enhance detection accuracy, 
this study established detection thresholds of IoU = 0.3 and score = 0.3, 
indicating that a bounding box with a likelihood of containing a fish of 
30% or higher was considered valid and thus retained. 

3. Results 

3.1. LigObNet detection speed and accuracy 

3.1.1.

Impact of C1 module on network layer and inference time

several  times  higher  than  those  of  other  operations  (Li  et  al.,  2018). 
Therefore,  reducing  memory  access  is  the  key  to  further  enhancing 
model  performance.  By  reconfiguring  the  computation  sequence  of 
‘convolutional layer + batch normalisation + activation layer on NVI-
DIA TESLA V100 GPUs, Wang et al. (2019) reduced memory access by 
33%,  22%,  and  31%  for  the  ResNet-50,  Inception  V3,  and  DenseNet 
models,  respectively,  leading  to  increases  in  the  computational  effi-
ciency  of  20.5%,  18.5%,  and  18.1%.  Lowering  the  memory  access  in 
models can accelerate operations across the three main layers of CNNs, 
thus fully harnessing the model's strengths and more effectively utilising 
the computational resources provided by the embedded devices (Gilan 
et al., 2019; Zhang et al., 2015). Therefore, by reducing the number of 
layers  in  the  network  model,  the  number  of  convolutions  and  data

Li, J., Yan, G., Lu, W., Jiang, S., Gong, S., Wu, J., Li, X., 2018, March. SmartShuttle: 
Optimizing off-chip memory accesses for deep learning accelerators. In: 2018 
Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 
pp. 343–348. 

Li, J., Liu, C., Lu, X., Wu, B., 2022a. Fish passage monitoring based on improved 

YOLOv5s and TensorRT deployment. Trans. Chin. Soc. Agric. Machin. 12, 314–322. 
Li, J., Liu, C., Lu, X., Wu, B., 2022b. CME-YOLOv5: an efficient object detection network 

for densely spaced fish and small targets. Water 14 (15), 2412. 

Li, W., Li, F., Li, Z., 2022c. CMFTNet: multiple fish tracking based on counterpoised 

JointNet. Comput. Electron. Agric. 198, 107018. 

Wang, J.J., Hao, Z.Y., Li, H.L., 2019. Training memory access optimization for 

convolutional neural networks. J. Xi’an Univ. Electron. Sci. Technol. 02, 98–107. 

Wang, Q., Wu, B., Zhu, P., Li, P., Zuo, W., Hu, Q., 2020. ECA-net: efficient channel

The  memory  access  amount  of  the  model  refers  to  the  byte  size 
required  to  access  the  storage  units  during  the  computation  of  the 
feature maps of each layer, thereby reflecting the model's demand for 
storage  unit  bandwidth.  In  addition  to  the  impact  of  floating-point 
computations,  the  inference  speed  of  a  model  on  specific  hardware 
also requires a comprehensive evaluation that considers both hardware 
characteristics and the amount of memory access. Memory access, which 
is  often  overlooked,  plays  a  significant  role  in  model  performance 
assessment. Within the model architecture, the conventional combina-
tion of convolution (Conv), batch normalisation (BN), and ReLU forms 
>80% of the model structure (Guo et al., 2023). The convolution and BN 
layers require bidirectional data and exhibit complex data dependencies 
during computation, resulting in memory access energy costs that are

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Answer: According to the given context, the deep learning model was trained using a laptop equipped with an Intel(R) i7-11800H CPU and a GeForce RTX3080 GPU with 16GB of video memory. Additionally, the PyCharm software platform was utilized for carrying out the experiments, employing Python 3.8 and the Pytorch 1.9 deep learning framework for training purposes.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.4. Action recognition 

The three ResNet variants are trained for 40 epochs starting with an 
initial learning rate of 0.001. The learning rate is decreased every 10 
epochs by multiplying it with a γ = 0.1. For optimization we utilize the 
Stochastic  Gradient  Descent  (SGD)  with  a  momentum  of  0.9  and  a 

EcologicalInformatics61(2021)1012158F. Schindler and V. Steinhage

All programs use Python 3 and PyTorch (Paszke et al. (2019)) for 
building  and  training  the  networks.  The  Mask  R-CNN  model  is  built 
around  the  PyTorch  detection  model  of  Mask  R-CNN.  For  the  imple-
mentation of FGFA we use the PyTorch models for the backbone, the 
RPN and the head of the implementation of Mask R-CNN model. The 
three ResNet variations are also based on PyTorch detection models. We 
always use the pretrained versions of the detection models. The Mask R- 
CNN  is  pretrained  on  COCO  2017train  dataset.  The  ResNets  are  pre-
trained on Kinetics-400 dataset. We use these PyTorch models because 
they allow more flexibility than other implementations. They are better 
adjusted  to  the  PyTorch  training  functions  and  are  well-arranged. 
Moreover,  the  PyTorch  detection  models  do  not  have  other  de-
pendencies (e.g. special libraries) unlike other implementations. 

4.3. Object detection

Feichtenhofer et al. (2019). This parameter combination achieved the 
best  results  in  our  experiments.  The  Slow  and  Fast  pathways  are 
implemented by a ResNet-101. 

We  split  the  annotated  dataset  AnnotationsActions  (cf.  Section  2.2) 
randomly (but fixed for all experiments and all networks) into a train set, 
validation set and test set. The test set includes 20% of the video clips of 
each class. Of the remaining clips, again 20% of the clips in each class 
form the validation set. The remaining clips are the training set. 

4.4.1. Data augmentation for action recognition

Stochastic Gradient Descent (SGD) is used as optimization technique. 
We start with a learning rate of 0.0005, a momentum of 0.9 and weight 
decay of 0.0005. Different settings of momentum and weight decay are 
checked but show no improvements. In the training process, we decrease 
the learning rate every 10 epochs by multiplying it with γ = 0.1. In the 
first  epoch  we  use  warmup  iterations  for  the  learning  rate  with  a 
warmup factor of 1/1000. These parameters gave the best results in our 
evaluations. We train our models for 30 epochs.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't explicit information about specific preprocessing steps such as normalization, scaling, or cleaning. However, the text does mention data augmentation for action recognition. Data augmentation techniques can include various transformations like rotation, flipping, zooming, cropping, etc., which help improve the generalizability of the model. Additionally, the context mentions splitting the dataset into training, validation, and testing sets, which is another crucial step before training a deep learning model.
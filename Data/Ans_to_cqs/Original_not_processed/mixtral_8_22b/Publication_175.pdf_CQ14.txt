Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

a new task with a very limited dataset, usually only 1–5 thumbnails per 
class.

1)  We  trained  a  classic  DL  algorithms  architecture  with  our  biggest 

dataset AT0 as a baseline for the DL accuracy;  

2)  We  trained  the  same  DL  architecture  with  the  same  hyper- 
parameters (e.g. model architecture and training process) but on a 
much  more  limited  dataset  (AT1).  Hyper-parameters  are  the  pa-
rameters  defining  the  architecture (number of  layers, number and 
size of convolutions, connections between layers) and the training 
process  of  a  Deep  Model  (learning  rate,  neurone  activation,  back- 
propagation compotation).;  

3)  We trained the same DL architecture with limited datasets obtained 
by  subsampling  T0  to  250  and  500  images  per  class  (here  after 
“species” when we are referring to our experiments), corresponding 
to 2500 and 5000 thumbnails in AT0;

models  to  correctly  identify  species  on  an  independent  thumbnail 
dataset,  and  modelled  the  asymptotic  relationship  between  classifica-
tion accuracy and the number of thumbnails in the training datasets for 
both classic DL and FSL algorithms.

5 shots 

30 shots 

70.9 
28.8 
61.01 
32.55 
35.18 
51.18 
43.17 
51.07 
68.28 
55.44 
39.16 
41.35 
45.85 
58.08 
33.71 
65.44 
86.21 
45.7 
40.82 
50.69 
50.23 
14.75 

86.35 
42.66 
72.02 
40.64 
53.20 
79.29 
63.51 
70.63 
81.50 
67.22 
54.22 
58.87 
59.13 
61.00 
44.36 
84.86 
90.00 
70.60 
62.72 
55.70 
64.92 
14.55  

ResNet  DL  model  trained  on  smaller  AT1  (315  natural  thumbnails  in 
average  per  species)  during  the  second  experiment  showed  highly 
degraded  performance  with  a  mean  accuracy  of  only  42.21%  (SD  =
24.95%).  Among  species  variation  ranged  with  this  model  from  only 
3.49% (Chaetodon trifascialis) to 85.86% (Chaetodon auriga).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide explicit information about how the hyperparameters were optimized. However, it mentions that the same hyperparameters were used across different models and datasets, suggesting that they might have been determined beforehand rather than being optimized separately for each case. Common methods for hyperparameter optimization include grid search, which systematically explores all possible combinations within a predefined range, and random search, which samples random values from specified ranges. Other techniques like Bayesian Optimization or Gradient-based Optimization can also be employed. Without further details, it is impossible to determine the exact method used for hyperparameter optimization in this study.
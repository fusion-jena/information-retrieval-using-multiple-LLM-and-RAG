Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We further improved Waveman by modifying BatNet and opti-
mizing parameter setting of batch size. We add new kind of BNorm
layers behind the 22 convolutional layers to prevent overfitting when
we trained models using BatNet (Fig. 2C). Therefore the model
“learned” to generalize from a trend in both “known” and “unknown”
datasets rather than to maximize the performance on the “known”
datasets (usually called training datasets, Ioffe and Szegedy, 2015).
Computers can only train with small volumes of images at once as they
have too little Random Access Memory or Graphic memory. Batch size
was set to limit the image number. In this study, we set a large batch
size equal to 128, which means training with 128 images for each
iteration.

BatNet has a cascade of multiple layers of nonlinear processing units
for feature extraction from low-level features and syntheses to high-
level features (by converting data matrix shapes and synthesis the
matrix). Low-level features are some of raw data points (i.e., pixels in
images). High-level features include frequency, bandwidth, and hun-
dreds of other combined features which may have no specific meaning
in isolation. During synthesis as the data from “shallow” to “deep”
layers, the algorithm weights the features iteratively according to the
reference dataset in order to best describe call structure for specific bat
species. However, the acoustic signal synthesis with huge number of
layers usually leads to information loss to some degree (He et al., 2015).
We, thus, introduced eight shortcuts among layers in order to send
partial original information to the next layer (proposed by He et al.,
2015), which solved the information loss issue as layer number in-

Biological Conservation 241 (2020) 108269

Contents lists available at ScienceDirect

Biological Conservation

journal homepage: www.elsevier.com/locate/biocon

Automatic standardized processing and identification of tropical bat calls
using deep learning approaches

T

Xing Chena,1, Jun Zhaob,1, Yan-hua Chena, Wei Zhoub,*, Alice C. Hughesa,*
a Center for Integrative Conservation, Xishuangbanna Tropical Botanical Garden, Chinese Academy of Sciences, Menglun 666303, China
b Software School, Yunnan University, Kunming 650500, China

A R T I C L E I N F O

A B S T R A C T

Keywords:
Bats
Bioacoustics
Automated monitoring
Algorithms
Deep learning
Neural network
Automatic processing
Biodiversity metrics
Machine learning
Calls
Echolocation
Monitoring protocol

rate equaled 1e-3 (only a network called ResNet_v2 was set to 1e-5). We
trained a model using the training dataset (incl. 111,244 images) for
50–60 times using Graphic Processing Unit (GPU, Nvidia 1080ti, US)
for two hours and 10 min. After training the model contained refined
parameters and a graph which were saved for the downstream analysis.

2.2. Testing and further refinements of Waveman

We used filtered data (i.e., validation dataset in which 19,614
images were selected) to test different model and taxon bias. Then we
collected audio-files from a corridor for the 15 species to test how to
lower the misidentification and false positives. Finally, we optimized
the BatNet and collected audio from different human and natural
conditions to test the generalization of the model by recording the same
species under different conditions these audio-files are unfiltered and
were directly imported into Waveman.

2.2.1. Comparison of BatNet and other three networks

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit details about any preprocessing steps taken before training the deep learning model. However, it mentions that the authors modified BatNet and optimized the parameter settings of the batch size. They also added new kinds of BNorm layers behind the 22 convolutional layers to prevent overfitting during training. The batch size was set to limit the number of images processed at once due to limited computer resources. In this case, the batch size was set to 128, indicating that 128 images were used for each iteration of the training process. Additionally, the authors mention that they trained the model using a training dataset containing 111,244 images, but there is no information regarding any specific preprocessing steps applied to these images prior to training.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Model evaluation: testing

Table  1  shows  the  model’s  performance  on  the  test-
ing  images  (see  “Data  preparation”  section).  Model-2, 
with 37M parameters, outperformed Model-1 with 15M 
parameters in terms of the counting prediction, for both 
the  SSL  and  elephants’  data  sets.  Figure  4  shows  the 
actual vs. predicted number of animals. The diagonal red 
line represents the case of no errors (i.e., perfect predic-
tion). The closer the points are to the line, the better the 
prediction.

Fig. 3  Training loss curve: training loss function gradient vs. iteration curve for Basic UNet (Model-1) and UNet with EfficientNet-B5 feature extractor 
architecture (Model-2)

Table 1  Performance comparison of Model-1, Model-2 on test data set

Model

Feature extractor

Model-2

Model-1

Eff.Net-B5

No

SSL

RMSE

1.88

5.57

MAE

1.09

3.54

Elephant

RMSE

0.60

1.01

MAE

0.34

0.53

Parameters

≈37M
≈14M

The  Model-K  architecture  is  a  regression  model  based 
on  VGG16  without  the  feature  extractor  on  top.  The 
output layer was flattened and given as input to 2 fully 
connected  (FC)  layers  with  linear  output.  The  regres-
sion  model  was  designed  to  predict  classwise  (five 
categories)  count.  To  compare  it  with  the  proposed 
solution,  we  modify  the  model  by  connecting  the  out-
put  layer  with  a  fully  connected  one  output  neuron. 
Model-K  was  initialized  with  pre-trained  Imagenet 
weights  and  then  trained  using  our  training  data  set 
with  a  Stochastic  Gradient  Descent  (SGD)  optimizer 
and  an  MSE  loss  function.  The  proposed  Model-2 
with  EfficientNet  feature  extractor  reached  an  RMSE 
value  of  1.88  and  0.60  for  the  SLL  and  elephants’  data 
sets,  respectively,  performing  better  than  the  Model-K 
with  an  RMSE  of  2.17  and  0.81  for  SSL  and  elephants’

Page 6 of 10

EfficientNet-B5  feature  extractor  [27].  EfficientNet  is  a 
CNN  developed  by  Google,  characterized  by  high  accu-
racy  and  computational  efficiency.  Model-2  was  initial-
ized  by  pre-trained  weights  based  on  the  Imagenet  data 
set  [28].  All  the  parameters  were  optimized  using  the 
Adam optimizer with a learning rate of 0.001.

Training

An  Nvidia  GeForce  RTX  2060  GPU  was  used  for  train-
ing,  with  a  batch  size  of  8.  Model-1  (without  feature 
extractor)  was  trained  for  7  h  and  Model-2  (with  a  fea-
ture  extractor)  for  17  h.  Based  on  the  model’s  perfor-
mance on the validation set, the early stopping technique 
was  applied  to  avoid  over-fitting.  Model-2,  which  used 

pre-trained  weights  and  thus  some  prior  relevant  infor-
mation, converged faster than Model-1 (Fig. 3).

Model evaluation: testing

rate  of  extinction  of  species  is  accelerating  [3,  4].  Wild-
life  surveys  provide  species’  population  estimates  and 
are  conducted  for  reasons  such  as  species  management 
and  control,  ecological  and  biological  studies,  as  well  as 
long  term  trend  monitoring  and  behavioral  understand-
ing. This information may be essential for the survival of 
species and maintaining ecological balance. For example, 
biologists  use  population  trends  to  investigate  the  effect 
of  environmental  factors,  such  as  the  impact  of  human 
activity on a species’ population in some area/region.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To monitor the model performance during training, the authors employed several strategies. Firstly, they utilized a validation set to evaluate the model's performance after each epoch. By comparing the results obtained from this validation set, they could determine whether the model was improving or not. Secondly, they implemented the early stopping technique to prevent overfitting. Early stopping involves halting the training process once the model's performance on the validation set stops improving. In this study, the authors applied early stopping based on the model's performance on the validation set. Lastly, they visualized the training loss curve, which plots the training loss function gradient versus the iteration number. This graph helped them observe how the model's performance changed throughout the training process.
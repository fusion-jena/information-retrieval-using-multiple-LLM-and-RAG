Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

sponses  and  open-ended  queries  to  get  insight  into  the  best  way  the 
monitoring program can bring value to the dive community. Responses 
were compiled  and data was used to calculate the average SUS score 
(Kumar, 2020).

of standardisation, by focusing on the frequency and abundance of specific taxa, while retaining a recreational 
dive  plan.  Additionally,  the  app  also  collects  metadata  on  location,  number  of  dives  and  number  of  divers 
enabling normalisation based on “sampling effort”. In this pilot study, the use of the app was tested to compile 
information on the frequency and abundance of 18 marine taxa selected by local experts based on their con-
servation  status,  commercial  interest,  ecological  function  and/or  their  non-indigenous  origin.  Additionally,  a 
question-based survey was conducted to assess the opinion of users on the app’s usability and the potential value 
for the diving community/industry, showcasing a high usability score and interest among users. Basic statistical 
analysis of the data crowdsourced over the 1-month trial illustrates the potential and value of regional moni-

Group 

Algae 
Algae 
Algae 
Algae 
Coral 
Coral 
Coral 
Coral 
Mollusc 
Mollusc 
Crustacea 
Ascidea 
Fish 
Fish 
Fish 
Fish 
Reptile 
Mammal  

Weighted normalised abundance = Normalised abundance/Effort  

Effort = Number of divers x Dive time

Contents lists available at ScienceDirect 

Ecological Informatics 

journal homepage: www.elsevier.com/locate/ecolinf 

Crowdsourcing biodiversity data from recreational SCUBA divers using 
Dive Reporter 

Matej Buzinkai a, *, Marko Radeta a,b,c, Claudio Rodrigues a, b, Francisco Silva a, b, 
Ruben Freitas a, b, Sahar Chebaane a, Paola Parretti a, Susanne Sch¨afer a, Rodrigo Silva a, 
Francesca Gizzi a, Silvia Almeida a, Sonia K.M. Gueroun a, Jo˜ao Canning-Clode a, d, Jo˜ao 
Gama Monteiro a, e 
a MARE – Marine and Environmental Sciences Centre / ARNET - Aquatic Research Network, Agˆencia Regional para o Desenvolvimento da Investigaç˜ao Tecnologia e 
Inovaç˜ao (ARDITI), Funchal, Madeira, Portugal 
b Wave Labs, Faculty of Exact Sciences and Engineering, University of Madeira, Portugal 
c Department of Astronomy, Faculty of Mathematics, University of Belgrade, Serbia 
d Smithsonian Environmental Research Center, USA 
e Faculty of Life Sciences, Universidade da Madeira, Portugal

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide information about the evaluation metrics used for the deep learning model. However, common metrics used to evaluate the performance of deep learning models include accuracy, precision, recall, F1 score, and area under the receiver operating characteristic curve (AUROC). Accuracy measures the proportion of correct predictions out of total predictions made. Precision is the ratio of true positive predictions to the sum of true positives and false positives. Recall is the ratio of true positive predictions to the sum of true positives and false negatives. F1 score is the harmonic mean of precision and recall. AUROC is a measure of how well a binary classifier separates positive and negative classes.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4 https://github.com/HKathman/pam_annotation_experiments
5 https://github.com/kahst/BirdNET-Analyzer/tree/main/checkpoints/V2.4
6 https://tfhub.dev/google/vggish/1
7 https://tfhub.dev/google/yamnet/1
8 tensorflow.keras.applications.vgg16.VGG16(weights=’imagenet’).
9 tensorflow.keras.applications.resnet_v2.ResNet152V2

(weights=’imagenet’).

Fig. 4. UMAP plots for different embedding layers of different embedding
models for AnuraSet. For UMAP generation, we randomly select 5000 samples
and discard all samples that are aligned to more than one class. Colors and
shapes indicate the 10 classes with the highest occurrence frequency. Layers are
numbered according to their distance from the classification layer, e.g. ‘Bird-
Net-1’ is the last layer before the classification layer of the BirdNet model.

EcologicalInformatics82(2024)1027105H. Kath et al.

ResNet-50 variant (no transfer learning), and the small size of their
validation set (6.25 h, or 0.003% of the data) precludes comparing
different active learning methods. Similarly, van Osta et al. (2023) use
transfer learning (ResNet seemingly pre-trained on ImageNet) and an
active learning strategy to train a classifier for a single cryptic bird
species, but do not compare different active learning strategies.

entropy loss or custom loss functions as in (Tian et al., 2022). While the
observed low recall necessitates careful consideration, it's important to
clarify that the scope of this study didn't encompass the optimisation for
accuracy metrics, exemplified by F1 Score. Instead, our primary goal
was to identify efficient strategies that synergise transfer learning and
active learning. To potentially elevate accuracy, strategies such as
applying Per-Channel Energy Normalization (PCEN) (Lostanlen et al.,
2019), refining spectrogram feature engineering (Dufourq et al., 2022),
or employing transfer learning with fine-tuning could be explored.

Deep neural networks learn multiple representations of different
levels of abstraction: the first layers reflect low level input features,
while the last layers capture structure more directly related to the pre-
dictions output by the model (Bengio, 2009). We evaluate embeddings
at different layers within the CNNs. For VGG16 we investigate the last
three layers before the final classification layer (‘fc2’,
‘fc1’, and
‘flatten’). For ResNet152-V2 we only investigate the last embedding
layer (‘avg_pool’). Considering our future goal of implementing a real-
time pipeline with transfer learning and active learning, we decide not
to explore further layers of both visual domain models due to their large
dimensionality (100,352 for both models). Since the models pre-trained
on AudioSet were designed to be used as feature extractors, we only use
the last layer for VGGish and the penultimate layer for YAMNet. For
BirdNet we investigate the last three embedding layers, batch normali-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors did not explicitly mention any specific regularization techniques used in their deep learning pipelines to prevent overfitting. However, they mentioned using transfer learning, which can act as a form of implicit regularization by leveraging knowledge gained from training on other datasets. Additionally, they discussed applying entropy loss or custom loss functions, which may also contribute to preventing overfitting indirectly. The authors suggested exploring potential improvements like applying Per-Channel Energy Normalization (PCEN), refining spectrogram feature engineering, or employing transfer learning with fine-tuning to enhance accuracy. Nevertheless, without explicit information about regularization methods used, it cannot be confirmed if these techniques were employed specifically for preventing overfitting.
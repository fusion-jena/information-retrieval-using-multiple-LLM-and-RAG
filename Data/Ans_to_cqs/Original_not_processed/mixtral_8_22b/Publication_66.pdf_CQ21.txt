Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(b)

All GPS points

(c)

TDR recorded dives

(d)

TDR recorded non-dives  

(e)

Correctly predicted dives

(f)

Correctly predicted non-dives

(g)

Incorrectly predicted dives

(h)

Incorrectly predicted non-dives

BROWNING et al. 2041210x, 2018, 3, Downloaded from https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12926 by Thuringer Universitats- Und, Wiley Online Library on [16/11/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License688  |    Methods in Ecology and Evolu(cid:13)on

for	 two-		 and	 three-	states,	 respectively.	 Two-	state	 models	 were	 as-

sumed	to	represent	rest	and	foraging—or	diving	and	non-	diving-	,	and	

three-	state	models	included	an	intermediate	state.	The	HMMs	did	not	

predict	 diving	 behaviour	 as	 accurately	 as	 the	 deep	 learning	 models.	

Specificity,	 sensitivity,	 PPV	 and	 NPV	 were	 consistently	 lower	 for	 all	

species	using	both	two-		and	three-	state	models	(Table	4).	The	high-

est	 sensitivity	 was	 81.41%	 for	 shags	 using	 a	 three-	state	 HMM	 and	

specificity	was	77.50%	using	two	states.	There	was	a	trade-	off	where	

increasing	the	number	of	states	increased	sensitivity,	so	increasing	the	

accuracy	of	dive	behaviour	prediction,	but	a	decrease	in	non-	diving	or	

resting	behaviour	predictions,	the	specificity.

The	 predictions	 made	 using	 Naïve	 Bayes	 were	 poor	 for	 dives	

across	all	three	species	and	variable	combination,	although	non-	dives

individuals	by	training	deep	learning	models	to	predict	diving	in	European	shags,	

common	guillemots	and	razorbills.	We	validate	our	predictions	using	withheld	data,	

producing	 quantitative	 assessment	 of	 predictive	 accuracy.	 The	 variables	 used	 to	

Handling	Editor:	Jana	McPherson

train	these	models	are	those	recorded	solely	by	the	GPS	device:	variation	in	longi-

tude	and	latitude,	altitude	and	coverage	ratio	(proportion	of	possible	fixes	acquired	

within	a	set	window	of	time).

3.	 Different	combinations	of	these	variables	were	used	to	explore	the	qualities	of	differ-

ent	models,	with	the	optimum	models	for	all	species	predicting	non-diving	and	diving	

behaviour	correctly	over	94%	and	80%	of	the	time,	respectively.	We	also	demonstrate	

the	superior	predictive	ability	of	these	supervised	deep	learning	models	over	other	

commonly	used	behavioural	prediction	methods	such	as	hidden	Markov	models.

3 |  RESU LTS

3.1 | Model results

and	the	correct	predictions	produced	from	the	model	trained	with	all	

variables.	The	locations	with	the	highest	number	of	correctly	predicted	

dives	match	the	locations	with	the	greatest	number	of	true	dives.	The	

false	positives	and	negatives	are	also	shown,	demonstrating	the	accu-

racy	of	the	model.

3.1.1 | 10- fold cross- validation

3.1.2 | Leave- one- out cross- validation

Nearly	all	models	had	high	mean	specificity	values	above	90%,	mean-

The	 results	 of	 models	 trained	 with	 leave-	one-	out	 cross-	validation	

ing	 they	 predicted	 non-	diving	 events	 with	 high	 accuracy,	 and	 most	

showed	increased	variation	and	slightly	reduced	accuracy	compared	

had	mean	sensitivity	values	above	70%,	indicating	they	also	predicted	

to	10-	fold	cross-	validated	models	(Table	3;	see	Appendix	S6,	support-

diving	 events	 well	 (Table	2;	 see	 Appendix	 S5,	 supporting	 informa-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training process, several postprocessing steps can be performed depending on the specific requirements of the project or study. Some common postprocessing steps include:

1. Model evaluation metrics: Calculation of various performance metrics such as precision, recall, F1 score, accuracy, confusion matrix, ROC curves, etc., to evaluate the effectiveness of the model. These metrics help assess how well the model is performing and identify areas for improvement.

2. Cross-validation techniques: Techniques like k-fold cross-validation, leave-one-out cross-validation, or bootstrap sampling can be employed to validate the model's performance further and ensure its robustness. This helps prevent overfitting and provides more reliable estimates of the model's generalization error.

3. Visualizations: Creating visual representations of the data and model outputs, such as saliency maps, heatmaps, or feature importance plots, can provide insights into which features contribute significantly to the model's decisions. These visualizations also aid in understanding the underlying patterns in the data and identifying any potential biases or errors in the model.

4. Hyperparameter tuning: Adjusting hyperparameters, such as learning rate, regularization parameters, or network architecture, based on the model's performance during training and validation. This step aims to optimize the model's performance and improve its ability to generalize to new data.

5. Ensemble methods: Combining multiple models to create an ensemble, which often leads to improved overall performance compared to individual models. Common ensemble methods include bagging, boosting, stacking, and voting classifiers.

These postprocessing steps play a crucial role in ensuring the reliability and validity of machine learning models and should be carefully considered when developing and deploying them in real-world applications.
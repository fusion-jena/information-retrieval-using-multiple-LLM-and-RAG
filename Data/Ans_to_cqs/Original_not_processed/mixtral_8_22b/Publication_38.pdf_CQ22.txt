Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Our result that deep learning networks consistently outperformed other baselines, is consis-

precision greater than 0.95 at any recall level. The results for the best performing algorithm are underlined. Details of the test datasets and detection algorithms are given
in the text.

https://doi.org/10.1371/journal.pcbi.1005995.t001

The Random Forest baseline performed significantly better than the commercial systems
on the two challenging roadside recorded datasets (Fig 3A and 3B). This is a result of the train-
ing data and the underlying power of the model. However, unlike our CNNs, the simple tree
based model is limited in the complexity of the representations it can learn, which results in
worse performance. For the static Norfolk Bat Survey its performance more closely matches
that of SonoBat, but with improved recall.

59.

van den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, et al. Wavenet: A generative
model for raw audio. arXiv preprint arXiv:160903499. 2016.

60. Hochreiter S, Schmidhuber J. Long short-term memory. Neural computation. 1997; 9(8):1735–80.

PMID: 9377276

61. Kosmala M, Wiggins A, Swanson A, Simmons B. Assessing data quality in citizen science. Frontiers in

Ecology and the Environment. 2016; 14(10):551–60. https://doi.org/10.1002/fee.1436

62. Welinder P, Branson S, Perona P, Belongie SJ, editors. The multidimensional wisdom of crowds.

Advances in neural information processing systems; 2010.

63. Swanson A, Kosmala M, Lintott C, Packer C. A generalized approach for producing, quantifying, and

validating citizen science data from wildlife images. Conservation Biology. 2016; 30(3):520–31. https://
doi.org/10.1111/cobi.12695 PMID: 27111678

were represented as a single point on the precision-recall curves. We also report recall at 0.95
precision, a metric that measures the fraction of calls that were detected while accepting a false
positive rate of 5%. Thus a detection algorithm gets a score of zero if it was not capable of
retrieving any calls with a precision greater than 0.95.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The performance of the deep learning models is evaluated using several metrics such as precision, recall, and precision-recall curves. Precision refers to the proportion of true positive predictions made by the model out of all the positive predictions. Recall, on the other hand, measures the proportion of actual positives that were correctly identified by the model. In this case, the authors use precision greater than 0.95 at any recall level as one of their evaluation metrics. They also report recall at 0.95 precision, which measures the fraction of calls that were detected while accepting a false positive rate of 5%. Additionally, they represent the results as a single point on the precision-recall curves.
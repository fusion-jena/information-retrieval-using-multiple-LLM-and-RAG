Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

model that adjusts too well to the training data, but it does not generalize to the unseen
before dataset, i.e., a test dataset), after 50,000 iterations, the learning rate was reduced
following an exponential decay parameter of 0.5 by an SGD scheduler. Aside from this,
we used rotation, noise, and ﬂip (as in [50]) for data augmentation, and we were capable
of augmenting the dataset by six times. With the data augmentation technique, we can
make the CNN classiﬁcation more robust and generalize better. In Figure 9, we can see the
schematic diagram for the evaluation process.

The most signiﬁcant variation of performance is concerning the number of parameters
and with training and inference time. Despite being the best architecture in performance,
According to Table 2, DeepLabv3+ needed more parameters than the other architectures,
about 2.75 times more parameters than the U-Net, the least requiring one. The need for a
more signiﬁcant number of parameters often implies a higher demand for training samples
that our dataset or another dataset may not have met that the methods present in this
research paper may be applied, possibly causing the DeepLabV3+ architecture to perform
below its potential.

Remote Sens. 2021, 13, 3054

11 of 19

Figure 11. Convergence of the evaluated networks.

Table 2. Number of Parameters and Processing Time of the proposed approaches. The training time
represents the results for the test set of each method. The inference time stands for the time taken by
each model to make predictions for each image.

Method

FCN

U-Net

SegNet

Method

FCN

U-Net

SegNet

DeepLabV3+

DDCN

Number of Parameters
(in millions)

3.83

1.86

2.32

Training Time
(GPU hours)

Inference Time
(GPU min.)

Inference Time
(CPU min.)

Inference Time
(GPU min./ha)

Inference Time
(CPU min./ha)

485

450

472

1.4

1.9

1

1.3

1.1

1.5

5.16

486

1.4

1.9

2.08

500

5.1

6.2

0.042

0.030

0.033

0.042

0.153

0.057

0.039

0.045

0.057

0.186

3.3. Visual Analysis

Some features maps, learned by the convolutional layers, are presented in Figure 12.
Speciﬁcally, this image presents low-, mid- and high-level feature maps learned by the
ﬁrst, a middle, and the last layers of the networks, respectively. We can see the each CNN
performs very differently from one another.

Dynamic Dilated Convolutional Network (DDCN): Proposed by [35], the DDCN is
designed to preserve the input image resolution. Ref. [43] describe this network in more
detail. However, in summary, the Dynamic Dilated Convolutional Network uses a multi-
scale training strategy that implements dynamically-generated input images to converge a
dilated model that does not downsample the input data due to a speciﬁc conﬁguration of
stride and padding. The model has eight dilated blocks; each block comprises a dilated
convolution and a pooling layer; the blocks are followed by a standard convolutional layer
responsible for the ﬁnal prediction map. In each iteration of the training procedure, a
dimension is randomly selected from this distribution and used to create a new batch. The
model captures multi-scale information by processing these batches with a pre-determined
size, advancing in the processing phase. The network selects, based on scores obtained

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't any explicit mention of specific regularization methods such as dropout or L2 regularization used to prevent overfitting in the deep learning pipeline. However, some techniques mentioned in the text could potentially help mitigate overfitting indirectly. For instance, data augmentation techniques like rotation, noise, and flip were employed to increase the size of the dataset by six times. This approach helps improve the model's ability to generalize better and makes the CNN classification more robust. Additionally, the learning rate was reduced using an exponential decay parameter of 0.5 by an SGD scheduler after 50,000 iterations. This step might also contribute to preventing overfitting since reducing the learning rate can stabilize the optimization process and avoid overshooting the optimal solution. Nevertheless, without further details about the implementation of the models, it cannot be confirmed if additional regularization methods were utilized.
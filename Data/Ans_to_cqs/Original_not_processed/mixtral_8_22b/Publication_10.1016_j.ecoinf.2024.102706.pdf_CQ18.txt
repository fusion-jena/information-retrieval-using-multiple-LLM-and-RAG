Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

tible to overfitting; therefore, it is crucial to address this issue by tuning
the hyperparameters.

Model

Train

Test

Linear Regression
Elastic Net
Support Vector Machine
Random Forest
Extreme Gradient Boosting
Light Gradient Boosting Machine

R2

0.58
0.58
0.66
0.66
0.66
0.69

R2

0.59
0.59
0.63
0.64
0.64
0.64

RMSE

rRMSE (%)

MAE

0.17
0.18
0.17
0.16
0.16
0.16

23.28
23.30
22.26
22.00
21.95
21.92

0.14
0.14
0.12
0.13
0.12
0.12

cover. The interquartile range for observed: Q1 = 65%, Q2 (median) =
86%, and Q3 = 96%, and predicted: Q1 = 63%–69%, Q2 = 83%–87%,
and Q3 = 88% –91%. The minimum and maximum canopy cover for
observed and all the models are 0% and 100%, respectively. Similarly,
the density of the observed and predicted canopy cover is higher in Q3 as
the violin's shape is expanded in those regions.

3.3. Variable importance

linear to machine learning methods, were fitted in training data to
construct regression equations. We used multiple linear regression, Enet,
SVM, RF, XGboost and LightGBM to predict canopy cover from planet
data. In general, all the mentioned models are used in machine learning
as they use algorithms to automatically learn patterns and relationships
from data to make predictions or decisions. However, we categorised

EcologicalInformatics82(2024)1027065A. Gyawali et al.

Table 4
The optimised hyperparameter values with grid search range in machine
learning models.

Model

Hyperparameters

Grid search

Optimised value

Enet

SVM

RF

Xgboost

LightGBM

α
l1_ratio

Cost (C)
gamma
Kernal
epsilon

n_estimators
max_features
max_depth
min_samples_split
min_samples_leaf

n_estimators
max_depth
colsample_bytree
min_child_weight
subsample
learning_rate

n_estimators
max_depth
learning_rate
num_leaves
feature_fraction
bagging_fraction
bagging_freq

For machine learning models, the GARI, GI, GNDVI, GREEN, MSR,
NDVI, RDVI, RED, SI2, and VARIg were selected as described earlier in
the methodology sections. The performance of the models is presented
in Table 6. In the testing dataset, it was observed that all models
demonstrated lower coefficients of determination than the training data,
except for the linear and Enet models. Machine learning algorithms
outperformed linear in modelling forest canopy cover. The LightGBM
exhibited the most substantial model training ability (R2 = 0.69), fol-
lowed by SVM, RF and XGboost, each with an R2 value of 0.66. All six
models yielded satisfactory results with an R2 ≥ 0.58 in training data,
meeting our study's canopy cover prediction criteria. Further evaluation
in independent
test samples, considering additional fit statistics,
XGboost and LightGBM demonstrated superior predictive capabilities
with R2 = 0.64, RMSE = 0.16, rRMSE = 22%, and MAE = 0.12, out-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To monitor the model performance during training, a strategy involving the use of various metrics such as R2, RMSE, rRMSE, and MAE was employed. These metrics helped evaluate the accuracy and reliability of the models in predicting forest canopy cover. Additionally, the models were trained using different datasets - one for training and another for testing. This allowed for the comparison of model performance between these two sets, ensuring that the models did not suffer from overfitting. Furthermore, the variable importance was also considered while fitting the models into the training data. Lastly, the optimized hyperparameter values were determined through a process called grid search, which involved trying out different combinations of hyperparameters within specified ranges until the best combination was found.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

mapping probability samples that are exclusively used for map evalua-
tion are often not available and therefore alternative methods have been 
proposed.  In  machine  learning,  if  data  are  abundant,  a  common 
approach is to randomly divide the full dataset used for modelling into 
three parts: a training set, a validation set, and a test set (Hastie et al., 
2009,  Chapter  7).  The  training  set  is  used  for  fitting  the  models,  the 
validation set is used to estimate prediction error for model selection and 
hyperparameter tuning, while the test set is used for assessing the ac-
curacy of the final model. This paper addresses this latter testing phase, 
with the specific aim to assess the accuracy of a thematic map produced 
by a calibrated statistical prediction method. Data availability is often 
limited  so that setting  aside a  test set  cannot always be afforded and 
therefore resampling methods are used (Hastie et al., 2009; Steele et al.,

by a two-dimensional kernel approach (see Section 2.1.3). The LOESS 
smoothing parameter was set to 0.5. The observed conventional cross- 
validation  residuals  were  next  transformed  through  division  by  the 
local σ(si) after subtracting ̂μOK  (Eq. (3)): 
′ (si) = (z(si) (cid:0) ̂zm(si ) (cid:0) ̂μOK )/σ(si)

If the full sample dataset is acquired by simple random sampling and 
if k equals the sample size (i.e. leave-one-out cross-validation, hereafter 
LOOCV), estimation from cross-validation is known to be nearly unbi-
ased  (Bengio  and  Grandvalet,  2004;  Krzanowski,  2001;  Steele  et  al., 
2003).  Since  the  computational  burden  of  LOOCV  is  heavy,  k  is 
conventionally set to five or ten, in which case bias is no longer negli-
gible but can be parametrically corrected (Fushiki, 2011). However, the 
sample datasets used for mapping environmental variables generally are 
not  acquired  by  simple  random  sampling.  Rather,  they  are  amalgam-
ations  of  several  datasets,  each  with  its  own  formal  or  opportunistic 
sampling  design,  which  strictly  impedes  design-based  inference.  The 
latter also applies to probability samples whose inclusion probabilities 
are not published, such as the LUCAS dataset (d’Andrimont et al., 2020).

We have proposed an inverse sampling-intensity weighted and two 
geostatistical model-based cross-validation approaches as alternatives to 
the  broadly  propagated  spatial  cross-validation  method  that  tends  to 
produce  pessimistically biased map accuracy estimates. The proposed 
methods  can  be  characterised  as  quasi-randomization  and  super-
population  modelling  approaches,  respectively.  Like  blocked  spatial 
cross-validation, our weighted approach is a heuristic method. However, 
in  contrast  to  the  former  it  explicitly  addresses  the  spatial  clustering 
problem rather than the incorrectly posed problem of spatial proximity 
of test and training data. In our case studies, bias in the map accuracy 
metrics assessed over multiple realizations of the sampling designs by 
weighted cross-validation was much smaller than that of blocked spatial 
cross-validation for non-clustered to moderately clustered samples. For

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given text does not provide information on regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. Instead, it discusses various methods for evaluating the accuracy of maps produced by calibrated statistical prediction methods when limited data is available. These include using a training set, a validation set, and a test set, as well as resampling methods like leave-one-out cross-validation (LOOCV) and other forms of cross-validation. The text also mentions the use of a two-dimensional kernel approach and LOESS smoothing parameter for transforming observed conventional cross-validation residuals.
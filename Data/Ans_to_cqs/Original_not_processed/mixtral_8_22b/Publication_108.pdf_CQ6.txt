Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The overall workﬂow of preprocessing, network design, and prediction are shown in Figure 2.
Due to the size of the orthophotos and due to memory limitations when doing computations, it is
intuitive to process the orthophotos into relatively small labeled tiles (or rather small image cubes) and
feed them into the CNN. Each orthophoto of size 10,000 × 10,000 pixels was divided into 1525 tiles of
256 × 256 pixels. The data (orthophoto and corresponding labels) were then split into three datasets:
training (80%), validation (20%), and test (two full images of 10,000 × 10,000 pixels). The training
data were used for optimizing the neural network, while the validation dataset was used to assess the
performance during the training process. The test dataset was used to assess the performance of the
ﬁnal optimized neural network. The training and validation images were read into arrays with the

2.5.1. Encoding Path

The encoding path of our network was composed of three encoding blocks; each block was
composed of a convolutional layer with a ﬁlter of size (3, 3) and a ReLU activation function [32] ,
a dropout layer to force each neuron to learn more than only one feature, a second convolutional layer
followed by ReLU activation, and a max pooling layer with the size (2,2), which replaced each 2 × 2
region on the feature map by the region’s maximum value and thus decreased the size of the feature
map by keeping only the highest values.

2.5.2. Decoding Path

31. Kingma, D.P.; Ba, J. Adam: A Method for Stochastic Optimization. arXiv 2015, arXiv:1412.6980.
32. Nair, V.; Hinton, G.E. Rectiﬁed linear units improve restricted boltzmann machines. In Proceedings of the
27th International Conference on Machine Learning (ICML-10), Haifa, Israel, 21–24 June 2010; pp. 807–814.
33. Veliˇckovi´c, P. Be nice to your neurons: Initialisation, Normalisation, Regularisation and Optimisation.

In Introduction to Deep Learning (COMPGI23); University College London: London, UK, 2017.

34. Moré, J.J. The Levenberg-Marquardt algorithm: implementation and theory. In Numerical Analysis; Springer:

Berlin, Germany, 1978; pp. 105–116.

35. Wang, W.; Qu, J.J.; Hao, X.; Liu, Y.; Stanturf, J. Post-hurricane forest damage assessment using satellite

remote sensing. Agric. For. Meteorol. 2010, 150, 122–132. [CrossRef]

36. Remelgado R.; Notarnicola, C.; Sonnenschein, S. Forest damage assessment using SAR and optical data:

Table 1. Hyperparameter ﬁne-tuning results.

Scenario Number of Blocks Number of Filters

Learning Rate Mean IoU Accuracy

1
2
3
4
5
6
7
8
9
10

3
4
5
6
4
4
4
4
4
4

64 , 64, 64
64, 64, 64, 64
64, 64, 64, 64, 64
64, 64, 64, 64, 64, 64
16, 32, 64, 128
32, 64, 128, 256
64, 128, 256, 512
16, 32, 64, 128
16, 32, 64, 128
16, 32, 64, 128

0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.01
0.0005
0.00001

0.30
0.38
0.36
0.32
0.42
0.38
0.31
0.008
0.42
0.39

82%
89%
86%
86%
94%
88%
84%
71%
94%
90%

Scenario 5 was selected as the optimal solution as it had the best values for IoU and accuracy
while training on fewer epochs. The resulting architecture is shown in Figure 4 and described in
the following.

2.5.1. Encoding Path

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model such as a Convolutional Neural Network (CNN), several preprocessing steps can be applied to the input data. These steps may include normalization, scaling, cleaning, or even transforming the data into smaller chunks.

In this case, the authors processed large orthophotos into smaller labeled tiles (image cubes) of 256 x 256 pixels due to memory limitations and computation efficiency. This step can be considered as a form of data transformation where larger images are broken down into manageable sizes.

Normalization is another common preprocessing technique used to scale pixel intensity values between 0 and 1 or -1 and 1. However, there isn't any explicit mention of normalization being performed in this specific scenario.

Cleaning the data might involve removing irrelevant features, handling missing values, or correcting inconsistencies. Again, no specific information about these processes has been mentioned here.

Therefore, based on the given context, the primary preprocessing step involves dividing large orthophotos into smaller tiles for efficient processing and training of the CNN.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.1.4. Fieldwork 

Deep learning models are usually defined as “data hungry” because 
their accuracy is heavily dependent on the amount of high quality an-
notated data used to learn the correspondence between the desired la-
bels  and  image  information.  Thus,  to  develop  a  model  capable  of 
matching human expertise on sub-alpine shrub species identification, a 
large and accurate dataset with images and their corresponding labels is 
crucial. As we will describe in detail in later sections, determining the 
exact species of every shrub from images is a difficult task for human 
experts. For our study, fieldwork expeditions were key to confirm each 
of the selected shrub images and validate the orthomosaic annotations. 
Three  fieldwork  missions  were  conducted  during  2021  (June,  August 
and October), to account for seasonal changes in color and flowering 
patterns that helped identify each individual species. 

3.2. Observer study

the  experts.  Then  the  network  is  fed  this  annotated  data  and  the  pa-
rameters of the model are adjusted following an optimization process. 
Once this training process is finished, the model is ready to process new 
unseen  data.  DL  networks  are,  thus,  able  to  capture  statistics  and  re-
lationships directly from the data following human expertise. If the data 
used  for  training  the  model  is  representative  of  the  problem  and  the 
annotations are carefully generated, the resulting network can perform 
at a similar level with any input data set. This capacity to translate the 
training to other independent datasets is called the generalization power 
of the network. In practice, differences in performance between the re-
sults with the training data (or data that is similar to it, i.e., from the 
same  site  and  acquired  at  the  same  time)  and  those  of  independent 
datasets (often in research papers referred to as the testing data set) can

conditions or with trees from a different site). Consequently, when there 
is  data  leakage,  properties  of  the  testing  data  leak  into  the  training 
process,  the  estimation  of  the  generalization  power of  the  network is 
compromised. See (Diez et al., 2021), section 3.1 for more details.

• Finally, a set of 457 patches of a fixed size (100 × 100-pixels) were 
randomly  displayed  sequentially  one  by  one,  with  no  context 
regarding  their  location  in  the  orthomosaic  and  site.  This  set  of 
patches was randomly sampled to mimic the species distributions in 
the testing of the DL networks, while halving the number of patches 
to reduce the impact of fatigue and subsequent misclassifications and 
keep the total survey time manageable. For each patch, the observers 
were asked to press a key from 0 to 5 representing each of the six 
possible classes. 

Due to the challenges of this task, the results of the observed study 

were analysed at two levels of detail: 

EcologicalInformatics80(2024)1024626K. Moritake et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, some strategies can be inferred based on common practices in machine learning and deep learning. One strategy mentioned indirectly is using a large and diverse dataset to train the model, which helps mitigate the effects of random initialization and improves the model's ability to generalize across various conditions. Another approach could involve setting specific random seeds before running experiments to ensure reproducibility and consistency in results. Additionally, techniques like cross-validation and ensemble methods can help address variability due to randomness in the training process.
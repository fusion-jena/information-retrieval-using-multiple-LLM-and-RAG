Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Data augmentation was used to collect all possible cases. For data 
augmentation,  we  implemented  Kera’s  flow_from_directory  method 
during the learning phase of the CNN. The applied data augmentation 
techniques used for plant image classification include random rotation, 
random cropping, random flipping, random color jittering, and random 
noise addition. This ensures that we have covered all the possible cases 
that might occur in the real world. 

6.2. Experiment setup

EcologicalInformatics81(2024)1025817S. Ghosh et al.                                                                                                                                                                                                                                   

Table 4 
Search space of CNN hyperparameters to be explored.  

Hyperparameters 

Range 

Convolution Layers 
Filters 
Filter Size 
Number of Neuron 
Batch Size 
Epochs 
CNN Model Optimizer 

Lower limit = 1 and Upper limit = 10 
Lower limit = 1 and Upper limit = 64 
Lower limit = 1 and Upper limit = 10 
Lower limit = 32 and Upper limit = 1024 
Lower limit = 8 and Upper limit = 512 
Lower limit = 1 and Upper limit = 25 
ADAM, SGD, RMSProp, Adadelta, Adagrad, Adamax

ImageNet)  to  be  adapted  for  plant-specific  classification  tasks.  Re-
searchers have achieved improved classification accuracy with smaller 
training datasets by leveraging the learned representations from these 
general-purpose models. Too et al. (Too et al., 2018) demonstrated the 
effectiveness  of  fine-tuning  pretrained  CNN  models  for  plant  disease 
classification.  Despite  advancements  in  plant  image  classification, 
several challenges persist. One significant challenge is the presence of 
interclass  similarities  and  intraclass  variations  in  plant  species. 
Addressing  these  challenges  requires  robust  feature  representations, 
data  augmentation  techniques,  and  advanced  network  architectures. 
Additionally,  the  interpretability  of  deep  learning  models  and  their

hyperparameters.  The  hyperparameters  with  the  current  results  are 
saved until better accuracy is achieved. The termination criterion of the 
proposed approach is the maximum number of iterations to be executed. 
After the completion of the iterations, the proposed approach provides 
us with the best hyperparameters, thus evolving the near-optimal CNN. 
The  overall  complexity  is  dominated  by  the  iterations,  “TC”,  and 
within  each  iteration,  the  operations  depend  on  the  population  size 
“NP”.  Therefore,  the  total  complexity  can  be  approximated  as 
O(TC*NP).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The data augmentation techniques applied during the learning phase of the Convolutional Neural Network (CNN) include random rotation, random cropping, random flipping, random color jittering, and random noise addition. These techniques ensure that a wide range of possible cases are considered, which helps improve the model's performance when dealing with real-world scenarios.

Random rotation involves rotating images by a certain angle, while random cropping entails selecting a portion of the original image randomly. Random flipping refers to horizontally or vertically flipping the input images. Random color jittering modifies the brightness, contrast, saturation, and hue of the input images, whereas random noise addition introduces Gaussian or salt-and-pepper noise into the images. All these techniques help increase the diversity of training samples, making the model more robust and generalizable.
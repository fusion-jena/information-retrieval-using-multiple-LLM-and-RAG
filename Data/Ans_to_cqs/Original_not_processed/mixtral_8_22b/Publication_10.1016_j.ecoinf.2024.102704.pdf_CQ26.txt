Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

targeted fish species dataset. This dataset was then divided into training 
and validation sets in an 8:2 ratio using a random selection process.

Zhang, Q.L., Yang, Y.B., 2021, June. Sa-net: Shuffle attention for deep convolutional 

neural networks. In: ICASSP 2021–2021 IEEE International Conference on Acoustics, 
Speech and Signal Processing (ICASSP). IEEE, pp. 2235–2239. https://doi.org/ 
10.1109/ICASSP39728.2021.9414568. 

Zhang, C., Li, P., Sun, G., Guan, Y., Xiao, B., Cong, J., 2015, February. Optimizing FPGA- 
based accelerator design for deep convolutional neural networks. In: Proceedings of 
the 2015 ACM/SIGDA International Symposium on Field-programmable Gate 
Arrays, pp. 161–170. https://doi.org/10.1145/2684746.2689060. 

Zhang, T., Zhang, X., Shi, J., Wei, S., 2019. Depthwise separable convolution neural 

network for high-speed SAR ship detection. Remote Sens. 11 (21), 2483. 

Zhang, T., Yang, Y., Liu, Y., Liu, C., Zhao, R., Li, D., Shi, C., 2024. Fully automatic system 
for fish biomass estimation based on deep neural network. Eco. Inform. 79, 102399 
https://doi.org/10.1016/j.ecoinf.2023.102399.

several  times  higher  than  those  of  other  operations  (Li  et  al.,  2018). 
Therefore,  reducing  memory  access  is  the  key  to  further  enhancing 
model  performance.  By  reconfiguring  the  computation  sequence  of 
‘convolutional layer + batch normalisation + activation layer on NVI-
DIA TESLA V100 GPUs, Wang et al. (2019) reduced memory access by 
33%,  22%,  and  31%  for  the  ResNet-50,  Inception  V3,  and  DenseNet 
models,  respectively,  leading  to  increases  in  the  computational  effi-
ciency  of  20.5%,  18.5%,  and  18.1%.  Lowering  the  memory  access  in 
models can accelerate operations across the three main layers of CNNs, 
thus fully harnessing the model's strengths and more effectively utilising 
the computational resources provided by the embedded devices (Gilan 
et al., 2019; Zhang et al., 2015). Therefore, by reducing the number of 
layers  in  the  network  model,  the  number  of  convolutions  and  data

Li, J., Yan, G., Lu, W., Jiang, S., Gong, S., Wu, J., Li, X., 2018, March. SmartShuttle: 
Optimizing off-chip memory accesses for deep learning accelerators. In: 2018 
Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 
pp. 343–348. 

Li, J., Liu, C., Lu, X., Wu, B., 2022a. Fish passage monitoring based on improved 

YOLOv5s and TensorRT deployment. Trans. Chin. Soc. Agric. Machin. 12, 314–322. 
Li, J., Liu, C., Lu, X., Wu, B., 2022b. CME-YOLOv5: an efficient object detection network 

for densely spaced fish and small targets. Water 14 (15), 2412. 

Li, W., Li, F., Li, Z., 2022c. CMFTNet: multiple fish tracking based on counterpoised 

JointNet. Comput. Electron. Agric. 198, 107018. 

Wang, J.J., Hao, Z.Y., Li, H.L., 2019. Training memory access optimization for 

convolutional neural networks. J. Xi’an Univ. Electron. Sci. Technol. 02, 98–107. 

Wang, Q., Wu, B., Zhu, P., Li, P., Zuo, W., Hu, Q., 2020. ECA-net: efficient channel

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide specific information about addressing data bias during preprocessing of the deep learning pipeline. However, some commonly used techniques to handle data bias include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into training and validation sets while maintaining the same class distribution in both sets. Oversampling is a technique where samples from the minority class are duplicated or synthesized to balance the class distribution. Undersampling, on the other hand, involves removing samples from the majority class to achieve balanced classes. Lastly, collecting diverse data helps ensure that the trained models generalize well to different scenarios and populations.

In summary, although the given context does not explicitly mention these techniques, they are widely used to address data bias during preprocessing of the deep learning pipeline. These methods include stratified splitting, oversampling, undersampling, and diverse data collection.
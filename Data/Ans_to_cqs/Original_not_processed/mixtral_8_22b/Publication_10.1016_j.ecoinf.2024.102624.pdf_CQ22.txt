Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The  reference  dataset  was  split  into  training  and  testing  sets  at  a 
70:30 ratio, and we applied two types of accuracy assessments. (i) In the 
model  building  phase,  using  the  training  dataset,  we  applied  10-fold 
cross-validation  with  three  repetitions.  The  training  dataset  was 
randomly split into 10 folds; 9 folds were used for model training and 1 
for testing, and this procedure ended when all folds were testing data. 
The whole sequence was repeated using two new random splits (Abriha 
et al., 2023; Molinaro et al., 2005). This type of accuracy assessment, 
with  hyperparameter  tuning,  helps  to  find  the  best  models  with  the 
largest  overall  accuracy  (OA),  and  also  reflects  the  resulting  30  OAs 
distribution  models  with  minimums,  maximums,  quartiles,  and  me-
dians. The smaller the range of possible model outcomes (OAs), the more 
reliable the prediction. (ii) Based on the testing dataset, we predicted the

We  classified  all  the  input  dataset combinations  (10  variable  sets; 
Table  2)  with  each  classifier  (RF,  SVM,  MARS)  and  with  two  sets  of 
classes (7C and 13C); thus, we had 60 models as a result of the classi-
fications. Classifications and hyperparameter tuning were conducted in 
R  4.2  (R  Core  Team,  2023)  with  the  caret  package  (Kuhnaut  et  al., 
2022). 

2.4. Accuracy assessment

indices,  the  Run  percentage  performed  well,  with  the  highest  impor-
tance in previous research (Schlosser et al., 2020), thus, we calculated 
the Run percentage (Eq. 4), using the 8-bit version of the NDVI layer in 
the Orfeo Toolbox in the QGIS 3.16 environment (Grizonnet et al., 2017; 
Miyamoto and Jr, 2008) with nine different kernel and offset parame-
ters. NDVI was selected because it preserved band information, repre-
sented  biomass,  and  guaranteed  visually  distinct  features  for  various 
genus groups. The following parameters were applied: two, three, and 
five kernel (k) options and one, three, and five offset (o) options in all

models and calculated the user's accuracies (UAs) and producer's accu-
racies (PAs).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, there is no explicit mention of specific evaluation metrics such as accuracy, precision, or recall being used to measure the performance of a deep learning model. However, the text does discuss the use of accuracy assessments in evaluating the performance of their models. Two types of accuracy assessments are mentioned: (i) 10-fold cross-validation with three repetitions, which aims to find the best models with the largest overall accuracy (OA), and (ii) predictions based on the testing dataset, where they calculate user's accuracies (UAs) and producer's accuracies (PAs). Therefore, from the given context, it can be inferred that the authors primarily focus on overall accuracy (OA), user's accuracies (UAs), and producer's accuracies (PAs) as their chosen evaluation metrics.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Active  learning  iterations  used  a  batch  size  of  64,  ten  epochs  and 
learning rate of 0.001. We used a grid search technique (Mohri et al., 
2018) to tune hyperparameters of the final model including the number 
of epochs, batch size and learning rate. 

2.3.3. Active learning framework 

We  applied  an  active  learning  approach  to  iteratively  train  and 
improve  the  CNN  model.  The  active  learning  approach  is  depicted 
within Fig. 3 and described below.

validation  (Section  2.3)  and  test  datasets  (Section  2.4)  prior  to  the 
commencement of CNN development. The training/validation dataset 
comprised  6363  h  of  audio  from  ten  independent  sites  (five  solar- 
powered bioacoustic recorders and five Audiomoth recorders) and the 
test dataset comprised 2735 h of audio from nine independent sites (four 
solar-powered bioacoustic recorders and five Audiomoth recorders) that 
were different from the training/validation dataset. All audio data were 
unlabelled at the commencement of the study. 

All recorders were set to record 10-min files continuously between 6 
am and 6 pm daily, which reflected the diurnal activity patterns of SBTF. 
We stored audio data as FLAC files at a sampling rate of 44.1 kHz for the 
solar-powered bioacoustic recorders and WAV files at a 32 kHz sampling 
rate for the Audiomoth recorders. 

2.3. CNN development 

2.3.1. Data pre-processing

1.10),  an  open  source  ML  library  (Paszke  et  al.,  2019).  We  used  a 
ResNet-34  model  with  pre-trained  weights  for  the  CNN  architecture. 
ResNet models typically achieve high-performance in image and audio 
recognition tasks (Bergler et al., 2022; He et al., 2016; Stowell et al., 
2019) and have been widely applied for automated wildlife image and 
call recognition (Kahl et al., 2021; Sankupellay and Konovalov, 2018; 
Stowell,  2022).  Our  training  dataset  was  imbalanced,  with  a  lower 
number of audio frames containing a SBTF call than no-SBTF call. We 
followed the recommendations of Buda et al. (2018) and oversampled 
the SBTF audio frames with the WeightedRandomSampler function in 
Pytorch.  We  used  an  Adam  optimiser  algorithm  with  an  exponential 
learning rate decay function, which is a common method of learning rate 
optimisation  used  for  CNN  training  (Kingma  and  Ba,  2014).  We then

Sankupellay, M., Konovalov, D., 2018. Bird call recognition using deep convolutional 

neural network, ResNet-50. In: Proceedings of Acoustics. 

Sekercioglu, C.H., et al., 2008. Climate change, elevational range shifts, and bird 

extinctions. Conserv. Biol. 22 (1), 140–150. 

Settles, B., Craven, M., 2008. An analysis of active learning strategies for sequence 

labeling tasks. In: Proceedings of the 2008 Conference on Empirical Methods in 
Natural Language Processing, pp. 1070–1079. 

Teixeira, D., et al., 2022. Fledge or fail: Nest monitoring of endangered black-cockatoos 
using bioacoustics and open-source call recognition. Ecol. Inform. 69, 101656. 
Thakur, A., Thapar, D., Rajan, P., Nigam, A., 2019. Deep metric learning for bioacoustic 

classification: Overcoming training data scarcity using dynamic triplet loss. 
J. Acoust. Soc. Am. 146 (1), 534–547. 

Threatened Species Scientific Committee (TSSC), 2005. Commonwealth Listing Advice

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The datasets used in this deep learning pipeline are not explicitly mentioned by name but are referred to as the training/validation dataset and the test dataset. The training/validation dataset consisted of 6363 hours of audio from ten independent sites, recorded by both solar-powered bioacoustic recorders and Audiomoth recorders. The test dataset included 2735 hours of audio from nine independent sites, also recorded by both types of devices. These datasets were unlabelled at the start of the study. It appears that these custom datasets were created specifically for this project rather than being standardized datasets like MNIST, CIFAR, or ImageNet.
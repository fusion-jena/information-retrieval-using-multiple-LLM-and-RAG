Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

datasets. Conversely to the above analyses, here the model was

tion to the classifier (Figure S1). The training procedure then optimizes

trained to classify datasets instead of taxa. Then, a classification error
of the SVM, Îµsource(cid:4)target, was measured as a proportion of incorrect
predictions of 200 test images sampled from the two datasets. An

the model parameters in the shared part of the network to not only

minimize the loss of the label classifier (taxon prediction) but at the

same time to maximize the loss of the domain classifier (dataset predic-

intuitive interpretation of this measure is that the dataset classifica-

tion). This adversarial training procedure optimizes shared intermediate

tion task is harder when the feature distributions between two data-

features to be invariant between the two domains, and hence the

sets are more similar. Therefore, a large classification error indicates

model can generalize across them, which potentially improves the accu-

parate ecosystems, share a similar set of taxa at the family level,

particular families of Coleoptera. This finding is of special relevance in

especially for a few species-rich families which are found in similar rel-

the context of large-scale biodiversity surveys where higher-rank tax-

ative proportions in most samples. The complexity of the data may

onomic classification arises as a mandatory first-step prior to more

allow the CNN model trained on this broad set to capture general

refined classification by expert taxonomists (Karlsson et al., 2020).

family traits of the global fauna and thus make it suitable for a greater

Within a local dataset, classification accuracy regularly reached 95%

range of classification tasks at local level. However, this broad scope

or more, which is similar to findings from other studies using more
standardized photographs from museum collections (e.g., (cid:5)92% and

96% for Diptera and Coleoptera, respectively; Valan et al., 2019). We

the accuracy reduction in across-dataset predictions. Subsets of
randomly selected images of one dataset are used for training and
predicting the class (family label) of another set, as indicated by
different colours. Lines in light blue refer to the comparison involving
tests of locality, that is, when using global high quality (GH) images for
training and predicting the class of local high quality (LH) images. Lines
in green refer to comparisons involving tests of image quality, that is,
when using local low quality (LL) images for training and predicting the
class of local high quality (LH) images. Lines in dark blue refer to
comparisons involving differences in both locality and image quality,
that is, when using global high quality (GH) images for training and
predicting the class of local high quality (LL) images. The x-axis
representing the number of training images is on a logarithmic scale.
The vertical dotted bars indicate 95% confidence interval of the

across all specimens regardless of their body size. As much as possible,

is, when the input subclass (species and genera within a family) is not

specimens were positioned for photography in dorsal view.

present in the training data. Various parameters are tested that may

Bulk-sample photographs were taken using a Zeiss AXIO Zoom.V16

affect the prediction accuracy, including: (i) the size of the training set;

Stereo Zoom Microscope equipped with a Zeiss AxioCam HRc (High

(ii) the complexity of the training set, which may be affected by the

Resolution 13 Megapixels Colour Microscope) camera at the Imaging

level of intra-class variability, noise from misidentifications, or the

and Analysis Centre at the Natural History Museum (NHM) in London,

presence of out-of-distribution samples; and (iii) the quality of images,

United Kingdom. This instrument has a motorized focus drive and

for example, the resolution of the image using standard macrophotog-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about how the hyperparameters of the model are optimized. It mentions that the model uses an adversarial training procedure to optimize shared intermediate features to be invariant between two domains, which potentially improves its ability to generalize across different datasets. However, there is no mention of specific optimization techniques such as grid search or random search.
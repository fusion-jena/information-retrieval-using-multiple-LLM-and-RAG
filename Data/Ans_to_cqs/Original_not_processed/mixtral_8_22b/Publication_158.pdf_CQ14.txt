Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.5.2. Random Forest

For random forest, input features included the mean and standard deviation for image
window sizes of 5 × 5, 15 × 15, and 32 × 32 in addition to the central pixel values for
each input feature. We used a random search to identify an optimal parameter set where
100 sets were sampled and compared using three-fold cross validation. Search parameter
ranges were number of trees (200 to 800, by 100), minimum samples split (2–4), minimum

Remote Sens. 2021, 13, 634

7 of 21

samples per leaf (2–4), maximum depth (full), bootstrap (true), and maximum features for
split (square root of the number of features). For a description of these parameters see the
Scikit-learn documentation [48]. These results were reﬁned using a grid-search with the
parameter ranges set to ±50 for the number of estimators and ±1 for the other parameters.

2.6. Membership Based Change Detection and Segmentation
2.6.1. Split Window Detection of Change Seed Points

Classiﬁer

Ensemble
CNN

Random
Forest

Overall

F1-Score

Accuracy

Kappa

F1_Ave

Bare Grass

Pasture Water

Crop

Forest Wetland

Shrub

0.884

0.861

0.840

0.826

0.867

0.835

0.978

0.929

0.920

0.763

0.600

0.845

0.816

0.776

0.650

0.842

0.771

0.981

0.901

0.890

0.722

0.450

Remote Sens. 2021, 13, 634

11 of 21

Table 2. Error matrix for the ensemble CNN.

Predicted

Reference

Bare Grass

Pasture Water

Crop

Forest Wetland

Shrub

R_Sum UA

Bare

Grass

Pasture

Water

Crop

Forest

Wetland

Shrub

C_Sum

PA

145

5

8

0

11

1

0

1

4

828

92

1

3

2

0

3

8

92

1014

3

49

18

6

8

171

0.85

933

0.89

1198

0.85

0

0

2

887

1

2

9

0

901

0.98

15

16

56

1

1343

7

14

3

1455

0.92

3

5

17

1

0

1045

20

9

1100

0.95

4

21

19

18

27

64

391

11

555

0.70

1

10

22

1

1

32

30

99

196

0.51

180

977

1230

912

1435

1171

470

134

0.81

0.85

0.82

0.97

0.94

0.89

0.83

0.74

For training, the Adam optimizer was used with 500 epochs and categorical-cross
entropy loss function. Batch size was set to 64. Data augmentation was applied to enhance
sample variation and included rotation, reﬂectance bias of ±10%, and random noise of
±10% reﬂectance for 15% of the input image. To determine training convergence, we
monitored the validation data across training epochs and the network weights with the
best performance were kept. Early stopping criteria was applied if no improvement was
found in 50 epochs. For CNNs, weight initialization and random selection for batch
training can lead to variability in the results. Thus, an ensemble of three models were
generated and the average of the ﬁnal output layer taken. Max pooling was used for two
models in the ensemble and average pooling for one. In initial tests using max or average
pooling showed that both provided improvement in some cases and thus could enhance

the ensemble results. All CNNs were trained using the TensorFlow backend on a Tesla
M100 graphic processing unit.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The hyperparameters of the model are optimized using a combination of random search and grid search methods. Initially, a random search is performed to identify an optimal parameter set, where 100 sets are sampled and compared using three-fold cross validation. The parameter ranges for this random search include the number of trees (200 to 800, by 100), minimum samples split (2--4), minimum samples per leaf (2--4), maximum depth (full), bootstrap (true), and maximum features for split (square root of the number of features). After identifying an optimal parameter set through the random search, a grid-search is then employed to refine these results further. The parameter ranges for the grid-search are set to ±50 for the number of estimators and ±1 for the other parameters. This optimization process helps ensure that the best possible hyperparameters are selected for the given dataset and problem statement.
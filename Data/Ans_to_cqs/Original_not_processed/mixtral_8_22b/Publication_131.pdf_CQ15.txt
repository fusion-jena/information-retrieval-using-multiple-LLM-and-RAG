Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

proper databases for CNN training.

13 of 19

ﬁlters per convolution layer is 32 for the ﬁrst one, 64 for the second one, and 128 for the
ﬁnal one. A kernel size (height and width of the convolution window) of 3 × 3 is used
on each convolution layer. The fully connected layer (FC) consists of 512 neurons and
the softmax layer with the number of units for classiﬁcation (one for each type of sound).
Figure 11 shows the structure of the CNN.

Figure 11. Structure of the designed CNN.

For network training, 80% of the samples were used as training samples, and 20%
as tests. This strategy is widely used to enhance the generalization of a machine learning
model and prevent overﬁtting. A balanced sampling was carried out for the test set. For
it, one audio sample (with its corresponding data augmentation samples) out of 5 was
selected for each type of anuran call. Thus, 7623 samples were used for training and 1892
for testing.

number of output filters per convolution layer is 32 for the first one, 64 for the second one, and 128 for the final one. A kernel size (height and width of the convolution window) of 3 × 3 is used on each convolution layer. The fully connected layer (FC) consists of 512 neurons and the softmax layer with the number of units for classification (one for each type of sound). Figure 11 shows the structure of the CNN. Sensors 2021, 21, 3655

15 of 19

Figure 13. Power consumption of the IoT executing the CNN.

3.2. CNN Models

The overall accuracy results obtained for the ﬁrst network, which classiﬁed the sounds
in four classes, were 96.34% for training and 97.53% for the test data. The confusion matrix
for the test data in this network is detailed in Table 2.

Table 2. Confusion matrix for the ﬁrst CNN (4 classes).

Predicted Values

Ep. cal. st&ch

Ep.cal. amplexus

Al. obs. standard

Al. obs. distress

Actual values

Ep. cal. st&ch

96.51% (775)

3.49% (28)

Ep.cal. amplexus

Al. obs. standard

Al. obs. distress

4.9% (7)

1.21% (11)

2.28% (1)

95.10% (136)

0

0

0

0

98.79% (902)

0

0

0

0

97.72% (43)

The dispersion of the error between the four classes is low, standing between 1.21% for
the Alytes obstetricans class (11 errors versus 913 audios) and 4.9% for the Epidalea calamita
class with amplexus vocalization (7 errors versus 143 audios).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't any information available regarding the specific optimization techniques applied in the deep learning pipeline mentioned in the given context. Therefore, it is not possible to provide an accurate response based solely on the provided text. However, some commonly used optimization techniques in deep learning include Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), Root Mean Square Propagation (RMSProp), and others. These methods help minimize the loss function during the training process by adjusting the model parameters iteratively.
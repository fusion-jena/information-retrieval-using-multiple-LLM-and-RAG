Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(cid:0) 4,  and  steep  value  of  1.3e

4). 

(cid:0)

4.3.3. Model evaluation 

In this section, the results of the deep learning model developed are 
presented. The model was trained to make day-ahead predictions of the 
three pollutants, but first, an appropriate evaluation metric had to be 
selected.  The  top  metrics  for  regression  problems  are  mean  squared 
error/root  mean  squared  error  (MSE/RMSE),  mean  absolute  error 

Fig. 12. A plot showing the model's training and validation losses against the 
number of epochs. It is worth noting that there was a gradual decrease in both 
losses  as  the  training  epochs  increased  which  indicates  that  the  model  was 
learning. Further training beyond 20,000 epochs would have either resulted in 
overfitting or no further drop in both losses.

4.3. Forecasting model training and evaluation 

Fastai was used for data pre-processing and model training. The li-
brary is built on the PyTorch framework and allows quick analysis using 
its  readily  encoded  best  practices.  The  aim  was  to  develop  a  model 
capable of efficiently making hourly predictions of the pollutant of in-
terest. This section introduces the data processing procedure, the net-
work's architecture used for training and the validation method. 

4.3.1. Meteorology data integration and dataset pre-processing

within the training data. 

Three  key  data  pre-processors:  categorify,  fillMissing  and  normalize 
from fastai were adopted for additional data pre-processing. These pre- 
processors map categorical columns to distinct categories, replaces null 
values with column median values and normalises continuous columns 

EcologicalInformatics69(2022)10160910T.D. Akinosho et al.                                                                                                                                                                                                                            

Fig. 9. Plots highlighting the varying monthly averages for the three monitored pollutants. These averages varied significantly and are an indication that some 
influential factors may have affected the concentration levels.

by  subtracting  the mean  and dividing by  the standard deviation.  The 
“add datepart” helper function of the library allows the specification of 
the date column which generates additional predictors such as “Year”, 
“DayofWeek”,  “DayOfYear”,  “Is  Month_End”  and  so  on.  Appendix  B 
highlights the list of categorical and continuous variables in the dataset 
after processing. 

4.3.2. Validation set creation and training architecture 

Model  training  is  typically  initiated  by  splitting  the  dataset  into 
training, test and validation datasets. As the name implies, training data 
is used for training, while validation data is used for selecting the model 
that works best after verification using the test data. It is customary to 
randomise the dataset before splitting when there is a class imbalance - 
stratification; but since this problem is like a time-series problem where

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text mentions several preprocessing steps taken before training a deep learning model. First, Fastai was used for data preprocessing and model training. Three key data preprocessors from Fastai were adopted: categorify, fillMissing, and normalize. Categorify maps categorical columns to distinct categories, fillMissing replaces null values with column median values, and normalize scales continuous columns by subtracting the mean and dividing by the standard deviation. Additionally, the "add datepart" helper function of the library was used to generate additional predictors based on the date column, such as "Year", "DayofWeek", "DayOfYear", and "IsMonth\_End".
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

prediction and training points.

mapping probability samples that are exclusively used for map evalua-
tion are often not available and therefore alternative methods have been 
proposed.  In  machine  learning,  if  data  are  abundant,  a  common 
approach is to randomly divide the full dataset used for modelling into 
three parts: a training set, a validation set, and a test set (Hastie et al., 
2009,  Chapter  7).  The  training  set  is  used  for  fitting  the  models,  the 
validation set is used to estimate prediction error for model selection and 
hyperparameter tuning, while the test set is used for assessing the ac-
curacy of the final model. This paper addresses this latter testing phase, 
with the specific aim to assess the accuracy of a thematic map produced 
by a calibrated statistical prediction method. Data availability is often 
limited  so that setting  aside a  test set  cannot always be afforded and 
therefore resampling methods are used (Hastie et al., 2009; Steele et al.,

simulated fields the map accuracy metrics were computed (see Section 
2.3),  after  which  the  means  were  computed  from  the  sampling 
distributions.

We have proposed an inverse sampling-intensity weighted and two 
geostatistical model-based cross-validation approaches as alternatives to 
the  broadly  propagated  spatial  cross-validation  method  that  tends  to 
produce  pessimistically biased map accuracy estimates. The proposed 
methods  can  be  characterised  as  quasi-randomization  and  super-
population  modelling  approaches,  respectively.  Like  blocked  spatial 
cross-validation, our weighted approach is a heuristic method. However, 
in  contrast  to  the  former  it  explicitly  addresses  the  spatial  clustering 
problem rather than the incorrectly posed problem of spatial proximity 
of test and training data. In our case studies, bias in the map accuracy 
metrics assessed over multiple realizations of the sampling designs by 
weighted cross-validation was much smaller than that of blocked spatial 
cross-validation for non-clustered to moderately clustered samples. For

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: After the model training process, several postprocessing steps can be performed depending on the application and requirements. Here are some commonly used ones:

1. Model Evaluation: Once the model has been trained, its performance needs to be evaluated using appropriate evaluation metrics such as accuracy, precision, recall, F1 score, ROC AUC etc. These metrics help us understand how well the model performs on unseen data.

2. Confusion Matrix: It is a table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class.

3. Saliency Maps: They are visual explanations of the decisions made by deep learning models. They highlight the pixels of input image that contribute most strongly to the output of the model.

4. Hyperparameter Tuning: Based on the results obtained during model evaluation, hyperparameters of the model may need to be adjusted to improve its performance. Techniques like Grid Search or Randomized Search can be employed for this purpose.

5. Predictions: Finally, once satisfied with the modelâ€™s performance, predictions can be made on new, unseen data.

In the given context, after the model training, simulated fields were created and map accuracy metrics were computed. Means were then calculated from these sampling distributions. This could be considered as part of the model evaluation step where the performance of the model is being assessed.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.1.2. Annotation and split 

We manually segmented all salient trunks in each image to obtain 
pixel-wise  salient  object  annotations.  Before  being  annotated,  each 
image  was  resized  to  512 × 512  in  pixel.  During  annotation,  we  dis-
carded a small fraction of images because their trunks are too blurry and 
obscure even for the human eye. We also examined each result one by 
one and re-labeled the poor-quality ones. Finally, more than 2550 im-
ages and their high-quality binary ground-truth masks were generated to 
represent salient trunks.

72 
72 
128 
64 

DN-3 
72 
72 
256 
64 

36 
36 
256 
128 

DN-4 
36 
36 
512 
128 

18 
18 
512 
256 

DN-5  
18  
18  
1024  
256   

9 
9 
512 
256         

networks often fail in extracting global information from shallow layers 
because of the small receptive fields (Liu et al., 2019b; Liu et al., 2021). 
For  creating  feature  maps  with  much  global  information,  multiple 
dilated convolutions are used for shallow layers (Zhao et al., 2020)— 
which, however, entail more computation resources. U2-Net defines a 
two-level  nested  model  (i.e.,  a  stack  of  nested  encoder-decoder)  to 
capture the contextual information in different scales at a moderate level 
of computation cost.

da Silva Vieira et al. (2019) 
Majeed et al. (2020) 
Zhang et al. (2021) 

1470 
336 
2095 

52 
191 
674 

1470 
1600 
- 

- 
191 
674 

Box 
Box 
Box 

Pixel 
Pixel 
Pixel 

SG, SP 
MP, SM 
MP, SM 

MP, SP 
SG, SM 
SG,OC, SM 

CL 
CT 
CT 

CL 
CT 
CT 

Max 

128 
1292 
1920 

600 
912 
1920 

Min  

72 
480 
720 

288 
608 
1080 

-Bounding box annotation 
-Homogeneous objects 
-Monotonous background          

-Single tree species 
-Monotonous background 
-Multiple similar images          

Grondin et al. (2022) 

101 

920 

Pixel 

MP, DV 

CT, DV 

1280 

720 

-Uniform object size          

Silva et al. (2022) 

1868 

1868 

Line 

SG, DV 

CT, DV 

128 

128 

-Single object  
-Straight line annotation          

ST-D (ours) 

2569 

5900 

Pixel 

MP, OC, DV 

CT, DV 

512 

512 

-Urban scenes

After  annotating  and  examining  the  images,  we  divided  them 
randomly into a training set, and a testing set using a ratio of 65% to 
35%  respectively.  Consequently,  our  dataset  was  divided  into  1691 
images for training and 878 images for testing. 

2.1.3. Dataset properties 

Not  only  is  the  proposed  ST-D  large  in  scale  and  high-quality  in 

annotation, but also it features good diversity which is listed below. 

Tree trunk size. To study the size of all salient tree trunks per pic-
ture,  we  calculated  the  percentage  of  pixels  in  each  picture  that  are 
occupied by salient tree trunks. In our dataset, the sizes of salient objects 
range from 0.39% to 97.8%, with an average of 34.53%. As shown in 
Fig. 3(a), our ST-D has more diverse tree trunk sizes in its images and 
balances the large and small objects, which is beneficial to train the SOD 
models and evaluate their generalization performance.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text describes several methods of data annotation used in the creation of the ST-D dataset for studying salient tree trunks. The primary method employed here is pixel-wise semantic segmentation, where each pixel in an image is labeled according to whether or not it belongs to a salient tree trunk. This technique allows for precise identification and delineation of individual tree trunks within the images.

Additionally, the authors mention discarding some images due to their low quality, indicating that manual inspection and filtering were part of the annotation process. They also discuss dividing the dataset randomly into a training set and a testing set using a ratio of 65% to 35%, respectively. However, no other specific data annotation techniques such as bounding box annotation or instance segmentation are explicitly mentioned in this excerpt.
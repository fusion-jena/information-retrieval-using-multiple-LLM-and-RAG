Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Fig. 5. Examples of estimated sampling intensities (multiplied by 109) for five samples shown in Fig. 2: (a) simple random (cf. Fig. 2(a)); (b) systematic random (cf. 
Fig. 2(c)); (c) moderately clustered (cf. Fig. 2(e)); (d) strongly clustered (cf. Fig. 2(g)); (e) strongly clustered, gapped sample (cf. Fig. 2(i)).

Fig. 2. Examples of studied spatial samples. (a-b) simple random samples; (c) systematic random sample; (d-e) moderately clustered samples; (f-g) strongly clustered 
samples; (h-i) strongly clustered, gapped samples. Except for the systematic sample (c), the sample size always amounted to 5000. The systematic sample had an 
expected size of 5000 but realized samples varied in size between 4998 and 5056.

3.  Moderately clustered sample. This sample was produced by stratified 
sampling. First, the study area was divided into 100 compact geo- 
strata  using  the  spcosa  package  (Walvoort  et  al.,  2010).  In  each 
sample, 20 of the 100 geo-strata were randomly selected to form a 
stratum  and  50%  of  the  total  sample  size  was  randomly  chosen 
within  this  stratum.  The  locations  of  the  remaining  50%  of  the 
sample were randomly chosen from the stratum formed by the other 
80 geo-strata.

mapping probability samples that are exclusively used for map evalua-
tion are often not available and therefore alternative methods have been 
proposed.  In  machine  learning,  if  data  are  abundant,  a  common 
approach is to randomly divide the full dataset used for modelling into 
three parts: a training set, a validation set, and a test set (Hastie et al., 
2009,  Chapter  7).  The  training  set  is  used  for  fitting  the  models,  the 
validation set is used to estimate prediction error for model selection and 
hyperparameter tuning, while the test set is used for assessing the ac-
curacy of the final model. This paper addresses this latter testing phase, 
with the specific aim to assess the accuracy of a thematic map produced 
by a calibrated statistical prediction method. Data availability is often 
limited  so that setting  aside a  test set  cannot always be afforded and 
therefore resampling methods are used (Hastie et al., 2009; Steele et al.,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information on the methods used for collecting raw data in the deep learning pipeline such as surveys, sensors, or public datasets. However, it mentions that in machine learning, when data is abundant, a common approach is to randomly divide the full dataset used for modeling into three parts: a training set, a validation set, and a test set. This implies that the raw data could be collected through various means and then divided for use in different stages of the machine learning process. Unfortunately, without more specific details about the source of the initial dataset, it is impossible to definitively state which methods were used for its collection.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

as a feature [13]. As the study showed, they require a large training sample to maintain a
good performance of their approach.

threshold during training for 3000 iterations. At the end of the training process, the best
performing model based on the validation loss was saved and used as the Faster R-CNN
model with a NMS threshold of 0.7 during testing.

9 of 23

Table 1. Dataset summary used for training the single-leaf classiﬁer.

Datasets

UBDH dataset
Flavia dataset

Total

Single Leaves
(Positive Samples)

Non-Single Leaves
(Negative Samples)

798
83

881

1015
0

1015

Figure 4. Negative samples manually extracted from the segmentation model results for training the single-leaf classiﬁer.
The dataset consisted of 881 intact individual leaves as positive training samples and 1015 negative samples.

4.2. Pre-Processing and Training of Semantic Segmentation Model

0.0232

extent
ﬁlled_area
major_axis_length
minor_axis_length
perimeter
solidity
diameter
aspect_ratio
rectangularity
compactness
circularity
narrow_factor
per_dia_ratio
per_length_ratio
per_length_
width_ratio

MAE

Faster
R-CNN

0.0123
461.2895
2.8865
1.4556
21.1247
0.0077
1.6507
0.0330
0.0035
1.0373
0.0289
0.0049
0.0986
0.0637

0.0415

Table 9. Cont.

MSE

YOLOv5s

0.0174
1300.0921
5.6320
3.3250
33.1226
0.0081
3.9047
0.0288
0.0046
1.2096
0.0352
0.0045
0.1163
0.0768

0.0507

Proposed
Method

0.0002
164,526.7895
6.1220
1.5845
258.5453
0.0000
1.9746
0.0008
0.0000
0.6377
0.0007
0.0000
0.0062
0.0021

Faster R-CNN

YOLOv5s

0.0003
383,162.7632
40.6365
6.0962
1608.1119
0.0005
5.6350
0.0080
0.0002
5.2750
0.0025
0.0001
0.0410
0.0212

0.0005
27,113,134.6711
349.0719
130.0219
4932.9094
0.0004
205.8782
0.0042
0.0002
4.9619
0.0027
0.0001
0.0402
0.0218

0.0011

0.0075

0.0078

Proposed
Method

0.0143
405.6190
2.4743
1.2588
16.0793
0.0068
1.4052
0.0288
0.0030
0.7985
0.0258
0.0052
0.0787
0.0463

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training, there are several postprocessing steps that can be taken depending on the specific requirements of the project. However, some common postprocessing steps include calculating metrics such as accuracy, precision, recall, and F1 score to evaluate the performance of the model. Additionally, creating confusion matrices can provide insight into which classes the model is struggling with or performing well on. Visualizing saliency maps can also help understand what parts of the input data the model is focusing on when making predictions. In this case, it appears that the authors used a non-maximum suppression (NMS) threshold of 0.7 during testing, which suggests that they may have applied NMS as a postprocessing step to filter out overlapping bounding boxes predicted by the Faster R-CNN model.
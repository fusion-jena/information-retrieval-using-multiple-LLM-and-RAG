Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

accuracy  limits  that  we  encountered  training  DNN  models  with  the 
detailed-labelled NIPS4Bplus dataset (Bravo Sanchez et al., 2021). We 
drilled down on the results using another high-quality model (BirdNET) 
and  compared  it  to  our  SincNet  trained  models.  For  that,  we  experi-
mented  with  dimensionality  reduction  of  embeddings  and  an  initial 
dimensionality reduction technique (t-SNE). We generated embeddings 
using  the  BirdNET  model  on  a  randomly  selected  40%  sample  of  the

labelled NIPS4Bplus dataset. We reduced the dimensionality of BirdNET 
embeddings from 420 to 2 dimensions using t-SNE (Fig. 2). Note that 
BirdNET requires a 3-s sound input which we obtained from the sound 
file but, as indicated in methods, it was not always possible to extract a 
whole 3-s extract from the sound file by commencing at the label start. 
The maximum length of the sound files in the dataset is about 5 s, so in 
many cases a 3-s extract from the label start would extend beyond the

Neural information processing scaled for bioacoustics-from neurons to Big Data. In: 
Proceedings of Neural Information Processing Scaled for Bioacoustics: From Neurons 
to Big Data, 2013. http://sabiod.univ-tln.fr/NIPS4B2013_book.pdf. 

Gupta, Gaurav, Kshirsagar, Meghana, Zhong, Ming, Gholami, Shahrzad, Ferres, Juan 

Lavista, 2021. Comparing recurrent convolutional neural networks for large scale 
bird species classification. Sci. Rep. 11 (1), 17085. https://doi.org/10.1038/s41598- 
021-96446-w. 

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, Sun, Jian, 2015. ‘Deep residual learning 

for image recognition’. arXiv. http://arxiv.org/abs/1512.03385. 

Hershey, Shawn, Chaudhuri, Sourish, Ellis, Daniel P.W., Gemmeke, Jort F., Jansen, Aren, 
Moore, Channing, Plakal, Manoj, et al., 2017. CNN Architectures for Large-Scale 
Audio Classification. In: International Conference on Acoustics, Speech and Signal 
Processing (ICASSP).. https://arxiv.org/abs/1609.09430.

has a frame size of 16 milliseconds and frame shift of 1 millisecond that 
are averaged over the labelled sound to obtain a prediction. In the same 
way we averaged the outputs of the penultimate layer of the model to 
obtain embeddings for each labelled sound that have 4096 dimensions. 
The ResNet152-architecture is based on the ResNet152 model (He 
et al., 2015) developed for image classification, but we did not use pre- 
trained weights. We relied on Keras (Chollet et al., 2015) implementa-
tion of ResNet152 to obtain the model architecture, added a dense layer 
and trained the models from scratch with randomly initialised weights. 
The input was 3 s sound frames converted into mel spectrograms using a 
fast Fourier transform window size of 1024, hop length of 896 and 128 
mel filterbanks. The resulting models had around 58 million trainable 
parameters.  We  trained  two  types  of  models  using  extracts  from  the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In this study, the authors used the detailed-labelled NIPS4Bplus dataset (Bravo Sanchez et al., 2021) for training their Deep Neural Network (DNN) models. They also utilized another high-quality model called BirdNET for comparison purposes. To generate embeddings using the BirdNET model, they employed a randomly selected 40% sample of the labeled NIPS4Bplus dataset. It should be noted that BirdNET requires a 3-second sound input, which was extracted from the sound file, although it wasn't always feasible to extract a complete 3-second segment starting from the label start due to the varying lengths of the sound files in the dataset.

Additionally, the authors mentioned using the ResNet152-architecture, which is based on the ResNet152 model (He et al., 2015) originally designed for image classification. However, they didn't utilize any pre-trained weights for this purpose. Instead, they leveraged the Keras implementation of ResNet152 to acquire the model architecture, appended a dense layer, and trained the models from scratch with randomized initial weights. The input data consisted of 3-second sound frames transformed into mel spectrograms utilizing a fast Fourier transform window size of 1024, hop length of 896, and 128 mel filterbanks. Consequently, the resulting models contained approximately 58 million trainable parameters.

Therefore, the primary dataset used in this research is the NIPS4Bplus dataset, while the ResNet152 model was adapted for audio classification tasks without employing any well-known image datasets like MNIST, CIFAR, or ImageNet.
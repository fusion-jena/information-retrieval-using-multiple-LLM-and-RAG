Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

respectively.  N  represented  the  number  of  images  in  the  test  set  or 
validation set. 

2.2.6. Model training and hyper-parameters 

We  trained  models  on  the  training  sets  and  tracked  the  model 
training process on the validation set. The training process was stopped 
when  the  loss  value  of  the  model  on  the  validation  set  no  longer 
decreased. We then tested the model performance on the test set. Our 
computing platform was a Dell PowerEdge C4130 rack server with two 
Tesla  K80  GPUs  and  256  GB  of  memory.  For  the  deep  learning  envi-
ronment,  we  utilized  the  PyTorch  1.7.1  platform.  We  employed  the 
Stochastic Gradient Descent (SGD) optimizer with a momentum value of 
μ  = 0.9 to train the model. Other hyper-parameter settings for model 
training were shown in Table S.4 of the Supporting Information. 

3. Results 

3.1. Experiment results of transfer strategy optimization

Convolutional Neural Networks, in: Proceedings of the 5th ACM on International 
Conference on Multimedia Retrieval, ICMR ‘15. Association for Computing 
Machinery, New York, NY, USA, pp. 643–650. https://doi.org/10.1145/ 
2671188.2749408. 

Ghafoorian, M., Mehrtash, A., Kapur, T., Karssemeijer, N., Marchiori, E., Pesteie, M., 

Guttmann, C.R.G., de Leeuw, F.-E., Tempany, C.M., van Ginneken, B., Fedorov, A., 
Abolmaesumi, P., Platel, B., Wells, W.M., 2017. Transfer learning for domain 
adaptation in MRI: Application in brain lesion segmentation. In: Descoteaux, M., 
Maier-Hein, L., Franz, A., Jannin, P., Collins, D.L., Duchesne, S. (Eds.), Medical 
Image Computing and Computer Assisted Intervention (cid:0) MICCAI 2017. Springer 
International Publishing, Cham, pp. 516–524.

2. Datasets and method 

2.1. Datasets 

This  study  used  three  significantly  different  datasets:  ImageNet, 
Snapshot  Serengeti  (SS),  and  Lasha  Mountain  (LSM).  ImageNet  is  a 
benchmark dataset in the field of computer vision. Each image in the 
ImageNet dataset is manually annotated, and the image sizes vary, with 
the largest size being 4288 × 2848 pixels and the smallest being 75 × 56 
pixels. The ImageNet dataset we used is ImageNet-1 k, which contains 
1000 classes and 1.2 million training samples. This study used the pre- 
trained  model  on  the  ImageNet-1  k  dataset  as  one  of  the  source 
models for transfer learning.

We  used  the  ResNext-101  model  to  systematically  study  transfer 
learning techniques. This model combines the advantages of the ResNet 
and InceptionV3 models, which are widely used for the empty image 
recognition task. Our proposed optimal update layer schemes based on 
the ResNext-101 model can be applied to any member of the ResNext 
family  (i.e.,  ResNext-X,  where  X  = 18,  34,  50,  101,  152,  etc.).  We 
speculate that these schemes are also suitable for models of the ResNet 
family (i.e., ResNet-X, where X = 18, 34, 50, 101, 152, etc.), but further 
validation is still needed in practical projects.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors utilize Stochastic Gradient Descent (SGD) as their optimization technique in the deep learning pipeline. They specifically use the SGD optimizer with a momentum value of μ = 0.9 to train the model. No other specific optimization techniques such as Adam, RMSprop, or Adagrad are mentioned in the given context.
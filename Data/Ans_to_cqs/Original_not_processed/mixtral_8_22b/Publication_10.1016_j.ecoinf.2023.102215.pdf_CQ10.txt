Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

72 
72 
128 
64 

DN-3 
72 
72 
256 
64 

36 
36 
256 
128 

DN-4 
36 
36 
512 
128 

18 
18 
512 
256 

DN-5  
18  
18  
1024  
256   

9 
9 
512 
256         

networks often fail in extracting global information from shallow layers 
because of the small receptive fields (Liu et al., 2019b; Liu et al., 2021). 
For  creating  feature  maps  with  much  global  information,  multiple 
dilated convolutions are used for shallow layers (Zhao et al., 2020)— 
which, however, entail more computation resources. U2-Net defines a 
two-level  nested  model  (i.e.,  a  stack  of  nested  encoder-decoder)  to 
capture the contextual information in different scales at a moderate level 
of computation cost.

TrunkNet’s  feature  extraction  includes  11  well-configured  MIF
blocks,  which  are  connected  in  an  encoder-decoder  framework  of  U- 
structure.  The  MIF block  is  also  an  encoder-decoder  network  and  in 
particular,  embedded  with  an  attention  mechanism  (the  TA module) 
such that it can well capture multiscale features. The MIF block and the 
TA module will be articulated in later sections. Six MIF blocks, marked 
with EN-i for 1⩽i⩽6 are encoders, while the others, marked with DN-j for 
1⩽j⩽5 are decoders. Encoder EN-6 and all five decoders output a feature 
map each; these output maps form a set of multiscale features, which are 
passed into the feature aggregation part for further processing.

2.3. Model training 

Our study can be categorized as a binary classification. We supervise 
TrunkNet’s training with the Binary Cross-Entropy (BCE) loss function, 
which  is  a  commonly  employed  loss  function  in  supervised  deep 
learning (Zhao et al., 2019b). The BCE loss function measures the dif-
ference between the predicted probability and the actual value. Mini-
mizing this difference indicates a better training effect. Specifically, the 
BCE loss function in use is expressed with 

L BCE = (cid:0)

∑H

∑W

x=1

y=1

G(x, y)logSpre(x, y) + (1 (cid:0) G(x, y))log(1 (cid:0) Spre(x, y)),

(3)  

where (x,y) is the pixel coordinate, H and W are the height and width of 
the image, respectively, G is the ground-truth, and Spre  is the final pre-
dicted saliency map.

and reconstruction mechanism for harvesting robot via convolutional neural 
network and image segmentation. Comput. Electron. Agricult. 192, 106609. 

Liu, G., Fan, D., 2013. A model of visual attention for natural image retrieval. In: 2013 
International Conference on Information Science and Cloud Computing Companion. 
IEEE, pp. 728–733. 

Wan, H., Zeng, X., Fan, Z., Zhang, S., Kang, M., 2023. U2espnet—a lightweight and high- 
accuracy convolutional neural network for real-time semantic segmentation of 
visible branches. Comput. Electron. Agricult. 204, 107542. 

Liu, J., Wang, X., Wang, T., 2019a. Classification of tree species and stock volume 

Wang, T., Chen, B., Zhang, Z., Li, H., Zhang, M., 2022. Applications of machine vision in 

estimation in ground forest images using deep learning. Comput. Electron. Agricult. 
166, 105012.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given context, there is no explicit mention or reference to any specific code repository link for the deep learning pipeline. Therefore, I cannot provide the requested information. It would be best to refer back to the original source material or contact the authors directly for this information.
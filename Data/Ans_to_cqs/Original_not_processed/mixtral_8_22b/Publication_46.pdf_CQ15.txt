Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Convolution is used for feature extraction, in which a kernel is applied to an input
tensor. A feature map is thus obtained through the product of kernel elements and tensor
input. The procedure is then repeated on multiple kernels to obtain random feature maps
that represent different feature extractors. The hyperparameters involved in convolution
operations are the size and number of kernels. The size could be anything from 3 × 3 to
5 × 5 to 7 × 7, and the kernel could be chosen randomly.

Remote Sens. 2021, 13, 3284

9 of 17

A Convolutional Neural Network (CNN) is a type of deep learning-based model for
processing multidimensional data that follows a grid pattern [60]. The model is developed
in such a way that the algorithm learns and adapts to the spatial hierarchies of features by
itself from the lower to the higher levels of the pattern. Mathematically, it is composed of
three layers or building blocks: convolution, pooling, and fully connected layers. Feature
extraction is conducted using the ﬁrst two layers and mapping the extracted features to the
output is conducted by the third layer.

applications and relevance to most current MAXENT studies. Divers. Distrib. 2014, 20, 1–9. [CrossRef]
Schmidhuber, J. Deep learning in neural networks: An overview. Neural Netw. 2015, 61, 85–117. [CrossRef]

60.
61. Guo, Y.; Liu, Y.; Oerlemans, A.; Lao, S.; Wu, S.; Lew, M.S. Deep learning for visual understanding: A review. Neurocomputing 2016,

62.

187, 27–48. [CrossRef]
Fukuda, S.; De Baets, B.; Waegeman, W.; Verwaeren, J.; Mouton, A.M. Habitat prediction and knowledge extraction for spawning
European grayling (Thymallus thymallus L.) using a broad range of species distribution models. Environ. Model. Softw. 2013, 47,
1–6. [CrossRef]

63. Harris, D.J. Generating realistic assemblages with a joint species distribution model. Methods Ecol. Evol. 2015, 6, 465–473.

[CrossRef]

64. Lin, M.; Chen, Q.; Yan, S. Network in network. arXiv Prepr. 2013, arXiv:1312.4400.
65. Agarap, A.F. Deep learning using rectiﬁed linear units (relu). arXiv Prepr. 2018, arXiv:1803.08375.
66.

of weights between the neurons in the adjoining layers and will maximize the network
performance, which would help the neurons focus on speciﬁc patterns in the data. In the
ﬁnal layer, the parameters are passed through the SoftMax function, which transforms
them into probabilities that sum to 1, as shown in Equation (1).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not explicitly mention any specific optimization techniques applied in the deep learning pipeline, such as Stochastic Gradient Descent (SGD), Adam, or others. However, we can infer that some form of optimization technique must be employed during the training process of a Convolutional Neural Network (CNN) mentioned in the text.

In general, CNNs use backpropagation to train their weights, which involves computing gradients of the loss function concerning each weight in the network. This gradient information is then used to update the weights via an optimization algorithm like SGD or Adam. These algorithms iteratively adjust the weights to minimize the overall error or loss of the network.

While the given context does not provide explicit details about the optimization techniques used, it mentions that the final layer passes its parameters through the SoftMax function. This suggests that the network might employ a cross-entropy loss function, commonly used in multi-class classification problems. Cross-entropy loss is often optimized using variants of stochastic gradient descent, including popular choices like Adam, RMSProp, or Adagrad.

Therefore, while the exact optimization technique used in this particular case remains unspecified, one can reasonably assume that a variant of stochastic gradient descent, possibly Adam or another similar method, was likely utilized based on common practices in deep learning pipelines involving CNNs.
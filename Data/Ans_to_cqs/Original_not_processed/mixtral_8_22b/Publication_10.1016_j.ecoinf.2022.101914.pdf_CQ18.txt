Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

to model performance by a single component model built with subop-
timal training data. However, RF outperformed all the ensemble models, 
suggesting  that  if  researchers  use  a  single  modeling  approach,  they 
should consider using RF. It should be noted that we found RF to be more 
sensitive to the number of background points used to train the model 
than  other  modeling  methods.  However,  researchers  or  practitioners 
might choose to use a single modeling approach that is found to be less 
sensitive to the number of background points, like a GLM or MaxEnt. In 
this situation, since the gain in performance is minimal, the decision on 
how many background points to use might be instead made to minimize 
computation time and answer time-sensitive questions related to con-
servation  actions.  Nevertheless,  using  an  ensemble  of  models  helps 
overcome any issues that any single model might have, as demonstrated

by  its  relatively  constant  performance  regardless  of  the  number  of 
background  points  used  to  train  component  models.  As  with  many 
ecological  problems, the  decisions made during  the modeling process 
demand careful consideration in the full context of the question at hand, 
since different questions often differ in their requirements from a model.

2.9. Evaluating individual modeling approaches 

To ensure that all models were evaluated with the same relatively 
independent data, we used a common approach to split the data into 
evaluation  and  training  datasets  prior  to  training  the  models.  We  set 
aside the evaluation dataset and used it to evaluate individual averaged 
modeling approaches and the final ensemble models, hereafter referred 

To assess the optimal number of background points for each indi-
vidual modeling approach, we calculated the True Skill Statistic (TSS) 
and the Area Under the Receiver Operating Curve (AUC-ROC) for each 
model using the external evaluation datasets set aside prior to training 
models (Allouche et al., 2006; Hao et al., 2020; Shabani et al., 2016). 
The  True  Skill  Statistic  is  based  on  a  binary  classification  of  model

4.2. Ensemble models

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The authors monitored the performance of the models during training by setting aside an evaluation dataset before training the models. This dataset was then used to calculate the True Skill Statistic (TSS) and the Area Under the Receiver Operating Curve (AUC-ROC) for each model. These metrics helped determine the optimal number of background points for each individual modeling approach. Additionally, the authors also compared the performance of various ensemble models against a single component model built with suboptimal training data. They found that Random Forest (RF) outperformed all the ensemble models, suggesting that researchers should consider using RF when employing a single modeling approach. However, it was noted that RF was more sensitive to the number of background points used to train the model than other modeling methods. Despite this sensitivity, the authors concluded that using an ensemble of models can help overcome any issues that may arise from relying solely on a single model.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4.2.  Hyperparameters 

Hyperparameter tuning was done manually. As a starting point the 
learning rate was set as low as possible and the number of iterations as 
high as computationally feasible (Hastie et al., 2009). The learning rate, 
which is also called a shrinkage parameter, determines the contribution 
of each tree to the model. The maximum tree depth which reflects the 
degree of interaction in the model is usually low. GBRT performs best 
using fairly shallow trees, so-called tree stumps.

For  the  RI  models  the  same  initial  parameters  were  set  for  the 
analysis of the CS and NW datasets, then a grid search was performed 
on both datasets with a two-part split to give hyperparameters with the 
highest level of accuracy. These were as following: learning rate 0.05 
and maximum tree depth of 4 for both the CS and the NW models. The 
early stopping technique was used to determine when to stop the model 
training  to  avoid  overfitting.  Using  early  stopping  the  number  of 
iterations required until convergence was 120 for CS and 106 for NW. 
Grid search was not required for a three-part splits, and further tuning 
of the parameters was done manually. 

For the RY models, in contrast to the RI models, the best performing 
models (based on MSE) were models with a slow learning rate of 0.005. 
Additionally, the NW model had a very shallow tree (tree depth of 2). 
The number of iterations was kept at 500.

Spatial resolutions for the local environmental datasets are representative of the latitude and longitude where herring populations are found in the Celtic Sea and off 
the North-West of Ireland areas. Abbreviations C3, D4 and C4 refer to standard areas used in the CPR.  

steepest descent: The steepest descent direction is the negative gradient 
of the loss function evaluated at the current model Fm−1 which can be 
calculated for any differentiable loss function: 

Where the step length γm  is chosen using line search: 

(4)  

(5)  

The accuracy of gradient boosting can be improved by introducing 
randomisation  into  the  procedure  through  taking  randomly  selected 
subsets  of  training  data  at  each  iteration  (hence  stochastic  gradient 
boosting).

The  tuned  hyperparameters  of  the  final  models  are  shown  in  

Table 2. 

2.4.3.  GAM modelling of most influential parameters 

The top four most influential variables from the GBRT models (as 
indicated by VI scores) were included in a series of generalised additive 

Table 2 
Tuned model parameters.       

Model 

splits 

Number of iterations 

Learning rate 

Tree depth  

RI CS 
RI CS 
RI NW 
RI NW 
RY CS 
RY NW 

2 
3 
2 
3 
2 
2 

120 
102 
106 
144 
500 
500 

0.05 
0.05 
0.01 
0.05 
0.005 
0.005 

4 
4 
6 
6 
8 
2 

Number of iterations was defined by early stopping. Subsample rate was 0.75% 
in  all  cases.  Parameters  for  2-part  split  were  defined  through  grid  search, 
whereas parameters for 3-part split were tuned manually.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In this study, the authors use Gradient Boosting Regression Trees (GBRT), a type of ensemble machine learning algorithm, rather than deep learning methods. Therefore, traditional optimization techniques such as Stochastic Gradient Descent (SGD) or Adam are not directly applicable here. However, there are still some optimization techniques employed within the GBRT framework.

One key aspect of optimizing GBRT models involves tuning their hyperparameters. In this case, manual tuning and grid search were used to find optimal values for the learning rate, maximum tree depth, and number of iterations. The learning rate, also known as the shrinkage parameter, controls how much each tree contributes to the overall model. A lower learning rate means smaller contributions per tree but requires more iterations to converge. Maximum tree depth affects the degree of interaction between features in the model; shallower trees tend to perform better in GBRT due to reduced complexity and risk of overfitting.

To prevent overfitting, early stopping was utilized to determine when to halt model training. This method monitors performance on a validation dataset during training and stops once improvements plateau or begin to decline. Additionally, random subsets of training data were taken at each iteration for stochastic gradient boosting, adding another layer of regularization and improving overall model robustness.

While these techniques do not correspond exactly to those typically associated with deep learning pipelines like SGD or Adam, they serve similar purposes in terms of optimizing model performance and preventing overfitting.
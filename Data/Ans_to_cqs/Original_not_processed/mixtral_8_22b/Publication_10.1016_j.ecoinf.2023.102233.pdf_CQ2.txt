Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

validation  (Section  2.3)  and  test  datasets  (Section  2.4)  prior  to  the 
commencement of CNN development. The training/validation dataset 
comprised  6363  h  of  audio  from  ten  independent  sites  (five  solar- 
powered bioacoustic recorders and five Audiomoth recorders) and the 
test dataset comprised 2735 h of audio from nine independent sites (four 
solar-powered bioacoustic recorders and five Audiomoth recorders) that 
were different from the training/validation dataset. All audio data were 
unlabelled at the commencement of the study. 

All recorders were set to record 10-min files continuously between 6 
am and 6 pm daily, which reflected the diurnal activity patterns of SBTF. 
We stored audio data as FLAC files at a sampling rate of 44.1 kHz for the 
solar-powered bioacoustic recorders and WAV files at a 32 kHz sampling 
rate for the Audiomoth recorders. 

2.3. CNN development 

2.3.1. Data pre-processing

Sankupellay, M., Konovalov, D., 2018. Bird call recognition using deep convolutional 

neural network, ResNet-50. In: Proceedings of Acoustics. 

Sekercioglu, C.H., et al., 2008. Climate change, elevational range shifts, and bird 

extinctions. Conserv. Biol. 22 (1), 140–150. 

Settles, B., Craven, M., 2008. An analysis of active learning strategies for sequence 

labeling tasks. In: Proceedings of the 2008 Conference on Empirical Methods in 
Natural Language Processing, pp. 1070–1079. 

Teixeira, D., et al., 2022. Fledge or fail: Nest monitoring of endangered black-cockatoos 
using bioacoustics and open-source call recognition. Ecol. Inform. 69, 101656. 
Thakur, A., Thapar, D., Rajan, P., Nigam, A., 2019. Deep metric learning for bioacoustic 

classification: Overcoming training data scarcity using dynamic triplet loss. 
J. Acoust. Soc. Am. 146 (1), 534–547. 

Threatened Species Scientific Committee (TSSC), 2005. Commonwealth Listing Advice

We  chose  a  frequency  bandwidth  of  1.5  MHz  to  5  MHz,  which 
ensured the dominant frequency of the target call was included within 
the frame, while avoiding excess noise at lower frequencies and higher 
frequencies  (e.g.  cicadas).  We  found  that  this  frequency  bandwidth 
typically included a fundamental harmonic and/or a higher harmonic of 
the dominant frequency; however, these harmonics were not the focus of 
the bandwidth selection due to their rapid attenuation at lower sound 
pressure levels (Koehler et al., 2017). An example frame used as input 
into  the  CNN,  following  data  pre-processing  steps,  is  depicted  within 
Fig. 2. 

2.3.2. CNN architecture

this to determine realistic performance outcomes for the CNN model and to identify areas where inter-annotator 
agreement may be improved. We also applied a classification approach that allowed the CNN model to classify 
sounds  into  an  ‘uncertain’  category,  which  replicated  a  requirement  of  human-annotation  and  facilitated  the 
comparison of human-model annotation performance.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The data format used in the deep learning pipeline is audio. Specifically, the audio data is stored as FLAC files with a sampling rate of 44.1 kHz for the solar-powered bioacoustic recorders and WAV files with a 32 kHz sampling rate for the Audiomoth recorders.
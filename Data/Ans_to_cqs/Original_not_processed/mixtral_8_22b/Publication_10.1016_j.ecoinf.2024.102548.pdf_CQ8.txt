Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In contrast, ResNet50, with a staggering 76,019,586 parameters and 
a  training  time  of  952.39  s,  exhibited  relatively  inferior  performance 
when  compared  to  the  MobileNet-based  models.  It  has  a  substantial 
number of parameters (68,495,586) and requires 1022.48 s for training, 
reflecting its complexity. ResNet50 is a deeper and more complex ar-
chitecture specifically designed to address the challenge of training very 
deep neural networks. While it has achieved remarkable performance in 
various computer vision tasks (Mukti and Biswas, 2019), its structure 
may not have been optimized for the specific classification task in this 
study. The increased depth and complexity of ResNet50 may have made 
it more difficult for the model to capture important features and pat-
terns, resulting in lower accuracy and higher loss.

Transfer learning is an efficient technique in deep learning that le-
verages  pre-trained  models  to  address  data  scarcity  and  accelerate 
model  training.  Our  study  employed  several  well-known  pre-trained 
models,  namely  MobileNet,  VGG-19,  EfficientNet,  ResNet-50,  Incep-
tionV3, and InceptionResNetV2, to harness their learned representations 
and adapt them to our specific classification task. These models have 
been extensively trained on large-scale datasets, allowing them to cap-
ture general features that are useful across various domains (Weiss et al., 
2016).  Transfer  learning  involves  two  main  phases:  fine-tuning  and 
feature extraction. In the feature extraction phase, the pre-trained model 
is used as a fixed feature extractor. The input image is passed through 
the modelâ€™s convolutional layers, and the output features are obtained. 
These features capture meaningful information from the image, forming

3.2.4. ResNet-50 

ResNet-50 is a widely used deep residual neural network architecture 
that addresses the challenges of training very deep networks. It employs 
residual blocks with skip connections to enable the flow of gradients and 
facilitate the training of deeper models. ResNet-50 has been pre-trained 
on  large-scale  datasets,  allowing  it  to  capture  complex  patterns  and 
achieve strong performance. 

3.2.5.

InceptionV3 

InceptionV3 is an advanced CNN architecture that incorporates the 
concept of inception modules. These modules utilize parallel convolu-
tions  of  different  kernel  sizes  to  capture  multi-scale  features.  Incep-
tionV3  has  been  pre-trained  on 
large-scale  datasets  and  has 
demonstrated  excellent  performance  in  various  image  classification 
tasks. 

3.2.6.

InceptionResNetV2

Contents lists available at ScienceDirect 

Ecological Informatics 

journal homepage: www.elsevier.com/locate/ecolinf 

MobileNet-GRU fusion for optimizing diagnosis of yellow vein mosaic virus 

Tisha Chawla , Shubh Mittal , Hiteshwar Kumar Azad * 

School of Computer Science and Engineering, Vellore Institute of Technology, Vellore 632014, Tamil Nadu, India   

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Yellow vein mosaic virus 
MobileNet 
Gated recurrent unit 
Recurrent neural networks 
Transfer learning 
Deep learning

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide any direct information about where the code repository of the deep learning pipeline is available. However, based on common practices in research publications, authors often share their code repositories along with their published papers or articles. Therefore, it would be reasonable to check if the paper by Tisha Chawla, Shubh Mittal, and Hiteshwar Kumar Azad titled "MobileNet-GRU fusion for optimizing diagnosis of yellow vein mosaic virus" provides a link to the code repository. If no such link is found within the article itself, one could also search for the author names on popular code hosting platforms like GitHub, GitLab, or BitBucket to see if they have shared the relevant code there.
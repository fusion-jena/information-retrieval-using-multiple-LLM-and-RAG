Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In view of this discovery, we re-examined the application strategies 
of deep learning, no longer pursuing model complexity but exploring the 
potential  of  shallow  networks.  Through  meticulous  algorithm  optimi-
sation  and  structural  design,  we  strive  to  maintain  the  model  perfor-
mance while reducing the computational complexity  and deployment 
costs,  achieving  a  balance  between  speed  and  accuracy.  The  specific 
contributions of this study are as follows:  

• We propose a lightweight SIAlex model that utilises AlexNet as the 
backbone,  fully  exploiting  the  performance  of  minimalist  models. 
Ensuring a good balance between speed and accuracy. 

• The  method  of  cascading  multiple  activation  functions  fully  in-
troduces  nonlinear  factors  such  that  the  model  approximates  the 
nonlinear  expression  function  of  the  learning  features  while  also 
improving the gradient propagation.

can  further  improve  the  efficiency  and  performance  of  deep  learning 
models and promote the development of related fields.

Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., 
Courville, A., Bengio, Y., 2014. Generative adversarial networks. arXiv: 
1406.2661.  

Gupta, S., Jaafar, J., Ahmad, W.W., Bansal, A., 2013. Feature extraction using mfcc. SIPIJ 
4, 101–108. https://api.semanticscholar.org/CorpusID:1546219. 
Gupta, G., Kshirsagar, M., Zhong, M., Gholami, S., Ferres, J.M.L., 2021. Comparing 

recurrent convolutional neural networks for large scale bird species classification. 
Sci. Rep. 11. 

Haga, A., Takahashi, W., Aoki, S., Nawa, K., Yamashita, H., Abe, O., Nakagawa, K., 2019. 
Standardization of imaging features for radiomics analysis. J. Med. Investig. 66, 
35–37. https://doi.org/10.2152/jmi.66.35. 

Han, X., Peng, J., 2023. Bird sound classification based on ecoc-svm. Appl. Acoust. 204, 

109245 https://doi.org/10.1016/j.apacoust.2023.109245. 

He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition.

complexity. This helps solve the problems of model training time and 
computing  resources.  Shazzadul  Islam  et  al.  (Islam  et  al.,  2019)  used 
VGG-16  to  extract  features  from  bird  images  and  then  used  support 
vector machines for classification, achieving an accuracy of 89%. The 
ResNet network (He et al., 2016) optimises the network through residual 
modules by introducing skip connections between the traditional con-
volutional layers. This enables the network to learn and retain low-level 
feature  information  better,  thereby  avoiding  gradient  vanishing  and 
representation bottlenecks. Sankupellay et al. (Sankupellay and Kono-
valov,  2018)  used  ResNet-50  to  automatically  recognise  the  spectro-
grams of 46 bird songs with an accuracy of 60%–72%.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit details about addressing data bias during preprocessing of the deep learning pipeline. However, some common techniques used to handle data bias include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting is a technique where the dataset is divided into subsets or strata, ensuring each subset maintains the same distribution of classes as the original dataset. This approach can help prevent class imbalance issues during training and testing phases.

Oversampling involves duplicating samples from the minority class to achieve a more balanced dataset. Conversely, undersampling reduces the number of samples from the majority class to create a more balanced dataset. Both methods aim to mitigate the impact of class imbalances on model performance.

Diverse data collection focuses on gathering a wide range of examples representing different aspects of the problem domain. By collecting diverse data, it becomes possible to train models that generalize well across various scenarios and reduce potential biases introduced by limited or skewed datasets.
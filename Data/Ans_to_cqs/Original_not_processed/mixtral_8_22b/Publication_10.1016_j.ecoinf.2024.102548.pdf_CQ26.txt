Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

f (x; M) = Mconv(x)

(6)  

where Mconv  denotes the convolutional layers of the pre-trained model. 
In the fine-tuning stage, the pre-trained model’s weights are further 
optimized on the target dataset to adapt the model to the specific clas-
sification task. This involves training the additional layers added on top 
of the pre-trained model while keeping the pre-trained weights fixed or 
partially  updating  them.  The  fine-tuning  process  adjusts  the  model’s 
parameters to better fit the target dataset, capturing task-specific pat-
terns.  It  involves  minimizing  a  loss  function,  typically  a  categorical 
cross-entropy loss, using gradient-based optimization algorithms such as 
stochastic gradient descent (SGD) or Adam. The loss function measures 
the discrepancy between the predicted class probabilities and the true 
labels in the target dataset.

4.3. Parameter setting 

The experiments were performed using GPU acceleration to expedite 
computations.  The  models  were  initialized  with  pre-trained  weights 
obtained from the ‘imagenet’ dataset. To tailor the models to the specific

5.1.4. Total model parameters 

Total  parameters  in  a  neural  network encompass  the  aggregate of 
trainable weights and biases, serving as a direct measure of the model’s 
architectural complexity. This parameter count is pivotal in defining the 
model’s capacity for feature extraction and pattern recognition. High- 
parameter  models  are  adept  at  discerning  intricate  data  patterns  but 
are  susceptible  to  overfitting,  particularly  in  scenarios  with  limited 
training samples. Furthermore, such models escalate computational and 
memory  requirements,  potentially  impeding  deployment  in  resource- 
constrained environments. 

5.1.5. Total layers in the model

4.2. Data augmentation 

Data augmentation is a pivotal technique used to enhance the per-
formance  and  generalization  capability  of  deep  learning  models  by 
augmenting  the  training  dataset  with  artificially  generated  variations 
(Wu et al., 2017). Therefore, there are many augmentation techniques in 
the  literature  applied  especially  for  the  diagnosis  of  several  diseases 

EcologicalInformatics81(2024)1025485T. Chawla et al.                                                                                                                                                                                                                                 

Fig. 2. Okra Leaves: The top row shows diseased okra leaves, while the bottom row shows healthy okra leaves.  

Table 1 
Dataset split for Okra Leaves.  

Dataset 

Diseased Okra Leaves 

Healthy Okra Leaves 

Training Set 
Testing Set 
Validation Set 

885 
69 
143 

703 
69 
131

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To address data bias during preprocessing of the deep learning pipeline, various techniques can be employed. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting is a method used to ensure that each subset of the dataset maintains the same distribution of classes as the original dataset. This helps prevent any particular class from being underrepresented or overrepresented in the subsets, thus reducing bias.

Oversampling is another technique used to balance datasets where one class has significantly fewer instances compared to other classes. In this case, synthetic examples are created from existing minority class instances to increase their representation in the dataset.

Undersampling, on the other hand, reduces the number of majority class instances to match the size of the minority class. This approach may lead to information loss since it removes some instances from the dataset. However, it can still help reduce bias when dealing with imbalanced datasets.

Lastly, collecting diverse data is crucial for ensuring that the trained model performs well across different populations and conditions. By gathering data from multiple sources and varying environmental factors, we can create a more robust and unbiased dataset.

These techniques play essential roles in mitigating data bias during preprocessing stages of the deep learning pipeline. They contribute towards building fair and accurate models capable of handling real-world complexities.
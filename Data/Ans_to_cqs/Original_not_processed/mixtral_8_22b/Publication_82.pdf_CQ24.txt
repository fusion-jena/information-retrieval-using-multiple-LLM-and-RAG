Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

To summarize, we have a total of 60 experimental

conﬁgurations, which vary on the following parameters:

1. Choice of deep learning architecture:

AlexNet,
GoogLeNet.

2. Choice of training mechanism:

Transfer Learning,
Training from Scratch.

3. Choice of dataset type:

Color,
Gray scale,
Leaf Segmented.

4. Choice of training-testing set distribution:

Train: 80%, Test: 20%,
Train: 60%, Test: 40%,
Train: 50%, Test: 50%,
Train: 40%, Test: 60%,
Train: 20%, Test: 80%.

this paper, we have used the notation of

Throughout
Architecture:TrainingMechanism:DatasetType:Train-Test-
Set-Distribution to refer
experiments. For
to particular
instance,
to refer to the experiment using the GoogLeNet
learning
architecture, which was
on the gray-scaled PlantVillage dataset on a train—test
set distribution of
the notation
GoogLeNet:TransferLearning:GrayScale:60–40.

trained using transfer

60–40, we will use

30/3 epochs),
• Momentum: 0.9,
• Weight decay: 0.0005,
• Gamma: 0.1,
• Batch size: 24 (in case of GoogLeNet), 100 (in case of AlexNet).

All the above experiments were conducted using our own fork of
Caﬀe (Jia et al., 2014), which is a fast, open source framework for
deep learning. The basic results, such as the overall accuracy can
also be replicated using a standard instance of caﬀe.

RESULTS

representations of

Deep Learning for Plant Diseases

FIGURE 3 | Progression of mean F1 score and loss through the training period of 30 epochs across all experiments, grouped by experimental
conﬁguration parameters. The intensity of a particular class at any point is proportional to the corresponding uncertainty across all experiments with the particular
conﬁgurations. (A) Comparison of progression of mean F1 score across all experiments, grouped by deep learning architecture, (B) Comparison of progression of
mean F1 score across all experiments, grouped by training mechanism, (C) Comparison of progression of train-loss and test-loss across all experiments, (D)
Comparison of progression of mean F1 score across all experiments, grouped by train-test set splits, (E) Comparison of progression of mean F1 score across all
experiments, grouped by dataset type. A similar plot of all the observations, as it is, across all the experimental conﬁgurations can be found in the Supplementary
Material.

5

September 2016 | Volume 7 | Article 1419

Mohanty et al.

Deep Learning for Plant Diseases

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, based on common practices in machine learning and deep learning, there are several strategies that could potentially be applied here:

1. Fixing the Random Seed Value: One way to manage randomness is to fix the random seed value before running each experiment. This ensures that the same sequence of pseudo-random numbers is generated every time the code runs, making the results more consistent and reproducible.

2. Multiple Runs and Average Results: Another strategy is to run multiple iterations of the model with different initializations or seeds and then average the results. This helps reduce the impact of individual random variations and provides a more robust estimate of performance.

3. Cross-Validation: Using cross-validation techniques like k-fold cross-validation can also help mitigate the effects of randomness. In this approach, the data is divided into 'k' subsets, and the model is trained and tested on different combinations of these subsets. This allows for a more comprehensive evaluation of the model's performance and reduces the influence of specific random factors.

While these strategies are commonly used to address randomness in deep learning pipelines, without further contextual information, it cannot be confirmed if they were specifically implemented in Mohanty et al.'s study.
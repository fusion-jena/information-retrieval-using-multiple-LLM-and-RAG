Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Vision Transformers (ViT) (Dosovitskiy et al., 2021) are based on an 
alternative attention-based model design rather than CNN. The trans-
former  architecture was first applied on Natural Language Processing 
(NLP)  problems.  The  recent  ground-breaking  research  outputs  of 
Generative  Pre-trained  Transformer  (GPT)  and  Bidirectional  Encoder 
Representations from Transformers (BERT) are all based on the trans-
former architecture. The concepts of attention were later applied to the 
image classification problems as Vision Transformers (Dosovitskiy et al., 
2021). The transformers use the relationship between words, termed as 
attention, whereas the vision transformers determine relationships be-
tween patches of images. ViT have since achieved performance similar 
to convolution-based architectures. There are many vision transformer 
architectures for image classification but we chose MobileViT (Mehta 
and Rastegari, 2022) models proposed as a small model for embedded

Neural architecture search (NAS) was used for developing the Effi-
cientNetV2B0 model, and for the optimization of parameter efficiency 
and  training  speed  (Tan  and  Le,  2021).  The  model  performance  was 
improved  using  progressive  learning  to  adapt  regularization  to  the 
image size (Tan and Le, 2021). EfficientNetV2 is a family of models (B0 
to B7), and B0 is the smallest variant in the family (Keras, n.d), with the 
other  models  progressively  increasing  in  performance  going  up  to 
EfficientNetV2B7.

The  transfer  learning  for  the  TensorFlow  models  used  the  three 
possibilities,  feature  extraction,  fine  tuning  with  some  layers  trained, 
and  fine  tuning  with  all  layers  trained.  The  results  have  shown  an 
improvement in the classification performance with all layers trained. 
However, it is important to note that with feature extraction most of the 
pre-trained learning is preserved and used on newer tasks. The feature 
extraction was also seen as achieving high accuracy especially for small 
datasets with a reduced training time (Simoes et al., 2023; Willi et al., 
2019).  A  performance  improvement  of  MobileNetV2  model  was  re-
ported  with  the  model  fine-tuning  (Cunha  et  al.,  2021).  Similarly, 
depending on how many layers are trained in fine tuning, this can affect 
the  model  performance,  with  fine  tuning  of  more  layers  resulting  in 
learning of more features from the new dataset. In comparison to other

For model deployment to an edge device, e.g., RPi, the model needs 
to be converted to the TensorFlow Lite format, which although reduces 
its size, but also reduces the modelâ€™s accuracy. The model can then be 
used on an edge device for classification of as yet unseen images. The 
TensorFlow or TensorFlow Lite model may also optionally be optimized 
for size, or latency depending on the application requirements and the 
edge device specifications. The RPi or similar edge devices can also be 
used to  run XAI  techniques such as  Grad-CAM  for generating  visuali-
zations through heatmaps. 

3.3. Transfer learning approaches for image classification

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not provide information about the data annotation techniques used in the deep learning pipeline. However, I can share some common data annotation techniques used in deep learning pipelines for image classification tasks. These include:

1. Bounding Box Annotation: This technique involves drawing rectangular boxes around objects within an image to identify their location. It is commonly used for object detection tasks.

2. Semantic Segmentation: This technique involves labeling each pixel in an image with a class label. It is often used for scene understanding tasks where the goal is to understand the overall layout of a scene.

3. Instance Segmentation: This technique combines semantic segmentation and object detection by identifying individual instances of objects within an image and assigning them unique labels.

4. Polygon Annotation: This technique involves manually tracing the outline of objects within an image using polygons. It is useful when dealing with irregularly shaped objects.

5. Landmark Annotation: This technique involves marking specific points on an object within an image. It is often used for facial recognition tasks.

6. Image Classification: This technique involves assigning a single label to an entire image. It is commonly used for categorizing images into different classes.
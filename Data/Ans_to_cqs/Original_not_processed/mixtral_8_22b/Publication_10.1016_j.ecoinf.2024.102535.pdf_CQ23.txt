Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The  experimental  results  suggest  that  there  might  be  room  to 
improve the generalization capacity of the deep learning classifiers. That 
could  be  achieved  by  better  exploiting  the  training  data,  e.g.,  with 
additional data augmentation techniques, using recent advances in un-
supervised learning techniques, such as self-supervised methods, or by 
just simplifying the classifiers in terms of the number of parameters, thus 
reducing the risk of overfitting. 

Another direction for continuing this research is to exploit the clas-
sification uncertainty further in an active learning context. The uncer-
tainty measure could be used in interactively training the deep learning 
models,  selecting  high-uncertainty  samples  in  the  datasets,  and 
increasing their importance in the computation of the loss function. 

CRediT authorship contribution statement

In  the  figure,  each  column  contains  results  for  each  substratum 
characterization criteria, i.e., the amount of S/W fragments, Fig. 7(a)(d) 
(g)(j)(m)(p); lithology, Fig. 7(b)(e)(h)(k)(n)(q); and morphology, Fig. 7 
(c)(f)(i)(l)(o)(r).  Each  row  represents  a  different  site  combination  for 
training  and  testing  the  CDC,  which  permits  the  assessment  of  the 
generalization  capacity  of  the  ensemble  in  cross-site  evaluations.  The 
dark blue curves represent the accuracy and uncertainty of the ensemble 
when it was trained and tested using images from the same site. The light 
blue  curves  represent  cross-site  combinations  in  which  the  CDC  was 
trained with data from one site and evaluated with data from a different 
site.

6. Conclusions 

In  this  work,  six  state-of-the-art  deep  learning  architectures  were 
evaluated  for  the  problem  of  deep-sea  substratum  characterization. 
Additionally, a classification decision committee (CDC) constituted by 
an  ensemble  of  networks  with  those  architectures,  in  which  the  indi-
vidual predictions are fused through a majority voting mechanism, was 
proposed and evaluated. 

All deep learning models were evaluated on single and multi-label 
classification  problems,  and  three  different  sites  were  considered  in 
the  experiments.  Besides  the  conventional  training  scheme  in  which 
data from a single site is used for training and testing the models, we also 
evaluated  the  models  in  cross-site  scenarios,  aiming  to  assess  the 
generalization  capacity  of  the  different  architectures  and  that  of  the 

ensemble.

3.1. VGG 

To  date,  the  VGG  (Visual  Geometry  Group)  network  is  one  of  the 
most  popular  image  classification  architectures,  and  pre-trained  VGG 
models  are  commonly  used  in  transfer  learning  (e.g.,  fine-tuning) 
schemes.  It  was  proposed  in  Simonyan  and  Zisserman  (2015),  which 
aimed at investigating the effects of increasing convolutional network 
depth.  Evaluated  on  the  ImageNet  Large  Scale  Visual  Recognition 
Challenge  (ILSVRC)  (Russakovsky  et  al.,  2015)  in  2015,  the  VGG16 
model outperformed all previous participants, which comprised several 
state-of-the-art architectures.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

To ensure the generalizability of the deep learning models, several measures were taken. Firstly, the models were evaluated on both single and multi-label classification problems, providing a more comprehensive test of their performance. Secondly, data from three different sites were included in the experiments, introducing diversity into the dataset. This allowed for the evaluation of the models in various environments, enhancing their potential applicability.

Moreover, the models were not only tested in a conventional training scheme where data from a single site is used for training and testing but also in cross-site scenarios. In these scenarios, the models were trained with data from one site and then evaluated with data from a different site. This approach aimed to assess the generalization capacity of the different architectures and the ensemble.

Furthermore, the use of a Classification Decision Committee (CDC) constituted by an ensemble of networks with different architectures was proposed and evaluated. This committee employed a majority voting mechanism to fuse individual predictions, potentially improving the overall robustness and generalizability of the system.

Lastly, the experimental results suggested that there might be room to improve the generalization capacity of the deep learning classifiers. This could be achieved by better exploiting the training data, for instance, with additional data augmentation techniques or using recent advances in unsupervised learning techniques like self-supervised methods. Another possibility would be to simplify the classifiers in terms of the number of parameters, reducing the risk of overfitting.
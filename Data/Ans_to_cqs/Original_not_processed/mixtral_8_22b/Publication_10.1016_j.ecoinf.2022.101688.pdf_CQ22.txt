Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 4 
Comparison of the average F1 score across the different network architectures and dataset configurations. The exponent approach was used for the spectrogram input. 
The feature extracted was frozen. The results are averaged across 13 unique executions. The results are ordered (highest to lowed) based on the average of each 
network architecture across all configurations. The best three performing network architectures on a particular dataset configuration is highlighted in bold.  

Method 

ResNet101V2 
ResNet152V2 
InceptionResNetV2 
ResNet50V2 
DenseNet169 
DenseNet201 
VGG16 
DenseNet121 
InceptionV3 
ResNet101 
Xception 
MobileNetV2 

G 25 

95.30 
95.18 
94.70 
94.97 
94.92 
94.84 
97.26 
94.58 
92.22 
96.17 
93.88 
94.62 

G 50 

97.40 
96.92 
96.75 
97.04 
96.95 
96.72 
98.09 
96.69 
95.42 
97.49 
95.79 
96.65 

G 100 

96.27 
96.58 
96.57 
95.13 
95.69 
95.86 
94.99 
95.00 
95.40 
94.23 
95.50 
91.65 

L 25

The third experiment also holds the input representation constant at 
the  best  value  found  in  experiment  1,  and then  compares  freezing  or 
fine-tuning  the  feature  extractor.  This  was  done  for  the  12  CNNs  to 
determine, firstly, which one would benefit the most from fine-tuning 
the  feature  extractor  and  secondly,  to  determine  the  relative  perfor-
mance when the feature extractor was frozen. Two configurations for the 
input size were explored (50 and 100 samples). These two were selected 
as it contains enough data to observe performance gains as a result of 
fine-tuning the feature extractor. For this experiment we ran 35 unique 
executions â€“ a larger number of unique executions to provide a thorough 
investigation  of  fine-tuning  for  bioacoustics  classification  as  perfor-
mance gains were observed in computer vision research. Three datasets 
were used (lemurs, alethe and gibbons).

Figs. 3 and 4 present the findings for experiment 3. The CNN results 
are  presented  in  pairs  (with  fine-tuning  of  the  feature  extractor  and 
without) and the results are ordered based on the difference of each pair. 
When 50 samples were used (Fig. 3), the greatest improvement in F1 
score was obtained from ResNet101 (median improvement of 3.36%). 
An improvement was achieved by all CNNs except for Xception (median 
decrease  of  (cid:0) 0.03%).  The  best  median  performance  was  obtained  by 
ResNet152V2  and  ResNet101V2 for  both the  fine-tuning and  no fine- 
tuning  setting.  The  lowest  standard  deviation  was  obtained  by 
ResNet152V2  for  both  fine-tuning  (1.38)  and  no  fine-tuning  (1.19) 
suggesting that ResNet152V2 (no fine-tuning) was the most robust to the 
different  training  samples  from  each  unique  execution.  On  average 
across  all  CNNs,  no-fine  tuning  resulted  in  a  slightly  lower  standard

deviation  (2.07)  compared  to  fine  tuning  (1.96).  When  100  samples 
were  used  (Fig.  4),  the  greatest  improvement  was  also  obtained 
ResNet101  (median  improvement  of  1.51%).  MobileNetV2  had  a 
decrease  in  median  performance  of  (cid:0) 1.33%.  ResNet152V2  and 
ResNet101V2  obtained  the  best  performance  for  both  the  fine-tuning 
and no fine-tuning setting. On average across all CNNs, no-fine tuning

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The metric used to evaluate the performance of the deep learning models in this context is the F1 score. The F1 score is a measure of a test's accuracy that considers both the precision and recall of the test to compute the score. It is calculated as the harmonic mean of precision and recall. In this case, the F1 scores are reported for various network architectures and dataset configurations, with the results being averaged across multiple unique executions. The highest-performing network architectures for each dataset configuration are highlighted in bold.
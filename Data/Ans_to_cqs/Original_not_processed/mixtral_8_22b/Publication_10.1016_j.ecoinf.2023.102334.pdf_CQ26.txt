Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

learning and L2 regularization. Under this perspective, in this work, two 
types  of  data  were  only  used,  whereas  some  restrictions  arise  on  its 
potential application to additional datasets.

nism (Dosovitskiy et al., 2021). In contrast to CNN, ViT's Self Attention 
was  able  to  combine  the  data  from  the  entire  image,  and  accurately 
model  lengthy  sequence  information.  In  the  standard  ViT  model,  the 
relationship between distinct tokens was crucial since all images were 
turned into fixed-size tokens. By reusing the relationship matrix of the 
image  features,  the  lower  level  ViT  can  perform  more  accurate  rela-
tionship modelling. The images were converted into tokens in a process 
that  is  equivalent  to  downsampling.  A  larger  downsampling  multiple 
indicates that fewer tokens are obtained and more information is lost. 
When  more  tokens  are  acquired  and  the  downsampling  multiple  is 
lower, the lost information is less. When converting a picture to a token, 
the image's content is not taken into account. Such downsampling is too 
fine-grained for complicated pictures, which results in a loss of accuracy.

6.1. Vision transformer 

Transformer (Vaswani et al., 2017), which is a deep neural network 
first created for natural language processing (NLP), was built on a self- 
attention mechanism that makes it possible for noticeably huge recep-
tive  fields.  The Transformer  has become  a  de facto industry  standard 
procedure in NLP because of its remarkable efficiency, and it has also 
motivated  researchers  in  the  field  of  computer  vision  to  investigate 
applications  by  utilizing  the  long-term  relationships  between  pixels 
(Khan et al., 2022).

Wen, Y., Zhang, K., Li, Z., Qiao, Y., 2016. A discriminative feature learning approach for 
deep face recognition. In: European Conference on Computer Vision. Springer, 
pp. 499–515. 

Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., et al., 2021. Rethinking semantic 
segmentation from a sequence-to-sequence perspective with transformers. In: IEEE 
Conference on Computer Vision and Pattern Recognition, 2, pp. 6881–6890. 
Zhou, M., Elmore, J.A., Samiappan, S., Evans, K.O., Pfeiffer, M.B., Blackwell, B.F., 
Iglay, R.B., 2021. Improving animal monitoring using small unmanned aircraft 
systems (sUAS) and deep learning networks. Sensors 21 (17), 5697. 

Zhu, Xizhou, Su, Weijie, Lu, Lewei, Li, Bin, Wang, Xiaogang, Dai, Jifeng, 2020. 

through attention. In: Proceedings of the 38th International Conference on Machine 
Learning arXiv:2012.12877.  

Deformable DETR: deformable transformers for end-to-end object detection. CoRR 
arXiv preprint arXiv:2010.04159.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide specific details about addressing data bias during preprocessing of the deep learning pipeline. However, based on general knowledge, there are several techniques used to mitigate data bias:

1. Stratified Splitting: This technique involves dividing the dataset into training and testing sets while maintaining the same distribution of classes or labels in both sets. It ensures that the model learns from a representative sample of each class.

2. Oversampling: This method increases the number of instances in the minority class by duplicating existing samples or creating synthetic ones. It helps balance the dataset when dealing with imbalanced classes.

3. Undersampling: Contrary to oversampling, this technique reduces the number of instances in the majority class to achieve a balanced dataset. It may involve removing random samples or selecting those that contribute most to the model's performance.

4. Diverse Data Collection: Collecting diverse data from various sources can help reduce bias introduced by limited or skewed data. This includes gathering data from different geographical locations, demographics, or time periods.

These techniques aim to improve the fairness and robustness of deep learning models by reducing biases present in the input data.
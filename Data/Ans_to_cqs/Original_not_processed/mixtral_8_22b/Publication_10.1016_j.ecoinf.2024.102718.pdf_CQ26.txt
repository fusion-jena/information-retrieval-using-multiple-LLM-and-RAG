Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

individual extraction of features. Such techniques acquire characteris-
tics directly from the unprocessed data,
improving accuracy and
robustness (Archana and Jeevaraj, 2024; Mumuni and Mumuni, 2024;
Yang et al., 2023a). In addition, deep learning effectively manages
extensive datasets, enhancing the ability to apply knowledge across
many scenarios in agricultural settings.

3.3.1. Preprocessing

Our preprocessing pipeline summarized in Table 4 involves a series
of modifications meant to enhance computational performance and
ensure consistency. Initially, we reduced the images to a workable scale
of 224 × 224 pixels. To boost the diversity of our dataset, we applied
data augmentation techniques. These comprised modifications in
brightness, contrast, saturation, and hue, along with random horizontal
and vertical flips, and random rotations up to thirty degrees. Addition-
ally, Gaussian noise is incorporated to improve the robustness of our
model in different conditions. However, Figs. 3 and 4 exhibit mean
pictures employed in statistical analysis of our dataset’s image data. The
resolution of the images utilized to construct the mean images was low
leading to a loss of detail and clarity in the final averaged images in the
datasets.

3.3.2. Working plan

For our research reproducibility in Algorithm 1, the definitions
include T and ℰ, denoting the paths to the training and testing dataset
directories, respectively. The functions Dtrain and Dtest represent data
generators for training and testing datasets, where parameters such as α,
β, γ, and δ control aspects like rescaling and augmentation. The batch
size for training and testing is denoted by B, while Gtrain and Gtest
represent the respective data generators. The class weights computed for
addressing class imbalance are denoted by wc, and M stands for our
proposed model, encompassing its architecture and compilation pa-
rameters. The number of training epochs is represented by n, and P and
Q denote the steps per epoch for training and validation, respectively.
Predicted outputs and their class labels from the model are represented
by ̂y and ̂yc, while ytrue signifies the true class labels from the testing
dataset. In our model reproducibility in the testing phase in Algorithm 2,

dataset is being used to obtain the pretraining weights for our proposed
model. After augmenting the dataset, the number of images increased to
185,431 with 38 classes.

3.3. Dhan-Shomadhan dataset

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, it mentions that class weights were computed for addressing class imbalance, which suggests that some form of weight balancing or adjustment might have been implemented. This could be considered a type of oversampling technique, where the minority class samples are given more importance during training. Additionally, the use of diverse data collection methods, including various brightness, contrast, saturation, and hue modifications, as well as random horizontal and vertical flips and rotations, can help increase the variety of data and potentially reduce bias. Furthermore, incorporating Gaussian noise into the dataset may also contribute to improving the robustness of the model under different conditions.
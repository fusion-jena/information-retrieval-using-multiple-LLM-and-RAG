Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

to calibrate Deep Learning models using a single parameter for all  classes35. This parameter is used, instead of 
the traditional softmax function, to convert the vector output from the neural network into a real probability.

Whatever the goal, our framework is highly flexible and can be adapted by tuning the species thresholds 
regulating the trade-off between classification robustness and coverage in an attempt to monitor biodiversity 
through big datasets where species are unidentified. To unclog the bottleneck of information extraction about 
organism forms, behaviors and sounds from massive digital data, machine learning algorithms, and particularly 
the last generation of deep learning algorithms, offer immense promises. Here we propose to help the users to 
control their error rates in ecology. This is a valuable addition to the ecologist’s toolkit towards a routine and 
robust analysis of big data and real-time biodiversity monitoring from remote sensors. With this control of error 
rate in the hands of users, Deep Learning Algorithms can be used for real applications, with acceptable and 

Scientific RepoRtS  |        (2020) 10:10972  | 

https://doi.org/10.1038/s41598-020-67573-7

11

training datasets.

•  In the first phase, a classification model is built by training a CNN on a given database T0 (Fig. 2a)
•  Then, the second phase consists of tuning a risk threshold τi specific to each class (i.e. each species in our 

case), noted i, with i ∈ {1, ..., n} , using a second and independent database noted T1 (Fig. 2b).

In terms of classification, it means we transform the 2 classification options (correct, wrong) in 3 options 

(Fig. 3) by applying Eqs. (15, 16).

Computing the confidence thresholds.  After the phase 1 (model training phase), for an image X of the 
threshold tuning dataset processed by the classifier, we obtain an output {C(X), S(X)} , where C(X) is the class 
(i.e. species, belonging to the trained set of species) with the highest classification score S(X). For this image, we 
know the ground truth Y in {1, .., n} belonging to the same set of species classes.

that can feed initiatives like the Group on Earth Observations Biodiversity Observation Network (GEO BON) 
and provide robust estimates of Essential Biodiversity Variables (EBVs)3,4.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention any specific techniques such as diverse datasets, cross-validation, or stratified splitting to ensure the generalizability of the deep learning model. However, there are some indications that suggest efforts have been made to improve the model's performance and applicability.

Firstly, the authors use two separate databases, T0 and T1, during different phases of the process. The initial phase involves building a classification model by training a Convolutional Neural Network (CNN) on the T0 database. Subsequently, the second phase focuses on tuning a risk threshold specific to each class using the independent T1 database. By employing distinct datasets for these stages, the researchers aim to enhance the model's adaptability and prevent overfitting.

Secondly, the authors discuss adjusting species thresholds to regulate the trade-off between classification robustness and coverage. This approach allows them to customize the model based on the user's requirements, making it more versatile and applicable across various scenarios.

Lastly, the paper mentions providing robust estimates of Essential Biodiversity Variables (EBVs), which implies that the model has been designed to deliver reliable results even when dealing with large and complex datasets. Although the text does not detail the exact methods employed to achieve this, it suggests that the researchers have considered the need for generalizability while developing their deep learning algorithm.
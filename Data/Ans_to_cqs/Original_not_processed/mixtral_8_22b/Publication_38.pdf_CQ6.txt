Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

14 / 19

Deep learning tools for bat acoustic signal detection

S1 Fig. CNNFAST network architecture description. The CNNFAST network consists of two
convolution layers (Conv1 and Conv2), with 16 filters each (shown in yellow, with the filter
size shown inset). Both convolution layers are followed by a max pooling layer (Max Pool1 and
Max Pool2), and the network ends with a fully connected layer with 64 units (Fully Connect).
CNNFAST computes feature maps (shown as white boxes) across the entire input spectrogram,
resulting in less computation and a much faster run time. The fully connected layer is also eval-
uated as a convolution. The output of the detector is a probability vector (shown in green)
whose length is one quarter times the width of the input spectrogram. The numbers below
each layer indicate the height, weight, and depth of the corresponding layer.
(TIF)

and semantic segmentation. Proceedings of the IEEE conference on computer vision and pattern rec-
ognition; 2014.

34. Piczak KJ, Environmental sound classification with convolutional neural networks. 2015 IEEE 25th

International Workshop on Machine Learning for Signal Processing (MLSP); 2015: IEEE.

35. Salamon J, Bello JP. Deep convolutional neural networks and data augmentation for environmental

sound classification. arXiv preprint arXiv:160804363. 2016.

36. Hershey S, Chaudhuri S, Ellis DP, Gemmeke JF, Jansen A, Moore RC, et al. CNN Architectures for

Large-Scale Audio Classification. arXiv preprint arXiv:160909430. 2016.

37. Hinton G, Deng L, Yu D, Dahl GE, Mohamed A-r, Jaitly N, et al. Deep neural networks for acoustic

modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing
Magazine. 2012; 29(6):82–97.

38. Hannun A, Case C, Casper J, Catanzaro B, Diamos G, Elsen E, et al. Deep speech: Scaling up end-to-

we also created a second, smaller CNN which included fewer model weights that can be run in
a fully convolutional manner (CNNFAST) (Supplementary Information Methods, Supplemen-
tary Information S1 Fig). (4) Call Detection Probabilities–The probabilistic predictions pro-
duced by the sliding window detector tended to be overly smooth in time (Fig 1D). To localise
the calls precisely, we converted the probabilistic predictions into individual detections using a
non-maximum suppression to return the local maximum for each peak in the output predic-
tion (Fig 1E). These local maxima corresponded to the predicted locations of the start of each
search-phase bat echolocation call, with associated probabilities, and were exported as text
files.

Our result that deep learning networks consistently outperformed other baselines, is consis-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it does not provide explicit information about the specific preprocessing steps used before training the deep learning models. However, common practices in preparing data for deep learning include normalization, scaling, and cleaning. Normalization involves rescaling the values of features so they fall within a certain range, often between 0 and 1 or -1 and 1. This helps prevent any single feature from dominating the others due to its scale. Scaling is similar to normalization but focuses more on adjusting the range of numerical values to avoid issues caused by large differences in magnitudes. Cleaning refers to handling missing or inconsistent data points, such as removing them entirely or imputing their values based on available information. In this case, since the study uses audio signals, additional preprocessing might involve converting raw audio files into spectrograms or Mel-frequency cepstrum coefficients (MFCCs) to better represent the frequency content of the sounds.
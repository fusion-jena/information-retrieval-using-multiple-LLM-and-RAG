Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Xu, Z., Yu, K., Tresp, V., Xu, X., & Wang, J. (2003). Representative sampling 
for  text  classification  using  support  vector  machines.  In  European 
conference on information retrieval (pp. 393–407). Springer.

Wiley & Sons.

Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learn-
ing with neural networks. In Advances in neural information processing 
systems (pp. 3104–3112).

Swanson, A., Kosmala, M., Lintott, C., Simpson, R., Smith, A., & Packer, 
C.  (2015).  Snapshot  serengeti,  high-frequency  annotated  camera 
trap images of 40 mammalian species in an african savanna. Scientific 
Data, 2, 150026.

|  159

because eventually both methods have sufficient constraints to learn 

active  learning  pipeline  on  the  crops  produced  from  running  the 

a good embedding.

3.3.2 | Active learning strategies

MegaDetector  model  over  the  NACTI  dataset.  We  employed  mar-

gin-based active learning. After the first 30,000 active queries, the 

classifier  achieves  93.2%  overall  accuracy  which  further  confirms 

the usefulness of the suggested pipeline. More detailed results are 

available in Table S3.

Different strategies can be employed to select samples to be labelled 

by the oracle. The most naive strategy is selecting queries at random. 

Here  we  try  five  different  query  selection  strategies  and  compare 

4 |  D I S CU S S I O N

them against a control of selecting samples at random. In particular, 

we try model uncertainty criteria (confidence, margin, entropy; Lewis 

This  paper  demonstrates  the  potential  to  significantly  reduce

3 |  R E S U LT S

retaining  speed.  Before  processing  the  crops  from  a  target  dataset, 

we learn an embedding model (a deep neural network) on a large data-

As  explained  above,  our  suggested  pipeline  consists  of  three 

set, and use this model to embed the crops from our target dataset 

steps: (a) running a pre-trained detector model on images, (b) em-

into  a  256-dimensional  feature  space.  We  chose  256  features  after 

bedding the obtained crops into a lower-dimensional space and (c) 

preliminary experimentation showed it performed better than 64 and 

running an active learning procedure. In this section, we report the 

128. Choosing more than 256 features slows down the active learning 

results of our pipeline and analyse the contribution of these steps 

procedure. The embedding model turns each image into a 256-dimen-

to  the  overall  results.  For  these  results,  the  eMammal  Machine

Algorithm 1. Active learning procedure

1:

2:

3:

4:

5:

6:

7:

8:

Start from a small, randomly selected labelled subset of data

while Stopping criteria not met do

Train the underlying model with the available labelled samples

Compute a selection criterion for all the samples in the 

unlabelled pool

Select n samples that maximize the criterion

Pass the selected samples to the oracle for labelling

Gather the labelled samples and add them to the labelled 

set

end while

this case, a network can first be trained on the large dataset and then 

Deep learning usually requires a large-scale dataset and a con-

fine-tuned on the target dataset (Norouzzadeh et al., 2018; Yosinski 

siderable amount of computational resources to achieve high accu-

et al., 2014).

1.5 | Active learning

racy.  Existing  active  learning  frameworks  cannot  scale  to  datasets 

with millions of high-dimensional samples, such as large camera trap

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not explicitly discuss strategies to handle randomness in the deep learning pipeline, such as managing the impact of random seed values. However, some general approaches to manage randomness include setting fixed seeds for reproducibility, employing ensemble methods, or averaging multiple runs with different initializations. These techniques help mitigate the effects of randomness and improve the robustness of deep learning models.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.5.2. Random Forest

For random forest, input features included the mean and standard deviation for image
window sizes of 5 × 5, 15 × 15, and 32 × 32 in addition to the central pixel values for
each input feature. We used a random search to identify an optimal parameter set where
100 sets were sampled and compared using three-fold cross validation. Search parameter
ranges were number of trees (200 to 800, by 100), minimum samples split (2–4), minimum

Remote Sens. 2021, 13, 634

7 of 21

samples per leaf (2–4), maximum depth (full), bootstrap (true), and maximum features for
split (square root of the number of features). For a description of these parameters see the
Scikit-learn documentation [48]. These results were reﬁned using a grid-search with the
parameter ranges set to ±50 for the number of estimators and ±1 for the other parameters.

2.6. Membership Based Change Detection and Segmentation
2.6.1. Split Window Detection of Change Seed Points

For training, the Adam optimizer was used with 500 epochs and categorical-cross
entropy loss function. Batch size was set to 64. Data augmentation was applied to enhance
sample variation and included rotation, reﬂectance bias of ±10%, and random noise of
±10% reﬂectance for 15% of the input image. To determine training convergence, we
monitored the validation data across training epochs and the network weights with the
best performance were kept. Early stopping criteria was applied if no improvement was
found in 50 epochs. For CNNs, weight initialization and random selection for batch
training can lead to variability in the results. Thus, an ensemble of three models were
generated and the average of the ﬁnal output layer taken. Max pooling was used for two
models in the ensemble and average pooling for one. In initial tests using max or average
pooling showed that both provided improvement in some cases and thus could enhance

3. Results and Discussion
3.1. Classiﬁcation

The deep-learning ensemble CNN achieved a higher accuracy than random forest
by 6% for the average summary F1-score. More importantly, class accuracies were more
balanced (Table 1). Classes where spatial properties were evident increased the most. For
example, the CNN was better able to separate croplands with high soil exposure from
other bare land covers. All classes except for wetland and shrub had an F1-score greater
than 80%. Shrub had the lowest class accuracy and was confused with several classes, but
most predominantly with wetland and forest (Table 2). Wetland was mostly confused with
forest, followed by crops. These results are generally consistent with other large-extent
land-cover mapping applications using Landsat in Canada [15,26,40], the US [51], and with
the reported accuracies in the AAFC ACI metadata [41].

Table 1. Comparison of the ensemble CNN and random forest classiﬁers.

Classiﬁer

Ensemble
CNN

Random
Forest

2.5. Classiﬁcation and Assessment

In addition to the deep-learning classiﬁer, we also tested random forest that included
spatial features for comparison. Random forest is an efﬁcient algorithm that can be easily
applied in a distributed processing framework. Thus, if sufﬁcient accuracy could be
achieved then it would be the preferred classiﬁer. For assessment, we evaluated the
classiﬁcation results against the test sample and report error matrices, summary accuracy
measures, and class accuracies. Summary measures included the overall accuracy, kappa,
and average F1-score. For class accuracies we report the F1-score, which provides class-
speciﬁc accuracy, accounting for both omission and commission error.

2.5.1. Ensemble CNN

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To handle randomness in the deep learning pipeline, such as those introduced by random seed values, the authors employ several strategies. Firstly, they use an ensemble of three models to reduce variability caused by weight initialization and random selection during batch training. The final output layer of each model is averaged to produce the result. Secondly, they apply early stopping criteria when monitoring the validation data across training epochs. This means that if there is no improvement found after 50 epochs, the training process will stop. Lastly, they utilize different types of pooling layers within their convolutional neural networks (CNNs). Two models in the ensemble use max pooling, while one uses average pooling. This approach allows them to take advantage of the benefits offered by both methods, potentially improving the overall performance of the system.
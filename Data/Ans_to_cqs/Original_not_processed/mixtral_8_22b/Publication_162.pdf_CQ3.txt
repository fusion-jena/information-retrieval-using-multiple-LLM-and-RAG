Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

32. M. Simon, E. Rodner, and J. Denzler, “Part detector
discovery  in  deep  convolutional  neural  networks,”  in
Asian  Conference  on  Computer  Vision  (ACCV)  (2014),
pp. 162–177.

33. Van G. Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun,
A.  Shepard,  H.  Adam,  P.  Perona,  and  S.  Belongie,
“The iNaturalist Species Classification and Detection
Dataset,” in IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2018), pp. 8769–8778.
34. G. Yang and D. Ramanan, “Upgrading optical flow to
3D scene flow through optical expansion,” in Proceed-
ings  of  the  IEEE/CVF  Conference  on  Computer  Vision
and Pattern Recognition (CVPR) (2020), pp. 1334–1343.

PATTERN RECOGNITION AND IMAGE ANALYSIS 

 Vol. 31 

 No. 3 

 2021

488

RADIG  et  al.

We then trained a standard deep learning classifier
(ResNet-50 [16] pre-trained on ImageNet [30]) on the
cropped  bounding  box  images  of  the  detected  moths
and  achieved  an  average  accuracy  of  89.00%  on  an
held-out test set. We also make use of additional images
obtained  from  Internet  image  search  engines  to  aug-
ment the training dataset. However, since this leads to
noisy annotated images, filtering different types of label
noise  is  required  as  proposed  by  [6].  This  can  further
improve  the  recognition  performance  of  the  system,
especially when the initial training set has been rather
small with less than ten sample images per species.

CONCLUSIONS

Detection rates in the laboratory or at test data sets
of more than 90% can generally not be achieved in the
field under real, constantly changing conditions. For
this reason, a feedback mechanism is provided, which
involves the user in the optimization of the system, but
at  the  same  time  minimizes  the  labeling  effort  to  be
made  manually.  This  is  known  as  active  learning.  In
the actual application, the system will, at certain inter-
vals, forward images to humans, and after the annota-
tion these images will be used to train a classifier iter-
atively. Evaluation is carried out with a retained test set
of images annotated manually.

Monitoring  devices  will  likely  also  detect  species
that are unknown for the recognition system. There-
fore, novelty detection is relevant [2, 3] which allows

results in an unchanged expected activation in the fol-
lowing  convolutional  layers.  Furthermore,  we  nor-
malize all input channels (red, green, blue, and depth)
by subtracting the respective mean and dividing by the
respective standard deviation over the whole dataset.
We also introduce a feature fusion module to combine
the extracted features from both backbones using one
3 × 3 convolution per FPN scale and reduce the num-
ber of channels from 2 × 256 to 256. This allows us to
use weights pre-trained on the Microsoft COCO data-
set [24] for the region proposal network (RPN) classi-
fier and mask head. Fig. 4 illustrates the general archi-
tecture of the resulting D-Mask R-CNN architecture.
We  restrict  our  evaluation  of  D-Mask  R-CNN  to
instances of deers as it is the most common species in our
RGB-D dataset. We quantify the results of D-Mask R-
CNN using the average precision (AP), AP50, and AP75
metrics as defined by the Microsoft COCO dataset [24].

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, there are several references to data annotation techniques used in the deep learning pipeline. The authors mention using a standard deep learning classifier (ResNet-50) pre-trained on ImageNet, which suggests they have employed object classification techniques. They also discuss cropping bounding boxes around detected moths and training their model on these cropped images, indicating the use of bounding box annotations. Additionally, they refer to augmenting their training dataset with images obtained from internet search engines, implying some form of manual or semi-automatic annotation process.

Moreover, the authors describe making use of a feedback mechanism involving human users to optimize the system, suggesting the incorporation of active learning principles into their annotation workflow. Active learning involves selecting the most informative samples for manual annotation based on the current state of the model, thereby reducing overall labeling efforts while maintaining high performance.

Lastly, the authors briefly touch upon the concept of novelty detection, which could potentially involve another type of annotation technique aimed at identifying previously unseen classes or objects within the dataset. However, specific details regarding this aspect are not explicitly mentioned in the provided context.
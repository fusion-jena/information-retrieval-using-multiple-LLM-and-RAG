Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.5. Model selection procedure 

The model selection procedure was performed as follows (Capinha 
et  al.,  2021;  Van  Kuppevelt  et  al.,  2020):  we  randomly  generated  5 
models for each of the four available deep-ANN architecture types (20 
models in total) and trained each one with a small subset of the training 
data  (data  partition  At)  for  4  epochs  (an  “epoch”  corresponds  to  the 
complete training dataset being passed forward and backward across the 
network  one  time;  Capinha  et  al.,  2021).  The  accuracy  of  candidate 
models,  as  provided  by  mcfly  (i.e.,  the  “proportion  of  cases  correctly 
classified”), was then compared using a left-out validation data set (data 
partition Av) and the model with the highest performance was selected 
for training on the full training data (data partition Bt; Bt = At + Av) for 
up to 30 epochs.

Table 3 
Parameters tested and values for each classical machine learning model used and each test year. xgbTree = extreme gradient boosting tree, RF = random forest, NNET =
neural network, DNN = deep neural network.  

Model 

xgbTree 

RF 
NNET 

DNN 

Parameter 

2013 

2014 

2015 

2016 

2017 

2018 

2019 

Nrounds 
max_depth 
Eta 
Gamma 
colsample_bytree 
min_child_weight 
Subsample 
Mtry 
Size 
Decay 
layer1 
layer2 
layer3 
hidden_dropout 
visible_dropout 

150 
3 
0.4 
0 
0.8 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.8 
1 
1 
2 
3 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150  
3  
0.3  
0  
0.6  
1  
1 
2 
5  
0.1 
1  
0  
0  
0  
0

The data used for model training and selection were randomly par-
titioned into: data for training candidate models (25% of the data; At); 

data for validating candidate models (50%; Av); data for training the 
selected  candidate  model  (75%;  Bt  = At  + Av);  validation  data  to 
determine the optimal number of epochs to train the selected candidate 
model (25%; Bv). Data partition was performed in R with package dismo 
(Hijmans et al., 2017; R Core Team, 2022). 

2.5. Model selection procedure

10.3 
1.6 
0.6 
74.9 
0.8 
0.5 
11.4 

8.4 
1.0 
0.7 
79.4 
0.5 
0.3 
9.6 

10.4 
0.9 
0.8 
75.3 
0.7 
0.5 
11.4  

represented  correspond  to  the  lower  decreases  or  increases  in  the 
number of eggs (i.e., 1 to 25% and > 25 to 50%; Table 1). 

3.2. Deep learning model accuracy, predictive performance, and predictor 
importance 

Concerning the performance of the candidate models, the best vali-
dation accuracies were achieved by model 12 in all the years (accuracy 
= 0.833 ± 0.005; mean of years ± sd) (Fig. 2), a model having a Deep 
Convolutional Long Short-Term Memory (DeepConvLSTM) architecture. 
Model  19,  also  with  a  DeepConvLSTM  architecture  had  high  vali-
dation accuracies too (accuracy = 0.829 ± 0.003; mean of years ± sd), 
followed  by model 14, a Convolutional Neural Network  (CNN) archi-
tecture reaching a validation accuracy of 0.821 ± 0.005 (mean of years 
± sd).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps can be applied to the input data. These may include:

1. Normalization: This involves rescaling the features so they have zero mean and unit variance. It is often done to ensure that different features contribute equally to the final output.

2. Scaling: Similar to normalization, this step involves transforming the feature values to a specific range, such as between 0 and 1 or -1 and 1. Scaling helps improve convergence during optimization.

3. Cleaning: This process removes any missing or inconsistent data points from the dataset. Inconsistencies could arise due to errors in measurement, recording, or transmission.

4. Encoding categorical variables: Categorical variables need to be converted into numerical form before feeding them into a deep learning algorithm. One common method is one-hot encoding, which converts each category into a binary vector.

5. Feature extraction/selection: Depending on the complexity of the problem, it might be necessary to extract relevant features from raw data or select only those features that significantly impact the target variable.

In the given context, no explicit mention of these preprocessing steps has been made. However, based on general practices in deep learning, some or all of these steps would likely have been carried out prior to training the candidate models.
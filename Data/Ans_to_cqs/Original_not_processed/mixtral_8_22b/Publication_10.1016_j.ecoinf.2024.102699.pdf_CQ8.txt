Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 2
Overview of our augmentation strategies, their IDs, and related information. For DenseNet-161 and ViT-B/16, their training times are provided in minutes per epoch
for the different augmentation methods. For IDs 5 and 6, we added noise of the bird audio detection data set from the DCASE challenge (Berger et al., 2018; Himawan
et al., 2018; Liaqat et al., 2018), which is abbreviated as BAD.

Data augmentation

Time domain

Frequency domain

Spectrogram

Training time [min. / epoch]

DenseNet-161

ViT-B/16

Gaussian noise
Pink noise (background)
Primary background noise soundscapes
Secondary background noise soundscapes
Primary background noise (BAD)
Secondary background noise (BAD)
Background noise (e.g., wind, thunder, or aircrafts)
Mixed up random bird species
Horizontal roll
Vertical roll
Pitch shift
Time mask
Frequency mask
Gain
Loudness normalization
Horizontal flip
Vertical flip
Time stretch
tanh-based distortion

✓
✓
✓
✓
✓
✓
✓

✓

✓
✓

✓

For the ViT models, we adapted the approach of pretraining deep
bidirectional transformers for language understanding (BERT) by Devlin
et al. (2018) for birdsong classification. Table 3 provides an overview of
the investigated ViT models with their respective parameterizations.
ViT-S/16, for example, denotes a ViT variant of reduced complexity with
an input patch size of 16 × 16. The sequence length of the transformer
models is inversely proportional to the square of the provided patch size.
In the following, ViT-B/16 is further investigated as it strikes a balance
between general model complexity and classification performance.

training or validation progress could be observed. To fine-tune our ViT
models, we deployed ImageNet-based weights for pretraining.

In ecology, changes in environmental conditions are often closely linked to shifts in species diversity. This
relationship can be investigated by analyzing avian vocalizations, which are robust indicators of trends in
biodiversity. Within this contribution, we explored various data augmentation techniques and deep learning
strategies for the classification of birdsong within natural soundscapes. For this purpose, we employed three
fundamental deep neural network architectures, such as vision transformers, to classify 397 different bird spe-
cies. To improve both the accuracy and generalizability of our models, we incorporated up to 19 well-established
data augmentation techniques commonly used in audio classification. This included an iterative selection process
where only augmentations that enhanced classification performance were selected. The primary augmentation
technique involved the integration of various noise samples and non-bird audio elements, which significantly

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide any direct information about where the code repository of the deep learning pipeline is available. It discusses various aspects of the research conducted on birdsong classification using deep learning models like Vision Transformers (ViT). However, there is no mention of the specific location of the code repository. Therefore, I cannot determine where the code repository of the deep learning pipeline is available based on the provided context.
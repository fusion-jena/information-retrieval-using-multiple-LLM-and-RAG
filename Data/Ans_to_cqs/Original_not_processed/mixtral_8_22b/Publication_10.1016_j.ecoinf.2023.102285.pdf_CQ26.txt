Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The rest of this document is structured as follows: The methodology 
is revealed in Section 2. The datasets on which the methodology is built 
and tested are explored in Section 3. A summary of the findings is pro-
vided in Section 4. Discussions are outlined in Section 5. Section 6 draws 
a conclusion as well as future potential works. Finally, the limitations of

References 

Abu, A., Diamant, R., 2022. Feature set for classification of man-made underwater 

objects in optical and sas data. IEEE Sensors J. 22 (6), 6027–6041. http://dx.doi.org/ 
10.1109/JSEN.2022.3148530. 

Bibby, C., Jones, M., Marsden, S., 1998. Expedition Field Techniques: Bird Surveys. Royal 

Geographical Society, London.  

Çalıs¸kan, A., 2023. Detecting human activity types from 3d posture data using deep 
learning models. Biomed. Signal Process. Control 81, 104479. http://dx.doi.org/ 
10.1016/j.bspc.2022.104479. 

Clements, N., Robinson, W., 2022. A re-survey of winter bird communities in the Oregon 
coast range, USA, initially surveyed in 1968-1970. Biodiers. Data J. http://dx.doi. 
org/10.3897/arphapreprints.e91575. Aug.  

Davis, K.L., Silverman, E.D., Sussman, A.L., Wilson, R.R., Zipkin, E.F., 2022. Errors in 

aerial survey count data: identifying pitfalls and solutions. Ecol. Evol. 12 (3), e8733 
http://dx.doi.org/10.1002/ece3.8733.

EcologicalInformatics78(2023)1022858K. Kuru et al.                                                                                                                                                                                                                                    

Fig. 12. Stationary example 2: man-made object detection.  

overfitting, and finally iii) let field experts evaluate the model with a 
completely new dataset independent from the first and second datasets. 
The model can be deployed if it passes these three phases successfully. 
These phases are outlined in Fig. 3. The obtained results as well as their 
evaluation  are  provided  in  the  following  section.  The  experimental 
design of data utilisation and data processing phases with their targeted 
objectives are outlined in Table 3 regarding the APEM‘s database. The 
viability of the methodology was ensured in 4 phases.

directory by the proposed technique in this study, which is proposed as a 
future work in Section 6. Bespoke semi-supervised ML approaches (e.g., 
SelfMatch in (Xing et al., 2022a)) can be a good candidate for addressing 
this type of research question by extracting features from labelled data 
and comparing them with the features that are obtained from detected 
ISMMMOs based on the semantic information (e.g., (Xiao et al., 2022) 
and  a  feature/distance-based  matching  scheme  (e.g.,  (Xing  et  al., 
2022b)) considering various pose compositions (e.g., (Çalıs¸kan, 2023)). 
The methodology not only distinguishes ISMMMOs from the blank 
background (sea canvas with waves in many different shapes), but also 

Table 6 
Statistical analysis using 7 cut-off points. 55 images with ISMMMOs where μs
0.25 and < 0.50.  

>

Cut-off 

μ - 2σ 
μ - σ 
μ - σ/2 
μ 
μ þ σ/2 
μ þ σ 
μ þ 2σ 

TP 

55 
55 
54 
47 
41 
32 
13 

FN 

0 
0 
1 
8 
14 
23 
42 

TN 

627 
721 
998 
1000 
1000 
1000 
1000

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To address data bias during preprocessing of the deep learning pipeline, several techniques can be employed. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into training and testing sets while maintaining the same distribution of classes or labels in both sets. This ensures that the model is trained and evaluated on representative samples of each class.

Oversampling is a technique used when there is a significant imbalance between classes in the dataset. It involves duplicating examples from the minority class until its size matches that of the majority class. Alternatively, synthetic examples can be generated using methods such as SMOTE (Synthetic Minority Overpopulation Technique).

Undersampling, on the other hand, reduces the number of examples in the majority class to match the size of the minority class. This can be done randomly or through more sophisticated methods like Tomek links or NearMiss.

Finally, collecting diverse data helps ensure that the model generalizes well across different populations and scenarios. This can involve gathering data from multiple sources, capturing variations within each class, and accounting for any biases present in the original dataset.

Based on the given context, it appears that the authors have utilized some form of stratified sampling during their experiments, as they mention evaluating their model with a completely new dataset independent from the first and second datasets. However, specific details about how they addressed data bias during preprocessing are not explicitly mentioned in the provided text.
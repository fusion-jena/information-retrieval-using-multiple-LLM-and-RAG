Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The Neural Networks were built from scratch, implemented using
TensorFlow (tf) (Abadi et al., 2015) and Keras (Chollet et al., 2015).
Coupled with TensorFlow, an open-source framework developed by
Google, Keras serves as a high-level Neural Networks API, offering a
seamless and user-friendly approach to building, training, and deploying
deep learning models. The MCD layers consist of a modification of the
Keras Dropout class, while the layers based on the VBI method derive
from the TensorFlow Probability (tfp) module (in particular, we used the
DenseVariational type of layers). These two architectures have the same
input and output layers, they differ for the structure of the hidden layers.
The size of the input layer depends on the problem we are facing: it has
two nodes, one for DOY and the other for GDD. As our focus will be on
dealing with regression problems, the output layer is actually a combi-
nation of a fully-connected layer (or Dense layer) with two nodes and a

siderations made earlier, a Bayesian Deep Learning model, if properly
configured and trained, can return satisfactory results even with limited
datasets, which is the scenario studied in the present work.

Fig. 4. Neural networks architecture consists of: an input layer with two nodes, one for DOY and the other for GDD; two hidden layers with 32 nodes each, Dense
layers with Monte Carlo Dropout in the MCD approach, DenseVariational layers in the VBI approach; an output layer which is the combination of a Dense layer with
two nodes and a DistributionLambda layer that maps the values of these two nodes into the parameters of a Gaussian distribution, used to predict the BBCH value.

Fig. 5. Prediction, over 1000 runs, of the BBCH_res value of a single test point. The actual value is BBCH_res = 4. In blue we have the result of the VBI approach, in
red that of the MCD approach. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

Fig. 6. Performance of the two different architectures during the prediction phase on test set. Temperature threshold, Tbase, is set to 0
interval is shown using the percentiles.

◦

• Second method: use of BBCH_res values during the training phase
(standardization is applied here too); the output is converted to the
BBCH scale through a dictionary; in this case, in order to use the
dictionary, it is necessary to apply the rounding function on the

output before converting it to the BBCH scale, so it is not possible to
evaluate the results with and without rounding separately.

Before proceeding, an important observation must be made: in order
to use the dictionary in the second method, it is necessary to ensure that
the network outputs are consistent with the keys of the dictionary itself;
therefore negative values must be forced to zero, while values that
exceed the upper bound of the keys, i.e. 91, must be forced to 91.
Furthermore, aiming to have a legitimate comparison between the two
methods, these corrections are also made during the execution of the
first method (even if not strictly necessary).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there isn't any explicit mention of specific data formats such as images, audio, or video being used in the deep learning pipeline. However, since the neural networks are designed to handle inputs like Day Of Year (DOY) and Growing Degree Days (GDD), it suggests that the data format used might be numerical data, possibly stored in structured formats like CSV files. This assumption is further supported by the fact that the study focuses on dealing with regression problems, where the goal is often to predict continuous outcomes based on numeric input features. Therefore, it seems likely that the data format used in this deep learning pipeline would involve some form of tabular or numerical data rather than multimedia formats like images or videos.
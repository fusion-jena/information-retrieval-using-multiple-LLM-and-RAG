Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

respectively.  N  represented  the  number  of  images  in  the  test  set  or 
validation set. 

2.2.6. Model training and hyper-parameters 

We  trained  models  on  the  training  sets  and  tracked  the  model 
training process on the validation set. The training process was stopped 
when  the  loss  value  of  the  model  on  the  validation  set  no  longer 
decreased. We then tested the model performance on the test set. Our 
computing platform was a Dell PowerEdge C4130 rack server with two 
Tesla  K80  GPUs  and  256  GB  of  memory.  For  the  deep  learning  envi-
ronment,  we  utilized  the  PyTorch  1.7.1  platform.  We  employed  the 
Stochastic Gradient Descent (SGD) optimizer with a momentum value of 
μ  = 0.9 to train the model. Other hyper-parameter settings for model 
training were shown in Table S.4 of the Supporting Information. 

3. Results 

3.1. Experiment results of transfer strategy optimization

When it comes to selecting update layers, we recommend selectively 
updating the last few layers of the DCNN model to enhance its perfor-
mance in empty image recognition. This approach can lead to a signif-
icant  improvement  in  the  model’s  performance,  as  opposed  to  solely 
updating  the  fully  connected  layer.  Specifically,  when  using  a  model 
from  the  ResNext  or  ResNet  family  pre-trained  on  ImageNet  as  the 
source model, we recommend using TLS_4 as the transfer strategy. In 
contexts where pronounced discrepancies exist between the target and 
source  domain  datasets,  it  could  prove  advantageous  to  augment  the 
number of convolutional layers subject to update. For example, in the 
case  of  ResNext  series  models,  using  TLS_3  can  further  enhance  the 
performance of the transfer model.

We  used  the  ResNext-101  model  to  systematically  study  transfer 
learning techniques. This model combines the advantages of the ResNet 
and InceptionV3 models, which are widely used for the empty image 
recognition task. Our proposed optimal update layer schemes based on 
the ResNext-101 model can be applied to any member of the ResNext 
family  (i.e.,  ResNext-X,  where  X  = 18,  34,  50,  101,  152,  etc.).  We 
speculate that these schemes are also suitable for models of the ResNet 
family (i.e., ResNet-X, where X = 18, 34, 50, 101, 152, etc.), but further 
validation is still needed in practical projects.

Yang, D.-Q., Ren, G.-P., Tan, K., Huang, Z.-P., Li, D.-P., Li, X.-W., Wang, J.-M., Chen, B.- 
H., Xiao, W., 2021b. An adaptive automatic approach to filtering empty images from 
camera traps using a deep learning model. Wildl. Soc. Bull. 45, 230–236. https://doi. 
org/10.1002/wsb.1176. 

Yang, D.-Q., Tan, K., Huang, Z.-P., Li, X.-W., Chen, B.-H., Ren, G.-P., Xiao, W., 2021c. An 
automatic method for removing empty camera trap images using ensemble learning. 
Ecol. Evol. 11, 7591–7601. https://doi.org/10.1002/ece3.7591. 

Yosinski, J., Clune, J., Bengio, Y., Lipson, H., 2014. How transferable are features in deep 

neural networks?. In: Proceedings of the 27th International Conference on Neural 
Information Processing Systems - Volume 2, NIPS’14. MIT Press, Cambridge, MA, 
USA, pp. 3320–3328.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific regularization methods used to prevent overfitting in their deep learning pipeline. However, some common regularization methods used in deep learning include dropout, L1 regularization, and L2 regularization. Dropout randomly drops out neurons during training to prevent overfitting by making the network more robust to changes in input data. L1 regularization adds a penalty term to the cost function that encourages sparsity in the weights, while L2 regularization adds a penalty term that discourages large weight values. These methods help prevent overfitting by reducing the complexity of the model and improving generalization to new data. It is possible that one or more of these methods may have been used in this study, but without additional information, it cannot be confirmed.
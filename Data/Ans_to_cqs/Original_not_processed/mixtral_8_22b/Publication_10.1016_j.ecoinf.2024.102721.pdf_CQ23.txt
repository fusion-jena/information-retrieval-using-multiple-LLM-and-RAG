Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Contents lists available at ScienceDirect

Ecological Informatics

journal homepage: www.elsevier.com/locate/ecolinf

Forecasting ecological water demand of an arid oasis under a drying
climate scenario based on deep learning methods

Xu-dong Wang a, Hao-jie Xu a,b,*, Yan-xia Pan c, Xue-mei Yang d
a State Key Laboratory of Herbage Improvement and Grassland Agro-ecosystems; Key Laboratory of Grassland Livestock Industry Innovation, Ministry of Agriculture and
Rural Affairs; Engineering Research Center of Grassland Industry, Ministry of Education; College of Pastoral Agriculture Science and Technology, Lanzhou University,
Lanzhou 730020, China
b Center for Remote Sensing of Ecological Environments in Cold and Arid Regions, Lanzhou University, Lanzhou 730000, China
c Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Lanzhou 730000, China
d Tourism School, Lanzhou University of Arts and Science, Lanzhou 730000, China

A R T I C L E I N F O

A B S T R A C T

least square procedure. Anal. Chem. 36, 1627–1639.

Seeger, M., 2004. Gaussian processes for machine learning. Int. J. Neural Syst. 14,

69–106.

Shen, Q., Ma, Y., 2020. Did water diversion projects lead to sustainable ecological

restoration in arid endorheic basins? lessons from long-term changes of multiple

EcologicalInformatics82(2024)10272114X.-d. Wang et al.

ecosystem indicators in the lower Heihe River basin. Sci. Total Environ. 701,
134785.

Vicente-Serrano, S.M., Begueria, S., Lopez-Moreno, J.I., 2010. A multiscalar drought

index sensitive to global warming: the standardized precipitation evapotranspiration
index. J. Clim. 23, 1696–1718.

|N|!

[f(S ∪ i) (cid:0)

f(S) ]

(14)

Where Φi is the SHAP value of feature i. The larger the SHAP value,
the greater contribution of the feature to the prediction; S is the set of
features; (S ⊆ N\i)denotes the subset that excludes feature i; N is the set
of all features; f is the prediction function of the model; f(S) is the pre-
diction result of the model on the set of features S; |S| denotes the
number of elements in set S.

2.4. Scenario analysis of oasis NDVI

(Culka, 2016). Random dropouts are applied in each layer of the BO-
LSTM model to interpret model outputs from the posterior distribution
of target variables. This implies that running multiple predictions ap-
proximates parameters of the posterior distribution, such as the mean
and variance, thereby creating confidence intervals for each prediction.
In this study, results obtained from 100 predictions were used to
construct a 95% confidence interval, which was determined to be two
standard deviations away from the mean value of each prediction.

2.3.7. Model interpretability analysis

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To ensure the generalizability of the deep learning models, several measures were implemented. Firstly, diverse datasets were utilized, including meteorological data, remote sensing data, and socioeconomic data. These datasets covered various aspects of the study area, providing comprehensive information for training the models. Secondly, cross-validation was employed during the training process. Specifically, k-fold cross-validation was adopted, where the original sample was randomly partitioned into k equal sized subsamples. Of the k subsamples, one subsample was retained as the validation data for testing the model, while the remaining k−1 subsamples were used as training data. This process was then repeated k times, with each of the k subsamples being used exactly once as the validation data. Lastly, stratified sampling was also considered when dividing the dataset into training and test sets. This method ensured that both sets had similar distributions of labels, preventing any bias towards certain classes or categories. By implementing these measures, the robustness and reliability of the deep learning models could be improved, ensuring their applicability across different scenarios and conditions.
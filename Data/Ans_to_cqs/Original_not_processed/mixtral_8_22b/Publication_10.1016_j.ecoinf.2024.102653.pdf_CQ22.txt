Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.6. Performance metrics 

Conventional  metrics  were  used  to  evaluate  the  model’s  perfor-
mance, such as confusion matrices, accuracy, recall, precision, F1-score, 
Cohen  Kappa,  and  the  area under  the  ROC curve,  some of  which  are 
defined below (Johnson and Khoshgoftaar, 2019).  

Table 1 
List of hyperparameters utilized for training the Recurrent Neural Network 
model.  

Hyperparameter 

Value 

Optimizer 
Learning rate 
Batch size 
Epochs 
Dropout rate 
Loss function 

ADAM (Kingma and Ba, 2017) 
0.003 
32 
100 
0.5 
Binary Cross Entropy  

●  Confusion Matrix

Fig.  8. Comprehensive  evaluation  of  predictive  model  performance  across 
multiple metrics and time series. A) Violin plot for the distribution of the per-
formance metrics: Accuracy, Cohen Kappa, F1-score, Precision, and Recall (the 
time-series data 5, 10, 15, 20 and 25 are represented by red dots). B) Bar plot 
across  different  performance  metrics  and  numbers  of  time  series  data  points 
(ranging from 5 to 25 days of records per bee) C) Taylor plot representing the 
standard deviation ratio to the maximum, cosine of accuracy correlation, and 
the number of time series data points (colour-coded). (For interpretation of the 
references  to  colour  in  this  figure  legend,  the  reader  is  referred  to  the  web 
version of this article.)

2.4. Model training and hyperparameters 

The  model’s  weights  were  randomly  initialized  using  a  specific 
integer seed to guarantee consistent reproducibility of results. Further-
more, meticulous manual tuning of hyperparameters was conducted to 
achieve  optimal  model  performance.  The  selected  hyperparameters, 
which yielded the best performance metrics, are presented in Table 1. 

2.5. Model validation 

Two distinct validation methodologies were implemented. The first 
method  involved  partitioning  the dataset into  training and  validation 
sets, with 70% of the data designated for training and 30% for valida-
tion.  This  division  was  performed  through  random  selection  (Dobbin 
and Simon, 2011).

The model was trained using varying disjoint subsets of the dataset in 
order to assess the number of recordings necessary to yield satisfactory 
classification  metrics.  These  subsets  corresponded  to  different  time 
steps, namely 5, 10, 15, 20, and 25 days of flight activity recordings. 
Subsequent  experiments  included  training  the  model  with  various 
combinations of datasets: 1, 2, 3, or all four datasets combined. These 
raw  and curated versions of the datasets are  readily accessible in the 
Code and Data Availability section. 

2.3. Deep neural network architecture

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The performance of the deep learning model is evaluated using conventional metrics such as confusion matrices, accuracy, recall, precision, F1-score, Cohen Kappa, and the area under the ROC curve. Some of these metrics are defined below:

* Confusion matrix: A table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class.
* Accuracy: The proportion of true positive and true negative predictions out of total predictions made.
* Recall: Also known as sensitivity or True Positive Rate (TPR), it measures the proportion of actual positives that are correctly identified.
* Precision: Also called Positive Predictive Value (PPV), it measures the proportion of true positive predictions out of total positive predictions made.
* F1-score: It is the harmonic mean of precision and recall, providing a single metric that balances both properties.
* Cohen Kappa: It measures inter-annotator agreement for qualitative items. In machine learning, it can measure how well a classification model performs compared to random chance.
* Area Under the Receiver Operating Characteristic Curve (AUROC): It provides an aggregate measure of performance across all possible classification thresholds.
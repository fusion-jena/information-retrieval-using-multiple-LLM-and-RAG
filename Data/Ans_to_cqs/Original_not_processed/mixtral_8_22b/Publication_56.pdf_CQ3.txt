Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Remote Sens. 8, 506 (2016).

 11.  Liu, Y., Minh Nguyen, D., Deligiannis, N., Ding, W. & Munteanu, A. Hourglass-shapenetwork based semantic segmentation for 

high resolution aerial imagery. Remote Sens. 9, 522 (2017).

 12.  Lees, T. et al. A machine learning pipeline to predict vegetation health. Eighth International Conference on Learning Representations 

1–5, (2020).

 13.  Zhao, W. & Du, S. Learning multiscale and deep representations for classifying remotely sensed imagery. ISPRS J. Photogramm. 

Remote Sens. 113, 155–165 (2016).

 14.  Rußwurm, M. & Körner, M. Multi-temporal land cover classification with long short-term memory neural networks. Int. Arch. 

Photogramm. Remote Sens. Spat.Inf. Sci. 42, 551 (2017).

Scientific Reports |        (2020) 10:17188  | 

https://doi.org/10.1038/s41598-020-74215-5

10

Remote Sens.Mag. 4, 22–40 (2016).

 20.  Zhu, X. X. et al. Deep learning in remote sensing: A comprehensive review and list of resources. IEEE Geosci. Remote Sens.Mag. 

5, 8–36 (2017).

 21.  Ma, L. et al. Deep learning in remote sensing applications: A meta-analysis and review. ISPRS J. Photogramm. Remote Sens. 152, 

166–177 (2019).

 22.  Goodfellow, I., Bengio, Y. & Courville, A. Deep Learning (The MIT Press, New York, 2016).
 23.  Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, 1735–1780 (1997).
 24.  Campos-Taberner, M. et al. Processing of extremely high-resolution lidar and RGB data: Outcome of the 2015 IEEE GRSS data 

fusion contest-part a: 2-d contest. IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens. 9, 5547–5559 (2016).

sample with its corresponding remote sensing sequence of data. Finally, 70% of the data were used for training 
the algorithms whereas the remaining 30% were used only for validation.

able AI: Interpreting, Explaining and Visualizing Deep Learning 193–209 (Springer, Berlin, 2019).

 37.  Arras, L. et al. Explaining and interpreting lstms. In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning 211–238 

(Springer, Berlin, 2019).

 38.  Zhou, B., Khosla, A., Lapedriza, A., Oliva, A. & Torralba, A. Learning deep features for discriminative localization. Proceedings of 

the IEEE Conference on Computer Vision and Pattern Recognition 2921–2929, (2016).

 39.  Wolanin, A. et al. Estimating and understanding crop yields with explainable deep learning in the Indian wheat belt. Environ. Res. 

Lett. 15, 024019 (2020).

 40.  Marcos, D., Lobry, S. & Tuia, D. Semantically interpretable activation maps: What-where-how explanations within CNNS. In 2019 

IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), 4207–4215 (IEEE, 2019).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, none of the provided sources explicitly mention the specific data annotation techniques used in their respective deep learning pipelines. However, it can be inferred from the context that some form of image labeling or pixel-level annotations was likely employed due to the nature of the tasks being performed, such as semantic segmentation, object detection, and land cover classification. These tasks often require detailed annotations like bounding boxes, polygons, or pixel-wise labels to train deep learning models effectively. For example, the paper by Liu et al. focuses on semantic segmentation for high-resolution aerial imagery, which typically involves pixel-level annotations. Similarly, the work by Lees et al. discusses a machine learning pipeline to predict vegetation health, suggesting that some form of image labeling might have been utilized to identify healthy and unhealthy vegetation areas. Nevertheless, without explicit information about the data annotation techniques used in these studies, one cannot provide a definitive answer.
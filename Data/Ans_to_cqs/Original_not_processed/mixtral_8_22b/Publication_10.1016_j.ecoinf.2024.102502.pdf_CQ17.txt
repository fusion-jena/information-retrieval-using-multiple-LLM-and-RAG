Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.1. Data filtering

using 

The  first  step  (data  filtering)  involves  data  acquisition  and  masks 
application  (Fig.  1).  From  the  EECE,  users  can  retrieve  the  EOS  data 
catalogue  from  the  GEE  online  repository  (https://developers.google. 
com/earth-engine/datasets/), 
ee.ImageCollection() 
constructor. Constructor arguments must also be included, such as fil-
terDate, which limits the collection to a selected period, and the inner 
clip function that cuts it within a specific study area extent. The identi-
fied  public  image  series  (time  series)  may  be  preprocessed,  but  also 
undergo the previously mentioned QA bands-based masking processes 
for each satellite, through optionally implementable masking functions 
(in yellow as depicted below). 

the

under study. Also, data scaling may be particularly useful for gap-filling 
operations,  offering  opportunities  to  avoid  missing  information  within 
extracted  data.  Briefly,  by  increasing  the  spatio-temporal  scale  around 
ground  observations,  users  may  ensure  higher  chances  of  avoiding 
missing  data,  through  the  aggregation  of  spatio-temporal  information 
within moving windows (Fig. 1). However, this implies also a decrease in 
data-matching  accuracy  (Fig.  1).  Key  decisions  and  trade-offs  must  be 
therefore made when extracting spatio-temporal multiscale data, possibly 
involving  particular  strategic  decisions  to  overcome  eventual  imple-
mentation constraints (see the Supplementary information S1.2 section). 

2.4. Worfklow replicability

2.3. Data scaling 

The final step refers to data scaling, and intrinsically to gap filling 
and data matching levels required by the user. Different scaling domains 
have been suggested by Fritsch et al. (2020). As the interest relies on 
aggregating  data  across  scales  prior  to  modelling,  our  approach  con-
forms  to  a  “pre-modelling  scaling”  protocol  (Fritsch  et  al.,  2020). 
Sequential  processes  were  implemented  within  GEE  for  scaling  up 
environmental  time  series  from  one  spatial  and/or  temporal  scale  to 
another during data extraction.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there is no mention of any regularization methods used to prevent overfitting in the deep learning pipeline in the given context. Regularization methods like dropout or L2 regularization are techniques used to prevent overfitting in machine learning models, especially neural networks. Dropout is a technique where randomly selected neurons are ignored during training, while L2 regularization adds a penalty term to the loss function to discourage large weights. However, without further context, I cannot provide more details about the use of these methods in the deep learning pipeline.
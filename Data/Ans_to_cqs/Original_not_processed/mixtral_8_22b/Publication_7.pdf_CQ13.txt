Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on November 16,2023 at 09:41:16 UTC from IEEE Xplore.  Restrictions apply. 

978-1-5090-1537-5/16/$31.00 ©2016 IEEEA. Classiﬁcation Process

Image representations extracted from deep neural networks,
trained on large datasets such as ImageNet [9] and ﬁne tuned
on domain speciﬁc datasets, have shown state-of-art perfor-
mance in numerous image classiﬁcation problems [14]. The
activation vectors of the ﬁrst fully connected layer of a pre-
trained VGGnet [24] are employed as feature representations
in our work. The weights of this deep network are ﬁne tuned
using the Benthoz15 dataset [23] which consists of expert-
annotated and geo-referenced marine images from Australian
seas.

Convolutional neural networks (CNNs) [9], also known as
deep networks, are an important class of machine learning
algorithms applicable, among others, to numerous computer
vision problems. Deep CNNs, in particular, are composed of
multiple layers of processing involving linear as well as non-
linear operators. To solve a particular task, the parameters of
networks are learned in an end-to-end manner. Image represen-
tations extracted from deep CNNs trained on a large dataset
such as ImageNet [10] have shown to produce a promising
performance for diverse classiﬁcation and recognition tasks
[11], [12], [13], [14] and [15]. Spatial pyramid pooling (SPP)
[16] and Multi-scale Orderless Pooling (MOP) [17] schemes
have made CNNs independent of the input image size and
robust for diverse classiﬁcation and recognition applications.
In this paper, we propose a computer vision and deep
learning based framework for the automatic annotation of

The image representations extracted at these three scales were
then max-pooled to retain the most prominent information
which is present in the neighbourhood of a labelled pixel.
These multi-scale deep features were used to train a Multi
Layer Perceptron (MLP) network for classiﬁcation. This net-
work consists of two fully connected hidden layers of neurons
followed by an output layer with 2 nodes: corals and non-
corals. The number of neurons in the hidden layers were
optimized for maximum performance. Fig. 3 shows the block
diagram of our proposed classiﬁcation method.

119,273
63,600
55,050

1,377
1,272
1,101

TABLE I: WA subset of Benthoz15 in numbers.

labels in this dataset which make it quite challenging. Table. I
details some statistics of the Western Australia (WA) subset of
this dataset. We have used a subset of this dataset containing
images from Western Australia (WA) to train our classiﬁer.
This subset consists of 4,750 images with 237,500 expert-
annotated points collected over a span of 3 years (2011 to
2013).

B. Pre-processing

We applied color channel stretch on each image in the
dataset. We calculated the 1% and 99% intensity percentiles
for each color channel. The lower intensity was subtracted
from all the intensities in each respective channel and the
negative values were set to zero. These intensities were then
divided by the upper percentile. The resulting intensities
achieved a better performance compared to the original ones.

C. Classiﬁcation Experiments and Results

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't enough information available in the given context to determine the specific hyperparameters used in the deep learning model, such as the learning rate or the type of optimizer. However, it can be inferred that the authors used a pre-trained VGGnet and fine-tuned its weights using the Benthoz15 dataset. They also mention employing a Multi Layer Perceptron (MLP) network for classification, consisting of two fully connected hidden layers of neurons followed by an output layer with 2 nodes: corals and non-corals. The number of neurons in the hidden layers was optimized for maximum performance. Nevertheless, details regarding other hyperparameters like learning rate, batch size, epochs, or the choice of optimizer remain unknown.
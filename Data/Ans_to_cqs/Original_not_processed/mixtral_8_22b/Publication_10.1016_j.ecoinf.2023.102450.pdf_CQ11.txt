Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

CRediT authorship contribution statement 

Zixuan Yin: Data curation, Methodology, Software, Writing – orig-
inal  draft,  Writing  –  review  &  editing.  Yaqin  Zhao:  Methodology, 
Writing  –  review  &  editing.  Zhihao  Xu:  Data  curation.  Qiuping  Yu: 
Software. 

Data availability 

Data will be made available on request. 

Acknowledgements 

Supported  by  National  Natural  Science  Foundation  of  China 

(32371583). 

References 

Aarts, R.M., Irwan, R., Janssen, A.J.E.M., 2002. Efficient tracking of the cross-correlation 
coefficient. IEEE Trans. Speech Audio Proc. 10 (6), 391–402. https://doi.org/ 
10.1109/TSA.2002.803447. 

Babenko, B., Yang, Ming-Hsuan, Belongie, S., 2011. Robust object tracking with online 

multiple instance learning. IEEE Trans. Pattern Anal. Mach. Intell. 33 (8), 
1619–1632. https://doi.org/10.1109/TPAMI.2010.226. 

Beery, S., Wu, G., Rathod, V., Votel, R., Huang, J., 2020. Context R-CNN: long term

Clune, J., 2018. Automatically identifying, counting, and describing wild animals in 
camera-trap images with deep learning. Proc. Natl. Acad. Sci. 115 (25), 
e5716–e5725. https://doi.org/10.1073/pnas.1719367115. 

Okafor, E., Pawara, P., Karaaba, F., Surinta, O., Codreanu, V., Schomaker, L., 

Wiering, M., 2016. Comparative study between deep learning and bag of visual 
words for wild-animal recognition. In: 2016 IEEE Symposium Series on 
Computational Intelligence (SSCI), pp. 1–8. https://doi.org/10.1109/ 
SSCI.2016.7850111. 

Riffenburgh, R.H., 2012. Epidemiology. In: Statistics in Medicine. Elsevier, pp. 535–549. 

https://doi.org/10.1016/B978-0-12-384864-2.00025-1. 

Rose, P.E., Nash, S.M., Riley, L.M., 2017. To pace or not to pace? A review of what 

abnormal repetitive behavior tells us about zoo animal management. J. Vet. Behav. 
20, 11–21. https://doi.org/10.1016/j.jveb.2017.02.007. 

Schindler, F., Steinhage, V., 2021. Identification of animals and recognition of their

The Siamese relation network(Cheng et al., 2021) used a deep re-
sidual neural network (ResNet-50) as its backbone to extract multi-scale 
features from images of animals. The cascade structure of a region-based 
convolutional neural network was used to classify the proposed location 
boxes. The Siamese relation network introduced two primary modules 
into  the network structure, namely, a relation detector module and a 
refinement  module.  The  relation  detector  module  was  equipped  with 
three different nonlinear detectors, including the global, local, and patch 
detectors. These detectors were used to compare the relationships be-
tween the features generated by the target and multi-scale regression 
features,  including  pixel-wise,  patch-wise,  and  global  features.  The 
refinement module converted the output of the relation detector into a 
matching  score  followed  by  an  element-wise  replication  between  the

matching score and cross-correlation feature map of the classification 
branch. Finally, a refined classification confidence score was generated 
using a convolution operation. The refinement module effectively inte-
grated  the  relation  detector  into  the  Siamese  framework  to  suppress 
false-positive detections and filter background interference.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't enough information to provide a direct link to the data repository for the deep learning pipeline mentioned in the given context. However, it does state that "Data will be made available on request." Therefore, if you are interested in accessing this data, you may need to contact the authors directly using the contact information provided in the paper or through other means such as email or academic platforms like ResearchGate or Academia.edu. Please note that the specifics of how to request the data might vary depending on the policies of the authors and their institution.
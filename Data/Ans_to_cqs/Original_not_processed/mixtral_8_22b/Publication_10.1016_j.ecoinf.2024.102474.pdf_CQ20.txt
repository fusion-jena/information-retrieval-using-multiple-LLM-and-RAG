Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3. Results 

3.1. Model training efficiency 

Model  training  on  our  hardware  (NVIDIA  GTX  1080  GPU)  took 
~460  h.  The  optimization  was  stopped  once  the  validation  ENS 
repeatedly  failed  to  improve  compared  to  the  score  evaluated  from 
previous  epochs.  The  model  at  the  epoch  with  the  highest  attained 
validation  score  was  selected.  We  noted  a  significant  acceleration  of 
convergence  when  employing  the  baseline  framework,  as  shown  in

2.2. Data 

The data used here was provided as part of the EarthNet2021 Chal-
lenge and consists of 23,904 training datacubes located across Europe

In this work, we resorted to purely deterministic variants of these 

architectures.  The  models  were  implemented  using  the  deep  learning 
framework PyTorch Lightning (Falcon, 2019) which is built on top of 
PyTorch  (Paszke  et  al.,  2019)  and  enables  improved  scalability.  The 
hyperparameters were tuned using an Optuna-based (Akiba et al., 2019) 
hyperparameter optimization procedure.

2.4. Model training 

23,401  datacubes  (97.9%  of  the  original  EarthNet2021  training 
dataset) were used for training, 500 datacubes for validation, and three 
datacubes were discarded (see above). The model was trained using the 
L2  loss  determined  on  the  predicted  and  observed  RGBI  channels 
(ignoring the cloud-contaminated pixels). The EarthNetScore (described 
in Section 2.5) is used for validation. The learning rate is set to 0.0003 

and the batch size is set to 4. We decided to use the AdamW optimizer, 
which,  unlike  the  standard  Adam  optimizer  (Kingma  and  Ba,  2014), 
decouples the weight decay and has also shown to improve on gener-
alization  (Loshchilov  and  Hutter,  2017).  The  SGConvLSTM  and  the 
SGEDConvLSTM were trained for 92 and 45 epochs, respectively. For 
completeness,  a  full  list  of  the  model  parameters  is  provided  in 
Tables A.2 and A.3 in the Supporting Information. 

2.5. Evaluation 

2.5.1. EarthNetScore

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The deep learning model was trained using NVIDIA GTX 1080 GPU as the hardware resource. This information can be found in section 3.1 under Model Training Efficiency where it states, "Model training on our hardware (NVIDIA GTX 1080 GPU) took ~460 h."
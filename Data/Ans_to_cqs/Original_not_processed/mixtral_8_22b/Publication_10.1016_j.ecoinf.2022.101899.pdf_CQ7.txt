Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Halgamuge et al. (2020) focused on utilizing deep learning to esti-
mate the best optimizer selection methods for wildfire event predictions 
in Australia. They developed an algorithm to collect the data automat-
ically and analyzed six different optimizer models using deep learning 
technology. They concluded that this approach could predict a natural 
disaster, including wildfire events, by investigating historical environ-
mental  conditions  before  and  after  wildfires  occurred  using  the  deep 
learning methodology. Dowdy (2020) focused on demonstrating a sea-
sonal prediction system for wildfire event prediction in Australia. This 
measured  spatio-temporal  variations  and  seasonality  across  several 
datasets  and  utilized  this  methodology  to  predict  wildfire  events, 
focusing on the seasonal predictions using the 2019–2020 wildfire sea-
son  as  an  example  and  measuring  long-term  predictions.  Coen  et  al.

In Section 2, we describe the study area and dataset and explain the 
method  used  in  this  study.  In  Section  3,  the  results  of  the  proposed 
Bayesian  modelling  framework  are  presented.  In  Section  4,  general 
conclusions and discussions are given. 

Fig. 1. Forested area for data extraction from GEE.  

EcologicalInformatics73(2023)1018993G. Charizanos and H. Demirhan                                                                                                                                                                                                            

2. Material and methods 

2.1. Study area and data 

2.1.1. Study area

To consider the model uncertainty, the models given in Fig. 15 are 
considered  as  the  set  of  models,  Ψ  of  Section  2.2.2.  Then  the  BMA 
approach is implemented. Each model’s prior probability is set to 0.2 to 
give equal chances a priori. The resulting posterior model probabilities, 
P(m| D), and Bayes factors to compare the models are given in Fig. 15. 
Bayes factors can also be interpreted as the improvement in the proba-
bility of occurrence for a given model. This analysis provides insights 
into  which  model  is  most  impactful  in  predicting wildfire  occurrence 
probabilities  and  provides  evidence  on  if  forest  vulnerability  is  the 
dominant  predictor.  The  results  indicate  forest  vulnerability  as  the 
dominant predictor, as this model is 2.5 times more likely to occur based 
on the posterior odds. The model that includes forest vulnerability with 
mean  wind  speed  is  the  second  most  likely, with  2.3  times  improved

EcologicalInformatics73(2023)10189913G. Charizanos and H. Demirhan                                                                                                                                                                                                            

Fig. 14. The values of independent features for the instances where the models return a false-negative classification.  

Fig. 15. BMA results.  

Fig. 16. Wildfire probability exponential growth against forest vulnerability. Black curve shows the actual observations.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors do not explicitly mention the specific criteria used to split their data into training, testing, and validation sets for their deep learning model. However, it is common practice in machine learning to use a random stratified sampling technique to ensure that each subset maintains similar class distributions as the original dataset. Additionally, typical ratios for splitting datasets include 70/30 or 80/20 for training and testing, respectively, while some practitioners may further divide the training set into separate training and validation subsets. Without explicit information regarding the authors' chosen method, these standard practices can serve as a helpful reference.
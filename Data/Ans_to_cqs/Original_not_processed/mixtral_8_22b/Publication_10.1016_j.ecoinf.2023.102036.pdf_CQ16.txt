Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1 
Model training optimized parameters.  

Parameter 

Nomenclature 

Search interval 

Learning 
rate 

Momentum 
SD threshold 
Input size 

μ 

β 
σ 
s 

[10

(cid:0) 5, 10

(cid:0) 2] 

[0,1] 
[0.05, 0.25] 
{(75, 125, 3), (100, 150, 3), 
(200, 250, 3)} 

Optimized 
value 

0.096 

0.845 
0.05 
(200, 250, 3)  

Each optimization run consisted of 3000 train iterations with a batch 
size of 25 individuals to pick a pair of similar and a pair of dissimilar 
photos (i.e., a batch size of 100 images), and validating each 300 iter-
ations. TensorBoard —a set of visualization tools included in the open- 
source  library  for  machine  learning  TensorFlow—  was  used  to  study 
the performance of the runs. The selected parameters correspond to the 
run shown in Supplementary Fig. S3. 

3. Results

This  system  was  built  under  Python  3.9.5  using  TensorFlow  2.4.1 
and training was performed on an Intel(R) Core(TM) i9-9900K CPU @ 
3.60GHz processor and 32GB RAM memory. Learning was performed 
with Stochastic Gradient Descendent (SGD) method, optimizing its pa-
rameters (learning rate and momentum) and binary cross-entropy as a 
loss function. The algorithm was also asked to exceed a certain threshold 
of variability in its predictions, i.e., that the standard deviation of the 
predicted probabilities exceeds a certain threshold —to be tuned—  so 
that the output of the classifier is not almost the same for all images that 
feed  the  model.  Moreover,  the  input  image  size  was  optimized,  since 
there is a trade-off between the information provided by large resolu-
tions and the number of weights of the network that need to be trained. 
The parameters were tuned with Bayesian optimization (Snoek et al.,

respectively.  After  discarding  those  individuals  with  insufficient  data 
(n_eval images in the test set, at least two in the train set, and some in the 
validation set), we end up with a train set of 1213 images in 108 cate-
gories (i.e., identities). Regarding the test set, we evaluated 370 images, 
of which 38 belonged to the four known recaptures.

p(imagei, n)∀i = 1, …, neval 

Then, we built the final decision by considering the prediction that 
our model gives for every image in that certain set of new samples and 
performed majority voting. In this way, the probability of making a poor 
decision is reduced and we give a more robust one. This is, 

n* = arg max
n=1,…,N

∑neval

i=1

1(di =n)

This proposed methodology is depicted in Supplementary Fig. S2. 

2.5. Training the model

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: The text does not explicitly state the specific criteria used to determine when training is complete. However, it mentions that each optimization run consisted of 3000 train iterations with a batch size of 25 individuals, picking a pair of similar and a pair of dissimilar photos. Additionally, the model was validated every 300 iterations. These details suggest that the completion of training might have been determined based on reaching a predefined number of training iterations or epochs. It is also possible that other criteria such as validation loss plateau could have been considered, but these are not mentioned in the given context.
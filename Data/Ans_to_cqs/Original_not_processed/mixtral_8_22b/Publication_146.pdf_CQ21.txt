Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.3. Training

3. Results

3.1. Training Evaluation

The model was trained for 350 epochs and evaluated using the scores precision, recall, F1, GIoU,
and mAP, as seen in Table 2. The scores are calculated from model validation data, consisting of
321 image tiles. The F1 score and mAP score are seen to follow each other closely, given indications on
the training at diﬀerent stages, the model quickly reached F1 and mAP scores of ~0.4, whereafter they
continue to increase but at a slower rate. At the end of the training, the model achieved the accuracy
scores of F1 = 0.530 and mAP = 0.557. The GIoU is steadily decreasing, indicating that the model is
becoming better at correctly locating targets.

Table 2. Training metrics at diﬀerent iteration stages.

Epoch

Precision Recall

F1 Score

GIoU

100
200
300
350

0.656
0.583
0.493
0.476

0.321
0.549
0.610
0.600

0.430
0.541
0.534
0.530

2.14
1.58
1.29
1.16

mAP

0.407
0.542
0.548
0.557

schema. (Source: European Space Agency “Sentinel High Level Operations Plan (HLOP)”). ISPRS Int. J. Geo-Inf. 2020, 9, x FOR PEER REVIEW 12 of 15  Figure 10 shows an example of iceberg training data used in the model and is a good indicator of a complex situation where data are to be labelled. It could be argued that too many objects are not labelled, leaving them out of training, but in opposition to this, one could say that too many small objects are included, and these are not of great importance. In Figure 11, training objects are also seen to be located within the large piece of floating ice in the right side of the image, this also raises the question of icebergs being present in other types of ice, such as in this study [23], or if such large pieces of floating ice should be in a class of their own or maybe not be included at all.  Figure 10. Example of icebergs detected with an adaptive thresholding algorithm in Sentinel-1 data. Since ships are clearly defined objects,

ISPRS Int. J. Geo-Inf. 2020, 9, x FOR PEER REVIEW 12 of 15  Figure 10 shows an example of iceberg training data used in the model and is a good indicator of a complex situation where data are to be labelled. It could be argued that too many objects are not labelled, leaving them out of training, but in opposition to this, one could say that too many small objects are included, and these are not of great importance. In Figure 11, training objects are also seen to be located within the large piece of floating ice in the right side of the image, this also raises the question of icebergs being present in other types of ice, such as in this study [23], or if such large pieces of floating ice should be in a class of their own or maybe not be included at all.  Figure 10. Example of icebergs detected with an adaptive thresholding algorithm in Sentinel-1 data. Since ships are clearly defined objects, populating a SAR dataset with these does not face the same issues as the icebergs. The

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training, several postprocessing steps can be performed depending on the specific requirements of the project. Based on the provided context, it appears that the model was evaluated using various metrics such as precision, recall, F1 score, GIoU, and mAP. These metrics were likely calculated during the postprocessing stage to assess the performance of the model. Additionally, the context mentions that the model was trained for 350 epochs and evaluated using scores calculated from model validation data. This suggests that the validation data may have been used to generate saliency maps or confusion matrices to further evaluate the model's performance. However, the context does not explicitly mention any postprocessing steps beyond metric calculation. Therefore, without additional information, it cannot be determined what specific postprocessing steps were taken after the model training.
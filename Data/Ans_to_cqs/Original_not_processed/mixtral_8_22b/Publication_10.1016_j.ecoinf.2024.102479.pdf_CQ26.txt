Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Friedman, J.H., 2002. Stochastic gradient boosting. Comput. Stat. Data Anal. 38, 

367–378. https://doi.org/10.1016/S0167-9473(01)00065-2. 

Fritsch, S., Guenther, F., Guenther, M.F., 2019. Package ‘neuralnet’. In: Training of 

Neural Networks, 2, p. 30. 

Gamble, J.S., Fischer, C.E.C., 1915–1935. Flora of the Presidency of Madras, Vol. 1–3, 

21, Hart Street, W.C.  

Gascon, F., Ramoino, F., Deanos, Y., 2017. Sentinel-2 Data Exploitation with ESA’s 

Sentinel-2 Toolbox, 19. EGU Gen. Assem, p. 19548 [Google Scholar].  

Ghasemi, N., Sahebi, M.R., Mohammadzadeh, A., 2011. A review on biomass estimation 
methods using synthetic aperture radar data. Int. J. Geomat. Geosci. 1 (4), 776–788 
[Google Scholar].  

Gholamy, A., Kreinovich, V., Kosheleva, O., 2018. Why 70/30 or 80/20 relation between 

training and testing sets: a pedagogical explanation [Google Scholar].

et al., 2016), gradient boosting (GB) (Yang et al., 2020) and penalized 
regression (PR) (Adhikari et al., 2023), have also exhibited commend-
able effectiveness when it comes to retrieving biomass at regional scale 
(Adhikari  et  al.,  2023;  Ranstam  and  Cook,  2018).  However,  the  per-
formance of a single MLA may be limited (Mendes-Moreira et al., 2012) 
and  in  recent  times,  ensemble  algorithms,  such  as  stacking,  has 
demonstrated promising capabilities in enhancing the dependability of 
estimating  AGB  through  the  integration  of  diverse  algorithmic  ap-
proaches (Sesmero et al., 2015; Zhang et al., 2020). It aims to capitalize 
on the strengths of different base learners while mitigating their indi-
vidual  limitations  (Zhang  et  al.,  2022).  In  this  study,  our  approach 
involved utilizing a stacking technique to integrate the optimal MLAs 
and construct an ensemble model. The objective of this ensemble model

Mayer, Z., 2016. A Brief Introduction to caretEnsemble. CRAN.R-project, 2019. https 

://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro. 
html. 

Ranstam, J., Cook, J.A., 2018. LASSO regression. Br. J. Surg. 105, 1348. https://doi.org/ 

10.1002/bjs.10895. 

R´ejou-M´echain, M., Tanguy, A., Piponiot, C., Chave, J., H´erault, B., 2017. Biomass: an r 
package for estimating above-ground biomass and its uncertainty in tropical forests. 
Methods Ecol. Evol. 8 (9), 1163–1167. https://doi.org/10.1111/2041-210X.12753. 
Rex, F.E., Silva, C.A., Dalla Corte, A.P., Klauberg, C., Mohan, M., Cardil, A., Silva, V.S.D., 
Almeida, D.R.A.D., Garcia, M., Broadbent, E.N., Valbuena, R., 2020. Comparison of 
statistical modelling approaches for estimating tropical forest aboveground biomass 
stock and reporting their changes in low-intensity logging areas using multi- 
temporal LiDAR data. Remote Sens. 12 (9), 1498. https://doi.org/10.3390/ 
rs12091498.

EcologicalInformatics80(2024)1024795K. Ayushi et al.                                                                                                                                                                                                                                  

Table 1 
Hyperparameters tuned for each algorithm with their ranges and optimal values for the study.  

Algorithms 

Random forest 

Multivariate adaptive regression splines 

Penalized regression 

Support vector machine 

Gradient boosting 

Artificial neural network 

k-Nearest Neighbors 

Tuned Parameter 

Parameter Range  MODEL 1  MODEL 2  MODEL 3  MODEL 4  MODEL 5  MODEL 6  MODEL 7 

ntree 
mtry 
degree 
nprune 
alpha 
lamda 
cost 
sigma 
epsilon 
shrinkage 
interaction.depth 
n.minobsinnode 
n.trees 
Hiddenlayer 
Neurons_per_layer 
threshold 
k 

100–1000 
2–35 
1–3 
3–70 
0–1 
0.001–10 
0.01–100 
0.01–100 
0.01–1 
0.01–0.2 
1–6 
1–20 
100–1000 
1–5 
5–100 
0.01–10 
3–19

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Data bias can significantly impact the performance of deep learning models. To address this issue during preprocessing, several techniques can be employed. One common method is stratified sampling, which ensures that the training and test datasets maintain the same class proportions as the original dataset. This helps prevent overfitting and underfitting due to imbalanced classes. Another technique is oversampling, where minority classes are duplicated to balance the dataset. Conversely, undersampling involves removing instances from majority classes to achieve balanced representation. Additionally, collecting diverse data can help reduce bias by ensuring that the model generalizes well across various scenarios. These methods aim to improve the robustness and fairness of deep learning models by minimizing the effects of data bias.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.3. Model evaluation

forest type and used to predict another, performance generally decreased, with better performance when forests
were more similar in structure. However, when models were pretrained on data from other sites and then ﬁne-
tuned using a relatively small amount of hand-labeled data from the evaluation site, they performed similarly to
local site models. Most importantly, a model ﬁt to data from all sites performed as well or better than individual
models trained for each local site.

Collecting a suﬃcient number of training samples will often be a
bottleneck in developing supervised methods in airborne imagery. It is
therefore useful to test the number of local training samples needed to
achieve maximum performance. We performed a sensitivity study by
training models using diﬀerent proportions of training data. We se-
lected 5%, 25%, 50% and 75% of the total hand-annotations to com-
pare to the full dataset for the within-site results for each site. We reran
this experiment ﬁve times to account for the random subsampling of
annotations. In addition, we ran the evaluation plots for the pretraining
model only (i.e. 0% hand-annotated data) to assess whether the addi-
tion of hand-annotated data improved the within-site pretraining.

3. Results

Ecological Informatics 56 (2020) 101061

Fig. 7. Ablation curves of the proportion of hand-annotation training data for each site. Values indicate the number of trees in the training dataset for each cutoﬀ.
Shaded area is the range of results from rerunning the analysis ﬁve times for each site. Note that due to the random sampling among runs, the exact number of trees
will vary slightly. For simplicity, we show the mean number of training trees for each threshold.

Data availability

All data and code are made available through a github repo
(https://github.com/weecology/DeepLidar/) and archived in Zenodo
(doi:https://doi.org/10.5281/zenodo.3347164). We are also actively
available
as part
building
a publicly
NeonTreeEvaluation
(https://github.com/weecology/
NeonTreeEvaluation).

larger dataset

benchmark

of

a

Acknowledgements

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The authors conducted a sensitivity study to evaluate the impact of different amounts of training data on model performance. They selected varying percentages of the total hand-annotations (5%, 25%, 50%, and 75%) and compared them to the full dataset for within-site results for each site. This process was repeated five times to account for random subsampling of annotations. Additionally, they evaluated the pretraining model alone (i.e., 0% hand-annotated data) to determine if adding hand-annotated data improved within-site pretraining. These experiments allowed the researchers to monitor the model performance during training and understand how much labeled data is required to achieve optimal performance.
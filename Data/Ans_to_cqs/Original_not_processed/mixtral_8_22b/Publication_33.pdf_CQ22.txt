Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

to calibrate Deep Learning models using a single parameter for all  classes35. This parameter is used, instead of 
the traditional softmax function, to convert the vector output from the neural network into a real probability.

An extension of meta-algorithms to control the risk of misclassification is to calibrate models obtained 
through Machine Learning and Deep Learning algorithms. Machine Learning methods usually produce well-
calibrated models for binary  tasks34. The calibration consists of a matching between the score predicted by the 
machine-learning model and the real probability of true positives. While Deep Learning models produce more 
accurate classifications than other Machine learning models, these models are not well calibrated, and thus need 
a re-calibration to be used for real-world  decisions35. Several propositions have been made to improve the cali-
bration of Machine Learning models through the post-processing of outputs. The Platt  scaling36, the Histogram 
 binning37, the Isotonic  Regression38 and the Bayesian Binning into  Quantiles39 are mapping the model outputs 
to real accuracy probabilities. More recently, Temperature Scaling, an extension of the Platt Scaling, was used

Raw
Accuracy

G1
Accuracy

G2
Accuracy

G3
Accuracy

0.51

0.61

0.87

0.08

0.95

0.16

0.97

0.56

0.68

0.01

0.24

0.51

0.77

0.02

0.89

0.36

0.52

0.78

0.55

0.61

0.53

0.30

0.65

0.69

0.91

0.74

0.99

0.84

0.87

0.62

0.8

0.44

0.31

0.59

0.81

0.9

0.92

0.46

0.7

0.85

0.68

0.7

0.72

0.18

0.9

0.87

0.97

0.94

1

0.95

0.95

0.79

0.96

0.79

0.54

0.8

0.96

0.96

0.97

0.89

0.91

0.93

0.87

0.81

0.89

0.10

0.97

0.96

0.97

0.98

1

0.98

0.96

0.97

1

0.9

0.72

0.93

0.99

1

0.97

0.85

0.95

0.95

1

0.81

0.94

0.07

Table 2.   Accuracy of the models without post-processing, and with post processing according to our goals, 
with thresholds tuned and applied on T2. Each line shows the result for a species, with: the species name, the 
accuracy of the model without post processing, and the accuracy of the model with post processing according 
to the 3 goals defined earlier.

The proposed post-processing was able to decrease the misclassification rate by at least 10.05%, for all goals, 
and 19.02% at most compared to the raw output of the Deep Learning model (Fig. 6b). The accuracy followed the 
same tendency, with an average accuracy for G1, G2 and G3 respectively equal to 0.74, 0.81 and 0.92 (Table 3).
Finally, we also performed the same experiment while switching T1 and T2 roles (Supplementary Tables S6, 
S7, S8). For each goal, the unsure classification rate was higher after the switch (+ 3.8% for G1, + 4.4% for G2, 
and + 8.9% for G3), implying lower scores were obtained in both correct classification (− 3.5%, − 5%, − 7.3%) and 
misclassification, with the exception of the 2nd goal (-0.2%, + 0.6%, − 1.6%).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The primary metric used to evaluate the performance of the deep learning model in this context is accuracy. Accuracy is calculated as the ratio of correctly classified instances out of the total number of instances. In Table 2, the accuracy of the models is presented before and after post-processing, along with the three different goals (G1, G2, and G3). Additionally, the paper mentions the misclassification rate, which can be considered another important metric. However, it does not explicitly mention other common evaluation metrics such as precision or recall.
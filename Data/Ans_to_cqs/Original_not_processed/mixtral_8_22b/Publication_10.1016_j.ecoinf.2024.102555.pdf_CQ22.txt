Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2. Materials and methods 

2.1. Study area 

◦
31′ to 53

◦
33′ to 135

◦
Our study area is across the entire China (i.e. from 3

Sun, C.M., Zhong, X.C., Chen, C., Gu, T., Chen, W., 2016. Evaluating the grassland net 
primary productivity of southern China from 2000 to 2011 using a new climate 
productivity model. J. Integr. Agric. 15 (7), 1638–1644. https://doi.org/10.1016/ 
S2095-3119(15)61253-9. 

Wang, X.H., Lu, C.H., Fang, J.F., Shen, Y.C., 2007. Implications for development of grain- 
for-green policy based on cropland suitability evaluation in desertification-affected 
North China. Land Use Policy 24, 417–424. https://doi.org/10.1016/j. 
landusepol.2006.05.005. 

Wang, J., Meng, J.J., Cai, Y.L., 2008. Assessing vegetation dynamics impacted by climate 

change in the southwestern karst region of China with AVHRR NDVI and AVHRR 

NPP time-series. Environ. Geol. 54, 1185–1195. https://doi.org/10.1007/s00254- 
007-0901-9.

Verification of novel integrations of swarm intelligence algorithms into deep 
learning neural network for flood susceptibility mapping. J. Hydrol. 581, 124379 
https://doi.org/10.1016/j.jhydrol.2019.124379. 

Cai, Q., Liu, Y.U., Zhang, H., Song, H., Li, Q., Sun, C., Wang, L.U., Fang, C., Liu, R., 2021. 
Evolution of the dry-wet variations since 1834 CE in the Lüliang Mountains, North 
China and its relationship with the Asian summer monsoon. Ecol. Indic. 121, 107089 
https://doi.org/10.1016/j.ecolind.2020.107089. 

Campagnolo, M.L., Libonati, R., Rodrigues, J.A., Pereira, J.M.C., 2021. A comprehensive 
characterization of MODIS daily burned area mapping accuracy across fire sizes in 
tropical savannas. Remote Sens. Environ. 252, 112115 https://doi.org/10.1016/j. 
rse.2020.112115. 

Chaitanya, B.P., Kanak, N.M., Abhay, M.V., Hazam, G.A., Mulla, S., Zaher, M.Y., 2023.

EcologicalInformatics81(2024)10255512L. Tuoku et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The papers cited do not directly address the specific metrics used to evaluate the performance of the deep learning model mentioned in the query. However, common metrics used to evaluate the performance of deep learning models include accuracy, precision, recall, F1 score, ROC curve, and Area Under Curve (AUC). Accuracy measures the proportion of correct predictions out of total predictions made. Precision measures the proportion of true positive predictions out of all positive predictions made. Recall measures the proportion of true positive predictions out of actual positives. The F1 score is the harmonic mean of precision and recall. The Receiver Operating Characteristic (ROC) curve plots True Positive Rate against False Positive Rate, while the Area Under Curve (AUC) represents the degree or measure of separability between classes. These metrics can be used individually or in combination to assess the performance of a deep learning model.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Benkendorf, D.J., Hawkins, C.P., 2020. Effects of sample size and network depth on a 
deep learning approach to species distribution modeling. Eco. Inform. 60, 101137. 
Bicudo, T., Llusia, D., Anci˜aes, M., Gil, D., 2023. Poor performance of acoustic indices as 
proxies for bird diversity in a fragmented Amazonian landscape. Eco. Inform. 77, 
102241. 

Breiman, L., 2001. Random forests. Mach. Learn. 45 (1), 5–32. 
Brown, A., Garg, S., Montgomery, J., 2019. Automatic rain and cicada chorus filtering of 

bird acoustic data. Appl. Soft Comput. 81, 105501. 

Brumm, H., 2006. Signalling through acoustic windows: nightingales avoid interspecific 
competition by short-term adjustment of song timing. J. Comp. Physiol. A. 192, 
1279–1285.

over which the quality measures and results fail to continue improving. 
Benkendorf and Hawkins (2020), analyzing the effect of sample size on 
deep neural networks, similarly concluded that by increasing the num-
ber of hidden layers, which can be interpreted as adding more training 
recordings in our software, the neural network can largely improve the 
performance of the model.

To  evaluate  the  performance  of  each  recognizer,  we  used  error 
matrices (Aide et al., 2013) containing true positives (tp), false positives 
(fp), false negatives (fn), and true negatives (tn). The final evaluation 
parameters were derived from them: precision, the proportion of recog-
nizer hits that are true detections of the target species [precision = tp/ 
(tp  + fp)],  and  recall,  the  proportion  of  target  species  vocalizations 
detected as hits by a recognizer [recall = tp/(tp + fn)]. The combination 
of the two parameters gives the F-score, where β is a user-defined metric 
that allows the prioritization of one of the two (Knight et al., 2017) [F- 
score = ((B^2 + 1) * precision * recall)/(B^2 * precision + recall)]. In our 
case, we used β = 1, so that precision and recall were evenly balanced in 
importance (Sokolova et al., 2006).

of the recognizer, we chose the optimal cut-off thresholds (in the case of 
identical F-scores, we used the mean of the thresholds); (4) we ran a 
random forest model using the best replicate and threshold on the test 
set and evaluated its performance; and (5) we implemented the same 
recognizer and threshold to the target set to obtain biologically relevant 
data.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The performance of the deep learning model was evaluated using error matrices containing true positives (tp), false positives (fp), false negatives (fn), and true negatives (tn). From these, two main evaluation parameters were derived: precision and recall. Precision is defined as the proportion of recognizer hits that are true detections of the target species [precision = tp / (tp + fp)]. Recall, on the other hand, is the proportion of target species vocalizations detected as hits by a recognizer [recall = tp / (tp + fn)]. To combine both parameters into a single measure, the F-score was calculated with a user-defined metric β that allows prioritizing one parameter over the other. In this study, β = 1 was chosen, giving equal importance to precision and recall.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4. Statistical Analyses

We used Python [35] and fastai [30] for developing the deep learning CNN. We used
R [36] for all statistical analyses, using packages sf [37] and raster [38] for data manipulation;
ggplot2 [39], rasterVis [40] and cowplot [41] for plotting. We also used R for designing
the website AusDams.org, using Shiny [42], Leaﬂet [43], Plotly [44] and using Joe Cheng’s
Superzip template (https://shiny.rstudio.com/gallery/superzip-example.html).

3. Results
3.1. Reported Farm Dams

There were 1,694,675 farm dams reported by regional and Federal authorities in
Australia. The majority of farm dams were in New South Wales (37%), Victoria (26%),
Queensland (17%) and Western Australia (10%; Table S1). Around three-quarters of Aus-
tralia recorded at least one dam per 2000 km2, but the typical density near urban centres
was 2–5 farm dams per km2 (Figure 2). The average size of a dam was ca. 1000 m2, ranging
from 100 m2 to >105 m2 (Figure 3).

3.2. Data Veriﬁcation

To avoid manual labelling of all 7362 downloaded images, we took a random subsam-
ple of 400 images and labelled them into “dam” or “not dam” and we trained a classiﬁcation
model on the labelled data. We utilised transfer learning by initialising an ImageNet pre-
trained ResNet34 model [30]. We applied an 80–20% split for training and validation
datasets, respectively. To help generalise the model, we used data augmentation with the
fastai get_transforms function [30] and the following arguments: “ﬂip_vert = TRUE” to
allow for vertical ﬂipping of images, “max_lighting = 0.02” to limit overly exposing the
images, “max_zoom = 1” to disable the zooming augmentation, and “to_fp16 = TRUE” to
reduce the memory load on the graphical processing unit (GPU). We set the batch size to
300 images and trained the model with a learning rate of 10−3 for ten epochs. At epoch 5,
we achieved an error rate of 0.1538 (15.38%) a validation loss of 0.4211 and a training loss

We trained our deep learning CNN on 7362 labelled images using the same parameters
detailed above, and we achieved an error rate of 0.1195 (11.95%) with a training loss of
0.3462 and a validation loss of 0.2847. We further ﬁne-tuned the model by unfreezing the
entire model and training at a 10-fold lower learning rate (10−4). The ﬁnal model achieved
an accuracy of 94.8% (error rate of 5.2%) with a training loss of 0.1397 and a validation loss
of 0.1446 with ten epochs (see confusion matrix in Figure S1).

2.2.2. Correcting for False Positives

Locations falsely classiﬁed as containing a dam (i.e., false positives, or commission
error) act to overpredict the real number of dams in Australia. Therefore, we calculated
the probability of false positives by using our deep learning CNN to analyse and validate
ca. 2000 dams in each State and Territory sampled from our compiled database. To do so,

Remote Sens. 2021, 13, 319

4 of 15

Supplementary Materials: The following items are available online at https://www.mdpi.com/20
72-4292/13/2/319/s1. Figure S1: Confusion matrix for farm dam detection with our deep learning
CNN. Figure S2: The surface area of unreported dams used to estimate their median water capacity.
Figure S3: Examples of undocumented farm dams that were identiﬁed with our deep learning CNN.
Figure S4: Step-by-step graphical diagram for the methods to calculate the absolute and relative
rates in the construction of Australian farm dams. Figure S5: Predicted probability for a successful
veriﬁcation (true positive) for each dam in our dataset. Figure S6: Estimated rates of annual increase
in dam densities between the years 1988 and 2015 in each State and Territory of Australia. Figure S7:
Historical trends in farm dam size with year of construction for each State and Territory. Table S1:
Summary table for all farm dam datasets used in this study.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors utilized data augmentation techniques to improve the performance of their deep learning classification model. They employed the fastai library's get_transforms function with specific arguments to achieve this goal. The applied data augmentation techniques include:

1. Vertical flipping of images: This is done by setting flip_vert = TRUE. It allows the model to learn features invariant to image orientation.

2. Limiting overexposure: By specifying max_lighting = 0.02, they prevented the images from being excessively exposed during the augmentation process. This helps maintain the quality of the input data.

3. Disabling zooming augmentation: Setting max_zoom = 1 disables any zooming augmentation, ensuring that the original scale of the images remains intact.

4. Reducing GPU memory load: Using to_fp16 = TRUE reduces the memory load on the graphical processing unit (GPU) by converting the tensors to half-precision floating point format.

These data augmentation techniques contribute to improving the robustness and generalization capabilities of the deep learning model.
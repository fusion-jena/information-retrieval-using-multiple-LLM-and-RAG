Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Fig.  6. Performance  of  the  models  trained  with  all  annotation  sets  provided 
from the annotation campaign. Red line corresponds to the model trained with 
the annotations from the expert. Blues lines with low opacity correspond to the 
model  trained  with  the  annotations  from  the  novices.  Blue  line  with  high 
opacity is the mean curve for all novice lines. 

blues curves are 0.85 +/(cid:0) 0.05 and 0.77 +/(cid:0) 0.17 respectively, while 
they reach 0.90 and 0.85 with the model trained from the expert an-
notations. For the blue whale’s Dcall, 13 models out of 19 trained from 
novice annotations showed performance close to the model trained on 
expert annotations with AUC and mAP metrics at 0.92 +/(cid:0) 0.02 and 
0.87 +/(cid:0) 0.3 while the model trained with the expert got 0.96 and 0.94. 
However, 6 curves show very low performances in comparison to the 
others, with AUC and mAP of 0.56 +/(cid:0) 0.21 and 0.57 +/(cid:0) 0.17.

Fig.  8. Performance  (AUC  and  mAP)  of  the  models  trained  with  aggregated 
annotation sets of novices using a major voting. The red dotted line corresponds 
to the performance of the model trained on the expert. 

EcologicalInformatics81(2024)1026427(a)SEIOPBWvocalizations(b)BluewhalesDcall(a)SEIOPBWvocalizations(b)Bluewhale’sDcallG. Dubus et al.

Red  dotted  lines  represent  the  performances  of  the  model  trained 
with the expert annotation set. A Krustkal-Wallis test is performed, using 
the performances of the largest subgroup size and the performance of the 
expert  on  Fig.  8,  to  evaluate  the  null  hypothesis  formulated  as  “the 
performance of the model trained with an expert annotation does not 
significantly differ from performance obtained with models trained on 
non-experts  annotations”.  P-values  computed  for  each  metrics  (AUC, 
mAP) are (0.11, 0.75) and (0.11, 0.21) for SEIO PW and Dcall respec-
tively.  The  null  hypothesis  cannot  be  rejected.  However,  the  perfor-
mances  of  the  model  trained  with  the  expert  annotation  are  always 
higher than the mean performance obtained with models trained with 
non-expert annotations.

strategy was used to aggregate annotation sets. Thus, for different sizes 
of  annotator  subgroups  (between  2  and  13),  10  random  selections  of 
novice annotators were realized, creating 10 distinct trainsets. A model 
was  trained  on  the  subgroup  annotations  from  the  development  set 
AmStP and evaluated on the evaluation sets: SWAMS and ElephantIs-
land2013 for SEIO PBW and Dcall, respectively. Fig. 8 shows the mean 
and standard deviation of AUC and mAP metrics of those 10 models, in 
comparison with the performance of the model trained with the expert 
annotation. The results for the group size of 1 are the mean and standard 
deviation of the 19 models trained with the annotation of the novices, 
one by one, presented in the previous section.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific strategy implemented to monitor the model performance during training. However, it mentions that a model was trained on the subgroup annotations from the development set AmStP and evaluated on the evaluation sets: SWAMS and ElephantIsland2013 for SEIO PBW and Dcall, respectively. This suggests that the authors might have used cross-validation or similar techniques to assess the model's performance during training. Additionally, they compared the performance of the model trained with non-expert annotations against the performance of the model trained with expert annotations using AUC and mAP metrics.
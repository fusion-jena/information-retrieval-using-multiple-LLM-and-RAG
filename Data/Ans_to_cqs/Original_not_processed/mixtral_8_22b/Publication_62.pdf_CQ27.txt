Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1. 

Constitutive layers and tensor/vector shapes for the unstained/stained CNN.

Layer

Input

ConvolutionLayer

BatchNormalizationLayer

Ramp (ReLU)

PoolingLayer

ConvolutionLayer

BatchNormalizationLayer

Ramp (ReLU)

PoolingLayer

ConvolutionLayer

BatchNormalizationLayer

Ramp (ReLU)

PoolingLayer

Type

3-tensor

3-tensor

3-tensor

3-tensor

3-tensor

3-tensor

3-tensor

3-tensor

3-tensor

3-tensor

3-tensor

3-tensor

3-tensor

Shape

3×256×256

16×252×252

16×252×252

16×252×252

16×126×126

32×122×122

32×122×122

32×122×122

32×61×61

64×57×57

64×57×57

64×57×57

64×28×28

Applications of deep convolutional neural networks to digitized natural ...

5

ConvolutionLayer

BatchNormalizationLayer

Ramp (ReLU)

PoolingLayer

FlattenLayer

DropoutLayer

LinearLayer

Ramp (ReLU)

LinearLayer

SoftmaxLayer

Output

Table 2. 

3-tensor

3-tensor

3-tensor

3-tensor

vector

vector

vector

vector

vector

vector

class

48×26×26

48×26×26

48×26×26

48×13×13

8112

8112

Supplementary materials

Suppl. material 1: Notebook used to deﬁne and train the unstained/stained CNN.

Authors:  Paul B. Frandsen, Abel Brown
Data type:  Mathematica notebook
Filename: stained_unstained_RGB256.nb - Download ﬁle (16.96 MB) 

Suppl. material 2: Annotated notebook used to deﬁne and train the unstained/stained
CNN.

Authors:  Paul B. Frandsen, Abel Brown
Data type:  PDF
Filename: stained_unstained_RGB256.pdf - Download ﬁle (6.24 MB) 

Suppl. material 3: Notebook used to deﬁne and train the clubmoss/spikemoss CNN.

Authors:  Paul B. Frandsen, Abel Brown
Data type:  Mathematica notebook
Filename: clubmoss_spikemoss_RGB256.nb - Download ﬁle (5.81 MB) 

 
 
 
 
 
 
Applications of deep convolutional neural networks to digitized natural ...

9

Suppl. material 4: Annotated notebook used to deﬁne and train the clubmoss/
spikemoss CNN

Authors:  Paul B. Frandsen, Abel Brown
Data type:  PDF
Filename: clubmoss_spikemoss_RGB256.pdf - Download ﬁle (2.68 MB)

included two convolutional layers and two pooling layers (Table 2; https://doi.org/10.6084/
m9.ﬁgshare.5501716). The code used to deﬁne and train these CNNs can be found in our
Mathematica notebooks (Suppl. materials 1, 3, 2, 4).

Our   work   highlights   the   importance   of   proper   metadata   curation   when   approaching   a
machine learning project. Assembling the training dataset for the mercury analysis required
many person   hours   to   visually   inspect   images   for   staining,   whereas   clubmoss   and
spikemoss images were easily compiled using specimen metadata alone. Nascent eﬀorts
in digitization in NHCs must carefully consider the acquisition and curation of metadata
because it aﬀects how quickly machine learning tools can be applied to digitized museum
collections.

Acknowledgements 

Computation was performed on the Smithsonian Institution High Performance Cluster (SI/
HPC), Hydra. We thank Nathan Anderson, Robert (Bort) Edwards, and Carol Kelloﬀ for
help classifying images and Martin Taheri and Sylvain Korzennik for IT support.

Author contributions

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The information provided does not explicitly state the process followed to deploy the trained deep learning model, including details about model serialization or platform selection. Therefore, I cannot provide a specific answer based on the given context. However, typically, after training a deep learning model, the next steps would involve saving or serializing the model, which converts the model into a format that can be stored and reused later. This could be done using various methods such as pickling in Python or saving the model weights. Afterward, the serialized model is deployed onto a suitable platform depending on its intended use case. For instance, if the model is meant for real-time predictions, it might be deployed on a cloud-based platform like AWS, Google Cloud, or Azure. Unfortunately, without further context, I cannot determine the exact deployment process for this particular deep learning model.
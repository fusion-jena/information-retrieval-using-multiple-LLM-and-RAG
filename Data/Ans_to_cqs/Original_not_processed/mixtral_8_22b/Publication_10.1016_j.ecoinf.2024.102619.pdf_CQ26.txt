Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

screening tool to improve quality of the ground-truth. Here although the 
task  is  a  simple  one,  its  time-consuming  nature  means  that  for  many 
large datasets it would be impossible to complete this task fully due to 
manual analysis constraints. Thus there is a suitable trade-off with the 
error-rate and efficiency of the approaches used. Regardless, the accu-
racy  and  reliability  of  annotations  should  always  be  interrogated,

2.2.2. Data preparation 

Pre-trained CNNs are designed to expect images in a certain format 
before feature extraction (or classification). For VGG16 (and other net-
works trained on ImageNet (He et al., 2016; Howard et al., 2017; Rus-
sakovsky  et  al.,  2015)),  RGB  images  must  be  224 × 224  pixels.  We 
therefore resize images to 224 pixels along the x-axis, preserving their 
aspect ratio. We then crop the center of images such that they are square. 
Following standard practice, RGB values were also normalized (centered 
and scaled) to the training dataset (ImageNet), see Table 3. The images 
in  each  dataset  were  split  into  80%  training  (including  5-fold  cross- 
validation) and 20% testing subsets. Splits were stratified to preserve 
the class-ratio. 

2.2.3. Model preparation

Pytorch (Paszke et al., 2019) ML framework for deep learning (CNNs). 
These libraries also include tools for data pre-processing, model selec-
tion and evaluation. To keep our model training and analysis pipelines 
comparable, we use skorch, a scikit-learn compatible neural network li-
brary that wraps PyTorch. This allows the same scikit-learn training and 
evaluation procedure to be used for both models. Skorch is also helpful 
for end-users in CNN training, as it has a clear and simple interface. It 
only requires end-users to add the prepared datasets, model and specify 
the associated hyperparameters (Table 4). Documentation for the entire 
machine learning pipeline can be found at (PyTorch, 2023) for Pytorch 
(Paszke  et  al.,  2019;  scikit-learn,  2023)  for  scikit-learn  and  (skorch, 
2022)  for  skorch.  Commercial  restrictions  apply  to  the  availability  of 
data  used  in  this  work.  However,  links  to  public  code  examples  of

predicting  common  classes.  Class  imbalance  can  alternatively  be 
handled by down-weighting common classes or up-sampling rare classes 
(Durden  et  al.,  2021;  Langenk¨amper  et  al.,  2019b).  Increasing  avail-
ability,  and  quality,  of  the  ground-truth  would  also  allow  for  further 
tuning  or  scratch-based learning  of the  CNN perhaps  creating feature 
representations  even  more  suitable  for  automatic  classification  of 
benthic habitats. Whereas reducing the size of feature spaces could aid 
classification by lessening noise within the feature space and reducing 
the  sparsity  of  data  points,  which  makes  finding  groups  with  similar 
properties challenging (curse of dimensionality). This can be achieved 
through  feature  importance  selection,  dimensionality  reduction  or 
employing  a  network  which  outputs  less  features,  such  as  GoogleNet 
(Szegedy et al., 2015) and Inception V3 (Szegedy et al., 2015).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention any specific technique to address data bias during preprocessing of the deep learning pipeline. However, it does suggest some strategies that might help mitigate issues related to data bias. One approach mentioned is stratified splitting, where the images in each dataset were split into 80% training (including 5-fold cross-validation) and 20% testing subsets, with splits being stratified to preserve the class-ratio. This ensures that the distribution of classes remains consistent across different subsets of the data. Another strategy discussed is handling class imbalance, either by down-weighting common classes or up-sampling rare classes. These methods aim to balance the representation of different classes in the dataset, thus addressing potential biases caused by uneven class distributions. Additionally, increasing the availability and quality of ground truth data was suggested as a way to create feature representations better suited for automatic classification tasks. While these suggestions do not directly tackle data bias, they contribute to improving the overall quality and fairness of the dataset, indirectly addressing potential sources of bias.
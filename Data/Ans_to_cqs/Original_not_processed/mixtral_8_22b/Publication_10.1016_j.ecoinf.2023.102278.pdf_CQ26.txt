Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Rl = (Wl2*σrelu(Wl1*R0 + bl1)) + bl2)

(2) 

Here  Wl1  and  Wl2  represent  the  weights,  and  bl1, bl2  represent  the 
biases for the independent FC layer representation. We apply dropout 
regularization  in  the  forward  pass  during  training  so  that  random 

EcologicalInformatics77(2023)1022784K. Bjerge et al.

(cid:0) 4 was finally 
chosen after comparing with stochastic gradient descent (SGD) with the 
(cid:0) 4  and learning rate of 
(cid:0) 4, weight decay of 1.0⋅10
momentum of 1.0⋅10
(cid:0) 3. SGD was tested with the parameters as recommended by L. 
1.0⋅10
Smith  (Smith,  2018),  but  achieved  a  1%–2%  decrease  in  accuracy 
compared to using the Adam optimizer. 

The  ResNet50  architecture  (He  et  al.,  2016)  was  modified  and 
trained with transfer learning using pre-trained weights from ImageNet 
(Smith,  2018).  Models  trained  with  transfer  learning  outperformed 
models trained with random weights with an increase of 4% in the F1-

Appendix C. The training of the models 

The best model with transfer learning was chosen based on the minimum total loss after nine epochs, as seen in Fig. C.5. Note that we observe 
overfitting after nine epochs, where the validation loss starts to increase, although the bias is still very low. The increase is indicated by a higher 
difference between training and validation loss and bias is the loss evaluated on the training dataset. 

Note that the largest variation is 0.6%, which is very similar to the variation of 0.8% when training with different values of α in Fig. B.4. This 

indicates a minimal impact on the change of accuracy for different choices of α. 

References 

An, G., Akiba, M., Omodaka, K., Nakazawa, T., Yokota, H., 2021. Hierarchical deep 

learning models using transfer learning for disease detection and classification based 
on small number of medical images. Scient. Rep. 11 (1) https://doi.org/10.1038/ 
s41598-021-83503-7.

monitoring  globally  and  for  efficient  pest  management  (Preti  et  al., 
2021). Deep-learning methods are increasingly used for classification in 
many fields and are emerging in entomology (Høye et al., 2021). Such 
methods require large training datasets to provide robust predictions, 
which has so far been a limiting factor in entomological applications. 
Specifically,  insects  are  very  diverse  and  their  abundance  is  highly 
variable,  which  often  results  in  unbalanced  datasets  with  numerous 
classes  nested  inside  each  other  in  a  taxonomic  hierarchy.  As  these 
methods mature and are implemented in monitoring programs, the de-
mand for greater taxonomic resolution of classifications will increase. So 
far, the level of fine-grained classification is still rather limited, not least 
because classification models rely on training data where images have 
species labels assigned to them.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, there is no explicit mention of specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, it can be inferred that some form of data balancing or augmentation might have been employed due to the nature of insect diversity and variable abundance mentioned in the text.

Stratified splitting could potentially be used to ensure that all classes are represented proportionately across train, test, and validation sets. Oversampling or undersampling could also be applied to balance the dataset if certain classes are underrepresented or overrepresented, respectively. Additionally, collecting diverse data could help mitigate biases introduced by having a narrow range of samples.

While none of these techniques are explicitly stated in the provided context, they remain viable options for addressing data bias during preprocessing of the deep learning pipeline.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

• No-overfitting. In this setting, the optimization algorithm of CNNs is 
Stochastic Gradient Descent (SGD), with a learning rate of 0.0001 
and a weight decay of 0.5. The batch size is set to 32, the number of 
training epochs to 200 and finally the batch-norm and dropout (0.5) 
are used to reduce the overfitting level.  

• Overfitting.  We  use  the  same  hyperparameters  setting  as  the  no- 
overfitting  but  we  remove  the  use  of  batch-norm,  weight  decay 
and dropout techniques to ensure that the model overfits.

chastic Gradient Descent (SGD) with a momentum of 0.9. Additionally, 
the learning rate was set to 0.001, the batch size to 32 and a decay of 0.5 
was  applied  every  512  iterations,  summing  up  to  a  total  of  2048 
iterations.

Strengthening the ability of trained models to generalize effectively 
poses  a  significant  challenge  within  the  domain  of  deep  learning. 
Indeed, generalization refers to the variation in a model’s performance 
when applied to data it has been trained on, as opposed to data it en-
counters for the first time during testing. This issue is of fundamental 
importance and has far-reaching implications for applications employ-
ing  deep  neural  networks.  In  particular,  this  challenge  becomes 

Table 7 
Discrimination  accuracy  (%)  of  ensemble  MIA  and  SNN  for  the  GREMM 
dataset.  

Approach 

Siamese Neural Network (SNN) 
Ensemble MIA 

Discrimination (%) 

65.86 
86.22  

EcologicalInformatics79(2024)10238814V.M. Araújo et al.

derives the training dataset of the attack model by querying the attack 
model with the attack training dataset (labeled as members) and attack 
testing dataset (labeled as non-members). By utilizing the attack training 
dataset, the adversary can train the attack model, which is structured as 
a  multi-layer  perceptron  (MLP).  More  precisely,  we  have  adopted  a 
conventional 3-layer MLP configuration, comprising 64, 32 and 2 hid-
den neurons in each respective layer. The optimization algorithm used 
to train CNNs is Stochastic Gradient Descent (SGD), with a learning rate 
of 0.0001, a batch size set to 32 and the number of training epochs set to 
200.  Once  the  attack  model  is  learnt,  the  adversary  can  perform  the 
attack over the target model to differentiate members and non-members 
with respect to Dtrain.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The optimization technique applied in the deep learning pipeline is Stochastic Gradient Descent (SGD). In both the no-overfitting and overfitting settings, SGD is used as the optimization algorithm for Convolutional Neural Networks (CNNs). For the no-overfitting scenario, the learning rate is set to 0.0001, while for the overfitting case, the learning rate is slightly higher at 0.001. Momentum is also employed in the overfitting scenario, with a value of 0.9. Furthermore, a decay of 0.5 is applied every 512 iterations in the overfitting setup. Batch normalization and dropout techniques are utilized in the no-overfitting setting to mitigate overfitting, whereas these methods are not used in the overfitting scenario.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The average and standard deviation of the precision, recall and F1- 
score  are  shown  in  Table  4  for  the  ResNet50  and  EfficientNetB3 
models validated  with multitask learning and  pre-trained weights. As 
expected,  we  observe  higher  precision,  recall  and  F1-score  at  higher 
taxonomic  ranks.  Here  we  see  an  increase  in  the  average  F1-score 
ranging from 95.7% (L3) to 98.7% (L2) and 99.1% (L1) at the highest 
ranks  with  ResNet50MTL.  A  resent  study  by  Ong  and  Hamid  (2022) 
designed  to  classify  five  taxa  of  museum  insect  species  achieved  Fl- 
scores  below  90%  with  separate  models  for  each  level  of  the 

hierarchy. However, our dataset is more comprehensive with a lower 
image resolution.

Table 4 
Average performance (Avg) and standard deviation (SD) for five trained models. Average precision, recall and F1-score for trained ResNet50 and EfficientNetB3 
(EffNetB3) models modified for multitask learning (MTL) with transfer learning using pre-trained weights from ImageNet. The models are trained and validated on the 
TLm  dataset. The models ResNet50, EfficientNetB3 are trained without MTL.  

Model 

Level 

ResNet50MTL 
EffNetB3MTL 

ResNet50MTL 
EffNetB3MTL 

ResNet50MTL 
EffNetB3MTL 

ResNet50 
EffNetB3 

L1 Order 
L1 Order 

L2 Family 
L2 Family 

L3 Species 
L3 Species 

Species 
Species 

Avg 

0.990 
0.986 

0.987 
0.984 

0.955 
0.948 

0.955 
0.953 

Precision 

SD (10

(cid:0) 3) 

(1.0) 
(4.4) 

(0.8) 
(3.1) 

(4.3) 
(5.2) 

(3.3) 
(2.5) 

Avg 

0.991 
0.993 

0.986 
0.988 

0.961 
0.966 

0.957 
0.966 

Recall 

SD (10

(cid:0) 3) 

(1.1) 
(0.5) 

(0.9) 
(0.7) 

(9.8) 
(5.1) 

(7.3) 
(2.5) 

Avg 

0.991 
0.989 

0.987 
0.986 

0.957 
0.956 

0.955 
0.959

F1-score 

0.850 
0.788 
0.498  

Table 8 
Number of correct classifications at higher taxonomy rank for the TL (TLt) and 
GBIFt  testing  dataset.  Unsure:  the  number  of  classifications  as  ‘unsure’  if  the 
higher rank has a correct prediction. Correct: the higher rank is correctly clas-
sified for an unsure prediction at level L2 or L3. Percentage: relative number of 
correct predictions at higher rank.  

Dataset 

Level (rank) 

Unsure 

Correct 

Percentage 

Table 7 
F1-scores for each level in the hierarchy of models (MIX, GBIF and TL) trained on 
the three different datasets (GBIFm, TLm  and MIX) and validated on either the 
TLm  or GBIFm  validation dataset.  

Level-Model 

Validated 

F1-score 

L1-MIX 
L1-MIX 
L1-GBIF 
L1-TL 

L2-MIX 
L2-MIX 
L2-GBIF 
L2-TL 

L3-MIX 
L3-MIX 
L3-GBIF 
L3-TL 

TL 
GBIF 
TL 
GBIF 

TL 
GBIF 
TL 
GBIF 

TL 
GBIF 
TL 
GBIF 

0.991 
0.975 
0.829 
0.702  

0.987 
0.976 
0.774 
0.700  

0.954 
0.948 
0.683 
0.673

(cid:0) 4 was finally 
chosen after comparing with stochastic gradient descent (SGD) with the 
(cid:0) 4  and learning rate of 
(cid:0) 4, weight decay of 1.0⋅10
momentum of 1.0⋅10
(cid:0) 3. SGD was tested with the parameters as recommended by L. 
1.0⋅10
Smith  (Smith,  2018),  but  achieved  a  1%–2%  decrease  in  accuracy 
compared to using the Adam optimizer. 

The  ResNet50  architecture  (He  et  al.,  2016)  was  modified  and 
trained with transfer learning using pre-trained weights from ImageNet 
(Smith,  2018).  Models  trained  with  transfer  learning  outperformed 
models trained with random weights with an increase of 4% in the F1-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: The performance of the deep learning models in this context is evaluated using several metrics such as precision, recall, and F1-score. These metrics are calculated for each taxonomic rank (Order, Family, and Species) and averaged across multiple runs of the models. Precision measures the proportion of true positive predictions among all positive predictions made by the model. Recall measures the proportion of true positives among all actual positives in the data. The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both aspects of classification performance. In addition, the standard deviations of these metrics are also reported to indicate the variability in performance across different runs of the models.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Model 

AlexNet 
VGG-19 
ResNet-101 
DenseNet-201 

Accuracy 

Raw 

Train 

98.75% 
96.77% 
82.25% 
93.71% 

Validation 

97.16% 
98.30% 
89.04% 
91.30% 

Test 

96.16% 
95.15% 
83.30% 
86.48% 

Pre-processed  

Train 

98.21% 
96.94% 
77.25% 
91.61% 

Validation 

97.92% 
97.92% 
79.02% 
87.33% 

Test 

95.98% 
96.52% 
75.44% 
86.29%  

Table 5 
Accuracy of the models swapping the testing sets (source → target).  

Model 

Accuracy 

Raw → Pre-processed 

Pre-processed → Raw 

AlexNet 
VGG-19 
ResNet-101 
DenseNet-201 

82.35% 
82.70% 
69.22% 
65.26% 

54.76% 
78.87% 
29.56% 
33.97%  

Fig. 5. Confusion matrices for the VGG-19 architecture.  

et al., 2017) and SmoothGrad (Smilkov et al., 2017) methods over each 
model. These methods plot a point cloud, where the density denotes the 
input space relevance. Thus, a higher density in a region suggests that 
the network ponderates it the most when classifying.

Like any other complex model, DL requires a large amount of data to 
fit appropriately, which is hard in our context. To overcome this limi-
tation, we employ different architectures pre-trained with the ImageNet 
dataset. Pre-trained models capture low-level features (e.g., edges, cor-
ners, color spots, etc.) from one domain and transfer them to another 
with  similar  characteristics.  The  transfer  process  is  called  fine-tuning 
due  to  the  model  only  learns  specific  higher-level  features  (e.g.,  ar-
rangements, venations, etc.). We compare four pre-trained models: (1) 
AlexNet (Krizhevsky et al., 2012, 2) VGG-19 (Simonyan and Zisserman, 
2014, 3) ResNet-101 (He et al., 2016, 4) DenseNet-201 (Huang et al., 
2017).  The  fully  connected  block  is  adjusted  to  feed  off  the  feature 
vector and output the ten species of leaves. Table 3 describe architecture 
characteristics 

Ih
R[i, j] = 3.2405⋅Ih
Ih
G[i, j] = (cid:0) 0.9693⋅Is
Ih
B[i, j] = 0.0556⋅Is

Deep Learning (DL) methods are at the top of the state-of-the-art on 
feature representation for different domains; albeit, DL lacks interpret-
ability. According to Doshi-Velez and Kim (2017), interpretability lets 
human specialists understand what a model is learning, making them 
flexible  real-world  solutions.  Given  such  topics,  this  paper  has  three 
significant contributions:  

1.  The  paper  introduces  the  first  Peruvian  Amazon  Forestry  Dataset, 
including  its  detailed  metadata  and  the  acquisition  protocol 
description. The dataset collects 59,441 samples from ten of the most 
profitable  and  endangered species (Finer  et al., 2014; Pinedo-Vas-
quez et al., 1992). Further-more, we employ six different commercial 
cameras  to  ensure  variability  and  to  develop  any  flexible  solution 
with real-world conditions in the future.

Kingma, D.P., Ba, J., 2014. Adam: A Method for Stochastic Optimization. arXiv preprint 

arXiv:1412.6980. 

Kornblith, S., Shlens, J., Le, Q.V., 2019. Do better imagenet models transfer better?. In: 

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 
pp. 2661–2671. https://doi.org/10.1109/CVPR.2019.00277. 

Krause, J., Sapp, B., Howard, A., Zhou, H., Toshev, A., Duerig, T., Philbin, J., Fei-Fei, L., 
2016. The unreasonable effectiveness of noisy data for fine-grained recognition. In: 
European Conference on Computer Vision. Springer, pp. 301–320. https://doi.org/ 
10.1007/978-3-319-46487-9_19. 

Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classification with deep 
convolutional neural networks. In: Advances in Neural Information Processing 
Systems, pp. 1097–1105. 

Pizer, S.M., Amburn, E.P., Austin, J.D., Cromartie, R., Geselowitz, A., Greer, T., ter Haar

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the hyperparameters used in the deep learning models. However, it mentions using pre-trained models like AlexNet, VGG-19, ResNet-101, and DenseNet-201, which were originally trained on the ImageNet dataset. It also states that these models are fine-tuned for the specific task, but there is no mention of the exact hyperparameters used during training or fine-tuning. Additionally, the reference list includes a citation to Kingma and Ba (2014)'s work on Adam optimization, suggesting that the Adam optimizer might have been used in the study. But without more specific details, it cannot be confirmed.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

loss function, effectively preventing overfitting and promoting simpler 
models,  utilization  of  a  more  advanced  optimization  approach  that 
combines  first-order  gradients  (loss  function  gradient)  with  second- 
order  gradients  (loss  function  curvature)  which  makes  it  faster  than 
some other models, ability to handle missing data during tree building 
by  employing  weighted  quantile  sketch,  built-in  cross-validation  sup-
port  that  aids  the  model  evaluation  and  hyperparameter  tuning,  and 
imbalanced  data  handling.  Thanks  to  its  exceptional  efficiency  and 
performance and its availability in various programming languages like 
Python, R, and Java, XGBoost has found widespread adoption in land-
slide risk assessment (Akinci et al., 2021; Badola et al., 2023; Can et al., 
2021; Hussain et al., 2022b). 

{(cid:0)

Given  a 
)}(cid:0)
xi, yi

set  with  n 
|D|= n, xi ∈ Rm, yi ∈ R

Recent  research  has  demonstrated  the  effectiveness  of  leveraging 
single-algorithm ensemble methods to improve the predictability of ML- 
based  LSM.  Notable  algorithms  in  this  category  include  the  Random 
Forest  (RF)  model  as  demonstrated  by  studies  such  as  (Achour  and 
Pourghasemi,  2020;  Adnan  et  al.,  2020;  Akinci  et  al.,  2021;  Kalantar 
et al., 2020; Youssef and Pourghasemi, 2021). Additionally, the XGBoost 
model  as  indicated  by  (Badola  et  al.,  2023)  and  (Can  et  al.,  2021), 
LightGBM (Ye et al., 2022), and AdaBoost (Gupta and Shukla, 2023; Nhu 
et al., 2020) have been recognized for their ability to improve the ac-
curacy  of  ML-based  LSM  (Adnan  et  al.,  2020).  In  addition  to  single- 
algorithm  ensembles,  mixed-based  algorithm  ensembles,  which 
combine different models, have also shown promise in improving LSM 
performance. These ensemble techniques, such as stacking (Fang et al.,

EcologicalInformatics81(2024)1025832G. Agboola et al.                                                                                                                                                                                                                                

Fig. 2. Research workflow  

Pourghasemi, 2021), and the Support Vector Machine (SVM) models (F. 
Huang et al., 2022; Y. Huang and Zhao, 2018; Shahzad et al., 2022; Xing 
et al., 2021). These models are flexible and adaptable systems that can 
manage diverse types of data and scenarios for LSM. Another type of 
MLM that has been explored for LSM is the Artificial Neural Network 
(ANN), especially in the Radial Basis Function (RBF) (W. Chen et al., 
2018; Ngo et al., 2021) and Multi-Layer Perceptron (MLP) configuration 
(Adnan  et  al.,  2020;  Chang  et  al.,  2022;  Z.  Zhao  et  al.,  2022).  These 
models can learn from data and adjust their parameters to improve their 
performance.

)

(cid:0)

Ω

fj

= τT +

1
2

λ||w||2

(9)  

2.2.6. Heterogenous ensemble techniques 

Heterogeneous ensemble learning is the practice of combining pre-
dictions of multiple models that are different in terms of their underlying 
algorithms  or  architectures  to  improve  overall  performance.  Utilizing 
heterogeneous ensemble techniques can address biases and improve the 
implemented  two 
effectiveness  of  models. 

In  our  study,  we 

EcologicalInformatics81(2024)10258312G. Agboola et al.                                                                                                                                                                                                                                

Fig. 11. The proportion of area covered by each susceptibility category in each scenario.  

heterogeneous  techniques  to  optimize  the  performance  of  five  base 
learners and mitigate potential overfitting:

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information on the specific strategy used to monitor the model performance during training. However, based on the given context, one could infer that the authors might have employed built-in cross-validation support available in XGBoost, which aids in model evaluation and hyperparameter tuning. Cross-validation involves splitting the dataset into several folds, where each fold serves as a validation set once while the remaining folds form the training set. This process helps evaluate the model's performance and prevent overfitting. Furthermore, the use of regularization techniques, such as loss function, mentioned in the context, effectively prevents overfitting and promotes simpler models. Therefore, these strategies may be utilized to monitor the model performance during training.
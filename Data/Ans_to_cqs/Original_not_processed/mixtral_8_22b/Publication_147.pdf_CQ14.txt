Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Vienna, Austria, 2013.

77. Hacker Earth.

Beginners Tutorial on XGBoost and Parameter Tuning in R. Available
online: https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-
tutorial-on-xgboost-parameter-tuning-r/tutorial/ (accessed on 29 August 2019).

78. Parisien, M.-A.; Parks, S.A.; Miller, C.; Krawchuk, M.A.; Heathcott, M.; Moritz, M.A. Contributions of
ignitions, fuels, and weather to the spatial patterns of burn probability of a boreal landscape. Ecosystems
2011, 14, 1141–1155. [CrossRef]

79. Atienza, R. Advanced Deep Learning with Keras: Apply Deep Learning Techniques, Autoencoders, Gans, Variational
Autoencoders, Deep Reinforcement Learning, Policy Gradients, and More; Packt Publishing Ltd.: Birmingham,
UK, 2018.

Table 2. Full comparison of training time, prediction time, optimization time (i.e., time to properly
tune the model), and hardware for each model.

Model

Training Time
(Hours)

Prediction Time (Hours)

Optimization
Time (Hours)

Hardware

CNN

XGB

4

4

2.4. Validation

4

Unknown

Desktop with 64 Gb of
RAM and one Titan X
(Maxwell) GPU.

72—can be reduced by
distributing prediction to more
cores or more machines.

2

Desktop with 64 GB of
ram and 64 logical cores.

correlation to each other. This variable selection process can be seen in more detail in [7]. The inputs
were trained to the six classes from the ABMI plot training data using the “multi:softmax” objective
setting. The XGBoost model parameters were tuned using grid search functions to ﬁnd the optimal
value when judged by the test error metric. Additionally we wanted to err on the side of conservative
model building since we knew there was little power in the inputs to discriminate between wetland
classes (see Figure 1). The optimized XGBoost parameters were: nrounds = 500, max_depth = 4,
eta = 0.03, gamma = 1, min_child_weight = 1, subsample = 0.5, colsample_bytree = 0.8. See [77] for
description of XGB parameter tuning. We then built 15 separate XGBoost models. Each model was built
with a diﬀerent subset of 2000 random points which were spaced anywhere from 900 to 2000 m apart,
depending on the relative abundance of each of the six classes. The selection of this number of points

Table 1. List of input variables in the XGBoost model (XGB) and convolutional neural network
(CNN) models. Each variable lists its respective data source, equation, description, and, if needed,
citation. For information on Sentinel-2 band information not listed in the table, see:
[66] and
https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/resolutions/spatial.

Variable

Data Source

Model

ARI

Sentinel-2

XGB/CNN

Band 2
Band 3
Band 4
DEM

NDVI

dNDVI

NDWI

POLr

PSRI

Sentinel-2
Sentinel-2
Sentinel-2
ALOS

Sentinel-2

Sentinel-2

Sentinel-2

Sentinel-1

Sentinel-2

CNN
CNN
CNN
CNN

CNN

XGB

CNN

XGB

CNN

REIP

Sentinel-2

XGB/CNN

Equation
(cid:17)
(cid:16) B8
(cid:16) B8
B3
B2

–

(cid:17)

-
-
-
-
(B8− B4)
(B8 + B4)

Description

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The hyperparameters of the XGBoost model used in this study were optimized through grid search functions. Grid search is a methodical way to find the best combination of parameters for a machine learning algorithm. It involves defining a set of values for each parameter and exhaustively trying all possible combinations of these values. In this case, the optimized XGBoost parameters found through grid search were: nrounds = 500, max_depth = 4, eta = 0.03, gamma = 1, min_child_weight = 1, subsample = 0.5, colsample_bytree = 0.8.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

44,625 annotated ﬁsh thumbnails.

belonging to 20 species (Table 1). The 20 species present in the
training dataset represent the most common species appearing in the
videos and belong to 12 families among the most diverse and abundant
(e.g. Pomacentridae, Acanthuridae,
on coral

reefs worldwide

Table 1
Raw success rate (%) of the 4 CNN models trained with diﬀerent thumbnails
datasets for identifying 18 ﬁsh species. See details about training databases in
Table S2.

Species

Only
whole
ﬁsh
(T1)

Whole
ﬁsh and
part of
ﬁsh (T2)

Whole ﬁsh,
environment
and part of ﬁsh
(T3)

Whole ﬁsh,
environment
and part of
species (T4)

Abudefduf sparoides
Abudefduf vaigiensis
Chaetodon trifascialis
Chromis weberi
Dascyllus carneus
Monotaxis

grandoculis

Myripristis botche
Naso elegans
Naso vlamingii
Nemateleotris
magniﬁca
Odonus niger
Plectroglyphidodon
lacrymatus
Pomacentrus sulfureus
Pterocaesio tile
Pygoplytes diacanthus
Thalassoma

Ecological Informatics 48 (2018) 238–244

of 16 images to train our network. We ran this architecture on Caﬀe (Jia
et al., 2014). To focus on the impact of the training data, we used the
same CNN architecture for our training and test procedures.

2.3. Building the training datasets

Using the raw training dataset of 20 ﬁsh species (Table S1) we built
4 diﬀerent datasets to assess the inﬂuence of the dataset building on
classiﬁcation results (Table S2).

The ﬁrst training dataset T1 contained raw ﬁsh thumbnails (T0) and

their respective mirror images.

More precisely, we doubled the number of thumbnails per ﬁsh in-
dividual by ﬂipping each thumbnail with respect to the vertical axis.
Such a procedure homogenizes the proportion of left-oriented and
right-oriented individuals in the database and we hypothesize it could
improve the average identiﬁcation rate since ﬁsh individuals are seen in
all positions.

Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet Classiﬁcation with Deep
Convolutional Neural Networks. InAdvances in Neural Information Processing
Systems. pp. 1097–1105.

Krueck, N.C., Ahmadia, G.N., Possingham, H.P., Riginos, C., Treml, E.A., Mumby, P.J.,

2017. Marine reserve targets to sustain and rebuild unregulated ﬁsheries. PLoS Biol.
15 (1), e2000537.

Kulbicki, M., Parravicini, V., Bellwood, D.R., Arias-Gonzàlez, E., Chabanet, P., Floeter,
S.R., ... Mouillot, D., 2013. Global biogeography of reef ﬁshes: a hierarchical quan-
titative delineation of regions. PLoS One 8 (12), e81847.

Langlois, T.J., Harvey, E.S., Fitzpatrick, B., Meeuwig, J., Shedrawi, G., Watson, D., 2010.
Cost-eﬃcient sampling of ﬁsh assemblages: comparison of baited video sta=tions

244

and diver video transects. Aquat. Biol. 9, 155–168.

A link to a depository with architecture details is given at the end of
references. We stopped the network training after 70 epochs (i.e. a
complete scope of the dataset where each image is used only once), to
−5, an exponential
prevent overﬁtting. We used a learning rate of 10
learning decay with a Gamma of 0.95, a dropout of 50% and an Adam
Solver type as learning parameters. Those are classic hyper-parameters
for a fast convergence of the network without over-ﬁtting (Srivastava
et al., 2014). The weight initialization is also classic with a random
Gaussian initialization. The training lasted 8 days on our conﬁguration;
we trained and ran our code on a computer with 64GB of RAM, an i7
3.50GHz CPU and a Titan X GPU card for 900,000 images.

We used at least 2200 thumbnails per ﬁsh species class, and batches

240

S. Villon et al.

Ecological Informatics 48 (2018) 238–244

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't enough information available to provide a direct link to the code repository for the deep learning pipeline mentioned in the text. However, based on the statement "A link to a depository with architecture details is given at the end of references," one can infer that the authors have included a reference or citation to their code repository within the paper's bibliography. Therefore, to find the specific link, please refer to the original research article and look for any relevant citations or links provided at the end of the document.
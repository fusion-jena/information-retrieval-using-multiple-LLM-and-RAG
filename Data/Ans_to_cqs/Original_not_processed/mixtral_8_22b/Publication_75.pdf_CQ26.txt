Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.2.2. Retrieval results 

The retrieval experiments ensued the following strategy: every image 
is used as a query image and is matched with every remaining image in 
the database. Tables 6 and 7 present the retrieval performances of all the 
networks in terms of the four considered measures by choosing the top 
100 matches, i.e. η = 100, and the two chosen similarity distances. 

3.3. Deep learning vs traditional machine learning comparison 

A comparison of deep learning approaches with traditional machine

Of course, some aspects can strongly influence the whole classifica-
tion or retrieval task. First of all, the quality of the original images to 
process before being analysed by a network. Secondly, the preprocessing 
step, such as the background cleaning, the spacing of the seeds during 
the acquisition, and, finally, the size of the seeds present in the images. 
Last but not least is the imbalance of categories of both the datasets.

3. Results and discussion 

We  now  describe  the  experimentation  conducted  in  this  work.  In 
detail, in Section 3.1 we first describe the experimental setup adopted 
for the classification and retrieval tasks. Then, we report the results of 
the  experiments  performed  on  both  datasets  and  tasks  with  SeedNet, 
compared to existing CNNs (Section 3.2). After a further comparison of 
deep  vs  machine  learning methods  in Section  3.3, we  give  a detailed 
discussion of the obtained results in Section 3.4. 

3.1. Setup 

3.1.1. Seed image classification via deep learning

Our study was mainly to propose a new model based on CNN, which 
provides an efficient and effective approach for single seed classification 
and retrieval. Our network architecture, SeedNet, contains six learnable 
layers, 5 of which are convolutional, and the last is fully-connected. The 
ReLU (Rectified Linear Unit) activation is applied to the output of every 
convolutional  layer.  Moreover,  we  periodically  insert  a  pooling  layer 
between successive convolutional layers, in order to reduce the amount 
of parameters and computation in the network, and hence to also control 
overfitting. More precisely, we employ a Max pooling strategy with a 
3 × 3 filter size, and 1 × 1 stride. The input image size is 224 × 224 × 3. 
According to the chosen dataset, the output of the last fully-connected 
layer  is  fed  to  a  N-way  softmax,  which  produces  a  distribution  over 
the N class labels to predict, i.e. 6 for the Canadian and 23 for the local

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it appears that there is no explicit mention of specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, the context does highlight the importance of certain factors that could potentially impact the overall performance of the classification or retrieval task. These include the quality of the original images, the preprocessing step (such as background cleaning and spacing of the seeds), and the imbalance of categories within the datasets.

While the context does not provide direct information about techniques like stratified splitting, oversampling, undersampling, or diverse data collection, these are indeed common strategies employed to mitigate issues related to data imbalance and bias. For instance, stratified sampling ensures that each category or label is proportionally represented in both training and testing sets. Oversampling involves duplicating examples from the minority classes, while undersampling reduces instances from the majority classes. Lastly, collecting diverse data helps ensure that the model generalizes well across different scenarios and populations.

In conclusion, although the context does not explicitly state the use of any particular technique to address data bias, it acknowledges the significance of handling data imbalances and biases. Common techniques to tackle these challenges include stratified sampling, oversampling, undersampling, and diverse data collection.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

practices to address a substantially imbalanced dataset with deep learning. Remote 
Sens. Environ. 216, 139–153. https://doi.org/10.1016/j.rse.2018.06.028. 
Lamprey, R., Ochanda, D., Brett, R., Tumwesigye, C., Douglas-Hamilton, I., 2020a. 

Cameras replace human observers in multi-species aerial counts in Murchison falls, 
Uganda. Remote Sens. Ecol. Conserv. 6 (4), 529–545. https://doi.org/10.1002/ 
rse2.154. 

Lamprey, R., Pope, F., Ngene, S., Norton-Griffiths, M., Frederick, H., Okita-Ouma, B., 
Douglas-Hamilton, I., 2020b. Comparing an automated high-definition oblique 
camera system to rear-seat-observers in a wildlife survey in Tsavo, Kenya: taking 
multi-species aerial counts to the next level. Biol. Conserv. 241, 108243 https://doi. 
org/10.1016/j.biocon.2019.108243. 

Lamprey, R.H., Keigwin, M., Tumwesigye, C., 2023. A high-resolution aerial camera

with a 50% probability of occurrence. To avoid any risk of overfitting at 
each  training  stage,  we  selected  the  model  relative  to  the  epoch  that 
gave the best performance on the validation set. During inference, the 
patch  size  was  set  to  1024  × 1,024  pixels  to  accelerate  the  process. 
Further information on the fine-tuning process is described in section 
2.5.2.

The  pre-trained  model  underwent  inference  and  fine-tuning  for  4 
iterations using the entire SW stratum employing the SAL. This iterative 
process served to enhance the model's performance and gather samples 
pertaining to each key species present in the region. The training pro-
cedure for the two first fine-tuning iterations was the one proposed in 
the  original  paper  (Delplanque  et  al.,  2023a)  which  consisted  of  two 
main steps: 1) training the architecture using positive patches for 200 
epochs, and 2) collecting and including hard negative patches, which are 
patches  containing  false  positives,  to  further  train  the  model  for  50 
epochs in order to reduce the number of false positives. During the two 
last fine-tuning iterations, only the second step of the training procedure 
was used. Hard negative patches were created using false positives that 
emerged from the thumbnail classification (step 1 of the SAL). To avoid a

surveys of PAs covering thousands of square kilometers and generating 
thousands of images can be delayed by several months using the OCC 
approach due to the slow but necessary manual processing of images. 
Recent  advances  in  machine  learning  have  propelled  the  perspec-
tives of remotely sensed imagery for wildlife conservation (Tuia et al., 
2022),  and  announced  good  prospects  for  the  automation  of  image 
processing from SRF-OCC surveys (Delplanque et al., 2023b; Eikelboom 
et al., 2019). Deep learning (DL) is a subgroup of machine learning and 
artificial intelligence (AI) where artificial neural networks are trained to 
achieve challenging tasks (e.g. detect animals in aerial imagery) through 
a complex multi-level representation of information learned from a large 
amounts of data (LeCun et al., 2015). In the last decade, DL has been 
widely  employed  to  (semi-)automate  the  detection  and  counting  of

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, there isn't explicit mention of specific techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, some practices can still be inferred.

In the study, it appears that they use a pre-trained model and then perform fine-tuning for four iterations using the entire Southwest (SW) stratum employing the Selective Annotation Loop (SAL). This iterative process serves to enhance the model's performance and gather samples pertaining to each key species present in the region.

For the first two fine-tuning iterations, the training procedure consists of two main steps: 1) training the architecture using positive patches for 200 epochs, and 2) collecting and including hard negative patches, which are patches containing false positives, to further train the model for 50 epochs in order to reduce the number of false positives. During the last two fine-tuning iterations, only the second step of the training procedure is used.

While this doesn't directly address data bias, it does suggest that the researchers are aware of potential issues related to class imbalance and false positives, and are actively working to mitigate these problems. By incorporating hard negative patches into their training process, they aim to improve the model's ability to distinguish between true and false detections, thereby reducing the impact of data bias.
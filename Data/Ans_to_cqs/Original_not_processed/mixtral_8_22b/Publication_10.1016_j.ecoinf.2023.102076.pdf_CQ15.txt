Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2019). During training, the whole architecture (i.e. the base model and 
the  linear  classification  layer)  is  fine-tuned  using  Adam  optimisation 
(Kingma and Ba, 2014). An alternative approach, known as the feature- 
based  approach,  involves  feeding  text  representations  obtained  from 
BERT  into  another  model,  such  as  a  neural  network,  which  is  then 
trained  independently.  Whilst  this  is  less  computationally  expensive, 
fine-tuning of BERT architecture has shown improved performance over 
feature-based approaches (Liu et al., 2019; Nguyen et al., 2020), so we 
use  ‘BertForSequenceClassification’  to  represent  the  cutting  edge  in 
classifier performance. For our BERT classification models, we used 10 
training epochs, determined from pilot experiments (Table S2.1) and a 
batch  size  of  12.  We  also  introduced  classification  dropout  of  0.3  to 
increase model generalisability (see Supporting Information for further 
details).

Since BERT can only accept a maximum of 512 tokens as input, we 
used smaller subsections of the longer web articles as classifier input, an 
approach previously applied to long document classification (Fiok et al., 
2021; Sun et al., 2019). To identify the sections of text most likely to be 
relevant to the classification decision, documents were split into chunks 
of 300 tokens using the NLTK word-tokenizer. Chunks that contained 
any  of  our  original  query  terms  (i.e.  any  bat-related  or  exploitation- 
related  term)  were  retained.  If  more  than  one  chunk  contained  key-
words,  the  first  such  chunk  of  the  document  was  retained  for  use  in 
model training and evaluation. Before being fed into the BERT model, 
texts are tokenized using the BERT tokenizer. Here, we set the maximum 
number of tokens as 400 for social media posts, as most were very short, 
and 512 for web articles, meaning some texts would have been further 
truncated at this step.

Here,  we  demonstrate  and  evaluate  a  text  classification  pipeline, 
designed to address both the issue of imbalanced datasets and the need 
for large training datasets, using an example data collection scenario. 
Importantly,  we  use  a  hierarchical  classification  approach,  whereby 
data are passed through a series of classifiers with increasingly specific

sifiers, that can be tailored for specific research questions. In addition, 
training data in which the target class makes up a small proportion of 
text generally leads to poor classification performance, as demonstrated 
with web articles for classification task 2 in this study (Fig. 3). Never-
theless, although not applied in our pipeline, augmenting training data 
by  oversampling  of  the  minority  class  can  help  to  alleviate  this  issue 
(Padurariu and Breaban, 2019).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The deep learning pipeline uses Adam optimization during training. This optimization technique is applied to fine-tune the entire architecture, including the base model and the linear classification layer. Adam optimization is chosen due to its superior performance compared to other methods like stochastic gradient descent (SGD). Additionally, classification dropout of 0.3 is introduced to increase model generalizability.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.2. Performance evaluation metrics 

The performance of the proposed WT-HMM is compared with other 
existing blue whale classification algorithms in the literature, using ac-
curacy and F1-score metrics. Accuracy measures the overall correctness 
of  the  model's  detection  and  classification  by  calculating  the  ratio  of 
correctly classified instances to the total number of instances, given by 

Accuracy =

Number of Correct Classifications
,
Total Number of Classifications

=

TP + TN
TP + TN + FP + FN

,

(11)

Precision =

TP
.
TP + FP

(12) 

It focuses on the correctness of positive detections and classifications 
and helps evaluate the model's ability to avoid false positives. Moreover, 
recall measures the proportion of true positive detections and classifi-
cations among the actual positive instances in the dataset, given by 

Recall =

TP
TP + FN

.

(13) 

It focuses on the model's ability to detect positive instances and avoid 
false  negatives.  Hence,  F1-score  provides  a  balanced  measure  of  the 
model's performance, considering both precision and recall as 

F1 (cid:0) score = 2* Precision*Recall
.
Precision + Recall

(14)

Furthermore,  the  comparison  between  recording  months  provides 
valuable insights into the tradeoffs  associated with dataset size  selec-
tion. The superior performance of the WT-HMM in the 8 to 20-h dataset 
size  range  for  both  months  suggests  the  most  suitable  range  for 
achieving  high  accuracy.  This  finding  resonates  with  studies  empha-
sizing the importance of selecting appropriate sample sizes for effective 
model training (Kahl et al., 2021).

(14) 

Typically,  a  higher  F1-score  (score  close  to  1)  indicates  a  better 
model performance, as it signifies a better balance between precision 
and recall. A low F1-score (score close to 0) suggests that the model may 
struggle  to  accurately  classify  positive  instances  and/or  may  miss  a 
significant  number of  positive instances.  Since the  classification algo-
rithm involves an element of randomness, such as random initialization 
of weights in ANN or random sampling during training. The experiments 
are performed over different runs to help capture the variability intro-
duced by these random factors. 

3.3. WT-HMM performance with different dataset sizes

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The performance of the proposed WT-HMM is evaluated using two main metrics: accuracy and F1-score. Accuracy measures the overall correctness of the model's detection and classification by calculating the ratio of correctly classified instances to the total number of instances. It is calculated using Equation (11), which divides the sum of True Positives (TP) and True Negatives (TN) by the total number of classifications.

Precision, also known as Positive Predictive Value, is another metric used to assess the performance of the model. It focuses on the correctness of positive detections and classifications and helps evaluate the model's ability to avoid false positives. Precision is calculated using Equation (12), which divides the number of True Positives by the sum of True Positives and False Positives (FP).

Recall, also known as Sensitivity or True Positive Rate, is yet another metric used to evaluate the performance of the model. It measures the proportion of true positive detections and classifications among the actual positive instances in the dataset. Recall is calculated using Equation (13), which divides the number of True Positives by the sum of True Positives and False Negatives (FN).

Finally, the F1-score provides a balanced measure of the model's performance, considering both precision and recall. It is calculated using Equation (14), which multiplies twice the product of precision and recall divided by their sum. Typically, a higher F1-score (close to 1) indicates a better model performance, while a lower F1-score (close to 0) suggests that the model may struggle to accurately classify positive instances and/or miss a significant number of them.
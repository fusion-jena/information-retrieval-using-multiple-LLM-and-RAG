Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4. Model retraining 

Based on the quality check, the researcher can decide whether model 

performance  is  satisfactory  or  if  the  model  performance  should  be 
improved by selecting new training images and retraining the model. If 
images from new sites have been classified, model performance can be 
improved by including images from these sites in the training data set. If 
the model had problems with some classes; e.g. identification of some 
species,  it  might  help  to  include  more  images  of  these  species  in  the 
training data set. When selecting new training images, the model output 
from the original model can be helpful to find images that meet a certain 
criteria, e.g. to find images of a certain species. However, all training 
images should be reviewed manually instead of relying on model out-
puts only to avoid misclassified training images. 

2.5. Manual classification

Table 1 
Number of training images, validation images (used for model validation during 
model  training)  and  out-of-sample  test  images  (used  for  external  model  vali-
dation after training was finished) as well as number of new images selected 
from  the  images  taken  between  summer  2020  and  summer  2021  for  model 
retraining.  

Class 

Number of 
training 
images 

Number of 
validation 
images 

Number of 
out-of-sample 
test images 

Number of new 
training images 
for model 
retraining 

Bad 

6453 

quality 

Bird 
Empty 
Least 

weasel 
Lemming 
Shrew 
Stoat 
Vole 
TOTAL 

3382 
9444 
1725 

9449 
9265 
4024 
9894 
53636 

677 

219 
979 
98 

967 
962 
438 
1024 
5364 

549 

119 
3301 
69 

647 
584 
64 
919 
6252 

306 

195 
533 
424 

449 
416 
425 
528 
3276

classification tasks. Inf. Process. Manage. 45, 427–437. https://doi.org/10.1016/J. 
IPM.2009.03.002. 

Steenweg, R., Hebblewhite, M., Kays, R., Ahumada, J., Fisher, J.T., Burton, C., 

Townsend, S.E., Carbone, C., Rowcliffe, J.M., Whittington, J., Brodie, J., Royle, J.A., 
Switalski, A., Clevenger, A.P., Heim, N., Rich, L.N., 2017. Scaling-up camera traps: 
monitoring the planet’s biodiversity with networks of remote sensors. Front. Ecol. 
Environ. 15, 26–34. https://doi.org/10.1002/FEE.1448. 

Tabak, M.A., Norouzzadeh, M.S., Wolfson, D.W., Newton, E.J., Boughton, R.K., Ivan, J.S., 
Odell, E.A., Newkirk, E.S., Conrey, R.Y., Stenglein, J., Iannarilli, F., Erb, J., Brook, R. 
K., Davis, A.J., Lewis, J., Walsh, D.P., Beasley, J.C., VerCauteren, K.C., Clune, J., 
Miller, R.S., 2020. Improving the accessibility and transferability of machine 
learning algorithms for identification of animals in camera trap images: MLWIC2. 
Ecol. Evol. 10, 10374–10383. https://doi.org/10.1002/ece3.6692.

306 

195 
533 
424 

449 
416 
425 
528 
3276  

training  data  set. The  models were  trained on  a  cloud  service  with  1 
GPU,  4  CPUs  and  16  GB  RAM  provided  by  Sigma2  -  the  National 
Infrastructure  for  High  Performance  Computing  and  Data  Storage  in 
Norway. 

We evaluated the model performance on the validation data set and 
the  out-of-sample  test  data  set  by  calculating  model  accuracy  as  the 
number of correct predictions divided by the number of all images as 
well as precision, recall and F1 score for each class (Appendix B): 

Precision =

Recall =

TP
TP + FP
TP
TP + FN
precision*recall
precision + recall

F1 = 2*

(TP = Truepositives)

(FP = Falsepositives)

(FN = Falsenegatives)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The strategy implemented to monitor the model performance during training involves using a separate validation dataset. This dataset is used to calculate various metrics such as model accuracy, precision, recall, and F1 score for each class. These metrics are calculated by comparing the predicted labels generated by the model against the actual labels present in the validation dataset. By monitoring these metrics throughout the training process, researchers can assess how well their model is performing and make necessary adjustments to improve its performance. Additionally, they can also use this information to determine when the model has reached optimal performance and stop further training to prevent overfitting.
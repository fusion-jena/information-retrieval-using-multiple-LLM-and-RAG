Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3. Results

During the monitoring of the training of the neural network using TensorBoard [30], it was
noticed that the validation loss was consistently lower than the training loss before the overﬁtting
point, as shown in Figure 5. This is explained by the effect of the dropout layers as suggested by
Veliˇckovi´c [33]. During the training process, the dropout layers randomly dropped some neurons,
causing the feed-forward network to perform worse, while dropout was deactivated and all neurons
were kept during the validation.

In general, the limitations of deep learning in comparison to other machine learning methods
are the requirement of large and high-quality training data, as well as hardware limitations related to
GPU computing power. The most notable advantage of deep learning is the grade of automatization
and a high potential to generalize when using large amounts of representative training data, which
might, however, not always be available; especially with respect to ground-truth labels that might be
scarce or not exist at all.. Furthermore, the black-box nature makes these algorithms a good choice for
classiﬁcation as suggested in this study, but a challenge for modeling physical properties, as is often
the case in forestry. This case study relied on high-resolution orthophotos with 20-cm spatial resolution
obtained from an aerial survey. The great advantage of these data is that many details can be detected.

Remote Sens. 2019, 11, 1976

12 of 14

greater than 0.5 ha. As these results also required choosing optimal parameters, the transferability of
the method was somewhat more complex compared to the pretrained U-Net architecture, which can be
applied without tuning additional parameters speciﬁc to the area of interest. An in-depth investigation
of the performance of the proposed architecture on further datasets would be an interesting future
project, especially with respect to the power of generalization, which can be increased by different data
augmentation techniques.

4.2. Limitations of This Study

The decoding path was symmetrical to the encoding path and was divided into decoding blocks.
Each decoding block was composed of a concatenation layer that merged the feature maps from the
symmetrical encoding block, a convolutional layer with ReLU activation, a dropout layer, a second
convolutional layer, and a deconvolutional layer with a ﬁlter size of (3, 3) to increase the spatial
dimensionality of the input. The dropout layer was used for regularization. The dropout rate was set
to increase from one encoding block to the next by a step of 0.1 (starting from 0.1), resp. decreasing
from one decoding block to the next, and was found to work well for our setting. After the second
convolutional layer, the activation function used was SeLU. The intuition behind this choice was to
use the same activation function used in the encoding path to concatenate homogeneous feature maps
(the same value indicates the same signal in the data). The output layer was a deconvolutional layer

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In the given context, the regularization method used to prevent overfitting in the deep learning pipeline is mentioned as dropout layers. Dropout is a technique where randomly selected neurons are ignored during training, which helps to avoid overfitting. In this study, the dropout rate was set to increase from one encoding block to the next by a step of 0.1 (starting from 0.1), and was found to work well for their specific setting. However, it should be noted that there are other regularization methods such as L2 regularization, which adds a penalty term to the loss function to discourage large weights, and early stopping, which stops the training process once the model starts to overfit. These methods were not explicitly mentioned in the provided context, but they are commonly used in deep learning pipelines to prevent overfitting.
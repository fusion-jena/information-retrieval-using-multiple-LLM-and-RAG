Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

noise present in the herbarium sheets. Additionally, as
per results on only herbarium data, it would be a good
idea to start working on a model whose hyperparame-
ters, architecture and data augmentation are thought for
herbarium in particular, to maximize accuracy for a sys-
tem dedicated to herbarium in mind. More experiments
with bigger leaf datasets are recommended, since some
viability of using herbarium for fine tuning on leaf images
was observed. Concerning the question of how herbar-
ium data could be useful for field images classification,
we believe we should rather try to model the drying pro-
cess itself typically by learning a transfer function between
a representation space dedicated to herbarium images
and another one dedicated to field images. In order to

Table 5 synthesises the results of these experiments.
The main conclusion is that initializing the models
with ImageNet always results in better accuracy for all
experiments. If we compare experiments R.CR.CR and
H255.CR.CR, fine tuning over herbaria against the ran-
domly initialized baseline offers an accuracy increase of
4.7 and 3.8% for top-1 and top-5 respectively. By compar-
ing experiments R.CR.CR and H255I.CR.CR, the increase
goes up to 12.1 and 8.6% respectively, but still, it is less
effective than fine-tuning directly from the ImageNet
dataset (I.CR.CR). This result is aligned with previous
evaluations in the literature (see e.g. [30, 31]). It confirms
that models trained on a big generalist dataset such as
ImageNet can be used as generic feature extractors for any
domain-specific task. On the contrary, the visual features
learned on Herbarium255 are more specific to herbarium

• H255.CR.CR: The neural network was pre-trained on
the Herbarium255 dataset to initialize the weights,
fine-tuned on the Costa-Rica leaf scans training set
(70%) and tested on the Costa-Rica leaf scans test set
(30%).

• H255I.CR.CR: The neural network was pre-trained
on ImageNet and then on Herbarium255 before
being fine-tuned on the Costa-Rica leaf scans training
set (70%) and finally tested on the Costa-Rica leaf
scans test set (30%).

• R.PC.PC: The neural network was initialized

randomly, trained on the PlantCLEF training set
(80%) and tested on the PlantCLEF test set (20%).
• I.PC.PC: The neural network was pre-trained on the
generalist dataset ImageNet to initialize the weights,
fine-tuned on the PlantCLEF training set (80%) and
tested on the PlantCLEF test set (20%).

• H1K.PC.PC: The neural network was pre-trained on
the Herbarium1K dataset to initialize the weights,
fine-tuned on the PlantCLEF training set (80%) and
tested on the PlantCLEF test set (20%).

7×7/2
3×3/2

3×3/1
3×3/2

Convolution

Max pool

Batch norm

LRN

Convolution

Max pool

Batch norm

LRN

Inception (3a)

Inception (3b)

Max pool

3×3/2

Batch norm

Inception (4a)

Inception (4b)

inception (4c)

Inception (4d)

Inception (4e)

Max pool

3×3/2

Batch norm

Inception (5a)

Inception (5b)

Avg pool

7×7/1

Batch norm

Linear

Softmax

0

0

0

0

2

2

2

0

0

0

0

112×112×64 1
56×56×64
56×56×64
56×56×64
56×56×192
28×28×192
28×28×192
28×28×192
28×28×256
28×28×480
14×14×480
14×14×480
14×14×512
14×14×512
14×14×512
14×14×528
14×14×832
7×7×832
7×7×832
7×7×832
7×7×1024
1×1×1024
1×1×1024
1×1×10000
1×1×10000

1

2

2

0

2

2

2

0

0

2

2

0

0

2.7K

34M

112K

360M

159K

380K

128M

304M

364K

437K

463K

580K

840K

73M

88M

100M

119M

170M

1072K

54M

1388K

71M

1000K

1M

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't enough information available in the given context to determine exactly how the hyperparameters of the model were optimized. However, common methods for optimizing hyperparameters include techniques like Grid Search, Random Search, Bayesian Optimization, or even manual tuning. These methods involve selecting different combinations of hyperparameters and evaluating their performance on a validation set to find the best combination. Without further details about the specific process followed during this study, it cannot be definitively stated which method was employed here.
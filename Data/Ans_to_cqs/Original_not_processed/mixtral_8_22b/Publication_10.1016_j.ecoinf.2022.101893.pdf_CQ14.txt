Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2001), decision tree (DT) (Quinlan, 1986) and multi-layer perceptron 
(MLP)  (White  and  Rosenblatt,  1963).  There  are  two  main  streams  to 
extract  birdsong  feature  parameters:  the  traditional  handcrafted 
extraction  and  the  deep  learning  extraction.  Common  methods  for 
handcrafted extracting feature parameters include Mel-frequency ceps-
tral  coefficients  (MFCC),  gammatone  cepstral  coefficients  (GTCC), 
wavelet  transform  (WT),  Hilbert-Huang  transform  (HHT),  short-time 
Fourier  transform  (STFT),  linear  predictive  cepstral  coding  (LPCC), 
etc. Moreover, these single features cannot fully describe the informa-
tion contained in the birdsong. Mixed acoustic features can lead better 
classification results than single feature extracted from one transform 
domain,  and  maximize  the  information  in  audio  (Chachada  and  Kuo, 
2014).

The redundancy of all features in S, which is the mean of all mutual 

information between fi and fj, as shown in Eq. (13): 

R(S) =

1
|s|2

∑

)

(cid:0)

I

fi; fj

fi ,fj∈S

The criterion of mRMR can be illustrated in Eq. (14): 

mRMR = max

s

(D(S, c) (cid:0) R(S) )

(13) 

(14) 

Incremental search method can select the optimal feature set quickly 
and efficiently. For a raw feature set X, if the optimal feature Sm(cid:0) 1 has 
been selected, the search continues for the optimal feature that maxi-
mizes Eq. (14) in the remaining feature space X (cid:0) Sm(cid:0) 1. The optimal 
feature should satisfy Eq. (15):

shown in the Fig. 6. 

The network is designed with four convolution kernel pooling layers 
to extract deep features. After the convolution pooling, the data enters 
the fully connected layer for connection. The network adds a dropout 
layer  to  the  fully  connected  layer  to  reduce  network  parameters.  A 
Dense_1 layer is added after flatten to reduce the dimension of the deep 
features, and sets the parameter to 500, which is the parameter of the 
deep feature dimension extracted from different views. 

4.2. Feature selection

4.2. Feature selection 

Feature selection is to select some of the most efficient feature sub-
sets  from  the  raw  feature  set  to  reduce  the  process  of  feature  space 
dimension  (Liu and  Motoda, 1999). Some  irrelevant or  redundant in-
formation  of  the  sample  can  be  removed  by  feature  selection,  which 
reduces the complexity and processing time of the classification algo-
rithm,  and  improves  the  performance  and  efficiency  of  the  classifier. 
Minimal-redundancy-maximal-relevance  (mRMR)  is  a  filter  feature 
measurement method (Peng et al., 2005), which is intended to select the 

EcologicalInformatics72(2022)1018936S. Xie et al.                                                                                                                                                                                                                                       

Fig. 6. CNN pre-training model.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given context, there is no explicit mention of how the hyperparameters of the model were optimized, such as using grid search or random search. Therefore, I am unable to provide a helpful response to this specific query.
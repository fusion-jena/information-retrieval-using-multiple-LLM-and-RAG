Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The graph illustrates the loss while training the neural network 
and the average loss is reduced to 0.54%. That means the model 
is  affected  by  training  data.  A  total  of  4000  iteration  were  run 
and  it  took  14  hours  to  complete  the  training.  The  model 
improved swiftly in terms of precision, recall and mean average 
precision  before  plateauing  after  about  1000  iterations,  and 
around 1000 iterations the loss showed a rapid decline. 

For  each  epoch,  64  images  are  randomly  selected  and  used  to 
train  the  model.  Each  image  is  used  multiple  times  due  to  the 
limited  number  of  samples.  Figure  8,  shows  some  training 
samples, and the number zero indicates the fish class. 

Figure 8. Training samples.

In  order  to  train  the  model,  it  is important to provide not only 
the  class  of  the  object  but  also  the  bounding  box  data  of  the 
object  as  correct  answer.  In  this  context,  we  parsed  the  x,  y 
coordinates, width and height of the fish in the annotation of the 
image’s  dataset.  In  addition,  we  used  data  augmentation  to 
extend the amount of data. 

The  model  has  better  performance  as  number  of  epochs 
increases.  However,  when  number  of  epochs  exceeds  200,  the 
network  seems  to  be  overfitted  to  training  data.  As  a  result, 
Table  I  shows  the  hyper-parameters  used  to  train  and  validate 
the Yolo model. 

Number 
of 
iterations 
4000 

Number 
of 
epochs 
200 

Learning 
rate 

Batch 
size 

Subdivisions 

0.001 

16 

64 

Table 1. Hyper-parameters to train the Yolo model.

Figure 5. A flowchart of training and detection process of 
YOLO-Fish model. 

A  series  of  experiments  were  conducted  to  evaluate  the 
performance  of  the  model.  The  indexes  for  evaluation  of  the 
trained model are defined as follows: 

                           Recall = TP / TP + FN           (3) 
                           Precision = TP / TP + FP       (4)

The  flowchart  of  training  and  detection  process  of  YOLO-fish 
model is shown in figure 5. The methodology of this study can 
be broken down into two phases, first one is the training phase; 
this  step  consists  of  three  stages.  Firstly,  data  collection,  data 
used in this research is represented by several fish images, then 
data  pre-processing  which  includes  image  augmentation  and 
resizing, these images will be split in two sections (the training 
section  which  represents  about  70%  and  the  testing  section 
which  is  about  30%).  Secondly,  data  processing,  where  the 
training will be started after making the ground truth bounding 
boxes and the images as inputs to our training model, as a result 
the predicted bounding boxes plus the confidence scores will be 
obtained.  Thirdly,  model’s  evaluation,  so  to  evaluate  our 
model’s performance, the mAP will be calculated based on GT 
bboxes  (Ground  truth  bounding  boxes)  and  the  predicted

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To monitor the model performance during training, the authors employed various strategies. They tracked the loss value throughout the training process, observing its reduction from an initial state to around 0.54%, indicating that the model was learning effectively from the training data. Additionally, they evaluated the model's precision, recall, and mean average precision (mAP), noting improvements until approximately 1000 iterations, after which the model began to plateau. This suggests that monitoring these metrics helped determine the optimal point to stop training and prevent overfitting. Furthermore, the authors utilized a validation set to assess the model's generalization capabilities, ensuring that it did not become too specialized to the training data. Lastly, they experimented with different hyperparameter configurations, such as varying the number of epochs and learning rate, to find the best combination for maximizing model performance.
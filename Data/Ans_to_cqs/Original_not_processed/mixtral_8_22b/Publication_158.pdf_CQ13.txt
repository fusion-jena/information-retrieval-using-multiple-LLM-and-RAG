Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Figure 3. Schematic of the CNN used in the study. Input image size was 32 by 32 pixels shown as the
large grey square. Numbers represent the number of features output from a convolution or dense
layer, except for the cropping layers where the crop dimension is shown.

2.5.2. Random Forest

For random forest, input features included the mean and standard deviation for image
window sizes of 5 × 5, 15 × 15, and 32 × 32 in addition to the central pixel values for
each input feature. We used a random search to identify an optimal parameter set where
100 sets were sampled and compared using three-fold cross validation. Search parameter
ranges were number of trees (200 to 800, by 100), minimum samples split (2–4), minimum

Remote Sens. 2021, 13, 634

7 of 21

samples per leaf (2–4), maximum depth (full), bootstrap (true), and maximum features for
split (square root of the number of features). For a description of these parameters see the
Scikit-learn documentation [48]. These results were reﬁned using a grid-search with the
parameter ranges set to ±50 for the number of estimators and ±1 for the other parameters.

2.6. Membership Based Change Detection and Segmentation
2.6.1. Split Window Detection of Change Seed Points

For training, the Adam optimizer was used with 500 epochs and categorical-cross
entropy loss function. Batch size was set to 64. Data augmentation was applied to enhance
sample variation and included rotation, reﬂectance bias of ±10%, and random noise of
±10% reﬂectance for 15% of the input image. To determine training convergence, we
monitored the validation data across training epochs and the network weights with the
best performance were kept. Early stopping criteria was applied if no improvement was
found in 50 epochs. For CNNs, weight initialization and random selection for batch
training can lead to variability in the results. Thus, an ensemble of three models were
generated and the average of the ﬁnal output layer taken. Max pooling was used for two
models in the ensemble and average pooling for one. In initial tests using max or average
pooling showed that both provided improvement in some cases and thus could enhance

There are numerous aspects to consider for enhancing the deep-learning classiﬁer,
speciﬁcally regarding the input image size; network conﬁguration, depth, and width; and
training strategy. In initial experimentation, several standard forms of deep-learning CNN
architectures were tested based largely on ResNet [46] and Inception [63], but these did
not perform well due to the range of image objects size/scales encountered. To account
for objects represented as a single pixel up to larger assemblages extending beyond the
32 by 32 pixel input image, the ensemble with varying input image sizes was selected.
Segmentation-based approaches were also considered as an alternative, but sufﬁcient
quality training data for segmentation was a limitation.

3.3.3. Change Detection

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text provides information about the use of a Convolutional Neural Network (CNN) in the study. However, it does not explicitly mention all the hyperparameters used in the deep learning model. From the given context, we can infer that the Adam optimizer was used with 500 epochs and categorical-cross entropy loss function. The batch size was set to 64. Additionally, data augmentation techniques such as rotation, reflectance bias of ±10%, and random noise of ±10% reflectance for 15% of the input image were applied. It is mentioned that early stopping criteria was applied if no improvement was found in 50 epochs. Furthermore, an ensemble of three models was generated, and the average of the final output layer was taken. Two models in the ensemble used max pooling, while one used average pooling.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In view of this discovery, we re-examined the application strategies 
of deep learning, no longer pursuing model complexity but exploring the 
potential  of  shallow  networks.  Through  meticulous  algorithm  optimi-
sation  and  structural  design,  we  strive  to  maintain  the  model  perfor-
mance while reducing the computational complexity  and deployment 
costs,  achieving  a  balance  between  speed  and  accuracy.  The  specific 
contributions of this study are as follows:  

• We propose a lightweight SIAlex model that utilises AlexNet as the 
backbone,  fully  exploiting  the  performance  of  minimalist  models. 
Ensuring a good balance between speed and accuracy. 

• The  method  of  cascading  multiple  activation  functions  fully  in-
troduces  nonlinear  factors  such  that  the  model  approximates  the 
nonlinear  expression  function  of  the  learning  features  while  also 
improving the gradient propagation.

AlexNet is a deep CNN. Its structure can be divided into three main 
parts: conv-block, max-block and classifier. The conv-block comprises a 
series of convolution layers, normalisation layers, and activation func-
tion layers. It extracts features from the input data and captures local 
features through convolution layers. The normalisation layer is used to 
accelerate training and prevent overfitting, while the activation function 
layer  uses  the objective  function  to  increase  the nonlinear  expression 
ability of the model. The max-block module is located behind the conv- 
block.  Through  the  down-sampling  operation,  the  size  of  the  input 
feature map is decreased, effectively reducing the amount of data while 
retaining the essential features, thus reducing the amount of calculations 
of the model. Second, it can increase the robustness of the model and 
make it more adaptive to input characteristics. A classifier is a crucial

generalised  model  by  adding  convolution  modules  or  introducing  re-
sidual  connection  techniques.  However,  more  complex  and  deeper 
models  often  incur  higher  computational  costs  and  require  longer 
training  time.  To  solve  these  problems,  the  AlexNet  network  was 
designed to be lightweight and optimised to achieve deep training and 
short-term testing (Zhang et al., 2022). Specifically, the two convolution 
layers are merged in the model deployment mode while ensuring that 
the nonlinear factors caused by the activation function are not reduced. 
By reducing the number of layers in the model, the computational effi-
ciency is improved (Ding et al., 2021; Zhao et al., 2017) and the infer-
ence time of the model is significantly shortened.

complexity. This helps solve the problems of model training time and 
computing  resources.  Shazzadul  Islam  et  al.  (Islam  et  al.,  2019)  used 
VGG-16  to  extract  features  from  bird  images  and  then  used  support 
vector machines for classification, achieving an accuracy of 89%. The 
ResNet network (He et al., 2016) optimises the network through residual 
modules by introducing skip connections between the traditional con-
volutional layers. This enables the network to learn and retain low-level 
feature  information  better,  thereby  avoiding  gradient  vanishing  and 
representation bottlenecks. Sankupellay et al. (Sankupellay and Kono-
valov,  2018)  used  ResNet-50  to  automatically  recognise  the  spectro-
grams of 46 bird songs with an accuracy of 60%–72%.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention any specific optimization techniques like Stochastic Gradient Descent (SGD), Adam, etc. However, it discusses several optimization strategies employed in the deep learning pipeline. These include meticulous algorithm optimization and structural design aimed at maintaining model performance while reducing computational complexity and deployment costs.

One strategy mentioned involves merging two convolution layers in the model deployment mode, which ensures that the nonlinear factors caused by the activation function are not reduced. By reducing the number of layers in the model, the computational efficiency is improved, and the inference time of the model is significantly shortened.

Additionally, the use of normalization layers accelerates training and prevents overfitting. Down-sampling operations reduce the size of the input feature map, effectively decreasing the amount of data while retaining essential features. This reduces the amount of calculation required by the model.

Furthermore, the method of cascading multiple activation functions introduces nonlinear factors so that the model approximates the nonlinear expression function of the learning features. This also improves the gradient propagation.

While these strategies do not directly correspond to specific optimization algorithms like SGD or Adam, they represent broader optimization approaches within the deep learning pipeline.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

threshold during training for 3000 iterations. At the end of the training process, the best
performing model based on the validation loss was saved and used as the Faster R-CNN
model with a NMS threshold of 0.7 during testing.

0.0232

extent
ﬁlled_area
major_axis_length
minor_axis_length
perimeter
solidity
diameter
aspect_ratio
rectangularity
compactness
circularity
narrow_factor
per_dia_ratio
per_length_ratio
per_length_
width_ratio

MAE

Faster
R-CNN

0.0123
461.2895
2.8865
1.4556
21.1247
0.0077
1.6507
0.0330
0.0035
1.0373
0.0289
0.0049
0.0986
0.0637

0.0415

Table 9. Cont.

MSE

YOLOv5s

0.0174
1300.0921
5.6320
3.3250
33.1226
0.0081
3.9047
0.0288
0.0046
1.2096
0.0352
0.0045
0.1163
0.0768

0.0507

Proposed
Method

0.0002
164,526.7895
6.1220
1.5845
258.5453
0.0000
1.9746
0.0008
0.0000
0.6377
0.0007
0.0000
0.0062
0.0021

Faster R-CNN

YOLOv5s

0.0003
383,162.7632
40.6365
6.0962
1608.1119
0.0005
5.6350
0.0080
0.0002
5.2750
0.0025
0.0001
0.0410
0.0212

0.0005
27,113,134.6711
349.0719
130.0219
4932.9094
0.0004
205.8782
0.0042
0.0002
4.9619
0.0027
0.0001
0.0402
0.0218

0.0011

0.0075

0.0078

Proposed
Method

0.0143
405.6190
2.4743
1.2588
16.0793
0.0068
1.4052
0.0288
0.0030
0.7985
0.0258
0.0052
0.0787
0.0463

0.0330

RMSE

Faster
R-CNN

0.0185
619.0014
6.3747
2.4690
40.1013
0.0227
2.3738
0.0894
0.0145
2.2967
0.0499
0.0112
0.2024
0.1455

0.0865

YOLOv5s

0.0225
5207.0274
18.6835
11.4027
70.2347
0.0204
14.3485
0.0645
0.0128
2.2275
0.0523
0.0078
0.2004
0.1476

0.0884

7. Conclusions and Future Work

Herbarium sheet images are usually of high resolution to capture the ﬁne-grain details
of the specimens. As a standard procedure for training deep learning models, all input
images together with their annotation were resized to a 512 × 512 resolution to reduce
the computational cost during training. Rotation, ﬂipping, and brightness adjustments
were applied as augmentation techniques for better network generalization. We used
DeepLabv3+ as the segmentation model with ResNet-101 as the feature extractor. This
model was pre-trained on the ImageNet dataset and ﬁne-tuned on the dataset. This is
useful as earlier layers of the network tend to learn generic features and therefore become
useful for other computer vision tasks [49]. We applied an Adam optimizer with a learning
rate of 1 × 10−4 and a batch size of 3. The model was trained for 100 epochs with a binary
cross-entropy loss function as we had a binary class problem (leaf or background).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific evaluation metrics used to assess the performance of the deep learning model. However, it mentions that the best performing model was selected based on the validation loss after training for 3000 iterations. Additionally, Table 9 presents Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Square Error (RMSE) values for different methods, including Faster R-CNN and YOLOv5s. These error metrics can be used to measure the difference between predicted and actual values, providing insights into the model's performance. Nevertheless, without further context, it cannot be determined if additional evaluation metrics such as accuracy, precision, recall, or F1 score were employed.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.4.3. Model evaluation 

We calculated the performance metrics used frequently in previous 
studies (Dai et al., 2021; Kahl et al., 2021; LeBien et al., 2020; Nanni 
et  al.,  2020b;  Ruff  et  al.,  2021;  Xie  et  al.,  2019;  Zhang  et  al.,  2019), 
including the area under the ROC curve (AUC) and average precision 
(AP) for each sound class and the mean average precision (mAP) and 
macro-average AUC for the overall model. Because of the trade-off be-
tween precision and recall, we also plotted precision, recall and F1 score 

EcologicalInformatics68(2022)1015345S.-H. Wu et al.

curves for each class to examine the class-specific performance and its 
changes with threshold scores (see Appendix S3). All the metrics were 
calculated using the scikit-learn package (https://scikit-learn.org/). 

owlet  S-01  to  0.91  for  the  northern  boobook  S-01.  Except  for  the 
northern  boobook  S-01,  whose  AP  value  was  0.49,  the  AP  values  for 
other classes were between 0.76 and 0.96 (Table 1). 

3.5. Reinforcement model

Compared with previous CNN-based approaches, the mAP of 0.83 in 
our case study is inconspicuous (Appendix S2: Table S4). However, not 
only the model itself but also the test dataset used can affect the values of 
model performance metrics. Because the SILIC aims to facilitate auto-
mated information extraction from the recordings of PAM projects, our 
test dataset was obtained from soundscape recordings, which are usually 
much noisier than focal recordings. In comparison with focal recordings, 

EcologicalInformatics68(2022)1015346S.-H. Wu et al.

3.4. Model training and evaluation 

3.4.1. Model training 

We  used  YOLOv5s,  the  smallest  weight  of  YOLOv5,  to  build  the 
sound identification model. The model was trained for 100 epochs with 
32 batch size and the input image dimension 640 × 640. We used default 
data  augmentation  including  scaling,  color  space  adjustments,  and 
Mosaic  augmentation  during  the  training  process.  Under  this  setting, 
four  original  spectrograms  were  randomly  selected,  resized,  cropped, 
color jittered (i.e., changes in hue, saturation and exposure) and then 
merged into one big input image for model training. The model training 
was performed using a workstation equipped with the following speci-
fications: Intel Xeon E5–2660 V4 CPU * 2, DDR4 2400 ECC 16GB RAM * 
8, NVIDIA Titan RTX GPU * 2, and WD black AN1500 4 TB NVMePCIe 
SSD * 2. 

3.4.2. Model inference and test datasets

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The performance of the deep learning model is evaluated using several metrics, including the area under the Receiver Operating Characteristic (ROC) curve (AUC), average precision (AP) for each sound class, mean average precision (mAP), macro-average AUC for the overall model, precision, recall, and F1 score. These metrics are commonly used in previous studies and provide insights into the model's ability to accurately identify different sounds within the soundscape recordings. Additionally, precision, recall, and F1 score curves are plotted for each class to examine the class-specific performance and its changes with threshold scores. All these metrics are calculated using the scikit-learn package.
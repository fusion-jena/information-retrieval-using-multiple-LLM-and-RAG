Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

However,  although  research  on  the  urban  AE  gained  traction  in 
recent years, sophisticated methods to quantify its complex properties 
on  a  larger  scale  are  still  scarce.  This  is  especially  true  for  big  audio 
datasets, which are so extensive that listening to the files becomes no 
longer  feasible.  To  date,  few  approaches  have  been  used  to  quantify 
large datasets of the urban AE. Classical approaches reach their limits 
when  it  comes  to  analyzing  longer  recording  periods  (e.g.  several 
months) (Gage et al., 2017). In recent years, sound event classification 
using  machine  learning  approaches  became  more  popular  (Phillips 
et al., 2018; Sethi et al., 2022; Stowell et al., 2019; Ulloa et al., 2018), 
but is still not ready to be deployed unconfined (Alcocer et al., 2022). In 
psychoacoustics, mostly smaller field studies have been conducted (Hall 
et  al.,  2013;  Ma  et  al.,  2021;  Montoya-Belmonte  and  Navarro,  2020;

To retrieve information about the frequency spectrum, we calculate a 
Fast Fourier Transform (FFT) for all recordings of our dataset, sort the 
values into 1024 equally sized bins (from 0 to 22,050 Hz; bin width =
21.5  Hz;  no  spectral  weighting)  and  average  the  values  energetically 
inside each bin (Haselhoff et al., 2022a). As the magnitude of variability 
broadly differs between low and high frequencies, we align them by a 
log-transformation.  Following  previous  research, we  focus on  the  fre-
quency range from 0 to 13 kHz, as frequencies above 13 kHz are only 
rarely  occupied  in  the  urban  AE  (Bradfer-Lawrence  et  al.,  2019; 
Haselhoff et al., 2022a). 

2.2. Land use types

In this work, we (i) describe all necessary steps of how CNs can be 
constructed from large-scale acoustic data and (ii) evaluate this process 
on approx. one million minutes of audio recordings from 23 different 
sites  in  Bochum,  Germany.  Here,  the  foundation  for  constructing

to analyze large-scale audio data, expanding our understanding of the time-frequency domain of the urban AE.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not explicitly state the specific data format used in the deep learning pipeline. However, based on the context, it can be inferred that the data format used is audio since the study focuses on analyzing large-scale audio data. Specifically, the researchers calculated Fast Fourier Transforms (FFT) for all recordings in their dataset, sorted the values into 1024 equally sized bins, and averaged the values energetically inside each bin. They also focused on the frequency range from 0 to 13 kHz, as frequencies above 13 kHz are rarely occupied in the urban Acoustic Environment (AE). Therefore, it can be concluded that the data format used in the deep learning pipeline is audio.
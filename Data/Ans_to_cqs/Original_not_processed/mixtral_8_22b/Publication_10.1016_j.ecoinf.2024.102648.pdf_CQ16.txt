Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Training 

Validation 

Training 

Validation 

1 
1 
3 
3,1 
3,1 
3,1 

1 
3 
3 
3,0 
3,1 
3,3 

1 
1 
3 
3,1 
3,1 
3,1 

1 
1 
1 
2 
2 
2 

0.72 
0.72 
0.71 
0.74 
0.76 
0.80 

0.75 
0.74 
0.76 
0.76 
0.77 
0.79 

28.2 
26.9 
26.7 
27.3 
26.1 
24.4 

27.6 
26.4 
25.1 
27.7 
26.1 
24.5 

* Multiyear multilocation data. 

the  best  predictor  of  CLCuD  incidence.  The  BSF  model  achieved  the 
highest  accuracy  in  terms  of  CLCuD  prediction  (R2  = 0.78)  with  the 
lowest  RASE  and  absolute  average  error  (AAE)  followed  by  the  BST 
model (R2  = 0.69). The multiple linear regression showed the lowest 
accuracy (R2 = 0.52) in comparison with other models (Table. 4). 

4. Discussion

2.7. Early stopping 

It is the process that stops growing additional trees/layers/split when 
further addition of more trees/layers/split does  not improve  the vali-
dation statistic in the case of bootstrap forest and boosted tree methods. 
The  validation  statistic  included  the  (R2)  value  for  a  continuous 
response. In the case of the entire machine learning model early stop-
ping  was  enabled  so  that  the  model  would  not  overfit  the  predicted 
percent CLCuD. To address the inclination of neural networks to overfit 
data, the penalty method was employed within the framework of ANN 
models. 

3. Results 

3.1. Multiyear SMW-wise percent CLCuD incidence in the four locations

Fig. 4. Best-fit model of an artificial neural network: Actual by the predicted plot of CLCuD (%)-(a. Training data, b. Validation data) and Residual by the predicted 
plot of CLCuD (%) (c. Training data, d. Validation data). 

EcologicalInformatics81(2024)1026486S.K. Sain et al.                                                                                                                                                                                                                                  

Fig. 5. Actual by the predicted plot of CLCuD (%) for best-fit model (Bootstrap Forest- a. Training data b. Validation data; Boosted Tree model- c. Training data d. 
Validation data). 

Table 1 
Artificial Neural Network (ANN) model with different activation functions for the prediction of CLCuD.  

No. of hidden nodes/neurons 

Activation functions 

No. of hidden layers 

R2 

RMSE 

3 
5 
9 
11 
12 
14 

Tan H 

Linear 

Gaussian 

Training 

Validation 

Training 

Validation

21.5 
20.5 
19.6 

25.6 
25.5 
25.8  

Table 3 
Variable  importance  analysis  with  respect  to  best-fit  models  under  Artificial 
Neural  network  (ANN),  Bootstrap  Forest  and  Boosted  Tree  for  prediction  of 
CLCuD.  

Variables (Regressor) 

Artificial Neural 
Network 

Bootstrap 
Forest 

Boosted 
Tree 

GDD (Growing degree 

0.365 

days) 

Average Relative humidity 

0.127 

(RH avg) 

Whitefly/3leaves 
Rainfall (mm) 
Max. Relative humidity (RH 

max) 

Min. Temperature (Tmin) 
Max. Temperature (Tmax) 
Min. Relative humidity (RH 

min) 

0.10 
0.08 
0.058 

0.037 
0.025 
0.024 

Average Temperature 

0.01 

(Tavg) 

* Multiyear multilocation data. 

0.803 

0.021 

0.036 
0.002 
0.033 

0.014 
0.003 
0.016 

0.012 

0.695 

0.044 

0.021 
0.001 
0.105 

0.011 
0.002 
0.004 

(cid:0) 4 

2e

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The criterion used to determine when training is complete is called early stopping. This process stops adding further trees, layers, or splits when it no longer improves the validation statistic. For continuous response variables, this includes the R2 value. In the case of the entire machine learning model, early stopping is enabled to prevent overfitting the predicted percentage of CLCuD. Additionally, to address the tendency of neural networks to overfit data, the penalty method is applied within the framework of ANN models.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The supervised learning workflow adheres to the steps outlined in 
Sections 2.3.1–2.3.5. However, as detailed in Section 2.3.2, we deviated 
by not generating 400 random points over the entire boundary. Instead, 
we used the vegetation types described by Gertenbach (1983) to guide 
the placement of our sampling plots, from which we then collected the 
training and validation points (Fig. A.2). The creation of several small 
sampling plots within each vegetation type, instead of using one large 
area, also helped speed up the sampling process. Performing the data 
collection  within  the  sampling  plots  ensured  that  we  had  training/ 
validation  points  that  were  representative  of  the  diverse  vegetation 
within each boundary. To determine the size of the sampling plots, we 
first calculated 5% of the total area covered by the boundary, divided 
that area by the number of vegetation types intersecting the boundary

The results obtained in this study can be used as a baseline for future 
LULC  analysis  performed  with  other  methodologies,  such  as  deep 
learning  CNN  (Jagannathan  and  Divya,  2021).  Going  forward,  the 
analysis  of  aerial  images  of  KNP  taken  about  every  two  years  deep 
learning methods will be most effective and useful to map land cover or 
more specifically woody cover. Integrating large-area historical datasets 
in  land-use  and  land-cover  analysis  can  serve  as  a  resource  to  better 
understanding  long-term  landscape  changes  and  support  ecological 
monitoring programs. The results of studies such as this one can be used 
to  better  protect  and  preserve  our  natural  heritage,  enable  effective 
management  strategies,  and  contribute  to  the  conservation  of  global 
biodiversity. 

CRediT authorship contribution statement

based Landsat 8 data classification in Google earth engine using random Forest: the 
case study of Maiella National Park. Remote Sens. 13, 2299. https://doi.org/ 
10.3390/rs13122299. 

Trabucco, A., Zomer, R., 2014. Influence of Aridity on Vegetation [WWW Document]. 

https://doi.org/10.2499/9780896298460_23. 

United Nations Convention to Combat Desertification, 2022. Global Land Outlook, 2nd 

edition. UNCCD, Bonn.  

Urban, M., Heckel, K., Berger, C., Schratz, P., Smit, I.P.J., Strydom, T., Baade, J., 

Schmullius, C., 2020. Woody cover mapping in the savanna ecosystem of the Kruger 
National Park using Sentinel-1 C-band time series data. Koedoe 62, 1–6. https://doi. 
org/10.4102/koedoe.v62i1.1621. 

Verburg, P.H., Neumann, K., Nol, L., 2011. Challenges in using land use and land cover 
data for global change studies. Glob. Chang. Biol. 17, 974–989. https://doi.org/ 
10.1111/j.1365-2486.2010.02307.x.

2.3.4. Woody cover classification 

To  obtain  the  woody  cover  classification  we  chose  the  Random 
Forest classifier, which is a supervised learning algorithm that produces 
multiple decision trees using a randomly selected subset of training data 
and  independent  variables  (Breiman,  2001).  It  has  received  much 
attention in the last few years due to its excellent classification results, 
fast-processing  speed,  ease  in  handling  different  types  of  data,  e.g., 

shape  and  texture,  and  no  requirement  for  normally  distributed  data 
(Belgiu and Dr˘agut¸, 2016; Rodriguez-Galiano et al., 2012; Vogels et al., 
2017).  Another  advantage  of  the  Random  Forest  is  that  it  is  able  to 
compute  variable  importance  measures  that  allow  us  to  identify  the 
variables that exerted a substantial influence on the classification pro-
cess (Rodriguez-Galiano et al., 2012).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there is no explicit mention of any specific deep learning framework being used to build the model. Therefore, it cannot be determined which framework was utilized.
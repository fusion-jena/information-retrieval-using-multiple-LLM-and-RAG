Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

encoding and decoding blocks and are summarized in Table 1. These experiments were monitored
using TensorBoard and validated on the validation portion of the dataset.

The decoding path was symmetrical to the encoding path and was divided into decoding blocks.
Each decoding block was composed of a concatenation layer that merged the feature maps from the
symmetrical encoding block, a convolutional layer with ReLU activation, a dropout layer, a second
convolutional layer, and a deconvolutional layer with a ﬁlter size of (3, 3) to increase the spatial
dimensionality of the input. The dropout layer was used for regularization. The dropout rate was set
to increase from one encoding block to the next by a step of 0.1 (starting from 0.1), resp. decreasing
from one decoding block to the next, and was found to work well for our setting. After the second
convolutional layer, the activation function used was SeLU. The intuition behind this choice was to
use the same activation function used in the encoding path to concatenate homogeneous feature maps
(the same value indicates the same signal in the data). The output layer was a deconvolutional layer

In general, the limitations of deep learning in comparison to other machine learning methods
are the requirement of large and high-quality training data, as well as hardware limitations related to
GPU computing power. The most notable advantage of deep learning is the grade of automatization
and a high potential to generalize when using large amounts of representative training data, which
might, however, not always be available; especially with respect to ground-truth labels that might be
scarce or not exist at all.. Furthermore, the black-box nature makes these algorithms a good choice for
classiﬁcation as suggested in this study, but a challenge for modeling physical properties, as is often
the case in forestry. This case study relied on high-resolution orthophotos with 20-cm spatial resolution
obtained from an aerial survey. The great advantage of these data is that many details can be detected.

The overall workﬂow of preprocessing, network design, and prediction are shown in Figure 2.
Due to the size of the orthophotos and due to memory limitations when doing computations, it is
intuitive to process the orthophotos into relatively small labeled tiles (or rather small image cubes) and
feed them into the CNN. Each orthophoto of size 10,000 × 10,000 pixels was divided into 1525 tiles of
256 × 256 pixels. The data (orthophoto and corresponding labels) were then split into three datasets:
training (80%), validation (20%), and test (two full images of 10,000 × 10,000 pixels). The training
data were used for optimizing the neural network, while the validation dataset was used to assess the
performance during the training process. The test dataset was used to assess the performance of the
ﬁnal optimized neural network. The training and validation images were read into arrays with the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, the deep learning pipeline primarily uses image data. Specifically, high-resolution orthophotos with 20-cm spatial resolution obtained from an aerial survey are utilized. These orthophotos are processed into smaller labeled tiles or image cubes of 256 x 256 pixels each. Therefore, the primary data format used in this deep learning pipeline is image data.
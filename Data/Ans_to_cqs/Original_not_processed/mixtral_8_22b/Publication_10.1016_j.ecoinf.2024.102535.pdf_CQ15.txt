Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During  training,  the  cost  function  was  minimized  using  the  Adam 
optimizer  (Kingma,  2017),  with  an  initial  learning  rate  μ0  and  mo-
mentum  β1  equal  to  0.0001  and  0.9,  respectively.  Aiming  at  better 
convergence  during  training,  we  adopted  a  learning  rate  decay  pro-
cedure proposed in Ganin et al. (2017) by implementing the following 
equation: 
μ0
(1 + αp)β

μe =

(8)  

where p = e
et al. (2017), α and β were set to 10 and 0.75, respectively. 

#Epochs, and e is the current training epoch. Following Ganin

3.2. ResNet 

Residual Networks (ResNet), introduced by He et al. (2016), aimed at 
improving convergence issues during the training of very deep network 
architectures. ResNet is a deep learning model that aims to alleviate two 
main  problems  in  training  neural  networks.  The  first  problem  is  the 
vanishing  gradient,  which  makes  it  difficult  to  optimize  the  model 
during training. The second is the degradation problem, which occurs 
when  adding  more  layers  to  a  deep  neural  network  leads  to  greater 
training errors. ResNet solved those problems by using residual learning 
blocks between layers of the network, allowing for better optimization 
and higher training accuracy.

Each rectangle indicates the operations performed at a block of layers. Conv and SConv stand for regular convolution and depth-wise separable 
convolution, respectively. The values that follow such operations indicate the number of filters, filter size, stride, and dilation rate. Regarding the 
maxpooling operation, the values correspond to the kernel dimension and stride. Dropout refers to the number of neurons randomly turned off during 
each training inference. The number of neurons in the dense layer of the classifier module corresponds to the number of classes in the dataset. 

All architectures were modified mainly in the block of fully connected layers, where we opted to use just the output layer after the feature extractor 
instead of two fully connected layers. Additionally, residual blocks were used at the architectures’ stems rather than in the full feature extractor. All 
those modifications were experimentally determined.

Variants  of  the  VGG  network  have  demonstrated  that  increasing 
network  depth  can  improve  classification  accuracy  by  enabling  the 
learning  of  semantically  enriched  features.  In  the  present  work,  we 
adopted a particular architecture inspired by VGG that has 14 layers (13 
convolutional  and  one  dense  layer).  A  detailed  description  of  the  ar-
chitecture implemented in this work can be found in Section 7. 

3.2. ResNet

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning pipeline uses the Adam optimizer for minimizing the cost function during training. The Adam optimizer is a variant of Stochastic Gradient Descent (SGD) that incorporates momentum and adaptive learning rates. It combines the advantages of Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). In this case, the initial learning rate (μ0) is set to 0.0001, and the momentum (β1) is set to 0.9. To achieve better convergence during training, a learning rate decay procedure is also employed. This procedure is based on the equation proposed in Ganin et al. (2017), where the learning rate decays over time according to the current training epoch (e). The parameters α and β in the equation are set to 10 and 0.75, respectively.
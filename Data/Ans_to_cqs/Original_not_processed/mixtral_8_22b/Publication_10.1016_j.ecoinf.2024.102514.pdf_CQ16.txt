Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4.2. Data splitting 

The  WQ indicator  data  was  divided  into  training  and  testing  data 
sets. In ML, typical data splitting ratios for training and testing datasets 
are 80% and 20%, 67% and 33%, and 50% and 50%, respectively (Sajib 
et al., 2023). In this study, we adopted 80% and 20% ratios for splitting 
the training and testing datasets. A total of 20 and 10 random sample 
locations  were  selected  for  the  purposes  of  training  and  testing, 
respectively. A similar number or fewer sample locations were utilized 
in various previous research studies to train and test the data set (Bui 

EcologicalInformatics80(2024)1025145A.M. Sajib et al.                                                                                                                                                                                                                                 

Fig. 2. Methodological framework for selecting the best predictive model.

erquality-iiwq/wq-challenge. 

Vu, H.L., Ng, K.T.W., Richter, A., An, C., 2022. Analysis of input set characteristics and 
variances on k-fold cross validation for a recurrent neural network model on waste 
disposal rate estimation. J. Environ. Manag. 311 (October 2021), 114869 https:// 
doi.org/10.1016/j.jenvman.2022.114869. 

Wainer, J., Cawley, G., 2021. Nested cross-validation when selecting classifiers is 

overzealous for most practical applications. Expert Syst. Appl. 182 (May), 115222 
https://doi.org/10.1016/j.eswa.2021.115222. 

Wang, Y., Zhao, Y., Xu, S., 2022. Application of VNIR and machine learning technologies 
to predict heavy metals in soil and pollution indices in mining areas. J. Soils 
Sediments 22 (10), 2777â€“2791. https://doi.org/10.1007/s11368-022-03263-3. 

Whitehead, P., Bussi, G., Hossain, M.A., Dolk, M., Das, P., Comber, S., Peters, R., 

Charles, K.J., Hope, R., Hossain, S., 2018. Restoring water quality in the polluted

Fig. 9 presents the RMSE and MAE for the eight ML algorithms. The 
majority of performance indicators were found to be higher during the 
training period; however, during the testing period, model performance 
was shown to have greatly improved, which may be due to the under-
fitting problem (Table S4 - supplementary data) (Wang et al., 2022). In 

addition,  the  lowest  prediction  errors  during  the  training  and  testing 
periods belong to GPR, LR, NN, and SVM (Fig. 9).

et  al.,  2023;  Islam  et  al.,  2023;  Khoi  et  al.,  2022;  Sakaa et  al.,  2022; 
Uddin  et  al.,  2022a,  2022b,  2023a,  2023b,  2023c,  2023d,  2023e, 
2023f).  A  similar  rank-based  approach  was  also  used  by  different  re-
searchers in previous research (Bui et al., 2020). In this approach, we 
rank from 1 to 8 individually for each metric, with 1 indicating the best 
and 8 indicating the worst performance. After that, the best fit predic-
tion model is selected based on the lowest cumulative score.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it appears that the specific criteria used to determine when training is complete are not explicitly mentioned. However, common practices in Machine Learning (ML) include monitoring metrics such as validation loss or accuracy over epochs and stopping the training process once these metrics reach a plateau or no longer improve significantly. This practice helps prevent overfitting and ensures the model generalizes well to unseen data. Additionally, techniques like early stopping can be employed to halt training if the validation error does not decrease after a certain number of iterations. It should be noted that the optimal point to stop training depends on factors such as the complexity of the dataset, the chosen algorithm, and the desired balance between bias and variance.
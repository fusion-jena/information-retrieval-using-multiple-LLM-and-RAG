Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We  trained  algorithms  with  the  amount  of  data  available  after 
manually  removing  the  testing  set,  as  described  above.  During  the 
training, 10% of data served as the validation set to check for overfitting. 
We used a min-max scaler reverted at the end to compute the perfor-
mance metrics in the test set. In all cases, we used a batch size of 64 
samples and trained the algorithm during 100 epochs using Adam as the 
algorithm  to  optimize  the  network  parameters.  Algorithms  were 
implemented in Pytorch (Paszke et al., n.d.) and models were trained 
and tested in Google Colaboratory.2

3.3. Comparison of DMF and state of the art 

In this experiment, the period analyzed, the configuration of DMF4, 
and its training are the same as those described in subsection 3.2. The 
MF model (Rivera-Mu˜noz et al., 2021) used here for comparison was 
tuned by employing a grid search. As a result of the tuning process, λ was 
set at 0.1, k = 360, and the number of iterations to 100. The imputation 
by EM algorithm is carried out using the function em of Python package 
impyute and eps parameter is set 0.1. For MICE, we used the function 
IterativeImputer  of  Python  package  sklearn  (Pedregosa  et  al.,  2011b) 
with  the  recommended  parameters  (BayesianRidge  as  estimator  and  a 
max_iter = 10).

process is carried out for the part of the day.

Temporal  

Hour  

1–720 

t 

vij =

projections of the characteristics on continuous vector spaces (Sharma 
et al., 2020); consequently, they are more computationally efficient to 
training models.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific postprocessing steps involved after the model training. However, it does mention some relevant details. After training the algorithms with the available data, excluding the testing set, the performance metrics were computed in the test set. This suggests that one of the postprocessing steps might involve calculating performance metrics such as accuracy, precision, recall, or F1 score. Additionally, the text mentions that a min-max scaler was used during training and then reverted at the end, which could indicate normalization or scaling of the input features before computing these metrics. Furthermore, since the study compares DMF with other methods like MF, MICE, and imputation by EM algorithm, it can be inferred that another postprocessing step might involve comparing the results obtained from different models through techniques such as confusion matrices or statistical tests. Lastly, although not explicitly stated, visualizing the results using tools like saliency maps or heatmaps could also be considered as potential postprocessing steps to better understand the model's behavior and interpretability.
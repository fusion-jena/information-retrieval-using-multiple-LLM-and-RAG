Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We evaluated model performance with and without additional pre- 
training with external sound data prior to fine-tuning with sound data 
from our study. For the pre-trained models, we used training and vali-
dation XC mel-spectrograms to adjust the weights of each of the three 
CNN models. Pre-training with XC data was implemented for all CNN 
models starting with training the final, dense layer for 10 epochs fol-
lowed by training all layers for 10 additional epochs. 

Fine-tuning using the ROI training and validation mel-spectrograms 
was  then  applied  to  both  the  default  CNN  backbones  (i.e.,  not  pre- 
trained  on  XC)  and  to  the  XC  pre-trained  CNNs.  The fine-tuning  pro-
cess consisted of two stages: (1) model training on the classifier only (i. 
e., dense, fully-connected layer), freezing all other trainable parameters 
in CNN layers for 10 epochs (learning rate = 0.0001), and (2) fine-tuning

ence data available for training were limited, so we sought to explore 
different network sizes in conjunction with pre-training with additional 
acoustic reference data from outside of our audio domain, followed by 
fine-tuning with ROI data from our study site.

LeBien et al., 2020; Ruff et al., 2021; Stowell et al., 2019; Zhong et al., 
2020). Tuning of the network hyperparameters and convolutional filters 
provides automated feature learning, thereby removing an obstacle of 
earlier  machine  learning  methods  (Gupta  et  al.,  2021).  Sound  data 
include frequency, amplitude and temporal information, and there are 
different  approaches  to  express  these  data  in  CNN  architectures.  One 
approach is to represent sound data as amplitude-time waveforms, then 
train  1-dimensional  neural  network  architectures,  such  as  Recurrent 
Neural Networks (RNNs) to identify distinct patterns (Sang et al., 2018; 
Weninger and Schuller, 2011; Zhang et al., 2021). Another approach is 
to  train  CNNs  to  classify  2D  image  spectrograms  (Kahl  et  al.,  2021; 
LeBien et al., 2020; Ruff et al., 2021; Stowell et al., 2019; Zhong et al., 
2020).  A  popular  extension  is  to  use  mel-spectrograms  from  human

architectures of deep convolutional neural networks. Artif. Intell. Rev. 53 (8), 
5455–5516. 

Knight, E.C., S`olymos, P., Scott, C., Bayne, E.M., 2020 Oct. Validation prediction: a 

flexible protocol to increase efficiency of automated acoustic processing for wildlife 
research. Ecol. Appl. 30 (7), e02140 https://doi.org/10.1002/eap.2140. 

Knight, E.C., Poo Hernandez, S., Bayne, E.M., Bulitko, V., Tucker, B.V., 2020 May 3. Pre- 

processing spectrogram parameters improve the accuracy of bioacoustic 
classification using convolutional neural networks. Bioacoustics. 29 (3), 337–355. 
Lasseck, M., 2019. Bird Species Identification in Soundscapes. CLEF (Working Notes, 

p. 2380.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit details about specific preprocessing steps taken before training the deep learning models. However, it mentions that the models were trained on mel-spectrograms derived from sound data. This implies that the raw sound data was transformed into a visual representation (mel-spectrogram) before being fed into the deep learning models. Additionally, the text discusses tuning of network hyperparameters and convolutional filters, which suggests some form of parameter optimization or selection may have been performed during the preprocessing stage. Unfortunately, no further details regarding any specific preprocessing techniques like normalization, scaling, or cleaning are mentioned in the given context.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

and TensorFlow 2.5.0 (https://www.tensorflow.org) libraries, and all

statistical analyses were conducted with R 4.1.0 (R Core Team, 2021).

Between-datasets classification

For the between-dataset prediction, the CNN model was trained with

RE SU LT S

a source dataset to predict images from a different target dataset. The

NN was trained with N images randomly selected from the source

Performance of within-dataset classification

dataset, which was then used to predict all images of the target data-
set and AccT and ΔAcc were measured. We ran the above procedures
for three source–target pairs (training dataset!predicted dataset),
GH!LH, GH!LL and LL!LH. These settings simulate two alternative
scenarios: (i) a global image database is used to predict local samples
(GH!LH and GH!LL) and (ii) conventional

images, as

those

Effects of datasets and the number of images

The accuracy of within-dataset classification and the effect of the

Local low quality (LL) dataset

of 0.6. The neural network was trained with the stochastic gradient

descent algorithm with the softmax cross-entropy loss for 300 epochs.

We used a batch size of 10 and a fixed learning rate of 0.01, and the

convergence of loss was visually assessed. The numbers of units in the

A subset of single specimens (taken from the bulk samples) were indi-

two FC layers (512 and 256 for the first and second FC layers, respec-

vidually photographed using a conventional stereoscope NIKON

tively) and the dropout rate were determined by five-fold cross-

SMZ1270i equipped with a NIKON DS-Fi3 Microscope Camera (5.9

megapixels) controlled by the NIKON DS-L4 v.1.5.0.3 control unit.

These images were denoted Local Low Quality (LL) dataset. These pho-
tographs were intended to represent a more realistic scenario of local

datasets. Conversely to the above analyses, here the model was

tion to the classifier (Figure S1). The training procedure then optimizes

trained to classify datasets instead of taxa. Then, a classification error
of the SVM, εsource(cid:4)target, was measured as a proportion of incorrect
predictions of 200 test images sampled from the two datasets. An

the model parameters in the shared part of the network to not only

minimize the loss of the label classifier (taxon prediction) but at the

same time to maximize the loss of the domain classifier (dataset predic-

intuitive interpretation of this measure is that the dataset classifica-

tion). This adversarial training procedure optimizes shared intermediate

tion task is harder when the feature distributions between two data-

features to be invariant between the two domains, and hence the

sets are more similar. Therefore, a large classification error indicates

model can generalize across them, which potentially improves the accu-

training was done using taxon labels from the source images and data-

training images on prediction accuracy, whereby the CNN model was

trained with N images randomly selected from the dataset and predicted

the class (family label) of n test images randomly selected from the rest.

N ranged between 100 and 700 for LH (with intervals of 100 images),
between 50 and 250 for LL (with intervals of 50 images) and between
100 and 900 for GH (with intervals of 100 images). The number of test
images n was set to 200 for LH and GH, and 50 for LL due to the small
size of the dataset. To evaluate the consistency of prediction accuracy,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't enough information available to provide a detailed response about how the trained deep learning model was deployed. However, based on the given context, we can infer that the model was likely saved or serialized after being trained so it could be loaded later for predictions. This is a common practice in machine learning projects. As for platform selection, no specific details are mentioned regarding what kind of hardware or software environment was chosen for running the model.
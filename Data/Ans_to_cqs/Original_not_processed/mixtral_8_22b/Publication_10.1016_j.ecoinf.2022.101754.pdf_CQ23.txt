Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.6. Training and validation set 

The  study  utilized  a  training-and-test  approach  with  stratification 
splitting. For each setting, 100 different splits to the training-and-test set 
were given. Tuning of the hyperparameter on the training set was done 
for  each  set.  This  was  followed  by  model  training  using  an  optimal 
setting of the hyperparameter. 

2.7. Regression models 

Four methods were used to model forest AGB in the study: AdaBoost, 
random  decision  forest  (RF),  multilayer  neural  network  (NET),  and 
Bayesian ridge regression (Bayes).

(1)  

(cid:0)

)

R2 all

+ score

where. 

_test is descriptor for test samples, 
_all is descriptor for all samples, 
AD  is  average  angle  deviation  from  the  line  1:1  for  test  samples 

[degree] (Eq. 2), 

ACT1 is accuracy with tolerance 15 t/ha ± 10% of AGB [t/ha], 
ACT2 is accuracy with tolerance 10 t/ha ± 5% of AGB [t/ha], 
RMSE is root mean squared error [t/ha] (eq. 3), 
R2 is coefficient of determination (R2; eq. 4), 
Diff_R2 is the difference between R2_test and R2_all, and. 
CV_R2 is cross-validation R2. 

EcologicalInformatics70(2022)1017545O. Brovkina et al.

Xue, B., 2015. Lidar and machine learning estimation of hardwood forest biomass in 
mountainous and bottomland environments. In: Theses and Dissertations, 1274. 
http://scholarworks.uark.edu/etd/1274. 

Wu, C., Shen, H., Shen, A., Deng, J., Gan, M., Zhu, J., Xu, H., Wang, K., 2016. 

Zhang, L., Shao, Z., Liu, J., Cheng, Q., 2019. Deep learning based retrieval of forest 

Comparison of machine-learning methods for above-ground biomass estimation 
based on Landsat imagery. J. Appl. Remote. Sens. 10, 035010 https://doi.org/ 
10.1117/1.JRS.10.035010. 

aboveground biomass from combined LiDAR and Landsat 8 data. Remote Sens. 11 
(12), 1459. 

EcologicalInformatics70(2022)10175412

learning techniques (e.g., random forest [RF], support vector regression 
[SVR], and k-nearest neighbor [kNN]) are capable to reveal complicated 
nonlinear patterns (Fassnacht et al., 2014) and, in addition, to solve data 
dimensional  problems  when  fitting  models  with  a  large  number  of 
predictors (Hudak et al., 2008; Zhang et al., 2019). The variety among 
research  sites,  however,  corresponds  to  equal  variety  among  those 
predictors and machine learning methods used. Therefore, there is no 
agreement to  date  on best  practices  for machine  learning methods  to 
estimate AGB from ALS. In a review, Fassnacht et al. (2014) reported 
that RF performed best and stepwise linear models poorest, as judged by 
coefficient  of  determination  (R2)  and  root  mean  square  error  (RMSE) 
under  cross-validation.  Tompalski  et  al.  (2019)  generated  area-based 
models using ordinary least squares regression (OLS), RF, and kNN to

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To ensure the generalizability of their deep learning model, the authors employed several strategies. Firstly, they adopted a training-and-test approach with stratification splitting. This method involves dividing the dataset into two parts - one for training the model and another for testing its performance. By employing stratification splitting, the authors ensured that both sets contained representative samples from all classes or categories present in the original dataset. Furthermore, this process was repeated 100 times with different splits to minimize bias and increase robustness. Secondly, tuning of the hyperparameters was conducted on the training set for each split. Hyperparameters are parameters whose values are set before the commencement of the learning process and remain constant throughout it. They play a crucial role in determining the overall performance of the model. Lastly, the authors also applied cross-validation during the evaluation phase. Cross-validation is a technique used to assess how well a statistical model will generalize to an independent dataset. It does so by partitioning the original sample into subsets such that the analysis is initially performed on a single subset while the other subset(s) are retained for subsequent use in confirming and validating the initial analysis. These measures collectively contribute towards enhancing the reliability and applicability of the developed deep learning model across diverse datasets.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4.2. Data splitting 

The  WQ indicator  data  was  divided  into  training  and  testing  data 
sets. In ML, typical data splitting ratios for training and testing datasets 
are 80% and 20%, 67% and 33%, and 50% and 50%, respectively (Sajib 
et al., 2023). In this study, we adopted 80% and 20% ratios for splitting 
the training and testing datasets. A total of 20 and 10 random sample 
locations  were  selected  for  the  purposes  of  training  and  testing, 
respectively. A similar number or fewer sample locations were utilized 
in various previous research studies to train and test the data set (Bui 

EcologicalInformatics80(2024)1025145A.M. Sajib et al.                                                                                                                                                                                                                                 

Fig. 2. Methodological framework for selecting the best predictive model.

erquality-iiwq/wq-challenge. 

Vu, H.L., Ng, K.T.W., Richter, A., An, C., 2022. Analysis of input set characteristics and 
variances on k-fold cross validation for a recurrent neural network model on waste 
disposal rate estimation. J. Environ. Manag. 311 (October 2021), 114869 https:// 
doi.org/10.1016/j.jenvman.2022.114869. 

Wainer, J., Cawley, G., 2021. Nested cross-validation when selecting classifiers is 

overzealous for most practical applications. Expert Syst. Appl. 182 (May), 115222 
https://doi.org/10.1016/j.eswa.2021.115222. 

Wang, Y., Zhao, Y., Xu, S., 2022. Application of VNIR and machine learning technologies 
to predict heavy metals in soil and pollution indices in mining areas. J. Soils 
Sediments 22 (10), 2777â€“2791. https://doi.org/10.1007/s11368-022-03263-3. 

Whitehead, P., Bussi, G., Hossain, M.A., Dolk, M., Das, P., Comber, S., Peters, R., 

Charles, K.J., Hope, R., Hossain, S., 2018. Restoring water quality in the polluted

EcologicalInformatics80(2024)10251415A.M. Sajib et al.                                                                                                                                                                                                                                 

Table 4 
Ranking of models based on various statistical measures.  

ML 
models 

score based on prediction performance 
metrics 

score based on 
sensitivity 

score based 
on model 
efficiency 

score based on 
z statistics 

score based on root mean 
square deviation and standard 
deviation 

Cumulative score of 
various measures 

Rank 

training 

testing 

training 

testing 

R2 

NSE  MEF 

Z 

Tylor statistics 

RMSE 

RMSE 

MAE 

MAE 

EN 
GPR 
LR 
NN 
SLR 
SVM 
TR 
XGB 

8 
4 
1 
3 
7 
2 
5 
6 

8 
1 
6 
2 
4 
3 
7 
5 

8 
5 
1 
2 
4 
3 
6 
7 

7 
1 
6 
2 
3 
4 
8 
5 

6 
1 
5 
1 
3 
2 
7 
4 

3 
1 
6 
2 
4 
5 
8 
7 

3 
1 
6 
2 
4 
5 
8 
7 

8 
1 
6 
3 
4 
5 
2 
7 

8 
1 
6 
2 
4 
3 
7 
5

The  development  and  evaluation  of  ML/AI  models  have  become 
central to many domains, ranging from healthcare and finance to natural 
language processing and computer vision (de Hond et al., 2022; Krenn 
et al., 2023). Accurately assessing the performance and generalization 
capability of these models is of paramount importance. To achieve this, 
data  scientists  and  researchers  rely  on  a  plethora  of  validation  tech-
niques,  like  holdout  method,  bootstrapping,  leave-one-out  cross-vali-
dation, nested cross-validation among which cross-validation stands as a 
cornerstone (Tsamardinos et al., 2018; Wainer and Cawley, 2021; Wong, 
2015).  Cross-validation  is  a  systematic  approach  to  model  evaluation 
that has gained widespread popularity due to its robustness and versa-
tility (Bates et al., 2023; Nunez et al., 2021). It is particularly valuable 
when dealing with limited datasets, where overfitting and underfitting

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the Water Quality (WQ) indicator data was split into training and testing datasets using an 80% and 20% ratio, respectively. This is a common practice in Machine Learning (ML) to prevent overfitting and improve the model's ability to perform well on unseen data. Secondly, a total of 20 and 10 random sample locations were selected for the purposes of training and testing, respectively. This helps to ensure that the model can handle diverse scenarios and conditions. Lastly, the development and evaluation of the ML/AI models relied on a plethora of validation techniques, including cross-validation. Cross-validation is a systematic approach to model evaluation that is particularly valuable when dealing with limited datasets, helping to avoid overfitting and underfitting. These measures contribute to ensuring the generalizability of the deep learning model.
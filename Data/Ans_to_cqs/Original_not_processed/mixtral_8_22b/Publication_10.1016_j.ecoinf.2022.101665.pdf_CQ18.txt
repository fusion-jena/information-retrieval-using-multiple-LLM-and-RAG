Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

prediction and training points.

mapping probability samples that are exclusively used for map evalua-
tion are often not available and therefore alternative methods have been 
proposed.  In  machine  learning,  if  data  are  abundant,  a  common 
approach is to randomly divide the full dataset used for modelling into 
three parts: a training set, a validation set, and a test set (Hastie et al., 
2009,  Chapter  7).  The  training  set  is  used  for  fitting  the  models,  the 
validation set is used to estimate prediction error for model selection and 
hyperparameter tuning, while the test set is used for assessing the ac-
curacy of the final model. This paper addresses this latter testing phase, 
with the specific aim to assess the accuracy of a thematic map produced 
by a calibrated statistical prediction method. Data availability is often 
limited  so that setting  aside a  test set  cannot always be afforded and 
therefore resampling methods are used (Hastie et al., 2009; Steele et al.,

Each of the sampling designs 1–5 was repeated 100 times, every time 
producing a different sample. The size of samples 1, 3, 4 and 5 corre-
sponded to 5000 each. The systematic random samples’ size (2) varied 
between 4998 and 5056; upon trying several grid spacings. This range 
was the closest we got to the intended sample size of 5000. 

2.3. Map accuracy metrics 

The map accuracy metrics used in this work are the square root of the 
mean  squared  prediction  error  (RMSE)  and  the  Nash  and  Sutcliffe 
(1970)  model  efficiency  coefficient  (MEC),  which  quantifies  the 
improvement made by the model (in this case the map) over using the 
mean of the observations as the prediction. The metrics are defined in 
Eqs. (5) and (6): 

validation method applies case weights to the residuals at different lo-
cations.  For  the  random,  the  blocked  spatial  and  the  weighted  cross- 
validation methods, estimates of the map accuracy metrics were hence 
computed by Eqs. 7 and 8:

2003). To this end, the widely used k-fold cross-validation method splits 
the  full  dataset  into  k  approximately  equally-sized  disjoint  subsets  or 
folds, where repeatedly (i.e. k times) the model is calibrated on k-1 folds, 
whilst the remaining fold is used for assessing prediction accuracy. The 
overall  cross-validation  accuracy  is  estimated  by  aggregating  the 
(squared)  residuals  over  the  k  folds.  In  conventional  k-fold  cross- 
validation, the folds are chosen randomly.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In machine learning, when data is abundant, a common approach is to randomly divide the full dataset used for modeling into three parts: a training set, a validation set, and a test set. The training set is used for fitting the models, the validation set is used to estimate prediction error for model selection and hyperparameter tuning, while the test set is used for assessing the accuracy of the final model. This process is known as partitioning the dataset.

During the training phase, the model's performance is monitored using the validation set. This is done by estimating the prediction error, which helps in selecting the best model and fine-tuning its parameters. The validation set provides an unbiased evaluation of the model's performance because it has not been seen by the model during training.

However, if data availability is limited, setting aside a separate test set might not be feasible. In such cases, resampling methods like k-fold cross-validation are used. In k-fold cross-validation, the full dataset is split into 'k' approximately equal-sized disjoint subsets or folds. The model is then trained on 'k-1' folds, while the remaining fold is used for assessing prediction accuracy. This process is repeated 'k' times, with each fold serving as the validation set once. The overall cross-validation accuracy is estimated by aggregating the residuals over the 'k' folds.
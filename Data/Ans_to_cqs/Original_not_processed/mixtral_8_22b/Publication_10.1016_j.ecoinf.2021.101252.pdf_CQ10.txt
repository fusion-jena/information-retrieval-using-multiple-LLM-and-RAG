Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

0.91 

Architecture 

Case study 2 

ResNet 
InceptionTime 
LSTM 
CNN 
ResNet 
LSTM 
InceptionTime 
CNN 
CNN 
ResNet 
InceptionTime 
LSTM 
ResNet 
CNN 
InceptionTime 
LSTM 
CNN 
InceptionTime 
LSTM 
ResNet 

Accuracy of 
candidate 
models (% 
correct) 

AUC of 
selected 
model 

Architecture 

Accuracy of 
candidate 
models (% 
correct) 

0.67  
0.63  
0.52  
0.82 
0.61  
0.52  
0.67  
0.52  
0.48  
0.67  
0.66  
0.52  
0.66  
0.77  
0.67  
0.52  
0.52  
0.75  
0.52  
0.7  

0.95 

Case study 3 

ResNet 
InceptionTime 
LSTM 
CNN 
ResNet 
LSTM 
InceptionTime 
CNN 
CNN 
ResNet 
InceptionTime 
LSTM 
ResNet 
CNN 
InceptionTime 
LSTM 
CNN 
InceptionTime 
LSTM 
ResNet 

0.5  
0.81 
0.8  
0.69  
0.5  
0.81  
0.76  
0.51  
0.5  
0.49  
0.5  
0.75  
0.5  
0.66  
0.5  
0.61  
0.5  
0.51  
0.77  
0.5

hyperparameters  (i.e.,  ‘AutoML’;  He  et  al.,  2021).  This  represents  an 
important  advantage  for  non-experts  in  deep  learning,  as  it  does  not 
require  the  manual  assembly  of  the  models  and  definition  of  their 
hyperparameters. The AutoML procedure starts by generating a set of 
candidate models with architectures and hyperparameters (e.g. number 
of layers; learning rate) selected at random from a prespecified range of 
values (see Fig. 2). Each candidate model is trained using a small subset 
of the data (data partition At; Fig. 2) during a small number of epochs. 
After  training,  the  performance  of  the  candidate  models  is  compared 
using a left-out validation data set (Av; Fig. 2). The selected candidate 
model (usually the best performing among candidates) is then trained on 
the full training data (Bt; Fig. 2). In this step it is required to identify an 
optimal number of training epochs, to avoid under- or overfitting of the

Reichstein, M., Camps-Valls, G., Stevens, B., Jung, M., Denzler, J., Carvalhais, N., 
Prabhat, 2019. Deep learning and process understanding for data-driven earth 
system science. Nature 566, 195–204. https://doi.org/10.1038/s41586-019-0912-1. 

Reside, A.E., VanDerWal, J.J., Kutt, A.S., Perkins, G.C., 2010. Weather, not climate, 

defines distributions of vagile bird species. PLoS One 5, e13569. https://doi.org/ 
10.1371/journal.pone.0013569. 

Ryo, M., Aguilar-Trigueros, C.A., Pinek, L., Muller, L.A.H., Rillig, M.C., 2019. Basic 

principles of temporal dynamics. Trends Ecol. Evol. 34, 723–733. https://doi.org/ 
10.1016/j.tree.2019.03.007. 

Schmidhuber, J., 2015. Deep learning in neural networks: an overview. Neural Netw. 61, 

85–117. https://doi.org/10.1016/j.neunet.2014.09.003.

Bencatel, J., 

Universidade de ´Evora, ´Evora.  

Bengio, Y., Simard, P., Frasconi, P., 1994. Learning long-term dependencies with 

gradient descent is difficult. IEEE Trans. Neural Netw. 5, 157–166. https://doi.org/ 
10.1109/72.279181. 

Booth, T.H., Nix, H.A., Busby, J.R., Hutchinson, M.F., 2014. bioclim: the first species 

distribution modelling package, its early applications and relevance to most current 
MaxEnt studies. Divers. Distrib. 20, 1–9. https://doi.org/10.1111/ddi.12144. 
Brodrick, P.G., Davies, A.B., Asner, G.P., 2019. Uncovering ecological patterns with 
convolutional neural networks. Trends Ecol. Evol. 34, 734–745. https://doi.org/ 
10.1016/j.tree.2019.03.006. 

Bush, A., Sollmann, R., Wilting, A., Bohmann, K., Cole, B., Balzter, H., Martius, C.,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given information, there is no direct mention of a specific code repository link for the deep learning pipeline used in these case studies. It would be necessary to have more detailed information about the source of these case studies or any associated publications to provide a precise answer. However, considering the common practice in sharing research materials, especially in machine learning and deep learning fields, it might be worth checking popular repositories like GitHub, GitLab, or BitBucket for projects related to the mentioned authors, keywords, or methodologies. For instance, searching for 'Deep learning', 'ResNet', 'InceptionTime', 'LSTM', 'CNN', or author names such as 'M. Reichstein', 'G. Camps-Valls', etc., could potentially lead to relevant repositories containing the desired deep learning pipelines.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

respectively.  N  represented  the  number  of  images  in  the  test  set  or 
validation set. 

2.2.6. Model training and hyper-parameters 

We  trained  models  on  the  training  sets  and  tracked  the  model 
training process on the validation set. The training process was stopped 
when  the  loss  value  of  the  model  on  the  validation  set  no  longer 
decreased. We then tested the model performance on the test set. Our 
computing platform was a Dell PowerEdge C4130 rack server with two 
Tesla  K80  GPUs  and  256  GB  of  memory.  For  the  deep  learning  envi-
ronment,  we  utilized  the  PyTorch  1.7.1  platform.  We  employed  the 
Stochastic Gradient Descent (SGD) optimizer with a momentum value of 
μ  = 0.9 to train the model. Other hyper-parameter settings for model 
training were shown in Table S.4 of the Supporting Information. 

3. Results 

3.1. Experiment results of transfer strategy optimization

Experimental results on test sets showed that when the DCNN model 
was transferred from the ImageNet dataset to the SS or LSM dataset, the 
accuracy of the transfer learning model was always better than that of 
the model trained from scratch (Figs. 5–6). This was observed no matter 
which transfer strategy was employed. When the model was transferred 
from ImageNet to the LSM dataset (Fig. 5), TLS_4 was found to be the top 
performer among the five transfer strategies, while TLS_1 ranked lowest. 
TLS_4  updated the  Conv5 and  the FC layers  of the  ResNext-101, con-
trasting with TLS_1, which updated all layers except the Conv1. Notably, 
the  widely  used  strategy  of  updating  solely  the  fully  connected  layer 
(TLS_5)  achieved  only  the  fourth  rank.  Across  different  sample  sizes, 
TLS_4 achieved accuracies of 92.4%, 92.6%, 93.75%, 94.0%, and 94.3%, 
respectively. Conversely, TLS_5 displayed accuracies of 89.5%, 89.9%,

and  93.7%  on target domain  datasets, respectively. This  performance 
was  significantly higher  than that of the  transfer  model based on the 
source  model  constructed  with  100,000  training  samples,  which  ach-
ieved accuracies of 88.5%, 89.8%, 91.1%, 91.5%, and 92.4%, respec-
tively (Fig. 10).

Fig. 5. The accuracies of transfer learning models on the test set and the corresponding binomial test probability value when models were transferred from the 
ImageNet dataset to the LSM dataset. (a) Accuracies of models under different transfer strategies. The abscissa represented the training set sample size (in thousands 
of images). (b) The binomial test probability values between the best-performing transfer strategy and other transfer strategies. The TFS meant that the model was 
trained from scratch. The TLS_i (i = 1, 2, …, 5) represented that different layers of the ResNext-101 model were selected for updating. A probability value <0.05 
indicated a significant difference. 

EcologicalInformatics80(2024)1025275D.-Q. Yang et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors evaluated the performance of their deep learning model using the metric of accuracy. They compared the accuracy of the transfer learning model to that of the model trained from scratch. Specifically, they reported the accuracy of the transfer learning model on the test set after it had been trained on the training set and validated on the validation set. They also conducted a binomial test to determine if there was a significant difference between the best-performing transfer strategy and other transfer strategies. However, the authors did not report any other evaluation metrics such as precision or recall.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We used ResNet 100 (He et al., 2016) as our classic deep-learning 
algorithm.  Resnet  is  a  convolutional  neural  network  (CNN),  a  DL  ar-
chitecture which is able to both extract features from images and classify 
these images thanks to those features (Lecun et al., 2015). In order for a 
CNN  to  build  an  image  classification  model,  the  architecture  is  fed  a 
large dataset, composed of pairs of labels and images. Using this dataset, 
the algorithms change their inner parameters in order to minimize the 
classification  error,  through  a  process  called  back-propagation.  The 
ResNet architecture achieved the best results on ImageNet Large Scale 
Visual Recognition Competition (ILSVRC (Russakovsky et al., 2015)) in 
2015, considered the most challenging image classification competition. 
It is still one of the best classification algorithms, while being easy to use 
and implement.

2)  adapt  this  model  to  a  new  task  with  few  examples,  while  not 
forgetting  the  concepts  learned  previously  (Gidaris  et  al.,  2018;  Har-
iharan et al., 2017). Finally, optimization-based methods are designed to 
adapt quickly to new tasks, hence able to learn a classification task with 
few examples (Finn et al., 2017; Nichol and Schulman, 2018; Sun and 
Chua, 2018). Optimization-based algorithms showed promising results 
in  deep  learning  few-shot  classification  (Finn  et  al.,  2017;  Jamal  and 
Cloud, 2019; Wang et al., 2019). Such methods propose to pre-train (or 
“meta-train”)  a  model  with  existing  databases  (e.g.  MiniImageNet 
(Russakovsky et al., 2015), Ominglot (Lake et al., 2019)) on different 
tasks so it can adapt easily to a new one. For object identification, a task 
is defined by the classes the model has to discriminate. Once this model, 
called “meta-model” has been trained, it can then be tuned to operate on

Comput. Vis. 211–252. 

Salman, A., et al., 2016. Oceanography : Methods Fish Species Classification in 

Unconstrained Underwater Environments Based on Deep Learning, pp. 570–585. 

Shiu, Y., et al., 2020. Deep Neural Networks For Automated Detection of Marine 

Mammal Species, pp. 1–12. 

Stuart-smith, R.D., et al., 2013. Integrating abundance and functional traits reveals new 

global hotspots of fish diversity. Nature 501 (7468), 539–542. 

Sun, Q., Chua, Y.L.T., 2018. Meta-transfer learning for few-shot learning. Conf. Comput. 

Vis. Pattern Recognit. 403–412.

In this study we propose to compare the efficiency of optimization- 
based  few-shot  learning  and  standard  large  dataset  deep-learning 
methods  to  identify  coral  reef  fish  species  on  images.  More  specif-
ically, we aim to determine how well a classic deep learning architecture 
trained with thousands of images and the benefit of data augmentation 
(hereafter DL) and FSL algorithms perform in situations where training 
thumbnail dataset is large or limited. To achieve this, we first trained a 
classic DL architecture built for image classification (He et al., 2016) on 
a large dataset of 69,169 thumbnails, and on a more limited dataset of 
6320 thumbnails for 20 coral reef fish species. Then, we trained a few- 
shots,  optimization-based  learning  algorithm  (Nichol  and  Schulman, 
2018) on the exact same training datasets while varying the number of 
shots from 1 to 30. Finally, we compared the capacity of DL and FSL

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) used within the deep learning pipeline. However, it mentions that CNN architectures like ResNet utilize a process called back-propagation to adjust internal parameters and minimize classification errors. This implies that some form of gradient descent optimization technique was employed during the training phase. Additionally, the reference to optimization-based methods suggests that these methods might involve using optimization techniques to fine-tune models for better performance on new tasks.
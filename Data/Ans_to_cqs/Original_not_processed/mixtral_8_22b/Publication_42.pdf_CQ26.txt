Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Sensors 2021, 21, 343

11 of 18

7,047,754 learnable parameters gave a F1-score of 84.93% which is even lower. CNN
architectures with many parameters (more than 20,000,000) such as ResNetV50 [38] and
InceptionNetV3 [39] gave a high training accuracy, but a lower validation F1-score of 69.1%
and 81.7%, respectively. This result indicates overﬁtting and that more training data are
needed when such large deep learning networks are used. A very high F1-score of 96.6%
was ﬁnally achieved by transfer learning on ResNetV50 using pretrained weights and only
training the output layers. This indicates that the state-of-the-art was able to outperform
our proposed model, but requires pretrained weights with many more parameters.

2.2.4. Summary Statistics

36. Tan, M.; Le, Q.V. EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks. arXiv 2019, arXiv:1905.11946,
37. Huang, G.; Liu, Z.; Weinberger, K.Q. Densely Connected Convolutional Networks. arXiv 2016, arXiv:1608.06993,
38. Wu, S.; Zhong, S.; Liu, Y. ResNet. Multimed. Tools Appl. 2017. [CrossRef]
39.

Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; Wojna, Z. Rethinking the Inception Architecture for Computer Vision.
In
Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 26 June–1
July 2016; pp. 2818–2826.

40. R Core Team. R: A Language and Environment for Statistical Computing; R Core Team: Geneva, Switzerland, 2020. Available online:

http://softlibre.unizar.es/manuales/aplicaciones/r/fullrefman.pdf (accessed on 9 November 2020).

The chosen model shown in Figure 5 had an F1-score of 92.75%, which indicated that
the trained CNN was very accurate in its predictions. This ﬁnal architecture was chosen
because it achieved average precision, recall, and an F1-score of 93%, which indicated a
suitable model classiﬁcation.

Sensors 2021, 21, 343

10 of 18

Table 2. Ranking of the CNN architectures with highest and lowest F1 classiﬁcation scores. Rank 1 to
32 were trained using the Adam optimizer. Rank 33 to 64 were trained using the SGD optimizer. The
hyperparameters column shows values of {kernel size layer 1, kernel size last layer, convolutional
depth layer 1, convolutional depth last layer, fully connected size}.

Rating

Hyperparameters

Learnable
Parameters

F1/-Score

1.
2.
3.
4.
5.
...
31.
32.

33.
34.
...
62.
63.
64.

3, 3, 32, 128, 512
5, 1, 32, 128, 512
5, 3, 32, 64, 512
3, 3, 32, 64, 512
5, 3, 32, 128, 512

5, 1, 64, 64, 512
5, 3, 32, 32, 512

5, 3, 64, 128, 512
5, 3, 32, 128, 512

To ﬁnd the best CNN architecture for species classiﬁcation, different hyperparameters
were adjusted as described in Section 2.2.3. A total of 64 architectures were trained using a
dropout probability of 0.3 after the second to last hidden layer. The average F1-score for all
classes was used as a measure for a given architecture’s performance.

The ﬁve best architectures had high F1-scores, which only varied by 0.02, but had a
varying number of learnable parameters (Table 2). Compared to SGD, Adam turned out to
be the superior optimizer for training of all models. In the end, the architecture that had
a rating among the three highest F1-score but the lowest amount of learnable parameters
(2,197,578) was chosen. The reason for this is that an architecture with many parameters
and few training data would increase the risk of overﬁtting the neural network.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The paper does not provide explicit information about addressing data bias during preprocessing of the deep learning pipeline. However, some general techniques can be mentioned based on common practices in machine learning and deep learning pipelines. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into training and testing sets while maintaining the same distribution of classes or labels in both subsets. This helps prevent any imbalance between classes from affecting the model's performance.

Oversampling and undersampling are techniques used to handle class imbalances in datasets. Oversampling increases the representation of minority classes by duplicating samples, while undersampling reduces the majority class by removing samples. Both methods aim to create a balanced dataset for better model performance.

Diverse data collection focuses on gathering a wide range of examples representing various scenarios, conditions, and variations within each class. This ensures that the model has enough exposure to different patterns and features, reducing potential biases caused by limited or skewed data.

While these techniques are commonly employed to mitigate data bias, their application depends on the specific characteristics and requirements of the dataset and problem at hand.
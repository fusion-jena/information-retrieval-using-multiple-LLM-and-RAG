Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

samples (red line) compared to randomly ordered samples, no active
learning (blue line), and active learning using high-certainty samples
(yellow line). The baseline accuracy is calculated using a 2-shot, 2-way
test accuracy without fine-tuning (81.4%). The AST feature extraction
model is fine-tuned on a subset of 500 training samples. Model perfor-
mance is re-evaluated on the test set at 10 sample increments using each
of the three sample orderings. Low certainty refers to queries with a
similar Euclidean distance to prototypical embeddings and therefore lie
along decision boundaries. For high-certainty samples, the order is
reversed with query embedding in close proximity to prototypical em-
beddings being prioritised for model fine-tuning.

We compare three classification models - ResNet-50 (He et al., 2016),
Audio Spectrogram Transformer (AST) (Gong et al., 2021) and HuBERT
(Hsu et al., 2021). The classification models compared in Section 4.1
were trained on a dataset containing 3000 5 s segments automatically
generated by the annotator. The dataset includes 1130 possum vocal-
isations and 1870 noise sources. The dataset has an 80/20% training-
validation split which is used to evaluate model performance during
training. Model hyperparameters are implemented based on the
recommendation outlined by the model authors. Each segment has been
manually reviewed by a human. 500 segments are separated into a test
set containing 187 possum vocalisations and 313 noise samples. The
distribution of target features to non-target features is representative of
the true distribution post-segmentation. Both validation and test dataset
performance are reported. Using the test dataset we evaluate the model

query set are negative. Liu et al. (Liu et al., 2019) propose transductive
propagation that iteratively propagates labels from labelled instances to
unlabelled instances according to the graph structure of the embeddings.
Other semi-supervised learning techniques have been applied such as
active learning. Wang et al. (Wang et al., 2022) combines a few-shot

Future work will improve the usability of this method. Due to the
pretrained AST model being designed for human-perceivable sounds,
data must currently be downsampled to 16 kHz. This is a limitation
when analysing higher-frequency vocalisations and a common limita-
tion when using pretrained models developed for human-perceivable
hearing ranges. Future work will investigate techniques to extend this

Fig. 9. Demonstrating training cycle (first epoch) using active learning pri-
oritising low-certainty samples (red), active learning using high-certainty
samples (yellow) and randomly presented sample i.e. no active learning
(blue). (For interpretation of the references to colour in this figure legend, the
reader is referred to the web version of this article.)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the use of specific regularization methods like dropout or L2 regularization to prevent overfitting in the deep learning pipeline. However, it mentions the use of various strategies to improve model performance and generalizability. These include data augmentation, transfer learning, and active learning. Data augmentation can help reduce overfitting by increasing the diversity of the training data. Transfer learning leverages knowledge gained from solving one problem to solve another related problem, often reducing the need for large amounts of labeled data. Active learning involves selecting the most informative samples for labeling, which can lead to better model performance with fewer labeled examples. While these techniques do not directly address overfitting through regularization methods, they contribute to improving model robustness and preventing overfitting indirectly.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

[23]C.Liu,H.Li,A.Su,S.Chen,andW.Li,“Identiﬁcationandgradingofmaizedroughtonrgbimagesofuavbasedonimprovedu-net,”IEEEGeoscienceandRemoteSensingLetters,pp.1–5,2020.[24]T.Kattenborn,J.Eichel,andF.Fassnacht,“Convolutionalneuralnetworksenableefﬁcient,accurateandﬁne-grainedsegmentationofplantspeciesandcommunitiesfromhigh-resolutionuavimagery,”ScientiﬁcReports,vol.10,p.17656,112019.[25]N.LibaandJ.Berg-J¨urgens,“AccuracyofOrthomosaicGeneratedbyDifferentMethodsinExampleofUAVPlatformMUSTQ,”IOPConferenceSeries:MaterialsScienceandEngineering,vol.96,no.1,p.012041,nov2015.[Online].Available:https://iopscience.iop.org/article/10.1088/1757-899X/96/1/012041[26]R.Takahashi,T.Matsubara,andK.Uehara,“Ricap:Randomimagecroppingandpatchingdataaugmentationfordeepcnns,”inAsianConferenceonMachineLearning.PMLR,2018,pp.786–798.[27]D.P.KingmaandJ.Ba,“Adam:Amethodforstochasticoptimization,”arXivpreprintarXiv:1412.6980,2014.[28]F.P.DosSantos,C.Zor,J.Kittler,andM.A.Ponti,“Learningimagefeatureswithfewerlabelsusingasemi

ti,“Learningimagefeatureswithfewerlabelsusingasemi-superviseddeepconvolutionalnetwork,”NeuralNetworks,vol.132,pp.131–143,2020.[29]I.Ragnemalm,“Theeuclideandistancetransforminarbitrarydimensions,”PatternRecognitionLetters,vol.14,no.11,pp.883–888,1993.

edtoimproveresultsquickly.Afterbeingtestedwithmultipleconﬁgurations,dataaugmentationprovedtobeanefﬁcientwaytoincreasetheF1score.ForimagescollectedbyUAVﬂightsataconstantheightaboveground,smallchangesinbrightnessandzoomcanhelptoimprovesigniﬁcantly,butthechangingrangeinheightandwidthcanturnallvegetationtooclosevisuallyandcreateconfusionforCNN.Mediumorlargechangesinzoomrangealsocausedegradationofresults.Identifyingspeciﬁcvegetationmixedwithnativevegeta-tionfromUAVﬂightshassomechallengesandoneoftheAuthorized

morerecentlyandsuccessfullyforvegetationsegmentation.Accordingtotheauthors,theU-Netneuralnetworkshouldconvergewitharelativelysmallamountofdata[21].TheU-networkwastested[24]inasegmentationap-proachcombinedwithtrainingdirectlyderivedfromthevisualinterpretationofhigh-resolutionUAV-basedRGBimagesfordetailedmappingofvegetationspecies.Theauthorsshowthatthisapproachsegmentsandaccuratelymapsvegetationspeciesandcommunities(atleast84%accuracy).Furthermore,theauthorsonlyuseRGBimagessuggeststhatspatialpatternsratherthanspectralinformationfacilitateplantidentiﬁcationatveryhighspatialresolutions.Consequently,thepresentedapproachiscompatiblewithlow-costUAVsystems,easytooperate,andapplicabletomanyusers.AmethodtosegmentdroughtareasanddistinguishmaizeseverityclassiﬁcationusingRGBimagescapturedbyaUAVwasproposed[23].ThroughexpertevaluationandvisualinterpretationwiththeaidofNDVIimages,theycreatetheGroundTruth.ThenetworkusedwasU-NETwithsomestructuralandparameterimprovementsusingSE-ResNet-50.WhenusingRGBima

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, one specific optimization technique is mentioned for use in a deep learning pipeline. The authors [27] discuss the use of Adam, which stands for Adaptive Moment Estimation. It is a method for stochastic optimization and can be used instead of classical stochastic gradient descent procedure to update network weights iterative based on training data. Adam computes adaptive learning rates for different parameters from estimates of first and second moments of the gradients. This allows it to combine the advantages of two other extensions of stochastic gradient descent: AdaGrad, which maintains a per-parameter learning rate, and RMSProp, which divides the learning rate by an exponentially decaying average of squared gradients. Adam has been shown to work well in practice and is easy to implement.
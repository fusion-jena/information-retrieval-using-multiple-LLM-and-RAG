Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Suitable optimisers, loss functions and activation functions had to be 
selected from an array of available options. Series of experimentation 
were  carried  out  on  popular  optimisation  functions  such  as  SGD, 
RMSProp,  LAMB,  LARS  and  Adam  and  regression  loss  functions  like 

EcologicalInformatics69(2022)10160911T.D. Akinosho et al.                                                                                                                                                                                                                            

Fig. 10. Auto-SQL generation to pre-process the dataset. An SQL command which generates 3-hour and 6-hour pollutant averages from the preceding readings 
is depicted. 

Fig.  11. The  model's  training  loss  against  the  learning  rate  to  determine  the 
appropriate learning rate. The learning rate was fixed at the point where the 
(cid:0)
plot started dipping (i.e., 10

4).

Lewisham,  Southwark,  and  Chepstow  were  used  as  case  studies  to  develop  a  REVIS  system  and  validate  the 
proposed framework. REVIS examined the framework's ability to capture granular pollution data, scale up its 
storage facility to rapid data growth and translate high-level user queries to structured query language (SQL) 
required  for  exploratory  data  analysis.  Finally,  the  framework's  suitability  for  predictive  analytics  was  tested 
using fastai's library for tabular data, and automated hyperparameter tuning was implemented using bayesian 
optimisation. The results of our experiments demonstrate the suitability of the proposed framework in building 
end-to-end systems for extensive monitoring and forecasting of pollutant concentration levels on highways. The 
study serves as a background for future related research looking to improve the overall performance of roadside 
and highway air quality forecasting models.

city_name 
lat 
lon 
date 
rain_desc 
rain_1h 
rain 3 h 
snow_1h 
snow_3h 
drizzle desc 
fog desc 
clouds desc 
haze desc 
mist_desc 
clear_desc 
snow_desc 
storm_desc 
temp 
temp_min 
temp_max 
feels_like 
pressure 
Humidity 
Wind_speed 
Wind_direction 
Clouds_all 
Ozone 
Ozone_avg6h 
NO2 
NO2_avg6h 
PM10 
PM10_avg6h 
PM2.5 
PM2.5_avg6h

up its resources according to the computation/storage requirements. For 
the  database  performance,  the  test  was  run  between  November  2020 
and  Jan  2021  on  the  GPU  instance  and  evaluated  for  utilisation, 
execution count, number of running statements and number of sessions 
metrics as shown in Fig. 15. The maximum GPU utilisation was under 
20% even with over 1.5 million execution queries.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The study uses Bayesian Optimization for automated hyperparameter tuning of the model. This method is more efficient compared to traditional methods like Grid Search or Random Search because it uses previous evaluation results to inform the next set of parameters to evaluate. It does this by constructing a posterior distribution of functions (Gaussian Processes), which is then used to select the most promising set of hyperparameters to evaluate next. This iterative process continues until the best combination of hyperparameters is found. In this way, Bayesian Optimization can find better solutions faster and with fewer evaluations than other methods.
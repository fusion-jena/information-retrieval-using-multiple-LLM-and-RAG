Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Control() function helped to evaluate these tuning parameters for model 
performance. Table S3 shows the parameterization settings (i.e. model 
type,  number  of  tuning  parameters/iterations,  tuning  methods  and 
description) for each of the four ML algorithms.

and  Kappas,  2017).  The  kNN  model  is  a  non-parametric  model  that 
performs  LULC  classification  based  on  the  distance  between  k  closest 
samples drawn from training datasets. The approach depends on thor-
ough image (predictor) pre-processing so as to reduce sampling bias and 
ensure equal treatment of predictors when computing distance (Kuhn 
and Johnson, 2016). The SVM model uses support vectors (i.e. based on 
a subset of training data points closest to decision boundaries) to locate 
optimal decision boundaries that separate two LULC classes (Cortes and 
Vapnik, 1995; Kuhn and Johnson, 2016). The ANN model is a mathe-
matical model developed as an analogy of the human brain. Using an 
interconnected  group  of  responsive  and  conducting  nodes,  the  ANN 
model  mimics,  in  a  very  simplified  fashion,  the  functionality  of  the 
human brain for knowledge acquisition, recall, synthesis and problem

2.3.2.3. ANN: artificial neural networks classification. In classifying with 
the MLP ANN model, we used the package “nnet”, which provides pos-
sibilities for adjusting weighted decay and size, thereby countering the 
effects of model overfitting. We used an MLP ANN architecture with 1 
hidden layer established as a default setting within the “nnet” package, 
and with 6 neurons defined for our model inputs. The number of neurons 
in the input layer was equal to the number of used bands (6), and the 
output  layer  had  8  neurons  (representing  8  LULC  classes).  A  back 
propagation learning algorithm was used during the training phase of 
the model. Size and decay were used to define the primary model tuning 
parameters, and the control () function was used to control for model 
runs. As with the kNN approach, we defined the LULC classes of the test 
datasets  as  target  variables  and  the  band  reflectance  values  as  pre-

solving  (Kubat,  1999;  Yang,  2009).  In  LULC  classification,  the  Multi- 
Layer Perceptron (MLP) type of ANN has been used most often (Silva 
et al., 2020). MLP carries out backpropagation of training samples to 
accurately classify LULC. The RF was developed as an ensemble of ML 
models that use bootstrap techniques to build many single decision tree 
models (Breiman, 2001; Mellor et al., 2013; Rodriguez-Galiano et al., 
2012). The RF model uses subsets of predictor variables (e.g. Landsat 
bands) to split observation datasets into subsets of homogenous samples 
to build each decision tree (Mellor et al., 2013).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific hyperparameters used in the deep learning model, such as learning rate or optimizer. However, it mentions that the ANN model used the 'nnet' package, which allows for adjusting weighted decay and size to prevent overfitting. These could be considered as hyperparameters in this context. Additionally, the number of hidden layers and neurons in the input and output layers are also mentioned as part of the model configuration. For the ANN model, the primary tuning parameters were size and decay, controlled using the Control() function. Unfortunately, no further details regarding other potential hyperparameters are given in the provided context.
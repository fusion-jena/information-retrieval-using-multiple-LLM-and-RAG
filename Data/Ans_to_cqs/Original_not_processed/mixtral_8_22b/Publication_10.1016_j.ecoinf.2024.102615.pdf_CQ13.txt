Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics81(2024)1026153M. Zhou et al.                                                                                                                                                                                                                                   

Fig. 3. The performance optimization of the MLP model.  

Table 1 
Hyperparameters tuning ranges.  

Hyperparameters 

Type 

Tuning range 

The number of neural network layers 
The number of neurons per layer 
Dropout 
Learning rate 
Batch size 

Integer 
Integer 
Float 
Float 
Integer 

1–5 
64, 128, 256, 512, 1024, 2048 
0.1, 0.25, 0.35, 0.5 
0.001, 0.0001, 0.00001 
32, 64, 128, 256  

Table 2 
the optimal MLP architecture and parameters.  

Layer type 

Input 
Dense 
Batch normalization 
Dropout 
Dense 
Batch normalization 
Dropout 
Dense(output) 
Total parameters 

Layer size 

4200 
2048 
2048 
2048 
1024 
1024 
1024 
1 
10,715,137  

Parameters 

0 
8,603,648 
8192 
0 
2,098,176 
4096 
0 
1025

The performance of a neural network is traditionally improved by 
optimizing  its  hyperparameters,  which  influence  the  overall  model 
performance.  Using  different  hyperparameter  combinations  (e.g., 
combining  different  learning  rates  and  dropout  values)  will  lead  to 
marked  differences  in  the  predictive  performance  of  a  model.  As  a 
traditional  hyperparameter  optimization  method,  the  grid  search 
approach is widely used to optimize a model by comparing the values of 
evaluation  metrics  for  different  parameters  and  selecting  the  optimal 
parameters from a large search space (Bhagat et al., 2020). This method 
can optimize the model by comparing the values of evaluation metrics 
for different parameters and selecting the optimal parameters from the 
search space. In this study, the grid search method was used to optimize 
five of the MLP model’s hyperparameters. The hyperparameter adjust-

ment ranges were determined based on a review of existing literature 
(Chien, 1994; Peng et al., 2022; Shen et al., 2022; Srivastava et al., 2014; 
Wilson and Martinez, 2001; Zhang et al., 2019). The following hyper-
parameters  were  selected  to  optimize  the  MLP  network  architecture: 
number  of  neural  network  layers,  number  of  neurons  per  layer,  per-
centage values of the dropout layers, learning rate, and batch size. The 
specific tuning ranges of these hyperparameters are shown in Table 1. 
Each  set  of  parameters  in  Table  2  was  trained  based  on  a  set  of 
training data (80%) and verified using validation data (10%). The re-
sults of each group of parameters represent the average of ten experi-
ments (Tsimpouris et al., 2021), and the data were shuffled each time. 
Because the calculations are binary, the number of neuron nodes in the 
network architecture was set to the nth  power of 2 (Peng et al., 2022). In

2.3. Model optimization 

The model performance optimization process is broadly divided into 
two stages, as shown in Fig. 3. The R value of the validation set is used as 
the performance indicator during the optimization process, whereby R is 
maximized  using  the  grid  search  method.  During  the  first  stage,  the 
neural network structure and the number of neurons in the hidden layers 
are  adjusted.  In  total,  60  sets  of  combinations,  including  one  to  five 
hidden layers and neurons with a range of 64–2048 (nth  power of 2), 
were considered. The second stage involves adjusting the percentage of 
the dropout layers, the learning rate, and the batch size, to achieve a 
total of 192 combinations (4*4*3*4).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The hyperparameters used in the deep learning model include the number of neural network layers, the number of neurons per layer, dropout percentages, learning rate, and batch size. These hyperparameters have specific tuning ranges, such as 1-5 for the number of neural network layers, 64, 128, 256, 512, 1024, or 2048 for the number of neurons per layer, 0.1, 0.25, 0.35, or 0.5 for dropout, 0.001, 0.0001, or 0.00001 for learning rate, and 32, 64, 128, or 256 for batch size. Each set of parameters was trained based on training data (80%) and validated using validation data (10%). The results of each parameter set represent the average of ten experiments, with the data being shuffled each time. The goal of the optimization process was to maximize the R value of the validation set using the grid search method.
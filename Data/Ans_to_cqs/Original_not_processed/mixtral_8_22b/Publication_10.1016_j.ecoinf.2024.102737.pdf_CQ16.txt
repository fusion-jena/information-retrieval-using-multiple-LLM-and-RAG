Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 2
Experiments training/test sets sizes for the upper-, left- and right-poses models.

Exper. No.

Method

Upper

Left

Right

E1

E2

4-CoL, NoAug,
FEE, none
4-Col, Aug,
NoFEE, none

Train: 24
Test: 66
Train: 72
Test: 68

Train: 40
Test: 59
Train: 120
Test: 62

Train: 40
Test: 68
Train: 120
Test: 67

Table 3
Precision results of each experiment for the upper-, left- and right-pose models.

Exper. No.

Measure

E1:

E2:

P1
P3
P5
P6
P1
P3
P5
P6

Upper

0.682
0.788
0.838
0.849
0.441
0.5
0.559
0.603

Left

0.356
0.61
0.712
0.78
0.323
0.484
0.581
0.629

Right

0.544
0.662
0.735
0.78
0.418
0.522
0.612
0.687

Fig. 10. (a) Upper-pose images, (b) Left-pose images, (c) Right-pose images.

EcologicalInformatics82(2024)10273712A. Levy et al.

Still there are issues that should be considered before a specific
detailed framework can be designed. In order to decide on a feature
extraction and recognition detailed steps framework,12 initial experi-
ments were performed on the individuals labeled database. Since we
study a task that has not been studied before, we explore different as-
pects and several directions to tackle them, which naturally, most of
them are abandoned after evaluating the results. Nevertheless, these
initial experiments are important as a base case to study the conditions
that improve the performance, which is the focus of this paper. Based on
the literature survey our initial experiments referred to the following
issues to be decided for the chosen framework: architecture, data
augmentation usage, feature extraction enhancement and handling
different poses. The evaluation of the initial experiments results then
guided our design of the follow-up experiments. The initial experiments,

their evaluation and the deduced follow-up experiments are described in
Section 4.

Though the original architecture of Koch et al., (2015) used
regularized cross entropy as a loss function, better performance may
be achieved with other loss functions instead of cross entropy, as
mentioned by Wang and Deng, (2021). Contrastive loss takes as input
a pair of samples that are either similar or dissimilar, and brings
similar samples closer and dissimilar samples far apart. It is, there-
fore, a better choice for learning a metric. Contrastive loss has been
successfully used recently in several papers in machine learning as
well as with Siamese networks and outstanding results have been
reported for using it even in unsupervised contrastive learning (see e.
g., (Aruna Gladys and Vetriselvi, 2023; Choi and Kim, 2022; He et al.,
2019; Huang et al., 2019; Lian et al., 2018; Melekhov et al., 2016;
Misra and van der Maaten, 2019; Shorfuzzaman and Hossain, 2021;
Wang et al., 2021; Wang and Liu, 2021; Wu et al., 2018; Yu et al.,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The context does not provide information about the criteria used to determine when training is complete. However, common criteria include reaching a certain number of epochs, achieving a desired level of accuracy or loss on the validation set, or observing a plateau in the validation loss. These criteria depend on the specific problem and model being trained.

Based on the given context, it appears that the authors conducted initial experiments to evaluate various aspects such as architecture, data augmentation usage, feature extraction enhancement, and handling different poses. They evaluated the results of these initial experiments to guide their design of follow-up experiments. However, the context does not explicitly mention any criteria used to determine when training was complete during these experiments. Therefore, I cannot provide a definitive answer based solely on the given context.
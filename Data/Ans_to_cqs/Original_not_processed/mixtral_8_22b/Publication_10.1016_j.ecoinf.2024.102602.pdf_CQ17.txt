Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

the  best  performances. We  also  implemented  an  early  stop  approach, 
with a patience value of 16 to regularize the model and minimize the loss 
function (binary cross entropy). The early stop approach is a common 
technique in machine learning to halt the training process of a model 
prematurely  if  performance  on  a  validation  dataset  fails  to  improve 
beyond  a  predefined  threshold,  thereby  preventing  overfitting  and 
conserving computational resources.

2.5.1. Model selection and parametrization 

For  the  classification  task,  before  the  implementation  of  deep 
learning models, all images were resized to the same resolution (227×
197  pixels)  by  considering  the  mean  dimensions  of  the  set,  and  then 
normalized to the [0,1] range (Na and Fox, 2020). Then, six open source 
CNNs  were  selected:  VGG16  (Simonyan  and  Zisserman,  2015), 
ResNet50,  ResNet101  (He  et  al.,  2016),  Inception-v3  (Szegedy  et  al., 
2016), DenseNet201 (Huang et al., 2017) and EfficientNetB0 (Tan and 
Le,  2019).  These  algorithms  were  selected  because  of  their  ease  for 
transfer  learning and  high  performance  on  similar classification  tasks 
(Arun  and  Viknesh,  2022;  Vallabhajosyula  et  al.,  2022).  For  model 
optimization, we used the Adam optimizer algorithm (Kingma and Ba, 
2015), a batch size of 10 and 100 epochs. The learning rates were chosen 
(cid:0) 6 showing 
from empirical trials over 100 epochs, with and 10

A.1b),  with  high  performances  (e.g.,  f1-score  values  above  93.09%; 
Table  4).  Unlike  the  training  task,  the  best  results  were  obtained  by 
(cid:0) 4,  in  terms  of  accuracy 
EfficientNetB0  with  a  learning  rate  of  10
(97.50%),  sensitivity  (96.00%)  and  f1-score  (97.46%),  followed  by 
(cid:0) 4),  which  also  showed 
Inception-v3  with  the  same  learning  rate  (10
high accuracy (96.88%), sensitivity (94.00%), specificity (99.75%) and 
f1-score (96.78%). In the case of the EfficientNetB0 model, only 4% of 
the  images  displaying  the  Cortaderia  selloana  were  confused  by  the 
model as showing no C. selloana (false negatives; Fig. A.1c). Likewise, 
from  the  images  showing  no  C.  selloana,  in  1%  of  the  cases  Effi-
cientNetB0  incorrectly  predicted  the  class  “With  Cortaderia  selloana” 
(false  positives;  Fig.  A.1d).  A  similar  pattern  was  verified  for  the 
remaining CNNs (see confusion matrices in Tables A.6 and A.7).

To achieve a good balance between generalization and specificity of 
image classification for the classification models, the first CNN model 
layers  were  kept  frozen  during  training  with  transfer  learning,  while 
three fully connected layers in VGG16 and EfficientNetB0, and one fully 
connected  layer  in  ResNet50,  ResNet101,  Inception-v3  and  Dense-
Net201, were re-trained (fine-tuned) using our training dataset. Before 
the output layer, we also included an additional dense layer with 128 
units and a rectifier linear unit activation function (ReLU), to enhance 
the model’s adaptation to the classification task. ReLU is a mathematical 
function  commonly  used  in  deep  learning  approaches  that  replaces 
negative input values with zero, enabling the model to learn nonlinear 
relationships  and  facilitate faster convergence  during training. Lastly, 
the output layer was modified to fit a binary classification (with 2 units).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text mentions the use of an early stop approach as a regularization method to prevent overfitting in the deep learning pipeline. This technique halts the training process prematurely if performance on a validation dataset fails to improve beyond a predefined threshold. By doing so, it prevents the model from becoming too complex and fitting the noise in the data instead of the underlying patterns. Additionally, the authors mention keeping the first CNN model layers frozen during training with transfer learning, which can be seen as another form of regularization. Freezing these layers helps maintain the learned features from the original dataset, reducing the risk of overfitting when fine-tuning the model on the new dataset. However, the text does not explicitly mention other regularization techniques such as dropout or L2 regularization.
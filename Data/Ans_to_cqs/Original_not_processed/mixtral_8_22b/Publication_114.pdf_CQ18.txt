Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 4. The ﬁeld-surveyed and predicted tree species richness.

Species Richness

Ground truth
Prediction

Plot Number

1

8
9

2

9
8

3

4
6

4

6
4

5

7
7

6

3
3

7

5
5

8

5
9

9

4
6

10

6
8

11

8
11

12

5
6

Note: Ground truth means the actual number of tree species in the ﬁeld survey, and prediction denotes the predicted
number of tree species by our solution.

ﬂoss tree and the camphor tree; their user’s and producer’s accuracies were less than 60.00%. The
reason is probably because the number of training samples of silk ﬂoss tree and camphor tree was
small, and the CNN could not learn the features of the two classes well, as a training model is often
proﬁtable for species with a large amount of samples. Although the classiﬁcation accuracies were not
as good as the results obtained from the hyperspectral images [15], the high-resolution RGB images
showed relatively good results compared with the work using multispectral images [2]. In the research
of Dalponte et al. [15], the average classiﬁcation accuracy was about 80.4% when using hyperspectral
images and LiDAR, while in the research of Ferreira et al. [2], the average classiﬁcation accuracy was
about 70% when they used visible/near-infrared bands.

Tree Type

Silk ﬂoss tree
Banyan tree
Flame tree
Longan
Banana
Papaya
Bauhinia
Eucalyptus trees
Carambola
Sakura tree
Pond cypress
Alstonia scholaris
Bischoﬁa javanica
Hibiscus tiliaceus
Litchi
Mango tree
Camphor tree
Others

Table 2. Classiﬁcation accuracies of the three deep learning algorithms.

VGG16 (140,000)

ResNet50 (110,000)

AlexNet (100,000)

UA (%)

PA (%)

F1-Score UA (%)

PA (%)

F1-Score UA (%)

PA (%)

F1-Score

55.56
76.47
90.20
80.77
100.00
100.00
81.61
100.00
76.47
100.00
100.00
83.33
89.19
100.00
15.00
28.57
27.59
59.14

30.61
59.77
80.70
40.38
93.75
100.00
77.17
88.00
86.67
100.00
88.89
71.43
66.00
76.92
50.00
60.00
44.44
79.11
OA = 73.25%
Kappa = 69.76%

39.47
67.10
85.19
53.85
96.77
100.00
79.33
93.62
81.25
100.00
94.12
76.92
75.86
86.96
23.08
38.71
34.04
67.68

44.44
86.76
90.20
88.46
100.00
100.00
83.91
100.00
82.35
96.88
83.33
83.33
83.78
95.00
40.00
38.10
24.14
55.48

14 of 16

that is, the tree species training sample and its quality. As long as the tree sample can be measured well
in ﬁeld work, deep learning networks will work well. Further, it is beneﬁcial to collect tree samples
with the aid of crowd sourcing. In terms of the sampled area in species diversity, our method is also
applicable for other scales (50 × 50 m, 90 × 90 m, etc.), depending on the quadrat size in the ﬁeld work
or the requirements of species diversity estimation.

6. Conclusions

The results of this study indicate the potential of deep learning methods for applications in
tree species diversity mapping with high-resolution RGB images and LiDAR data. Our proposed
= 0.7202;
three-step workﬂow achieved accuracies of R2
=
R2
= 0.3053. The method design as well as the deep learning technology also allow the
0.5875, RMSEPielou
processing of large datasets and have the potential for transfer to other forest regions due to on-the-ﬂy
data acquisition and the processing capability.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there isn't any explicit mention of a specific strategy implemented to monitor the model performance during training. However, we can infer some possible strategies based on common practices in machine learning and deep learning models.

One common approach to monitoring model performance during training is through validation techniques such as cross-validation or holdout validation. These methods involve splitting the dataset into separate sets for training and validation, allowing the model's performance to be evaluated on unseen data throughout the training process. This helps prevent overfitting and provides insights into how well the model generalizes to new data.

Additionally, various metrics are typically employed to assess the model's performance during training. For example, classification tasks may use metrics like accuracy, precision, recall, or F1 score, while regression tasks might employ mean squared error (MSE), root mean square error (RMSE), or coefficient of determination (R^2).

In the given context, Table 2 presents the classification accuracies of three different deep learning algorithms – VGG16, ResNet50, and AlexNet. Each algorithm's User Accuracy (UA), Producer Accuracy (PA), and F1-score are listed, suggesting that these metrics were likely used to evaluate the model's performance during training. Furthermore, the overall accuracy (OA) and Kappa statistic are reported, which provide additional insight into the model's performance across all classes.

However, without more detailed information regarding the training process, it is impossible to definitively state what specific strategy was implemented to monitor the model performance during training.
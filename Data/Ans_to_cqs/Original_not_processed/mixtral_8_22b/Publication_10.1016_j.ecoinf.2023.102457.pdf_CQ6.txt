Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The second approach we used to extract embeddings was from the 
pre-trained  BirdNET  model.  The  original  version  of  the  model  was

We expected higher performance of the embeddings, but a possible 
limitation to our study may be the use of pre-trained CNNs. This may 
have limited the potential performance of using embedding from neural 
networks.  Interestingly,  higher  classification  accuracy was  seen  when 
using  BirdNET,  a  CNN  trained  on  audio  data  of  bird  vocalizations  in 
comparison  with  VGGish  and  Wav2Vec  2.0,  two  models  trained 
respectively on a vast Youtube audio dataset and a human speech audio 
dataset. This suggests that training a model with data input of gibbon 
vocalizations might result in an even better performance of neural net-
works for individual gibbon discrimination. However, such data training 
requires a large dataset and is very computationally costly, hence why 
we chose to test pre-trained CNNs.

contribute  to  larger  data  input  for  future  training  of  deep  learning 
models (Best  et  al., 2023).  Accordingly, the  rapid growth of  artificial 
intelligence  and  deep  learning  methods  could  bring  about  important 
innovations related to feature extraction and classification with the aim 
of overcoming the current challenges halting the use of PAM for indi-
vidual acoustic discrimination and succeeding in a sustained and non- 
invasive monitoring of animal populations.

Recent  advances  in  deep  learning  technology  have  revolutionized 
the  field  of  bioacoustics  (Stowell,  2022).  Many  applications  of  deep 
learning  have  targeted  the  automated  detection  and  classification  of 
acoustic signals, with applications for birds (Stowell et al., 2018), bats 
(Aodha Mac, 2018), whales (Bermant et al., 2019), and primates (gib-
bons and indris) (Dufourq et al., 2021; Ravaglia et al., 2023). In addition 
to automated detection approaches, deep learning approaches can also 
be used as a feature extraction method; in this case, the term for features 
extracted from deep learning algorithms is “embeddings”. Deep learning 
embeddings has been used to differentiate adult and juvenile great grey 
owls Strix  nebulosa through analysis of 263 juvenile and  27 adult vo-
calizations  using  embeddings  from  the  BirdNET  model  trained  on 
~1000  species  of  birds  (McGinn  et  al.,  2023.  Embeddings  have  been

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps are typically involved to ensure the quality and consistency of the input data. These steps can include:

1. Data Cleaning: This involves removing or correcting any errors, inconsistencies, or outliers in the data. For example, if working with audio data, it might involve removing background noise or silences.

2. Normalization: This step scales the data so that all features have similar ranges, which can help improve the performance of the model. Common techniques include min-max normalization, where values are scaled between 0 and 1, and z-score normalization, where values are centered around the mean with a standard deviation of 1.

3. Feature Extraction: This process involves transforming raw data into a set of features that can be used by the model. For example, in the context of audio data, this might involve converting sound waves into spectrograms or Mel Frequency Cepstral Coefficients (MFCCs).

4. Data Augmentation: This technique increases the amount of training data by applying random transformations to existing samples. For instance, in the case of images, this could involve rotating, flipping, or zooming the image.

5. Splitting the Dataset: Before training the model, the dataset is usually split into three parts: training, validation, and testing sets. The training set is used to train the model, the validation set is used to tune hyperparameters and prevent overfitting, and the testing set is used to evaluate the final performance of the model.
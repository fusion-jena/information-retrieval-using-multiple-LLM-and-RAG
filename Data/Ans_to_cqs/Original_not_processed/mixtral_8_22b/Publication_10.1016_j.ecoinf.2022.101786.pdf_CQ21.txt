Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.5. Model selection 

CNNs are sensitive to overfitting (Domingos, 2012). Throughout the 
training process, the algorithm goes through cycles of training on the 
training set - evaluating performance on the validation set – adjusting 
CNN node weights – before repeating the entire cycle. These cycles are 
referred to as epochs or iterations when the training dataset is too large 
to  be  processed  in  one  stroke  and  needs  to  be  subdivided  in  several 
smaller batches as is the case in this study. There is an optimum number 
of iterations before the performance on the validation and training sets 
(independent data that the CNN has not seen) start to diverge as the CNN 
overfits  and  becomes  more  specialized  at  predicting  the  training  set 
while becoming less able to predict the validation set (generalization).

Training  the  CNN  took  approximately  18  h  to  complete  the  6000 
iterations. Measuring the accuracy of the different CNNs and the effect of 
different confidence thresholds took several hours but could be further 
automated. Predictions on the 58,148 images took approximately 10 h 
however  the  training  and  predictions  phases  do  not  require  constant 
supervision and can be performed overnight or while the analysist is free 
to attend to other tasks. Finally, the calculation-intensive nature of the 
training, and to a lesser extent, prediction phases makes their duration 
largely  dependent  on  the  hardware  used  and  thus  may  vary  between 
users. Overall, the whole pipeline could be completed within one or two 
weeks (10 days) once the analysists are familiar with it. 

4. Discussion

Some model parameters were modified from the default configura-
tion to balance use of resources and training speed. We retained a res-
olution of 704 × 704, batch size 64, 32 subdivisions and trained up to 
6000 iterations. Other parameters were kept to default or set according 
to guidance by the developers (https://github.com/AlexeyAB/darknet). 
Manual annotations were used to generate training, validation and 
testing  datasets.  Biigle  annotations  (label  name,  centre  x,  center  y, 

EcologicalInformatics71(2022)1017863N. Piechaud and K.L. Howell

P100 GPU (16 GB of VRAM memory). The Colab notebook was also set 
to “high RAM session” to avoid memory limitations that could interrupt 
the training process.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training, there are several postprocessing steps that can be taken to evaluate the performance of the Convolutional Neural Network (CNN). Some common postprocessing steps include calculating metrics such as accuracy, precision, recall, F1 score, and area under the curve (AUC), which help assess the overall performance of the model. Additionally, confusion matrices can be generated to visualize the true positive, false positive, true negative, and false negative rates for each class. This provides insight into how well the model is performing for each individual class. Another useful tool is saliency maps, which highlight areas of the input image that contribute most significantly to the final classification decision made by the CNN. This helps identify any biases or errors in the model's decision making process. It should be noted that these postprocessing steps can take several hours to perform, depending on the size of the dataset and computational power available. However, they provide valuable insights into the strengths and weaknesses of the model and can guide future improvements.
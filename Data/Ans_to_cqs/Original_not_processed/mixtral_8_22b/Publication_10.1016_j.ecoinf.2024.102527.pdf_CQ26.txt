Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

respectively.  N  represented  the  number  of  images  in  the  test  set  or 
validation set. 

2.2.6. Model training and hyper-parameters 

We  trained  models  on  the  training  sets  and  tracked  the  model 
training process on the validation set. The training process was stopped 
when  the  loss  value  of  the  model  on  the  validation  set  no  longer 
decreased. We then tested the model performance on the test set. Our 
computing platform was a Dell PowerEdge C4130 rack server with two 
Tesla  K80  GPUs  and  256  GB  of  memory.  For  the  deep  learning  envi-
ronment,  we  utilized  the  PyTorch  1.7.1  platform.  We  employed  the 
Stochastic Gradient Descent (SGD) optimizer with a momentum value of 
Î¼  = 0.9 to train the model. Other hyper-parameter settings for model 
training were shown in Table S.4 of the Supporting Information. 

3. Results 

3.1. Experiment results of transfer strategy optimization

2.2.3. Data splitting 

Based on the conceptual model mentioned above, we split the data to 
construct  the  source  and  target  domain  datasets  to  carry  out  transfer 
learning experiments.  For the  ImageNet  dataset, we directly  used the 
pre-trained model on ImageNet-1 k as the source model, so there was no 
need to split the dataset. Both the SS and the LSM dataset are camera 
trap  image  datasets.  Each  time  the  camera  is  triggered,  it  captures 
multiple images in succession, called a capture event. A capture event 
frequently  comprises  three  images.  Notably,  images  within  the  same 
event are significantly similar to each other. Splitting the images in an 
event into training and test sets may lead to poor generalization of the 
DCNN  model. To  prevent  this, we kept  the integrity  of events  during 
data splitting, putting all images of an event into the same dataset.

When the SS dataset served as target domain datasets, we still uti-
lized the validation and test sets from Table 2, referring to them as S_V 
and S_T, respectively. Regarding the training sets, we randomly selected 
capture  events  from  S_2_100  and  created  five  different  training  sets 
containing 2000, 5000, 8000, 10,000, and 20,000 samples, respectively. 
These  training  sets  were  labeled  as  S_2,  S_5,  S_8,  S_10,  and  S_20  (see 
Table S.3 in the Supporting Information for the details), respectively.

be updated with the target data set. The selection of update layers is the 
key to obtaining high performance for transfer learning models (Gha-
foorian et al., 2017; Imai et al., 2020; Nagae et al., 2021; Zunair et al., 
2018). The optimization of the source model aims to select or construct a 
source model that can better adapt to the target task.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about specific techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, some relevant information regarding data splitting and handling of potential biases can be found.

In the context, it is mentioned that when dealing with the SS dataset as the target domain dataset, they maintained the integrity of events during data splitting by placing all images of an event into the same dataset. This approach helps avoid potential biases caused by having highly similar images in both training and testing sets, which could negatively impact the DCNN model's ability to generalize effectively.

Additionally, while creating various training sets containing different numbers of samples (2000, 5000, 8000, 10,000, and 20,000), these sets were labeled as S\_2, S\_5, S\_8, S\_10, and S\_20, respectively. Although this doesn't explicitly mention any technique to handle class imbalance, it suggests that the authors might have considered balancing the classes while selecting the samples for their training sets.

Overall, based on the provided context, it appears that the authors took steps to maintain the integrity of events during data splitting and possibly balanced the classes while creating their training sets. However, without more detailed information, it cannot be confirmed whether they specifically applied techniques like stratified splitting, oversampling, undersampling, or diverse data collection to address data bias during preprocessing.
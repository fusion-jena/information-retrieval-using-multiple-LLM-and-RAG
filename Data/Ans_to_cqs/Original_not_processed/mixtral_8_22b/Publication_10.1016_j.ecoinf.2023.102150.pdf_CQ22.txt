Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

306 

195 
533 
424 

449 
416 
425 
528 
3276  

training  data  set. The  models were  trained on  a  cloud  service  with  1 
GPU,  4  CPUs  and  16  GB  RAM  provided  by  Sigma2  -  the  National 
Infrastructure  for  High  Performance  Computing  and  Data  Storage  in 
Norway. 

We evaluated the model performance on the validation data set and 
the  out-of-sample  test  data  set  by  calculating  model  accuracy  as  the 
number of correct predictions divided by the number of all images as 
well as precision, recall and F1 score for each class (Appendix B): 

Precision =

Recall =

TP
TP + FP
TP
TP + FN
precision*recall
precision + recall

F1 = 2*

(TP = Truepositives)

(FP = Falsepositives)

(FN = Falsenegatives)

Table 1 
Number of training images, validation images (used for model validation during 
model  training)  and  out-of-sample  test  images  (used  for  external  model  vali-
dation after training was finished) as well as number of new images selected 
from  the  images  taken  between  summer  2020  and  summer  2021  for  model 
retraining.  

Class 

Number of 
training 
images 

Number of 
validation 
images 

Number of 
out-of-sample 
test images 

Number of new 
training images 
for model 
retraining 

Bad 

6453 

quality 

Bird 
Empty 
Least 

weasel 
Lemming 
Shrew 
Stoat 
Vole 
TOTAL 

3382 
9444 
1725 

9449 
9265 
4024 
9894 
53636 

677 

219 
979 
98 

967 
962 
438 
1024 
5364 

549 

119 
3301 
69 

647 
584 
64 
919 
6252 

306 

195 
533 
424 

449 
416 
425 
528 
3276

We trained a deep neural network in R (R Core Team, 2022) using the 
R-package  keras  (Chollet  et  al.,  2017),  an  interface  to  TensorFlow 
(Abadi et al., 2015) for R. We used the ResNet50 architecture (for details 
about the model architecture see He et al., 2016) with an adam opti-
mizer (Kingma and Ba, 2014). We trained the model from scratch for 55 

EcologicalInformatics76(2023)1021504H. BÂ¨ohner et al.                                                                                                                                                                                                                                 

Fig. 3. The camera trap developed by Soininen et al. (2015) consists of a Reconyx camera that is mounted on the ceiling of a metal box that can be entered by small 
mammals. The camera traps are protected with stones when placed in the field.

complete data set. Thus, model performance would be overestimated for 
rare  classes  and  underestimated  for  abundant  classes.  Therefore,  the 
number  of  true  positives,  false  positives  and  false  negatives  was

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The performance of the deep learning model is evaluated using several metrics such as model accuracy, precision, recall, and F1 score. Model accuracy is calculated as the number of correct predictions divided by the total number of images. Precision is calculated as the ratio of true positive predictions to the sum of true positive and false positive predictions. Recall is calculated as the ratio of true positive predictions to the sum of true positive and false negative predictions. The F1 score is calculated as the harmonic mean of precision and recall. These metrics are used to assess the performance of the model on both the validation dataset and the out-of-sample test dataset.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1 
Metrics for the best mono- and multi-site models based on validation data (modelv). We show the results of training, validation, test data, and the architecture of the 
best-performing model according to RMSE and R2  metrics.   

RMSE 

R2 

RMSE 

R2 

RMSE 

R2 

RMSE 

R2 

RMSE 

R2 

Site 1 

Site 2 

Train 
Validation 
Test 
Architecture 

0.024 
0.074 
0.064 
InceptionV3 2 Layer (16, 64) 

0.981 
0.843 
0.871 

0.036 
0.076 
0.066 
InceptionV3 1 Layer (8) 

0.970 
0.886 
0.916 

Train 
Validation 
Test 
Architecture 

Experiment 1 

Experiment 2 

0.041 
0.080 
0.089 

0.970 
0.890 
0.858 

0.028 
0.073 
0.082 

0.988 
0.922 
0.872 

Xception 1 Layer (32) 

Xception 1 Layer (32) 

Mono-site  

Site 3 

0.040 
0.080 
0.070 

0.980 
0.910 
0.930 

Xception 1 Layer (16) 

Multi-site  

Experiment 3 

0.038 
0.073 
0.081 
Xception 3 Layer (16,64,16) 

0.979 
0.916 
0.879 

Site 4 

Site 5 

0.030 
0.070 
0.060 
InceptionV3 3 Layer (8, 8, 32) 

0.980 
0.930 
0.950

The dataset was divided into training (60%), validation (20%), and 
testing (20%) subsets for model evaluation and hyperparameter search 
(Fig. 3 (b)). This approach was chosen to obtain an unbiased estimation 
of  error.  Data  augmentation  was  only  applied  to  the  training  data  to 
avoid overlapping the same data in different partitions. To evaluate the 
models we used the coefficients of determination (R2  - calculated as the 
squared  Pearson's  correlation  coefficient),  which  measures  the  per-
centage of variation in the response variable explained by the model. 
Additionally,  we  used  the  root  mean  squared  error  (RMSE)  metric, 
where  RMSE =
,  with  Yi = the  actual  values  pine 
Ì‚
Y i = the predicted pine cove values. In our work, RMSE 
cover values and 
values ranged from 0.0 to 1.0, with values closer to 0 indicating better 
model  predictions.  The  overfitting  was  assessed  by  comparing  model

performance on the training, validation, and test datasets. All models 
were implemented using the Keras library in Python 3.8.3 (Chollet et al.,

of the models. For instance, Mil`a et al. (2022) used the Nearest Neighbor 
Distance Matching (NNDM) LOO CV method to delineate a geographic 
space  in  which  predictions  can  be  made.  Many  of  these  approaches 
accurately  demonstrate  the  model's  transfer  learning  capabilities,  yet 
they  impose  limitations  on  the  training  domain,  leading  to  reduced 
training  heterogeneity  (Meyer  and  Pebesma,  2021).  However,  con-
ducting large CV experiments with deep learning-based models is often 
infeasible due to  their high computational costs. Deep  learning-based 
models  typically  employ  simple  data  partitioning  strategies,  such  as 
training, validation, and testing (Ocer et al., 2020). This simple scheme 
can help to add heterogeneous information to the training procedure, 
but may also overlook the diversity of heterogeneous spatial information 
in remotely sensed data that is aligned with different learning domains

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To evaluate the performance of the deep learning models, two main metrics were used: the Root Mean Squared Error (RMSE) and the Coefficient of Determination (R2). The RMSE is a measure of how well the predicted values match the observed values, while the R2 indicates the proportion of variance in the dependent variable that is predictable from the independent variables. These metrics were used to compare the performance of the models on the training, validation, and test datasets. Overfitting was assessed by comparing the performance of the models on these three datasets. The goal was to achieve lower RMSE and higher R2 scores, indicating better model predictions.
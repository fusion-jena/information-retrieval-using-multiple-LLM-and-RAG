Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

in Fig. 2. 

The  numerical  values  of  some  parameters  (Tan  et  al.,  2019)  were 
calculated and provided as measures of the association values to explain 
the data mining results. The support parameter measures the abundance 
or  frequency  (often  interpreted  as  important)  of  a  set  of  items  in  a 
database. The value of the support parameter is given by the ratio be-
tween  the  number  of  times  an  item  appears  and  the  total  number  of 
items. We refer to a set of molecules as a “set of frequent molecules” if 
support  exceeds  a  specified  minimum  threshold  we  can  identify.  In 
general, the threshold depends on the size of the dataset.

The Apriori algorithm consists of a sequence of steps for identifying 
the most frequent itemset in a given database (Fig. 2). Two main pro-
cesses  are  performed  iteratively  to  locate  the  most  frequent  itemsets: 
joining and pruning. As the name indicates, join represents the union of 
itemsets,  and  pruning  is  a  data  compression  technique  in  machine 
learning and search algorithms that reduces the size of decision trees by 
removing  sections  of  the  tree  that  are  non-critical  and  redundant  to 
classify instances. Pruning reduces the complexity of the final classifier 
and  improves  the  predictive  accuracy  by  reducing  overfitting  (Zhao 
et al., 2009).

The  user  must  decide  the  minimum  support  threshold  useful  for 
solving the assumed problem and apply the join and prune steps to the 
itemsets. In our case study, an itemset is represented by every sample's 
analytical  results,  whereas  a  k-itemset  represents  a  sample  subset  of 
various PPPs. Therefore, for each sample, there may be many k itemsets 
(with k ranging  from 1 to  n, where n  is the  number of analytical de-
terminations for a single sample). For these k-itemsets, we applied the 
Apriori algorithm to identify recurring patterns in the association rules. 
Frequent itemsets refer to itemsets whose relative frequency exceeds the 
pre-established minimum threshold the operator identifies according to 
the problem to be solved. The simplified procedures performed by the 
algorithm are as follows:

Gubner, J.A., 2006. Probability and Random Processes for Electrical and Computer 
Engineers. undefined 1–62. https://doi.org/10.1017/CBO9780511813610.003. 
Guo, W.W., Xue, H., 2012. An Incorporative Statistic and Neural Approach for Crop Yield 
Modelling and Forecasting STEM & International Education View Project Special 
Issue : Advances in Multiple Criteria Decision Analysis (Mathematics Journal) View 
Project an Incorporative Statist. https://doi.org/10.1007/s00521-011-0636-0. 
Guo, W.W., Xue, H., 2014. Crop yield forecasting using artificial neural networks: a 

comparison between spatial and temporal models. Math. Probl. Eng. 2014 https:// 
doi.org/10.1155/2014/857865. 

Gupta, D., Gujre, N., Singha, S., Mitra, S., 2022. Role of existing and emerging

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the provided context, it does not explicitly mention how the hyperparameters of the model are optimized, such as through methods like grid search or random search. It primarily focuses on explaining the concept of the support parameter and the Apriori algorithm used in data mining. However, without additional information about the specific model being used, I cannot provide a definitive answer regarding its hyperparameter optimization method.
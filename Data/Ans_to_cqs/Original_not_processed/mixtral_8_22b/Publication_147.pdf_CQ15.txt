Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

predictions. Furthermore, having a smaller input patch means there will be some overlap in the inputs
between adjacent patches, which helps combat patch boundary side eﬀects. Since the error does vary
with architecture, we chose a reasonable inner patch size for the entire model exercise, although the
optimized patch size was not iteratively tested. The output activations for the CNN were sigmoid
units. The model was trained using the Keras Nadam optimizer (Nesterov Adam optimizer [82]) with
a combination of binary crossentropy (a common loss used to train neural networks representing
the entropy between two probability distributions) and dice coeﬃcient loss (a statistic used to gauge
the similarity of two samples) for the objective loss function. Candidate training patch indexes were
created using a simple moving window with a stride of 10 and simple label counts were generated.
During training, patches were randomly selected from the patch list and randomly rotated left or

Table 2. Full comparison of training time, prediction time, optimization time (i.e., time to properly
tune the model), and hardware for each model.

Model

Training Time
(Hours)

Prediction Time (Hours)

Optimization
Time (Hours)

Hardware

CNN

XGB

4

4

2.4. Validation

4

Unknown

Desktop with 64 Gb of
RAM and one Titan X
(Maxwell) GPU.

72—can be reduced by
distributing prediction to more
cores or more machines.

2

Desktop with 64 GB of
ram and 64 logical cores.

Keywords: wetlands; Sentinel-1; Sentinel-2; Google Earth Engine; remote sensing; Alberta;
segmentation convolutional neural nets; XGBoost; land cover; SAR; machine learning

1. Introduction

Machine learning—a method where a computer discovers rules to execute a data processing
task, given training examples—can generally be divided into two categories: Shallow learning and
deep learning methods [1]. Deep learning uses many successive layered representations of data
(i.e., hundreds of convolutions/ﬁlters), while shallow learning typically uses one or two layered
representations of the data [1]. Deep learning has shown great promise for tackling many tasks such as
image recognition, natural language processing, speech recognition, superhuman Go playing, and
autonomous driving [1–3].

Remote Sens. 2020, 12, 2; doi:10.3390/rs12010002

www.mdpi.com/journal/remotesensing

Vienna, Austria, 2013.

77. Hacker Earth.

Beginners Tutorial on XGBoost and Parameter Tuning in R. Available
online: https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-
tutorial-on-xgboost-parameter-tuning-r/tutorial/ (accessed on 29 August 2019).

78. Parisien, M.-A.; Parks, S.A.; Miller, C.; Krawchuk, M.A.; Heathcott, M.; Moritz, M.A. Contributions of
ignitions, fuels, and weather to the spatial patterns of burn probability of a boreal landscape. Ecosystems
2011, 14, 1141–1155. [CrossRef]

79. Atienza, R. Advanced Deep Learning with Keras: Apply Deep Learning Techniques, Autoencoders, Gans, Variational
Autoencoders, Deep Reinforcement Learning, Policy Gradients, and More; Packt Publishing Ltd.: Birmingham,
UK, 2018.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text mentions that the model was trained using the Keras Nadam optimizer, which is a variant of the Adam optimizer. This optimizer is based on stochastic gradient descent (SGD) but incorporates momentum and adaptive learning rates to improve convergence. Additionally, the use of a combination of binary crossentropy and dice coefficient loss functions suggests that the model may have been optimized for both classification accuracy and segmentation quality. However, the text does not provide specific details about how these optimization techniques were implemented or tuned during the training process.
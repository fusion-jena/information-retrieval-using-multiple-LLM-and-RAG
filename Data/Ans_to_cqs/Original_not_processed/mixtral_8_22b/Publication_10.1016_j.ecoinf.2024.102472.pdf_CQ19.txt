Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

layers. ANNs require a substantial amount of training data to achieve 
effective learning. Among ANN architectures, the multi-layer perceptron 
(MLP) neural network stands out as the most used.

During  the  MLPNN  training  phase,  initial  arbitrary  connection 
weights were assigned. Inputs were forward-fed from the input to the 
hidden  layer.  Hidden  neurons  multiplied  inputs  by  weights,  summed 
products,  and  processed  sums  through  a  transfer  function.  Results 
propagated  to  the  output  layer,  with  output  values  compared  to  ex-
pected  values  for  error  computation.  Iterative  error  back-propagation 
adjusted  connection  weights  until  reaching  a  target  minimal  error. 
The network then accurately estimated carbon stocks for both training 
and new input data without training data. This required tests to deter-
mine optimal learning rate (0.01), momentum (0.18), and training it-
erations  (500).  The  trained  network  was  subsequently  used  for  feed- 
forward predictions on continuous spatial data. 

2.5.2. Optimal predictor variable selection

Extreme  Gradient  boosting  (XGBoost)  is  known  for  its  speed  and 
accurate  prediction  process,  especially  when  dealing  with  large  and 
complex  datasets.  Its  power  lies  in  its  ability  to  minimize  bias  error 
within  and  between  spectral  predictors.  XGBoost  builds  sequential 
models to reduce errors of predictors that could potentially lower the 
estimation accuracy (Huang et al., 2022; Tamiminia et al., 2022; Zhang 
et al., 2019b). In each sequent model, the weights of all weak learners 
(predictors)  are  increased  (or  boosted)  and  the  weights  for  strong 
learners in the model are decreased (Huang et al., 2022; Pham et al., 
2021). This process is repeated until the error is reduced, and the data is 
correctly  predicted.  Furthermore,  XGB  permits  model  tuning  through 
various  hyper-parameters,  namely:  ntree,  tree  node  size  and  gradient 
boosting iteration. Ntree builds a multitude of decision trees and select

by Boruta were then used to build separate models and their results were 
compared to an XGBoost and ANN model that included all 78 variables.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given information, there isn't any explicit mention of specific deep learning frameworks such as TensorFlow or PyTorch being used to build the deep learning models. Therefore, I cannot provide a definitive answer regarding which frameworks were utilized.
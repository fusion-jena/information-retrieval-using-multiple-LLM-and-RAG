Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4. Training data

4. Results 

4.1. Model performance 

The model achieved F1 scores above 0.991 on all the training and 
0.934 on all validation data (Table 2). For the testing data, it generalised 
exceptionally  well  over  time  at  all  locations  (“test-over_time”)  with 
scores above 0.934. Moreover, a nearly perfect F1 score was also ob-
tained  at  locations  it  had  never  seen  before  (“test-over_location”) 
reaching F1 scores of 0.985, 1, and 1 (see an example in Fig. 5). In Table 
S2, we report the F1 scores as well as the number of actual nests, TP, FP 
and FN for each individual orthomosaic. Notably, the three orthomo-
saics  in  test-over_location  contained  48  nests.  The  model  correctly 

EcologicalInformatics82(2024)1027074A. Cusick et al.

Table 2 
Mean F1 scores obtained per dataset split, with standard deviations. The very 
high scores obtained by the model on the training and validation data were also 
reproduced on the test data. Details regarding the scores for individual ortho-
mosaics can be found in Table S2.  

Dataset 
split 

Training 

Validation 

Test 

test- 
over_time 

test- 
over_location 

test- 
over_source 

F1 score 

0.998 ±
0.003 

0.963 ±
0.034 

0.977 ±
0.019 

0.995 ±
0.009 

0.837 ±
0.145

F1 =

TP
(FP + FN)

TP + 1
2

This metric is the harmonic mean between precision and recall and is 
a commonly used (e.g., Peng et al., 2020; Roy et al., 2023), conservative 
measure of performance that balances both missing predictions (FN) and 
not predicting too many instances (FP). When F1 takes its highest value 
of 1.0, it indicates excellent precision and recall; when precision or recall 
are zero, the value of F1 is 0. 

3.4. Computational requirements

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given context, there isn't enough information to provide a specific criterion used to determine when training is complete. However, common criteria include reaching a certain number of epochs, achieving a desired level of accuracy or loss on the validation set, or observing a plateau in the validation loss. These criteria help ensure that the model has learned from the training data without overfitting.

Based on the results section, it appears that the model performed exceptionally well on both the training and validation datasets, achieving high F1 scores. This suggests that the training process may have been stopped once the model reached satisfactory performance on the validation set. However, without explicit details about the stopping criteria, this remains speculative.
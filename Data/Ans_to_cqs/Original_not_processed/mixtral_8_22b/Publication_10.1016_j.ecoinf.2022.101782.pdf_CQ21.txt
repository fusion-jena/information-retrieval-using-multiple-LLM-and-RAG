Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

950 (4%) 
1110 (4%) 

Total 

24,675 
32,986  

passes over the entire training set) and a batch size of 64, with an ADAM 
(cid:0) 5. Image augmentation was applied 
optimiser and a learning rate of 10
in the form of horizontal flip, 0.2 degree counter clock wise shear and a 
random zoom between 0 and 0.2 - all leading to 224 Ã— 244 pixel RGB 
input tensors. Data was normalised to ImageNet mean values, and the 
pixels values were rescaled in the range of [0, 1]. Model training took 
roughly 4 days. The best model was selected based on minimal valida-
tion loss that occurred at epoch 448. This model showed a training loss 
of 0.256, a training accuracy of 0.899, a validation loss of 0.298 and 
validation accuracy of 0.891. We evaluated the red kite model perfor-
mance based on an independent test set of 2060 images (as described in 
3.3). 950 of these images were true positive red kites images and the

5. Discussion 

In this study we developed a workflow which leveraged citizen sci-
ence data to extract further relevant records from social media posts in 
the  same  region.  The  workflow  functions  as  a  data  filter  enabling 
downsampling of an initially very large dataset into a human analysable 
subset - in our case containing 0.5% of the original posts. By massively 
reducing  data  volumes,  it  becomes  realistic  to  analyse  the  remaining 
data by hand to select true positives, with around one hour required for 
the  4000  or  so  candidate  posts  we  identified.  Our  workflow  thus  ad-
dresses  the  research  gap  identified  by  Burke  et  al.,  2022,  using  gen-
eralisable  methods  to  extract  target  data  from  various,  unverified 
sources to enrich data.

Since our workflow is designed to be generic, take advantage of the 
text and image data and combine records from citizen science reports 
with social media data, it uses a combination of a simple rule based- 
approach, existing pre-trained models and a model trained specifically 
for our target species (Fig. 2). Our approach is designed to take advan-
tage  of  what  we  assume  to  be  high  quality  data  collected  by  citizen 
scientists with an interest in ornithology, use off-the-shelf models where 
possible, and reduce the initial number of social media posts in a given 
region to a manageable size for manual verification. Our workflow thus: 

EcologicalInformatics71(2022)1017823M.C. Hartmann et al.

In this paper we propose, implement and evaluate a workflow taking 
advantage of citizen science data documenting and recording sightings 
of birds, and more specifically red kites (Milvus milvus). Analysing social 
media data until recently has often used simple keyword-based methods 
to perform an initial filtering or search step, meaning that content tag-
ged in other ways was not found. However, improvements in content- 
based  classification  now  mean  that  it  is  also  possible  to  use  off-the- 
shelf,  pre-trained  algorithms  to  reliably  identify  predefined  classes 
such  as  presence  of  buildings,  people  or  birds  in  image  data  with 
reasonable accuracy. Our workflow uses these improvements, together 
with training data generated from citizen scientists, to identify further 
relevant sightings in a social media dataset using both textual metadata 
and  image  content.  Since  both  datasets  include  metadata  capturing

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper does not provide explicit information about the specific postprocessing steps involved after the model training. However, some insights can be derived from the evaluation process mentioned in the paper. After training the model, the authors selected the best model based on minimal validation loss occurring at epoch 448. They then evaluated the performance of the red kite model based on an independent test set of 2060 images. From this, they obtained metrics such as training loss, training accuracy, validation loss, and validation accuracy. These metrics suggest that the authors calculated the performance of their model using standard machine learning metrics like loss and accuracy. Additionally, since the authors mention evaluating the model on an independent test set, it can be assumed that they followed common practices such as splitting the dataset into training, validation, and testing sets before training the model. However, without explicit details, it cannot be confirmed if techniques like saliency maps or confusion matrices were employed during postprocessing.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

has been highly annotated and used in previous studies. The extensive 
annotation of the dataset allowed the feature representation produced 
by the auto-encoder to be easily evaluated and to be trained on a higher

3.3. Network architecture 

The architecture used was a heavily modified version of that used in 
“Unsupervised Deep Learning with Pytorch” by github user “eelxpeng” 
(eelxpeng, 2018). As the size of our audio spectrograms has been con-
strained to 128 × 128 pixel RGB images with values repeated in each 
color channel to produce a greyscale image. For this initial experiment 
testing  the feasibility of using an auto-encoder-generated feature rep-
resentation for ecoacoustic analysis, we chose a basic auto-encoder ar-
chitecture  to  minimise  the  complications  that  may  be  introduced  by 
more advanced architectures. A rectified linear unit (ReLU) based acti-
vation function was be used, to help mitigate the vanishing/exploding 
gradient  problem  (Xu  et  al.,  2015).  Networks  using  implicit  pooling 
(determined using pytorch) and explicit max-pooling were used.

The autoencoder based on explicit max-pooling consists of a 128x128x3 
input layer, and 4 convolutional layers using 3 × 3 pixel kernels, stride 
of 1 pixels and zero padding of 1 pixel on each but the first layer, and 4 
max-pooling layers using 4 × 4 pixel kernels, stride of 2 pixels and zero 
padding of 1 pixel. The encoder’s output is then flattened to produce a 
384x1x1  feature  vector.  The  decoder  network  accepts  the  384x1x1 
feature vector produced by the encoder network as its input, and consists 
of 4 convolutional layers with a 3 × 3 kernel, stride of 1 pixel and zero 
padding of 1 pixel on each layer except the last, and 4 max-unpooling 
layers using 4 × 4 pixel kernels, stride of 2 pixels and zero padding of 
1 pixel, which produces a 128x128x3 output.

The squared difference between a pixel (and each color channel of 
each pixel) of the output and its counterpart in the input was calculated, 
with the mean across all pixels giving the loss, with 2d batch normali-
zation and no dropout being applied between each layer. The “Adam” 
optimization algorithm was used. The pre-processed training data were 
then  used  to  train  the  auto-encoders.  Training  was  performed  for  3 
epochs for each network.  

–  The  implicit  pooling  network  was  trained  for  1  epoch  using  a 
learning rate of 0.001, then 1 epoch with a learning rate of 0.0001 
and 1 of 0.00001.  

–  The max-pooling network was trained for 3 epochs using a learning 

rate of 0.001. 

3.5. Evaluation 

To evaluate the performance of the latent representation in machine- 
learning  tasks,  hierarchical  clustering  was  used  as  a  sample  task  to 
evaluate our representation’s  ability to group bird calls into  different 
call types.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, there is no mention of specific regularization methods such as dropout or L2 regularization used to prevent overfitting in the deep learning pipeline. However, some techniques have been employed to ensure the model does not overfit the data. These include:

1. Batch Normalization: This technique is mentioned in the context where it states that 'the squared difference between a pixel...was calculated, with the mean across all pixels giving the loss, with 2d batch normalization'. Batch normalization helps reduce internal covariance shift, making the model more stable during training and reducing overfitting.

2. Adam Optimizer: The use of the Adam optimization algorithm is also noted. While not strictly a regularization method, Adam optimizer can help prevent overfitting due to its adaptive learning rates. It combines the advantages of two other extensions of stochastic gradient descent: Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp), providing an optimization algorithm that can handle sparse gradients on noisy problems.

3. Early Stopping: Although not explicitly stated, the fact that the networks were only trained for a few epochs could suggest early stopping as a form of regularization. Early stopping involves monitoring the validation error while training the model, and once the validation error starts increasing, training is stopped to avoid overfitting.

However, without further information about the implementation details, it cannot be definitively confirmed whether these techniques were specifically used as regularization methods to prevent overfitting.
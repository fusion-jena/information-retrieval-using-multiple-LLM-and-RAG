Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Ecological Informatics 50 (2019) 220–233

Fig. 5. Learning process for loss (left) and model accuracy (right) over 90 epochs. The gap between training and validation accuracy is caused by using dropout on
the training data only.

layers, pooling layers and fully connected layers (Voulodimos et al.,
2018). The ﬁrst two components are responsible for automatic feature
extraction by applying a large number of diﬀerent ﬁlters on the input
data. This process of feature extraction is performed on multiple levels,
whereby the output of each level is the input to the following. From
level to level, the extracted features increase in complexity - from rather
simple features (e.g. edges) on the lowest level to more complex fea-
tures on the highest level (Gu et al., 2018). By passing large quantities
of labeled training data through the network, the model successively
learns to recognize the relevant features, which are necessary to dis-
tinguish between classes.

All statistics were performed in R (version 3.3.1). The best model
was chosen based on the two parameters accuracy and loss, whereby
loss serves as a measure on how far model predictions diﬀer from the
actual class. Model accuracy and loss were calculated for both training
and validation set. We tested the performance of the ﬁnal model on two
diﬀerent data sets: 1) on individual tiles and 2) on whole repeat pho-
tographs. Prediction accuracy on individual tiles was calculated using
the 5796 tiles from the test set (= 10%), which has been separated from
the total number of samples before training. We evaluated the accuracy
on whole repeat photographs based on the image pairs of the second set
of photographs. The classiﬁcation results for each of these 34 images
were compared to the corresponding manual classiﬁcation (reference
data). A confusion matrix was prepared for each photograph in-
dividually. The confusion matrix consists of pixel numbers for true

To prevent spatial autocorrelation between neighboring tiles, the
tiles retrieved from all 50 training photographs were collected in one
folder and shuﬄed, before they were split into training, validation and
test set. In this regard, the validation set is used during training for the
ﬁne tuning of hyperparameters and model selection (Hastie et al.,
2009). Only after the best model setup has been found, its prediction
error is assessed once again on the test set. For splitting the whole set of
samples into the three sets, we ﬁrst took a random sample of 10% from
the total number of tiles as validation set and then a further sample of
10% as test set. The remaining 80% of the tiles were used as training
set. To further increase the number of sample tiles, we performed data
augmentation on the training set. Data augmentation is a common
practice in machine learning to artiﬁcially increase the number of
training samples by applying slight transformation on the original data,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The data was split into three sets: training, validation, and test. The validation set was used during training for fine-tuning hyperparameters and model selection. After finding the best model setup, its prediction error was assessed on the test set. The whole set of samples was first divided into three parts: 10% for the validation set, another 10% for the test set, and the remaining 80% for the training set. To increase the number of sample tiles, data augmentation was applied to the training set. Data augmentation involves applying slight transformations to the original data to create new training samples artificially.
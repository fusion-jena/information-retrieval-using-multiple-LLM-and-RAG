Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

performance of the trained SegFormer on each category. The average 
IOU and pixel accuracy (PA) for tree, lawn, and shrub are 72.82% and 
82.89%, respectively. However, the performance on the facility category 
is relatively poor, which could be attributed to the limited annotations 
available for this category in the UGE-D.

Parameter (m) 

Flops (g) 

FCN 
HRNet 
PSPNet 
DeeplabV3+
SegFormer 

ResNet50 
HRNet-W18 
MobileNetV2 
MobileNetV2 
MiT-B1 

59.12 
60.74 
54.94 
58.89 
63.73 

78.88 
80.58 
75.97 
78.72 
84.25 

0.719 
0.727 
0.664 
0.695 
0.745 

0.732 
0.739 
0.679 
0.712 
0.756 

0.716 
0.727 
0.689 
0.718 
0.761 

6.67 
2.63 
0.56 
1.39 
1.51 

32.951 
9.639 
2.377 
5.815 
13.665 

277.225 
32.598 
5.971 
52.753 
26.475  

Table 2 
IOU and PA of classes.  

Classes 

IOU 

PA 

classes 

Tree 
Lawn 
Shrub 
Flower 
Flat 

85.19 
72.95 
60.34 
78.13 
79.04 

92.27 
82.19 
74.2 
85.58 
90.88     

sky 
facility 
architecture 
water body 

IOU 

88.84 
32.05 
61.81 
82.63 

PA 

95.2 
42.78 
78.09 
90.5

6.3. User study 

We conducted a user study on our Urban GVI estimation system, and 
25 participants filled out a questionnaire. The questionnaire includes the 
following items:  

• How familiar are you with Green View Index (GVI)?  
• How  important  do  you  perceive  GVI  to  be  in  urban  green  space 

planning?  

• How willing are you to use GreenCam for capturing urban GVI?  
• How smooth did you find the operation of GreenCam?  

• How usable did you find GreenCam for capturing GVI?  
• How similar do you find the photos captured by GreenCam to what 

the human eye perceives?

Fig. 6. Structure of the neural network of learning eye-phone distance, where 
the  number  attached  to  each  layer  is  the  number  of  neurons  included  in 
that layer. 

structure of EPDNet, which works with the face box's width and height 
as  input  and  the  eye-phone  distance  as  output.  The  ReLU  activation 
function  and  sigmoid  are  used  to  cope  with  non-linearity.  The  five 
neuron  layers  following  the  concatenation  are  all  fully  connected.  In 
EPDNet training, we employ the Mean Squared Error (MSE) loss func-
tion and the Adam optimizer with a learning rate of 0.0001. We use the 
R2  metric to assess the model's fit to the observed data. After training, 
this model fits well with the R2  of 0.95.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific strategy implemented to monitor the model performance during training. However, it mentions that the Mean Squared Error (MSE) loss function was employed in the training process of EPDNet. This suggests that monitoring the MSE loss value might have been part of the strategy to evaluate the model's performance during training. Additionally, the R2 metric was used to assess the model's fit to the observed data after training, indicating that tracking the R2 score could also have been a component of the monitoring strategy.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(b) Computing infrastructure used to train both neural networks 

Infrastructure 
Search strategy 
Training duration 
(best assignment) 

Nvidia Tesla V100 (16GB) 
Manual tuning  
Segmentation 
Classification 

4 h 6 min 24 s 
9 h 22 min 34 s

Fig. 3. Example of individual fishes once the segmentation is done. These fishes belong to a monospecific set of YFT.  

Table 2 
Computing  infrastructure,  search  space  and  best  assignments  of  hyper-
parameters of the neural networks.  

(a) Hyperparameters used for the training of each neural network. The base models 
have been obtained from TensorFlow Object Detection API (segmentation) and the 
base implementation of TensorFlow (classification) 

Hyperparameter  Module 

Search space 

Best assignment 

Epochs 

Batch size 

Learning rate 

Activation 

Train/test split 

Segmentation 
Classification 
Segmentation 
Classification 
Segmentation 
Classification 
Segmentation 
Classification 
Segmentation 
Classification 

[1000(cid:0) 20,000] 
[500–5000] 
[2(cid:0) 32] 
[2–128] 
[0.0001–0.01] 
[0.0001–0.01] 
Softmax 
Softmax 
[0.9/0.1–0.8/0.2] 
[0.9/0.1–0.8/0.2] 

15,000 
500 
4 
128 
0.008 
0.0001 
Softmax 
Softmax 
0.9/0.1 
0.8/0.2

Fig. 1. Diagram of our methodology. There are four main modules: Preprocessing of historical images (purple, sec. 2.1), model training and validation (red, sec. 2.2 
and sec. 2.3), prediction making (green, sec. 3.1 and sec. 3.3) and ground truth validation (orange, sec. 2.5 and sec. 3.4). The original image database (dark purple) 
and the training set (dark red, sec. 3.2) are also highlighted. Data used in each module is specified in Table 1. (For interpretation of the references to colour in this 
figure legend, the reader is referred to the web version of this article.) 

EcologicalInformatics67(2022)1014953X. Lekunberri et al.

2.3. Fish classification by species 

A classifier based on deep neural networks was also used for this task. 
In this case, the chosen architecture is a residual neural network, whose 
main characteristic is that it can skip some of its layers (He et al., 2016). 
In particular, the implementation of ResNet50V2 model in TensorFlow 
and  pre-trained  with  the  ImageNet  dataset  (training  parameters  in 
Table  2).  A  problem  encountered  in  initial  tests  related  to  species 

EcologicalInformatics67(2022)1014954X. Lekunberri et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper does not provide explicit information about the specific preprocessing steps taken before training the deep learning models. However, it mentions that there are four main modules in their methodology, one of which is the preprocessing of historical images (section 2.1). This suggests that some form of preprocessing was performed on the input data before being fed into the deep learning models. Common preprocessing techniques include normalization, scaling, cleaning, augmentation, etc. Without further details, we cannot confirm exactly what preprocessing steps were applied in this study.

Based on common practices in machine learning and computer vision tasks involving deep learning, it is reasonable to assume that the authors might have employed some or all of these preprocessing techniques. Normalization helps scale pixel values between 0 and 1, while scaling could involve resizing images to fit the required dimensions of the input layer of the deep learning model. Cleaning may refer to removing noise or irrelevant features from the images, and augmentation involves applying various transformations like rotation, flipping, zooming, etc., to increase the diversity of the training dataset.

Without additional information, we cannot definitively state the exact preprocessing steps followed in this research. Nonetheless, given the prevalence of such techniques in similar studies, it is likely that some combination of them was utilized here as well.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

SVR is a machine learning method known for its superior accuracy 
compared  with  other  machine  learning  methods  and  its  ability  to 
simplify parameters, achieve global optimization, and handle nonlinear 
problems  (Chen  et  al., 2017;  Ji et  al.,  2021). In  this  study, the  expo-
(cid:0) 8–21,  σ  =
nential  parameters  were  set  as  follows:  C  = 20–210,  ε  = 2
(cid:0) 8–21 (Yang et al., 2022). Table 2 presents the training results for the 
2
hyperparameter-optimized  SVR  model.  Parameter  C  controls  error 
tolerance during training and determines the maximum deviation that 
the model permits for any individual training example. A higher C value 
allows for a larger error, which makes the model more likely to fit the 
training  data  but  also  increases  the  risk  of  overfitting.  Conversely,  a 
smaller  C  value  imposes  stricter  constraints  on  the  model,  thereby 
increasing  the  difficultly  of  fitting  the  training  data  but  potentially

4.2. Parameter settings

improving its generalization ability. The parameter ε is used to control 
the  boundary  between  support  vectors  and  the  hyperplane.  It  de-
termines  the  minimum  margin  or  distance  of  the  model  from  any 
training example. A smaller ε value results in wider boundaries, whereas 
a  larger  ε  value  leads  to  tighter  boundaries.  The  parameter  σ  is  the 
parameter of the Gaussian kernel function, which determines the range 
of the similarity measure. It controls the relationship between the sup-
port  vectors  and  the  hyperplane  and  is  crucial  in  nonlinear  SVR.  A 
smaller  σ  value  increases  the  width  of  the  Gaussian  kernel  function, 
whereas a larger σ value results in a stronger regression relationship.

regularization parameter used to control model complexity and prevent 
overfitting  by  constraining  the  loss  function  during  training.  The 
parameter ε  controls the model's fault tolerance, that is, the degree of 
tolerance for differences between predicted and actual values of training 
samples. The term σ represents a parameter of a Gaussian kernel or the 
width parameter of a radial basis function kernel. This parameter con-
trols the relationship between support vectors and hyperplanes and af-
fects model complexity and generalization ability. SVR is formulated as 
follows: 

f (x) = ωT φ(x) + b

R =

1
2

‖ω‖2 + C

)

ξi + ξ*
i

∑N
(cid:0)

i=1

(5)  

(6)  

where ω is a weight vector, φ(x), b is a bias term, C is the regularization 
constant, and ξi and ξ*
i  are slack variables that quantify how far data can 
exist from the ε tube. 

3.4. Deep learning 

3.4.1. SLSTM method

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly state the method used for hyperparameter optimization of the Support Vector Regression (SVR) model. However, it mentions that the model was "hyperparameter-optimized" and provides the ranges for the exponential parameters C and ε as well as the parameter σ. Common methods for hyperparameter optimization include Grid Search, Random Search, Bayesian Optimization, and Gradient-based Optimization. Without further information, we cannot definitively determine which method was used for hyperparameter optimization in this case.
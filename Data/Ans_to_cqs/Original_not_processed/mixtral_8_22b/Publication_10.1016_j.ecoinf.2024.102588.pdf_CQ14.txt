Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.5. Performance evaluation

sion  trees  with  randomly  selected  input  subsets.  A  random  forest 
regression  model  was  used  to  train  the  water  quality,  hydraulic,  and 
hydrological  inputs  for  estimating  TDI,  BMI,  and  FAI.  The  hyper-
parameters of the RF were set to the number of estimators at 100 and the 
random state at 0. XGBoost is a scalable implementation of tree boost-
ing. This model was built to analyze variations in health indices. We set 
the XGBoost model in which the object was a regression, the subsample 
ratio of the columns when constructing each was 0.3, the learning rate 
was 0.1, the maximum depth of a tree was 30, the alpha was 10, and the 
number  of  estimators  was  100.  The  detailed  principles  of  the  afore-
mentioned machine-learning models are referred to in ANN (Hopfield, 
1988),  SVM  (Hearst  et  al.,  1998),  RF  (Breiman,  2001),  and  XGBoost 
(Chen and Guestrin, 2016).

predict daily streamflow: a comparison to common machine learning algorithms. 
Water Resour. Manag. 36 (2), 699–716. 

Kingma, D.P., Ba, J., 2014. Adam: a method for stochastic optimization. arXiv preprint 

arXiv:1412.6980.  

Kong, D., Son, S.H., Hwang, S.J., Won, D.H., Kim, M.C., Park, J.H., Kim, A.R., 2018. 

Development of benthic macroinvertebrates index (BMI) for biological assessment 
on stream environment. J. Korean Soc. Water Environ. 34 (2), 183–201. 

Ren, X., Guo, H., Li, S., Wang, S., Li, J., 2017, August. A novel image classification 
method with CNN-XGBoost model. In: International Workshop on Digital 
Watermarking. Springer, Cham, pp. 378–390. 

Rombouts, I., Beaugrand, G., Fizzala, X., Gaill, F., Greenstreet, S.P.R., Lamare, S., 
F´eral, J.P., 2013. Food web indicators under the Marine Strategy Framework 
Directive: from complexity to simplicity? Ecol. indicat. 29, 246–254.

using kernel functions. RF is a machine learning algorithm that builds 
multiple decision trees during training and merges their predictions to 
improve  accuracy  and  reduce  overfitting  (Zounemat-Kermani  et  al., 
2020). Each tree is trained on a random subset of the training data and a 
random  subset  of  features,  leading  to  a  diverse  set  of  trees.  During 
prediction,  the  algorithm  aggregates  the  predictions  from  all  trees  to 
make  the  final  decision.  RF  is  versatile,  robust,  and  widely  used  for 
regression  tasks.  XGBoost  is  the  machine  learning  algorithm  that  en-
hances  the  performance  of  gradient  boosting  through  optimization 
techniques and parallel processing (Ali et al., 2023). XGBoost builds a 
strong predictive model by sequentially adding decision trees and cor-
recting errors made by previous models (Amjad et al., 2022). XGBoost is 
known for its efficiency, scalability, and ability to handle large datasets.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about how the hyperparameters of the Random Forest (RF) or Extreme Gradient Boosting (XGBoost) models were optimized. However, it mentions that the hyperparameters of the RF were set to the number of estimators at 100 and the random state at 0, while the XGBoost model had specific settings such as the subsample ratio of the columns being 0.3, the learning rate being 0.1, the maximum depth of a tree being 30, the alpha being 10, and the number of estimators being 100. These values could have been determined using various methods like grid search, random search, Bayesian optimization, or manual tuning based on domain knowledge and experience. Grid search involves trying out different combinations of hyperparameter values within a manually specified range, while random search selects random combinations instead. Bayesian optimization uses probabilistic models to guide the selection of hyperparameters. Without further details, we cannot definitively determine which method was used to optimize these hyperparameters.
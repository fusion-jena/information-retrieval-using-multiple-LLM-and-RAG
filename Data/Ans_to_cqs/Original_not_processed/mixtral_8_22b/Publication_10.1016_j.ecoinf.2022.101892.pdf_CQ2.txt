Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Vanhoucke, V., Rabinovich, A., 2015. Going deeper with convolutions. In: 
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 
pp. 1–9. https://doi.org/10.1109/CVPR.2015.7298594. 

Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le, Q.V., 2019. 

Mnasnet: Platform-aware neural architecture search for mobile. In: Proceedings of 
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 
pp. 2820–2828. https://doi.org/10.1109/CVPR.2019.00293. 

W¨agele, J.W., Bodesheim, P., Bourlat, S.J., Denzler, J., Diepenbroek, M., Fonseca, V., 
Frommolt, K.-H., Geiger, M.F., Gemeinholzer, B., Gl¨ockner, F.O., et al., 2022. 
Towards a multisensor station for automated biodiversity monitoring. Basic Appl. 
Ecol. 59, 105–138. https://doi.org/10.1016/j.baae.2022.01.003.

In  addition  to  calculating  MAE  values,  we  also  compared  the 
inference-time to determine the most computationally efficient model. 
We set the batch to 400 to achieve 100% GPU utilization of each model 
and to measure the maximum queries per second (QPS) achieved under 
real-world conditions (Ding et al., 2021). All models were tested under 
the same experiment setting and the same GPU to make comparisons 
between them fair. To do so, we first expand the dataset by a factor of 
five (from 8002 to 40,010 images). Then, we fed batches of 50 images 
into the model to warm up the hardware, before recording the inference- 
time of batches of 100 to quantify QPS. 

pi,j =

(cid:0)
exp

)

zi,j
(cid:0)

exp

zi,m

∑M

m=1

)

(1)  

3. Results

2.2.1. VGG 

VGG nets (Simonyan and Zisserman, 2015) achieved huge success in 
image recognition since it was proposed in 2014. VGG nets use small 
(3×3) convolutional filters to increase the depth of the networks, which 
shows  that  scaling  up  networks  by  depth  can  achieve  better  perfor-
mance. By stacking different numbers of convolutional layers can obtain 

EcologicalInformatics72(2022)1018923H.-X. Zang et al.

different VGG nets, such as VGG-11, VGG-16, and VGG-19. VGG-16 has 
been applied in many researches and it is often used to extract features in 
the age estimation tasks Pan et al. (2018). In this study, we replaced the 
last three Fully-Connected (FC) layers of the original VGG model with 
one FC layer to reduce parameters. And we adopt Batch Normalization 
(BN) (Ioffe and Szegedy, 2015) to accelerate the training of networks. 
For this study, we selected VGG-11-BN and VGG-16-BN to extract the 
features from panda facial images. 

2.2.2. ResNet

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The context does not provide explicit information about the specific data format used in the deep learning pipeline. However, based on the mention of "panda facial images" and the reference to VGG nets being successful in "image recognition," it can be inferred that the data format used in the deep learning pipeline is likely to be images. Additionally, the calculation of Mean Absolute Error (MAE) values suggests that the output of the deep learning pipeline is numerical, possibly representing predicted ages or other related metrics. Therefore, while the exact data format cannot be confirmed without additional context, it appears that the input data consists of images, and the output data is numerical.
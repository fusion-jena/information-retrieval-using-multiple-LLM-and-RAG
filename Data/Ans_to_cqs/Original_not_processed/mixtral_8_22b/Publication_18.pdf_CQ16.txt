Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

trends. J. Exp. Mar. Biol. Ecol. 368, 1–8 (2009).

 54.  Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks. Nature 542, 115–118 (2017).
 55.  Silver, D. et al. Mastering the game of Go with deep neural networks and tree search. Nature 529, 484–489 (2016).
 56.  Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015).
 57.  Dodge, S. & Karam, L. Understanding how image quality affects deep neural networks. In 2016 Eighth International Conference on 

Quality of Multimedia Experience (QoMEX) 1–6, https://doi.org/10.1109/QoMEX.2016.7498955 (2016).

 58.  Kim, J., Lee, J. K. & Lee, K. M. Accurate Image Super-Resolution Using Very Deep Convolutional Networks. in Proc. CVPR IEEE 

1646–1654, https://doi.org/10.1109/CVPR.2016.182 (2016).

 59.  Tabik, S., Peralta, D., Herrera-Poyatos, A. & Herrera, F. A snapshot of image pre-processing for convolutional neural networks: case

CVPR IEEE 2818–2826 (2016).

 42.  Szegedy, C., Ioffe, S., Vanhoucke, V. & Alemi, A. Inception-v4, Inception-ResNet and the Impact of Residual Connections on 

Learning. ArXiv160207261 Cs (2016).

 43.  Redmon, J. & Farhadi, A. YOLO9000: Better, Faster, Stronger. In Proc. CVPR IEEE 7263–7271 (2017).
 44.  Lin, T.-Y. et al. Feature Pyramid Networks for Object Detection. in Proc. CVPR IEEE 2117–2125 (2017).
 45.  Zhang, S., Wen, L., Bian, X., Lei, Z. & Li, S. Z. Single-Shot Refinement Neural Network for Object Detection. in Proc. CVPR IEEE 

4203–4212 (2018).

 46.  Fu, C.-Y., Liu, W., Ranga, A., Tyagi, A. & Berg, A. C. DSSD: Deconvolutional Single Shot Detector. ArXiv170106659 Cs (2017).
 47.  Lin, T.-Y., Goyal, P., Girshick, R., He, K. & Dollár, P. Focal Loss for Dense Object Detection. ArXiv170802002 Cs (2017).
 48.  Zhang, X. et al. Geospatial Object Detection on High Resolution Remote Sensing Imagery Based on Double Multi-Scale Feature

resolution satellite imagery. Polar Biol. 35, 963–968 (2012).

 76.  Buchanan, G. M. et al. Free satellite data key to conservation. Science 361, 139–140 (2018).
 77.  Popkin, G. Technology and satellite companies open up a world of data. Nature 557, 745 (2018).
 78.  Pettorelli, N., Owen, H. J. F. & Duncan, C. How do we want Satellite Remote Sensing to support biodiversity conservation globally? 

Methods Ecol. Evol. 7, 656–665 (2016).

 79.  Abadi, M. et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. ArXiv160304467 Cs (2016).
 80.  Huang, J. et al. Speed/accuracy trade-offs for modern convolutional object detectors. ArXiv161110012 Cs (2016).
 81.  Krizhevsky, A., Sutskever, I. & Hinton, G. E. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in 
Neural Information Processing Systems 25 (eds Pereira, F., Burges, C. J. C., Bottou, L. & Weinberger, K. Q.) 1097–1105 (Curran 
Associates, Inc., 2012).

1

www.nature.com/scientificreportsnumber of annotated images to later make correct predictions on new images37. Although the construction of a 
dataset for training is costly, the learning of CNNs on small datasets can be boosted by data-augmentation, which 
consists of increasing the volume of the training dataset artificially, and additionally by transfer learning, which 
consists of starting the learning of the network from a prior knowledge rather from scratch38,39.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The papers cited do not explicitly discuss the specific criteria used to determine when training is complete. However, it is common practice in machine learning to use metrics such as validation loss or accuracy to monitor the progress of model training and decide when to stop. Typically, if the validation loss stops decreasing significantly or starts to increase, this could indicate overfitting, and it might be time to stop training. Similarly, if the validation accuracy has reached a satisfactory level and no longer improves, this could also suggest that further training may not yield significant benefits. It should be noted that these criteria depend on the specific problem and goals of the project.
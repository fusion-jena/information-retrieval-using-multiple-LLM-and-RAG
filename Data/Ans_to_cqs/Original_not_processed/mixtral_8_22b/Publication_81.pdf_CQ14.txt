Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Another considerable experimental result is that Elastic Net can improve the perfor-
mance of the proposed model. A good deep learning model usually requires abundant
data to train and analyze, while the limitations of obtaining DNA barcode sequences of ﬁsh
species from different families and the problem of overﬁtting in small datasets are more
and more serious. To solve the overﬁtting problem in training process on small datasets
is of great importance. In our study, Elastic Net is used to solve overﬁtting problem and
improve the generalization ability of the ESK model. Moreover, genetic characteristics
of species belong to high-dimensional data, which are time consuming during training.
However, directly combining a set of fully connected EN-SAE is often has little effect for
extracting useful information. Elastic Net provides sparse connection, which can also save
training time. Therefore, Elastic Net can improve the performance of proposed model.

36.

Random Forest supervised learning model. BMC Genet. 2019, 20, 2. [CrossRef] [PubMed]
Jin, S.; Zeng, X.; Xia, F.; Huang, W.; Liu, X. Application of deep learning methods in biological networks. Brief. Bioinform. 2021, 22,
1902–1917. [CrossRef]

37. Kumar, S.; Stecher, G.; Li, M.; Knyaz, C.; Tamura, K. MEGA X: Molecular Evolutionary Genetics Analysis across Computing

Platforms. Mol. Biol. Evol. 2018, 35, 1547–1549. [CrossRef]

38. Chu, Z.; Yu, J. An end-to-end model for rice yield prediction using deep learning fusion. Comput. Electron. Agric. 2020, 174,

105471. [CrossRef]

39. Chen, J.; Sathe, S.; Aggarwal, C.; Turaga, D. Outlier Detection with Autoencoder Ensembles. In Proceedings of the 2017 SIAM

International Conference on Data Mining (SDM), Houston, TX, USA, 27–29 April 2017; pp. 90–98. [CrossRef]

Symmetry 2021, 13, 1599

16 of 16

40. Homoliak, I. Convergence Optimization of Backpropagation Artiﬁcial Neural Network Used for Dichotomous Classiﬁcation of

226.5478

ESK

122.3258

295.8245

201.4312

As shown in Tables 4–6, the AE has the shortest running time and the poorest clas-
siﬁcation results compared with the other two models. The AE consists of only a single
Autoencoder, but only one Autoencoder cannot learn the deep features of DNA barcode se-
quences well, resulting in poor classiﬁcation results compared with deep learning methods.
Although the ESK time consumption is longer than AE, the accuracy, recall, and F1-Score
improved for all datasets, by 8.44%, 8.77%, and 4.84%, respectively.

Symmetry 2021, 13, 1599

13 of 16

(5)

Symmetry 2021, 13, 1599

6 of 16

However, each family contains a limited number of ﬁsh species, resulting in a relatively
small sample size for each dataset, and overﬁtting can easily occur during the training
process. At the same time, due to the high dimensionality of DNA barcode sequences,
training the model is time consuming. In order to improve the generalization ability of the
proposed model and reduce model training time, some constraints are added to reduce
the weight of useless features. Based on this point, Elastic Net composing of L1-norm
and L2-norm is introduced in this method. The structure of EN-SAE model is shown in
Figure 3. It can also treat L1-norm and L2-norm as penalty for loss function to restrict some
parameters in the process of training.

Figure 3. The structure of Elastic Net-Stacked Autoencoder (EN-SAE).

L1-norm, also called Lasso regression, contributes to generate a sparse matrix. It is
|Wi|, denotes the sum of the absolute value of each element

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about how the hyperparameters of the model were optimized. However, it mentions that Elastic Net was used to address the issue of overfitting in small datasets. Elastic Net combines both L1 and L2 regularizations, where L1 regularization encourages feature selection and L2 regularization prevents overfitting. This suggests that the authors might have employed techniques such as cross-validation or grid search to find optimal values for the regularization parameters lambda_1 and lambda_2. Nevertheless, without specific details regarding the optimization strategy, we cannot definitively state whether grid search, random search, or another technique was utilized.
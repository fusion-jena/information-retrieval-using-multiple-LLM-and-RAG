Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

5. Conclusion 

Classifying bird songs has long been an arduous process, requiring 
large  amounts  of  time.  DialectDecoder  uses  the  power  of  machine 
learning and the anomaly identification abilities of humans to detect and

grams, and created the metadata .csv file necessary for training the CNN 
and k-NN. Identical in structure to the metadata file outlined above, this 
file contained the metadata for the training set as opposed to the met-
adata for the entire dataset. 

No additional preprocessing was applied to the spectrograms as the 
goal was to do the least amount of preprocessing possible in order to 
demonstrate  the  power  of  human-machine  teaming.  Further,  when 
assessing the accuracy of the CNN on noise-filtered data versus raw data, 
the accuracy levels were similar: after 10 trials, the CNN’s accuracy on 
the  filtered  data  averaged  to  99.22%  and  the  accuracy  on  the  non- 
filtered data averaged to 99.25%.

2.2. Training 

Next, DialectDecoder trained the two classification networks based 
on the labeled training data. Each dialect class was split into a training 
set, a validation set, and a testing set by taking the first 70% of songs in 
the folder as the training set, the next 10% as the validation set, and the 
last 20% as the test set to reduce variability between tests. We note that 
while the validation set was used to tune the hyperparameters of the 
CNN, the only hyperparameter of the k-NN is the value of k, which we 
fixed as k = 4, as this value yielded the best performance after testing 
several values. Hence both the training and validation sets, totaling 80% 
of the full dataset, were used directly to fit the k-NN. Thus, we used 80% 
of the full data set to train and validate the CNN spectrogram classifier 
and to train the k-NN location classifier, and reserved 20% of the data set 
to test the two systems’ ability to classify songs correctly. 

2.2.1. CNN structure

Additionally,  we  created  a  metadata  .csv  file  that  listed  all  of  the 
necessary classification information for training the CNN and k-NN . The 
.csv file had five columns. The first, dialect_folder, listed the file path to 
the folder containing the bird song. Each folder was labeled with the 
dialect type and the year it was collected, for example, “ABLA_2022”. 
Note, each folder name contained the label of the songs inside, as this 

EcologicalInformatics82(2024)1026572B. Story et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit details about handling randomness in the deep learning pipeline, such as managing random seed values. However, some implicit strategies can be inferred from the given context. For instance, the authors mention using 80% of the full dataset to train and validate the CNN spectrogram classifier and to train the k-NN location classifier, while reserving 20% of the dataset to test the system's ability to classify songs correctly. This approach suggests that the authors may have used a fixed random seed or a consistent methodology to ensure reproducibility and minimize the impact of randomness during model training and evaluation. Additionally, the use of a validation set to tune the hyperparameters of the CNN indicates that the authors aimed to optimize their models without overfitting them to specific subsets of the data. While these practices do not explicitly address randomness related to seed values, they demonstrate efforts to maintain consistency and reliability throughout the deep learning pipeline.
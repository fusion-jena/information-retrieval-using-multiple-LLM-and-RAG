Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

mapping probability samples that are exclusively used for map evalua-
tion are often not available and therefore alternative methods have been 
proposed.  In  machine  learning,  if  data  are  abundant,  a  common 
approach is to randomly divide the full dataset used for modelling into 
three parts: a training set, a validation set, and a test set (Hastie et al., 
2009,  Chapter  7).  The  training  set  is  used  for  fitting  the  models,  the 
validation set is used to estimate prediction error for model selection and 
hyperparameter tuning, while the test set is used for assessing the ac-
curacy of the final model. This paper addresses this latter testing phase, 
with the specific aim to assess the accuracy of a thematic map produced 
by a calibrated statistical prediction method. Data availability is often 
limited  so that setting  aside a  test set  cannot always be afforded and 
therefore resampling methods are used (Hastie et al., 2009; Steele et al.,

Fig. 5. Examples of estimated sampling intensities (multiplied by 109) for five samples shown in Fig. 2: (a) simple random (cf. Fig. 2(a)); (b) systematic random (cf. 
Fig. 2(c)); (c) moderately clustered (cf. Fig. 2(e)); (d) strongly clustered (cf. Fig. 2(g)); (e) strongly clustered, gapped sample (cf. Fig. 2(i)).

prediction and training points.

Thirdly,  we  used  a  heuristic,  quasi-randomization  method  (Elliott 
and Valliant, 2017) employing inverse sampling-intensity weighting to 
give  more  weight  to  observations  in  sparsely  sampled  areas  and  less 
weight to observations in densely sampled areas to correct for estima-
tion bias. Sampling intensities of the dataset were estimated using a two- 
dimensional  kernel  approach  where  the  kernel  width  was  computed 
from the sample using the Cronie and van Lieshout (2018) criterion as 
implemented in the spatstat package for R (Baddeley et al., 2016). The 
map  accuracy  metrics  were  computed  by  weighting  the  squared  re-
siduals obtained from conventional random cross-validation by the in-
verse of the estimated sampling intensity (details are in Section 2.3). The 
RMSE  estimator  bears  some  cursory  similarity  with  the  Horvitzâ€“-
Thompson  estimator  of  map  accuracy  metrics  in  a  stratified  sample

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information on data annotation techniques used in the deep learning pipeline such as bounding box annotation or instance segmentation. However, it discusses the use of datasets in machine learning for modeling purposes. These datasets can include various types of labeled data, which may involve different annotation techniques like bounding boxes, semantic segmentation, or polygon annotations.

In general, data annotation techniques in deep learning pipelines depend on the specific task and application. For object detection tasks, bounding box annotation is commonly used to draw rectangles around objects of interest within images. Instance segmentation involves labeling individual pixels belonging to each distinct object instance, providing more detailed information about their shapes and locations. Other annotation techniques include semantic segmentation, key point annotation, and text annotation, among others.

However, based solely on the provided context, I am unable to specify the exact data annotation techniques employed in the described process.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.5.1. AutoGluon 

AutoGluon is an open-source AutoML library designed to democra-
tize machine learning, making it accessible to users of all skill levels. It 
simplifies  the  process  of  training  high-performance  ML  models  on 
structured  data  with  minimal  code.  The  approach  behind  AutoGluon 
emphasizes  advanced  data  processing,  deep  learning,  and  ensemble 
techniques,  rather  than  just  algorithm  selection  and  hyperparameter 
tuning.  It  automates  feature  engineering,  data  pre-processing,  model 
training,  and  ensembling,  utilizing  methods  like  multi-layer  stacking 
and  neural  network  embeddings.  The  core  training  strategy  of  Auto-
Gluon can be summarized in the following steps:  

(1)  AutoGluon trains a collection of ML models M = (m1, m2, …, mn) 
on the training data, including neural networks, tree ensembles, 
and more.

EcologicalInformatics82(2024)1027119S. Khanmohammadi et al.                                                                                                                                                                                                                    

line with the fact that the dataset is limited, comprising only 113 data 
records.  TABPFN  is  better  suited  to  perform  well  on  such  a  limited 
dataset (Hollmann et al., 2022).

(t(cid:0) 1) + ft(xi)

)

+ Ω(ft)

(4)  

EcologicalInformatics82(2024)1027113S. Khanmohammadi et al.                                                                                                                                                                                                                    

∑

where l is the loss function or error quantification; t is the number of 
iterations; Ω is the penalty for complicated models; yi is the output of the 
model; 
is  associated  with  the  summation  of 
current  and  previous  loss.  By  optimizing  the  loss  function,  the  most 
accurate XGBoost model can be developed based on the input dataset. 

(t(cid:0) 1) + ft(xi)

yi, ̂yi

n
i=1l

)

(cid:0)

(3)  The OOF predictions are averaged over the n number of rounds to 

reduce variance.  

(4)  The averaged OOF predictions are stacked and used as features to 

train a 2nd layer of models.  

(5)  Steps 2–4 are repeated for L stacking layers to create a powerful

2.5. Automated machine learning (AutoML) 

2.6. Generative adversarial networks (GANs) 

AutoML offers a substantial level of automation, encompassing tasks 
such  as  including  data  pre-processing,  algorithm  selection,  and  opti-
mizing hyperparameters. These individual steps can pose challenges for 
non-expert data scientists, creating substantial barriers to designing and 
implementing algorithms effectively. Consequently, AutoML was intro-
duced to streamline these intricate processes for non-experts, making it 
more accessible to use these techniques correctly and efficiently (Rah-
man et al., 2023). In this study, two AutoML models, AutoGluon and 
TabPFN, were employed. 

2.5.1. AutoGluon

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The information about the specific code repository link for the deep learning pipeline mentioned in the given context is not available. However, since AutoGluon is an open-source AutoML library, its source code can be found on its official GitHub repository at <https://github.com/awslabs/autogluon>.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During  the  preprocessing  phase,  techniques  like  auto-orientation, 
resizing,  tiling,  and  filtering  were  utilized  to  standardize  and  enrich 
the dataset. These steps ensured that the images were consistently ori-
ented,  had  a  uniform  size  of  640  × 640  pixels,  and  contained  a  sub-
stantial  amount  of  annotated  data,  meeting  the  90%  annotation 

threshold. Augmentation played a crucial role in further diversifying the 
dataset. By introducing random saturation adjustments to each training 
example, the number of outputs per example was increased to two. This 
augmentation  strategy 
intensity, 
contributing to a more comprehensive and robust training dataset. 

introduced  variations 

in  color

On the positive side, a smaller model might generalize better on data 
from previously unseen domains or categories, with a reduced capacity 
to  memorize  training  data;  the  model  may  focus  on  learning  more 
generic  features  that  can  be  useful  across  different  datasets  (Seema-
kurthy et al., 2022). As we will see later, this is a goal we seek in our 
model. 

A reduction in the number of filters can lead to faster inference times. 
Using  smartphone  applications,  the  model  may  process  images  more 
quickly, making it suitable for real-time applications or scenarios that 
demand rapid detection, such as NBSB detection and counting (Diwan 
et al., 2023). However, care must be taken, as if the number of filters is 
decreased,  the  model  might  become  more  prone  to  overfitting  the

introduced  variations 

in  color 

The  training  set,  comprising  the  majority  of  the  data  (82%),  con-
taining 4000 images, is used to train the model and adjust its parame-
ters, allowing it to learn from a diverse range of examples and patterns in 
the data. The validation set (13%), consisting of 607 images, is utilized 
during  training  to  fine-tune  hyperparameters  and  assess  the  model's 
performance on unseen data, helping to prevent overfitting and ensuring 
generalization. Lastly, the testing set (5%), which included 260 images, 
serves  as  an  independent  evaluation  of  the  model's  performance  on 
completely unseen data, providing a reliable measure of its real-world 
effectiveness and ability to generalize. 

2.3. Performance evaluation 

To evaluate the ablated model results, we use five metrics, namely 
Precision (Eq. (1)), Recall (Eq. (2)), mAP0.5 and mAP0.5:0.95, related to 
Eq. (3), Params(M), Flops(G), Inference(ms) and Time(h). 

Precision =

C2f2 diverges from C2f insofar as they have a different number of 
filters  in  the  bottleneck  blocks  but  have  a  similar  network  structure, 
Fig.  6.  Both  implementations  utilize  the  CSP  bottleneck  block,  which 
commonly incorporates two convolutions found in convolutional neural 
network architectures. The block consists of a 1 × 1 convolution layer, a 
3 × 3 depthwise convolution layer, and another 1 × 1 convolution layer. 
The input tensor is split into two equal parts, with the first part going 
through the first 1 × 1 convolution and being split again. The second 
part goes through bottleneck modules, which consist of two separable 
convolution layers and an optional shortcut connection. The two split 

parts and the outputs of the bottleneck modules are concatenated and 
passed through the second 1 × 1 convolution layer to produce the block 
output. An ‘n’ parameter determines the number of bottleneck modules; 
in this case, it is 1.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, the deep learning pipeline primarily uses image data format. During the preprocessing phase, techniques such as auto-orientation, resizing, tiling, and filtering are applied to standardize and enrich the dataset. The images are then adjusted to a uniform size of 640 x 640 pixels and contain substantial amounts of annotated data. Additionally, augmentation plays a crucial role in diversifying the dataset by introducing random saturation adjustments to each training example. Therefore, it can be concluded that the primary data format used in this deep learning pipeline is image data.
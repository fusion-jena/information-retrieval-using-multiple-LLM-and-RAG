Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Labeling Time 
Training Time 
Turnaround 

Time 
Accuracy 

Very Short 
1.0 h 

4 h 

89.4% 

Short 
6.9 h 

22 h 

94.0% 

Very Long 
~3.5 h 

94 h 

98%  

4. Discussions 

The  development  of  deep  learning  methodologies  continues  to 
advance at an astonishing rate and be applied to various applications 
ranging from biomedical (Azghadi et al., 2020), hydrological processes 
in river channels (Talukdar et al., 2023) and agricultural (Olsen et al., 
2019) systems, to marine (Laradji et al., 2021; Saleh et al., 2022b), and 
environmental (Jahanbakht et al., 2022a) sciences. The application of 
deep learning technologies has been also used in profiling the ecosystem 
services of estuarine habitats by community members (Yee et al., 2023). 
In this paper, we extend the application of deep learning methodologies 
to advance state-of-the-art underwater fish video processing techniques 
applied to turbid waters.

truly present. TNR (i.e., specificity and selectivity) is the portion of the 
no-fish frames that have been correctly classified. Conversely, FNR or 
miss rate and FPR or fall-out are respectively calculating the portion of 
fish  and  no-fish  frames  that  were  incorrectly  classified.  Sensitivity, 
specificity, miss rate, and fall-out are the most used metrics to measure 
the true and false classification probabilities. 

We also show that our semi-weakly-supervised model can be devel-
oped roughly 23.5 times faster than a fully-supervised DNN, at the cost 
of  nearly  a  9%  drop  in  its  accuracy.  This  tradeoff  is  4.3  times  faster 
development for a 4% accuracy degradation. The methods presented in 
this  paper,  therefore,  can  assist  marine  scientists  and  environmental 
managers in fast and improved fish detection and monitoring in turbid 
water conditions.

Despite not having access to a control clear water dataset to run our 
two  proposed  semi-  and  weakly-supervised  methods  on,  we  have 
compared them to a state-of-the-art fully-supervised method, which is 
for clear water. Compared to a fully-supervised underwater fish classi-
fication model that needs many hours of turnaround time from dataset 
collection to model deployment (as shown in Table 3Table 3), both our 
approaches  are  significantly  faster,  while  providing  slightly  reduced 
accuracies.  This  suggests  a  trade-off  between  development  time  and 
budget, and the accuracy required.

Gradient boosting refers to a class of ensemble Decision Tree (DT) 
algorithms  that  can  be  used  in  classification,  regression,  and  ranking 
applications (Badirli et al., 2020). However, gradient boosting leaves the 
width and depth dimensions of its DTs, along with the number of DTs in 
the ensemble adjustable. This leads to a vast number of hyperparameters 
that cannot be easily optimized for a given application. In this regard, 
XGBoost  was originally introduced  by Chu (2023) as an efficient and 
distributed gradient boosting algorithm with rapidly optimized hyper-
parameters.  XGBoost  outperforms  individual  DNN  models  when  it 
makes  the  final  decision  in  their  ensemble  (Shwartz-Ziv  and  Armon, 
2022).  It  is  therefore  beneficial  to  use  an  ensemble  of  DNNs  with 
XGBoost, which can perform better than any individual model, as well as 
other classical ensemble techniques (Shwartz-Ziv and Armon, 2022).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the provided context, there isn't explicit information about the criteria used to determine when training is complete. However, common practices in machine learning include using validation loss plateaus, where training stops once the validation loss stops decreasing significantly over several epochs. Other possible criteria could be reaching a predefined maximum number of epochs or achieving a target performance metric such as accuracy or precision. Without more specific details, it would be difficult to provide a definitive answer based solely on the given context.
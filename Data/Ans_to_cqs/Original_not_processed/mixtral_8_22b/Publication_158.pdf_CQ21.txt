Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

the ensemble results. All CNNs were trained using the TensorFlow backend on a Tesla
M100 graphic processing unit.

2.5.2. Random Forest

For random forest, input features included the mean and standard deviation for image
window sizes of 5 × 5, 15 × 15, and 32 × 32 in addition to the central pixel values for
each input feature. We used a random search to identify an optimal parameter set where
100 sets were sampled and compared using three-fold cross validation. Search parameter
ranges were number of trees (200 to 800, by 100), minimum samples split (2–4), minimum

Remote Sens. 2021, 13, 634

7 of 21

samples per leaf (2–4), maximum depth (full), bootstrap (true), and maximum features for
split (square root of the number of features). For a description of these parameters see the
Scikit-learn documentation [48]. These results were reﬁned using a grid-search with the
parameter ranges set to ±50 for the number of estimators and ±1 for the other parameters.

2.6. Membership Based Change Detection and Segmentation
2.6.1. Split Window Detection of Change Seed Points

Figure 2. Samples used for classiﬁer training and assessment. Grey circles were used for training, yellow triangles were
used for deep-learning model convergence, and red squares were used for independent accuracy assessment. Black outlines
identify the areas used for change assessment. The land-cover map is the 2010 land cover of Canada [40].

2.3. Landsat Processing and Ancillary Data

Classiﬁer

Ensemble
CNN

Random
Forest

Overall

F1-Score

Accuracy

Kappa

F1_Ave

Bare Grass

Pasture Water

Crop

Forest Wetland

Shrub

0.884

0.861

0.840

0.826

0.867

0.835

0.978

0.929

0.920

0.763

0.600

0.845

0.816

0.776

0.650

0.842

0.771

0.981

0.901

0.890

0.722

0.450

Remote Sens. 2021, 13, 634

11 of 21

Table 2. Error matrix for the ensemble CNN.

Predicted

Reference

Bare Grass

Pasture Water

Crop

Forest Wetland

Shrub

R_Sum UA

Bare

Grass

Pasture

Water

Crop

Forest

Wetland

Shrub

C_Sum

PA

145

5

8

0

11

1

0

1

4

828

92

1

3

2

0

3

8

92

1014

3

49

18

6

8

171

0.85

933

0.89

1198

0.85

0

0

2

887

1

2

9

0

901

0.98

15

16

56

1

1343

7

14

3

1455

0.92

3

5

17

1

0

1045

20

9

1100

0.95

4

21

19

18

27

64

391

11

555

0.70

1

10

22

1

1

32

30

99

196

0.51

180

977

1230

912

1435

1171

470

134

0.81

0.85

0.82

0.97

0.94

0.89

0.83

0.74

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training process, several postprocessing steps can be performed depending on the specific requirements of the project or study. Some common postprocessing steps include:

1. Model evaluation metrics: Calculating various performance metrics such as F1 score, overall accuracy, Kappa coefficient, etc., to evaluate the model's performance. In this case, the table shows the error matrix for the ensemble CNN, which includes the predicted vs reference classes, user's accuracy (UA), producer's accuracy (PA), and overall accuracy.

2. Confusion matrix: A confusion matrix is often created to visualize the performance of the classification algorithm. It provides insights into the types of errors made by the classifier. In this case, Table 2 presents the confusion matrix for the ensemble CNN.

3. Saliency maps: Saliency maps can be generated to highlight important regions within images that contribute significantly to the final prediction. This helps in understanding how the model makes its decisions. However, there is no mention of saliency maps in the given context.

4. Cross-validation: To ensure robustness and generalizability of the models, techniques like k-fold cross-validation can be employed during the training phase. Here, it was mentioned that a random search was conducted to find an optimal parameter set using three-fold cross-validation for the random forest model.

5. Hyperparameter tuning: Grid search or randomized search methods can be applied to fine-tune hyperparameters and improve the model's performance further. In this case, a grid search was utilized to refine the parameter ranges for the random forest model.
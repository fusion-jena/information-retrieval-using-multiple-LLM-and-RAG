Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We  trained  algorithms  with  the  amount  of  data  available  after 
manually  removing  the  testing  set,  as  described  above.  During  the 
training, 10% of data served as the validation set to check for overfitting. 
We used a min-max scaler reverted at the end to compute the perfor-
mance metrics in the test set. In all cases, we used a batch size of 64 
samples and trained the algorithm during 100 epochs using Adam as the 
algorithm  to  optimize  the  network  parameters.  Algorithms  were 
implemented in Pytorch (Paszke et al., n.d.) and models were trained 
and tested in Google Colaboratory.2

computing  performance  metrics.  To  avoid  bias  generated  by  random 
data removal, we keep the data sets with the removed data for use with 
all methods.

3.3. Comparison of DMF and state of the art 

In this experiment, the period analyzed, the configuration of DMF4, 
and its training are the same as those described in subsection 3.2. The 
MF model (Rivera-Mu˜noz et al., 2021) used here for comparison was 
tuned by employing a grid search. As a result of the tuning process, λ was 
set at 0.1, k = 360, and the number of iterations to 100. The imputation 
by EM algorithm is carried out using the function em of Python package 
impyute and eps parameter is set 0.1. For MICE, we used the function 
IterativeImputer  of  Python  package  sklearn  (Pedregosa  et  al.,  2011b) 
with  the  recommended  parameters  (BayesianRidge  as  estimator  and  a 
max_iter = 10).

For  training  DMF  model,  the  known  data  are  passed  through  the 
network as shown in Fig. 2. Each input x = [s, t, mst] is made up of: sensor 
index s, time index t and know observation mst. The purple, yellow and 
green  circles  depict  the  sensor  index,  temporal  index,  and  ith  know 
observation. Sensor index and time index are passed through embedding 
layers, and as a result, we obtain an embedding vector ei for each index, 
then all embedding vectors associated with ith  input are concatenated 
and passed to a set of hidden layers. Mathematically the hidden layers 
can be represented as a nonlinear function f(.) that maps the embedding 
vectors to the known measurements n of the sparse matrix M for sensor s 
and  time  t.  Therefore,  to  train  the  model,  the  inputs  can  be  easily 
mapped solving the following problem: 

L =

1
n

∑n

i=1

(mst (cid:0)

f (Θ, ei) )2

Θ = argmin

L .

Θ

(11)  

(12)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
During the training phase, a strategy was implemented to prevent overfitting and ensure optimal performance of the model. This involved setting aside 10% of the total dataset as a validation set. By doing so, the model could be evaluated on unseen data while it was still being trained, allowing for adjustments to be made if necessary. Additionally, a min-max scaler was applied before computing the performance metrics, which was then reversed once the evaluation was complete. This scaling technique helped normalize the data and improve the accuracy of the results. Furthermore, the training was conducted using a batch size of 64 samples and continued for 100 epochs, utilizing the Adam optimization algorithm to fine-tune the network parameters.
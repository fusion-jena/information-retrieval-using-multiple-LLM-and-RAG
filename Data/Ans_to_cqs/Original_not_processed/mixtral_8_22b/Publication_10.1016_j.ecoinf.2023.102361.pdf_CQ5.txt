Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.2. Split the datasets 

In this research, the datasets were randomly divided into three parts 
for training, validation and testing. Here, 60% of the data was used to 
train the deep learning models, and 20% of them was kept to validate the 
models. The remaining 20% data was used to evaluate the performance 
of the models. Table 1 show the number of data in the dataset and how 
they are separated for training, testing and validation. 

2.3. Deep learning models

diction  of  the  deep  learning  model.  This  dataset  was  taken  to  verify 
whether our proposed approach can handle that issue. The dataset was 
shared  by  Jiang  et  al.  (2020)  through  the  Github  repository: 
https://github.com/zhangchuanyin/weed-datasets.

VGG16 and VGG19 are classical architectures that are well known 
for  simplicity  and  uniformity.  These  models  are  suitable  for  smaller 
datasets and can provide better accuracy by fine-tuning the pre-trained 
network (Sukegawa et al., 2020). The models have several drawbacks, 
such  as  vanishing  gradient  problems  and  loss  of  fine-grained  spatial 
information (Pan et al., 2020). On the other hand, ResNet-50 contains 
residual  connections,  which  can  overcome  the  vanishing  gradient 
problem and enable training very deep networks. The model performs 
well on both large and small datasets (Al-Masni et al., 2020).

3. Results 

In this section, we present the experimental results of our proposed 
approach and compare them with the traditional approach and the re-
sults achieved by the previous studies on all the datasets. 

3.1. Traditional pipeline 

We first trained the models using the original images only, without 
any  data  augmentation.  Table  2  summarises  the  accuracy,  precision, 
recall and F1 score of the ten models. There is a significant variation 
between  the  models  regarding  the  number  of  parameters  and  model 
depth.  Overall,  the  DenseNet  models  achieved  the  best  results  on  all 
datasets. DenseNet169 and DenseNet201 attained the same accuracy of 
95.01% for the DeepWeeds dataset, but the precision, recall and F1 score 
indicates that DenseNet169 is better than DenseNet201. The dataset has 
inter-class  similarity  and  intra-class  dissimilarity  issues.  That  affected 
the classification accuracy. Olsen et al. (2019) also mentioned that there

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention the specific datasets used in the deep learning pipeline. However, it mentions that the dataset was shared by Jiang et al. (2020) through the GitHub repository: https://github.com/zhangchuanyin/weed-datasets. Therefore, one of the datasets used could be from this source. Additionally, the text refers to the DeepWeeds dataset, indicating that this dataset was also used in the study. Unfortunately, no further information about the datasets is provided in the given context.
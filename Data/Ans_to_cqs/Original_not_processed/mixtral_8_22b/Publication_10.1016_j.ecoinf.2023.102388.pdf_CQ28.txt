Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.3. Target and attack models 

We adopted two popular neural network architectures as our target 
model:  ResNet50  (He  et  al.,  2016b)  and  DenseNet121  (Huang  et  al., 
2017).  

• ResNet50. The ResNet50 architecture contains 50 layers and uses a 
stack of three layers with 1 × 1, 3 × 3 and 1 × 1 convolutions as the 

Fig.  7. Distribution  of  the  dataset  for  the  experimental  setting.  The  main 
dataset is split in 1
3 of individuals (e.g., 60 individuals in each subset: target set, 
attack set and evaluation set for GREMM dataset). The arrows indicate that the 
same individuals and pictures were re-used from one dataset to another.

Traditional CNN-based identification methods usually operate under 
the  assumption  that  the  database  is  labeled  with  known  classes.  In 
contrast,  Maglietta  and  collaborators  (Maglietta  et  al.,  2020)  have 
introduced  techniques  that  allow  for  identification  without  being 

EcologicalInformatics79(2024)1023883V.M. Araújo et al.

Villon, S., Iovan, C., Mangeas, M., Vigliola, L., 2022. Confronting deep-learning and 
biodiversity challenges for automatic video-monitoring of marine ecosystems. 
Sensors 22, 497. 

Yeom, S., Giacomelli, I., Fredrikson, M., Jha, S., 2017. Privacy risk in machine learning: 
analyzing the connection to overfitting. In: 2018 IEEE 31st Computer Security 
Foundations Symposium (CSF). https://doi.org/10.48550/ARXIV.1709.01604. URL: 
https://arxiv.org/abs/1709.01604.  

Yoshihashi, R., Shao, W., Kawakami, R., You, S., Iida, M., Naemura, T., 2019. 

Classification-reconstruction learning for open-set recognition. In: Proceedings of the 
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4016–4025. 

EcologicalInformatics79(2024)10238817

• No-overfitting. In this setting, the optimization algorithm of CNNs is 
Stochastic Gradient Descent (SGD), with a learning rate of 0.0001 
and a weight decay of 0.5. The batch size is set to 32, the number of 
training epochs to 200 and finally the batch-norm and dropout (0.5) 
are used to reduce the overfitting level.  

• Overfitting.  We  use  the  same  hyperparameters  setting  as  the  no- 
overfitting  but  we  remove  the  use  of  batch-norm,  weight  decay 
and dropout techniques to ensure that the model overfits.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which platform was used to deploy the deep learning model (e.g., AWS, Azure, Google Cloud platform)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information about which platform was used to deploy the deep learning model. Therefore, it is not possible to determine whether AWS, Azure, Google Cloud Platform or any other platform was used for deployment based on the available information.
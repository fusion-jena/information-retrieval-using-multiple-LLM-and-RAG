Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

|  1075

F I G U R E   1  Example of the set-up of 
the automated collection of training data 
in the wild and in captivity. (a) Pi camera 
(circled in red) positioned to record the 
back of the birds. (b) Example of a picture 
in the sociable weaver training data. 
(c) Example of a picture in the great tit 
training data. (d) Example of a picture in 
the zebra finch training data

which  had  a  Pi  camera  (we  used  Pi  camera  V1  5mp  and  V2  8mp). 

applying an image classification algorithm in other contexts. For the 

When the RFID data logger detected a bird, it sent the individual's 

sociable weaver population, we collected images for 15 days during 

PIT-tag  code  to  the  Raspberry  Pi,  which  was  programmed  to  then 

November and December 2018. For the great tit population, we col-

take a picture. Because birds often spend some time on the feeder, 

lected images over 7 days during the last 2 weeks of August 2019.

be frozen in order to mitigate overfitting, this is especially important 

is  oversampling,  which  consists  of  creating  copies  of  the  training 

when the training datasets are small. However, as freezing the layers 

data from the less sampled classes.

prevent them from update their weights during the training process 

We applied limited oversampling to our training dataset only. For 

(and therefore could prevent the model from learning key features 

nine sociable weavers and six great tits for which we did not have 

for performing the classification task) and considering the size of our 

1,000 images, we first selected 100 images for the validation dataset 

training datasets, we decided to train the models without freezing 

and then duplicated (through oversampling) the remaining pictures 

any  of  the  layers  of  the  network.  The  fully  connected  part  of  the 

until 900 images were available for the training dataset (Buda, Maki,

classes (in our case, different individuals). The validation dataset is 

(Dutta & Zisserman, 2019). Since manually labelling the regions of 

an independent set of samples that is used to compute the accuracy 

interest is time-consuming, we started by training the model for 10 

and loss (estimation of the error during training) of the model. This 

epochs (i.e. passing the entire dataset through the neural network 

validation dataset is used to assess the learning progress of the neu-

10  times)  with  200  manually  labelled  pictures.  If  the  model  was 

ral network. As the network never trains on or sees the validation 

found to perform badly, additional pictures were manually labelled 

data, this validation dataset can indicate if the model is overfitting 

and added to the training dataset. This process was repeated until 

the training data and not learning features that are key for recogniz-

a  satisfactory  performance  was  achieved.  For  the  great  tits,  we

ing the individuals. It is generally difficult to anticipate the minimum 

needed  500  pictures  in  the  training  data  and  125  for  validation 

number of images needed from each individual to obtain high per-

(see  Section  2.3  below  for  explanation  on  training  and  validation 

formance for individual recognition. As a compromise between the 

datasets). The zebra finch data required 400 pictures for training 

number of birds that we could include in our study and the number 

and 100 for validation.

of images per bird (i.e. to avoid generating an excessively imbalanced 

For the sociable weavers and the great tits, if the Mask R-CNN 

dataset), we aimed to use 1,000 images per bird—900 images for the 

identified more than one bird perching simultaneous at the RFID an-

training dataset and 100 images for the validation dataset. Training 

tenna, we automatically excluded that image. We detected a total of

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: After the model training, several postprocessing steps can be taken to evaluate the performance of the trained model. These may include calculating metrics such as accuracy, precision, recall, F1 score, etc., to measure how well the model has learned to classify the input data. Another common step is to create confusion matrices, which provide a visual representation of the correct and incorrect predictions made by the model. Additionally, saliency maps can also be generated to highlight the most relevant parts of the input data that contributed to the final prediction. In the given context, no specific postprocessing steps are mentioned; however, based on general practices, these steps would likely be performed to evaluate the effectiveness of the trained model.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

hyperparameters  in  a  model.  It  identifies  the  most  effective  set  of 
hyperparameters  that  enhance  the  model's  performance.  During 
training,  GSCV  generates  a  series  of  models,  each  characterized  by  a 
different  set  of  hyperparameters.  The  purpose  of  this  technique  is  to 
systematically train and assess the performance of these various models 
through cross-validation. The process concludes with the selection of the 
model that demonstrates the most superior performance, determined by 
its optimal hyperparameter combination (Adnan et al., 2022). Details of 
the primary tuning parameters, their range and the optimal combination 
of  hyperparameters  utilized  for  each  algorithm  for  each  model  are 
shown in Table 1.

EcologicalInformatics80(2024)1024795K. Ayushi et al.                                                                                                                                                                                                                                  

Table 1 
Hyperparameters tuned for each algorithm with their ranges and optimal values for the study.  

Algorithms 

Random forest 

Multivariate adaptive regression splines 

Penalized regression 

Support vector machine 

Gradient boosting 

Artificial neural network 

k-Nearest Neighbors 

Tuned Parameter 

Parameter Range  MODEL 1  MODEL 2  MODEL 3  MODEL 4  MODEL 5  MODEL 6  MODEL 7 

ntree 
mtry 
degree 
nprune 
alpha 
lamda 
cost 
sigma 
epsilon 
shrinkage 
interaction.depth 
n.minobsinnode 
n.trees 
Hiddenlayer 
Neurons_per_layer 
threshold 
k 

100–1000 
2–35 
1–3 
3–70 
0–1 
0.001–10 
0.01–100 
0.01–100 
0.01–1 
0.01–0.2 
1–6 
1–20 
100–1000 
1–5 
5–100 
0.01–10 
3–19

Friedman, J.H., 2002. Stochastic gradient boosting. Comput. Stat. Data Anal. 38, 

367–378. https://doi.org/10.1016/S0167-9473(01)00065-2. 

Fritsch, S., Guenther, F., Guenther, M.F., 2019. Package ‘neuralnet’. In: Training of 

Neural Networks, 2, p. 30. 

Gamble, J.S., Fischer, C.E.C., 1915–1935. Flora of the Presidency of Madras, Vol. 1–3, 

21, Hart Street, W.C.  

Gascon, F., Ramoino, F., Deanos, Y., 2017. Sentinel-2 Data Exploitation with ESA’s 

Sentinel-2 Toolbox, 19. EGU Gen. Assem, p. 19548 [Google Scholar].  

Ghasemi, N., Sahebi, M.R., Mohammadzadeh, A., 2011. A review on biomass estimation 
methods using synthetic aperture radar data. Int. J. Geomat. Geosci. 1 (4), 776–788 
[Google Scholar].  

Gholamy, A., Kreinovich, V., Kosheleva, O., 2018. Why 70/30 or 80/20 relation between 

training and testing sets: a pedagogical explanation [Google Scholar].

Tuning algorithm hyperparameters is an essential step in modeling 
and is also critical for optimizing AGB estimation performance (Zhang 
et al., 2020). With numerous correlated remote sensing predictors, high- 
dimensional  inputs  can cause overfitting  and  multicollinearity (Marrs 
and Ni-Meister, 2019). Applying techniques like dimensionality reduc-
tion  and  regularization  before  modeling  is  essential  to  mitigate  these 
issues and improve robustness when integrating multiple satellite data 
sources  (Schumacher  et  al.,  2016;  Zhang  et  al.,  2020).  For  instance, 
Bulut  (2023)  used  least  absolute  shrinkage  and  selection  operator 
(LASSO)  technique  to  reduce  dimensionality  and  Singh  et  al.  (2022) 
used  Boruta  algorithm  (Kursa  and  Rudnicki,  2010)  to  address  multi-
collinearity  among  predictor  variables.  While  many  studies  have 
focused on predicting AGB using MLAs and remote sensing (Adhikari

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The table provided does not contain information about the hyperparameters used in a deep learning model such as learning rate or optimizer. However, it does show the hyperparameters used for other algorithms including Random Forest, Multivariate Adaptive Regression Splines, Penalized Regression, Support Vector Machine, Gradient Boosting, Artificial Neural Network, and k-Nearest Neighbors. These hyperparameters include ntree, mtry, degree, nprune, alpha, lamda, cost, sigma, epsilon, shrinkage, interaction.depth, n.minobsinnode, n.trees, Hiddenlayer, Neurons\_per\_layer, threshold, and k. Each parameter has its own range and optimal value for different models. Unfortunately, without additional information, I cannot provide specific details about the hyperparameters used in a deep learning model.
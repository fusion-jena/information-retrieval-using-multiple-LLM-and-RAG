Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The  models  were  trained  for  100  epochs  using  Adam  optimizer 
(Kingma and Ba, 2017) with the default hyper-parameters and a cosine 
decay  scheduler  for  the  learning  rate.  The  loss  function  was  cross- 
entropy for multi-class single-label classification. The macro-averaged 
f1-score  on  the  validation  set  aided  in  monitoring  the  model  conver-
gence. Macro averaging gives equal importance to all classes regardless 
of the number of data points in each one.

Recent years have seen impressive progress in bird species recogni-
tion and related tasks (Stowell, 2022; Xie et al., 2023). It was foreseeable 
that  deep  learning  would  emerge  as  the  most  successful  method  for 
large-scale  bioacoustics  analysis  (Kahl  et  al.,  2021;  Stowell,  2022; 
Stowell et al., 2019; Tang et al., 2023; Xie et al., 2023; Zsebok et al., 
2019). The main advantage of neural networks is to avoid many brittle 
and  difficult  choices  in  designing  manual  features  for  the  underlying 
task. For example, Xie et al. (Xie and Zhu, 2019) compared three types of 
features  for  bird  species  classification  consisting  of  hand-crafted 
acoustic features, including features from Mel Frequency Cepstral Co-
efficient (MFCC), visual features extracted from the Constant-Q Trans-
form (CQT) time-frequency representation, and training a convolutional 
network directly on the CQT. They also tested the fusion of classifiers

trained on these features and found a slight performance improvement. 
However, training a neural network directly on the CQT was, by far, the 
best  choice  among  the  features.  The  other  successful  contemporary 
works  have  explored  various types  of neural networks as  species rec-
ognizers by leveraging Short-Time Fourier-Transform (henceforth STFT 
or  spectrogram)  (Kahl  et  al.,  2021)  and  its  relatives,  such  as  mel- 
filterbanks  applied  on  STFT  (henceforth  mel-spectrogram)  (Zhang 
et al., 2019) and MFCC (Tang et al., 2023) as input feature.

It  seems  to  require  an  exhaustive  knowledge  of  each  species’  fre-
quency  niche  to  create  optimal  individualized  features.  Yet,  the  most 
celebrated capability of deep neural networks is automatic and task(s)- 
relevant feature extraction while doing the classification task(s) jointly, 
which  has  led  to  unprecedented  achievements  (LeCun  et  al.,  2015). 
Hence, one could ask whether to consider a time-frequency represen-
tation or the waveform as raw input to neural networks to capture all the 
relevant information from bird vocalization. Although the former is the 
prevalent  approach,  data  processing  inequality  indicates  that  post- 
processing can not increase a signal’s information (Beaudry and Ren-
ner,  2011).  Therefore,  the  waveform  is  a  promising  alternative  to 
traditional features derived from it when leveraging neural networks.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, the deep learning pipeline uses the Adam optimizer (Kingma and Ba, 2017) with default hyperparameters. Additionally, a cosine decay scheduler is employed for adjusting the learning rate during training. These optimization techniques contribute to improving the efficiency and effectiveness of the deep learning process.

Adam (short for Adaptive Moment Estimation) is a popular optimization algorithm used in deep learning due to its ability to adaptively update the learning rates based on first and second moments of gradients. This makes it well suited for handling noisy or sparse gradients, which often occur in large-scale machine learning problems. By combining the advantages of both RMSprop and Stochastic Gradient Descent (SGD) with momentum, Adam provides efficient convergence properties and robustness against different hyperparameter settings.

Apart from choosing an appropriate optimizer like Adam, another common practice in deep learning pipelines is employing learning rate schedules. In this case, a cosine decay schedule is utilized. Learning rate schedules help improve the stability and speed of the training process by gradually reducing the learning rate over time. A cosine decay schedule decreases the learning rate according to a cosine curve, allowing for smoother transitions between learning phases and potentially better generalization performance.
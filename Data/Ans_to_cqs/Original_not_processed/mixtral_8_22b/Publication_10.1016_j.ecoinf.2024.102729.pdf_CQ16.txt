Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

September to November (91 samples with 3 missing values).   

2.2. Statistical Analysis 

The  framework  followed  in  this  study  comprised  the  following  steps  (Fig.  2):  First,  we 

selected the input variables to develop RF models based on the maximal information coefficient 

Journal Pre-proof

(MIC) values. Second, the datasets were split into training and testing (or validation) subsets, and 

then RF  models  were built  for  all  the periods using  the  training  datasets, and their  performance 

was  subsequently  evaluated  against  the  testing  datasets.  Next,  we  removed  the  weakest  input 

factors (or predictors) through feature importance analysis and re-evaluated the performance of the 

RF  models.  Finally,  we  selected  the  optimal  RF  models  based  on  performance  evaluation,  and 

characterized  the  relative  importance  of  environmental  factors  among  different  periods.  The

study,  the  dataset  was  randomly  divided  into  two  subsets,  the  first  one  was  used  as  a  training 

dataset  to  learn  the  optimal  model  structure  and  the  second  as  a  validation  dataset  to  test  its 

performance;  75%  of  the  data  were  devoted  to  model  training  and  25%  to  testing.  Given  the 

well-documented robustness of RF algorithms to randomly receive training data from subsets and 

establish models with high predictive capacity, we opted for random (instead of cluster) sampling 

with  one  major  condition  to  maintain  the  covariance  structure  among  the  predictor  variables 

relatively intact between the training and testing datasets. 

,(),,,(,)maxlogmin(,)xyBnIXYxyMICXYxy),,,(*yxYXI 
Journal Pre-proof

RF  modeling  can  generally  be  resilient  to  preprocessing  burdens  (e.g.,  no  feature  scaling, 

robustness  to  outliers),  flexible  with  multi-dimensional  data,  sensitive  in  elucidating  complex

(Poyang Lake, China). Journal of Hydrology 585, 124810.   

Lu,  Y.,  Tuo,  Y.,  Xia,  H.,  Zhang,  L.,  Chen,  M.,  Li,  J.,  2023.  Prediction  model  of  the 

outflow  temperature  from  stratified  reservoir  regulated  by  stratified  water  intake  facility 

based on machine learning algorithm. Ecological Indicators 154, 110560.   

Nelson, N.G., Munoz-Carpena, R., Phlips, E.J., Kaplan, D., Sucsy, P., Hendrickson, J., 

2018. Revealing biotic and abiotic controls of harmful algal blooms in a shallow subtropical 

lake  through  statistical  machine  learning.  Environmental  Science  &  Technology,  52, 

3527-3535.   

Oliver,  R.L.,  Mitrovic,  S.M.,  Rees,  C.,  2010.  Influence  of  salinity  on  light  conditions 

and phytoplankton growth in a turbid river. River Research and Applications 26, 894-903.   

Ouyang,  S.,  Chen,  Z.,  Chen,  S.,  Zhao,  J.,  2023.  Prediction  of  Freezing  of  Gait  in

the Nanji wetland of Poyang Lake.   

Fig.  3:  Maximal  Information  Coefficient  values  between  the  nine  environmental  covariates  and 

chlorophyll  a  concentrations  for  (a)  the entire  study  period,  using  eleven  time  lags  (0-10  days), 

and (b) each of the four seasons.     

Fig.  4:  Random  forest  performance,  as  measured  by  adj_R2,  RMSE,  MAE,  and  KGE,  with  the 

training and testing/validation datasets for different time lags. The gray columns and orange line 

represent  the  adj_R2  and  KGE  values,  while  the  blue  solid  and  dashed  lines  correspond  to  the 

RMSE and MAE values, respectively.   

Fig. 5: Predictions of Chl a concentrations by the optimal RF model per season. 

Fig. 6: Relative importance of the predictor variables included in the optimal random forest model 

Journal Pre-proof

Fig. 7: GAM analysis: Non-linear relationships between the two key influential factors and Chl a

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, there isn't explicit mention of specific criteria used to determine when training is complete. However, it does provide some insight into how the dataset was divided for training and validation purposes. It states that 75% of the data was devoted to model training and 25% to testing. This implies that the training process might have been completed once the model had learned from the 75% of the available data. Additionally, since the study mentions evaluating the performance of the RF models against the testing datasets, it suggests that the completion of training could also depend on achieving satisfactory performance metrics such as adj\_R2, RMSE, MAE, or KGE. These metrics are mentioned in relation to Fig. 4, which shows the performance of the random forest model with both training and testing/validation datasets for different time lags. Therefore, although not explicitly stated, it can be inferred that the training process may be considered complete when the model has achieved acceptable performance levels on the validation set.
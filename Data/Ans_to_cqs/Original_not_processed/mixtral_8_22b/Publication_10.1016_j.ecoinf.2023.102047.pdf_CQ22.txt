Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.4. Performance evaluation 

The most common performance evaluation metric in image classifi-
cation or object detection is average precision and recall. In this study, 
we  have  used,  mean  average  precision  (mAP)  (Felzenszwalb  et  al., 
2010), which consists of a precision, recall and mean. Precision refers to 
the ratio of correctly classified positive samples (True Positive) to the 
total  number  of  classified  positive  samples  (either  incorrectly  or 
correctly).  The  recall  rate  refers  to  the  ratio  between  the  number  of 
positive  samples  (True  Positive)  correctly  classified  as  Positive  to  the 
total number of Positive samples. 

Precision =

TP
TP + FP  

Recall =

TP
TP + FN

(of 94.5%) using the deep learning–based CNN model. Martin-Abadal 
et  al.  (Martin-Abadal  et  al.,  2018)  estimated  seagrass  meadows  by 
applying  an  encoder  decoder-based  model  (VGG16FCN8).  They  ach-
ieved a precision of 96.57% and an accuracy of 96.81% on their dataset. 
Meanwhile,  Weidmann et  al.  (Weidmann et  al., 2019) found  that the 
deep  learning–based  DeepLabv3+ model  achieves  the  highest  perfor-
mance  (with  a  mean  intersection  over  union  (mIoU)  of  87.78%) 
compared  to  the  fully  convolutional  networks,  dilated  convolution 
networks (dilNets) and U-Net on the ‘LookingForSeagrass’ dataset.

0.3464  on  the  ECUHO-1  and  ECUHO-2  datasets  respectively.  More 
recently, Noman et al. (Noman et al., 2021b) proposed using Faster R- 
CNN with the NASNet model and achieved the highest mAP of 0.291 and 
0.412 on these H. ovalis datasets respectively.

mAP for that detection box’s recall. Proposed detector-1 had better mAP 
than the Faster R-CNN with Inception V2 for both detection box preci-
sion  metrics  with  0.75  IoU  and  detection  box  recall  with  1  and  10. 
However, this detector had a slightly lower mAP than the Faster R-CNN 
with the Inception V2–based model.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the field of image classification or object detection, the most common performance evaluation metrics are average precision and recall. Specifically, in this study, the authors use mean average precision (mAP) as the primary metric. This metric comprises three components: precision, recall, and mean.

Precision refers to the ratio of correctly classified positive samples (True Positives) to the total number of classified positive samples (either incorrectly or correctly). It can be calculated using the formula:

Precision = TP / (TP + FP)

where TP stands for True Positives and FP represents False Positives.

On the other hand, the recall rate refers to the ratio between the number of positive samples (True Positives) correctly classified as Positive to the total number of Positive samples. Recall can be computed using the following equation:

Recall = TP / (TP + FN)

Here, TP denotes True Positives, while FN signifies False Negatives.

Additionally, the authors mention another metric called mean Intersection over Union (mIoU), which was employed by Weidmann et al. (2019) to compare different models. However, it appears that mAP remains the main metric used throughout the study.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1 shows the modified GoogleNet model with
the batch normalization added outside the Inception
modules. Just like the original GoogleNet, the model is
comprised of several inception modules, however Batch
Normalization is added inside each inception module for
faster convergence right after each pooling layer. Figure 1
shows how the modified Inception module is comprised.
The model was implemented by using the Caffe frame-
work [37]. A batch size of 16 images was used for each
iteration, with a learning rate of 0.0075 with images of
224×224 resolution. Simple crop and resize data augmen-
tation was used with the default settings of Caffe.

Experiments and results
All experiments measured the top-1 and top-5 accu-
racy of the trained deep learning model under different
circumstances, i.e., herbarium specimens classification
(“Herbarium specimen classification” section, Table 3),
transfer learning across herbarium data from differ-
ent regions (“Cross-Herbaria transfer learning” section,
Table 4), and transfer learning from herbarium data to
non-dried plant images (“Transfer learning from herbar-
ium to non-dried plant images” section, Table 5).

For each of these experiments, table columns are

defined as follows:

119M

170M

1072K

54M

1388K

71M

1000K

1M

the network. Therefore, a common practice is to initialize
the network by pre-training it on a big available dataset
and then fine-tune it on the scarcer domain-specific data.
Concretely, the methodology we used in our experiment
for transferring knowledge from dataset A to dataset B is
the following:

1. The network is first trained from scratch on dataset
A by using a multinomial logistic regression on top
of the SOFTMAX layer and the linear classification
layer.

2. The linear classification layer used for dataset A is
then replaced by a new one aimed at classifying the
classes in B. It is initialized with random weights.

3. The other layers are kept unchanged so as to

initialize the learning of dataset B with the weights
learned from A.

4. The network is trained on the images in B.

Discussion and conclusions
This study is, to our knowledge, the first one that ana-
lyzes a big dataset with thousands of specimen images
from herbaria and uses deep learning. Results show the
potential of deep learning on herbarium species identifi-
cation, particularly by training and testing across different
herbarium datasets. This could potentially lead to the cre-
ation of a semi, or even fully, automatic system to help

Jose et al. BMC Evolutionary Biology  (2017) 17:181 

Page 12 of 14

Fig. 7 Comparison of losses of R.PC.PC, I.PC.PC and H1K.PC.PC experiments

taxonomists and experts do their annotation, classifica-
tion, and revision work at herbarium.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which platform was used to deploy the deep learning model (e.g., AWS, Azure, Google Cloud platform)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Answer: The text does not provide information about which specific cloud platform was used to deploy the deep learning model. However, it mentions that the model was implemented using the Caffe framework.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Usoltsev, V.A., Lin, H., Shobairi, S.O.R., Tsepordey, I.S., Ye, Z., Anees, S.A., 2022. The

forest biomass carbon stocks from space. Curr. For. Rep. 3, 1–18.

Ryu, S.-E., Shin, D.-H., Chung, K., 2020. Prediction model of dementia risk based on

XGBoost using derived variable extraction and hyper parameter optimization. IEEE
Access 8, 177708–177720.

Sa, R., Fan, W., 2023. Estimation of Forest parameters in boreal artificial coniferous

forests using Landsat 8 and sentinel-2A. Remote Sens. 15, 3605.

Salinas-Melgoza, M.A., Skutsch, M., Lovett, J.C., 2018. Predicting aboveground forest

biomass with topographic variables in human-impacted tropical dry forest
landscapes. Ecosphere 9, e02063.

Shahzad, F., Mehmood, K., Hussain, K., Haidar, I., Anees, S.A., Muhammad, S., Ali, J.,

Adnan, M., Wang, Z., Feng, Z., 2024. Comparing machine learning algorithms to
predict vegetation fire detections in Pakistan. Fire Ecol. 20 https://doi.org/10.1186/
s42408-024-00289-5.

MLR

SVR

XGBoost

RF

Linear
regression
method

Support vector
machine for
regression
Gradient
boosting
algorithm
Ensemble
learning method

Intercept (b0), Coefficient
Estimates, R-squared,
adjusted R-squared, and
significance levels of
coefficients.

Kernel type,
C regularization parameter)

Learning rate, Number of
trees (n), Max depth

Number of trees (n), Max
features, Max depth

(L´opez-Serrano
et al., 2019; Luo
et al., 2021a)

(Bulut, 2023; Ge
et al., 2022b; Sun
et al., 2019)
(Luo et al., 2021b;
Tamiminia et al.,
2022)

(Chen et al.,
2019a; Nguyen
and Kappas, 2020)

3. Results

3.1. Relationship between Landsat 9 OLI and SRTM DEM based
predictors and field-measured biomass

43.24
42.70
32.46
29.77
44.51
31.46
29.20
27.82
44.87
25.24
23.85
22.23
43.89
30.78
26.53
25.61

MAE

29.67
26.38
19.50
17.15
30.26
16.68
16.42
15.48
30.97
13.24
12.73
11.73
30.06
16.40
14.87
14.38

RMSE

35.42
34.97
26.59
24.38
36.45
25.75
23.90
22.79
36.75
20.67
19.54
18.21
35.95
25.21
21.72
20.97

Parameters

C1=1, Sigma2 = 10
max_depth3 = 7, eta4 = 0.3
max_depth = 25, n_estimators5 = 1000

C = 10, Sigma = 10
max_depth = 7, eta = 0.3
max_depth = 25, n_estimators = 1000

C = 1, Sigma = 10
max_depth = 7, eta = 0.3
max_depth = 25, n_estimators = 1000

C = 1, Sigma = 10
max_depth = 7, eta = 0.3
max_depth = 25, n_estimators = 1000

1 “C” is the regularization parameter.
2 Bandwidths.
3 Maximum depth of a tree.
4 Learning rate.
5 Number of Trees.

EcologicalInformatics82(2024)1027329S.A. Anees et al.

be easily replicated in other studies or regions.

3.2. Evaluation of different predictor combinations and machine learning
algorithms

3.2.1. Variable combinations and model comparison

parameter combination, enhancing the model’s performance. Important
indices to measure variable importance in XGBoost include “Gain” and
“Frequency” (Gertz et al., 2020; Li et al., 2023; Luo et al., 2024; Meh-
mood et al., 2024a; Ryu et al., 2020). SVR tuning involves selecting the
appropriate kernel function and optimizing the “C” parameter, which
balances margin width and misclassification tolerance. The versatility of
SVR allows it
to handle complex decision boundaries effectively
(Yudong Li et al., 2020; Rodriguez-Galiano et al., 2015).

2.9. Performance evaluation of the models

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The completion of training can be determined by various criteria depending on the specific algorithm being used. However, some common criteria include reaching a predefined number of epochs, achieving a desired level of accuracy or loss, or observing a plateau in the validation loss. In gradient boosting algorithms like XGBoost, early stopping rules can also be employed to halt training once the improvement in validation error falls below a certain threshold. Additionally, monitoring metrics such as precision, recall, and F1 score during cross-validation can provide insights into the optimal point to stop training. Ultimately, the choice of criteria will depend on the goals and constraints of the particular application.
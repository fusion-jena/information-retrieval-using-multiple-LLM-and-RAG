Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

950 (4%) 
1110 (4%) 

Total 

24,675 
32,986  

passes over the entire training set) and a batch size of 64, with an ADAM 
(cid:0) 5. Image augmentation was applied 
optimiser and a learning rate of 10
in the form of horizontal flip, 0.2 degree counter clock wise shear and a 
random zoom between 0 and 0.2 - all leading to 224 × 244 pixel RGB 
input tensors. Data was normalised to ImageNet mean values, and the 
pixels values were rescaled in the range of [0, 1]. Model training took 
roughly 4 days. The best model was selected based on minimal valida-
tion loss that occurred at epoch 448. This model showed a training loss 
of 0.256, a training accuracy of 0.899, a validation loss of 0.298 and 
validation accuracy of 0.891. We evaluated the red kite model perfor-
mance based on an independent test set of 2060 images (as described in 
3.3). 950 of these images were true positive red kites images and the

Birds 66 (183–224), 241–269. 

Di Minin, E., Fink, C., Hiippala, T., Tenkanen, H., 2019. A framework for investigating 
illegal wildlife trade on social media with machine learning. Conserv. Biol. 33 (1), 
210–213. https://doi.org/10.1111/cobi.13104. 

eBird., 2021. Ebird: An Online Database of Bird Distribution and Abundance. https:// 

media.ebird.org/catalog?taxonCode=redkit1%5C&mediaType=photo. 

Edwards, T., Jones, C.B., Corcoran, P., 2022. Identifying wildlife observations on twitter. 

Ecol. Inform. 67, 101500 https://doi.org/10.1016/j.ecoinf.2021.101500. 

ElQadi, M.M., Dorin, A., Dyer, A., Burd, M., Bukovac, Z., Shrestha, M., 2017. Mapping 
species distributions with social media geo-tagged images: case studies of bees and 
flowering plants in Australia. Ecol. Inform. 39, 23–31. https://doi.org/10.1016/j. 
ecoinf.2017.02.006.

5. Discussion 

In this study we developed a workflow which leveraged citizen sci-
ence data to extract further relevant records from social media posts in 
the  same  region.  The  workflow  functions  as  a  data  filter  enabling 
downsampling of an initially very large dataset into a human analysable 
subset - in our case containing 0.5% of the original posts. By massively 
reducing  data  volumes,  it  becomes  realistic  to  analyse  the  remaining 
data by hand to select true positives, with around one hour required for 
the  4000  or  so  candidate  posts  we  identified.  Our  workflow  thus  ad-
dresses  the  research  gap  identified  by  Burke  et  al.,  2022,  using  gen-
eralisable  methods  to  extract  target  data  from  various,  unverified 
sources to enrich data.

The two CNNs differ in size and complexity. The ResNet101 object 
detection model has double the amount of layers (101) but was used off 
the  shelf,  whereas  the  ResNet50  red  kite  model  was  trained  using 
transfer-learning on the citizen science data described in Table 2. These 
differences are important when interpreting the performance of the in-
dividual models within the workflow, since more layers are associated 
with higher precision at the cost of longer inference times and higher 
computational demand. 

The custom red kite model was trained using the Google Colab Pro 
infrastructure3  which  allows  Jupyter  notebooks  to  be  run  in  a  GPU 
enabled  runtime  environment  which  is  an  affordable  alternative  to 
acquiring  the  hardware  needed  for  model  training.  The  model  was 
trained with a NVIDIA TESLA T4® graphic card for 500 epochs (i.e. 500 

1  https://inaturalist.org/pages/apireference, accessed: 14.06.2021

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Unfortunately, there isn't enough information available to determine which specific deep learning framework was used to build the model. The given text does not mention any particular framework such as TensorFlow, PyTorch, Keras, etc. Therefore, I cannot provide a definitive answer to this query.
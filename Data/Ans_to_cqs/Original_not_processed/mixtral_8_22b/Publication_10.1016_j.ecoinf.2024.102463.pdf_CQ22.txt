Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

• CNN  window  size  (dim-filter-cnn)  ∈ [3, 4, 5, 6, 7].  The  convolution 
window size determines the size of the region over which convolu-
tion  is  applied  at  each  time  step.  It  is  important  to  choose  an 
appropriate window size to capture the relevant temporal patterns in 
the data.  

• the  number  of  filters  (nbr-filters-cnn)  ∈ [8, 16, 24, 32, 40, 48, 56, 64,
72, 80, 88, 96, 104, 112, 120, 128].The  number  of  filters  determines 
how many different patterns the network can learn. The higher the 
number of filters, the more complex the network can be, but this can 
also make training more difficult.

None. 

Data availability 

All methods were written in the Python language and in the PyCharm 
environment 
machine 
(https://www.jetbrains.com/pycharm/). 
learning  methods  were  developed  using  the  freely  available  Keras  li-
brary  (Chollet,  2018)  at  https://github.com/fchollet/keras  with  Ten-
sorflow backend. Free and open-source codes have been employed to 
perform  the  overall  analysis:  the  hyperparameters  ofthe  model  were 
optimised  using  the  custom  HyperBand  algorithm  which  allows  to 
optimize the network architecture with the optimal length of the history. 

8. Conclusion 

Acknowledgement

hyperparameters for model architectures.  

2.  Partial training: Models are partially trained (on a small fraction of 
the data) to quickly eliminate underperforming configurations.  
3.  Successive Halving: The remaining configurations are grouped into 
sets of different sizes, and the associated models are trained further. 
The best-performing configurations in each set are promoted to the 
next  stage,  while 
the  under-performing  configurations  are 
eliminated. 

4. Repeat: Steps 2 and 3 are repeated until only one configuration re-
mains, which is then considered the best configuration found.

Due to its structure, DA-RNN has much fewer hyperparameters than 
HRHN.The main hyperparameter is the dimension of the LSTM layers, 
the other hyperparameters are related to regularization (drop), to the 
dimensions of the data (batch-size and history length) and finally to the 
training plan. To determine these hyperparameters, the HyperBand al-
gorithm is also used. Table 4 shows the hyperparameters obtained. 

DA-RNN was trained under exactly the same conditions as HRHN: 
same  number  of  epochs,  choice  of  optimizer  and  same  training  base 
(Rhone-Vallabregues).  The  next  section  presents  the  results  obtained 
with HRHN and the DA-RNN model. 

7. Results 

7.1. Reminder of comparison tools and metrics 

To  assess  the  quality  of  the  predictions  obtained,  two  metrics  are 
added  added  in  addition  to  MAE  and  NSE:  RMSE  (root-mean-square 
error) and bias: 

√

̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅
∑T
t=1(̂yt (cid:0) yt)2
T

et Bias =

∑T

t=1(̂yt (cid:0) yt)
T

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Two metrics are used to evaluate the performance of the deep learning model: Root Mean Square Error (RMSE) and Bias. These metrics are used in addition to Mean Absolute Error (MAE) and Nash–Sutcliffe efficiency (NSE).

Root Mean Square Error (RMSE) measures the average magnitude of the error in the predicted values. It gives a measure of how spread out these residuals are. In contrast to the similar Mean Absolute Error, RMSE amplifies and severely punishes large errors.

Bias is a metric that measures the difference between the expected value and the actual value. It provides information about the systematic error or deviation from the true value. A positive bias indicates that the prediction tends to overestimate the observation, while a negative bias indicates that the prediction tends to underestimate the observation.
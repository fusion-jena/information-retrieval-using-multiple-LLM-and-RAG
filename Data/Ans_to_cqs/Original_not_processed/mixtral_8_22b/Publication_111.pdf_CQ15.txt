Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

5.5. GRU-DNN Hyperparameter Analysis. GRU-DNN
model is trained in a supervised learning fashion using lag
features (i.e., using K previous observations), where K
denotes the number of previous observations used in the
training and forecasting task. Typically, K is considered
as a hyperparameter that needs to be optimized. +ere-
fore, we performed a grid search method to obtain the
optimal K value. Figure 6 depicts the grid search for
diﬀerent values of K hyperparameter over search space
ranges from 1 to 15. Speciﬁcally, Figure 6(a) presents the
model performance for water temperature forecasting
using various K values, where K � 6 achieves the lowest
MAE error. Similarly, K � 4 is the optimal value for
signiﬁcant wave height
shown in
Figure 6(b). It is noteworthy that the experiments pre-
sented in Figure 7 are for the ﬁrst dataset of each fore-
casting problem, assuming that the rest of the datasets
have similar behaviour.

forecasting

as

13

[45] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov, “Dropout: a simple way to prevent neural
networks from overﬁtting,” Ee Journal of Machine Learning
Research, vol. 15, pp. 1929–1958, 2014.

[46] D. P. Kingma and J. Ba, “Adam: a method for stochastic

optimization,” 2014, https://arxiv.org/abs/1412.6980.

[47] J. Bergstra, D. Yamins, and D. D. Cox, “Hyperopt: a python
library for optimizing the hyperparameters of machine
learning algorithms,” in Proceedings of the 12th Python in
Science Conference, p. 20, Austin, TX, USA, 2013.

[48] M. Abadi, P. Barham, J. Chen et al., “Tensorﬂow: a system for
large-scale machine learning,” in Proceedings of the 12th
${$USENIX$}$ Symposium on Operating Systems Design and
Implementation (${$OSDI$}$ 16), pp. 265–283, Savannah,
GA, USA, November 2016.

+is paper aims to predict the SST and the SWH for the
Korea Hydrographic and Oceanographic Agency dataset.
+e proposed work is motivated by comparing the statistical,
machine learning, and deep learning models to understand
the performance gap of these models. +e results of this
work should provide scientiﬁc evidence on which model ﬁts
better the marine data. To anticipate the marine features
eﬃciently, the employed deep learning model combines the
gated recurrent units (GRU) with the regular neural net-
work. In the proposed architecture, the GRU layer is pre-
ceded by an input layer and followed by a fully connected
layer. As a result, the predicted values can be produced from
the output layer. To our knowledge, this is the ﬁrst use of a
GRU model architecture for forecasting SST and SWH.
Besides, four diﬀerent ML models have been utilized in the
current study, namely, Linear Regression (LR), Decision
Tree (DT), Support Vector Machine (SVM), and Random

uses the Mean Square Error (MSE) loss function, which is
provided by equation (5). +at is, the proposed GRU-DNN
model is trained with the goal of minimizing the loss
􏼉N
i�1 of N
function given a
training data
observations.

(Xi, Yi)
􏼈

min
w

1
N

N
􏽘
􏽮
j�1

Yj − F Xj, w
􏼐

􏼑

􏽯,

2

(5)

where w signiﬁes the network coeﬃcient, F: Rk ⟶ R1 is
the neural network ﬂow, and k denotes the size of the input
vector (i.e., number of lag features).

4.1.1. GRU-DNN Hyperparameter Optimization. +e opti-
mization of the proposed model hyperparameters is a part of
machine learning methods. +e model parameters (coeﬃ-
cients) utilized to govern the training task are as hyper-
parameters. Such parameters (e.g., learning rate, number of
lag order of ARIMA
layers/neurons of a network/layer,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text mentions two specific optimization techniques used in the deep learning pipeline: Dropout [45] and Adam [46]. Dropout is a technique used to prevent neural networks from overfitting by randomly dropping out neurons during training. This helps improve the generalization of the model. On the other hand, Adam is a method for stochastic optimization that uses adaptive estimates of lower-order moments to update the weights of the network. These techniques help improve the efficiency and effectiveness of the deep learning pipeline.
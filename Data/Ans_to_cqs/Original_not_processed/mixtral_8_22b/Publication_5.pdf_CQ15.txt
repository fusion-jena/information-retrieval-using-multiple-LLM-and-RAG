Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The training process was performed for a number of iterations in which all the training
data were exposed to the network until the loss function reached its minimum value. The
model score reached its maximum after approximately 5000 iterations with a NVIDIA
Titan X GPU (3584 CUDA cores). The number of trainable parameters was 85,569 and the
computational run-time was approximately 4 h with a training batch size of 1024.

Deep learning approaches have shown promising results in many scientiﬁc applica-
tions [67,68] but do not always guarantee better outcomes [69,70]. Therefore, we compared
the performance of the proposed DL model with a RF regression model as a baseline, since
RF showed reasonable results and is popular in various machine learning applications [71].
To determine the best values of the hyperparameters (number of trees, maximum tree
depths, and the maximum number of features) in the RF model, a grid search was used.

Neural Netw. Mach. Learn. 2012, 4, 26–31.

65. Chollet, F. Deep Learning with R; Manning Publications: Shelter Island, NY, USA, 2018.
66.

Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. Dropout: A simple way to prevent neural networks
from overﬁtting. J. Mach. Learn. Res. 2014, 15, 1929–1958.

67. Bhatnagar, S.; Gill, L.; Ghosh, B. Drone image segmentation using machine and deep learning for mapping raised bog vegetation

communities. Remote Sens. 2020, 12, 2602. [CrossRef]

68. Kim, Y.J.; Kim, H.-C.; Han, D.; Lee, S.; Im, J. Prediction of monthly Arctic sea ice concentrations using satellite and reanalysis data

based on convolutional neural networks. Cryosphere 2020, 14, 1083–1104. [CrossRef]

69. Korotcov, A.; Tkachenko, V.; Russo, D.P.; Ekins, S. Comparison of deep learning with multiple machine learning methods and

70.

71.

8 of 20

data, 5 hidden layers (Hj, j = 1, 2, . . . , 5) were used, and the number of neurons (n) in each
hidden layer was 64, 128, 256, 128, and 64, respectively. The hidden layers were stacked
one by one to transfer the input signals to the deeper layer, which could extract hidden and
unknown features related to species richness. We chose the RMSprop stochastic descent
optimizer with the default parameters [64], and a rectiﬁed linear unit (ReLU) as a nonlinear
activation function because of its promising performance in the literature [65]. Dropout
layers with a rate of 0.2 were added to each hidden layer to prevent model overﬁtting [66].
The L1 loss function, also known as the least absolute error, was used because it is not
sensitive to outliers and is intuitive. The detailed MLP model structure is described in
Figure 3a.

Due to the unique characteristics of neural networks, which solve problems by ex-
ploiting the hidden relationships inherent in multiple input variables, it was difﬁcult to
physically quantify the importance of the input variables. As an alternative, we performed
a statistical feature importance test (SFIT) to explain which feature had the greatest sig-
niﬁcance in the species richness retrievals and to determine the optimized features in
an operational retrieval system. For the SFIT, a single feature was randomly shufﬂed,
while all the other features were kept constant. We iterated this process by changing the
test variable. The feature importance shows the extent to which the model performance
decreased with random shufﬂing. In this study, we used the root mean square error (RMSE)
as the performance metric.

2.5. Independent Validation of Species Richness

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, the optimization technique applied in the deep learning pipeline is mentioned as the RMSprop stochastic descent optimizer. This optimizer is used with the default parameters. Additionally, dropout layers with a rate of 0.2 are added to each hidden layer to prevent model overfitting. However, specific details about the implementation of these techniques or their comparison with other optimization techniques like Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) are not provided in the given context.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(b) Computing infrastructure used to train both neural networks 

Infrastructure 
Search strategy 
Training duration 
(best assignment) 

Nvidia Tesla V100 (16GB) 
Manual tuning  
Segmentation 
Classification 

4 h 6 min 24 s 
9 h 22 min 34 s

Murua, J., Moreno, G., Itano, D., Hall, M., Dagorn, L., Restrepo, V., 2020c. ISSF Skippers 
Workshops. Technical Report Round 9. URL: https://iss-foundation. 
org/knowledge-tools/reports/technical-reports/. 

Qiao, M., Wang, D., Tuck, G.N., Little, L.R., Punt, A.E., Gerner, M., 2020. Deep learning 
methods applied to electronic monitoring data: automated catch event detection for 
longline fishing. ICES J. Mar. Sci. fsaa158 https://doi.org/10.1093/icesjms/fsaa158. 

Rathi, D., Jain, S., Indu, S., 2017. Underwater fish species classification using 

convolutional neural network and deep learning. In: 2017 Ninth International 

EcologicalInformatics67(2022)10149513X. Lekunberri et al.

Trained model 

-  Manually: 275 

segments  

-  Automatically: 
6921 segments  

-  Data 

augmentation: 
x12 

-  14,896 images  
-  Two trained 

models 

22 of the 30 
fishing sets 
(unannotated) 

Data of the 22 
fishing sets   

-  Annotations  
-  Species 

prediction  

-  Size 

distribution 
Comparison  

Ground truth 

validation (sec. 
2.5) 

Data from 4 fishing 
sets  

-  Official sources  
-  Estimations

Fig. 3. Example of individual fishes once the segmentation is done. These fishes belong to a monospecific set of YFT.  

Table 2 
Computing  infrastructure,  search  space  and  best  assignments  of  hyper-
parameters of the neural networks.  

(a) Hyperparameters used for the training of each neural network. The base models 
have been obtained from TensorFlow Object Detection API (segmentation) and the 
base implementation of TensorFlow (classification) 

Hyperparameter  Module 

Search space 

Best assignment 

Epochs 

Batch size 

Learning rate 

Activation 

Train/test split 

Segmentation 
Classification 
Segmentation 
Classification 
Segmentation 
Classification 
Segmentation 
Classification 
Segmentation 
Classification 

[1000(cid:0) 20,000] 
[500–5000] 
[2(cid:0) 32] 
[2–128] 
[0.0001–0.01] 
[0.0001–0.01] 
Softmax 
Softmax 
[0.9/0.1–0.8/0.2] 
[0.9/0.1–0.8/0.2] 

15,000 
500 
4 
128 
0.008 
0.0001 
Softmax 
Softmax 
0.9/0.1 
0.8/0.2

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The information provided does not explicitly mention the specific methods used for collecting raw data in the deep learning pipeline. However, it can be inferred that the authors might have used various techniques such as manual annotation, automatic segmentation, or publicly available datasets. For instance, they mentioned manually annotating 275 segments and automatically generating 6921 segments with x12 data augmentation, resulting in 14,896 images. Additionally, they referred to using official sources and estimations for validation purposes. Nevertheless, without explicit details on the exact methodologies employed for gathering raw data, one cannot definitively state what methods were utilized.

Based on the given context, we can only infer that the researchers may have used a combination of manual annotation, automatic segmentation, and possibly some form of publicly available datasets to collect their raw data. However, without more specific information, we cannot provide a precise answer to the query.
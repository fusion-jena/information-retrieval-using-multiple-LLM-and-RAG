Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

5.2.2. Training

The official, MS-COCO pre-trained, version of this architecture has 
then  been  further  trained,  a  technique  known  as  transfer-learning, 
using the WCS dataset and its recommended data split, giving a cred-
ible mean average precision mAP50 score of 0.75. The mAP metric, used 
to  evaluate  the  PASCAL  VOC  challenge  dataset  (Everingham  et  al., 
2010),  takes  the  mean  over  all  considered  species  of  their  respective, 
species-specific  average  precision  as  defined  below.  The  subscript  50 
refers  to  the  requirement  that  the  area  of  the  intersection  of  ground 
truth  bounding  box  and  predicted  bounding  box  needs  to  be  at  least 
50%  of  the  area  of  the  union  of  both  for  an  otherwise  true  species 
identification to be considered correct. The network does not output a 
binary  value  for  species  identification  but  instead  a  belief  b  ∈ [0, 1] 
that  a  particular  species  has  been  identified.  Thresholding  these

van Rijsbergen, C.J., 1979. Information Retrieval, 2nd ed. Butterworth-Heinemann, USA.  
Wang, C.Y., Bochkovskiy, A., Liao, H.Y.M., 2021. Scaled-YOLOv4: Scaling cross stage 
partial network. In: Conference on Computer Vision and Pattern Recognition 
(CVPR), pp. 13029–13038. 

Wei, W., Luo, G., Ran, J., Li, J., 2020. Zilong: a tool to identify empty images in camera- 
trap data. Ecol. Inform. 55, 101021 https://doi.org/10.1016/j.ecoinf.2019.101021. 
Weingarth, K., Zimmermann, F., Knauer, F., Heurich, M., 2013. Evaluation of six digital 

camera models for the use in capture-recapture sampling of Eurasian Lynx. 
Wald¨okologie Online 13, 87–92. 

Xi, T., Wang, J., Qiao, H., Lin, C., Ji, L., 2021. Image filtering and labelling assistant 

(ifla): expediting the analysis of data obtained from camera traps. Ecol. Inform. 64, 
101355 https://doi.org/10.1016/j.ecoinf.2021.101355.

6.2.1. Data 

The GBIF database10 contains over 2300 videos of animals, although 
many of these are not suitable for our purpose. We have curated a subset 
of  330  videos  that  are  shot  with  static  cameras.  These  videos  are 
therefore good representations of the style of input this pipeline is ex-
pected  to  receive.  Every  frame  from  each  of  these  videos  has  been 
manually annotated, indicating whether or not it contains an animal. We 
have provided the training (30) and validation (300) split as used to tune 
parameters and evaluate the system, respectively. It is important to note 
that  camera-related  parameters  have  not  been  annotated  or  used  in 
testing  the  DynAIkonTrap. For these  tests we have  assumed the  focal 
length for all cameras to be the same as that of the RPi camera module –

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

According to the given context, the data was split into two sets: a training set and a validation set. The training set consisted of 30 videos, while the validation set contained 300 videos. This split was used to tune parameters and evaluate the system, respectively. However, the specific criteria used to determine which videos were included in each set are not explicitly mentioned in the provided context. Therefore, based solely on the information available, it cannot be determined what exact criteria were used to split the data for deep learning model training.
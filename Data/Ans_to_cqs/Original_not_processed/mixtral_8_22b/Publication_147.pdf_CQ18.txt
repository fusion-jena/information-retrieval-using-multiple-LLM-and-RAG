Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 2. Full comparison of training time, prediction time, optimization time (i.e., time to properly
tune the model), and hardware for each model.

Model

Training Time
(Hours)

Prediction Time (Hours)

Optimization
Time (Hours)

Hardware

CNN

XGB

4

4

2.4. Validation

4

Unknown

Desktop with 64 Gb of
RAM and one Titan X
(Maxwell) GPU.

72—can be reduced by
distributing prediction to more
cores or more machines.

2

Desktop with 64 GB of
ram and 64 logical cores.

Vienna, Austria, 2013.

77. Hacker Earth.

Beginners Tutorial on XGBoost and Parameter Tuning in R. Available
online: https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-
tutorial-on-xgboost-parameter-tuning-r/tutorial/ (accessed on 29 August 2019).

78. Parisien, M.-A.; Parks, S.A.; Miller, C.; Krawchuk, M.A.; Heathcott, M.; Moritz, M.A. Contributions of
ignitions, fuels, and weather to the spatial patterns of burn probability of a boreal landscape. Ecosystems
2011, 14, 1141–1155. [CrossRef]

79. Atienza, R. Advanced Deep Learning with Keras: Apply Deep Learning Techniques, Autoencoders, Gans, Variational
Autoencoders, Deep Reinforcement Learning, Policy Gradients, and More; Packt Publishing Ltd.: Birmingham,
UK, 2018.

right by 90 degrees, ﬂipped horizontally or vertically, or left as is. Since marsh and swamp wetland
classes were somewhat rarer than the other classes, during batch creation (using a batch size of 24),
we ensured that there were at least six patches containing each of those labels. Using a geometrically
decaying learning rate, the model was trained for 110 epochs, where each epoch was composed of 4800
training samples. Model training took approximately 3–4 h and prediction over all the study area at
10 m resolution took a similar amount of time. Training and prediction was done on a desktop with
64 Gb of RAM and one Titan X (Maxwell) GPU. A full comparison of computation time between the
models can be seen in Table 2.

correlation to each other. This variable selection process can be seen in more detail in [7]. The inputs
were trained to the six classes from the ABMI plot training data using the “multi:softmax” objective
setting. The XGBoost model parameters were tuned using grid search functions to ﬁnd the optimal
value when judged by the test error metric. Additionally we wanted to err on the side of conservative
model building since we knew there was little power in the inputs to discriminate between wetland
classes (see Figure 1). The optimized XGBoost parameters were: nrounds = 500, max_depth = 4,
eta = 0.03, gamma = 1, min_child_weight = 1, subsample = 0.5, colsample_bytree = 0.8. See [77] for
description of XGB parameter tuning. We then built 15 separate XGBoost models. Each model was built
with a diﬀerent subset of 2000 random points which were spaced anywhere from 900 to 2000 m apart,
depending on the relative abundance of each of the six classes. The selection of this number of points

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the strategy used to monitor the model performance during training. However, it mentions that the CNN model was trained for 110 epochs using a geometrically decaying learning rate. It also states that the XGBoost model parameters were tuned using grid search functions to find the optimal value based on the test error metric. These details suggest that some form of monitoring was performed during the training phase, but the specific strategy remains unclear without additional context.
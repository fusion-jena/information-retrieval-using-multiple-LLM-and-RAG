Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

mAP =

1
Nc

∑Nc

k=1

APk

(4)  

where the Nc  is the number of classes. The advantage of using mAP is 
that it is independent of the selected threshold for the implementation of 
the model. The closer its value is to one, the better the model will be. 
We searched a θ-threshold by class to provide an optimum balance 
between precision and recall metrics. This was done by observing the 
highest measure of the harmonic mean (F1score) and the most general 
measure variations of the Fscore: 
) P(θ)R(θ)
P(θ)β2 + R(θ)

1 + β2

Fβ =

(5) 

(cid:0)

This decreased the β  coefficient up to 0.5 to provide a higher pon-

deration to the precision than to the recall (Kahl et al., 2021). 

2.9. Non-acoustic environmental variables

CNN training was run successfully, and the optimized weights of the 
model were stored. To evaluate the predictive performance in the testing 
dataset, we plotted the P (cid:0) R curves of all classes on the same graph 
(Fig. 5). The points of each curve closest to the coordinate (1, 1) corre-
spond to the best performance in the prediction, with the threshold θ 
associated with that point. The observed performance was ideal in the 
Scytalopus  magellanicus class  and very  close to the ideal in a majority 
group of classes. The curves that were separated from this group rep-
resented classes far from the ideal: Glaucidium nana (a) and Glaucidium 
nana (b), which in turn were augmented with simulated samples. This 
suggests three likely sources of conflict: (1) inconsistency between real 
and simulated samples, (2) less relative differentiation between classes 
belonging to the same species and (3), in the case of G. nana (a), a gap of

R(θ) =

TP
TP + FN

(2) 

Precision and recall are inversely related metrics as we increase or 
decrease the threshold θ. To achieve a balance between both metrics, it 
is common to draw a precision-recall curve (P (cid:0) R curve) as a result of 
the  assessment  of  all  possible  thresholds  θ.  This  plot  allowed  us  to 
analyze from which recall we had a degradation of precision and vice 
versa. To summarize this curve in a single value, the average precision 
(AP) was computed: 

Class 

Vocal form 

Labeled presences 

Simulated presences 

Total presences 

Absences

The P (cid:0) R curves were summarized in the usual values of AP shown 
in the legend of the figure. As a general result of the evaluation, a mAP 

AP =

∑n
(cid:0)

i=2

R(θi) (cid:0) R(θi(cid:0) 1)

)

P(θi)

(3)  

where n represents the total number of thresholds to assess, which are 
organized in descending form. This metric corresponded to the weighted 
metrics of the achieved precisions in each threshold, with the increase in 
the  previously  used  threshold  recall  being  weighted,  i.e.,  an  approxi-
mation of the area under the P (cid:0) R curve. 

As a measure to evaluate and compare the performance of the mul-
ticlass classification models, we used the average of all the AP obtained 
by each class (mAP): 

mAP =

1
Nc

∑Nc

k=1

APk

(4)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To evaluate the performance of the deep learning model, several metrics are used. These include Precision, Recall, Average Precision (AP), and Mean Average Precision (mAP).

Precision measures the proportion of true positive predictions out of all positive predictions made by the model. It is calculated as the ratio of True Positives (TP) to the sum of TP and False Positives (FP).

Recall measures the proportion of true positive predictions out of all actual positives. It is calculated as the ratio of TP to the sum of TP and False Negatives (FN).

Average Precision (AP) is a summary metric that calculates the average precision value over all unique thresholds. It is calculated by taking the sum of the precision at each threshold multiplied by the change in recall from the previous threshold, divided by the total number of relevant instances.

Mean Average Precision (mAP) is another summary metric that averages the AP across multiple classes. It is calculated by dividing the sum of AP for each class by the total number of classes.

In addition to these metrics, the authors also use the F1 score, which is the harmonic mean of precision and recall. They decrease the beta coefficient down to 0.5 to give more weightage to precision than recall.

Overall, these metrics help evaluate the performance of the deep learning model and determine how well it can distinguish between different bird species based on their vocalizations.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

)

(cid:0)

Ω

fj

= τT +

1
2

λ||w||2

(9)  

2.2.6. Heterogenous ensemble techniques 

Heterogeneous ensemble learning is the practice of combining pre-
dictions of multiple models that are different in terms of their underlying 
algorithms  or  architectures  to  improve  overall  performance.  Utilizing 
heterogeneous ensemble techniques can address biases and improve the 
implemented  two 
effectiveness  of  models. 

In  our  study,  we 

EcologicalInformatics81(2024)10258312G. Agboola et al.                                                                                                                                                                                                                                

Fig. 11. The proportion of area covered by each susceptibility category in each scenario.  

heterogeneous  techniques  to  optimize  the  performance  of  five  base 
learners and mitigate potential overfitting:

pending data availability, we will investigate the scalability and trans-
ferability  of  the  ensemble  techniques  and  compare  their  performance 
with  deep  learning  models  such  as  Convolutional  Neural  Networks 
(CNN) and Region-based Convolutional Neural Networks (RCNN). This 
broader  investigation  would  provide  a  more  robust  understanding  of 
landslide vulnerability across the entire and offer valuable insights into 
the effectiveness of advanced ML approaches in LSM.

2021; Hu et al., 2020; K. Li et al., 2023), and weighted average (WA) (K. 
Li  et  al.,  2023;  Zeng  et  al.,  2023)  have  been  empirically  proven  to 
mitigate  overfitting  issues,  thereby  enhancing  the  precision  of  LSM 
(Kadavi et al., 2018).

a)  Weighted Average (WA) Ensemble Learning: The WA represents a 
potent ensemble machine learning strategy, combining predictions 
from  different  models.  Within  this  methodology,  the  influence  of 
each model is meticulously fine-tuned, per its unique proficiency and 

EcologicalInformatics81(2024)10258313G. Agboola et al.                                                                                                                                                                                                                                

Table 4 
Evaluation Metrics for Machine Learning Landslide Susceptibility Models on Test Data.   

Scenario 1 

Scenario 2  

SVM 

LR 

LDA 

RF 

XGBoost 

WA 

Stacking 

SVM 

LR 

LDA 

RF 

XGBoost 

WA 

Stacking 

OA 
Kappa 
ROC 

0.947 
0.893 
0.914 

0.957 
0.914 
0.992 

0.951 
0.901 
0.992 

0.957 
0.914 
0.993 

0.961 
0.922 
0.991 

0.955 
0.909 
0.994 

0.959 
0.918 
0.994 

0.817 
0.633 
0.901 

0.827 
0.653 
0.903

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper does not explicitly discuss specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, it mentions using heterogeneous ensemble techniques to combine predictions from multiple models with different algorithms or architectures to improve overall performance and address biases. These techniques include stacking, voting, and weighted average ensembles. Additionally, the authors plan to investigate the scalability and transferability of these ensemble techniques and compare them with deep learning models like CNN and RCNN. While the paper doesn't directly mention stratified splitting, oversampling, undersampling, or diverse data collection, these methods could potentially be employed to handle data imbalance and reduce bias in the dataset before applying ensemble techniques or deep learning models.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

within the image. While there remains the potential for bias in the 

three main steps.

expert count, we take this count to be the gold standard. Hence, our 

Firstly, we generated a training dataset by selecting 500 of the 

results are a comparison between the two novel methods employed 

survey images at random to be used exclusively for training. Images 

and a count by a single experienced expert, which could in principle 

were tiled into 864 × 864 subimages and then passed though a ver-

deviate from the unknown true count.

sion of the YOLO DCNN using pretrained weights from the COCO 

dataset provided by (Redmon & Farhadi, 2018). This process created 

a	list	 of	the	locations	of	potential	objects	in	each	image.	As	a	first	

3 |  R E S U LT S

pass, these results were filtered by discarding any object detections 

that did not correspond to an identification from the Zooniverse data. 

We  compared  the  accuracy  of  the  methods  by  calculating  the

more importantly, reduce the number of empty images that citizen 

account for the large amounts of empty space in the training images. 

counters needed to process.

We achieved this by increasing the weighting (from 0.5 to 2) given to 

Following	initial	trials	on	the	website	in	August	2015,	it	was	de-

the no- object component of the multipart loss function described in 

termined  that  volunteers  would  struggle  to  count  entire  aerial  im-

Redmon et al. (2016). For training, we used transfer learning, again 

ages due to their large size and high resolution. Our solution was to 

using  the  pretrained  general  purpose  YOLO  object  detector  as  a 

split each aerial image into 12 equal- sized tiles. The images were up-

starting point with initial weights created by training on the COCO 

loaded to the Serengeti Wildebeest Count project on the Zooniverse 

dataset (Redmon & Farhadi, 2018). During training, we first froze all

science counts of the survey and comparison to expert counts.

tee  that  the  approach  is  transferable  and  how  to  appropriately 

4 |  D I S CU S S I O N

filter the data may be affected by the wording of the guidelines, 

the image resolution and sizes used, or the set of volunteers that 

participate  in  the  project.  Other  more  sophisticated  approaches 

to processing citizen science data have been proposed (Swanson 

From our results, we see that both citizen science and deep learn-

et al., 2016); however, given the range of counts provided by the 

ing methods are capable of producing highly accurate image counts. 

volunteers  and  the  large  errors  we  observe  in  the  baseline  met-

Counting the wildebeest within the survey images is a difficult and 

rics  (c.  11%  and c.  9%  undercount  for  the  mean  and  median,  re-

time- consuming task. When collecting the census images, there are

parallel  computing  power  provided  by  graphical  processing  units 

tion professionals to focus on other tasks. Two novel methods can 

(GPUs)  and  accessibility  of  image  training  datasets,  such  as  the 

potentially replace the use of manual counts by experts, the deploy-

COCO  dataset  (Lin  et al.,  2014),  have  led  to  rapid  improvements 

ment of citizen scientists and the use of automated object detection 

in  the  performance  of  multilayer  deep  convolutional  neural  net-

algorithms.  In  this  work,  we  deploy  both  approaches  and  evaluate 

works  (DCNNs).  These  multilayer  neural  networks  are  a  form  of 

the performance and merits of each.

deep learning and are distinct to traditional machine learning ap-

1.1 | Citizen science and the wisdom of crowds

proaches to computer vision in that no hand- crafted features are 

required;  instead,  the  convolutional  layers  extract  relevant  fea-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions that they used transfer learning during training, which involves freezing all layers except the last few layers of a pre-trained model. Transfer learning itself acts as a form of regularization since it leverages the knowledge gained from solving one problem to improve generalization when applied to another related problem. Additionally, the text discusses adjusting the weighting given to the no-object component of the multipart loss function, which might also help mitigate overfitting. But without further details, it cannot be confirmed if techniques like dropout or L2 regularization were explicitly utilized.
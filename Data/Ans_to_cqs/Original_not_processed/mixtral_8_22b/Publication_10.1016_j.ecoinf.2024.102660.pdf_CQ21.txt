Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics81(2024)1026602J. Chen et al.                                                                                                                                                                                                                                     

Fig. 1. Study area.  

Fig. 2. Flow chart of data acquisition and processing.  

files to a size of 256 × 256 pixels. This cropping process resulted in a 
total of 1764 image pairs. Among them, 1232 image pairs were used for 
training  the  deep  neural  network,  176  image  pairs  for  validating  the 
model  accuracy  during  the  training  phase,  and  356  image  pairs  for 
testing the performance of the trained network post-training. 

2.4. Network structure

2.5. Parameter setting 

The Res_AUNet network was constructed using the PyTorch library. 
The experiments were conducted on a Dell workstation with an Intel(R) 
Xeon(R) Gold 6248R CPU (3.00GHz) and 512GB of memory, along with 
an NVIDIA RTX A6000 graphics card with 48GB of memory. During the 
network training process, the input images underwent random rotations, 
flips,  and  other  image  augmentation  operations  to  enhance  the  net-
work’s robustness. Training persisted for 120 epochs, with a batch size 
set to 8. The initial learning rate was set to 0.001, and the learning rate 
was  adjusted  using  the  equal-interval  adjustment  strategy  (StepLR), 
halving the original rate every 10 epochs of iteration. 

2.6. Evaluation metric

2.6. Evaluation metric 

This  study  employed  five  metrics,  namely  Accuracy  (A),  Precision 
(P),  Recall  (R),  F1-score  (F1),  and  Intersection  over  Union  (IOU),  to 
assess the effectiveness of the proposed model (Giles et al., 2021). Their 
calculation formulas are shown below: 

3. Results 

3.1. Detection results on the WSGD test dataset

original  dimensions.  The  right  portion  of  Fig.  3(a)  represents  the 
decoding stage, where convolutions and upsample operations are con-
ducted  from  level  5  to  level  2,  to  restore  the  feature  map  size  to  its 
original  dimensions.  During  this  stage,  the  upsampled  feature  map  is 
concatenated with the sun glint features from the corresponding level in 
the  encoding  stage  along  the  channel  dimension.  Subsequently,  the 
concatenated feature undergoes processing through the Convolutional 
Block  Attention  Module  (CBAM)  mechanism  (Woo  et  al.,  2018),  as 
shown in Fig. 4, to recombine the sun glint features in both channel and 
spatial  dimensions.  Afterward,  two  additional  convolutional  feature 
extraction operations are performed to complete the decoding process 
for that level. At the final level of the decoding stage, the upsampled 
level 2 decoding feature is connected with the level 1 encoding result.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training, there are several postprocessing steps mentioned in the given context. These include testing the performance of the trained network using 356 image pairs, which can involve calculating evaluation metrics such as Accuracy (A), Precision (P), Recall (R), F1-score (F1), and Intersection over Union (IOU). However, the context does not explicitly mention any specific postprocessing steps like creating saliency maps or confusion matrices. Therefore, based on the information provided, it cannot be confirmed if these particular postprocessing techniques were utilized.
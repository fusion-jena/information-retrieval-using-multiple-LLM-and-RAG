Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(cid:0) 4 was finally 
chosen after comparing with stochastic gradient descent (SGD) with the 
(cid:0) 4  and learning rate of 
(cid:0) 4, weight decay of 1.0⋅10
momentum of 1.0⋅10
(cid:0) 3. SGD was tested with the parameters as recommended by L. 
1.0⋅10
Smith  (Smith,  2018),  but  achieved  a  1%–2%  decrease  in  accuracy 
compared to using the Adam optimizer. 

The  ResNet50  architecture  (He  et  al.,  2016)  was  modified  and 
trained with transfer learning using pre-trained weights from ImageNet 
(Smith,  2018).  Models  trained  with  transfer  learning  outperformed 
models trained with random weights with an increase of 4% in the F1-

Eq. (4) denotes the predicted class in level l as the maximum output 
value from the network at the level. An input image is also classified as 
‘unsure’ when the predicted class is not correct according to the higher 
rank in the taxonomic hierarchy as defined in Eq. (7). 

{

̃yl =

Unsure
̃yl

̃yl⇏̃yl(cid:0) 1

if
otherwise

or Rlj < th

(7)  

3.3. Training, augmentation and optimizers 

The training on the datasets was  performed using data augmenta-
tion, including image scaling, horizontal and vertical flip, perspective 
distortion and adding color jitter for brightness, contrast and saturation. 
Data augmentation mitigates overfitting by increasing the diversity of 
the training data. We selected a batch size of 20 for training our models 
since it is faster to update, and results in less noise, than smaller batch 
sizes. The accuracy of the models on the training and validation datasets 
was computed after each epoch. 

The Adam optimizer with a fixed learning rate of 1.0⋅10

Table 4 
Average performance (Avg) and standard deviation (SD) for five trained models. Average precision, recall and F1-score for trained ResNet50 and EfficientNetB3 
(EffNetB3) models modified for multitask learning (MTL) with transfer learning using pre-trained weights from ImageNet. The models are trained and validated on the 
TLm  dataset. The models ResNet50, EfficientNetB3 are trained without MTL.  

Model 

Level 

ResNet50MTL 
EffNetB3MTL 

ResNet50MTL 
EffNetB3MTL 

ResNet50MTL 
EffNetB3MTL 

ResNet50 
EffNetB3 

L1 Order 
L1 Order 

L2 Family 
L2 Family 

L3 Species 
L3 Species 

Species 
Species 

Avg 

0.990 
0.986 

0.987 
0.984 

0.955 
0.948 

0.955 
0.953 

Precision 

SD (10

(cid:0) 3) 

(1.0) 
(4.4) 

(0.8) 
(3.1) 

(4.3) 
(5.2) 

(3.3) 
(2.5) 

Avg 

0.991 
0.993 

0.986 
0.988 

0.961 
0.966 

0.957 
0.966 

Recall 

SD (10

(cid:0) 3) 

(1.1) 
(0.5) 

(0.9) 
(0.7) 

(9.8) 
(5.1) 

(7.3) 
(2.5) 

Avg 

0.991 
0.989 

0.987 
0.986 

0.957 
0.956 

0.955 
0.959

Score at the species level. To compare with other state-of-the-art net-
works, EfficientNetB3 (Tan and Le, 2019) was also evaluated with pre- 
trained  weights  from  ImageNet.  Transfer  learning  involves  training  a 
model on data from a source domain TS = P(y|X) and then transferring it 
to a target domain TT = P(y|X) – typically with less data available. In this 
case,  ImageNet  contains  1,000  classes  with  1,281,167  images  for 
training  and  50,000  for  testing,  while  the  training  insect  TL  (TLm) 
benchmark dataset only contains 9 classes with 25,028 TL images.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, two optimization techniques have been mentioned and used in the deep learning pipeline - Stochastic Gradient Descent (SGD) and Adam Optimizer.

Firstly, SGD was compared with the Adam optimizer. SGD was tested with the parameters as recommended by L. Smith, but it resulted in a 1%–2% decrease in accuracy compared to using the Adam optimizer. This indicates that the Adam optimizer was more effective in improving model performance.

Secondly, the Adam optimizer was chosen for use in the deep learning pipeline. It was used with a fixed learning rate of 1.0⋅10^-4 during the training process. Data augmentation techniques were also employed to prevent overfitting and improve generalization. These included image scaling, horizontal and vertical flips, perspective distortions, and adding color jitter for brightness, contrast, and saturation.

Therefore, based on the information provided, the optimization techniques applied in the deep learning pipeline include both SGD and Adam optimizers, with the latter being preferred due to its superior performance.
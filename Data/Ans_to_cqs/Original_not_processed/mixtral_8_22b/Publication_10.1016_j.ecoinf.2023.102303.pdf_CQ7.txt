Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Labeling Time 
Training Time 
Turnaround 

Time 
Accuracy 

Very Short 
1.0 h 

4 h 

89.4% 

Short 
6.9 h 

22 h 

94.0% 

Very Long 
~3.5 h 

94 h 

98%  

4. Discussions 

The  development  of  deep  learning  methodologies  continues  to 
advance at an astonishing rate and be applied to various applications 
ranging from biomedical (Azghadi et al., 2020), hydrological processes 
in river channels (Talukdar et al., 2023) and agricultural (Olsen et al., 
2019) systems, to marine (Laradji et al., 2021; Saleh et al., 2022b), and 
environmental (Jahanbakht et al., 2022a) sciences. The application of 
deep learning technologies has been also used in profiling the ecosystem 
services of estuarine habitats by community members (Yee et al., 2023). 
In this paper, we extend the application of deep learning methodologies 
to advance state-of-the-art underwater fish video processing techniques 
applied to turbid waters.

The remainder of this paper is organized as follows. Section 2 de-
scribes  the  weakly-supervised  database  and  proposes  two  DNN  struc-
tures  as  a  novel  solution  to  the  problem  of  fish  detection  in  turbid 
waters. We evaluate the accuracy of the proposed models, compare them 
with each other, and with the other fully-supervised models in Section 3. 
A  detailed  discussion  of  our  findings  is  carried  out  in  Section  4.  The 
paper is concluded in Section 5. 

2. Material and methods 

This section starts by describing our labeled dataset and then pro-
ceeds by proposing two DNN models to enhance the capability of marine 
scientists in turbid underwater and coastal environment monitoring. 

2.1. Weakly labeled public FishInTurbidWater dataset

1.  Collecting and presenting a new fish dataset in turbid waters.  
2.  Weakly  labelling  this  dataset  to  provide  the  worldâ€™s  first  weakly 

labeled fish dataset in turbid waters.  

3.  Developing two novel deep learning models to achieve two different 

goals.  
a.  A  semi-supervised  model  for  accelerating  model  development 

speed from data collection to deployment.  

b.  A  weakly-supervised  model  for  accelerating  the  development 
time,  while  also  conserving  the  accuracy,  compared  to  the  fast 
semi-supervised model.

It  is  worth  noting  that  the  typical  operation  of  fully-supervised 
models  in  Table  3  is  extracted  from  a  recent  survey  paper  by  Saleh 
et  al.  (2022b),  which  is  mainly  focused  on  clear-water  scenarios. 
Therefore, a more accurate comparison between our semi- and weakly- 
supervised models with a fully-supervised deep learning in both clear 
and turbid waters is required. We believe that applying the clear-water 
models of Saleh et al. (2022b) in turbid water situations (like our Fish-
InTurbidWater dataset) would dramatically degrade their performance. 
Besides,  the  approximate  turnaround  time  in  Table  3  is  measured 
against a non-experienced human agent. Here, we added an extra 30% 
to  the  labeling  time  of  fully-supervised  networks  to  consider  the 
necessary  double-checks  and  quality  controls.  Overall,  these  numbers 
are only rough estimates of the required time, and they heavily depend

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't any explicit information about the specific criteria used to split the data for deep learning model training into sets such as train, test, or validation. However, it can be assumed that common practices were followed. These typically include splitting the data randomly but proportionally into three subsets: training set, validation set, and testing set. The training set is used to fit the model, the validation set is used to tune hyperparameters and prevent overfitting, and the testing set is used to assess the final model's performance. The proportions often used are 70%, 15%, and 15% respectively, although these may vary depending on the size of the dataset and the requirements of the project.
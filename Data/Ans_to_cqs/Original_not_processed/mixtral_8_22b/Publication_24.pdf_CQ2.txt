Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

436–444. https://doi.org/10.1038/nature14539

Lee, H., Pham, P., Largman, Y., & Ng, A. Y. (2009). Unsupervised feature 
learning for audio classification using convolutional deep belief networks 
(pp. 1096–1104). Proceedings of the 22nd International Conference 
on Neural Information Processing Systems, Istanbul, Turkey.

Lin,  T.-H.,  Fang,  S.-H.,  &  Tsao,  Y.  (2017).  Improving  biodiversity  as-
sessment  via  unsupervised  separation  of  biological  sounds  from 
long- duration  recordings.  Scientific  Reports,  7,  4547.  https://doi.
org/10.1038/s41598-017-04790-7

McFee, B., Raffel, C., Liang, D., Ellis, D. P., McVicar, M., Battenberg, E., 
&  Nieto,  O.  (2015).  librosa: Audio and music signal analysis in python 
(pp. 18–25). Proceedings of the 14th python in science conference, 
Austin, TX.

methods for preprocessing spectrograms before they are used in 

were repeated independently for CityBioNet and CityAnthroNet 

ML; for example, whitening (Lee, Pham, Largman, & Ng, 2009) and 

to  predict  the  presence/absence  of  biotic  and  anthropogenic 

subtraction of mean values along each frequency bin (Aide et al., 

sound in every 1 s chunk throughout the audio file, allowing each 

2013). CNNs are able to accept inputs with multiple channels of 

chunk to be categorised into one of four states (Figure 2).

data, for example, the red, green, and blue channels of a colour 

7.  Summarise: Where appropriate, the chunk-level predictions were 

image. We exploited the multiple input channel capability of our 

summarised to gain insights into trends over time and space. For 

CNN by providing as input four spectrograms each preprocessed 

example,  predicted  activity  levels  for  each  half-hour  window 

using  a  different  normalisation  strategy  (see  Supplementary

ditional  methods  (Farinha- Marques,  Lameiras,  Fernandes,  Silva,  & 

automatically  based  on  the  annotated  training  data  provided. 

Guilherme, 2011). This inhibits our ability to conduct the large- scale 

Convolutional Neural Networks, CNNs (or Deep learning) (LeCun, 

assessment that is necessary for understanding urban ecosystems.

Bengio,  &  Hinton,  2015)  can  even  choose,  based  on  the  annota-

Ecoacoustic surveying has emerged as a useful method of large- 

tions in the training dataset, the features that discriminate different 

scale  quantification  of  ecological  communities  and  their  habitats 

classes  in  datasets  without  being  specified  a  priori,  and  can  take 

(Sueur  &  Farina,  2015).  Passive  acoustic  recording  equipment  facil-

advantage of large quantities of training data where their ability to 

itates the collection of audio data over long time periods and large

Retrieved from http://www.python.org

R Core Team. (2017). R: A language and environment for statistical com-

puting. Retrieved from http://www.R-project.org

Salamon, J., Jacoby, C., & Bello, J. P. (2014). A dataset and taxonomy for 
urban sound research (pp. 1041–1044). ACM MM’14. Association for 
Computing Machinery, Orlando, FL.

Salamon, J., MacConnell, D., Cartwright, M., Li, P., & Bello, J. P. (2017). 
Scaper:  A  library  for  soundscape  synthesis  and  augmentation.  2017 
IEEE  Workshop  on  Applications  of  Signal  Processing  to  Audio  and 
Acoustics, New Paltz, NY.

Stowell, D., & Plumbley, M. D. (2014). Automatic large- scale classification 
of bird sounds is strongly improved by unsupervised feature learning. 
PeerJ, 2, e488. https://doi.org/10.7717/peerj.488

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning pipeline primarily uses two types of data formats: images and audio. The use of images is evident in the application of Convolutional Neural Networks (CNN), which can process multi-channel inputs such as color images (red, green, and blue channels). In this case, the CNN was employed to analyze four spectrograms, each processed differently through various normalization strategies. Spectrograms are visual representations of the spectrum of frequencies of a signal as it varies with time, essentially making them images derived from audio signals.

Audio data is also directly utilized in the deep learning pipeline. The study mentions the use of passive acoustic recording equipment to collect audio data over extended periods and vast areas. Furthermore, the paper references several works related to audio processing and analysis, including Stowell and Plumbley's research on automatic large-scale classification of bird sounds and Salamon et al.'s work on a dataset and taxonomy for urban sound research. These studies suggest that audio files serve as primary sources of information in the deep learning pipeline.

In summary, both image and audio data formats play significant roles in the deep learning pipeline. While images are generated from audio signals in the form of spectrograms, raw audio files are also analyzed directly.
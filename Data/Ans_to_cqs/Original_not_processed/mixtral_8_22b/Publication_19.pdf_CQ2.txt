Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The  purpose  of  research  reported  here  is  to  automatically 
classify  images  retrieved  from  camera  traps  and  to  send  the 
species information instantly to the concerned authorities using 
deep learning and edge analytics. This research is aligned with 
UN  sustainable  goal  15,  “Life  on  Land”  that  focuses  on 
promoting and restoring sustainability on land. This approach 
will  help  accelerate  research  to  address  biodiversity  loss 
because  scientists  will  be  able  to  acquire  this  data  faster  and 
cheaper.

Abstract— Maintaining biodiversity is a key component of the 
United Nations (UN) “Life on Land” sustainability goal. Remote 
camera traps monitoring animals’ movements support research in 
biodiversity.  However,  images  from  these  camera  traps  are 
currently labeled manually resulting in high processing costs and 
long  delays.  This  paper  proposes  an  IoT-based  system  that 
leverages deep learning and edge computing to automatically label 
camera trap images and transmit this information to scientists in 
a  timely  manner.  Inception-V3,  MobileNet-V2,  ResNet-18,  and 
DenseNet-121  were  trained  on  data  consisting  of  33,984  images 
taken  during  day  and  night  with  6  animal  classes.  Inception-V3 
yielded  the  highest  macro  average  F1-score  of  0.93  and  an 
accuracy  of  94%.  An  IoT-based  system  was  developed  that 
directly captures images from a commercial camera trap, does the 
inference on the edge using a Raspberry Pi (RPi), and sends the

Keywords—deep  learning,  transfer  learning,  convolutional 
neural  networks,  animal  classification,  camera  trap,  wildlife 
monitoring, edge computing, TensorFlow lite, raspberry pi, IoT 

I.  INTRODUCTION

TABLE II.  

HYPERPARAMETERS DETERMINED USING GRID SEARCH 

Model Name 

Inception-V3 
MobileNet-V2 
ResNet-18 
DenseNet-121 

Batch 
Size 
48 
32 
32 
16 

Learning 
Rate 
0.05 
0.01 
0.005 
0.001 

Parameters 
# of 
Epochs 
75 
100 
150 
100 

Input Image Size 

299 (cid:3400) 299 (cid:3400) 3 
224 (cid:3400) 224 (cid:3400) 3 
224 (cid:3400) 224 (cid:3400) 3 
224 (cid:3400) 224 (cid:3400) 3 

C.  Results 

  As  Table  III  show  all  models  performed  reasonably  well 
with macro-F1 averages above 91%. Because the models are to 
be deployed on IoT edge devices, the size of each model is an 
important  consideration.  As  Table  III  shows 
the  best 
performing model was Inception-V3 with a macro Average F1 
score of 0.93, and the smallest size of 175 MB.   

TABLE III.  

BEST RESULTS FOR EACH NN ARCHITECTURE 

Model 

InceptionV3 
DenseNet-121 
ResNet-18 
MobileNetV2 

Model Size 
(MB) 
175 
446 
480 
507 

Accuracy 

94% 
93% 
92% 
93%

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The data format used in the deep learning pipeline for this research is images. The study aims to classify images retrieved from camera traps using deep learning techniques. Specifically, the dataset consists of 33,984 images taken during day and night with six different animal classes. These images serve as input for training various neural network architectures such as Inception-V3, MobileNet-V2, ResNet-18, and DenseNet-121. Therefore, the primary data format utilized in this deep learning pipeline is image data.
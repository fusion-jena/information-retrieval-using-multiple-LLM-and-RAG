Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Training 

We  train  the  proposed  deep  CNN-LSTM 
regressively with the learning time gradually 
decreased. The model was based on the pre-
trained  model  [38]  and  was  trained  to 
classify plant species  and therefore identify 
the new incoming unknown species. The loss 
was taken as the sum of the firing rates of the 
error  neurons  in  the  zeroth  pixel  layer.  A 
random 
search  was 
performed  over 
fourth-  and  fifth-layer 
models  of  the  leaf  features  positions.  Our 
model consists of 20 layers with 3 by 3 filter 
sizes  of  all  convolutions  and  stack  size  per 
layer  of  (1,32,64,64,128,256).  The  initial 

hyperparameter 

training  rate  is  set  to  0.001  dropped  by 
learning ratio of 10 after every 60 epochs.  

Softmax 

The proposed system uses k-way softmax 
classifier to classify image to one among k 
plant species. The loss due to this 
architecture is given by

Generally, the proposed network system has 
20  layers:  12  convolutional  layers,  five 
pooling  layers,  one  Fully  Connected  layer, 
one LSTM layer, and one output layer with 
the  softmax  function  for  classification.  The 
convolutional  layer  with  a  size  of  3  ×  3 
kernels is used for extracting feature and it is 
activated  by  the  ReLU  function.  The  max-
pooling layer with a size of 2 × 2 kernels is 
applied to reduce the dimensions of an input 
image. In the last part of the architecture, the 
function map is shifted to the LSTM layer to 
for 
information 
extract 
classification  purpose.  Using  the  reshape 
method,  the  input  size  of  the  LSTM  layer 
becomes (196,512). After analyzing the time 
features,  the  model  sorts  the  plant  species 
images  through  a  fully  connected  layer  to 

required 

time 

predict whether they belong under any of the 
100 categories of the plant species. 

Experimental set up

appear  in  controlled  backlit  and  front-lit 
versions,  with  several  samples  per  species. 
They vary considerably in sharpness, noise, 
illumination patterns, and shadows. Since the 
dataset is not large enough for deep learning 
to overcome the problem of overfitting, data 
augmentation was used to enlarge the dataset 
training  and  overcoming 
for  network 
overfitting. The dataset used in this work can 
be  downloaded  at  UCI  Machine  Learning 
Repository:  One-hundred  plant  species 
leaves  data  set  Data  Set  database.  Figure  5 
shows sample of image data of 24 species in 
grey  scale  from  UCI  Machine  learning 
repository. 

Figure 4: Feature extraction from a plant leaf. 

Figure 5: A sample plant species image from UCI repository. 

97 

 
Tanzania Journal of Forestry and Nature Conservation, Vol 90, No. 3 (2021) Special Issue: 
Embracing Science and Technology in Nature Conservation. pp 93-103 

Identification of leaf pattern 

indicate

In  this  work,  a  combined  architecture  is 
proposed  where  by  the  CNN  layer  is 
embedded with the long short-term (LSTM) 
memory  layer  to  pass  those  significant 
features extracted in the initial layers to the 
deeper layers of the network. This supports 
effective  aggregation  of  feature  maps  for 
precise classification and ultimately accurate 
recognition.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the provided context, there is no explicit mention of specific regularization methods such as dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. However, the text does indicate that data augmentation was employed to address the issue of overfitting due to insufficient training data. Data augmentation is a common technique used to increase the amount of available training data by applying various transformations to existing images, thereby creating new variations of the original data. This helps improve the generalizability of the model and reduces the risk of overfitting. While data augmentation is mentioned as a method to combat overfitting, it should be noted that other regularization techniques like dropout or L2 regularization could also have been applied but were not explicitly stated in the provided context.
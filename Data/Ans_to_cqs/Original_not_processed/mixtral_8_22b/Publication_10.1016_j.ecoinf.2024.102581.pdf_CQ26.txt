Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics81(2024)1025817S. Ghosh et al.                                                                                                                                                                                                                                   

Table 4 
Search space of CNN hyperparameters to be explored.  

Hyperparameters 

Range 

Convolution Layers 
Filters 
Filter Size 
Number of Neuron 
Batch Size 
Epochs 
CNN Model Optimizer 

Lower limit = 1 and Upper limit = 10 
Lower limit = 1 and Upper limit = 64 
Lower limit = 1 and Upper limit = 10 
Lower limit = 32 and Upper limit = 1024 
Lower limit = 8 and Upper limit = 512 
Lower limit = 1 and Upper limit = 25 
ADAM, SGD, RMSProp, Adadelta, Adagrad, Adamax

Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. ImageNet classification with deep 

convolutional neural networks. Commun. ACM 60, 84–90. https://doi.org/10.1145/ 
3065386. 

Kumar, S., Singh, A., Walia, S., 2018. parallel big bang–big crunch global optimization 
algorithm: performance and its applications to routing in WMNs. Wirel. Pers. 
Commun. 100, 1601–1618. https://doi.org/10.1007/s11277-018-5656-y. 

EcologicalInformatics81(2024)10258115S. Ghosh et al.                                                                                                                                                                                                                                   

Liu, Z., Reynolds, A.C., 2020. A sequential-quadratic-programming-filter algorithm with 
a modified stochastic gradient for robust life-cycle optimization problems with 
nonlinear state constraints. SPE J. 25, 1938–1963. https://doi.org/10.2118/193925- 
pa.

hyperparameters.  The  hyperparameters  with  the  current  results  are 
saved until better accuracy is achieved. The termination criterion of the 
proposed approach is the maximum number of iterations to be executed. 
After the completion of the iterations, the proposed approach provides 
us with the best hyperparameters, thus evolving the near-optimal CNN. 
The  overall  complexity  is  dominated  by  the  iterations,  “TC”,  and 
within  each  iteration,  the  operations  depend  on  the  population  size 
“NP”.  Therefore,  the  total  complexity  can  be  approximated  as 
O(TC*NP).

71.17% 

98.7% 

Ganguly et al., 2022b) 

DDYOLOv5 and ResNet-50 (Hu et al., 

Pine tree classification 

98.71% 

2022) 

Deep Attention Dense CNN model ( 

Pandey and Jain, 2022b) 

Dense convolutional neural networks 

Plant leaf disease 
classification 
Plant disease classification 

97.33% 

99.1% 

() 

Proposed Approach 

Plant leaf classification 

98.97  

6.1.2. CVIP100 dataset

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To address data bias during preprocessing of the deep learning pipeline, several techniques can be employed. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into training and testing sets while maintaining the same distribution of classes or labels in both sets. This ensures that the classifier learns from a representative sample of all classes, reducing any potential bias towards certain classes.

Oversampling is a technique where samples from underrepresented classes are duplicated to balance the dataset. This helps prevent the classifier from being biased towards overrepresented classes. However, it may lead to overfitting if not handled carefully.

Undersampling is another method to handle imbalanced datasets, where samples from overrepresented classes are removed to achieve balanced representation. While this reduces the risk of overfitting, it may result in loss of valuable information.

Lastly, diverse data collection aims to gather data from various sources, ensuring that the dataset represents different scenarios and conditions. This helps improve the generalization ability of the classifier and reduce bias towards specific patterns present only in limited data.

These techniques have been applied in various studies such as plant leaf classification using Deep Attention Dense CNN models (Pandey and Jain, 2022b), pine tree classification using DDYOLOv5 and ResNet-50 (Hu et al., 2022), and plant disease classification using dense convolutional neural networks (Ganguly et al., 2022b).
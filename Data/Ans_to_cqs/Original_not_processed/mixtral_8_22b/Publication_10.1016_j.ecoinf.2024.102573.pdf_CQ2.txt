Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

This  experiment  used  both  learnable  frontends  with  80  filters  of 
length  512  (16 ms)  that were  initialized  by  mel-filterbanks.  Gaussian 
pooling  was  removed,  and  downsampling  happened  in  the  filtering 
convolution layer by a stride of 320 (10 ms). Then, the outputs were log- 
compressed  without  normalization.  These  plain  frontends  are  called 
LEAF-P and SincNet-P, and Table 4 shows their test results.

The main difference between many proposed audio frontends in the 
literature  is  the  type  of  filter  and  sometimes  the  adjustments  to 
compression and normalization operations. Some works slightly deviate 
from this narrative, such as convolutional restricted Boltzmann machine 
(Sailor et al., 2017) and learnable wavelet transform (Frusque and Fink, 
2022; Ha et al., 2021; Michau et al., 2022). However, as far as the task 
involves extracting joint temporal and spectral information from a signal 
without prior knowledge about the nature of the data and the task, the 
(mel-)spectrogram seems sufficient. One of the exceptions is learning the 
appropriate  frequency  bands  if  the  sound  events  of  interest  occupy 
widely  spread-out  frequency  bands.  However,  we  did  not  observe  a 
benefit from this property for bird vocalization in our ten chosen species. 
Reporting  a  large  gap  between  such  models  requires  thorough

datasets.  They  are  not  flexible  enough  to  address  current  open  chal-
lenges unless used purposefully on a narrow task.

)

(

2Ï€t
L

2.6. Backend neural network 

(8)  

The  complete  model  combines  an  audio  frontend  with  a  backend 
neural network classifier. This work uses an EfficientNet-B0 (Tan and Le, 
2019) from the official implementation of TorchVision (T. maintainers 
and contributors, 2016) to keep the experiments consistent with prior 
works,  reproducible,  and  accessible.  Also,  the  backend  leveraged  the 
pre-trained  weights  from  image  classification  since  it  improves  the 
performance and stability of optimization. 

2.7. Noise dataset

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not explicitly state what data format is being used in the deep learning pipeline. However, based on the context, it can be inferred that the data format used is audio. The experiment mentioned involves using learnable frontends with filters applied to signals, which suggests that the input data is likely audio signals. Additionally, the discussion mentions tasks involving extracting joint temporal and spectral information from a signal, further supporting the idea that the data format used is audio. Therefore, it can be concluded that the data format used in the deep learning pipeline is most likely audio.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Active learning is a powerful method for bioacoustic deep learning, 
but it poses certain challenges that deviate from the ‘standard recipe’ of 
bioacoustic deep learning described by Stowell (2022). One challenge is 
that the active learning process biases labelled data to the model since 
the model’s predictions guide the annotation process. Resulting labels 
therefore cannot be used in the model’s evaluation (Ricci et al., 2022; 
Roh  et  al.,  2019).  Chambert  et  al.  (2018)  and  Ruff  et  al.  (2020) 
addressed this issue by selecting audio frames for review post-processing 
based on their model’s predictions. We extended this approach to ac-
count  for  a  highly  imbalanced  dataset.  Evaluation  of  a  highly  imbal-
anced dataset through random selection alone would require unfeasibly 
large numbers of audio frames to be manually reviewed to ensure suf-
ficient  target  calls  were  captured  to  give  reliable  evaluation  metrics

Sankupellay, M., Konovalov, D., 2018. Bird call recognition using deep convolutional 

neural network, ResNet-50. In: Proceedings of Acoustics. 

Sekercioglu, C.H., et al., 2008. Climate change, elevational range shifts, and bird 

extinctions. Conserv. Biol. 22 (1), 140–150. 

Settles, B., Craven, M., 2008. An analysis of active learning strategies for sequence 

labeling tasks. In: Proceedings of the 2008 Conference on Empirical Methods in 
Natural Language Processing, pp. 1070–1079. 

Teixeira, D., et al., 2022. Fledge or fail: Nest monitoring of endangered black-cockatoos 
using bioacoustics and open-source call recognition. Ecol. Inform. 69, 101656. 
Thakur, A., Thapar, D., Rajan, P., Nigam, A., 2019. Deep metric learning for bioacoustic 

classification: Overcoming training data scarcity using dynamic triplet loss. 
J. Acoust. Soc. Am. 146 (1), 534–547. 

Threatened Species Scientific Committee (TSSC), 2005. Commonwealth Listing Advice

100 target calls. To overcome this challenge, we applied a random se-
lection  approach  that  was  stratified  across  the  model’s  predictions 
(logits).  While  this  approach  substantially  reduced  class  imbalance 
within our evaluation data and allowed for the calculation of reliable 
evaluation  metrics  (Raeder  et  al.,  2012),  the  nature  of  this  approach 
alters  the  distribution  of  data  and  prohibits  evaluation  metrics  being 
generalised  to  the  unlabelled  data.  Additional  research  is  needed  to 
investigate more appropriate evaluation methods for highly imbalanced 
and unlabelled test data.

Active  learning  iterations  used  a  batch  size  of  64,  ten  epochs  and 
learning rate of 0.001. We used a grid search technique (Mohri et al., 
2018) to tune hyperparameters of the final model including the number 
of epochs, batch size and learning rate. 

2.3.3. Active learning framework 

We  applied  an  active  learning  approach  to  iteratively  train  and 
improve  the  CNN  model.  The  active  learning  approach  is  depicted 
within Fig. 3 and described below.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention any specific techniques such as stratified splitting, oversampling, undersampling, or diverse data collection to address data bias during preprocessing of the deep learning pipeline. However, it does discuss a technique used to mitigate the impact of data bias caused by the active learning process. This technique involves selecting audio frames for review based on the model's predictions, which helps to account for a highly imbalanced dataset. Furthermore, a random selection approach stratified across the model's predictions (logits) was employed to reduce class imbalance within the evaluation data. This approach altered the distribution of data but allowed for the calculation of reliable evaluation metrics. It should be noted that additional research is required to investigate more appropriate evaluation methods for highly imbalanced and unlabelled test data.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

tions of the training data, and the gap between these and the test
data on which the developed algorithms will be evaluated.28

42. Seresinhe, C.I., Preis, T., and Moat, H.S. (2017). Using deep learning to
quantify the beauty of outdoor places. R. Soc. Open Sci. 4, 170170.
43. Berendsohn, W., G€untsch, A., Hoffmann, N., Kohlbecker, A., Luther, K.,
and M€uller, A. (2011). Biodiversity information platforms: from standards
to interoperability. ZooKeys 150, 71–87.

44. Botella, C., Joly, A., Bonnet, P., Monestiez, P., and Munoz, F. (2018).
Species distribution modeling based on the automated identiﬁcation of
citizen observations. Appl. Plant Sci. 6, e1029.

45. Affouard, A., Joly, A., and Bonnet, P.. Pl@ntNet automatically identiﬁed
occurrences, accessed via GBIF.org on 2020-07-27 https://doi.org/10.
15468/mma2ec.

46. Fox, N., Mancini, F., Graham, L., Sutter, L., and August, T.
Photosearcher 1.0. https://docs.ropensci.org/photosearcher/.

(2020).

of images, using GPS location, focusing on keywords in
image metadata, or using high-level AI classiﬁers to re-
move non-target images.

3. What is the appropriate taxonomic resolution for your
study? This will be driven by your research question, as
well as an assessment of the AI naturalist’s accuracy. Clas-
siﬁers will tend to be more accurate at higher taxonomic
levels, but this may vary between taxonomic groups.

4. What reporting biases exist in your dataset? For example,
to what degree are charismatic species over-represented,
or nocturnal species under-represented? Can you ﬁlter the
data, or model the results to account for these biases if
they are relevant?

5. Do reporting biases change over space or time? We
observed signiﬁcant differences in reporting bias between
urban and rural settings, and we anticipate that temporal
biases are likely to exist where public interest in elements
of the natural environment change over time.

the Pl@ntNet user community (currently 1.8 million user accounts). At the time
of writing, the CNN architecture used is the inception model49 extended with
batch normalization.50 The network is pre-trained on the commonly used Im-
ageNet dataset and ﬁne-tuned on Pl@ntNet data. Pl@ntNet currently covers
30,261 species illustrated by more than 2.9 million images. The taxonomic
coverage of our study is therefore one to three orders of magnitude larger
than previously published studies making use of automated species identiﬁca-
tion for ecological research. The training of Pl@ntNet CNN requires the mobi-
lization of a high-performance computing infrastructure and expertise in deep,
distributed, and large-scale learning. Thus, the resulting classiﬁcation tool is in
itself a major advance in biodiversity data science.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not explicitly mention any specific metrics used to evaluate the performance of the deep learning model. However, it can be inferred that some form of evaluation was conducted given the statement about the classifier being more accurate at higher taxonomic levels. Common metrics used to evaluate deep learning models include accuracy, precision, recall, and F1 score. Accuracy measures the proportion of correct predictions out of all predictions made. Precision measures the proportion of true positive predictions out of all positive predictions made. Recall measures the proportion of true positive predictions out of all actual positives. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both. Given the nature of the task, it is possible that multiple metrics were used to evaluate the performance of the deep learning model.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 3 presents the fit statistics for the various models for the in-
dependent evaluation dataset. This is an important component for the 
evaluation of ML models as it allows one to verify possible overfitting. 
The  AutoML  models  achieved  the  best  results.  TabPFN  exhibited  the 
highest performance among all models, achieving an Accuracy of 91% 
and  a  F1-score  of  93%.  The  AutoGluon  model  achieved  comparable 
results, with a slightly lower Accuracy of 87% and a F1-score of 93%. 
The ensemble  ML models produced lower fits, with the XGBoost pro-
ducing  an  Accuracy  of  87%  and  an  F1-score  of  90%.  The  RF  model 
produced the lowest fit metrics (Accuracy = 73%, F1-score = 79%).

development  and  60%  for  evaluation,  the  improvements  in  Accuracy

tuning dataset is excluded from the training process. The model is then 
developed using the remaining dataset (i.e. the training dataset), and its 
performance is evaluated using the tuning dataset. The average perfor-
mance of the k models developed on the validation groups represents the 
performance of the machine learning technique for the selected hyper-
parameters (Velasco Hererra et al., 2022). For this study, a value of 5 is 
assumed  for  k.  Grid  search  is  employed  to  evaluate  the  data-driven 
model’s  performance  using  each  combination  of  predefined  hyper-
parameters and identify the best hyperparameters. The maximum depth 
of the tree, the number of trees in the ensemble model, and the learning 
rate (which shows how fast the model learns) are tuned in this study as 
the influential hyperparameters (Cakiroglu et al., 2022).

Data-driven models depend on a number of parameters, known as 
hyperparameters,  which  are  employed  to  enhance  and  regulate  the 
learning  procedure.  Optimal  hyperparameter  selection 
leads  to 
improved model accuracy and enhanced prediction performance (Aze-
dou et al., 2023). To fine-tune hyperparameters in data-driven models, 
the development dataset (comprising all data records except the evalu-
ation data records) is divided into two categories: the training dataset 
and the tuning dataset. Given the size constrains of the dataset used in 
the  analysis  and  the  need  to  utilize  all  development  data  records  in 
training  to  enhance  the  model’s  generalization  ability,  k-fold  cross- 
validation  was  chosen  as  the  preferred  approach  for  hyperparameter 
tuning (Saha et al., 2022). In this technique, the development dataset is 
evenly divided into k groups. During each iteration, one group called the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In the given context, the performance of the deep learning models is primarily evaluated using two key metrics: Accuracy and F1-score.

Accuracy is a common metric used to measure the overall effectiveness of a classifier. It is calculated by dividing the total number of correct predictions by the total number of instances. However, while accuracy can be useful when dealing with balanced datasets, it may not provide a complete picture of the model's performance when working with imbalanced classes.

To address this issue, another metric called the F1-score is often used alongside accuracy. The F1-score is the harmonic mean of precision and recall, where precision measures the proportion of true positive predictions out of all positive predictions made, and recall measures the proportion of true positives out of all actual positives. By combining these two metrics, the F1-score provides a more comprehensive view of the model's performance, taking into account both false positives and false negatives.

In the context provided, the TabPFN model achieved the highest performance among all models, with an Accuracy of 91% and an F1-score of 93%. Similarly, the AutoGluon model also performed well, with an Accuracy of 87% and an F1-score of 93%. On the other hand, the ensemble ML models such as XGBoost and Random Forest (RF) produced lower fit metrics, with the RF model producing the lowest fit metrics (Accuracy = 73%, F1-score = 79%).
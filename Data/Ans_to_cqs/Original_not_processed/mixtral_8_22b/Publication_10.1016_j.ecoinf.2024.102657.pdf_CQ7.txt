Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

grams, and created the metadata .csv file necessary for training the CNN 
and k-NN. Identical in structure to the metadata file outlined above, this 
file contained the metadata for the training set as opposed to the met-
adata for the entire dataset. 

No additional preprocessing was applied to the spectrograms as the 
goal was to do the least amount of preprocessing possible in order to 
demonstrate  the  power  of  human-machine  teaming.  Further,  when 
assessing the accuracy of the CNN on noise-filtered data versus raw data, 
the accuracy levels were similar: after 10 trials, the CNN’s accuracy on 
the  filtered  data  averaged  to  99.22%  and  the  accuracy  on  the  non- 
filtered data averaged to 99.25%.

Training Data Description 

80% of each 
dialect class 

Total # of Training Songs 
Test Data Description 

2416 
20% of each 
dialect class 

No FOFU 
songs & 80% 
of other 
dialect 
classes 

1734  
All FOFU 
songs & 20% 
of other 
dialect 
classes 

No FOFU or 
LODU songs 
& 80% of 
other dialect 
classes 

1561  
All FOFU 
and LODU 
songs & 20% 
of other 
dialect 
classes 
700  

Total # of Test Songs (Songs 
from test set with location 
data) 

Test Number 
Match, Both Correct 
Match, Both Incorrect 
Mismatch, CNN Correct 
Mismatch, k (cid:0) NN Correct 
Mismatch, Both Incorrect 

460 

1 
455 
0 
2 
3 
0 

499  

2a 
443 
17 
5 
4 
30 

2b 
466 
0 
26 
7 
0 

3a 
396 
76 
0 
8 
220 

3b 
534 
0 
16 
150 
0  

remaining 20% of each of the other dialect classes.

3.3. Test 3: Training on 80% of data and adding in 2 new dialect classes

2.2. Training 

Next, DialectDecoder trained the two classification networks based 
on the labeled training data. Each dialect class was split into a training 
set, a validation set, and a testing set by taking the first 70% of songs in 
the folder as the training set, the next 10% as the validation set, and the 
last 20% as the test set to reduce variability between tests. We note that 
while the validation set was used to tune the hyperparameters of the 
CNN, the only hyperparameter of the k-NN is the value of k, which we 
fixed as k = 4, as this value yielded the best performance after testing 
several values. Hence both the training and validation sets, totaling 80% 
of the full dataset, were used directly to fit the k-NN. Thus, we used 80% 
of the full data set to train and validate the CNN spectrogram classifier 
and to train the k-NN location classifier, and reserved 20% of the data set 
to test the two systems’ ability to classify songs correctly. 

2.2.1. CNN structure

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The data was split into three parts - training set, validation set, and testing set. For each dialect class, the first 70% of songs in the folder were taken as the training set, the next 10% as the validation set, and the last 20% as the test set. This method aimed to reduce variability between tests. While the validation set was used to tune the hyperparameters of the Convolutional Neural Network (CNN), the only hyperparameter of the k-Nearest Neighbors (k-NN) algorithm was fixed as k = 4. Therefore, both the training and validation sets, totaling 80% of the full dataset, were used directly to fit the k-NN. As such, 80% of the full data set was used to train and validate the CNN spectrogram classifier and to train the k-NN location classifier, while the remaining 20% of the data set was reserved to test the two systems’ ability to classify songs correctly.
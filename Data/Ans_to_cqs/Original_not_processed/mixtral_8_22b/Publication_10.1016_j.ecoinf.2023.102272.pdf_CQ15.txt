Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.5. Model selection procedure 

The model selection procedure was performed as follows (Capinha 
et  al.,  2021;  Van  Kuppevelt  et  al.,  2020):  we  randomly  generated  5 
models for each of the four available deep-ANN architecture types (20 
models in total) and trained each one with a small subset of the training 
data  (data  partition  At)  for  4  epochs  (an  “epoch”  corresponds  to  the 
complete training dataset being passed forward and backward across the 
network  one  time;  Capinha  et  al.,  2021).  The  accuracy  of  candidate 
models,  as  provided  by  mcfly  (i.e.,  the  “proportion  of  cases  correctly 
classified”), was then compared using a left-out validation data set (data 
partition Av) and the model with the highest performance was selected 
for training on the full training data (data partition Bt; Bt = At + Av) for 
up to 30 epochs.

Table 3 
Parameters tested and values for each classical machine learning model used and each test year. xgbTree = extreme gradient boosting tree, RF = random forest, NNET =
neural network, DNN = deep neural network.  

Model 

xgbTree 

RF 
NNET 

DNN 

Parameter 

2013 

2014 

2015 

2016 

2017 

2018 

2019 

Nrounds 
max_depth 
Eta 
Gamma 
colsample_bytree 
min_child_weight 
Subsample 
Mtry 
Size 
Decay 
layer1 
layer2 
layer3 
hidden_dropout 
visible_dropout 

150 
3 
0.4 
0 
0.8 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.8 
1 
1 
2 
3 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150  
3  
0.3  
0  
0.6  
1  
1 
2 
5  
0.1 
1  
0  
0  
0  
0

10.3 
1.6 
0.6 
74.9 
0.8 
0.5 
11.4 

8.4 
1.0 
0.7 
79.4 
0.5 
0.3 
9.6 

10.4 
0.9 
0.8 
75.3 
0.7 
0.5 
11.4  

represented  correspond  to  the  lower  decreases  or  increases  in  the 
number of eggs (i.e., 1 to 25% and > 25 to 50%; Table 1). 

3.2. Deep learning model accuracy, predictive performance, and predictor 
importance 

Concerning the performance of the candidate models, the best vali-
dation accuracies were achieved by model 12 in all the years (accuracy 
= 0.833 ± 0.005; mean of years ± sd) (Fig. 2), a model having a Deep 
Convolutional Long Short-Term Memory (DeepConvLSTM) architecture. 
Model  19,  also  with  a  DeepConvLSTM  architecture  had  high  vali-
dation accuracies too (accuracy = 0.829 ± 0.003; mean of years ± sd), 
followed  by model 14, a Convolutional Neural Network  (CNN) archi-
tecture reaching a validation accuracy of 0.821 ± 0.005 (mean of years 
± sd).

each  test  year,  by  computing  AUC  with  package  cvAUC  in  R  (LeDell 
et  al.,  2014;  R  Core  Team,  2022).  Our  aim  was  to  evaluate  which 
approach  (deep  learning  versus  classical  machine  learning)  would 
deliver better predictions.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given information, it is not explicitly stated what specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) are applied in the deep learning pipeline. However, there are some indications about the use of certain hyperparameters related to optimization. For instance, the table shows parameters like 'Eta', 'Gamma', 'Decay', and 'hidden\_dropout'. These can be associated with optimization techniques.

In general, Eta is often referred to as the learning rate, Gamma is a regularization parameter, Decay is used to reduce the learning rate over time, and hidden\_dropout is a technique used to prevent overfitting during training. While these terms suggest the application of optimization techniques, they do not directly indicate whether SGD, Adam, or any other specific optimizer is employed. Therefore, based solely on the provided context, I cannot definitively state which optimization techniques are applied in the deep learning pipeline.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Kingma, D.P., Ba, J., 2014. Adam: A Method for Stochastic Optimization. arXiv preprint 

arXiv:1412.6980. 

Kornblith, S., Shlens, J., Le, Q.V., 2019. Do better imagenet models transfer better?. In: 

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 
pp. 2661–2671. https://doi.org/10.1109/CVPR.2019.00277. 

Krause, J., Sapp, B., Howard, A., Zhou, H., Toshev, A., Duerig, T., Philbin, J., Fei-Fei, L., 
2016. The unreasonable effectiveness of noisy data for fine-grained recognition. In: 
European Conference on Computer Vision. Springer, pp. 301–320. https://doi.org/ 
10.1007/978-3-319-46487-9_19. 

Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classification with deep 
convolutional neural networks. In: Advances in Neural Information Processing 
Systems, pp. 1097–1105. 

Pizer, S.M., Amburn, E.P., Austin, J.D., Cromartie, R., Geselowitz, A., Greer, T., ter Haar

Model 

AlexNet 
VGG-19 
ResNet-101 
DenseNet-201 

Accuracy 

Raw 

Train 

98.75% 
96.77% 
82.25% 
93.71% 

Validation 

97.16% 
98.30% 
89.04% 
91.30% 

Test 

96.16% 
95.15% 
83.30% 
86.48% 

Pre-processed  

Train 

98.21% 
96.94% 
77.25% 
91.61% 

Validation 

97.92% 
97.92% 
79.02% 
87.33% 

Test 

95.98% 
96.52% 
75.44% 
86.29%  

Table 5 
Accuracy of the models swapping the testing sets (source → target).  

Model 

Accuracy 

Raw → Pre-processed 

Pre-processed → Raw 

AlexNet 
VGG-19 
ResNet-101 
DenseNet-201 

82.35% 
82.70% 
69.22% 
65.26% 

54.76% 
78.87% 
29.56% 
33.97%  

Fig. 5. Confusion matrices for the VGG-19 architecture.  

et al., 2017) and SmoothGrad (Smilkov et al., 2017) methods over each 
model. These methods plot a point cloud, where the density denotes the 
input space relevance. Thus, a higher density in a region suggests that 
the network ponderates it the most when classifying.

(13)  

4.2. Architecture configurations 

Convolutional neural networks (CNN) outstand over DL techniques 
by disentangling high-level representations across multiple processing 
layers. CNN’s process data on two levels: a convolutional block for the 
automatic  feature  extraction,  and  fully  connected  layers  to  establish 
feature-output  correlation.  The  convolutional  block  comprises  con-
volutional,  ReLU,  and  max-pooling  layers.  Each  set  of  convolutional 
layers diversify features by applying a set of parallel filters that process 
local sections of the input space. The feature vector integrates low-level 
local features from the first layers and higher-level representations from 
the latest ones.

Kamilaris, A., Prenafeta-Boldú, F.X., 2018. Deep learning in agriculture: a survey. 

Comput. Electron. Agric. 147, 70–90. https://doi.org/10.1016/j. 
compag.2018.02.016. 

Kaya, A., Keceli, A.S., Catal, C., Yalic, H.Y., Temucin, H., Tekinerdogan, B., 2019. 

Analysis of transfer learning for deep neural network based plant classification 
models. Comput. Electron. Agric. 158, 20–29. https://doi.org/10.1016/j. 
compag.2019.01.041. 

Keenan, R.J., Reams, G.A., Achard, F., de Freitas, J.V., Grainger, A., Lindquist, E., 2015. 

Dynamics of global forest area: results from the fao global forest resources 
assessment 2015. For. Ecol. Manag. 352, 9–20. https://doi.org/10.1016/j. 
foreco.2015.06.014. 

Keni, N.D., Ansari, R.A., 2017. Content based image retrieval for leaf identification using 

structural features and neural networks. In: 2017 4th International Conference on 
Signal Processing and Integrated Networks (SPIN). IEEE, pp. 298–303. https://doi. 
org/10.1109/SPIN.2017.8049963.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper titled "Adam: A Method for Stochastic Optimization" by Kingma and Ba discusses the use of the Adam optimizer in stochastic gradient descent algorithms. This method is commonly used in deep learning pipelines to improve convergence rates and reduce training time. Other popular optimization techniques include Stochastic Gradient Descent (SGD), which updates model parameters iteratively based on the gradients of the loss function, and Momentum, which helps accelerate SGD in relevant directions and dampens oscillations. Additionally, other adaptive learning rate methods such as Adagrad, Adadelta, and RMSprop have also been proposed to further enhance the performance of deep learning models.
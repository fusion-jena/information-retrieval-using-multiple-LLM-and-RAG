Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4.2.  Hyperparameters 

Hyperparameter tuning was done manually. As a starting point the 
learning rate was set as low as possible and the number of iterations as 
high as computationally feasible (Hastie et al., 2009). The learning rate, 
which is also called a shrinkage parameter, determines the contribution 
of each tree to the model. The maximum tree depth which reflects the 
degree of interaction in the model is usually low. GBRT performs best 
using fairly shallow trees, so-called tree stumps.

The  tuned  hyperparameters  of  the  final  models  are  shown  in  

Table 2. 

2.4.3.  GAM modelling of most influential parameters 

The top four most influential variables from the GBRT models (as 
indicated by VI scores) were included in a series of generalised additive 

Table 2 
Tuned model parameters.       

Model 

splits 

Number of iterations 

Learning rate 

Tree depth  

RI CS 
RI CS 
RI NW 
RI NW 
RY CS 
RY NW 

2 
3 
2 
3 
2 
2 

120 
102 
106 
144 
500 
500 

0.05 
0.05 
0.01 
0.05 
0.005 
0.005 

4 
4 
6 
6 
8 
2 

Number of iterations was defined by early stopping. Subsample rate was 0.75% 
in  all  cases.  Parameters  for  2-part  split  were  defined  through  grid  search, 
whereas parameters for 3-part split were tuned manually.

3.3.  GBRT model performance

C) 

ICES Herring Assessment Working Group (HAWG) report 2015; Table 4.6.2.4 

http://www.ices.dk 

ICES HAWG report 2015; Table 5.6.12 

http://www.ices.dk 

ICES HAWG report 2014; Table 4.6.2.13 

http://www.ices.dk 

ICES HAWG report 2015; Table 5.6.14  

ICES HAWG report 2014; Table 4.6.2.13 

http://www.ices.dk 

ICES HAWG report 2014; Table 4.6.2.14  

∘ 
x 2

∘ 
Reynolds Historical Reconstructed SST (2
resolution) as derived from the 
Reynolds Optimally Interpolated SST (from the Advanced Very High Resolution 
Radiometer, AVHRR) and in-situ observations, available from the NASA Jet 
Propulsion Laboratory (extracted through Hydrax/OpenDAP server), provided 
by Reynolds, National Climatic Data Center 
MET Office Hadley Centre EN4 quality controlled ocean data version: EN.4.2.1. 

https://www.esrl.noaa.gov 

https://www.metoffice.gov.uk/ 
hadobs/en4/download-en4-2-1. 
html 

sal CS,NW 

Salinity 5 m (PSU) 

Month 

Month, included to account for a month 
of capture

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there is no information provided about the specific hardware resources used for training the deep learning model in this given context. Therefore, it cannot be determined whether GPUs, TPUs or any other type of hardware resource was utilized during the training process.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In this work, we resorted to purely deterministic variants of these 

architectures.  The  models  were  implemented  using  the  deep  learning 
framework PyTorch Lightning (Falcon, 2019) which is built on top of 
PyTorch  (Paszke  et  al.,  2019)  and  enables  improved  scalability.  The 
hyperparameters were tuned using an Optuna-based (Akiba et al., 2019) 
hyperparameter optimization procedure.

4.3. The prediction task

2.4. Model training 

23,401  datacubes  (97.9%  of  the  original  EarthNet2021  training 
dataset) were used for training, 500 datacubes for validation, and three 
datacubes were discarded (see above). The model was trained using the 
L2  loss  determined  on  the  predicted  and  observed  RGBI  channels 
(ignoring the cloud-contaminated pixels). The EarthNetScore (described 
in Section 2.5) is used for validation. The learning rate is set to 0.0003 

and the batch size is set to 4. We decided to use the AdamW optimizer, 
which,  unlike  the  standard  Adam  optimizer  (Kingma  and  Ba,  2014), 
decouples the weight decay and has also shown to improve on gener-
alization  (Loshchilov  and  Hutter,  2017).  The  SGConvLSTM  and  the 
SGEDConvLSTM were trained for 92 and 45 epochs, respectively. For 
completeness,  a  full  list  of  the  model  parameters  is  provided  in 
Tables A.2 and A.3 in the Supporting Information. 

2.5. Evaluation 

2.5.1. EarthNetScore

2.2. Data 

The data used here was provided as part of the EarthNet2021 Chal-
lenge and consists of 23,904 training datacubes located across Europe

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not provide information about any specific preprocessing steps taken before training the deep learning models. However, it can be assumed that some form of preprocessing would have been necessary due to the nature of the dataset, which includes datacubes with multiple channels (RGBI). Common preprocessing techniques for such datasets include normalization, scaling, or cleaning to ensure consistent input formats and reduce noise. Normalization typically involves adjusting pixel values to a common scale (e.g., between 0 and 1), while scaling may involve resizing images or adjusting their dimensions. Cleaning could entail removing or correcting corrupted or missing data points. Nevertheless, without explicit details from the provided context, one cannot definitively state what preprocessing steps were employed in this case.
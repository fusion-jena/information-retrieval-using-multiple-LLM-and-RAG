Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

by class sample size. We used the SGD (stochastic gradient descent) optimizer with an initial learning rate of 
0.01 for all models except MansNet, which was initially set at 0.1. The learning rate was reduced by a factor of 
10 after steps of 30 epochs. We used batch normalization and models were trained for 150 epochs using Nvidia 
Tesla K80 or P100 GPUs.

Scientific Reports |         (2021) 11:7580  | 

https://doi.org/10.1038/s41598-021-87210-1

4

Vol:.(1234567890)www.nature.com/scientificreports/Model

#Params 
(million)

Model speed 
(ms)

Top-1 accuracy Top-2 accuracy Top-3 accuracy Top-4 accuracy Top-5 accuracy Macro recall

Wide-ResNet101 124.9

InceptionV3

ResNet101

MnasNet-A1

24.0

42.6

1.0

5.46

3.34

3.33

3.28

0.9171

0.9162

0.9133

0.8579

0.9627

0.9610

0.9633

0.9335

0.9782

0.9767

0.9787

0.9609

0.9850

0.9834

0.9852

0.9730

0.9897

0.9882

0.9892

0.9814

0.8552

0.8519

0.8499

0.7689

Macro 
precision

0.8831

0.8881

0.8740

0.8250

Macro 
precision

0.8831

0.8881

0.8740

0.8250

Table 1.   Comparison of model size, speed, and performance, ordered by top-1 accuracy. Text in bold indicates 
the best value in each category. Wide-ResNet101 has the highest top-1 accuracy and macro precision but 
was substantially slower than the other models. MnasNet-A1 was the fastest model but had relatively poor 
performance. InceptionV3 was relatively fast while maintaining good model performance with the highest 
precision and second highest accuracy and recall.

Table 2.   InceptionV3 confusion matrix showing the number of test images for each prediction.

information passes through the end of the network. (1) ResNet-10128 was proposed to address this problem by 
using skip connections that allow useful layer output to pass over groups of layers (or residual blocks) and thus 
penetrate much further into deep networks. (2) Wide-ResNet-10129, on the other hand, is wider but shallower 
than ResNet-101. This difference in architecture allows information to more easily pass through the network 
while providing a greater number of channels to maintain performance. (3)  InceptionV330 has fewer layers but 
achieves  greater  computational  efficiency  by,  for  example,  using  factorized  convolution,  which  breaks  down 
channels into simpler linear sequences while maintaining spatial context within the image. (4) MnasNet-A131 
was designed to be faster for mobile devices and thus has the fewest layers of the models compared here. The

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning models mentioned in the given context include Wide-ResNet101, InceptionV3, ResNet101, and MnasNet-A1. All these models, except MnasNet-A1, use the Stochastic Gradient Descent (SGD) optimizer with an initial learning rate of 0.01. For MnasNet-A1, the initial learning rate was set at 0.1. The learning rate was reduced by a factor of 10 after every 30 epochs. Batch normalization was also employed during training. These models were trained for 150 epochs using either Nvidia Tesla K80 or P100 GPUs.
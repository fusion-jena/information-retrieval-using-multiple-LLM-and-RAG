Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Each rectangle indicates the operations performed at a block of layers. Conv and SConv stand for regular convolution and depth-wise separable 
convolution, respectively. The values that follow such operations indicate the number of filters, filter size, stride, and dilation rate. Regarding the 
maxpooling operation, the values correspond to the kernel dimension and stride. Dropout refers to the number of neurons randomly turned off during 
each training inference. The number of neurons in the dense layer of the classifier module corresponds to the number of classes in the dataset. 

All architectures were modified mainly in the block of fully connected layers, where we opted to use just the output layer after the feature extractor 
instead of two fully connected layers. Additionally, residual blocks were used at the architecturesâ€™ stems rather than in the full feature extractor. All 
those modifications were experimentally determined.

The selected CNNs typically contain an encoder stage, often called a 
feature  extractor,  which  reduces  the  spatial  resolution  of  the  input 
through convolution and pooling operations in consecutive layers. The 
encoder is then followed by fully connected layers of neurons that pre-
dict the input image class based on the previously extracted features to 
perform image classification. 

After  briefly  introducing  the  chosen  CNN  architectures,  we  will 
discuss the decision fusion process. That process is based on a decision 
committee  that collects  the  predictions  of the  respective  CNN-trained 
models.  We  will  also  explain  uncertainty  assessment  based  on  model 
predictions. 

3.1. VGG

3.1. VGG 

To  date,  the  VGG  (Visual  Geometry  Group)  network  is  one  of  the 
most  popular  image  classification  architectures,  and  pre-trained  VGG 
models  are  commonly  used  in  transfer  learning  (e.g.,  fine-tuning) 
schemes.  It  was  proposed  in  Simonyan  and  Zisserman  (2015),  which 
aimed at investigating the effects of increasing convolutional network 
depth.  Evaluated  on  the  ImageNet  Large  Scale  Visual  Recognition 
Challenge  (ILSVRC)  (Russakovsky  et  al.,  2015)  in  2015,  the  VGG16 
model outperformed all previous participants, which comprised several 
state-of-the-art architectures.

Appendix A. Network architectures 

The network architectures of the models evaluated in the experiments are described in detail in Fig. 9. The light green areas in that figure represent 
feature extractor modules based on the different architectures, i.e., VGG, ResNet, and Xception. The orange area represents the architecture of the 
classifier module, to which the outputs of each feature extractor are submitted.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The pipeline uses a specific type of deep learning model known as Convolutional Neural Networks (CNN). This can be deduced from various parts of the given context. For instance, it mentions that the chosen CNN architectures have an encoder stage, often referred to as a feature extractor, which employs convolution and pooling operations to reduce the spatial resolution of the input images. Furthermore, the text discusses the VGG network, which is a well-known CNN architecture designed for image classification tasks. Lastly, Appendix A explicitly states that the evaluated models in the experiments are based on different architectures, including VGG, ResNet, and Xception, all of which are types of CNNs. Therefore, it can be concluded that the pipeline utilizes CNNs as its primary deep learning model.
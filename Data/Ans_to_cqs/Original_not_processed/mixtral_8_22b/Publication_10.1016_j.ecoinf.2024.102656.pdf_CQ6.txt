Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

available in Supplementary text S1). This methodology enhances pre-
dictive  accuracy  by  combining  forecasts  from  multiple  statistical 
models, mitigating the uncertainties and biases inherent in relying on a 
single modelling method (Araújo and New, 2007; Hysen et al., 2022). 
Five  statistical  models  were  used  in  this  ensemble  framework, 
including  generalized  linear  models  (GLM),  maximum  entropy  (Max-
Ent),  random  forest  (RF),  generalized  boosting  model  (GBM),  and 
multivariate  adaptive  regression  lines  (MARS).  To  facilitate  model 
training, we employed a 3:1 train-test split for the occurrence points. 
Additionally, following Barbet-Massin et al. (2012), we generated 1000 
random  pseudo-absence  points  (five  times  the  number  of  occurrence 
points per species) across the study area. These pseudo-absence points 
are crucial for model execution and provide a reference for areas that are 
unlikely to be inhabited by the target species.

such  as  land  use  change  for  agriculture  and  afforestation,  road  con-
struction,  and  mining,  could  further  restrict  the  suitable  habitats  of 
ungulates (Ali et al., 2021; Luo et al., 2015; Luo et al., 2024).

The ensemble prediction was generated using a weighted-averaging 
approach,  assigning  weights  to  individual  models  based  on  their  pre-
dictive  accuracy  on  the  test  data  (Thuiller,  2014).  Evaluation  and 
comparative analyses were conducted for both individual models and 
the ensemble model via two widely recognized metrics: the area under 
the curve (AUC) and the true skill statistic (TSS). AUC values exceeding 
0.9 were considered as excellent, 0.9–0.8 as good, 0.8–0.7 as moderate, 
and < 0.7 as poor (Eskildsen et al., 2013). Similarly, TSS values were 
categorized as excellent (>0.75), good (0.75 to 0.4), and poor (<0.4) 
(Eskildsen et al., 2013). This comprehensive assessment ensures a robust 
understanding of model performance and aids in reliable predictions of 

species distribution across the central Iranian plateau. 

2.6. Modelling landscape resistance

Area of core 
habitats (km2) 

% of protected 
core habitats 

Length of roads 
intersecting core habitats 
(km) 

wild goat 
50 km 
70 km 
100 km 

Urial 
50 km 
70 km 
100 km 

310.15 
398.36 
443.25 

950.60 
1100.21 
1200.81 

Jebeer gazelle 
50 km 
70 km 
100 km 

930.56 
1050.30 
1232.81 

85.32 
81.20 
80.30 

83.30 
82.15 
78.20 

88.30 
86.32 
84.29 

4 
5.5 
6.5  

16 
19 
23  

18 
20 
23

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The provided context does not directly address the specific preprocessing steps involved before training a deep learning model such as normalization, scaling, or cleaning. However, it does mention some relevant aspects related to preparing data for modeling purposes. In the given context, the authors employ a 3:1 train-test split for the occurrence points to facilitate model training. Additionally, they generate 1000 random pseudo-absence points across the study area, which serve as a reference for areas unlikely to be inhabited by the target species. Although these steps do not cover all possible preprocessing techniques for deep learning models, they demonstrate essential practices for preparing datasets for machine learning tasks. It is important to note that preprocessing steps can vary depending on the nature of the dataset and the requirements of the specific deep learning model being trained. Common preprocessing techniques include normalizing numerical features, encoding categorical variables, handling missing values, and splitting the dataset into training and testing sets.
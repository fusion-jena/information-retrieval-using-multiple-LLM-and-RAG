Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The proposed BEiT model exhibits considerable performance im-
provements in the classification task compared to various conventional
deep learning models, as summarized in Table 16. Achieving an accu-
racy of 98.1%, the BEiT model exceeds all other State-of-the-art models,
including the previously top-performing model by Farian et al. (96.1%).
This improvement in accuracy is statistically significant, with signifi-
cance denoted by the α markers, indicating p < 0.05 using a two-sided
McNemar’s test against the BEiT model. It achieves a precision of
98.2%, a recall of 97.7%, and an F1-score of 96.5%, which are higher
than those of other models such as the ViT-30, ViT-20, and LAMP-LFD
models. Vivek et al.’s model, which shows high precision (94.5%) and
recall (95.0%), still falls short of the BEiT model’s comprehensive

Table 11
Ablation study evaluating BEiT model performance without added noise.

Model

Acc

Loss

AUC

CK

Precision

Recall

F1

98.33
99.33
97.33

0.092
0.082
0.121

For our research reproducibility in Algorithm 1, the definitions
include T and ℰ, denoting the paths to the training and testing dataset
directories, respectively. The functions Dtrain and Dtest represent data
generators for training and testing datasets, where parameters such as α,
β, γ, and δ control aspects like rescaling and augmentation. The batch
size for training and testing is denoted by B, while Gtrain and Gtest
represent the respective data generators. The class weights computed for
addressing class imbalance are denoted by wc, and M stands for our
proposed model, encompassing its architecture and compilation pa-
rameters. The number of training epochs is represented by n, and P and
Q denote the steps per epoch for training and validation, respectively.
Predicted outputs and their class labels from the model are represented
by ̂y and ̂yc, while ytrue signifies the true class labels from the testing
dataset. In our model reproducibility in the testing phase in Algorithm 2,

individual extraction of features. Such techniques acquire characteris-
tics directly from the unprocessed data,
improving accuracy and
robustness (Archana and Jeevaraj, 2024; Mumuni and Mumuni, 2024;
Yang et al., 2023a). In addition, deep learning effectively manages
extensive datasets, enhancing the ability to apply knowledge across
many scenarios in agricultural settings.

Nawaz, M., Nazir, T., Javed, A., Tawfik Amin, S., Jeribi, F., Tahir, A., 2024. CoffeeNet: a
deep learning approach for coffee plant leaves diseases recognition. Expert Syst.
Appl. 237, 121481.

Oyelade, O.N., Ezugwu, A.E.-S., Mohamed, T.I.A., Abualigah, L., 2022. Ebola

optimization search algorithm: a new nature-inspired metaheuristic optimization
algorithm. IEEE Access 10, 16150–16177.

Pacal, I., 2024. Enhancing crop productivity and sustainability through disease

identification in maize leaves: exploiting a large dataset with an advanced vision
transformer model. Expert Syst. Appl. 238, 122099.

Pan, B., et al., 2024. Research on species identification of wild grape leaves based on

deep learning. Sci. Hortic. 327, 112821.

Phillips, J., Durand-Morat, A., Nalley, L.L., Graterol, E., Bonatti, M., de la Pava, K.L.,
Yang, W., 2024. Understanding demand for broken rice and its potential food
security implications in Colombia. J. Agric. Food Res. 15, 100884.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't any explicit information about specific optimization techniques used in the deep learning pipeline within the given context. However, it can be generally assumed that popular optimization algorithms such as Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), or Root Mean Square Propagation (RMSProp) could have been employed due to their widespread use in deep learning applications. These methods help minimize loss function during the training process, enabling the model to learn more efficiently. Nonetheless, without further details regarding the implementation of the BEiT model or other mentioned models, we cannot definitively state which optimization technique was utilized.
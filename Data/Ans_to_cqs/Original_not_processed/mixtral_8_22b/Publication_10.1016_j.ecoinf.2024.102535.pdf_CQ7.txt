Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The batch size was 32, and the early stopping procedure was used to 
avoid over-fitting. The patience parameter, which controls the number 
of epochs without improvements in the validation loss, was set to 10. 
Each network, with a particular architecture and considering a specific 
substratum characterization, was trained and executed three times, each 
time with a different (random) initialization of the trainable parameters 
and with a different data fold. As already mentioned, the results shown 
in  the  next  section  are  averages  of  those  three  executions.  Data 
augmentation was applied to all extracted patches: a 90∘  rotation and 
vertical and horizontal flips. 

4.4. Performance metrics

The  experimental  results  suggest  that  there  might  be  room  to 
improve the generalization capacity of the deep learning classifiers. That 
could  be  achieved  by  better  exploiting  the  training  data,  e.g.,  with 
additional data augmentation techniques, using recent advances in un-
supervised learning techniques, such as self-supervised methods, or by 
just simplifying the classifiers in terms of the number of parameters, thus 
reducing the risk of overfitting. 

Another direction for continuing this research is to exploit the clas-
sification uncertainty further in an active learning context. The uncer-
tainty measure could be used in interactively training the deep learning 
models,  selecting  high-uncertainty  samples  in  the  datasets,  and 
increasing their importance in the computation of the loss function. 

CRediT authorship contribution statement

6. Conclusions 

In  this  work,  six  state-of-the-art  deep  learning  architectures  were 
evaluated  for  the  problem  of  deep-sea  substratum  characterization. 
Additionally, a classification decision committee (CDC) constituted by 
an  ensemble  of  networks  with  those  architectures,  in  which  the  indi-
vidual predictions are fused through a majority voting mechanism, was 
proposed and evaluated. 

All deep learning models were evaluated on single and multi-label 
classification  problems,  and  three  different  sites  were  considered  in 
the  experiments.  Besides  the  conventional  training  scheme  in  which 
data from a single site is used for training and testing the models, we also 
evaluated  the  models  in  cross-site  scenarios,  aiming  to  assess  the 
generalization  capacity  of  the  different  architectures  and  that  of  the 

ensemble.

Each rectangle indicates the operations performed at a block of layers. Conv and SConv stand for regular convolution and depth-wise separable 
convolution, respectively. The values that follow such operations indicate the number of filters, filter size, stride, and dilation rate. Regarding the 
maxpooling operation, the values correspond to the kernel dimension and stride. Dropout refers to the number of neurons randomly turned off during 
each training inference. The number of neurons in the dense layer of the classifier module corresponds to the number of classes in the dataset. 

All architectures were modified mainly in the block of fully connected layers, where we opted to use just the output layer after the feature extractor 
instead of two fully connected layers. Additionally, residual blocks were used at the architectures’ stems rather than in the full feature extractor. All 
those modifications were experimentally determined.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, it appears that the data was divided into different folds for training and execution purposes. Each network was trained and executed three times, each time with a different random initialization of the trainable parameters and with a different data fold. However, the exact methodology used to create these folds is not explicitly stated in the provided information. It can only be inferred that some form of stratified sampling or k-fold cross-validation may have been employed due to the mention of multiple executions with different initializations and data folds. Furthermore, the concept of early stopping was utilized to prevent overfitting, indicating that the data was likely partitioned into separate training and validation sets. Unfortunately, without more detailed information about the specific splitting strategy used, it is impossible to provide a definitive answer regarding the precise criteria employed to divide the data for deep learning model training.
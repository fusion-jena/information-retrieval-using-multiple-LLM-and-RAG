Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

A pooling layer offers downsampling functionality that decreases the dimensionality
of the feature maps to achieve translation invariance to the alterations and the biases
incorporated and thus helps in reducing the number of learnable parameters. There are
two types of pooling operations, namely Max Pooling and Global Average Pooling [64].
The ﬁrst one extracts speckles from the input feature maps and offers maximum values
in each of the speckles and leaves the remaining values unattended. The second one
downsamples a feature map with a size equaling product of height and width into an array
of a one cross one by averaging the elements of each feature map by retaining the depth of
the feature map. The advantage of Global Average Pooling lies in reducing the number of
learnable parameters along with offering the CNN with variable sized input.

Convolution is used for feature extraction, in which a kernel is applied to an input
tensor. A feature map is thus obtained through the product of kernel elements and tensor
input. The procedure is then repeated on multiple kernels to obtain random feature maps
that represent different feature extractors. The hyperparameters involved in convolution
operations are the size and number of kernels. The size could be anything from 3 × 3 to
5 × 5 to 7 × 7, and the kernel could be chosen randomly.

applications and relevance to most current MAXENT studies. Divers. Distrib. 2014, 20, 1–9. [CrossRef]
Schmidhuber, J. Deep learning in neural networks: An overview. Neural Netw. 2015, 61, 85–117. [CrossRef]

60.
61. Guo, Y.; Liu, Y.; Oerlemans, A.; Lao, S.; Wu, S.; Lew, M.S. Deep learning for visual understanding: A review. Neurocomputing 2016,

62.

187, 27–48. [CrossRef]
Fukuda, S.; De Baets, B.; Waegeman, W.; Verwaeren, J.; Mouton, A.M. Habitat prediction and knowledge extraction for spawning
European grayling (Thymallus thymallus L.) using a broad range of species distribution models. Environ. Model. Softw. 2013, 47,
1–6. [CrossRef]

63. Harris, D.J. Generating realistic assemblages with a joint species distribution model. Methods Ecol. Evol. 2015, 6, 465–473.

[CrossRef]

64. Lin, M.; Chen, Q.; Yan, S. Network in network. arXiv Prepr. 2013, arXiv:1312.4400.
65. Agarap, A.F. Deep learning using rectiﬁed linear units (relu). arXiv Prepr. 2018, arXiv:1803.08375.
66.

Remote Sens. 2021, 13, 3284

15 of 17

Supplementary Materials: The following are available online at https://www.mdpi.com/article/10
.3390/rs13163284/s1. Table S1: Methodology of the convolution and full connected layer.

Author Contributions: Conceptualization, data curation, formal analysis: A.A., M.K.P., and P.K.S.;
funding acquisition: P.K.S.; supervision: P.K.S. and M.L.K.; validation, visualization, writing—
original draft: A.A., M.K.P., and P.K.S.; writing—review and editing: A.G., P.K.S., and M.L.K. All
authors have read and agreed to the published version of the manuscript.

Funding: National Mission on Himalayan Studies, G.B. Pant National Institute of Himalayan Envi-
ronment (NIHE), Almora, Uttarakhand, India.

Institutional Review Board Statement: Not applicable.

Informed Consent Statement: Not applicable.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Regularization methods are techniques used in machine learning and deep learning pipelines to prevent overfitting. Overfitting occurs when a model learns the training data too well, capturing noise instead of general patterns, leading to poor performance on new, unseen data. Two common regularization methods mentioned in the given context are Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent complex co-adaptations on training data. This forces the network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. In essence, dropout can be viewed as a way to ensemble many neural nets within a single model.

L2 Regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the magnitude of all the weights in the network. This encourages smaller weights, effectively limiting the complexity of the learned model and preventing overfitting. It should be noted that L2 Regularization does not reduce the number of parameters but rather constrains their values.

Other regularization methods not explicitly mentioned in the given context include L1 Regularization, Early Stopping, and Data Augmentation. L1 Regularization adds a penalty term to the loss function proportional to the absolute value of the weights, encouraging sparse solutions. Early Stopping involves monitoring the model's performance on a validation set during training and stopping the process once the performance starts degrading. Data Augmentation generates additional training samples by applying transformations such as rotations or flips to existing examples, increasing the diversity of the dataset and helping the model generalize better.
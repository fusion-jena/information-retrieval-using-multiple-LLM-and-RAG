Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

S2 Fig. Spectrogram annotation interface from Bat Detective. Boxes represent example user
annotations of sounds in a spectrogram of a 3840ms sound clip, showing annotations of two
sequences of search-phase echolocation bat calls (blue boxes), and an annotation of an insect
call (yellow box).
(TIF)

S3 Fig. Example search-phase bat echolocation calls from iBats Romania & Bulgaria train-
ing dataset. Each example is represented as a spectrogram of duration 23 milliseconds and fre-
quency range from 5–115 kHz using the same FFT parameters as the main paper, and contains
examples of different search-phase echolocation call type, but also a wide variety of back-
ground non-bat biotic, abiotic and anthropogenic sounds.
(TIF)

many more unused annotations in the Bat Detective dataset that could potentially increase our
training set size. However, we found some variability in the quality of the citizen science user
annotations, as in other studies [61]. To make best use of these annotations, we need user
models for understanding which annotations and users are reliable [62, 63]. The Bat Detective
dataset also includes annotations of particular acoustic behaviours (feeding buzzes and social
calls), which in future can be used to train detection algorithms for different acoustic behav-
iours [e.g., 64].

end speech recognition. arXiv preprint arXiv:14125567. 2014.

39. Goeau H, Glotin H, Vellinga W-P, Planque R, Joly A, editors. LifeCLEF Bird Identification Task 2016.
The Arrival of Deep Learning. Working Notes of CLEF 2016-Conference and Labs of the Evaluation
forum; 2016; E´ vora, Portugal.

40. Aide TM, Corrada-Bravo C, Campos-Cerqueira M, Milan C, Vega G, Alvarez R. Real-time bioacoustics

monitoring and automated species identification. PeerJ. 2013; 1:e103. https://doi.org/10.7717/peerj.
103 PMID: 23882441; PubMed Central PMCID: PMCPMC3719130.

41.

The IUCN Red List of Threatened Species. Version 2017–1 [Internet]. 2017 [cited Downloaded on 12
May 2017.]. Available from: http://www.iucnredlist.org.

42. Everingham M, Van Gool L, Williams CK, Winn J, Zisserman A. The pascal visual object classes (voc)

challenge. International journal of computer vision. 2010; 88(2):303–38.

43. Szewczak JM. Sonobat 2010.

44. Binary Acoustic Technology. SCAN’R. 2014.

mation S3 Fig). From this we randomly selected a training set of 2,812 clips, consisting of
4,782 individual search-phase echolocation call annotations from Romania and Bulgaria, with
which to train the CNNs (corresponding to data from 347 road-transect sampling events of
137 different transects collected between 2006 and 2011) (Fig 2A). Data were chosen from
these countries as they contain the majority of the most commonly occurring bat species in
Europe [41]. This training set was used for all experiments. The remaining annotated clips
from the same user were used to create one of our test sets, iBats Romania and Bulgaria (Fig
2A and see below). Occasionally, call harmonics and the associated main call were sometimes
labelled with different start times in the same audio clip. To address this problem, we auto-
matically merged annotations that occurred within 6 milliseconds of each other, making the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given context, there isn't enough information to provide a detailed explanation about the specific postprocessing steps involved after the model training. However, generally speaking, common postprocessing steps include generating saliency maps to understand what parts of the input data the model focuses on, calculating metrics such as accuracy, precision, recall, or F1 score to evaluate the performance of the model, and creating confusion matrices to visualize the performance of the model across different classes. These steps may vary depending on the specific requirements and goals of the project.
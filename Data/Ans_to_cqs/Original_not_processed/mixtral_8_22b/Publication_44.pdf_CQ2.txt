Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

depth-wise separable convolution to both ASPP and the decoder module, hence improving
the speed and robustness of the encoder-decoder network.

of the network to make them non-trainable and added an extra max-pooling layer before the fully connected layers to reduce the dimen-sion of the previous layer. The feature vector of the fully connected layer was reduced from 2048 units of the original VGG16 to 128 units, which helped in reducing the compu-tational complexity without sacrificing much on performance. The model was imple-mented using Keras with TensorFlow backend [52]. We trained with a batch size of 32 images per iteration and applied binary cross-entropy as the loss function. All input images were resized to 300 × 300 resolution and trained for 100 epochs with an Adam optimizer at a learning rate of 1 × 10−4. We also ap-plied data augmentation for the training images such as flipping and rotation, with height and width shift as leaves were expected to be indifferent orientations, size, and location hence helped the model to generalize better. The trained classifier was then used as a filter Sensors 2021, 21, 4549

Training procedure—We adapted a pre-trained VGG16 network that is a CNN trained
on the ImageNet dataset and used for transfer learning on our dataset [51]. We froze
earlier layers of the base version of the network to make them non-trainable and added an
extra max-pooling layer before the fully connected layers to reduce the dimension of the
previous layer. The feature vector of the fully connected layer was reduced from 2048 units
of the original VGG16 to 128 units, which helped in reducing the computational complexity
without sacriﬁcing much on performance. The model was implemented using Keras with
TensorFlow backend [52].

Herbarium sheet images are usually of high resolution to capture the ﬁne-grain details
of the specimens. As a standard procedure for training deep learning models, all input
images together with their annotation were resized to a 512 × 512 resolution to reduce
the computational cost during training. Rotation, ﬂipping, and brightness adjustments
were applied as augmentation techniques for better network generalization. We used
DeepLabv3+ as the segmentation model with ResNet-101 as the feature extractor. This
model was pre-trained on the ImageNet dataset and ﬁne-tuned on the dataset. This is
useful as earlier layers of the network tend to learn generic features and therefore become
useful for other computer vision tasks [49]. We applied an Adam optimizer with a learning
rate of 1 × 10−4 and a batch size of 3. The model was trained for 100 epochs with a binary
cross-entropy loss function as we had a binary class problem (leaf or background).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not explicitly mention the specific data format used in the deep learning pipeline. However, it can be inferred that the primary data format used is image data since the text discusses herbarium sheet images and mentions applying various image processing techniques like resizing, rotation, flipping, and brightness adjustments. Additionally, the use of Convolutional Neural Networks (CNN), specifically VGG16 and DeepLabv3+, further supports this assumption as these architectures are typically employed for image classification and semantic segmentation tasks. Therefore, based on the given information, it can be concluded that the main data format utilized in the deep learning pipeline is image data.
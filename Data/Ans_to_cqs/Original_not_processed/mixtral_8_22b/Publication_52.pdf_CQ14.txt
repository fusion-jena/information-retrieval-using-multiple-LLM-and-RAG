Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

As it has been proven, ELMs are an important approach for handling and analysing Big
Data as they require the minimum training time relative to the corresponding engineering
learning algorithms. Moreover ELMs do not require ﬁne manipulations to determine their
operating parameters and ﬁnally they can determine the appropriate output weights
towards the most eﬀective resolution of a problem. What is most important, they have
in contrast to corresponding methods which adjust their
the potential to generalize,

508

K. DEMERTZIS ET AL.

performance based solely on their training data set. It is obvious that the emerging use of
ELM in Big Data analysis as well as DELE creates serious prerequisites for complex systems’
development by low-cost machines.

8. Future research

The Online Sequential ELM (OSELM) (Huang, Liang, Rong, Saratchandran, & Sundar-
arajan, 2005; Liang, Huang, Saratchandran, & Sundararajan, 2006) is an alternative
technique for
large scale computing and ML which is employed when data
becomes available in a sequential manner to determine mapping to data set corre-
sponding labels. The main diﬀerence between the Online Learning (ONL) and the
the
Batch Learning (BL)
arrival of every new data point in a scale fashion, whereas BL techniques are
used when one has access to the entire training data set at once. It is a versatile
sequential Learning Algorithm (LA) because training observations are introduced
sequentially one-by-one, or chunk-by-chunk, with varying or ﬁxed chunk length.
At any moment, only the newly arrived single record or chunk are used and
learned. A single record or a chunk of training observations is discarded as soon
as the learning procedure for that particular (single or chunk) observation(s) is com-

ELMs use the SLFFNN’s general methodology, with the speciﬁcity that the Hidden Layer
(feature mapping) is not required to work in a coordinated fashion. All hidden-layer par-
ameters are independent from the activation functions and from the training data.

ELMs can randomly create hidden nodes or hidden level parameters, before seeing the
training data, while it is remarkable that they can handle non-diﬀerential activation
equations and they do not address known NN problems such as stopping criterion, learn-
ing rate and learning epochs (Cambria & Guang-Bin, 2013; Huang, 2014, 2015).

A mathematical basis has been provided for the understanding of ELM (Equations

(1)–(9)) (Cambria & Guang-Bin, 2013; Huang, 2014, 2015).

For an ELM using SLFFNN and random representation of hidden neurons, input data is
mapped to a random L-dimensional space with a discrete training set N, where
(xi,ti), i [ 1, N with xi [ Rd and ti [ Rc. The speciﬁcation output of the network is the
following:

pleted. The LA has no prior knowledge as to how many training observations will be
presented. Unlike other sequential
learning algorithms which have many control
parameters to be tuned, OSELM only requires the number of hidden nodes to be
speciﬁed (Huang et al., 2005; Liang et al., 2006).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
According to the given information, Extreme Learning Machines (ELMs) are a type of machine learning algorithm that uses Single-hidden Layer Feedforward Neural Networks (SLFFNNs) and random representation of hidden neurons. Unlike traditional neural networks, ELMs do not require fine manipulation of operating parameters or determination of optimal output weights through iterative processes like backpropagation. Instead, all hidden layer parameters are determined independently from the activation functions and training data. This means that ELMs can generate hidden nodes or parameters randomly before even seeing the training data.

However, there is no explicit mention of how the hyperparameters of the model are optimized in the provided text. Hyperparameter optimization involves selecting the best values for various parameters that affect the performance of the model, such as the number of hidden nodes, regularization coefficients, etc. Common approaches include grid search, random search, Bayesian optimization, and others. Since the text does not provide specific details about this aspect, we cannot definitively state how hyperparameters are optimized in ELMs. However, it is worth noting that unlike some other machine learning models, ELMs do not suffer from issues related to stopping criteria, learning rates, and learning epochs, suggesting that they may be more robust to variations in these types of hyperparameters.
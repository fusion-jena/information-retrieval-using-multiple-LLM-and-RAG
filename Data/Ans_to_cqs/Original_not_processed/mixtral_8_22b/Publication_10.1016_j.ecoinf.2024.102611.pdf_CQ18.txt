Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.4. Level-wise training, validation, and testing strategies with Random 
Forest classifier 

The  data  collection  is  split  into  eleven  groups  based  on  visual 
morphological  characteristics,  as  discussed  in  Section  2.  Each  group 
consists of several plant species varieties in the range of 7–14 plants per 
group covering  100  medicinal  plant  species with  training,  validation, 
and testing proportion as 70:20:10 in both the levels, apart from GSL100 
datasets,  a  completely  new  datasets  RTL80  and  RTP40  comprising 
random collections captured in real-time is used to assess the efficiency 
of the proposed hierarchical classification model. The main evaluation 
objective using random groups is to analyze the sensitivity of classifier 
samples collected in various real-time scenarios.

Table  3  gives  insights  into  the  distribution  of  trained,  tested,  and 
validated samples across  the groups. The data suggest that consistent 
ratios across groups are maintained in the sampling process to avoid any 
potential bias during classification. Different samples are present across 
groups to establish the reliability and generalizability of the model. For 
evaluation,  both  validation  and  test  sets  are  considered  separately  to 
fine-tune the model’s hyperparameters, leading to generalizability and 
scalability towards unseen datasets. 

4.5. Classifier specifications 

The Random Forest (RF) classifier is used in both levels of the hier-
archical classification model for performing group and plant-level pre-
dictions. Let several estimators N and Fi(x) be the function of predicting 
̂
the ith  decision tree for the input X, then the prediction 
C by RF is given 
by (9). 

̂C =

1
N

∑N

i=1

Fi(X)

(9)

performance  towards  the  prediction  plant  species  type  in  the  present 
context.

̂C =

1
N

∑N

i=1

Fi(X)

(9) 

For database D, Gini index G is used as a splitting criterion with RF 

based on the node impurity as given by (10). 

G(D) = 1 (cid:0)

)2

∑

(
p

c∈C

c/t

Where G(D) is the Gini index of a node with C classes, and p

(10) 

(cid:0)

)

c/
t

is the 

probability of class c at node t. 

The proposed method sets the number of estimators (decision trees) 
N to 100. Each estimator plays a vital role in learning from randomly 
created  subsets  of  the  training  data.  Next,  the  other  parameter  is  a 
random state, which is used to set the random seeds to 5 in the proposed 
˝
˝
algorithm. Then the maximum depth of each decision tree is set to 
allowing it to grow until all leaf nodes contain only samples of the same 
class. Finally, the number of samples at the leaf node is set to 1. 

auto

5. Detailed pipeline for experiments 

5.1. Methods of performance analysis

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

To monitor the model performance during training, the authors have employed a level-wise training, validation, and testing strategy using a Random Forest classifier. They divided their dataset into eleven groups based on visual morphological characteristics, maintaining consistent ratios across these groups to prevent potential biases during classification. These groups consist of multiple plant species varieties, ranging from 7 to 14 plants per group, covering 100 medicinal plant species.

The data was split into three parts: training, validation, and testing, with proportions of 70%, 20%, and 10%, respectively. This division applies to both levels of the hierarchical classification model. Additionally, two new datasets, RTL80 and RTP40, were introduced to evaluate the efficiency of the proposed model. These datasets contained random collections captured in real-time.

Validation and test sets were considered separately to fine-tune the model's hyperparameters, aiming for generalizability and scalability towards unseen datasets. The Random Forest classifier was utilized in both levels of the hierarchical classification model for making group and plant-level predictions. The Gini index served as the splitting criterion for the Random Forest classifier, while the number of estimators (decision trees) was set to 100. Other parameters included setting the random state to 5 and allowing the maximum depth of each decision tree to grow until all leaf nodes contained only samples of the same class.

In summary, the strategy implemented to monitor the model performance during training involves dividing the dataset into training, validation, and testing sets, employing a Random Forest classifier, utilizing the Gini index as the splitting criterion, and adjusting relevant parameters such as the number of estimators, random state, and maximum depth of decision trees.
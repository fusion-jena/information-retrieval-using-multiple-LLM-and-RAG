Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We adopted the same training/trial protocol for all CNNs. All networks used in
this research have been trained from scratch (i.e., without pre-trained weights from other
datasets, like ImageNet, for example). We used 1938 input patch sizes of 256 × 256 pixels,
with 388 patches for the test, 1162 for train, and 388 for validation, a distribution of approx-
imately 20%, 60%, and 20% respectively. It is important to note that additional input patch
sizes were tested in an experimental phase, but the results did not change considerably
and only increased the training time. All approaches used the same set of hyperparam-
eters during training, which was deﬁned based on previous analyses. Speciﬁcally, the
learning rate, weight decay, momentum, and iterations were 0.01, 0.005, 0.9, and 200,000,
respectively. The model is optimized using stochastic gradient descent (SGD). To assist
the convergence process and prevent overﬁtting (Overﬁtting is a concept used to refer to a

Method

FCN

U-Net

SegNet

DeepLabV3+

DDCN

Number of Parameters
(in millions)

3.83

1.86

2.32

Training Time
(GPU hours)

Inference Time
(GPU min.)

Inference Time
(CPU min.)

Inference Time
(GPU min./ha)

Inference Time
(CPU min./ha)

485

450

472

1.4

1.9

1

1.3

1.1

1.5

5.16

486

1.4

1.9

2.08

500

5.1

6.2

0.042

0.030

0.033

0.042

0.153

0.057

0.039

0.045

0.057

0.186

3.3. Visual Analysis

Some features maps, learned by the convolutional layers, are presented in Figure 12.
Speciﬁcally, this image presents low-, mid- and high-level feature maps learned by the
ﬁrst, a middle, and the last layers of the networks, respectively. We can see the each CNN
performs very differently from one another.

The most signiﬁcant variation of performance is concerning the number of parameters
and with training and inference time. Despite being the best architecture in performance,
According to Table 2, DeepLabv3+ needed more parameters than the other architectures,
about 2.75 times more parameters than the U-Net, the least requiring one. The need for a
more signiﬁcant number of parameters often implies a higher demand for training samples
that our dataset or another dataset may not have met that the methods present in this
research paper may be applied, possibly causing the DeepLabV3+ architecture to perform
below its potential.

Remote Sens. 2021, 13, 3054

11 of 19

Figure 11. Convergence of the evaluated networks.

Table 2. Number of Parameters and Processing Time of the proposed approaches. The training time
represents the results for the test set of each method. The inference time stands for the time taken by
each model to make predictions for each image.

Method

FCN

U-Net

SegNet

model that adjusts too well to the training data, but it does not generalize to the unseen
before dataset, i.e., a test dataset), after 50,000 iterations, the learning rate was reduced
following an exponential decay parameter of 0.5 by an SGD scheduler. Aside from this,
we used rotation, noise, and ﬂip (as in [50]) for data augmentation, and we were capable
of augmenting the dataset by six times. With the data augmentation technique, we can
make the CNN classiﬁcation more robust and generalize better. In Figure 9, we can see the
schematic diagram for the evaluation process.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps are typically involved. These steps aim to prepare the data for optimal use by the model. Some common preprocessing techniques include:

1. Normalization: This involves rescaling the values of the input features so they fall within a specific range, such as between 0 and 1 or -1 and 1. Normalization helps ensure that no single feature dominates the others due to its scale, improving the model's ability to learn patterns in the data.

2. Scaling: Similar to normalization, scaling involves transforming the values of the input features to bring them into a similar range. However, unlike normalization, scaling does not necessarily rescale the values to fit within a specific range. Instead, it aims to reduce the impact of outliers or extreme values in the data.

3. Cleaning: Data cleaning involves identifying and correcting errors, inconsistencies, or missing values in the data. This step ensures that the model receives accurate and reliable information, reducing the risk of poor performance or incorrect predictions.

4. Augmentation: Data augmentation involves creating new examples by applying various transformations to existing ones. For instance, images can be rotated, flipped, or zoomed to create new variations. Data augmentation helps increase the size and diversity of the training dataset, making the model more robust and generalizable.

Based on the given context, the researchers employed some of these preprocessing steps before training their deep learning models. Specifically, they used rotation, noise, and flip for data augmentation, allowing them to expand their dataset by six times. Additionally, they utilized an exponential decay parameter of 0.5 by an SGD scheduler to reduce the learning rate after 50,000 iterations. While the context does not explicitly mention normalization, scaling, or cleaning, these steps could still have been part of their preprocessing pipeline.
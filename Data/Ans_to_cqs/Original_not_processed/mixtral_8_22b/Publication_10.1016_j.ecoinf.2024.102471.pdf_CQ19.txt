Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Wav2vec is one of the latest state-of-art models for Automatic Speech 
Recognition  and  other  audio-related  problems  (Boigne  et  al.,  2020) 
(Shibano et al., 2021). The architecture of the Wav2vec is based on the 
transformer's  encoder,  with  a  training  objective  like  BERT's  masked 
language modelling objective but adapted for speech instead of text. The 
self-supervised  way  of  training  allows  Wav2vec  to  pre-train  on  unla-
belled data, which is more accessible (Hendrycks et al., 2019). Later, the 
model can be fine-tuned on a particular dataset for a specific purpose. 
This  approach  shows  remarkable  improvement  in  the  model's  perfor-
mance, especially if the dataset size is very small with label.

4.3. Baseline models 

Hybrid  CNN-LSTM  is  an  architecture  that  combines  the  temporal 
modelling ability of the LSTM with the CNN ability to learn invariant 
features  (Ayadi  and  Lachiri,  2022)  (Yadav  and  Vishwakarma,  2020) 
(Ashraf  et  al.,  2023).  The  combination  of  Convolutional  Neural  Net-
works  (CNN)  and  Long  Short-Term  Memory  (LSTM)  has  the  major 
benefit of learning spatial and temporal data. CNN excels in extracting 
spatial elements like edges, textures, and forms from an input. Mean-
while,  an  LSTM  excels  in  extracting  temporal  data,  like  the  order  of 
words in a phrase or frames in a movie. By combining the two, a model 

EcologicalInformatics80(2024)1024719B. Swaminathan et al.

Fig. 14. Performance comparison plot between baseline and proposed model.  

may learn spatial and temporal characteristics, which are important in 
applications like speech recognition and machine translation. Further-
more, because the two may supply complementing information to one 
another, CNN-LSTM models can learn complicated patterns from data 
more rapidly and correctly than either CNN or LSTM models alone.

The  motivation  comes  from  success  of  transformer  in  natural  lan-
guage processing (NLP) is the reason for evolution of advanced archi-
tecture called vision transformer. In (Dosovitskiy et al., 2021) proposed 
work, the attention mechanism is focused as network's core component 
for capturing long-range relationship among complex structure of image 
data.  The  vision  transformer  processes  each  input  image  as  patches 
where they are treated as tokens by NLP model. Then the model pro-
cessed the tokens to learn more about the visual representation of the 
input. Hence, in comparison to CNN techniques, transformer-based re-
sults  demonstrated  state-of-the-art  performance  by  processing  more 
deep features in limited computational resources. Subsequently, vision 
transformer is applied along with hyper-head attention mechanism for 
bird  sound  recognition  (Tang  et  al.,  2023).  In  this  work,  author  has

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not provide any explicit information regarding the use of specific deep learning frameworks such as TensorFlow or PyTorch for building the mentioned models (Wav2Vec, Hybrid CNN-LSTM, Vision Transformer). Therefore, it is impossible to determine which framework was utilized without additional details.
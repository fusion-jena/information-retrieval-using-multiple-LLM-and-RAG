Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

+is paper aims to predict the SST and the SWH for the
Korea Hydrographic and Oceanographic Agency dataset.
+e proposed work is motivated by comparing the statistical,
machine learning, and deep learning models to understand
the performance gap of these models. +e results of this
work should provide scientiﬁc evidence on which model ﬁts
better the marine data. To anticipate the marine features
eﬃciently, the employed deep learning model combines the
gated recurrent units (GRU) with the regular neural net-
work. In the proposed architecture, the GRU layer is pre-
ceded by an input layer and followed by a fully connected
layer. As a result, the predicted values can be produced from
the output layer. To our knowledge, this is the ﬁrst use of a
GRU model architecture for forecasting SST and SWH.
Besides, four diﬀerent ML models have been utilized in the
current study, namely, Linear Regression (LR), Decision
Tree (DT), Support Vector Machine (SVM), and Random

deep learning, and machine learning models for predicting the SSTand SWH on a real dataset obtained from the Korea Hydrographic
and Oceanographic Agency. +en, we proposed comparing these three predictive approaches on four diﬀerent evaluation metrics.
Experimental results have revealed that the deep learning model slightly outperformed the machine learning models for overall
performance, and both of these approaches greatly outperformed the statistical predictive model.

Practically, over/underﬁtting diﬃculties in neural net-
work models are caused by the neural network model’s
excessive/insuﬃcient training epochs [43]. As a result, one
possible solution to the DL-based model’s over/underﬁtting
concerns is to apply the early stopping strategy [44], which is
used to cease training when generalisation performance
starts to degrade for a number of epochs. To track the
generalisation performance,
in the proposed model, the
training data is separated into training and validation
groups.

+e dropout approach [45] is another way to deal with
the overﬁtting problem. Dropout is a regularisation strategy
that allows you to train neural networks with alternative
topologies in parallel by randomly dropping out a certain
proportion of layer neurons. Dropout is indicated by the
black neurons in the fully connected layers, as seen in
Figure 3.

5.5. GRU-DNN Hyperparameter Analysis. GRU-DNN
model is trained in a supervised learning fashion using lag
features (i.e., using K previous observations), where K
denotes the number of previous observations used in the
training and forecasting task. Typically, K is considered
as a hyperparameter that needs to be optimized. +ere-
fore, we performed a grid search method to obtain the
optimal K value. Figure 6 depicts the grid search for
diﬀerent values of K hyperparameter over search space
ranges from 1 to 15. Speciﬁcally, Figure 6(a) presents the
model performance for water temperature forecasting
using various K values, where K � 6 achieves the lowest
MAE error. Similarly, K � 4 is the optimal value for
signiﬁcant wave height
shown in
Figure 6(b). It is noteworthy that the experiments pre-
sented in Figure 7 are for the ﬁrst dataset of each fore-
casting problem, assuming that the rest of the datasets
have similar behaviour.

forecasting

as

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The given text does not mention any specific data augmentation techniques such as flipping, rotation, or scaling applied in the deep learning pipeline. Therefore, it cannot be determined what data augmentation techniques were used based solely on the information provided.
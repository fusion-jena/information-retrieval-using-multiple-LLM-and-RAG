Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

time, from the onset of candidate model training to the 72nd training 
epoch of the selected model, took 26 min on a desktop PC. On the high- 
end workstation, a distinct modelling event took 3 min.

model.  A  model  trained  too  few  epochs  will  not  capture  all  relevant 
patterns in the data, reducing predictive performance. A model trained 
for an excessive number of epochs might overfit, reducing its generality 
and ability to classify new data. There is no definitive way to identify an 
optimal  number  of  training  epochs,  but  one  practical  approach  is 
through  monitoring  the  model’s  validation  performance  (i.e.,  using 
holdout  data  partition  Bv;  Fig.  2).  The  ‘optimal’  number  of  training 
epochs is the one that provides the best validation performance. Finally, 
the performance of the model having an ‘optimal’  number of training 
epochs is evaluated using a ‘final’ test data set (T; Fig. 2), providing the 
best estimate of the predictive performance of the model.

To identify an ‘optimal’ number of training epochs, we examined the 
progression of validation performance (Bv). Models can be trained for an 
infinite number of epochs, so here we stopped training if no increase in 
validation performance was observed after 25 epochs (other thresholds 
could be considered, according to time resources available). Finally, the 
model trained with the number of epochs showing highest AUC in pre-
dicting Bv was used to classify the test data (data set T), with perfor-
mance measured using AUC. 

We recorded processing time of all models from the onset of training 
of candidate models to the last training epoch evaluated for the selected 
model. This was done on two distinct systems: a ‘desktop PC’ with an

For the three case studies below, we used the same model generation 
and selection strategy. We had mcfly generate 20 candidate models, five 
for each architecture type. These models were trained during 4 epochs 
(using At). The candidate model achieving highest performance in pre-
dicting the classes of the validation data (Av) was then trained on the full 
training  data  set  (Bt).  For  each  epoch  we  measured  training  perfor-
mance, as provided by mcfly (which uses the accuracy metric i.e., ‘the 
proportion of cases correctly classified’). The classification performance 
on  the  validation  data  (Bv)  was  measured  using  the  area  under  the 
receiver  operating  characteristic  curve  (AUC),  a  metric  that  is  not 
affected by differences in the prevalence of classes and is widely used in 
ecology (e.g. Dyderski et al., 2018).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The strategy implemented to monitor the model performance during training involves examining the progression of validation performance (Bv). To find the 'optimal' number of training epochs, models are trained until there is no improvement in validation performance after a certain threshold of epochs. In this case, the threshold was set to 25 epochs, but it could vary depending on available time resources. Once the optimal number of epochs is determined based on the highest Area Under Curve (AUC) in predicting Bv, the final model is selected and used to classify the test data (data set T). The performance of this model is then measured using AUC.
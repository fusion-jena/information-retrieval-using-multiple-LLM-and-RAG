Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

TABLE I.  

PERFORMANCE METRICS FOR UBD_45 DATASET 

Model 

Performance Metrics 

Sensitivity 

Specificity 

Precision 

F1-Score 

InceptionResnet2 

95.93 

Resnet50 

Xception 

Inceptionv3 

Mobilenetv2 

Googlenet 

95.93 

94.44 

91.85 

92.04 

85.00 

99.92 

99.90 

99.87 

99.85 

99.83 

99.62 

97.65 

97.20 

95.90 

94.73 

95.16 

86.15 

96.78 

96.56 

95.16 

93.27 

93.57 

85.57 

TABLE II.  

PERFORMANCE METRICS FOR VP_200 DATASET 

Model 

Performance Metrics 

Sensitivity 

Specificity 

Precision 

F1-Score 

InceptionResnet2 

91.20 

Resnet50 

Xception 

Inceptionv3 

Mobilenetv2 

Googlenet 

88.68 

91.80 

88.65 

85.58 

83.28 

99.96 

99.94 

99.96 

99.94 

99.93 

99.92 

91.92 

89.46 

92.83 

90.47 

87.20 

84.90 

91.56 

89.07 

92.31 

89.55 

86.38 

84.08

The  selected  network  models  were  initialized  with  pre-
trained ImageNet weights and then fine-tuned to our datasets. 
The performance of a deep neural network for a dataset highly 
depends on network hyperparameters. The selection and fine-
tuning of  optimized hyperparameters  is  generally  a  difficult 
and  time-consuming  task.  Instead  of  manually  selecting  the 
hyperparameters, we employed Bayesian optimization to find 
their optimal values for each of the six models.  A Gaussian 
process  model  of  the  objective  function  is  used  by  the 
Bayesian optimization technique.  Different variables can be 
optimized using this technique such as network section depth, 
batch size, initial learning rate, momentum, and regularization 
strength. For this study, we optimized the network for batch 
size  (between  1  and  32)  and  initial  learning  rate  (between 
1×10-4  and  1×10-2).  The  optimization  was  performed  by

(2019) 

BMC 

[23]  D. Wu, X. Han, G. Wang, Y. Sun, H. Zhang, H. Fu, Deep Learning 
with  Taxonomic  Loss  for  Plant  Identification,  Comput.  Intell. 
Neurosci. 2019 (2019) 1–8. https://doi.org/10.1155/2019/2015017. 
[24]  J. Haupt, S. Kahl, D. Kowerko, M. Eibl, Large-scale plant classification 
using  deep  convolutional  neural  networks,  CEUR  Workshop  Proc. 
2125 (2018) 1–7. 

[25]  M.  Lasseck,  Image-based  plant  species  identification  with  deep 
Convolutional Neural Networks, CEUR Workshop Proc. 1866 (2017). 
[26]  M. Mehdipour  Ghazi, B.  Yanikoglu,  E.  Aptoula,  Plant identification 
using  deep  neural  networks  via  optimization  of  transfer  learning 
parameters, 
228–235. 
https://doi.org/10.1016/j.neucom.2017.01.018. 

Neurocomputing. 

(2017) 

235 

[27]  M.  Šulc,  J.  Matas,  Fine-grained  recognition  of  plants  from  images, 
Plant  Methods.  13  (2017)  1–14.  https://doi.org/10.1186/s13007-017-
0265-4.

Recent studies have shown that using the transfer learning 
approach  can  drastically  improve  the  performance  of  CNN 
models even on a limited amount of training samples [16]. For 
example, Liu et al. [17] utilized a GoogleNet architecture to 
obtain an accuracy of 62.8% in classifying 50 different kinds 
of species trained from a total of 8,500 plant images. Dias et 
al.  [18]  obtained  an  accuracy  of  94.5%  by  fine-tuning  a 
GoogleNet  on  a  dataset  of  50  plant  species  with  a  total  of 
33,000  training  images.  Miao  et  al.  [19]  applied  a  transfer 
learning approach by fine-tuning Inception-V3 and Inception-
ResNet-v2 networks to classify 22 different plant species from 
more than 22,000 image samples. Compared to conventional 
neural  networks  and  CNN  trained  from  scratch,  better 
performance was obtained from transfer learning models with 
Inception-V3 and Inception-ResNet-v2 achieving an accuracy

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not provide information about the specific hardware resources used for training the deep learning models. However, it is common practice to use Graphics Processing Units (GPUs), Tensor Processing Units (TPUs), or other specialized hardware accelerators when training deep learning models due to their computational requirements.

Based on the context, we can infer that the authors likely used some form of GPU or TPU for training their models since they mention employing Bayesian optimization to find optimal hyperparameter values. This process typically involves multiple iterations of training and evaluation, which would require significant computational power. Additionally, the authors reference recent studies that utilize popular deep learning architectures like GoogleNet and Inception-ResNet-v2, both of which are known to benefit greatly from GPU or TPU acceleration during training.

However, without explicit mention of the hardware resources used, we cannot definitively state what type of hardware was employed for training these models.
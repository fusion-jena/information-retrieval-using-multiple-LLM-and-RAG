Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

the validation dataset and the model segmented 1195 objects.

∼

80(cid:176)W70(cid:176)W60(cid:176)W50(cid:176)W40(cid:176)W10(cid:176)S0(cid:176)NBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBRAZIL05001000 km(a)01020 kmN62(cid:176)W61.8(cid:176)W61.6(cid:176)W61.4(cid:176)W61.2(cid:176)W61(cid:176)W10.3(cid:176)S10.2(cid:176)S111111111222222222333333333444444444555555555666666666777777777888888888999999999Forest FormationsOther Non Forest Natural FormationAgriculture or PastureWater bodiesregions of interestGeoEye image extents(b)Remote Sens. 2020, 12, 2225

5 of 20

2.3. Forest Cover Mask and Clear-Cut Deforestation History from PRODES

as a sum of two terms: binary cross-entropy and Dice coefﬁcient-related loss of the three predicted
masks [50–52]. We used the optimizer RMSprop (unpublished, adaptive learning rate method proposed
by Geoff Hinton here http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
with an initial learning rate of 0.0001. We trained our network for 300 epochs with 24 images per batch.

×

×

3.7. Algorithm

The model was coded in the programming language R [48] with Rstudio interface to Keras [51,52]
2–20 hours using GPU of an Nvidia
and Tensorﬂow backend [53]. The training of the models took
RTX2080 graphics card with 8 GB of dedicated memory. Prediction using GPU of a single tile of 16,384

∼

16,384 pixels (

∼

×

67.10 km2) took approximately 6 min.

Remote Sens. 2020, 12, 2225

8 of 20

4. Results

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, it is not explicitly stated how the trained deep learning model was deployed. However, we can infer some details about the development environment and hardware used during the training phase.

The model was developed using the programming language R with the RStudio interface to Keras and TensorFlow backend. This suggests that the deployment might involve utilizing these libraries and frameworks to load the serialized model and perform predictions.

Additionally, the training was performed using an NVIDIA RTX2080 graphics card with 8GB of dedicated memory. This indicates that the deployment may also require a compatible GPU to ensure efficient prediction times.

Unfortunately, without specific information regarding the deployment process, such as whether the model was saved and loaded using a particular format (like.h5 or.pb), or if any cloud platform or custom application was utilized for serving predictions, I cannot provide a more detailed response.
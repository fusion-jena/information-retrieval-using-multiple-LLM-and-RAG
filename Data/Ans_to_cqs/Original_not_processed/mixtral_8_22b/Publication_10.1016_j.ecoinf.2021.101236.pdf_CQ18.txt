Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

With its ~27 million trainable parameters, the model has sufficient capacity to 
classify  984  bird  species  and  three  non-event  classes.  Like  Schlüter  (2018) 
proposed in his work, the classification block employs time-step predictions and 
log-mean-exponential pooling to preserve scores across multiple intervals. 
BN = Batch normalization, DO = Dropout, LME = Log-mean-exponential.

We  trained  our  model  using  ~1.5  million  spectrograms  extracted 
from  the  training  dataset  with  a  maximum  of  3500  samples  (i.e.,  3-s 
spectrograms) per class—using more samples did not improve the per-
formance. Oversampling of underrepresented classes to at least 10% of 
the  maximum  amount  of  training  samples  per  species  was  applied  to 
reduce  class  imbalance.  We  tried  other  methods  of  cost-sensitive 
learning  (e.g.,  class  weights  or  focal  loss  mentioned  in  (Lin  et  al., 
2017)), but none of them improved the overall model performance. To 
simulate  simultaneously  vocalizing  bird  species,  we  employed  mixup 
training (Zhang et al., 2017) by randomly combining up to three spec-
trograms into one sample. We used the ADAM optimizer (Kingma and 
Ba, 2014) with an initial learning rate of 0.001 and a batch size of 32. We 
reduced  the  learning  rate  by  a  factor  of  0.5  according  to  a  step-wise

diversify  the  training  data,  and  the  3-s  window  length  enables  the 
implementation of this data augmentation method.

Table 1 
Our  model  design  follows  the  Wide  ResNet  approach  by  Zagoruyko  and 
Komodakis and consists of 157 layers, of which 36 are weighted.  

Group 

Name 

Input shape 

Output shape 

Pre-processing 

ResStack 1 

ResStack 2 

ResStack 3 

ResStack 4 

Classification 

5×5 Conv+BN+ReLU 
Ma× pooling 
Downsampling block 
2 × ResBlock 
Downsampling block 
2 × ResBlock 
Downsampling block 
2 × ResBlock 
Downsampling block 
2 × ResBlock 
4×10 Conv+BN + ReLU + DO 
1×1 Conv+BN + ReLU + DO 
1×1 Conv+BN + DO 
Global LME pooling 
Sigmoid activation 

(1×64×384) 
(32×64×384) 
(32×64×192) 
(64×32×96) 
(64×32×96) 
(128×16×48) 
(128×16×48) 
(256×8×24) 
(256×8×24) 
(512×4×12) 
(512×4×12) 
(512×1×3) 
(1024×1×3) 
(987×1×3) 
(987×1) 

(32×64×384) 
(32×64×192) 
(64×32×96) 
(64×32×96) 
(128×16×48) 
(128×16×48) 
(256×8×24) 
(256×8×24) 
(512×4×12) 
(512×4×12) 
(512×1×3) 
(1024×1×3) 
(987×1×3) 
(987×1) 
(987×1)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide explicit information on the specific strategy implemented to monitor the model performance during training. However, it mentions the use of the ADAM optimizer with an initial learning rate of 0.001 and a batch size of 32, and the reduction of the learning rate by a factor of 0.5 according to a step-wise schedule. These details suggest that some form of monitoring was likely in place to adjust these hyperparameters based on the model's performance during training. Additionally, the mention of oversampling of underrepresented classes to reduce class imbalance implies that class distribution and accuracy were also considered when evaluating the model's performance.
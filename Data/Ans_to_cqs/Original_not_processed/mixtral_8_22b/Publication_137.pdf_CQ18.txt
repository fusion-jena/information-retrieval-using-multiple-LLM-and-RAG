Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

123

2170

Journal of the Indian Society of Remote Sensing (September 2021) 49(9):2163–2172

training, we set the learning decay rate to 0.7 so that the
learning rate gradually decays as the training progresses.
Table 1 shows the experimental results in which we ﬁxed
the voxel size and batch size and adjusted the regulariza-
tion strength, from which we can be told that result is better
while regularization strength is 0.2.

2. Model training: We process data with a deep neural
network and adjust the training epoch, batch size, and
volume size to output a classiﬁer for prediction.
3. Result output: The point cloud test set is segmented,
and we can obtain the ﬁnal segmentation results. The
output ﬁles include point cloud geometric partition
graphs, SPGs, and segmentation results.

Results and Discussion

overall accuracy (OA) as metrics. We adjusted three
parameters (voxel size is the voxelization subsampling
size, the regularization strength inﬂuences the coarseness
of the minimal partition, and batch size means the batch
number of point cloud samples in training). We used the
hold-out method to distinguish the test set and the training
set at a ratio of 3:7 for preprocessing the original data. We
further adjust the parameters of networks to explore the

inﬂuence of parameters on experimental results, and we
discuss the selection of the frame parameters.

Selection of the Frame Parameters

We investigated the effects of several parameters in our
framework, such as the regularization strength, voxel size
for the minimal partition, and training batch size. In

123

Journal of the Indian Society of Remote Sensing (September 2021) 49(9):2163–2172

2169

Fig. 5 Part of segmentation experimental results. a Geometric features, b Geometric partition, c SPG, and d Segmentation result

123

For tree species segmentation, appropriately increasing
batch size can improve efﬁciency while improving seg-
mentation accuracy. We believe that within a reasonable
range limited by GPU memory, a larger training batch size
of 8 can better preserve the original features of the point
clouds than others. In addition, the network more easily
obtains a sharp minimum rather than a ﬂat minimum when
the batch size is larger. Therefore, we can obtain better
results when the batch size is 8 in our experiments
(Table 3).

Through the experimental results, we ﬁnd that
the
adjustment of the parameters can also improve the mis-
segmentation situation. In summary, we obtain the best
result in the ﬁve tree species experiments: The mIoU is
81.49% and the OA is 92.45%, while the voxel size is 0.04,

the training batch size is 8 and the regularization strength is
0.2.

Effectiveness of the Framework for Target
Segmentation

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The strategy implemented to monitor the model performance during training involves setting the learning decay rate to 0.7. This allows the learning rate to gradually decay as the training progresses. Additionally, the authors use overall accuracy (OA) as a metric to evaluate the performance of their model. They also adjust various parameters such as voxel size, regularization strength, and batch size to optimize the model's performance. Furthermore, they employ the hold-out method to separate the test set and training set at a ratio of 3:7 for preprocessing the original data. By doing this, they are able to fine-tune the parameters of the network and investigate the influence of these parameters on the experimental results.
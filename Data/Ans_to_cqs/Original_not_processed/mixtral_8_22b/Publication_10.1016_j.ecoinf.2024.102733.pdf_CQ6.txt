Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During the training of the MLP, Adam optimizer (Kingma and Ba,
2015) is used, as well as the scheduler which adjusts the learning rate
when the network reaches a plateau. For the training, various parameter
values and different regularization techniques were explored, with the
option of using either Cross-Entropy or Focal Loss (Lin et al., 2017). In
detail, the learning rate was set to 0.00001 and 0.0001, batch sizes of 8,
16, 32, and 64 were tested, and weight decay was varied between
0.0001 and 0.001 with a dropout of 0.1. Additionally, Focal Loss (Lin
et al., 2017) introduce additional parameters to investigate: α and γ. The
former serves as a balancing factor, either as a fixed value for all classes
or as the inverse of each class frequency. The latter regulates the impact
of the scaling factor; specifically, when set to zero, Focal Loss is equiv-
alent to Cross-Entropy Loss. We experimented with α set as the inverse of

The MLP employed consists of four fully connected layers with the
size of 256, 128, 64, and 12, each comprising a linear layer followed by
PRelu as non-linearity. Only the third layer differs, as it is provided with
Batch Normalization (Ioffe and Szegedy, 2015). During optimization,
Batch Normalization was also applied to other layers as well, but the
network performance was not as good. The network gets the input of size
512 consisting of learned features extracted from the diffusion model
and the last layer has 12 neurons. The maximum neuron value from this
layer is then used for predicting the event class, as each neuron corre-
sponds to one of the events. The values of the 12 neurons are then
compared to the real event ground truth through a loss, which serves to
update the model parameters and perform the training.

Sampling

ODE solver
Time steps

Network and preconditioning
Architecture of Fθ
Skip scaling cskip(σ)

Output scaling cout(σ)

Input scaling cin(σ)

Noise scaling cnoise(σ)

Training
Noise distribution

Loss weighting

LMS
1
max + i
ρ

σ

T (cid:0) 1

(
σ

1
ρ
min

1
ρ
max

(cid:0) σ

))

ρ

MLP (see text)

σ2
data
σ2 + σ2

data
σ⋅σdata
̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅
σ2 + σ2
√

data
̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅
σ2 + σ2

data

√

1/

1
ln(σ)
4

)

(cid:0)

Pmean, P2
)
std

ln(σ) ∼ N
(cid:0)
σ2 + σ2
data
(σ⋅σdata)2

its maximum value at the last epoch of the training.

the decoder is composed of layers sized 3 × 128 and 6 × 64. During the
training of this model, the network weights are iteratively updated using
the Adam optimizer (Kingma and Ba, 2015). Additionally, a scheduler
adjusts the learning rate: when the loss plateaus, meaning it does not
decrease over a certain number of epochs, the learning rate is reduced.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't explicit information about specific preprocessing steps such as data cleaning, normalization, or scaling prior to training the deep learning models mentioned. However, some relevant details can be inferred regarding the handling of input data and its preparation for training.

In the case of the MLP model, the input data consists of learned features extracted from a diffusion model, resulting in a vector of size 512. This suggests that feature extraction might have been performed as part of the preprocessing step. Furthermore, Batch Normalization is applied during the training process, which helps standardize the inputs for each mini-batch, potentially reducing internal covariate shift and improving convergence rates.

For the decoder model, no explicit preprocessing steps are mentioned. However, the input data is processed through multiple layers of varying sizes, indicating that the raw data may have undergone dimensionality reduction or transformation into a suitable format for the model architecture.

Overall, while the context does not provide comprehensive details on traditional preprocessing steps like data cleaning, normalization, or scaling, it does suggest that feature extraction and dimensionality reduction could be considered as part of the preprocessing pipeline for these particular deep learning models.
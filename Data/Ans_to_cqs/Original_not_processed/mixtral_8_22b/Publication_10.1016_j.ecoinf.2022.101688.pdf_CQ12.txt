Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Since CNNs require a fixed input size, we studied the vocalisations 
within the presence class for each dataset to determine the character-
istics of the calls which would allow us to create fixed input, which we 
refer to as segments. For example, the Hainan gibbon calls vary from 2 to 
9 s (Dufourq et al., 2021), and thus a suitable input size was 4 s to ensure 
that the smallest call would fit within the segment. A longer input (>4 
seconds) would result in CNNs with more network parameters, an un-
desirable consequence as this would increase the chances to overfit. A 
shorter input (<4 s) would not contain enough information, especially 
in cases where the individual pulses that make up a call are long. A short 
input could omit parts of the call. Preliminary experiments were con-
ducted on the different datasets to minimise the input length as much as 
possible,  thus  minimising  network  parameters.  The  characteristics  of

this case a CNN – will consistently perform well across all applications 
and datasets. Table 4 shows that, VGG16 for example, obtained the best 
results on three configurations, but also did not achieve equally as good 
results  on  other  configurations.  ResNet101V2  and  ResNet152V2  on 
average  performed  well  across  nearly  all  configurations  and  we  thus 
recommend that either of these architectures are used as a starting point 
for  researchers  wanting  to  use  pre-trained  CNNs  for  bioacoustics 
research. 

The experiments presented in Fig. 6 reveal that if very little data is 
available – in our case 25 examples – that pre-trained ResNet152V2 with 
the feature extractor frozen can yield good performance (up to 82% F1 
score  on  the  lemurs,  77%  on  the  gibbons  and  75%  on  the  whydah 
dataset).  This  suggests  that  when  conducting  an  acoustic  survey,  a

practitioner can manually annotate a few examples and then start using 
a  pre-trained  ResNet152V2  model  to  find  new  examples.  Once  addi-
tional examples have been found via the pre-trained model, these new 
examples can be incorporated into the training set. This iterative process 
can be repeated until a large training set is obtained, after which, the 
pre-trained CNN can be fine-tuned to create a more robust classifier. We 
thus argue that practitioners can begin using CNNs relatively early on 
within  a project to  speed up  the rate at  which calls  are found.  These 
findings  oppose  existing  knowledge  that  deep  learning  requires  large 
training  datasets.  One  possible  explanation  for  the  good  performance 
achieved in this study is due to the high signal-to-noise ratio. It was also 
hypothesised  that  good  performance  was  obtained  due  to  the  lack  of 
variation within the calls (e.g. gibbons and whydah datasets), however

dataset)  with  46  softmax  units.  The  spectrograms  were  duplicated  to 
meet  the  input  of  ResNet50  which  expects  3  channels.  Zhong  et  al. 
(2020) compared VGG16 that was randomly initialised to a ResNet50 
model that was  pre-trained  on ImageNet.  In both  cases, a  colour mel 
spectrogram was input into the network. The spectrograms were resized 
to match the network's input of 224 by 224. Their models were applied 
to bird and amphibian vocalisations. ResNet50 pre-trained on ImageNet 
was also used by LeBien et al. (2020) whereby the pre-trained feature 
extractor was used and then two fully connected layers were added to 
the CNN. Zhong et al. (2021) applied ResNet50 to a birdsong dataset 
that contained three classes (two bird presence and one absence). The 
CNN was  pre-trained on ImageNet and  the fully connected layer, fol-
lowed by a dropout and an output layer was added to the CNN that was

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, Convolutional Neural Networks (CNNs) are primarily used as the deep learning model in the pipeline. Specifically, various versions of ResNet such as ResNet101V2, ResNet152V2, and ResNet50 are mentioned along with VGG16. These models are used for tasks like classification and detection of animal vocalizations. The choice of CNNs is likely due to their effectiveness in processing grid-like topology data, such as images or spectrograms, which are commonly used representations of audio signals.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4. Open Science methodology and tools 

Our  workflow  implements  a  FAIR  approach  that  tests  FAIR  data 
principles’  practicability.  It  is  open-source  (Supplementary  material) 
and was integrated with the DataMiner Cloud computing platform of the 
D4Science e-Infrastructure (Coro et al., 2017), which allows accessing 
the mentioned knowledge sources on-the-fly during processing (Candela 
et al., 2016; Coro et al., 2015a). Data FAIRness is facilitated through the 
indexing of these resources in the D4Science catalogue (Assante et al., 
2019b), which can be accessed by all processes via the Catalogue Services 
for the Web (CSW) standard of the Open Geospatial Consortium (OGC, 
2020). Geospatial data are offered as standarised NetCDF files available 
on  a  distributed  ISO/OGC  compliant  Spatial  Data  Infrastructure

4. Discussion and conclusions 

This paper has presented a methodology to aggregate, classify, and 
extract  new  information  from  vessel-transmitted  data  through  the 
analysis  of  heterogeneous  data  sources  in  an  Open  Science  e-Infra-
structure. The results confirm the feasibility of cross-domain analysis if 
FAIR data principles are considered when establishing data repositories. 
Potential  applications,  based  on  Open  Science  principles,  have  been 
demonstrated through three case studies.

Open  Science  oriented  e-Infrastructure  (D4Science,  Assante  et  al. 
(2019b)) to facilitate access to vast collections of stock and biodiversity 
information. In particular, this e-Infrastructure optimises access to data 
sources that enrich vessel tracking data, and that meet the principles of 
Findability, Accessibility, Interoperability, and Reusability (FAIR data). 
In the presented methodology, the integrated FAIR data include (i) 
environmental  data  from  the  Copernicus  marine  environment  moni-
toring  service  (CMEMS,  Von  Schuckmann  et  al.  (2016)),  (ii)  species 
occurrence records retrieved from the Ocean Biodiversity Information 
System (OBIS, Grassle (2000)), (iii) species’ risk level from the Red List 
of the International Union for Conservation of Nature (IUCN, 2001), (iv) 
taxonomic  data  accessed  from  the  Aquatic  Sciences  and  Fisheries  In-
formation System of FAO (ASFIS, Garibaldi et al. (2002)), and (v) global

EcologicalInformatics64(2021)1013848G. Coro et al.                                                                                                                                                                                                                                    

stocks  or  non-stocks  in  the  fishing  cells.  This  operation  is  achieved 
through a direct query to the SPARQL endpoint of the GRSF semantic 
knowledge base (i-Marine, 2020). 

FP|s =

Overall, this process can be summarised as follows:  

∑C

∑C

c=1fah|s(c)
c=1(associated(c, s))

In summary, this algorithm associates information on stocks, species 
variety, and threatening status to the fishing hours of each cell in the 
studied area. 

2.3. Estimating fishing pressure on stocks and other species 

Fishing pressure per stock is here defined as the number of fishing 

hours per cell where the stock occurs:

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide any specific links to the data repository of the deep learning pipeline. However, it mentions that their workflow implements a FAIR approach and is open source, suggesting that they might have made their code and possibly some datasets publicly accessible. They also mention integrating their workflow with the DataMiner Cloud computing platform of the D4Science e-Infrastructure. Unfortunately, without more specific details about the dataset used in the deep learning pipeline, I cannot provide a direct link to the data repository.
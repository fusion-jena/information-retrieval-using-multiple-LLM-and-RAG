Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.2.2. Training and testing data 

For each classification task in the pipeline, training, development, 
and  testing  sets  were  created  through  manual  annotation  of  2000 
randomly selected data points (randomly split into datasets using 60–20- 
20 ratio), according to the relevancy criteria outlined in Fig.1. To avoid 
over-fitting, development sets were used to evaluate classification per-
formance during experiments, whilst the testing sets were used used to 
evaluate the final classifier for each classification task. As the best per-
forming models and training parameters were identified during exper-
imentation, the testing sets allowed us to identify whether the classifiers 
can maintain their performance when applied to unseen data.

For each classification task, after experiments had been conducted, 
the best performing classifier was used to classify the whole dataset. The 

relevant texts from these predictions were then used as data from which 
training, development and testing sets were selected for the next clas-
sification task. 

2.3. Classification models 

2.3.1. Comparator models

2.2. Data collection and preparation 

2.2.1.

Initial data collection

Settles, B., Craven, M., 2008. An analysis of active learning strategies for sequence 
labeling tasks. In: Proceedings of the 2008 Conference on Empirical Methods in 
Natural Language Processing, pp. 1070–1079. 

Seymour, E., Damle, R., Sette, A., Peters, B., 2011. Cost sensitive hierarchical document 

classification to triage PubMed abstracts for manual curation. BMC Bioinform. 12 
(1), 1–11. 

Shinyama, Y., 2020. Pdfminer. https://pdfminersix.readthedocs.io/en/latest/. 
Stringham, O., Moncayo, S., Hill, K., Toomes, A., Mitchell, L., Ross, J.V., Cassey, P., 

2021a. Text classification to streamline online wildlife trade analyses. PLoS One 16 
(7) e0254007.584.  

Stringham, O., Toomes, A., Kanishka, A., Mitchell, L., Heinrich, S., Ross, J., Cassey, P., 
2021b. A guide to using the internet to monitor and quantify the wildlife trade. 
Conserv. Biol. 35 (4), 1130–1139.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific postprocessing steps involved after the model training. However, it can be inferred that some form of evaluation is performed based on the use of development and testing sets. These sets are likely used to calculate metrics such as accuracy, precision, recall, or F1 score, which could help determine the performance of the classifiers. Additionally, confusion matrices might also have been employed to visualize the performance of the classifiers. It is important to note that saliency maps are typically associated with image processing tasks rather than text classification tasks like those mentioned in this passage. Therefore, it would be speculative to suggest that they were used here.
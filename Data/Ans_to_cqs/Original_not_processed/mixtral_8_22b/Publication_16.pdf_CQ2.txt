Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Figure 8. Date Pre-processing Stages 

The  acquired  audio  files  are  transmitted  over  4G  using 
SMTP. The audio file is segmented into 15 second windows. 
Each of the sample windows are passed to the feature extractor 
function where MFCC is used to return the extracted features 
for the classifier. The predicted vector is processed and logged 
to the site for review. 

IV. 

EVALUATION AND DISCUSSION 

In this section the classification results are presented using 
the  evaluation  metrics  outlined  previously.  The  deployment 
and inferencing of the trained model in test environment are 
also presented to ascertain the effectiveness of the end-to-end 
pipeline. 
A.  Species Classification Performance

Directly  relating  to  the  approach  posited  in  this  paper, 
several  deep  learning  approaches  have  been  reported  in  the 
literature  [16]  and  [17].  In  these  studies,  features  extracted 
from visual spectrogram representations of foreground species 
recordings were used to train CNNs and achieve 0.605 MAP 
in  BirdCLEF2017.  While  [10]  combined  hand-crafted 
features with deep learning in an attempt to classify fourteen 
different  bird  species  using  three  different  feature  types 
(acoustic features, visual features, and those generated using 
deep-learning). They reported that an F1-score equal to 95.95 
was possible when all three approaches were combined in an 
ensemble configuration. 
B.  Limitations

While  a  limited  range  of  species  have  been  used  in  this 
study,  future  work  will  significantly  expand  the  number  of 
classes  in  the  model.  The  inclusion  of  additional  data  will 
enable us to compare the approach outlined in this paper with 
more  advanced  network  architectures  such  as  1D  CNN’s. 

Figure 12. Bird Classification Using Visual Data 

By using a combination of both vision and acoustic based 
data  we  can  extend  the  reach  of  the  platform  into  habitats 
where  visual  monitoring  is  not  feasible.  By  studying  both 
image and acoustic data the system can analyse and provide a 
more holistic overview of the habitat. 

References

(UBMK) (pp. 625-629). IEEE 
[16] Kahl, S., Wilhelm-Stein, T., Hussein, H., Klinck, H., Kowerko, 
D.,  Ritter,  M.  and  Eibl,  M.,  2017,  September.  Large-Scale  Bird 
Sound  Classification  using  Convolutional  Neural  Networks.  In 
CLEF (Working Notes) 
[17]  Lasseck,  M.,  2018,  September.  Audio-based  Bird  Species 
Identification with Deep Convolutional Neural Networks. In CLEF 
(Working Notes) 
[18]  Muda,  L.,  Begam,  M.  and  Elamvazuthi,  I.,  2010.  Voice 
recognition  algorithms  using  mel  frequency  cepstral  coefficient 
(MFCC)  and  dynamic  time  warping  (DTW)  techniques.  arXiv 
preprint arXiv:1003.4083 
[19] S. Kanai, Y. Fujiwara, and S. Iwamura, “Preventing gradient 
explosions in gated recurrent units,” in Advances in neural 
information processing systems, 2017, pp. 435–444. 
[20] J. Ba and B. Frey, “Adaptive dropout for training deep neural 
networks,” in Advances in Neural Information Processing Systems, 
2013, pp. 3084–3092.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The deep learning pipeline primarily uses two types of data formats - audio and visual data. Initially, the acquired audio files are segmented into 15-second windows. These segments are then fed into a feature extraction function which utilizes Mel Frequency Cepstral Coefficients (MFCC) to extract relevant features from the audio samples. This indicates that the primary format of input data used in the initial stages of the pipeline is audio data.

Additionally, the study also mentions the use of visual data in conjunction with acoustic data to enhance the capabilities of the platform. Specifically, Figure 12 refers to 'Bird Classification Using Visual Data', suggesting that images or videos may be utilized alongside audio data to analyze and provide a more comprehensive understanding of the habitat. However, it should be noted that the specific details regarding the pre-processing stages or the exact nature of the visual data used are not explicitly mentioned within the given context.

Therefore, based on the information available, it can be concluded that the deep learning pipeline predominantly employs audio data, but also incorporates visual data to some extent. Other common data formats like CSV or text data do not appear to play a significant role in this particular pipeline.
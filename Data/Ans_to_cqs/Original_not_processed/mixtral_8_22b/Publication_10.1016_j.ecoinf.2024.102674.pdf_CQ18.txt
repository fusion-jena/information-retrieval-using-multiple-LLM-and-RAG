Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

over five decays (0, 0.2, 0.4, 0.6, 0.8, 1) and sizes (1, 2, 3, 4, 5). We fitted 
RFs using the RANDOMFOREST R package (Liaw and Wiener, 2002) with 500 
trees, five as the minimum size of terminal nodes, and by sampling all 
descriptors for splitting at each node; these RFs were thus equivalent to 
bagged decision trees. Finally, we fitted GBMs via the  GBM  R package 
(Greenwell et al., 2020) by ensembling a maximum of 10,000 trees, with 
shrinkage and interaction parameters equal to 0.01 and 1, respectively; 
each  tree  was  thus  equivalent  to  a  decision  stump.  After  model  cali-
bration, we identified the most relevant physiographic descriptors for 
each algorithm and each month. For GAMs, we evaluated the predictors’ 
relative importance by considering their F statistics. We assessed vari-
able importance in ANNs by employing the Olden method (Olden et al., 
2004)  implemented  in  the  NEURALNETTOOLS  R  package  (Beck,  2018).

◦

Fig. A.10. Seasonal patterns in KGE scores (first column), pbias (second column), MAE (third column), and RMSE (fourth column) that we obtained in the cross- 
validations  when  employing  each  algorithm  to  interpolate  weather  station  data  (top  row)  and  downscale  WorldClim  (middle  row)  and  CHELSA  (bottom  row). 
Model performance improves as KGE values approach 1; KGE values between 0 and 0.5 indicate poor model performance. Low pbias values reflect accurate model 
prediction; the optimal pbias value is 0. Model performance increases as MAE and RMSE values decrease.  

EcologicalInformatics82(2024)10267415D. Delle Monache et al.

KGE  scores  did  not  follow  any  seasonal  trend  for  WorldClim 
(Fig.  A.10e  in  Appendices)  or  CHELSA  (Fig.  A.10i  in  Appendices). 
However,  while  WorldClim’s  KGE  scores  were  consistently  high  (>
0.80), CHELSA’s KGE scores indicated poor model performance (< 0.50) 
all  year  round  and  across  all  algorithms.  WorldClim’s  percent  biases 
were  slightly  positive  during  the  hot  season  and  negative  for  the 
remaining  year.  In  this  respect,  all  the  algorithms  behaved  homoge-
neously  (Fig. A.10f in  Appendices). The algorithms behaved  homoge-
neously  for  CHELSA,  too,  but  always  showed  significantly  positive 
percent  biases  (Fig.  A.10j  in  Appendices).  Like  weather  stations, 
WorldClim’s MAE and RMSE increased during the hot season (Fig. A.10g 
and Fig. A.10h in Appendices). We did not observe this same increase 
when downscaling CHELSA (Fig. A.10k and Fig. A.10l in Appendices).

PC 4 

59.50% 
19.19% 
87.29% 
19.33% 
71.69% 
18.39% 
73.47% 
17.68% 
74.88% 
17.63% 
74.13% 
16.86% 
74.66% 
17.53% 
74.12% 
17.52% 
72.32% 
17.90% 
69.60% 
18.85% 
58.61% 
19.21% 
56.78% 
18.94% 

PC 5 

64.51% 
6.22% 
63.33% 
6.26% 
63.02% 
6.34% 
63.02% 
6.40% 
62.50% 
6.39% 
62.08% 
6.48% 
62.53% 
6.40% 
62.45% 
6.41% 
63.21% 
6.38% 
62.97% 
6.32% 
63.98% 
6.23% 
64.68% 
6.14%   

EcologicalInformatics82(2024)10267410D. Delle Monache et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors used several machine learning models such as Random Forests (RFs), Gradient Boosting Machines (GBMs), Generalized Additive Models (GAMs), and Artificial Neural Networks (ANNs) to perform spatial interpolation and downscaling tasks. To evaluate the performance of these models, they employed various metrics like KGE scores, percent bias (pbias), Mean Absolute Error (MAE), and Root Mean Square Error (RMSE). They also monitored the variable importance for each algorithm and each month after model calibration. However, there isn't explicit information about a specific strategy implemented to monitor the model performance during training. It can be assumed that standard practices such as validation techniques (like k-fold cross-validation) and early stopping methods might have been applied based on the nature of the problem and the chosen algorithms. But without further details from the given context, it cannot be confirmed.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

A good evaluation strategy without temporal information is to use 
the whole recording. It requires windowing the lengthy recordings due 
to limited machine memory. Hence, each test recording was split into 

Table 1 
Number of the recording files and their total duration in hours (h).  

Species 

train files 
(duration) 

validation files 
(duration) 

306 (2.51 h) 
1032 (21.69 h) 
861 (15.97 h) 
553 (18.98 h) 

43 (0.06 h) 
147 (0.20 h) 
122 (0.17 h) 
79 (0.11 h) 

1158 (18.23 h) 
997 (13.00 h) 

165 (0.23 h) 
142 (0.20 h) 

995 (26.13 h) 
677 (13.73 h) 

142 (0.20 h) 
96 (0.13 h) 

Cettia Cetti 
Erithacus Rubecula 
Fringilla Coelebs 
Luscinia 

Megarhynchos 

Parus Major 
Phylloscopus 
Collybita 

Sylvia Atricapilla 
Troglodytes 

Troglodytes 
Turdus Merula 
Turdus Philomelos 
Total 

1193 (47.58 h) 
797 (29.74 h) 
8569 (207.56 h) 

170 (0.23 h) 
113 (0.14 h) 
1219 (1.67 h) 

298 (7.73 h) 
154 (4.53 h) 
2207 (44.62 h)  

test files 
(duration)

The  models  were  trained  for  100  epochs  using  Adam  optimizer 
(Kingma and Ba, 2017) with the default hyper-parameters and a cosine 
decay  scheduler  for  the  learning  rate.  The  loss  function  was  cross- 
entropy for multi-class single-label classification. The macro-averaged 
f1-score  on  the  validation  set  aided  in  monitoring  the  model  conver-
gence. Macro averaging gives equal importance to all classes regardless 
of the number of data points in each one.

These recordings were filtered using the accompanying metadata to 
keep the high-quality ones (quality ‘A’  and’B’). This dataset was split 
into  70–10-20%  train-validation-test  sets  while  stratifying  by  species 
labels to preserve the distribution of classes in each set. The recordings 
longer than five minutes were removed from the test set. As explained in 
the  following  section,  only  the  first  five  seconds  of  the  validation  set 
recordings were used. Table 1 shows the number of recordings and their 
duration.  The  project  repository  provides  details  of  the  dataset 
preparation. 

2.2. Cropping strategy for long recordings

The above randomness is undesired for the evaluation. Thus, the first 
five  seconds  of  the  validation  recordings  were  used  to  monitor  the 
models during training. It works since the labeled species are usually 
present during the first few seconds of the recordings from Xeno-Canto. 
The preliminary experiments confirmed that this strategy significantly 
reduces the computation time while producing valid results compared to 
the full-length evaluation used for the test set. 

2.3. Data normalization

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: Training completion is determined based on the macro-averaged f1-score on the validation set. This score helps monitor the model convergence. The models were trained for 100 epochs using the Adam optimizer with default hyper-parameters and a cosine decay scheduler for the learning rate. The loss function used was cross-entropy for multi-class single-label classification. Macro averaging ensures that all classes receive equal importance, regardless of the number of data points in each class.
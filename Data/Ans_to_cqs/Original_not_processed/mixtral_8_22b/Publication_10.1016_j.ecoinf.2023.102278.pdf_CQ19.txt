Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(cid:0) 4 was finally 
chosen after comparing with stochastic gradient descent (SGD) with the 
(cid:0) 4  and learning rate of 
(cid:0) 4, weight decay of 1.0⋅10
momentum of 1.0⋅10
(cid:0) 3. SGD was tested with the parameters as recommended by L. 
1.0⋅10
Smith  (Smith,  2018),  but  achieved  a  1%–2%  decrease  in  accuracy 
compared to using the Adam optimizer. 

The  ResNet50  architecture  (He  et  al.,  2016)  was  modified  and 
trained with transfer learning using pre-trained weights from ImageNet 
(Smith,  2018).  Models  trained  with  transfer  learning  outperformed 
models trained with random weights with an increase of 4% in the F1-

Table 4 
Average performance (Avg) and standard deviation (SD) for five trained models. Average precision, recall and F1-score for trained ResNet50 and EfficientNetB3 
(EffNetB3) models modified for multitask learning (MTL) with transfer learning using pre-trained weights from ImageNet. The models are trained and validated on the 
TLm  dataset. The models ResNet50, EfficientNetB3 are trained without MTL.  

Model 

Level 

ResNet50MTL 
EffNetB3MTL 

ResNet50MTL 
EffNetB3MTL 

ResNet50MTL 
EffNetB3MTL 

ResNet50 
EffNetB3 

L1 Order 
L1 Order 

L2 Family 
L2 Family 

L3 Species 
L3 Species 

Species 
Species 

Avg 

0.990 
0.986 

0.987 
0.984 

0.955 
0.948 

0.955 
0.953 

Precision 

SD (10

(cid:0) 3) 

(1.0) 
(4.4) 

(0.8) 
(3.1) 

(4.3) 
(5.2) 

(3.3) 
(2.5) 

Avg 

0.991 
0.993 

0.986 
0.988 

0.961 
0.966 

0.957 
0.966 

Recall 

SD (10

(cid:0) 3) 

(1.1) 
(0.5) 

(0.9) 
(0.7) 

(9.8) 
(5.1) 

(7.3) 
(2.5) 

Avg 

0.991 
0.989 

0.987 
0.986 

0.957 
0.956 

0.955 
0.959

The  performances  of  ResNet50  and  EfficientNetB3  with  transfer 
learning are very similar; however, at the species level (3), ResNet50 has 
a  higher  standard  deviation  (0.32%).  State-of-the-art  models  trained 
with transfer learning and without MTL have an F1-score of 96%, similar 
to models trained with MTL at the species level. The models trained with 
ResNet  and  EfficientNet  have  the  absolute  best  performance  and 
outperform the benchmark YOLOv5 model on the TL dataset referenced 
in (Bjerge et al., 2023a). This is expected since the tasks performed by 
YOLOv5 include both object detection as well as classification. No other 
state-of-the-art CNN models were evaluated since it is not expected to 
achieve scores higher than 96% to 99% already achieved and the main 
purpose  was  to  demonstrate  that  different  CNNs  can  be  modified  for

Rl = (Wl2*σrelu(Wl1*R0 + bl1)) + bl2)

(2) 

Here  Wl1  and  Wl2  represent  the  weights,  and  bl1, bl2  represent  the 
biases for the independent FC layer representation. We apply dropout 
regularization  in  the  forward  pass  during  training  so  that  random 

EcologicalInformatics77(2023)1022784K. Bjerge et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given context, there is no explicit information about which specific frameworks such as TensorFlow or PyTorch were used to build the deep learning models mentioned in this study. Therefore, I cannot provide a definitive answer regarding the use of these particular frameworks. However, considering that the authors compared their results with those obtained using Stochastic Gradient Descent (SGD), it could be inferred that they might have employed one of the popular deep learning libraries like TensorFlow or PyTorch, as these libraries support various optimization algorithms including SGD. Nonetheless, further clarification would require additional information or resources beyond what is currently available.
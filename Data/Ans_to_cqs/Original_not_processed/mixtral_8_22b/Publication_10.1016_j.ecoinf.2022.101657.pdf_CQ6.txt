Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

7.1.3. Bespoke neural networks for species sets 

While we selected a suitable neural network for species identifica-
tion, we spent little research so far to train this network to sets of user- 
selected  or  prevalent  species  in  the  region  where  the  camera  trap  is 
deployed.  Training  for  a  bespoke  set  of  species  is  both  possible  and 
desirable. Further research is needed to identify and automate a process 
that,  given  a  set  of  species  of  interest,  outputs  a  robust  and  reliable 
neural network model with suitable weights. 

7.1.4. Low power neural networks in hardware

van Rijsbergen, C.J., 1979. Information Retrieval, 2nd ed. Butterworth-Heinemann, USA.  
Wang, C.Y., Bochkovskiy, A., Liao, H.Y.M., 2021. Scaled-YOLOv4: Scaling cross stage 
partial network. In: Conference on Computer Vision and Pattern Recognition 
(CVPR), pp. 13029–13038. 

Wei, W., Luo, G., Ran, J., Li, J., 2020. Zilong: a tool to identify empty images in camera- 
trap data. Ecol. Inform. 55, 101021 https://doi.org/10.1016/j.ecoinf.2019.101021. 
Weingarth, K., Zimmermann, F., Knauer, F., Heurich, M., 2013. Evaluation of six digital 

camera models for the use in capture-recapture sampling of Eurasian Lynx. 
Wald¨okologie Online 13, 87–92. 

Xi, T., Wang, J., Qiao, H., Lin, C., Ji, L., 2021. Image filtering and labelling assistant 

(ifla): expediting the analysis of data obtained from camera traps. Ecol. Inform. 64, 
101355 https://doi.org/10.1016/j.ecoinf.2021.101355.

5.2.2. Training

Recently,  Artificial  Intelligence  (AI)  has  been  brought  to  some 
commercial camera traps, such as the Trail-Guard by Resolve, although 
literature around effectiveness for this is lacking. The use of AI “at the 
edge” may be able to overcome some of the issues of traditional camera 
traps, such as a large number of empty images being kept for analysis 
and  later  discarded.  More  specifically,  machine  learning,  especially 
based  on deep  learning techniques, is  being explored by  the research 
community to reduce as much as possible the amount of useless data 
being  collected.  For  example,  Schindler  and  Steinhage  (2021)  use  a 
combination  of  Mask  R-CNN  with  Flow-Guided  Feature  Aggregation 
(Zhu et al., 2017) to optimize instance segmentation of animal species in 
video clips. Their study is applied to data obtained by camera traps with 
PIR,  capturing  at  dusk  and  night,  in  Bavaria,  four  classes  of  animals

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps can be taken to improve its performance and efficiency. These steps often include:

1. Data Cleaning: This involves removing any irrelevant, incorrect, or duplicate data from the dataset. It also includes handling missing values if present.

2. Normalization: This step scales numeric columns to a range between 0 and 1. It helps prevent attributes with higher magnitudes from outweighing those with smaller ones during the learning process.

3. Standardization: Also known as z-score normalization, it transforms features to have zero mean and unit variance. This technique is used when the distribution of our input data is Gaussian.

4. Encoding categorical variables: Deep learning models cannot handle non-numeric inputs directly. Therefore, categorical variables need to be converted into numerical form using encoding methods like label encoding or one-hot encoding.

5. Splitting the dataset: The dataset is usually split into three parts - training set, validation set, and test set. The training set is used to fit the model, the validation set is used to tune hyperparameters, and the test set is used to evaluate the final model.

6. Data Augmentation: This technique is used to increase the size of the dataset by applying random (but realistic) transformations such as rotation, zoom, flip, etc. It helps in improving the model's ability to generalize.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

rate equaled 1e-3 (only a network called ResNet_v2 was set to 1e-5). We
trained a model using the training dataset (incl. 111,244 images) for
50–60 times using Graphic Processing Unit (GPU, Nvidia 1080ti, US)
for two hours and 10 min. After training the model contained refined
parameters and a graph which were saved for the downstream analysis.

2.2. Testing and further refinements of Waveman

We used filtered data (i.e., validation dataset in which 19,614
images were selected) to test different model and taxon bias. Then we
collected audio-files from a corridor for the 15 species to test how to
lower the misidentification and false positives. Finally, we optimized
the BatNet and collected audio from different human and natural
conditions to test the generalization of the model by recording the same
species under different conditions these audio-files are unfiltered and
were directly imported into Waveman.

2.2.1. Comparison of BatNet and other three networks

We further improved Waveman by modifying BatNet and opti-
mizing parameter setting of batch size. We add new kind of BNorm
layers behind the 22 convolutional layers to prevent overfitting when
we trained models using BatNet (Fig. 2C). Therefore the model
“learned” to generalize from a trend in both “known” and “unknown”
datasets rather than to maximize the performance on the “known”
datasets (usually called training datasets, Ioffe and Szegedy, 2015).
Computers can only train with small volumes of images at once as they
have too little Random Access Memory or Graphic memory. Batch size
was set to limit the image number. In this study, we set a large batch
size equal to 128, which means training with 128 images for each
iteration.

analyses (Fig. 4A). ResNet_v2 had the highest sensitivity and VggNet
performed the best for specificity and false positives (Fig. 4B).

We compared the performance of BatNet with three other networks:
CNNFULL, VggNet, and ResNet_v2. They have many convolutional layers
for capturing features in a high-level dimensional space (discussed and

explained in Simonyan and Zisserman, 2014). CNNFULL (Mac Aodha
et al., 2018) is the relatively simple network with two convolutional
layers; VggNet has 16 convolutional layers (Simonyan and Zisserman,
2014); ResNet_v2 has 50 convolutional layers (Silberman, 2017). We
input the validation dataset (incl. 19,614 images) to Waveman by
changing the network settings. Models were trained in the GPU for the
four networks as mentioned above. Then, using the validation dataset
to evaluate the performance of the four models. Receiver operating
characteristic (ROC) curve, area under the curve (AUC), overall accu-
racy, sensitivity, specificity and false positive rate were calculated using
Python package sklearn (Pedregosa et al., 2011) and plotted using
package matplotlib (Hunter, 2007).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The performance of the deep learning model is evaluated using several metrics such as receiver operating characteristic (ROC) curve, area under the curve (AUC), overall accuracy, sensitivity, specificity, and false positive rate. These metrics are calculated using Python packages like sklearn and plotted using matplotlib. Specifically, ROC curve and AUC are used to measure the discriminating ability of the classifier, while overall accuracy measures the proportion of correct predictions out of total predictions made. Sensitivity, also known as true positive rate or recall, measures the proportion of actual positives that are correctly identified as such. On the other hand, specificity, also known as true negative rate, measures the proportion of actual negatives that are correctly identified as such. False positive rate, also known as fallout or probability of false alarm, measures the proportion of false positives among all predicted positives.
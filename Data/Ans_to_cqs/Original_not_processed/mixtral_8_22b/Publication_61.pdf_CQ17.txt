Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Training a neural network requires to set several hyper-parameters
such as the learning rate, number of epochs and batch size. The learning
rate is a special one since it deﬁnes how much the weights are ‘moved’
to decrease the loss. A bigger one could cause the network not to learn,
on the other hand, a smaller one could require much more steps for
learning. To decrease this issue, we train the network with Adam op-
timizer (Kingma and Ba, 2015), a variant of the stochastic gradient
optimization where the learning rate is adjusted automatically. The rest
of the parameters are set empirically, more details are presented in the
experiments section.

5. Experiments

In this section, we present the experimental characterization of the
proposed approach. The implemented network was trained with Adam
optimizer (Kingma and Ba, 2015) on a Inter Core i7 machine with
NVIDIA GeForce 1080 GPU. The hyperparameters were set as follows:
learning rate 0.01, number of epochs 150, batch size 2500.

5.1. Data augmentation accuracy

A common practice to improve the accuracy of a deep learning
model is to do data augmentation. In this section, we compare the ac-
curacy of the network when data augmentation is included during
training. The compared training variations are the following:

(cid:129) No augmentation. The patches only are resized to 32 × 32 and

normalized.

Fig. 8. Training loss.

Fig. 9. Training accuracy.

Fig. 7. Custom LeNet-5 network for cactus recognition.

135

E. López-Jiménez, et al.

Ecological Informatics 52 (2019) 131–138

Fig. 10. Validation accuracy.

Fig. 12. Normalized confusion matrix.

(cid:129) Vertical and horizontal ﬂip. In addition to the resize and nor-
malization, the patches are ﬂip vertically or horizontally with prob-
ability 0.5. Both ﬂips are independent events.

2013) or image captioning (Vinyals et al., 2017). Unlike other machine
learning techniques, where several manually designed features are ex-
tracted (for example, edges, shapes or feature points) and passed to a
classiﬁer, DL automatically extracts relevant features and performs a
logistic regression (Goodfellow et al., 2016). A branch of DL is the su-
pervised DL, which uses already labeled examples to train a CNN by
minimizing the error provided by a diﬀerentiable loss metric that
compares the network prediction against the ground truth label. De-
spite the fast spreading of deep learning, such techniques require a large
amount of data to perform well. This is a limitation for the recognition
of all kind of vegetation. Our research focuses on the detection of en-
demic plants in the Tehuacan-Cuicatlan Valley. The ﬁrst challenge we
faced was that it did not exist an available dataset of images for this
particular task. Some limited examples were available as illustrations

5.4. Dataset extension

The proposed approach has shown good performance (accuracy) for
cactus recognition in aerial images. One way to improve the current
accuracy could be to make deeper the current CNN, however, im-
proving the accuracy with a deeper network will tend to memorize the
dataset due to the model capacity (Goodfellow et al., 2016), causing
model overﬁtting. A better improvement to the current work is to in-
clude more examples in the dataset. Such examples should be obtained
under diﬀerent conditions, some variations are diﬀerent daylight con-
ditions or diﬀerent year seasons. Diﬀerent season pictures will provide
a distinctive context because the rainy season changes all valley's ve-
getation.

Fig. 11. Confusion matrix.

Fig. 13. Example of a cactus image. The axes indicate pixel indices.

136

E. López-Jiménez, et al.

Ecological Informatics 52 (2019) 131–138

Fig. 14. Learned kernels and feature maps. The axes indicate pixel indices.

6. Conclusions

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, there is no explicit mention of any specific regularization method used to prevent overfitting in the deep learning pipeline. However, the authors discuss potential ways to avoid overfitting in their approach. They suggest that making the current convolutional neural network (CNN) deeper might lead to memorizing the dataset due to the model capacity, resulting in overfitting. Instead, they propose including more examples in the dataset under different conditions, such as varying daylight conditions or different year seasons. By doing so, they aim to improve the current accuracy without increasing the risk of overfitting.

Regularization methods like dropout and L2 regularization are commonly used to prevent overfitting in deep learning models. Dropout randomly sets a fraction of input units to 0 at each update during training time, effectively preventing complex co-adaptations between neurons. On the other hand, L2 regularization adds a penalty term to the loss function, discouraging large weights and promoting simpler models. These methods can help reduce overfitting by introducing noise into the training process or limiting the complexity of the learned representations.

However, based solely on the provided context, it cannot be determined whether these specific regularization methods were employed in the described deep learning pipeline.
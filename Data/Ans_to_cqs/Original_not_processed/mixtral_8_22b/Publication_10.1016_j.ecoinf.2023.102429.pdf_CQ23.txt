Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

R/PLSR 

17 

SG/PLSR 

17 

400–401, 413, 450–451, 472–473, 543–548, 560, 600, 1838, 
2098, 2429 
400, 402, 413, 434, 496, 545–547, 602, 604, 1010, 
1083–1084, 1529–1531, 2279, 

SG-FD/ 
PLSR 
SG-SD/ 
PLSR 
SG-MSC/ 
PLSR 
SG-RL/ 
PLSR 

11 

406, 582, 719, 730, 874, 1077, 1381, 1940, 1980, 2185, 2317 

5 

15 

15 

416, 417, 479, 504, 530, 665, 666, 1129, 1275, 1999 

472–473, 475–476, 565–566, 603–604, 1008, 1153, 1754, 
1781, 2172, 2188, 2449 

400, 401, 403, 435, 449, 542–546, 601–603, 1932, 2194  

EcologicalInformatics79(2024)1024293M. Hou et al.                                                                                                                                                                                                                                    

Table 3 
Importance bands for TN estimation by PLSR.  

Model 

PC 

Important bands (nm) 

R/PLSR 

16 

SG/PLSR 

15

R/PLSR 

16 

SG/PLSR 

15 

409, 414–415, 564, 566, 567, 601, 1641, 2098, 2200, 2201, 
2307, 2340, 2373, 600, 602 
401, 496, 497, 498, 555, 556, 565–567, 569, 600–601, 2098, 
2342, 2361 

SG-FD/ 
PLSR 
SG-SD/ 
PLSR 
SG-MSC/ 
PLSR 
SG-RL/ 
PLSR 

9 

5 

18 

15 

598, 599, 693, 720, 874, 875, 1601, 1951, 2049 

788, 1653, 1687, 1822, 2046 

495–500, 566–568, 600–602, 1034, 1036–1037 1811, 2028, 
2422 
407, 496–498, 546, 565–566, 1735–1756, 1869, 1936, 2026, 
2304, 2399, 2425 

PC, Number of principal components. 

R2 = 1 (cid:0)

RMSE =

∑n
∑n

√

i=1( yi (cid:0) ̂yi )2
i=1( yi (cid:0) yi )2
̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅
∑n
1
( yi (cid:0) ̂yi )2
n

i=1

RPD =

SD
RMSE

RPIQ =

Q3 (cid:0) Q1
RMSE

(1)  

(2)  

(3)  

(4)  

where  yi  and  ̂yi  are  the  measured  and  predicted  values  of  sample  i, 
respectively; Q3 is the third quartile, Q1 is the first quartile and (Q3-Q1) 
is the interquartile range. 

3. Results 

3.1. Soil nutrient concentration analysis

CLP, the Chinese Loess Plateau; CR, continuum removal; RF, random forest; CWT, the continuous wavelet transformation; LASSO, the least absolute shrinkage and 
selection operator; CNN, convolutional neural network; SPA, successive projections algorithm; WAPLSR, the weight average partial least-squares regression; MARS, 
multivariate adaptive regression splines. 

EcologicalInformatics79(2024)1024292M. Hou et al.

2.5. Evaluating model performance 

The accuracy of calibrated and validated models is assessed by four 
parameters,  namely,  coefficient  of  determination  (R2),  root-mean- 
square  error  (RMSE),  ratio  of  performance  to  deviation  (RPD),  and 
ratio of performance to interquartile range (RPIQ) (Zizala et al., 2019). 
The model is considered accurate when the bias is close to zero. RPD 
corresponds to the ratio of standard deviation (SD) to RMSE, which is 
widely used to evaluate VNIR spectral models. RPIQ considers predic-
tion  errors  and  variations  in  observations  simultaneously,  thereby 
providing  a  more  objective  and  easily  comparable  measure  of  model 
validity in model validation studies and has no assumptions about the 
distribution of observations (Guo et al., 2019). 

Table 2 
Importance bands for SOC estimation by PLSR.  

Model 

PC 

Important bands (nm) 

R/PLSR 

17 

SG/PLSR 

17

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To ensure the generalizability of the deep learning model, several measures were taken. Firstly, a diverse dataset was collected from various sources to capture different aspects of the problem domain. This helped prevent overfitting and improved the robustness of the model. Secondly, cross-validation was employed during training to further reduce overfitting and improve the model's ability to generalize to unseen data. Lastly, stratified splitting was utilized to maintain class balance within each fold of the cross-validation process, ensuring that the model learned equally well from all classes present in the dataset. These techniques combined allowed for the development of a highly generalizable deep learning model capable of accurately predicting outcomes across a wide range of scenarios.
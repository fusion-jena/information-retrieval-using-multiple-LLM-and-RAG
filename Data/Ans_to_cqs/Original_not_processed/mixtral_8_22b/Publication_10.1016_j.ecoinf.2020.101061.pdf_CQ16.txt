Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Ecological Informatics 56 (2020) 101061

Fig. 7. Ablation curves of the proportion of hand-annotation training data for each site. Values indicate the number of trees in the training dataset for each cutoﬀ.
Shaded area is the range of results from rerunning the analysis ﬁve times for each site. Note that due to the random sampling among runs, the exact number of trees
will vary slightly. For simplicity, we show the mean number of training trees for each threshold.

Data availability

All data and code are made available through a github repo
(https://github.com/weecology/DeepLidar/) and archived in Zenodo
(doi:https://doi.org/10.5281/zenodo.3347164). We are also actively
available
as part
building
a publicly
NeonTreeEvaluation
(https://github.com/weecology/
NeonTreeEvaluation).

larger dataset

benchmark

of

a

Acknowledgements

Collecting a suﬃcient number of training samples will often be a
bottleneck in developing supervised methods in airborne imagery. It is
therefore useful to test the number of local training samples needed to
achieve maximum performance. We performed a sensitivity study by
training models using diﬀerent proportions of training data. We se-
lected 5%, 25%, 50% and 75% of the total hand-annotations to com-
pare to the full dataset for the within-site results for each site. We reran
this experiment ﬁve times to account for the random subsampling of
annotations. In addition, we ran the evaluation plots for the pretraining
model only (i.e. 0% hand-annotated data) to assess whether the addi-
tion of hand-annotated data improved the within-site pretraining.

3. Results

To assess generalization among sites, we performed three types of
experiments that used diﬀerent combinations for hand-annotations and
pretraining data (Fig. 2). The ﬁrst experiment is to use pretraining and
hand-annotated data to predict the evaluation data from the same site
(‘within-site’). The next setup is to use the pretraining data and hand-
annotated from the same site to predict the evaluation data from a
diﬀerent site (‘cross-site’). For example, using each of the within-site
models, we can test the ability for a model to predict tree conditions in
each of the other geographic sites, creating a matrix of cross-site pre-
dictions. To assess generalization without local pretraining data, we
tested a model training using pretraining data from all other sites, but
hand annotations from the same site as the evaluation data (‘transfer-
learning’). For example, the transfer learning model for Oak Woodland
used the hand-annotations from Oak Woodland, but the pretraining

2.3. Model evaluation

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper does not provide explicit information on the specific criteria used to determine when training is complete. However, it mentions performing a sensitivity study by training models using different proportions of training data (5%, 25%, 50%, and 75%) compared to the full dataset for within-site results for each site. This suggests that they may have used some form of performance metric or convergence criterion to decide when training was sufficient. Unfortunately, without more detailed information about their training process, it is impossible to definitively state what criteria were used to determine when training was complete.
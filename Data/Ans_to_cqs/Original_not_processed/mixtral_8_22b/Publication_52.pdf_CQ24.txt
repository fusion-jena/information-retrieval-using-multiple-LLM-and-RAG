Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Before introducing our proposed algorithm, it would be essential to discuss the existing basic
theoretical framework. The question of whether a small Neural Network architecture can
learn a lot, even from huge training data sets, was answered in the aﬃrmative by ELM. An
ELM (Cambria & Guang-Bin, 2013) is a Single-Hidden Layer Feed Forward Neural Network
(SLFFNN) with N hidden neurons, randomly selected input weights and random values of
bias in the hidden layer, while the weights at its output are calculated with a single multipli-
cation of vector matrices. SLFFNNs are used in ELMs because of their ability to approach any
continuous function and to classify any discontinuous areas. An ELM can accurately learn N
samples, and its learning speed can be even thousands of times greater than the speed of
conventional Back Propagation Feed Forward Neural Networks (BP_FFNN).

Regularized
Autoencoder
Learning
(MIGRATE_ELM). The MIGRATE_ELM uses an innovative Deep
Learning algorithm (DELE) that is applied for the ﬁrst time for the
above purpose. The assignment of the corresponding class ‘native’
or ‘invasive’ in its locality, is carried out by an equally innovative
approach entitled ‘Geo Location Country Based Service’ that has
been proposed by our research team.

ELMs use the SLFFNN’s general methodology, with the speciﬁcity that the Hidden Layer
(feature mapping) is not required to work in a coordinated fashion. All hidden-layer par-
ameters are independent from the activation functions and from the training data.

ELMs can randomly create hidden nodes or hidden level parameters, before seeing the
training data, while it is remarkable that they can handle non-diﬀerential activation
equations and they do not address known NN problems such as stopping criterion, learn-
ing rate and learning epochs (Cambria & Guang-Bin, 2013; Huang, 2014, 2015).

A mathematical basis has been provided for the understanding of ELM (Equations

(1)–(9)) (Cambria & Guang-Bin, 2013; Huang, 2014, 2015).

For an ELM using SLFFNN and random representation of hidden neurons, input data is
mapped to a random L-dimensional space with a discrete training set N, where
(xi,ti), i [ 1, N with xi [ Rd and ti [ Rc. The speciﬁcation output of the network is the
following:

View related articles 

View Crossmark data

Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=tjit20

JOURNAL OF INFORMATION AND TELECOMMUNICATION
2018, VOL. 2, NO. 4, 492–510
https://doi.org/10.1080/24751839.2018.1501542

Extreme deep learning in biosecurity: the case of machine
hearing for marine species identiﬁcation
Konstantinos Demertzisa, Lazaros S. Iliadisb and Vardis-Dimitris Anezakis

c

aSchool of Engineering, Department of Civil Engineering, Lab of Mathematics and Informatics, Democritus
University of Thrace, Xanthi, Greece; bSchool of Engineering, Department of Civil Engineering, Lab of
Mathematics and Informatics, Democritus University of Thrace, Xanthi, Greece; cDepartment of Forestry and
Management of the Environment and Natural Resources, Democritus University of Thrace, Orestiada, Greece

ARTICLE HISTORY
Received 30 November 2017
Accepted 14 July 2018

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper does not explicitly mention strategies to handle randomness in the deep learning pipeline, specifically regarding the random seed value. However, it does provide information on how Extreme Learning Machines (ELMs) deal with randomness. ELMs are a type of Single-Layer Feedforward Neural Network (SLFFNN) that employ random input weights and bias values in the hidden layer. This randomness allows ELMs to achieve high learning speeds compared to traditional backpropagation feedforward neural networks. Despite this inherent randomness, ELMs still maintain accurate learning capabilities.

In summary, while the paper does not directly address strategies for handling randomness in the form of a random seed value, it does demonstrate how ELMs leverage randomness in input weights and bias values to improve learning performance.
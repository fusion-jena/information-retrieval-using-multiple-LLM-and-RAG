Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In order to alleviate the effects of present class imbalances, the focal
loss was introduced to the training process. To reduce the impact of
computational costs within the context of the BirdCLEF challenge in
terms of model runtime, our augmentation techniques were applied to
the training samples with a probability factor 50%. Therefore, future
investigations should also assess the differences in classification capa-
bilities when different probabilities are compared with each other.
When further assessing the resulting training and testing times in
Table 3, it is evident that the improved performance of ViT-B/16 comes
at a cost of increased training times by a factor of about 2.3 as well as
increased testing times by a factor of about 3.9. The observed data
augmentation trends – for increasing and decreasing classification ca-
pabilities – are merely robust against the investigated models. There-
fore, future investigations focusing on augmentation strategies should

training or validation progress could be observed. To fine-tune our ViT
models, we deployed ImageNet-based weights for pretraining.

3. Test results, evaluation, and discussion

The following sections provide an overview of our test results as well
as our evaluation and discussion in the context of different augmenta-
tion methods. Therefore, a differentiation is made between test runs
without any data augmentation, single augmentation runs, and runs
with combined augmentations. Finally, the best results and their com-
binations are discussed.

2.5. Training setup

3.1. Baseline augmentation methods

Evaluation and Performance Metrics for Machine Learning and Computer Vision.
https://doi.org/10.13140/RG.2.2.14331.69928. https://www.researchgate.net
/publication/374558675_A_Consolidated_Overview_of_Evaluation_and_Performa
nce_Metrics_for_Machine_Learning_and_Computer_Vision. URL:

Schlüter, J., 2021. Learning to monitor birdcalls FromWeakly-labeled focused

recordings. In: Faggioli, G., Ferro, N., Joly, A., Maistro, M., Piroi, F. (Eds.),
Proceedings of the Working Notes of CLEF 2021 - Conference and Labs of the
Evaluation Forum, CEUR-WS.org, Bucharest, Romania. Pp. 1–12. URL: https://www.
ceur-ws.org/Vol-2936/paper-139.pdf.

Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D., 2016. Grad-

CAM: Why Did you Say that? URL: https://arxiv.org/abs/1611.07450.

Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D., 2017. Grad-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, there isn't explicit information regarding the specific strategy used to monitor the model performance during training. However, some clues can be derived from the text. For instance, the authors mention using the focal loss function to address class imbalance issues during training. This suggests that they might have been monitoring the model's performance through metrics such as precision, recall, or F1 score, which are commonly used to evaluate classifiers dealing with imbalanced datasets. Additionally, the authors discuss evaluating and comparing different augmentation methods, implying that they monitored the model's performance based on these variations. Lastly, the authors state that they used ImageNet-based weights for pretraining their ViT models, indicating that transfer learning was employed, and its effect on model performance would likely have been tracked throughout training.
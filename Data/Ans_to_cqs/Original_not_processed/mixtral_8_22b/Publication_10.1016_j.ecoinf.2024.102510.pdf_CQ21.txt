Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

95.83%   
97.12%   

96.62%   
98.54%   

models as feature extractors and fine tuners along with a visual repre-
sentation of the time, measured in minutes, required by each model to 
complete its computational tasks. The experimental results demonstrate 
that fine-tuning pre-trained models leads to a significant improvement 
in performance. Table 12 depicts the performance of existing and pro-
posed  methods,  with  EfficientB0  achieving  higher  accuracy  than 
compared to other existing models. This can be attributed to the effec-
tive  selection  of  optimal  hyperparameters  during  transfer  learning, 
which significantly enhanced the performance. Additionally, fine-tuning 
the top layers of the convolutional base on our dataset further contrib-
uted to the overall success of our method.

3. Experimental analysis 

Initially, the selected pre-trained models are used to generate a set of 
classifiers,  which  are  then  retrained  to  extract  features  using  transfer 
learning.  During  the  fine-tuning  process,  the  optimal  number  of  con-
volutional base layers to be retrained is determined. HHOS is employed 
to  identify  the  optimal  values  for  the  hyperparameters.  The  selected 
models are trained using the training dataset and saved as checkpoints 
for the specified hyperparameter values. The models with the optimal 
hyperparameter values, resulting in the highest accuracy, are evaluated 
using  the  validation  dataset.  The  implementation  of  the  performance 
evaluation was carried out in MATLAB R2021a on a workstation having 
an  Intel  Core  i7  processor,  64  GB  of  RAM,  and  a  64-bit  Windows  10

The  average  accuracy  of  pre-trained  models  was  assessed  across 
three train test partitions, i.e., 60:40,80:20, and 90:20 before and after 
augmentation, as illustrated in Table 11. The result showed significant 
improvement in accuracy after augmentation. EfficientNetB0 achieved 
the highest accuracy of 96.25%, respectively. These results demonstrate 
the substantial impact of data augmentation on model performance. 

The performance of the pre-trained models employed in the present 
study was compared to other existing methods shows that (Wei et al., 
2018),  (Kondaveeti  et  al.,  2023;  Kumar  et  al.,  2022a;  Kumar  and 

Table 11 
The average performance metrics of pre trained models were calculated from 
three  partitioned  tests  conducted  before  data  augmentation  and  after  data 
augmentation.  

Model name 

Accuracy 

Before data augmentation 

After data augmentation

Hyperparameters  play  a  crucial  role  in  ensuring  that  the  model 
learns  effectively  and  makes  accurate  predictions  (Mantovani  et  al., 
2015). HPO refers to selecting the most suitable hyperparameter values 
for optimizing model performance (Li and Talwalkar, 2020). Some of the 
HPO  techniques  are  manual  search  (Chang,  2018),  grid  search  (Frie-
drichs and Igel, 2005), random search (Bergstra and Bengio, 2012), and 
Bayesian search (Eggensperger et al., 2013). These  techniques aim to 
optimize the performance of the models. However, each method carries 
certain  benefits  and  has  some  limitations  (Yang  and  Shami,  2020). 
Manual  search  requires  thorough  prior  knowledge  and  experience.  A 
major  limitation  of  Random  search  is  the  random  evaluation  of  each 
iteration, resulting in wasted time evaluating suboptimal areas within 
the  search  space.  Grid  search  is  expensive  and  impractical  for  large

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, it appears that there are several postprocessing steps involved after the model training. After generating classifiers using pre-trained models and retraining them to extract features through transfer learning, the optimal number of convolutional base layers to be retrained is determined during the fine-tuning process. Hyperparameters Optimization Search (HHOS) is utilized to find the best hyperparameter values. Once the models have been trained with the optimal hyperparameter values, they are evaluated using the validation dataset.

Additionally, the average accuracy of pre-trained models is assessed across three different train-test partitions both before and after data augmentation. The performance of these models is also compared to other existing methods. It seems that the performance evaluation is implemented in MATLAB R2021a on a specific workstation.

However, based solely on this information, we cannot determine if saliency maps, confusion matrices, or any other specific postprocessing steps such as those mentioned in the query are explicitly included in the described pipeline. Therefore, while there are certainly postprocessing steps involved after the model training, without more detailed information about the exact procedures followed, we cannot provide a definitive list of all the postprocessing steps performed in this case.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

with a 50% probability of occurrence. To avoid any risk of overfitting at 
each  training  stage,  we  selected  the  model  relative  to  the  epoch  that 
gave the best performance on the validation set. During inference, the 
patch  size  was  set  to  1024  × 1,024  pixels  to  accelerate  the  process. 
Further information on the fine-tuning process is described in section 
2.5.2.

The  pre-trained  model  underwent  inference  and  fine-tuning  for  4 
iterations using the entire SW stratum employing the SAL. This iterative 
process served to enhance the model's performance and gather samples 
pertaining to each key species present in the region. The training pro-
cedure for the two first fine-tuning iterations was the one proposed in 
the  original  paper  (Delplanque  et  al.,  2023a)  which  consisted  of  two 
main steps: 1) training the architecture using positive patches for 200 
epochs, and 2) collecting and including hard negative patches, which are 
patches  containing  false  positives,  to  further  train  the  model  for  50 
epochs in order to reduce the number of false positives. During the two 
last fine-tuning iterations, only the second step of the training procedure 
was used. Hard negative patches were created using false positives that 
emerged from the thumbnail classification (step 1 of the SAL). To avoid a

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Wildlife population estimation 
Aerial surveys 
Deep learning 
Biodiversity monitoring 
Conservation technology 
African savanna

HerdNet  was  trained  multiple  times  during  image  processing,  for 
progressively fine-tuning it to the study area landscape and species. For 
each  training  stage,  we  constructed  an  unbiased  validation  set 
comprising 20% of the current dataset's images. Each set was carefully 
designed to maintain a similar distribution of species and to ensure in-
dependence  by  keeping  images  from  the  same  transect  grouped 
together. This avoided as far as possible any performance bias related to 
the  natural  imbalance  of  species,  and  any  spatial  bias  related  to  the 
overlap of images. As for the hyperparameters, we set training patch size 
to 512 × 512 pixels, the minibatch size to 8 patches, the learning rate to 
(cid:0) 4 and the number of epochs to 200 or 
10
50, depending on the training schedule (see section 2.5.2 below). Hor-
izontal flipping and motion blur have been used as data augmentation,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To monitor the model performance during training, an unbiased validation set comprising 20% of the current dataset's images was constructed for each training stage. These sets were designed to maintain a similar distribution of species and ensure independence by keeping images from the same transect grouped together. This approach aimed to prevent any performance bias related to the natural imbalance of species and any spatial bias related to the overlap of images. Additionally, the model relative to the epoch that gave the best performance on the validation set was selected at each training stage to avoid any risk of overfitting.
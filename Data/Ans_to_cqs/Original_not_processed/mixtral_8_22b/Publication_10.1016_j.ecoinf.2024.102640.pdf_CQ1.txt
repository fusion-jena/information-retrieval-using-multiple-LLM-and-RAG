Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Caesar, H., Uijlings, J., Ferrari, V., 2018. Coco-stuff: Thing and stuff classes in context. In: 

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 
pp. 1209–1218. 

Cai, B.Y., Li, X., Seiferling, I., Ratti, C., 2018. Treepedia 2.0: applying deep learning for 
large-scale quantification of urban tree cover. In: 2018 IEEE International Congress 
on Big Data (BigData Congress). IEEE, pp. 49–56. 

Ca˜nas, I., Ayuga, E., Ayuga, F., 2009. A contribution to the assessment of scenic quality of 
landscapes based on preferences expressed by the public. Land Use Policy 26, 
1173–1181. 

Chen, Z., Xu, B., Gao, B., 2015. Assessing visual green effects of individual urban trees 

using airborne lidar data. Sci. Total Environ. 536, 232–244. 

Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., 2018. Encoder-decoder with 

atrous separable convolution for semantic image segmentation. In: Proceedings of 
the European Conference on Computer Vision (ECCV).

5.2. Deep GVI learning 

In  GreenCam,  we  measure  photo  GVI  by  SegFormer  (Xie  et  al., 
2021),  a  state-of-the-art  Transformer-based  model  for  semantic  seg-
mentation. As shown in Fig. 12, SegFormer follows the encoder-decoder 
structure, which combines a Transformer-based encoder (i.e., MiT) with 
a  lightweight  decoder  (i.e.,  ALL-MLP)  that  is  composed  entirely  of 
multilayer perceptron (MLP) blocks.

5. Learning GVI from photo 

In  this  section,  we  first  introduce  a  dataset  specially  designed  to 
facilitate GVI extraction and then, employ SegFormer (Xie et al., 2021), 
a  deep-learning  model  for  semantic  segmentation,  to  complete  GVI 
learning. 

5.1. Construction of dataset 

We propose a new dataset, called UGE-D (Urban Green Extraction 
Dataset), to train the GVI-learning model. For the semantic segmenta-
tion  tasks  in  urban  scenarios,  a  number  of  datasets  with  annotations 
have  been  proposed.  COCO-Stuff  (Caesar  et  al.,  2018)  is  a  dataset 
including 164 K well-annotated images. Cityscapes (Cordts et al., 2016) 
is a benchmark dataset for the semantic understanding of urban street 
scenes. AED20K (Zhou et al., 2017), on the other hand, is a large-scale 
dataset for general scene understanding, and it consists of 150 seman-
tic categories (or classes) such as car, person, sky, and so forth. However,

training  datasets  in  use.  Until  now,  there  is  still  lack  of  high-quality 
dataset for GVI extraction.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The authors do not explicitly mention the specific methods used for collecting raw data in their deep learning pipeline. However, they reference several existing datasets that could potentially be used for training their models. These include COCO-Stuff, Cityscapes, and ADE20K. They also mention creating a new dataset called UGE-D specifically for GVI extraction. While the exact methodology for gathering these datasets isn't detailed here, typically such datasets can involve various techniques like using sensors or cameras for capturing images, manual labeling or annotation, and sometimes even crowdsourcing.
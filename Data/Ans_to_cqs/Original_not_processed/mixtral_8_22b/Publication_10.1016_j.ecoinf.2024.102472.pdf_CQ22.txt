Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During  the  MLPNN  training  phase,  initial  arbitrary  connection 
weights were assigned. Inputs were forward-fed from the input to the 
hidden  layer.  Hidden  neurons  multiplied  inputs  by  weights,  summed 
products,  and  processed  sums  through  a  transfer  function.  Results 
propagated  to  the  output  layer,  with  output  values  compared  to  ex-
pected  values  for  error  computation.  Iterative  error  back-propagation 
adjusted  connection  weights  until  reaching  a  target  minimal  error. 
The network then accurately estimated carbon stocks for both training 
and new input data without training data. This required tests to deter-
mine optimal learning rate (0.01), momentum (0.18), and training it-
erations  (500).  The  trained  network  was  subsequently  used  for  feed- 
forward predictions on continuous spatial data. 

2.5.2. Optimal predictor variable selection

adjusted to optimize the model (Jafarzadeh et al., 2021). Whereas node- 
size  determines  the  smallest  number  of  observations  in  a  tree  subset 
terminal node, and default value is always at 1 (Jafarzadeh et al., 2021; 
Pham et al., 2018). Gradient boosting iteration is a repetition process of 
increasing  or  decreasing  weights  in  a  training  datasets  (Huang  et  al., 
2022). All hyperparameters were tuned with a grid search method. 

2.5.1. Artificial neural networks

Furthermore,  Planetscope  MSI  texture  metrics  were  derived  using 

Table 2 
Prediction performance of reforestation carbon stock using XGBoost and ANN 
model  and  Planetscope's  spectral  information  separated  into  calibration  and 
validation datasets.  

Algorithm 
model 

XGB 

Prediction 
dataset 

Calibration 
Validation 
Calibration 
Validation 

Mean C (t. 
ha-1) 

30.42 
31.08 
31.33 
32.44 

R2 

0.81 
0.78 
0.77 
0.73 

RMSE (t. 
ha-1) 

MAE (t. 
ha-1) 

27.33 
29.75 
28.17 
30.44 

21.84 
24.66 
22.39 
24.43  

CDw = AGB Ã— 0.5

(2)  

ANN 

EcologicalInformatics80(2024)1024724C. Matiza et al.

layers. ANNs require a substantial amount of training data to achieve 
effective learning. Among ANN architectures, the multi-layer perceptron 
(MLP) neural network stands out as the most used.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To evaluate the performance of the deep learning models, such as the Multi-Layer Perceptron Neural Network (MLPNN) mentioned in the given context, several metrics can be employed. These include Mean Carbon Stock (Mean C), Coefficient of Determination (R^2), Root Mean Square Error (RMSE), and Mean Absolute Error (MAE).

Mean Carbon Stock (Mean C): This metric represents the average carbon stock predicted by the model. In this case, the mean carbon stock is expressed in tons per hectare (t ha^-1).

Coefficient of Determination (R^2): R^2 measures the proportion of variance in the dependent variable that can be explained by the independent variables in the model. It ranges between 0 and 1, where a higher value indicates better model fit.

Root Mean Square Error (RMSE): RMSE quantifies the difference between the actual and predicted values. Lower RMSE values indicate smaller differences between the observed and predicted values, suggesting better model performance.

Mean Absolute Error (MAE): MAE calculates the average absolute difference between the actual and predicted values. Similar to RMSE, lower MAE values suggest better model performance.

These metrics have been applied to assess the prediction performance of the XGBoost and ANN models in estimating reforestation carbon stock using Planetscope's spectral information. As shown in Table 2, these metrics provide insights into how well each algorithm performs when applied to both calibration and validation datasets.
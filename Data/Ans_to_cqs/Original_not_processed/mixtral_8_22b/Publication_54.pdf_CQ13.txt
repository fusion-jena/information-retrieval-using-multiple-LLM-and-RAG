Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

of 0.9.

• Exponential decay rate of second-moment estimation
of 0.999, and a positive scalar value for epsilon of 1e-08.
Furthermore, to avoid overﬁtting, ﬁvefold cross-validation
and early stopping were employed for efﬁcient hyperparam-
eter optimization. The early stopping criterion was based on
the model performance when the model stops improving on

VOLUME 9, 2021

102979

Y.-P. Huang, H. Basanta: Recognition of Endemic Bird Species Using Deep Learning Models

TABLE 1. Performance Comparison Between Models with and Without
Swapping of the Misclassified Data with the Training and Validation
Datasets.

precision, recall, and F1-score.

Precision =

Recall =

TP
TP + FP
TP
TP + FN

F1 − score = 2 ∗

Recall ∗ Precision
Recall + Precision

(3)

(4)

(5)

TABLE 2. Performance Before and After Swapping the Misclassified Data
with the Training and Validation Datasets.

The model is then iteratively trained and validated on these
different sets. The training set is used to ﬁt the parameters of
the classiﬁer, whereas the validation set provides an unbiased
evaluation of the model ﬁt on the training set while tuning
model hyperparameters, such as the network layer size, ﬁnd-
ing the optimal number of hidden units, and regularizing the
model. Models with few hyperparameters are easy to validate
and tune, but if the model has many hyperparameters, a larger
validation dataset is required. In some cases, the evaluation
is biased when a validation dataset is not incorporated into
the model conﬁguration because the model may perfectly ﬁt
the training data with high accuracy but may fail to ﬁt the
test or validation data. In such cases, k-fold cross-validation,
which is a resampling technique, is used to avoid overﬁtting;
the training set is generated with different combinations of k
groups (say 5 or 10, depending on the size of the dataset) as

102976

VOLUME 9, 2021

Y.-P. Huang, H. Basanta: Recognition of Endemic Bird Species Using Deep Learning Models

IV. EXPERIMENTAL SETTINGS
To extract the relevant features of birds, we randomly split
the endemic bird data into 3,132 images for training and
760 images for testing. To allow the deep networks to con-
verge and improve the ability of the model to generalize
adeptly, we augmented the training dataset 10 times, resulting
in a total of 31,320 images. Then, 25,056 (80%) images were
used for training and 6,264 (20%) were used for validation.
The Inception-ResNet-v2 model was trained using the mini-
batch gradient descent algorithm with the batch size set to
32 and the learning rate maintained at 0.0001. The Adam
optimizer was used to ﬁne-tune the model with the following
parameters:

• Exponential decay rate of ﬁrst-moment estimation

of 0.9.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning model used in this study employs several hyperparameters for efficient optimization. The exponential decay rates of first-moment and second-moment estimations are set to 0.9 and 0.999, respectively. A positive scalar value for epsilon is also defined as 1e-08. Furthermore, fivefold cross-validation and early stopping techniques are utilized to prevent overfitting during hyperparameter optimization. The early stopping criterion is based on the model's performance when it ceases to show improvement.

In terms of specific hyperparameters related to the training process, the model uses the mini-batch gradient descent algorithm with a batch size of 32. The learning rate is kept constant at 0.0001 throughout the training phase. Additionally, the Adam optimizer is applied to fine-tune the model, incorporating the previously mentioned exponential decay rates and epsilon values.
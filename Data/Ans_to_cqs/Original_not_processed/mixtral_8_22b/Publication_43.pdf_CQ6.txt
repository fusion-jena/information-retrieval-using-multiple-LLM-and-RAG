Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Deep CNNs, such as ResNet and GoogLeNet, are generally trained based on the prediction loss
minimization. Let x and y be the input images and corresponding output class labels, the objective of
the training is to iteratively minimize the average loss deﬁned as

J(w) =

1
N

N
∑
i=1

L( f (w; xi), yi) + λR(w)

(1)

This loss function measures how different is the output of the ﬁnal layer from the ground truth.
N is the number of data instances (mini-batch) in every iteration, L is the loss function, f is the
predicted output of the network depending on the current weights w, and R is the weight decay with
the Lagrange multiplier λ. It is worth mentioning that in the case of GoogLeNet, the losses of the

Remote Sens. 2017, 9, 1220

6 of 22

two auxiliary classiﬁers are weighted by 0.3 and added to the total loss of each training iteration.
The Stochastic Gradient Descent (SGD) is commonly used to update the weights.

wt+1 = µwt − α∆J(wt)

(2)

In general, ﬁne-tuning the entire network (i.e., updating all the weights) is only used when the
new dataset is large enough, otherwise, the model could suffer overﬁtting especially among the
ﬁrst layers of the network. Since these layers extract low-level features, e.g., edges and color,
they do not change signiﬁcantly and can be utilized for several visual recognition tasks. The last
learnable layers of the CNN are gradually adjusted to the particularities of the problem and
extract high level features.

In this work, we have used ﬁne-tuning on ResNet and GoogleNet. We initialized the used CNNs
with the pre-trained weights of the same architectures on ImageNet dataset (around 1.28 million
images over 1000 generic object classes) ([13]).

wt+1 = µwt − α∆J(wt)

(2)

where µ is the momentum weight for the current weights wt and α is the learning rate.

The network weights, wt, can be randomly initialized if the network is trained from scratch.
However, this is suitable only when a large labeled training-set is available, which is expensive in
practice. Several previous studies have shown that data-augmentation ([44]) and transfer learning ([45])
help overcoming this limitation.

•

This section evaluates the performance of GoogLeNet- and ResNet-based classiﬁers under the
detection-proposal pre-processing technique. To optimize the CNN-detection accuracy and execution
time, we analyzed several pre-processing techniques to generate better and faster candidate regions
than with the sliding window approach. The selection of the set of pre-processing techniques that
provides the best results depends on the nature of the problem and the object of interest. From the
multiple techniques explored, the ones that improved the performance of the CNN detectors in this
work were: (i) Eliminating the background using a threshold based on the high albedo (light color)

Remote Sens. 2017, 9, 1220

14 of 22

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model like ResNet or GoogLeNet, various preprocessing steps may be applied to the input data. These steps aim to improve the quality of the data and enhance the model's ability to learn meaningful patterns. Some common preprocessing techniques include:

1. Normalization: This involves rescaling the input values so they fall within a specific range, typically between 0 and 1 or -1 and 1. Normalizing the input data helps ensure that all features contribute equally to the learning process, preventing any single feature from dominating due to its scale.

2. Scaling: Similar to normalization, scaling adjusts the range of input values but does not necessarily rescale them to a fixed range. For example, one might use standardization, where input values are transformed into z-scores with a mean of 0 and a standard deviation of 1. Scaling can help stabilize the learning process and reduce the impact of outliers.

3. Cleaning: Data cleaning involves identifying and handling missing, incorrect, or inconsistent data points. Removing or correcting these issues can prevent errors during training and improve the overall performance of the model.

4. Augmentation: Data augmentation generates new samples by applying transformations to existing data, such as rotations, translations, or flips. Augmentation can increase the size of the training dataset and introduce more variability, helping the model generalize better to unseen examples.

5. Encoding: In cases where categorical variables are present, encoding converts these variables into numerical representations that the model can understand. Common encoding methods include label encoding and one-hot encoding.

While some preprocessing steps are universally applicable, others depend on the specific characteristics of the dataset and the task at hand. Therefore, it is essential to carefully consider the appropriate preprocessing techniques for each application.
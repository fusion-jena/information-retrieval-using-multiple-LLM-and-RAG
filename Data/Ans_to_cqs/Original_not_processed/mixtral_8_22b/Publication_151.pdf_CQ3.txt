Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Embedding size. The Layer and Rhodes (2017) (LR)
formulation requires n
1 dimensions, which introduces
some challenges. According to the theory, the number of
dimensions needs to increase by one after inserting the
query. Our supervised learning formulation does not
allow that (the embedding size is ﬁxed after training).
Thus, there is no guarantee that the embeddings remain
correct after addition, even if they are before addition.
However, we note that, in LR embeddings, adding a
leaf would require simply dividing one of the n
1
dimensions into two dimensions, leaving the rest of the
embeddings intact. Thus, one can hope that having one
less dimension has a minimal practical impact. More
broadly, for large n, training models with n-dimensional
embedding is impractical. Thus, we often set k < n
1,
and the gap can be more than an order of magnitude for
some of our tests described below. In practice, we use a
rule of thumb to select the default k (which the user can

{

=

=

n
i

n
i

Di}
{

DR
i }

1 and

Placement.—Once the CNN model is trained, we use it to
map a given query sequence q to a vector of distances
D1 ...Dn. For data sets with missing data (gaps), we
compute two sets of distances,
1,
using the models with and without gap reconstruction,
respectively. The ﬁnal distances is set to the weighted
i , where �
sum of the distances, that is, (1
is the proportion of the sites with gaps in the query
sequences. The weighted sum is used to reduce the
impact of reconstructed bases (which are guessed, as
opposed to being observed) on the ﬁnal distance and
will be empirically tested. Given these distances, we then
place q onto T using distance-based placement (Balaban
et al. 2020), which uses dynamic programming to ﬁnd the
dqi(T))2,
placement with the minimum
where dqi(T) represents the tree-based distance between
(cid:30)
the query and each taxon i (Fig. 1).

�)Di +

(Di −

1 D−
i

�DR

−

n
i

=

2

Evaluation on Simulated Data Sets
DEPP training and parameter sensitivity.—We start by
evaluating DEPP on simulated data sets, testing the
ability to train the CNN model in reasonable times. As
the training epochs advance, the loss function (1) drops
rapidly and stabilizes after around 500 epochs in a typical
case (Fig. S1 of the Supplementary material available on
Dryad). Here, training, which is a one-time process for
each reference tree, ﬁnished in around 20 min for the 200-
taxon data set and 260 minutes for 10,000-taxon data set,
on a machine with one 2080Ti NVIDIA GPU and 8 CPU
cores. Placement of 1000 queries took 4 seconds for the
200-taxon and 30 s for the 10,000-taxon data sets using a
single CPU core. On the small 200-taxon data set, EPA-ng
has an advantage in terms of running time. However, in
the larger HGT data set (10,000-taxon), DEPP placements
are faster than the alternatives with half the running time
of EPA-ng. In terms of the memory usage, APPLES+JC

3
19

semisupervised approach with two steps: for every four
species (a quartet), classify the input data to one of the
three possible quartet topologies, then combine these
quartet trees. This formulation raises a question: where
are we to ﬁnd labeled training data in the high volume
needed by DNNs? Both papers turn to simulations for
an answer: use complex models to simulate data on
known trees, from which we then train the model. The
point of these methods is to use complex generative
models that can be sampled but do not avail themselves
to scalable inference. However, learning from simulated
data runs the risk of missing relevant parts of the
huge parameter space and model misspeciﬁcation. As
Zaharias et al. (2022) recently showed, these methods can
have lower accuracy than standard methods in careful
benchmarking.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not directly discuss specific data annotation techniques such as bounding box annotation or instance segmentation. Instead, it focuses on aspects related to phylogenetic analysis using deep learning approaches like Convolutional Neural Networks (CNNs).

In this context, the authors mention using simulated data sets for training their CNN model. They generate these datasets using complex models to sample data on known trees, which are then used to train the model. This methodology is referred to as a semisupervised approach, involving classification of input data into three possible quartet topologies and combining them to create the final output.

While the text does not explicitly state any traditional data annotation techniques, it highlights the use of simulation-based data generation for training deep learning models in the field of phylogenetics. It also mentions potential issues with this approach, such as missing relevant parts of the huge parameter space and model misspecification.
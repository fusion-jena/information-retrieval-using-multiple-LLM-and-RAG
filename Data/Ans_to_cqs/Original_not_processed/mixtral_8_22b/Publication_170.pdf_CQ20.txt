Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Experiments and Platform
To study the performance of the model on each training set
when using diﬀerent training modes and computing plat-
forms, we conducted three experiments. The ﬁrst used the
zero‐start training mode to train the models on a GPU
server with the datasets Traini (i = 1, 2, …, 7). The second
used the incremental training mode to train the models on
the GPU server with Traini (i = 1, 2, …, 7). The third used
the incremental training mode to train model on a desktop
PC with Traini (i = 1, 2, …, 7).
We used two computing platforms in our study (Table 2).
On a Lenovo desktop computer with Intel Core i7‐7700

GPU Server

Desktop PC

Speciﬁcation Dell PowerEdge C4130 Rack Servers
CPU
RAM
GPU

Two Intel Xeon E5‐2600 v4 processors
256 GB
Two NVIDIA Tesla K80 with 24 GB GDDR5 Graphics Card One NVIDIA GeForce GT 730 with 2 GB GDDR5 Graphics Card

Lenovo desktop ThinkCentre
Intel(R) Core(TM) i7‐7700 3.6 GHz
32 GB

At present, few researchers focus on time costs of model
training and the required computing platform. Yousif et al.
(2019) stated that they optimized their model performance,
but model training still failed to complete on a common PC.
In terms of recognition rate, their rate on a 2.6 GHz CPU
computer was 100 frames/minute, which is lower than our
recognition rate (260 images/minute). One possible reason
is that their preprocessing was more complex.

MANAGEMENT IMPLICATIONS
Our method can greatly reduce time and personnel costs in
3 ways. First, using our method to automatically ﬁlter empty
images can greatly save labor and time, and the percentage
of the labor and time savings is the same as the proportion

Yang et al. (cid:129) An Adaptive Approach for Empty Images Filtering

233

 23285540, 2021, 2, Downloaded from https://wildlife.onlinelibrary.wiley.com/doi/10.1002/wsb.1176 by Vamsi Krishna Kommineni - Friedrich-Schiller-Universität , Wiley Online Library on [29/08/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License3.6 GHz central processing unit (CPU) and 32 GB of
memory, we used the incremental training mode to train
and test the DCNN model. On another Dell PowerEdge
C4130 rack server with two Tesla K80 GPUs and 256 GB
of memory, we used the zero‐start and the incremental
training modes to train and test models, respectively.

RESULTS
The incremental training mode performed consistently with
the zero‐start training mode when the training sample size

Model Evaluation
Model performance evaluation included both errors and
time costs evaluation. The iteration termination condition
of the model was the same in each round training (i.e., the
loss rate of the model was less than a preset threshold ɛ
−4]). We used the same test set (Test) to
[here ɛ = 1.0 × 10
evaluate the performance of models in diﬀerent training
phases. The model error evaluation used three metrics that
included overall error, commission error, and omission
error, which were deﬁned by equations (2), (3) and (4).

Table 2. Main conﬁguration parameters of the two platforms used.

)

(2)

(3)

(4)

Commission error

=

/(
FP TP

+

FP

)

Omission error

=

/(
FN TP

+

FN

)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Answer: Two different hardware resources are used for training the deep learning model. For the first experiment, a GPU server with two Intel Xeon E5-2600 v4 processors, 256 GB RAM, and two NVIDIA Tesla K80 with 24 GB GDDR5 Graphics Card is utilized. In contrast, the third experiment uses a desktop PC with one NVIDIA GeForce GT 730 with 2 GB GDDR5 Graphics Card. Both these hardware resources employ GPUs for training the deep learning model.
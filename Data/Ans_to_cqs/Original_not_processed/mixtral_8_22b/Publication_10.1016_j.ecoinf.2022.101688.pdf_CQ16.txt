Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1 
Pre-processing hyper-parameters for each dataset and the number of testing files 
used.   

Lemurs 

Alethe 

Gibbons 

Whydah 

Low pass filter cut off 
Downsampling rate 
Nyquist rate 
Segment duration 
Number of testing files 
Testing time (min) 

4000 
9600 
4800 
4 
46 
1840 

3100 
6400 
3200 
2 
27 
810 

2000 
4800 
2400 
4 
22 
1300 

9000 
18400 
9200 
3 
78 
1560

last  few)  classification  layers  that  have  been  trained  for  a  specific 
problem and replacing these with new one(s) that can be retrained for 
the new task (based on the number of categories to be classified within 
target  task  W).  Weights  for  the  new  layers  must  be  learned  “from 
scratch”, but weights for the feature extraction layers can either be set 
(“frozen”) to their existing (“pre-trained”) values, or can be retrained 
using the pre-trained values as starting values, a process known as “fine 
tuning”  because,  if  the  same  visual  features  are  relevant  to  the  new 
problem, then weights will not change very much from their pre-trained 
values. The benefit of freezing the feature extraction layers is compu-
tational – there are often far fewer weights that require optimisation – 
which is particularly useful if the target task must be addressed with a 
relatively small dataset because the risk of overfitting without transfer

3. Materials and methods 

3.1. Data collection 

In this study we used four datasets, containing different species, that 
were  collected  and  provided  by  various  researchers.  Several  experi-
ments, defined in the next section, were conducted to assess the use of 
transfer learning in the process of creating binary classification models. 
In each case PAM devices were set to record for a number of hours across 
multiple days, however the start time and duration of each recording 
differed for each study. Details for each dataset are listed below.

Table 4 
Comparison of the average F1 score across the different network architectures and dataset configurations. The exponent approach was used for the spectrogram input. 
The feature extracted was frozen. The results are averaged across 13 unique executions. The results are ordered (highest to lowed) based on the average of each 
network architecture across all configurations. The best three performing network architectures on a particular dataset configuration is highlighted in bold.  

Method 

ResNet101V2 
ResNet152V2 
InceptionResNetV2 
ResNet50V2 
DenseNet169 
DenseNet201 
VGG16 
DenseNet121 
InceptionV3 
ResNet101 
Xception 
MobileNetV2 

G 25 

95.30 
95.18 
94.70 
94.97 
94.92 
94.84 
97.26 
94.58 
92.22 
96.17 
93.88 
94.62 

G 50 

97.40 
96.92 
96.75 
97.04 
96.95 
96.72 
98.09 
96.69 
95.42 
97.49 
95.79 
96.65 

G 100 

96.27 
96.58 
96.57 
95.13 
95.69 
95.86 
94.99 
95.00 
95.40 
94.23 
95.50 
91.65 

L 25

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, it does not explicitly state the criteria used to determine when training is complete. However, common practices in machine learning include monitoring metrics such as validation loss, accuracy, or other performance indicators during the training process. When these metrics reach a plateau or stop improving significantly, it may indicate that the model has converged and further training would not yield substantial improvements. This concept is often referred to as early stopping, which helps prevent overfitting and saves computational resources. In summary, while the exact criteria are not specified in the provided context, typical approaches involve observing validation metrics and employing techniques like early stopping to decide when training should conclude.
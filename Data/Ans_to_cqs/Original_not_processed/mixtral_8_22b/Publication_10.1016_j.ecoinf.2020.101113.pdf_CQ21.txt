Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.3.  Model training 

2.3.1.  Training data preprocessing 

The  CNN  model  used  in  this  study  requires  equally  sized  input 
images. We chose a time-frequency input window size of 2 s as it is near 
the mean and median template duration across target call types (Fig. 2). 
Most  call  types  have  a  duration  below  1  s,  and  for  those  above  2  s,

and  stretching  of  target  signals,  and  adding  Gaussian  noise  or  noise 
from soundscapes (Koh et al., 2019; Lasseck, 2019). This competition's 
training  data  consisted  primarily  of  recordings  targeting  single  fore-
ground species, but also a smaller validation set of annotated sounds-
capes  like  those  in  the  test  set.  Results  from  this  challenge  showed  a 
significant  increase  in  performance  for  submissions  that  incorporated 
the validation soundscape data in training (Kahl et al., 2019). Specifi-
cally,  Lasseck  (2019)  found  that  adding  background  noise  from  the 
validation  soundscapes  to  training  samples  significantly  improved 
performance  on  the  test  soundscapes.  This  indicates  that  acoustic 
monitoring  systems  can  benefit  significantly  from  location-specific 
training data.

other  sites  and  older  years  were  searched  to  increase  the  training 
sample  size  for  certain  species.  78%  of  recordings  were  collected  in 
2019, 11% in 2018, and 11% collected between 2015 and 2018. Most 
recordings were collected in the months March and April, a period of 
high acoustic activity.

causes nodes in the previous FC layer to be probabilistically ignored in 
each  training  iteration,  such  that  a  different  subset  of  nodes  is  con-
nected to the final FC layer at each iteration. This emulates training a 
group  of  different  models  and  reduces  the  chance  of  overfitting.  The 
first  FC  layer  consisted  of  512  nodes  and  used  the  common  “ReLU” 
activation  function,  which  simply  converts  negative  inputs  to  0.  The 
following drop-out layer was assigned a drop-out rate of 0.5, such that 
each node was ignored with a 50% probability. The final layer consists 
of 24 nodes, corresponding to the number of target species, and each 
node was assigned a sigmoid activation function. The sigmoid function 
S, defined below, maps the input to the range [0,1], and is commonly 
used for the prediction of binary outcomes. In our case, an independent 
output  score  within  [0,  1]  for  each  species  was  desired,  to  allow  for

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not provide information about the postprocessing steps involved after the model training. Therefore, I cannot provide a helpful answer based solely on the provided text. However, some common postprocessing steps include calculating metrics such as accuracy, precision, recall, F1-score, etc., creating confusion matrices, generating saliency maps, and performing other forms of analysis to evaluate the model's performance and interpret its predictions.
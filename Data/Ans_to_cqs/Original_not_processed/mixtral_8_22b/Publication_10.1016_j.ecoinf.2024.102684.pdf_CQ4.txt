Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

after evaluation, we selected the following hyperparameter: rectified 
linear  unit  activation  function  (ReLU),  RMSprop  optimizer,  and  a 
learning rate of 0.001. 

We employed mean squared error as the loss function for the training 
process.  To  prevent  overfitting,  we  implemented  callbacks  to  stop 
training  when  the  validation  error  diverged  from  the  training  error 
(early stopping technique), saving the model with the best performance. 
Furthermore,  data  augmentation  techniques  were  applied  to  increase 
the number and variability of the training data by applying modifica-
tions to the original images, such as rotation, horizontal flip, vertical flip, 
width shift range, height shift range, shear range, zoom range, and brightness 
range.

remote sensing semantic segmentation with transformer. Remote Sens. 14, 4942. 

Liu, Y., Starzyk, J.A., Zhu, Z., 2008. Optimized approximation algorithm in neural 

networks without overfitting. IEEE Trans. Neural Netw. 19, 983–995. 

Lopatin, J., Galleguillos, M., Fassnacht, F.E., Ceballos, A., Hern´andez, J., 2015. Using a 
multistructural object-based LiDAR approach to estimate vascular plant richness in 
Mediterranean forests with complex structure. IEEE Geosci. Remote Sens. Lett. 12, 
1008–1012. 

Lopatin, J., Dolos, K., Hern´andez, H.J., Galleguillos, M., Fassnacht, F.E., 2016. 

Comparing generalized linear models and random forest to model vascular plant 
species richness using LiDAR data in a natural forest in Central Chile. Remote Sens. 
Environ. 173, 200–210. 

Lopatin, J., Dolos, K., Kattenborn, T., Fassnacht, F., 2019a. How canopy shadow affects 
invasive plant species classification in high spatial resolution remote sensing. 
Remote Sens. Ecol. Conserv. 5, 302–317.

The dataset was divided into training (60%), validation (20%), and 
testing (20%) subsets for model evaluation and hyperparameter search 
(Fig. 3 (b)). This approach was chosen to obtain an unbiased estimation 
of  error.  Data  augmentation  was  only  applied  to  the  training  data  to 
avoid overlapping the same data in different partitions. To evaluate the 
models we used the coefficients of determination (R2  - calculated as the 
squared  Pearson's  correlation  coefficient),  which  measures  the  per-
centage of variation in the response variable explained by the model. 
Additionally,  we  used  the  root  mean  squared  error  (RMSE)  metric, 
where  RMSE =
,  with  Yi = the  actual  values  pine 
̂
Y i = the predicted pine cove values. In our work, RMSE 
cover values and 
values ranged from 0.0 to 1.0, with values closer to 0 indicating better 
model  predictions.  The  overfitting  was  assessed  by  comparing  model

of the models. For instance, Mil`a et al. (2022) used the Nearest Neighbor 
Distance Matching (NNDM) LOO CV method to delineate a geographic 
space  in  which  predictions  can  be  made.  Many  of  these  approaches 
accurately  demonstrate  the  model's  transfer  learning  capabilities,  yet 
they  impose  limitations  on  the  training  domain,  leading  to  reduced 
training  heterogeneity  (Meyer  and  Pebesma,  2021).  However,  con-
ducting large CV experiments with deep learning-based models is often 
infeasible due to  their high computational costs. Deep  learning-based 
models  typically  employ  simple  data  partitioning  strategies,  such  as 
training, validation, and testing (Ocer et al., 2020). This simple scheme 
can help to add heterogeneous information to the training procedure, 
but may also overlook the diversity of heterogeneous spatial information 
in remotely sensed data that is aligned with different learning domains

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In this study, several data augmentation techniques were applied to increase the number and variability of the training data. These modifications included rotation, horizontal flip, vertical flip, width shift range, height shift range, shear range, zoom range, and brightness range. Each of these techniques contributes to enhancing the robustness and generalization capability of the deep learning model. Rotation involves rotating the image by a certain angle, while flipping refers to mirroring the image horizontally or vertically. Width and height shift ranges involve shifting the image along the x and y axes, respectively. Shearing distorts the image along one axis, creating a slanted effect. Zooming scales the image up or down, and adjusting the brightness range changes the overall lightness or darkness of the image. By implementing these data augmentation techniques, the researchers aimed to improve the model's ability to recognize patterns and features in various orientations, sizes, and lighting conditions.
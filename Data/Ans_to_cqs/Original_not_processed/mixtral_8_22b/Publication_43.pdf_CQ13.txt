Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Deep CNNs, such as ResNet and GoogLeNet, are generally trained based on the prediction loss
minimization. Let x and y be the input images and corresponding output class labels, the objective of
the training is to iteratively minimize the average loss deﬁned as

J(w) =

1
N

N
∑
i=1

L( f (w; xi), yi) + λR(w)

(1)

This loss function measures how different is the output of the ﬁnal layer from the ground truth.
N is the number of data instances (mini-batch) in every iteration, L is the loss function, f is the
predicted output of the network depending on the current weights w, and R is the weight decay with
the Lagrange multiplier λ. It is worth mentioning that in the case of GoogLeNet, the losses of the

Remote Sens. 2017, 9, 1220

6 of 22

two auxiliary classiﬁers are weighted by 0.3 and added to the total loss of each training iteration.
The Stochastic Gradient Descent (SGD) is commonly used to update the weights.

wt+1 = µwt − α∆J(wt)

(2)

In general, ﬁne-tuning the entire network (i.e., updating all the weights) is only used when the
new dataset is large enough, otherwise, the model could suffer overﬁtting especially among the
ﬁrst layers of the network. Since these layers extract low-level features, e.g., edges and color,
they do not change signiﬁcantly and can be utilized for several visual recognition tasks. The last
learnable layers of the CNN are gradually adjusted to the particularities of the problem and
extract high level features.

In this work, we have used ﬁne-tuning on ResNet and GoogleNet. We initialized the used CNNs
with the pre-trained weights of the same architectures on ImageNet dataset (around 1.28 million
images over 1000 generic object classes) ([13]).

wt+1 = µwt − α∆J(wt)

(2)

where µ is the momentum weight for the current weights wt and α is the learning rate.

The network weights, wt, can be randomly initialized if the network is trained from scratch.
However, this is suitable only when a large labeled training-set is available, which is expensive in
practice. Several previous studies have shown that data-augmentation ([44]) and transfer learning ([45])
help overcoming this limitation.

•

,

recall =

True Positives
True Positives + False Negatives

,

F1 measure = 2 ×

precision × recall
precision + recall

and

5.1. Finding the Best CNN-Based Detector

For the experiments with GoogLeNet and ResNet-based models, we have used the open source
software library Tensorﬂow ([18]). For training CNNs, the image patches are resized from 80 × 80-pixels
to 299 × 299 by GoogLeNet and to 224 × 224 by ResNet. Such rescaling is due to the fact that the
architecture of all the layers of GoogLeNet and ResNet are adapted according to these input sizes,
independently from the original resolution of the input images.

5.1.1. CNN Training With Fine-Tuning and Data-Augumentation

To improve the accuracy and reduce overfitting we (i) used fine-tuning by initializing the evaluated
models with the pre-trained weights of ImageNet, and (ii) applied data augmentation techniques to
increase the size of the dataset from 100 to 6000 images. In particular, for data-augmentation we applied:

•
•
•
•

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The deep learning models used in this study are Deep Convolutional Neural Networks (CNNs) such as ResNet and GoogLeNet. These networks were trained using stochastic gradient descent (SGD) optimization algorithm. The hyperparameters used in the SGD include the learning rate (α) and the momentum weight (µ). However, the specific values of these hyperparameters are not mentioned in the given context. Additionally, the loss function used for training includes the weight decay term (R) with its Lagrange multiplier (λ). Again, the exact value of λ is not specified in the given context. Furthermore, it is also mentioned that the losses of two auxiliary classifiers in GoogLeNet are weighted by 0.3 and added to the total loss during each training iteration. Lastly, the concept of fine-tuning is discussed where the entire network is updated or only certain parts of the network are updated based on the size of the new dataset. This process helps prevent overfitting, especially in the first layers of the network.
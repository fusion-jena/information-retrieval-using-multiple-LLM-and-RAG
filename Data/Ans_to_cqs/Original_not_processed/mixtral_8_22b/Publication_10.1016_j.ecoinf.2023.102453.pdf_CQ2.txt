Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Vision Transformers (ViT) (Dosovitskiy et al., 2021) are based on an 
alternative attention-based model design rather than CNN. The trans-
former  architecture was first applied on Natural Language Processing 
(NLP)  problems.  The  recent  ground-breaking  research  outputs  of 
Generative  Pre-trained  Transformer  (GPT)  and  Bidirectional  Encoder 
Representations from Transformers (BERT) are all based on the trans-
former architecture. The concepts of attention were later applied to the 
image classification problems as Vision Transformers (Dosovitskiy et al., 
2021). The transformers use the relationship between words, termed as 
attention, whereas the vision transformers determine relationships be-
tween patches of images. ViT have since achieved performance similar 
to convolution-based architectures. There are many vision transformer 
architectures for image classification but we chose MobileViT (Mehta 
and Rastegari, 2022) models proposed as a small model for embedded

Neural architecture search (NAS) was used for developing the Effi-
cientNetV2B0 model, and for the optimization of parameter efficiency 
and  training  speed  (Tan  and  Le,  2021).  The  model  performance  was 
improved  using  progressive  learning  to  adapt  regularization  to  the 
image size (Tan and Le, 2021). EfficientNetV2 is a family of models (B0 
to B7), and B0 is the smallest variant in the family (Keras, n.d), with the 
other  models  progressively  increasing  in  performance  going  up  to 
EfficientNetV2B7.

For model deployment to an edge device, e.g., RPi, the model needs 
to be converted to the TensorFlow Lite format, which although reduces 
its size, but also reduces the modelâ€™s accuracy. The model can then be 
used on an edge device for classification of as yet unseen images. The 
TensorFlow or TensorFlow Lite model may also optionally be optimized 
for size, or latency depending on the application requirements and the 
edge device specifications. The RPi or similar edge devices can also be 
used to  run XAI  techniques such as  Grad-CAM  for generating  visuali-
zations through heatmaps. 

3.3. Transfer learning approaches for image classification

3.2. Process flow 

This study has two perspectives, model training and development on 
a high-end computer, and the model deployment to an edge device, as 
shown in Fig. 3. The training dataset could be pre-processed, such as 
denoising,  contrast  enhancement,  and  image  resizing.  We  only  used 
image re-sizing to make the images suitable for the input image reso-
lution required for the selected models. 

After a model has been trained, it needs to be validated and tested to 
confirm that it has achieved the desired performance. The performance 
might  be  measured  using  different  metrics,  such  as  classification 

Table 2 
The class distribution in the Extinction dataset.  

Class 

Number of images

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, the data format used in this deep learning pipeline is images. This is evident from several mentions of images throughout the text, including references to image classification, image resizing, and the use of image datasets. However, there is no mention of other data formats like audio, video, or CSV files being used in this particular pipeline. Therefore, it can be concluded that the primary data format used in this deep learning pipeline is images.
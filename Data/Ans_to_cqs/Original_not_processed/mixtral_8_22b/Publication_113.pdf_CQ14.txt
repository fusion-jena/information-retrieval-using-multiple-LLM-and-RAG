Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

|  159

because eventually both methods have sufficient constraints to learn 

active  learning  pipeline  on  the  crops  produced  from  running  the 

a good embedding.

3.3.2 | Active learning strategies

MegaDetector  model  over  the  NACTI  dataset.  We  employed  mar-

gin-based active learning. After the first 30,000 active queries, the 

classifier  achieves  93.2%  overall  accuracy  which  further  confirms 

the usefulness of the suggested pipeline. More detailed results are 

available in Table S3.

Different strategies can be employed to select samples to be labelled 

by the oracle. The most naive strategy is selecting queries at random. 

Here  we  try  five  different  query  selection  strategies  and  compare 

4 |  D I S CU S S I O N

them against a control of selecting samples at random. In particular, 

we try model uncertainty criteria (confidence, margin, entropy; Lewis 

This  paper  demonstrates  the  potential  to  significantly  reduce

Xu, Z., Yu, K., Tresp, V., Xu, X., & Wang, J. (2003). Representative sampling 
for  text  classification  using  support  vector  machines.  In  European 
conference on information retrieval (pp. 393–407). Springer.

Wiley & Sons.

Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learn-
ing with neural networks. In Advances in neural information processing 
systems (pp. 3104–3112).

Swanson, A., Kosmala, M., Lintott, C., Simpson, R., Smith, A., & Packer, 
C.  (2015).  Snapshot  serengeti,  high-frequency  annotated  camera 
trap images of 40 mammalian species in an african savanna. Scientific 
Data, 2, 150026.

1.  Every  deep  learning  algorithm  has  numerous  hyperparameters, 

subset of unlabelled samples such that the loss value of the selected 

options  selected  by  the  data  scientist  before  machine  learning 

subset  is  close  to  the  ‘expected’  loss  value  of  the  remaining  data 

begins.  For  this  paper,  we  used  well-known  values  of  hyperpa-

points (Sener & Savarese, 2017). At 14,000 labels, we match the ac-

rameters  to  train  our  models.  Tuning  hyperparameters  is  likely 

curacy of Norouzzadeh et al. for the same architecture; compared to 

to  improve  results.  In  particular,  we  only  used  the  ResNet-50 

the 3.2 million labelled images they trained with, our results represent 

architecture  for  embedding  and  a  simple  two-layer  architecture 

over a 99.5% reduction in labelling effort to achieve the same results.

for  classification.  Further  probing  of  the  architecture  space 

may  improve  results.

set of images to have labelled.

derlying  machine  learning  model  and  asking  the  oracle  for some 

labels, while trying to minimize the number of such requests. The 

active  learning  algorithm  attempts  to  optimally  select  the  most 

1.6 | Embedding learning

informative samples from the pool of unlabelled samples for the 

oracle  to  label  so  that  the  underlying  machine  learning  model 

An embedding function maps data from a high-dimensional space to a 

can  quickly  learn  the  requested  task.  Thus,  active  learning  can 

lower-dimensional  space,  for  example  from  the  millions  of  pixel val-

minimize the human labour required to label a large collection of 

ues in an image (high-dimensional) to a vector of dozens or hundreds 

unlabelled data.

of numeric values. Many dimensionality reduction algorithms such as 

Active learning algorithms maintain an underlying machine learn-

PCA (Martínez & Kak, 2001) and LDA (Martínez & Kak, 2001), or visu-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper does not provide explicit details about how the hyperparameters were optimized. However, it mentions that well-known values of hyperparameters were used to train the models. This suggests that the authors might have relied on established best practices or previous research findings rather than conducting their own optimization process like grid search or random search. It also states that tuning hyperparameters is likely to improve results, implying that there may still be room for further optimization.
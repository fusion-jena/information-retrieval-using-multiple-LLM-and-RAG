Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We used an Intel® Core™ i7-1165G7 CPU, and 32 GB of RAM for all
computations. The pre-processing, especially the computation of the
embeddings, requires approximately 100 h of CPU time for all three
datasets for all embedding models and layers. The active learning ex-
periments require approximately 6 h of CPU time per random seed for all
three datasets. Table 1 shows the CPU time required for all sampling
strategies, normalized to random sampling.

3. Results

An annotated PAM dataset typically serves one of two primary pur-
poses: as a resource for training new machine learning models for later
deployment for inference in a related domain (e.g., geographical region,
taxa), or as an end product in itself for subsequent analysis of ecological
phenomena within the same domain. In this study, we explore the po-
tential of combining transfer learning and active learning to accelerate
the annotation of species-level sound events in PAM datasets for both
purposes.

3.1. Transfer learning

4 https://github.com/HKathman/pam_annotation_experiments
5 https://github.com/kahst/BirdNET-Analyzer/tree/main/checkpoints/V2.4
6 https://tfhub.dev/google/vggish/1
7 https://tfhub.dev/google/yamnet/1
8 tensorflow.keras.applications.vgg16.VGG16(weights=’imagenet’).
9 tensorflow.keras.applications.resnet_v2.ResNet152V2

(weights=’imagenet’).

Fig. 4. UMAP plots for different embedding layers of different embedding
models for AnuraSet. For UMAP generation, we randomly select 5000 samples
and discard all samples that are aligned to more than one class. Colors and
shapes indicate the 10 classes with the highest occurrence frequency. Layers are
numbered according to their distance from the classification layer, e.g. ‘Bird-
Net-1’ is the last layer before the classification layer of the BirdNet model.

EcologicalInformatics82(2024)1027105H. Kath et al.

evaluation was done on focal and citizen-science datasets. Swaminathan
et al. (2024) extend the observation to attention-based architectures pre-
trained on human speech. Lauha et al. (2022) show that transfer
learning can be helpful also for small networks trained from scratch on
small, targeted datasets gathered from online resources such as Mac-
aulay Library2 (as opposed to foundation models trained on large
datasets). While the works cited above evaluate transfer learning models
based on classification performance, McGinn et al. (2023) take a
different approach and investigate the topology of fine grained, sub-
species sound events in the embedding space afforded by BirdNet;
they find that different call types of a same species (e.g., drumming
versus vocalization) form distinct clusters, and that the vicinity of each

EcologicalInformatics82(2024)1027102H. Kath et al.

Fig. 1. Workflow for annotating passive acoustic monitoring datasets,
comparing the conventional approach (left) with the proposed approach (right).
We compare the transfer learning models BirdNet, VGGish, YAMNet, VGG16
and ResNet152-V2 for generating data representations. As active learning
method we select uncertainty methods, diversity methods, adaptive methods
and combinations of those.

different accuracy metrics (namely precision and recall) and discuss
their practical implications.

2. Methods

2.1. Datasets

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide information about which specific hardware resources were used for training the deep learning model, such as GPUs or TPUs. However, it mentions that an Intel® Core™ i7-1165G7 CPU and 32 GB of RAM were used for computations. This suggests that the computational tasks might have been performed using CPUs rather than specialized hardware like GPUs or TPUs.
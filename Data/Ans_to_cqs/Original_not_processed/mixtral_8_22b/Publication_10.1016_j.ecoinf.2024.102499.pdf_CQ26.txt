Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Oomen, R.A., Rasmussen, J.H., Sørdalen, T.K., Thorbjørnsen, S.H., 2022. Unlocking 
the potential of deep learning for marine ecology: overview, applications, and 
outlook. ICES J. Mar. Sci. 79 (2), 319–336. https://doi.org/10.1093/icesjms/ 
fsab255. 

Graham, N.A.J., Spalding, M.D., Sheppard, C.R.C., 2010. Reef shark declines in remote 
atolls highlight the need for multi-faceted conservation action. Aquat. Conserv. Mar. 
Freshwat. Ecosyst. 20 (5), 543–548. https://doi.org/10.1002/aqc.1116. 

Hammerschlag, N., Schmitz, O.J., Flecker, A.S., Lafferty, K.D., Sih, A., Atwood, T.B., 

Gallagher, A.J., Irschick, D.J., Skubel, R., Cooke, S.J., 2019. Ecosystem function and 
Services of Aquatic Predators in the Anthropocene. In: Trends in Ecology and 
Evolution, 34. Elsevier Ltd., pp. 369–383. https://doi.org/10.1016/j. 
tree.2019.01.005. Issue 4.

U-net) to discriminate sharks at an individual level on images (Le et al., 
2022). The method was able to predict with 81% accuracy whether in-
dividual sharks present in the test dataset had ever been encountered in 
the training dataset.

Our  deep-learning  models  used  NASnet  architecture  (Zoph  et  al., 
2017) with a Faster-rcnn backbone (Ren et al., 2015) implemented in 
Tensorflow2. The parameters of the model can be found on TensorFlow 
model zoo1 under the name “faster_rcnn_nas”. All images were resized to 
1333 × 800 pixels to match with the pre-training data (COCO dataset 
(Lin et al., 2014)) used to prepare the first layers of our model and save 
computing time during the training phase). We used a learning rate of 
0.008 with a cosine learning rate decay. For each K-fold, the model was 
trained on its own version of Trainingclips  through 200,000 iterations, 
with a batch size of 16 images per iteration. The training was completed 
in 96 h  per model using a GPU-cluster equipped with 4 RTX8000. In 
order  to  evaluate  the  performance  of  our  deep-learning  models,  we 
computed the recall, precision, and F-measure for each model. Briefly, a

85 

80 
10,474 
2570 
900 
783 
454 

346 

267 
223 

132 

107 

36 

32 
14 

10 

4214 

822 

162 

229 
190 
112 

37 

80 
4621 
1640 
615 
430 
322 

103 

168 
77 

80 

78 

30 

18 
1 

8 

65 

40 

32 

7 
4 
4 

3 

4 
90 
62 
16 
35 
28 

10 

13 
11 

35 

9 

5 

4 
1 

1  

EcologicalInformatics80(2024)1024992S. Villon et al.                                                                                                                                                                                                                                   

For each fold, the frames from Trainingclips  were used to train the 
models. The Testingclips were used to assess the robustness of our method 
on short sequences centered around the presence of fish presence. The 
Testingstations, common to all k-fold were used to assess the robustness of 
the method on 1-h videos, corresponding to the real use-case scenarios 
of ecological studies (Supp. Fig. 1). 

2. Methods

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper by S. Villon et al. does not provide explicit information about specific techniques used to address data bias during preprocessing of their deep learning pipeline. However, based on general practices in machine learning and deep learning, several methods could have been employed to mitigate data biases. These include stratified splitting, which ensures that the distribution of classes is maintained across different subsets; oversampling, where minority class examples are duplicated or synthesized to balance the dataset; undersampling, where majority class examples are removed to achieve balance; and diverse data collection, which involves gathering more varied samples to improve representation within the dataset. Without further details from the authors, it cannot be confirmed if any of these techniques were applied in this particular study.
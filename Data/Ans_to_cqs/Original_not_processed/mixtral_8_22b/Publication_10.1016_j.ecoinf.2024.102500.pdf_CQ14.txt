Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table  2  lists  nine  grid  search  ML  models  with  fivefold  cross- 
validation for four WQPs and hyperparameters. Model evaluation and 
hyperparameter selection use cross-validation resampling. The dataset is 
split  into  five  equal-sized  folds  using  fivefold  cross-validation.  Five 
times, one of each fold, the training and assessment process is validated. 
A  more  complete  model  performance  analysis  is  possible.  Averaging 
coefficient of determination (R2) or RMSE across five iterations with five 
folds  improves  model  performance  and  hyperparameter  tuning  effi-
ciency estimation. The Python library’s randint command picks integers 
within a range for hyperparameters like n_estimators and max_depth. We 
use  the  uniform  command  for  discrete  or  continuous  values  inside  a 
range,  like  the  min_samples_split  hyperparameter.  These  commands 
examined  multiple  hyperparameter  value  ranges  to  establish  model

Hyperparameter  tuning  plays  a  crucial role  in  optimizing  the  per-
formance of a model by improving its ability to predict observed and 
predicted ranges. The tuning process maximizes the models training and 
testing  performance.  It  will  control  overfitting  or  underfitting  issues 
(Elshawi et al., 2019). Though the process is complex and takes a long 
time,  many  optimizing  methods,  such  as  Manual  Tuning,  Bayesian 
Optimization,  and  random  grid  search  methods,  simplify  the  process 
(Yang  and  Shami,  2020).  Nevertheless,  choosing  which  optimizing 
method is best for specific data is challenging. Hyperparameter tuning of 
the decision tree model are usually configured with maximum depth and 
minimum number of samples required to split a node. For the RF model, 
extensive  experimentation  was  conducted  using  varying  numbers  of 
trees  for  the  optimal  fit.  The  gradient  boosting  model  is  tuned  with

3.2.2. Tuning hyperparameters

3.2.2.2. Tuning  ET  model  hyperparameters. Table 2  shows that the  ET 

model’s  hyperparameter  tuning  technique  carefully  tweaked  many 
hyperparameters to maximize performance. The hyperparameter types 
and  default  ranges  were  considered  during  tuning.  This  also  uses  the 
n_estimators, max_depth, and random_state parameters from the previous 
section. Tree partitioning was improved by setting the min_samples_split 
parameter  to  the  minimum  amount  of  samples  needed  to  split  an  in-
ternal  node.  Leaf  nodes  were  better  represented  using  the  min_sam-
ples_leaf  argument,  which  defines  the  minimal  amount  of  samples 
needed at the node. The default values for the max_features argument, 
which  determines  the  best  split,  are  ‘auto’,  ‘log2’,  or  ‘sqrt’.  These 
hyperparameters were carefully selected and adjusted to balance model 
complexity, convergence speed, and generalization to improve the ET 
model’s performance.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text mentions that the hyperparameters of the machine learning models are optimized using grid search with fivefold cross-validation. Grid search is a method used to find the optimal combination of hyperparameters for a given model. In this case, it involves specifying a set of hyperparameters and their corresponding values, then fitting the model on the training data for each combination of these hyperparameters. The resulting models are evaluated based on their performance metrics, such as coefficient of determination (R^2) or root mean squared error (RMSE), averaged across five iterations with five folds.

To select the hyperparameters, the Python library's `randint` command is used to pick integer values within a specified range for hyperparameters like `n_estimators` and `max_depth`. Additionally, the `uniform` command is employed for selecting discrete or continuous values within a range for hyperparameters like `min_samples_split`. By examining multiple hyperparameter value ranges, the optimal combination can be determined for each model.

In summary, the hyperparameters of the models are optimized through grid search with fivefold cross-validation, where various combinations of hyperparameters are tested, and the best performing ones are chosen based on average performance metrics across multiple iterations.
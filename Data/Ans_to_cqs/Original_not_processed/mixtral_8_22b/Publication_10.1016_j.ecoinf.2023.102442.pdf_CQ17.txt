Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Gallwey, Jane, Robiati, Carlo, Coggan, John, Vogt, Declan, Eyre, Matthew, 2020. 
A Sentinel-2 based multispectral convolutional neural network for detecting 
artisanal small-scale mining in Ghana: applying deep learning to shallow mining. 
Remote Sens. Environ. 248, 111970 https://doi.org/10.1016/j.rse.2020.111970. 
Girardin, C´ecile A.J., Malhi, Yadvinder, Doughty, Christopher E., Metcalfe, Daniel B., 
Meir, Patrick, del Aguila-Pasquel, Jhon, Araujo-Murakami, Alejandro, et al., 2016. 
Seasonal trends of Amazonian rainforest phenology, net primary productivity, and 
carbon allocation: seasonal trends of Amazonian forests. Glob. Biogeochem. Cycles 
30 (5), 700–715. https://doi.org/10.1002/2015GB005270. 

Gorelick, Noel, Hancher, Matt, Dixon, Mike, Ilyushchenko, Simon, Thau, David, 

Moore, Rebecca, 2017. Google Earth engine: planetary-scale geospatial analysis for 
everyone. Remote Sens. Environ. 202, 18–27. https://doi.org/10.1016/j. 
rse.2017.06.031.

Castello, Leandro, Macedo, Marcia N., 2016. Large-scale degradation of Amazonian 
freshwater ecosystems. Glob. Chang. Biol. 22 (3), 990–1007. https://doi.org/ 
10.1111/gcb.13173. 

Couttenier, Mathieu, Di Rollo, Sebastien, Inguere, Louise, Mohand, Mathis, 

Schmidt, Lukas, 2022. Mapping artisanal and small-scale mines at large scale from 
space with deep learning. Edited by Ruxandra Stoean. PLoS One 17 (9), e0267963. 
https://doi.org/10.1371/journal.pone.0267963. 

Dethier, Evan N., Silman, Miles, Leiva, Jimena Díaz, Alqahtani, Sarra, Fernandez, Luis E., 
Pauca, Paúl, Çamalan, Seda, et al., 2023. A global rise in alluvial mining increases 
sediment load in tropical Rivers. Nature 620 (7975), 787–793. https://doi.org/ 
10.1038/s41586-023-06309-9. 

Elmes, Arthur, Ipanaqu´e, Josu´e Gabriel Yarlequ´e, Rogan, John, Cuba, Nicholas,

classification  of  mining  regions  are  eliminated  in  a  highly  accurate 
manner  (89.5%  accuracy  of  mining  regions),  implying  that  manual 
reclassification only needs to be done on large, easily identifiable areas 
of overclassification, such as the area of sediment deposits on the banks 
of the lake.

18. Select Mining Pixels Above 3 pixel sizes (PROCESS) - In order to remove the influence of isolated, small, misclassified pixels (that are usually 
1–2 pixels in size), the minimum size of a mine for this step of the model was deemed as 3 pixels in size (or > 2700 m2 in a Landsat Image). It is a 
common occurrence in LULC classification maps, especially those done on a large scale, to contain isolated, misclassified individual pixels. Therefore, 
only mines above 2700 m2 were selected, removing the influence of small, misclassified pixels on the reclassification of water bodies to ensure the 
most accurate result could be achieved.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The papers cited do not provide explicit information about the specific regularization methods used to prevent overfitting in their respective deep learning pipelines. However, it can be inferred that some form of regularization would have been employed due to its importance in preventing overfitting when training machine learning models. Common regularization techniques include dropout, L1 or L2 regularization, early stopping, data augmentation, and batch normalization. Dropout involves randomly setting a fraction of input units to 0 during training time, which helps reduce interdependent learning amongst neurons. L1 or L2 regularization adds a penalty term to the loss function, discouraging complex models and reducing overfitting. Early stopping monitors validation error while training and stops if it starts increasing, indicating overfitting. Data augmentation generates new training samples through transformations like rotation, scaling, etc., improving generalizability. Lastly, batch normalization standardizes inputs within each mini-batch, accelerating convergence and providing some regularization effect. Without explicit mention, we cannot confirm which method(s) were utilized in these studies.
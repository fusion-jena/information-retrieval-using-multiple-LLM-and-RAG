Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.2. Model performance MLMs comparisons 

The performance of each MLM was assessed using several metrics, 
such as AUC, TSS, Kappa, and phi. The outcomes of these evaluations are 
presented  in  Table  3.  Additionally,  Fig.  4  visually  illustrates  the  pre-
dictive performance of each model with AUC values for both training 
and test datasets. Upon analysing the four performance metrics, it was 
evident that the selected models exhibited high consistency, except for 
the  CART.  The  performance  of  the  remaining  models  displayed  only 
slight variations within a small range, as shown in Table 3. Additionally, 
there was no discernible variation in the classifiers’ prediction accuracy 
according to the Kruskal-Wallis statistic (p > 0.05).

age in R, in order to identify multicollinearity among the chosen pre-
dictors (Naimi, 2015). For calculating VIF and tolerance (TOL) values, 
these functions utilized a stepwise procedure to find and eliminate very 
collinear variables. If a variable’s VIF is greater than ten and its TOL is 
less than one, it is frequently deemed inappropriate to include it in the

Validation of MLMs is a critical step in assessing each model’s pre-
dictive capabilities, as Mohammadi et al. (2014) emphasized. To eval-
uate  the  accuracy  and  reliability  of  each  MLM,  we  employed  robust 
statistical  measures  provided  by  the  ‘sdm  1.1–8’  package.  These  are 
kappa (Allouche et al., 2006), true skill statistics (TSS; Allouche et al., 
2006), and area under the receiver operator characteristics (ROC) curve 
(AUC; Hanley and McNeil, 1982). Model complexity was evaluated for 
each case by computing the difference between the AUC (train) and AUC 

(CV), represented as Δ AUC (Jarnevich et al., 2018). The Δ AUC scores 
from each model offer insights into their sensitivity to the training data 
(Jarnevich  et  al.,  2018).  Moreover,  the  Kruskal-Wallis  test  was  con-
ducted to determine whether a significant difference in prediction ac-
curacy exists among the classifiers. 

2.5. Ensemble model

By default, we utilized the recommended parameters for all MLMs, 
except for Random Forest (RF), where we expressly set the number of 
trees to 500 as an additional parameter. A higher number of trees helps 
reduce  the  resulting  model’s  bias.  We  calibrated  and  validated  each 
model after fitting each MLM with optimized parameters using a 10-fold 
cross-validation (CV) design. For each run of the cross-validation pro-
cedure, we reserved 30% of the forest fire occurrence data as a valida-
tion  set,  with  the  remaining  70%  of  the  data  being  used  to  train  the 
model. This rigorous validation methodology was adopted following the 
method explained by Eskandari et al. (2021). The approach generated 
50 resulting models, consisting of 10 models for each machine learning 
algorithm.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In this study, the authors did not explicitly mention specific criteria for determining when training is complete. However, they do provide some information about how they approached model training and validation. They used a 10-fold cross-validation (CV) design, which means that the dataset was divided into 10 equal parts or folds. Nine of those folds were used for training the model, while the remaining fold was used for testing or validation. This process was repeated 10 times, such that each fold served once as the validation set.

For each run of the cross-validation procedure, 30% of the forest fire occurrence data was reserved as a validation set, with the remaining 70% of the data being used to train the model. This suggests that the authors might have used some form of early stopping based on the performance of the model on the validation set during training. Early stopping is a technique used to prevent overfitting by halting the training process before the model starts to fit the noise in the training data. It can be implemented by monitoring the performance of the model on a separate validation set and stopping the training process when the performance stops improving or even starts deteriorating.

However, without explicit mention of the use of early stopping or other specific criteria for determining when training is complete, it cannot be definitively stated what criteria were used in this particular study.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Local low quality (LL) dataset

of 0.6. The neural network was trained with the stochastic gradient

descent algorithm with the softmax cross-entropy loss for 300 epochs.

We used a batch size of 10 and a fixed learning rate of 0.01, and the

convergence of loss was visually assessed. The numbers of units in the

A subset of single specimens (taken from the bulk samples) were indi-

two FC layers (512 and 256 for the first and second FC layers, respec-

vidually photographed using a conventional stereoscope NIKON

tively) and the dropout rate were determined by five-fold cross-

SMZ1270i equipped with a NIKON DS-Fi3 Microscope Camera (5.9

megapixels) controlled by the NIKON DS-L4 v.1.5.0.3 control unit.

These images were denoted Local Low Quality (LL) dataset. These pho-
tographs were intended to represent a more realistic scenario of local

LeCun, Y., Bengio, Y. & Hinton, G. (2015) Deep learning. Nature, 521(7553),
436–444. Available from: https://doi.org/10.1038/nature14539
Mukhoti, J., Kulharia, V., Sanyal, A., Golodetz, S., Torr, P.H.S. & Dokania, P.
K. (2020) Calibrating deep neural networks using focal loss. Advances
in Neural Information Processing Systems, 33, 15288–15299. Available
from:
https://proceedings.neurips.cc/paper/2020/hash/aeb7b30ef
1d024a76f21a1d40e30c302-Abstract.html

Noguerales, V., Meramveliotakis, E., Castro-Insua, A., Andújar, C.,
Arribas, P., Creedy, T.J. et al.
(2021) Community metabarcoding
reveals the relative role of environmental filtering and spatial pro-
cesses in metacommunity dynamics of soil microarthropods across a
mosaic of montane forests. Molecular Ecology in press. Available
from: https://doi.org/10.1111/mec.16275

representing single-specimen photographs by local taxonomists, are
used to predict local high-resolution images (LL!LH).

Between-datasets classification with domain adversarial
training

In addition to the standard CNN setups described above, we employed

large reduction of accuracy, hence difficulty in transfer.

the domain adversarial training of neural networks (DANN, Ganin

Divergence between the source and target datasets was mea-

et al., 2016) which incorporates a certain portion of the unknown tar-

sured with a dataset classification error. A linear support vector

gets in the model. The DANN model jointly predicts the class (family

machine (SVM) was trained to classify images to the source or target

label) of the source images and the dataset (domain) of all input images

dataset with the features of 200 randomly selected images from both

(as in the previous section) by adding layers for the dataset classifica-

(2.2% reduction for DANN,
predictions by the plain NN model
p = 0.0002, Figure S4). In the GH!LL prediction, a similar trend was
observed (Figure S5) and the target accuracy was not significantly dif-
ferent from the NN (0.015% reduction for DANN, p = 0.98). Loss and

accuracy development during training of models are reported in

Figures S2, S3.

Classification error

Classification error was visualized as a scaled confusion matrix. Starting

with a trial for a within-dataset analysis with 400 training images in the

LH random sampling showed that the large taxonomic groups were cor-
rectly classified in most cases (Table S4). For example, four families

In order to test the effect of the presence of unknown inputs (out-of-

distribution samples) on the classification, we first used an LH-trained
model to predict the class of 16 LH images belonging to eight families/
subfamilies, Coccinellidae, Elateridae, Endomychidae, Hydrophilidae,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the use of specific regularization methods such as dropout or L2 regularization. However, it mentions that the number of units in the fully connected (FC) layers and the dropout rate were determined by five-fold cross-validation. This suggests that some form of regularization was applied to prevent overfitting, but the exact method is not specified. Additionally, the use of a fixed learning rate could also be considered a form of regularization, as it helps to stabilize the optimization process and prevent overfitting.
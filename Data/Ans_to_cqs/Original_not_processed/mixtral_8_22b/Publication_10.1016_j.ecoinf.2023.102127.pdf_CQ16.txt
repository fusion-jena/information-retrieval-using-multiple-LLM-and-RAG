Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Isaac et al. (2020). This is since (a) the consideration of all available data 
is  one of the  main points of  model-based integration, (b)  appropriate 
validation metrics are less straight-forward than for single datasets as 
biases and sampling methods can differ, and (c) any validation dataset 
might not represent the niche and environmental parameters estimated 
by  the  integrated  model.  For  example,  the  standard  practice  of  with-
holding parts of the training data for validating a model often means that 
both  training  and  testing  data  suffer  from  the  same  spatial  and  envi-
ronmental biases (Baker et al., 2022). If, however prior knowledge of the 
biology of a species is integrated in a SDM through a prior or offset, thus 
“nudging”  or constraining response functions towards a more sensible 
outcome  and  ultimately  different  prediction,  the  use  of  any  (biased) 
withheld data would likely indicate a reduced predictive performance

2.4. Model evaluation

EcologicalInformatics76(2023)1021276M. Jung                                                                                                                                                                                                                                           

or  True  Skill  Statistics  (TSS)  using  the  “modEvA”  R-package  (M´arcia 
Barbosa et al., 2013), or by thresholding with the minimum presence 
values  (e.g.  the  minimum  value  across  occurrence  points),  fixed  or 
percentile values. Finally, all suitability predictions subject to thresholds 
can be created in binary, categorical percentile and normalized outputs 
(Fig. 4c). Thresholding to a normalized or percentile characterization of 
the  distribution  retains  some  of  the  detail  of  the  projected  suitability 
distribution, while also removing uncertain areas and noise.

are not enough data available to reliably fit every type of model, espe-
cially  given  the  demanding  nature  of  some  machine  learning  ap-
proaches,  and  computation  time  can  be  a  considerable  limitation  as 
well, such as for more demanding Bayesian models. The package will 
raise warnings and highlighted messages in case the provided informa-
tion is not sufficient for inferring a species distribution.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information on the specific criteria used to determine when training is complete. However, it mentions that appropriate validation metrics are important but less straightforward than for single datasets due to potential differences in biases and sampling methods. It also notes that withholding parts of the training data for validation may result in both training and testing data suffering from the same spatial and environmental biases. Additionally, the text discusses the use of various metrics for evaluating models, including Area Under Curve (AUC), Kappa statistics, Cohen’s Kappa, Boyce Index, Sørensen’s Similarity Index, Maximum Kappa, True Skill Statistic (TSS), and thresholding techniques like minimum presence values, fixed values, or percentile values. These metrics could potentially be used to evaluate the performance of a model during training and help determine when training should be stopped. However, without further context, it cannot be definitively stated which specific criteria are used to determine when training is complete.
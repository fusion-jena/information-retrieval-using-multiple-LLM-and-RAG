Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Fig. 6. Training accuracy vs validation accuracy - MobileNet-BiLSTM.  

EcologicalInformatics81(2024)102548824681000.20.40.60.81EpochsAccuracyTrainingAccuracyValidationAccuracyLoss24681000.20.40.60.81EpochsLossTrainingLossValidationLoss24681000.20.40.60.81EpochsAccuracyTrainingAccuracyValidationAccuracyLossT. Chawla et al.                                                                                                                                                                                                                                 

GRU, LSTM, and BiLSTM consistently demonstrated superior results as 
shown  in  Figs.  4  to  9,  with  MobileNet-GRU  emerging  as  the  highest 
performing model. This model obtained an accuracy of 99.27% with a 
minimal loss of 0.02, and it was also efficient in terms of training time 
(890.16 s), parameter count (11,864,770), and a moderate number of 
layers (6).

Fig. 7. Training loss vs validation loss - MobileNet-BiLSTM.  

Fig. 8. Training accuracy vs validation accuracy - MobileNet-LSTM.  

Fig. 9. Training LOSS VS VALIDATION LOSS - MobileNet-LSTM.  

EcologicalInformatics81(2024)102548924681000.20.40.60.81EpochsLossTrainingLossValidationLossT. Chawla et al.                                                                                                                                                                                                                                 

Table 2 
Comparison of performance metrics of different models in terms of accuracy, 
binary cross entropy loss, model training time, total parameters and total layers.  

significantly. The results across different metrics of six transfer learning 
models, three RNNs and three hybrid models implemented are presented 
in Table 2. 

Total 
parameters 

No. of 
layers 

5.3. Error analysis 

Model 

Accuracy 
(%) 

Loss

A =

TP + TN
TP + TN + FP + FN

(9) 

Here,  TP  represents  the  number  of  true  positive  predictions,  TN 
corresponds to the number of true negative predictions, FP signifies the 
number  of  false  positive  predictions,  and  FN  indicates  the  number  of 
false negative predictions. 

5.1.3. Model training time 

Model training time quantifies the time required for a neural network 
to converge to a predefined accuracy or loss threshold. This metric is 
inherently linked to the model’s architectural complexity, the volume 
and  dimensionality  of  the  training  dataset,  and  the  computational 
throughput  of  the  training  environment  (e.g.,  GPU  acceleration).  In 
practical scenarios, optimizing training time is useful for resource allo-
cation and rapid model iteration. 

5.1.4. Total model parameters

classification tasks. The evaluation of model performance was based on 
the  accuracy  metric.  Each  model  underwent  training  for  10  epochs, 
utilizing  a  batch  size  of  32.  The  class  mode  was  set  to  ‘categorical,’ 
indicating  that  the  labels  were  encoded  using  one-hot  encoding  for 
multi-class  classification  tasks.  These  generalized  parameter  settings 
were consistently applied across all models, ensuring comparability and 
standardization throughout the study experiments.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The criterion used to determine when training is complete is not explicitly mentioned in the given context. However, there are some clues that suggest how this might be determined. For instance, the text mentions that each model undergoes training for 10 epochs, which could imply that training stops after a certain number of epochs have been completed. Additionally, the term 'predefined accuracy or loss threshold' is mentioned in relation to model convergence, suggesting that training may stop once a specific level of accuracy or loss has been achieved. It should be noted that these are only speculations based on the information available, and further clarification would be needed to provide a definitive answer.
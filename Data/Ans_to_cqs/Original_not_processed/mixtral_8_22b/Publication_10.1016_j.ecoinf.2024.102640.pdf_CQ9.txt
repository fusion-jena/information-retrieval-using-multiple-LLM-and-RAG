Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

training  datasets  in  use.  Until  now,  there  is  still  lack  of  high-quality 
dataset for GVI extraction.

5.2. Deep GVI learning 

In  GreenCam,  we  measure  photo  GVI  by  SegFormer  (Xie  et  al., 
2021),  a  state-of-the-art  Transformer-based  model  for  semantic  seg-
mentation. As shown in Fig. 12, SegFormer follows the encoder-decoder 
structure, which combines a Transformer-based encoder (i.e., MiT) with 
a  lightweight  decoder  (i.e.,  ALL-MLP)  that  is  composed  entirely  of 
multilayer perceptron (MLP) blocks.

5. Learning GVI from photo 

In  this  section,  we  first  introduce  a  dataset  specially  designed  to 
facilitate GVI extraction and then, employ SegFormer (Xie et al., 2021), 
a  deep-learning  model  for  semantic  segmentation,  to  complete  GVI 
learning. 

5.1. Construction of dataset 

We propose a new dataset, called UGE-D (Urban Green Extraction 
Dataset), to train the GVI-learning model. For the semantic segmenta-
tion  tasks  in  urban  scenarios,  a  number  of  datasets  with  annotations 
have  been  proposed.  COCO-Stuff  (Caesar  et  al.,  2018)  is  a  dataset 
including 164 K well-annotated images. Cityscapes (Cordts et al., 2016) 
is a benchmark dataset for the semantic understanding of urban street 
scenes. AED20K (Zhou et al., 2017), on the other hand, is a large-scale 
dataset for general scene understanding, and it consists of 150 seman-
tic categories (or classes) such as car, person, sky, and so forth. However,

these datasets are unsuitable for our task, which especially focuses on 
the trees, lawns, shrubs and other veges. The limitations of the proposed 
datasets have motivated us to construct a new dataset for GVI extraction: 
(i) The proposed datasets have their own specific application scenarios, 
such as road traffic or indoor environments, rather than the urban green 
spaces that we are interested in. (ii)The proposed datasets also contain 
annotations for plants, but the proportion of such annotations within the 
entire  dataset  is  relatively  low,  resulting  in  an  imbalance  in  class  la-
beling.  (iii)There  are  numerous  irrelevant  categories  within  the  pro-
posed datasets that are not related to GVI extraction. Models should not 
focus  on  these  categories  to  avoid  impacting  the  efficiency  of  GVI 
extraction. And for GVI extraction, attention or learning target should be 
given to the objects that persist in a landscape, rather than to the moving

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given information, I cannot provide the exact location of the data repository for the deep learning pipeline used in GreenCam for GVI extraction. It appears that the authors created a custom dataset named UGE-D (Urban Green Extraction Dataset) specifically tailored for training the GVI-learning model. However, they did not mention any details about where this dataset can be accessed or downloaded.

It would be best to contact the original authors or refer to their published work for more detailed information regarding the availability of the UGE-D dataset.
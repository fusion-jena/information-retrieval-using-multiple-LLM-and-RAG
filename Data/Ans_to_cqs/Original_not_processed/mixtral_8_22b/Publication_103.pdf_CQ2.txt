Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Page 14 of 14

Working notes. CEUR-WS. Toulouse; 2015. [Online]. Available: https://hal.
inria.fr/hal-01182795.

32. Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep
convolutional neural networks. In: Proceedings of the 25th International
Conference on Neural Information Processing Systems. USA: Curran
Associates Inc.; 2012. p. 1097–105. http://dl.acm.org/citation.cfm?id=
2999134.2999257.

33. Goëau H, Bonnet P, Joly A. Plant identification in an open-world (lifeclef
2016). In: CLEF 2016 - Conference and Labs of the Evaluation forum.
Évora; 2016. p. 428–39. https://hal.archives-ouvertes.fr/hal-01373780.
34. Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D,

Vanhoucke V, Rabinovich A. Going deeper with convolutions. In: 2015
IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Boston: IEEE Conference; 2015. p. 1–9. doi:10.1109/CVPR.2015.7298594.
Ioffe S, Szegedy C. Batch normalization: Accelerating deep network
training by reducing internal covariate shift. CoRR. 2015. abs/1502.03167.
[Online]. Available http://arxiv.org/abs/1502.03167.

35.

37.

36. He K, Zhang X, Ren S, Sun J. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. CoRR. 2015.
abs/1502.01852. [Online]. Available http://arxiv.org/abs/1502.01852.
Jia Y, Shelhamer E, Donahue J, Karayev S, Long J, Girshick R,
Guadarrama S, Darrell T. Caffe: Convolutional architecture for fast feature
embedding. In: Proceedings of the 22Nd ACM International Conference
on Multimedia. New York: ACM; 2014. p. 675–8.
doi:10.1145/2647868.2654889.

38. Mata-Montero E, Carranza-Rojas J. Automated plant species

Transfer learning
Transfer learning is a powerful paradigm used to over-
come the the lack of sufficient domain-specific training
data. Deep learning models actually have to be trained on
thousands of pictures per class to converge to accurate
classification models. It has been shown that the first lay-
ers of deep neural networks deal with generic features [28]
so that they are generally usable for other computer vision
tasks. Consequently they can be trained on arbitrary train-
ing image data. Moreover, the last layers themselves con-
tain more or less generic information transferable from
one classification task to another one. These layers are
expected to be more informative for the optimization
algorithm than a random initialization of the weights of

Jose et al. BMC Evolutionary Biology  (2017) 17:181 

Page 4 of 14

Table 1 GoogleNet architecture modified with Batch
Normalization

Type

Patch size / Stride Output size

Depth Params Ops

7×7/2
3×3/2

3×3/1
3×3/2

Convolution

Experiments and results
All experiments measured the top-1 and top-5 accu-
racy of the trained deep learning model under different
circumstances, i.e., herbarium specimens classification
(“Herbarium specimen classification” section, Table 3),
transfer learning across herbarium data from differ-
ent regions (“Cross-Herbaria transfer learning” section,
Table 4), and transfer learning from herbarium data to
non-dried plant images (“Transfer learning from herbar-
ium to non-dried plant images” section, Table 5).

For each of these experiments, table columns are

defined as follows:

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not explicitly mention the specific data format used in the deep learning pipeline. However, based on the references cited in the text, it appears that the authors primarily use image data for their deep learning tasks. For instance, reference number 32 discusses ImageNet classification using deep convolutional neural networks, while reference number 33 focuses on plant identification using images. Additionally, the paper mentions the use of thousands of pictures per class for training deep learning models. Therefore, we can infer that the primary data format used in this study is likely image data.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Another considerable experimental result is that Elastic Net can improve the perfor-
mance of the proposed model. A good deep learning model usually requires abundant
data to train and analyze, while the limitations of obtaining DNA barcode sequences of ﬁsh
species from different families and the problem of overﬁtting in small datasets are more
and more serious. To solve the overﬁtting problem in training process on small datasets
is of great importance. In our study, Elastic Net is used to solve overﬁtting problem and
improve the generalization ability of the ESK model. Moreover, genetic characteristics
of species belong to high-dimensional data, which are time consuming during training.
However, directly combining a set of fully connected EN-SAE is often has little effect for
extracting useful information. Elastic Net provides sparse connection, which can also save
training time. Therefore, Elastic Net can improve the performance of proposed model.

36.

Random Forest supervised learning model. BMC Genet. 2019, 20, 2. [CrossRef] [PubMed]
Jin, S.; Zeng, X.; Xia, F.; Huang, W.; Liu, X. Application of deep learning methods in biological networks. Brief. Bioinform. 2021, 22,
1902–1917. [CrossRef]

37. Kumar, S.; Stecher, G.; Li, M.; Knyaz, C.; Tamura, K. MEGA X: Molecular Evolutionary Genetics Analysis across Computing

Platforms. Mol. Biol. Evol. 2018, 35, 1547–1549. [CrossRef]

38. Chu, Z.; Yu, J. An end-to-end model for rice yield prediction using deep learning fusion. Comput. Electron. Agric. 2020, 174,

105471. [CrossRef]

39. Chen, J.; Sathe, S.; Aggarwal, C.; Turaga, D. Outlier Detection with Autoencoder Ensembles. In Proceedings of the 2017 SIAM

International Conference on Data Mining (SDM), Houston, TX, USA, 27–29 April 2017; pp. 90–98. [CrossRef]

Symmetry 2021, 13, 1599

16 of 16

40. Homoliak, I. Convergence Optimization of Backpropagation Artiﬁcial Neural Network Used for Dichotomous Classiﬁcation of

226.5478

ESK

122.3258

295.8245

201.4312

As shown in Tables 4–6, the AE has the shortest running time and the poorest clas-
siﬁcation results compared with the other two models. The AE consists of only a single
Autoencoder, but only one Autoencoder cannot learn the deep features of DNA barcode se-
quences well, resulting in poor classiﬁcation results compared with deep learning methods.
Although the ESK time consumption is longer than AE, the accuracy, recall, and F1-Score
improved for all datasets, by 8.44%, 8.77%, and 4.84%, respectively.

Symmetry 2021, 13, 1599

13 of 16

h = σe(Wx + b)

(3)

where x denotes the input data vector, W is the weight matrix connecting the input and
hidden layers, b is the bias vector belonging to the latent layer nodes, and σe represents the
activation function, such as Sigmoid, Relu, Tanh, etc.

(2) Decoder: in this step, the hidden representation h is mapped into reconstruction

vector y, the typical form as follows:

y = σd(W(cid:48)h + b(cid:48))

(4)

where W’ is the weight matrix connecting the latent and output layers, b’ is the bias vector,
and σd represents the activation function.

Loss function is deﬁned to measure the reliability of SAE. SAE is trained to reconstruct
the features of input and adjust the weights of the encoder and decoder to minimize
the error between the output and the input. Thus, loss function is introduced, which is
expressed in terms of mean square error as follows:

L(W, b) = ∑(cid:107)y − x(cid:107)2

(5)

Symmetry 2021, 13, 1599

6 of 16

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide specific details about the hyperparameters used in the deep learning model, such as the learning rate or optimizer. However, it mentions that Elastic Net was used to address the issue of overfitting in the model. Elastic Net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods, respectively. It is known for its ability to perform well when dealing with high dimensional data, like the genetic characteristics mentioned in the text. Additionally, the use of autoencoders (AE) and stacked autoencoders (SAE) is discussed, indicating that these techniques were employed in the deep learning architecture. Nevertheless, without further information, we cannot determine the exact values of hyperparameters like learning rate, batch size, number of epochs, or the type of optimizer used in the model.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Convolutional neural networks (CNNs) [9], also known as
deep networks, are an important class of machine learning
algorithms applicable, among others, to numerous computer
vision problems. Deep CNNs, in particular, are composed of
multiple layers of processing involving linear as well as non-
linear operators. To solve a particular task, the parameters of
networks are learned in an end-to-end manner. Image represen-
tations extracted from deep CNNs trained on a large dataset
such as ImageNet [10] have shown to produce a promising
performance for diverse classiﬁcation and recognition tasks
[11], [12], [13], [14] and [15]. Spatial pyramid pooling (SPP)
[16] and Multi-scale Orderless Pooling (MOP) [17] schemes
have made CNNs independent of the input image size and
robust for diverse classiﬁcation and recognition applications.
In this paper, we propose a computer vision and deep
learning based framework for the automatic annotation of

[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 248–255, IEEE,
2009.

[11] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and
T. Darrell, “Decaf: A deep convolutional activation feature for generic
visual recognition,” International Conference on Machine Learning
(ICML), pp. 647–655, 2014.

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on November 16,2023 at 09:41:16 UTC from IEEE Xplore.  Restrictions apply. 

NC
C

NC
C

(a)

(b)

(c)

Fig. 4: Mosaics and Coral Maps: (a) the 3 sites of the Abrolhos Island; (b) Site 3 Coral Map for 2010 and (c) Site 3 Coral
Map for 2013. Legend key: C is coral and NC is non-coral

hybrid feature representations,” IEEE International Conference on Image
Processing, in press.

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on November 16,2023 at 09:41:16 UTC from IEEE Xplore.  Restrictions apply. 

978-1-5090-1537-5/16/$31.00 ©2016 IEEEA. Classiﬁcation Process

Image representations extracted from deep neural networks,
trained on large datasets such as ImageNet [9] and ﬁne tuned
on domain speciﬁc datasets, have shown state-of-art perfor-
mance in numerous image classiﬁcation problems [14]. The
activation vectors of the ﬁrst fully connected layer of a pre-
trained VGGnet [24] are employed as feature representations
in our work. The weights of this deep network are ﬁne tuned
using the Benthoz15 dataset [23] which consists of expert-
annotated and geo-referenced marine images from Australian
seas.

Automatic Annotation of Coral Reefs
using Deep Learning

A. Mahmood∗, M. Bennamoun∗, S. An∗, F. Sohel†, F. Boussaid∗, R. Hovey∗, G. Kendrick∗ and R.B. Fisher‡
∗The University of Western Australia, Australia; email: ammar.mahmood@research.uwa.edu.au
†Murdoch University, Australia
‡University of Edinburgh, Scotland

these intricate ecosystems. Millions of digital

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't enough information available to provide a direct link to the data repository of the deep learning pipeline mentioned in the given context. However, it can be noted that the authors used the Benthoz15 dataset [23] for fine-tuning their deep learning model. This dataset might contain the required data repository link or additional resources related to the deep learning pipeline. It would be best to refer to the original research article or contact the corresponding author for more specific details regarding the data repository link.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Data CollectionPre-Processing&ConstructionLagHyperparam- OptiDatapreprocessingHyperparametertuningTraining &ValidationModelValidationModelpredection &deploymentModeltrainingKOREA HYDROGRAPHIC ANDOCEANOGRAPHIC AGENCY 8483, 2021, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2021/8551167 by Vamsi Krishna Kommineni - Friedrich-Schiller-Universität , Wiley Online Library on [28/08/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons LicenseComputational Intelligence and Neuroscience

7

Figure 3: Stacked GRU-DNN model.

Table 1: +e search space of the GRU-DNN model hyperparameters.

Hyperparameters
No. of GRU cells
No. of FC layers
No. of FC layers’ units
Hidden layers activation
Batch size
Droupout rate of FC layers

Value
[4, 8, 16]
[1, 2]
[4, 8, 16]
[Relu, Linear]
[4, 8, 16]
[0.0, 0.1, 0.2]

Table 2: +e hyperparameters values of the ML models.

Best parameter
Criterion
max_depth
max_features
min_samples_split

Bootstrap
max_depth
max_features
min_samples_leaf
min_samples_split
n_estimators

C
Degree
Gamma
Epsilon
Kernel

Decision tree

Signiﬁcant wave height Water temperature

Mae
7
Auto
4
Random forest
True
100
3
5
8
200
SVR
100
1
Auto
Default
Poly

Mae
10
Log2
Default

True
90
2
5
12
200

1
1
Default
0.2
Poly

decomposing one sample (i.e., minimum of wave height-
series of ocean marine necessary parameters). We used the
seasonaldecompose function from the statsmodels library in

train than LSTM [16].

Practically, over/underﬁtting diﬃculties in neural net-
work models are caused by the neural network model’s
excessive/insuﬃcient training epochs [43]. As a result, one
possible solution to the DL-based model’s over/underﬁtting
concerns is to apply the early stopping strategy [44], which is
used to cease training when generalisation performance
starts to degrade for a number of epochs. To track the
generalisation performance,
in the proposed model, the
training data is separated into training and validation
groups.

+e dropout approach [45] is another way to deal with
the overﬁtting problem. Dropout is a regularisation strategy
that allows you to train neural networks with alternative
topologies in parallel by randomly dropping out a certain
proportion of layer neurons. Dropout is indicated by the
black neurons in the fully connected layers, as seen in
Figure 3.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The context does not provide explicit information about the specific postprocessing steps involved after the model training. However, based on common machine learning practices, some possible postprocessing steps could include evaluating the trained model using metrics such as accuracy, precision, recall, or F1 score, creating confusion matrices to visualize the performance of the classifier, generating saliency maps to understand the importance of different features, and performing cross-validation to ensure the robustness of the model. These steps can help assess the quality of the model and identify areas for improvement.

The given context mentions the use of a stacked GRU-DNN model and provides details about its hyperparameter tuning process. It also discusses techniques like early stopping and dropout to prevent overfitting during training. However, it does not explicitly mention any postprocessing steps taken after the model training phase. Therefore, while we cannot definitively state what postprocessing steps were involved in this particular case, we can suggest potential steps based on typical machine learning workflows.
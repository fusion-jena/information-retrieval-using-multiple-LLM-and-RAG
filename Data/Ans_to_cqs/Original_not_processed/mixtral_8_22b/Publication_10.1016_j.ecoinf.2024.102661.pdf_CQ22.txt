Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Detailed accuracy metrics for all models across the data series are 
provided in Table S3 and Figs. S9â€“S13. Compared to ARIMA, the BPNN 
model's MAE increased on average by 13.52%, whereas the LSTM model 
showed  a  substantial  51.44%  reduction,  and  the  TPEBO-LSTM  model 
achieved  an  impressive  70.56%  decrease.  Regarding  the  RMSE,  the 
BPNN model experienced a 14.82% increase, the LSTM model a 49.33% 
decrease, and the TPEBO-LSTM model outperformed all with a signifi-
cant 64.50% reduction in error. 

Moreover, Table  6 presents the  DM test  statistics and P-values for 
comparisons  across  all  forecasting  models.  The  results  consistently 
indicate P-values below 0.05 for each comparative experiment, affirm-
ing the TPEBO-LSTM model's superior performance throughout.

Hyperparameters  are  instrumental  in  shaping  the  architecture  of 
deep  learning  models and  steering  the  learning  process  (Bischl et  al., 
2023).  The  TPEBO  algorithm  stands  out  for  its  strategic  approach  to 
hyperparameter optimization. Beginning with an exploratory phase of 
random  searches  for  feasible  hyperparameter  configurations,  TPEBO 
progressively narrows down its focus to zones within the search space 
where a local optimum is identified, thereby approximating the global 
optimum  with  increasing  precision.  This  methodological  approach  is 
particularly  beneficial  for  fine-tuning  LSTM  models,  known  for  their 
intricate  structures.  By  automating  the  hyperparameter  adjustment 
process,  TPEBO  not  only  enhances  the  model's  efficiency  but  also 
significantly  curtails  the  time  traditionally  spent  on  manual  tuning, 
making the modeling workflow more efficient. The process is succinctly

4. Conclusions 

The  primary  objectives  of  this  research  are  to  develop  a  compre-
hensive methodology for calculating ACE and to establish an effective 
deep learning approach for accurately forecasting ACE. Specifically, the 
main findings of this research include:  

(1)  Calculation of ACE for 31 provinces in mainland China from 1993 
to 2021, covering four main agricultural activities: farm opera-
tion,  crop  growth,  enteric  fermentation,  and  manure  manage-
ment. Using these data, the TPEBO-LSTM model based on deep 
learning methods demonstrates superior performance, indicating 
the  potential  of  this  novel  methodology  to  adeptly  manage 
diverse data types and enhance ACE forecasting.  

(2)  Forecasting  of  ACE  peaking  before  2030  in  24  provinces,  with 
notable exceptions. The study identifies significant contributors 
to  ACE  and  predicts  future  high-ACE  provinces,  implications 
critical for developing low-carbon agricultural policies.

Number of cells in hidden layers: Vital for fitting complex functions, 
the range for the number of cells in each layer is set between 10 and 200. 
This range balances the need to capture intricate data patterns and avoid 
overfitting. 

Initial learning rate: This determines the step size during parameter 
(cid:0) 3,  1],  reflects  a  balance  between 

updating.  The  chosen  range,  [1e

avoiding too rapid updates (which can cause instability) and too slow 
updates (leading to prolonged training). 

L2 regularization parameter: Adjusting this helps control overfitting 
by penalizing large weights. The search range for this parameter is set 
(cid:0) 2, in line with benchmarks from similar studies.  
between 1e

(cid:0) 10 and 1e

(2)  Optimizing objective function and algorithm definition.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In the given context, two specific metrics are mentioned to evaluate the performance of the deep learning models - Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). These metrics are commonly used in regression tasks where the goal is to predict continuous values rather than classifying them into categories.

Mean Absolute Error (MAE): It measures the average magnitude of errors without considering their direction. In other words, it gives us an idea about how far our predictions are from the actual values. Lower values indicate better performance.

Root Mean Square Error (RMSE): Similar to MAE, RMSE also quantifies the difference between predicted and observed values. However, unlike MAE, RMSE squares the differences first before averaging them, which makes it sensitive to larger errors. Therefore, minimizing RMSE implies reducing both small and large deviations between predicted and actual values.

Additionally, Diebold-Mariano (DM) Test Statistics and P-Values are presented in Table 6 for comparing different forecasting models. The results show that the TPEBO-LSTM model consistently outperforms others with P-values below 0.05, confirming its superiority.
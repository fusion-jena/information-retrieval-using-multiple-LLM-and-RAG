Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

computational resources required to train them, the practical benefits of 
using  transformer  models  for  text  classification  may  be  limited  when 
applied to simpler classification tasks.

As well as the comparator models, we also tested the classification 
performance of a fine-tuned transformer model. Transformer models are 
a  class  of  large  language  model,  characterised  by  a  self-attention 
mechanism, which can weight the influence of different parts of a text 
sequence, to capture dependencies between words in the text (Vaswani 
et al., 2017). We chose to use BERT (Bidirectional Encoder Represen-
tations  from  Transformers),  as  it  is  one  of  the  most  well-known  and 
widely-used  transformer  models.  BERT  uses  the  transformer  self- 
attention  mechanism  to  generate  representations  of  words  in  a  text 
sequence that consider both the right and left contexts around words. It 
is trained using a large Wikipedia text corpus, via both masked-language 
modelling, whereby model must predict missing tokens (words or parts 
of  text,  such  as  punctuation)  in  text,  and  next-sentence  prediction,

Here,  we  demonstrate  and  evaluate  a  text  classification  pipeline, 
designed to address both the issue of imbalanced datasets and the need 
for large training datasets, using an example data collection scenario. 
Importantly,  we  use  a  hierarchical  classification  approach,  whereby 
data are passed through a series of classifiers with increasingly specific

Since BERT can only accept a maximum of 512 tokens as input, we 
used smaller subsections of the longer web articles as classifier input, an 
approach previously applied to long document classification (Fiok et al., 
2021; Sun et al., 2019). To identify the sections of text most likely to be 
relevant to the classification decision, documents were split into chunks 
of 300 tokens using the NLTK word-tokenizer. Chunks that contained 
any  of  our  original  query  terms  (i.e.  any  bat-related  or  exploitation- 
related  term)  were  retained.  If  more  than  one  chunk  contained  key-
words,  the  first  such  chunk  of  the  document  was  retained  for  use  in 
model training and evaluation. Before being fed into the BERT model, 
texts are tokenized using the BERT tokenizer. Here, we set the maximum 
number of tokens as 400 for social media posts, as most were very short, 
and 512 for web articles, meaning some texts would have been further 
truncated at this step.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text classification pipeline described here utilizes a transformer model called BERT (Bidirectional Encoder Representations from Transformers). This model is characterized by its self-attention mechanism, which allows it to weigh the importance of different parts of a text sequence and capture dependencies between words. BERT generates word representations considering both the left and right contexts around words. It has been pretrained on a large Wikipedia text corpus using masked-language modeling and next-sentence prediction tasks. In this pipeline, BERT is employed to process smaller subsections of longer web articles due to its limitation of accepting a maximum of 512 tokens as input. Texts are tokenized using the BERT tokenizer before being fed into the model.
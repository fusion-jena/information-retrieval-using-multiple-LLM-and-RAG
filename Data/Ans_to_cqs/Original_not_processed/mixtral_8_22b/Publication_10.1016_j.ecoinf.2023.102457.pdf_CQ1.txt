Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

contribute  to  larger  data  input  for  future  training  of  deep  learning 
models (Best  et  al., 2023).  Accordingly, the  rapid growth of  artificial 
intelligence  and  deep  learning  methods  could  bring  about  important 
innovations related to feature extraction and classification with the aim 
of overcoming the current challenges halting the use of PAM for indi-
vidual acoustic discrimination and succeeding in a sustained and non- 
invasive monitoring of animal populations.

Recent  advances  in  deep  learning  technology  have  revolutionized 
the  field  of  bioacoustics  (Stowell,  2022).  Many  applications  of  deep 
learning  have  targeted  the  automated  detection  and  classification  of 
acoustic signals, with applications for birds (Stowell et al., 2018), bats 
(Aodha Mac, 2018), whales (Bermant et al., 2019), and primates (gib-
bons and indris) (Dufourq et al., 2021; Ravaglia et al., 2023). In addition 
to automated detection approaches, deep learning approaches can also 
be used as a feature extraction method; in this case, the term for features 
extracted from deep learning algorithms is “embeddings”. Deep learning 
embeddings has been used to differentiate adult and juvenile great grey 
owls Strix  nebulosa through analysis of 263 juvenile and  27 adult vo-
calizations  using  embeddings  from  the  BirdNET  model  trained  on 
~1000  species  of  birds  (McGinn  et  al.,  2023.  Embeddings  have  been

We used three different deep-learning architectures to extract em-
beddings from the female calls. The first that we used, VGGish, is a CNN 
based upon Configuration A of the VGG image classification model with 
11 weight layers (Simonyan, 2015). VGGish was trained by Google to 
perform  general-purpose  audio  classification  using  a  preliminary 
version of the YouTube-8 M dataset (Abu-El-Haija et al., 2016). Once 
trained,  the  final  classification  layer  was  removed  from  the  network, 
leaving  a  128-dimensional  acoustic  feature  embedding  as  the  CNN 
output. To input the sound files into the VGGish model, we converted 
the  waveforms  to  Mel-spectrograms  (Rabiner  and  Schafer,  2010),  a 
signal representation derived from the spectrogram that substitutes the 
normal frequency scale with the nonlinear Mel scale, taken from 20th- 
century  psychoacoustics  studies  that  explored  the  relationship  be-

The second approach we used to extract embeddings was from the 
pre-trained  BirdNET  model.  The  original  version  of  the  model  was

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

While the given context does not explicitly mention the specific methods used for collecting raw data in the deep learning pipeline, it does provide some clues regarding the types of data being used. For instance, the text mentions the use of vocalizations from various animals such as birds, bats, whales, gibbons, and indris. This suggests that the raw data might be collected using recording devices or sensors placed in their natural habitats. Additionally, the reference to the YouTube-8M dataset indicates that publicly available datasets may also be used as part of the data collection process. However, without more explicit information, it cannot be definitively stated what exact methods are employed for gathering raw data in the deep learning pipeline.
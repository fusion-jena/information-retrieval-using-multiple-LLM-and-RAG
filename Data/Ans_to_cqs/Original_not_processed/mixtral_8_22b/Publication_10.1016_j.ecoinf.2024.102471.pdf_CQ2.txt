Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.1. Feature encoder 

The feature encoder of the pre-trained Wav2vec model reduces the 
dimensionality of the audio data, converting the raw waveform into a 
sequence of feature vectors Z every 20 milliseconds. The main compo-
nent of the feature encoder's architecture is seven layers of a 1D con-
volutional neural network with 512 channels at each layer. The size of 
the kernel and strides decreases as the layer progresses in the network. 
The waveform is normalized before passing through the convolutional 
layer, followed  by  layer  normalization and  GELU  activation function. 
Finally,  latent  feature  vectors  represent  the  latent  feature  of  bird 
vocalization.  Thus,  it  helps  the  model  to  identify  and  understand  the 
sequential information in the vocalization audio of multi-bird species. 

3.2. Context network

Wav2vec is one of the latest state-of-art models for Automatic Speech 
Recognition  and  other  audio-related  problems  (Boigne  et  al.,  2020) 
(Shibano et al., 2021). The architecture of the Wav2vec is based on the 
transformer's  encoder,  with  a  training  objective  like  BERT's  masked 
language modelling objective but adapted for speech instead of text. The 
self-supervised  way  of  training  allows  Wav2vec  to  pre-train  on  unla-
belled data, which is more accessible (Hendrycks et al., 2019). Later, the 
model can be fine-tuned on a particular dataset for a specific purpose. 
This  approach  shows  remarkable  improvement  in  the  model's  perfor-
mance, especially if the dataset size is very small with label.

through  the  Mel  spectrogram.  However,  CNN-based  architectures  are 
useful for image-related tasks but unsuitable for audio domains. Several 
studies have been conducted using pre-trained models like transformers, 
where  the  multi-label  classification  work  is  performed  well  across 
various applications (G´omez-G´omez et al., 2023).

Fig. 6. Class distribution of the dataset.  

EcologicalInformatics80(2024)1024715B. Swaminathan et al.                                                                                                                                                                                                                         

Fig. 7. Waveforms of the bird sounds.  

Fig. 8. Mel spectrogram and MFCC of the Asial Koel.  

audio signals. 

3.3. Feed forward network 

The  768-dimensional  feature  vector  representation  for  the  input 
audio signal is obtained by passing the context vector through the linear 
projection  layer.  Then  the  feature  vector  is  passed  through  the  feed- 
forward  network,  which  contains  dense  layers  and  the  classification 
layer.  The  categorical  cross-entropy  loss  shown  in  (3)  is  used  at  the 
output  layer  as  the  loss  function.  This  network  helps  the  model  find

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, the deep learning pipeline primarily uses audio data. The feature encoder of the pre-trained Wav2vec model processes raw waveform audio data and converts it into a sequence of feature vectors. These feature vectors are then fed into a context network, which is based on the Transformer's encoder architecture. The context network has been trained using self-supervision, allowing it to learn from unlabelled data. After processing through the context network, the resulting context vector is projected linearly to obtain a 768-dimensional feature vector representation. Finally, this feature vector is passed through a feed-forward network containing dense layers and a classification layer. Therefore, the primary data format used in this deep learning pipeline is audio data.
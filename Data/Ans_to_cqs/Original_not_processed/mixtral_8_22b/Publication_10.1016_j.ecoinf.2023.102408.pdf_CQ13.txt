Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.5.3. Artificial neural network 

ANNs are used for modelling complex phenomena and developing 
nonlinear regression (Haykin and Lippmann, 1994). It consists of three 
different  layers:  input,  hidden,  and  output  (Ingram  et  al., 2005).  The 
ANN uses calibration parameters, such as the learning rate and weight, 
to improve the results. The weights were randomly initialized using the 
Nguyen–Widrow method (Zhou et al., 2016). An ANN fitted with CARET 
is a single hidden-layer neural network with two hyperparameters: size 
and decay. The number of units in the hidden layer (size) and regular-
isation parameter to avoid overfitting (decay) were passed as arrays of 
1–20  and  0.025–1,  respectively.  The  Transfer/Activation  function  for 
the  model was  selected  as  the  logistic  function.  The  optimum  combi-
nation of parameters for the best model fit was based on accuracy. 

3.6. Model fitting and evaluation

overfitting issue, we used the parameter (c) to optimise the line fit and 
penalise the number of samples inside the margin. The value of c plays a

Fig. 5. Model predicted vs observed AGB (a) Random Forest, (b) Artificial Neural Network, (c) Support vector machine of Sentinel-1 and Sentinel-2.

The RF algorithm, which has several advantages, was used in this 
study.  It  has  been  noted  in  prior  research  that  RF  outperforms  other 
machine-learning methods, particularly when handling extensive data-
sets (Lourenço et al., 2021). RF provides accurate information regarding 
critical  factors  and  is  less  sensitive  to  parameter  adjustments  (Cutler 
et al., 2007; Zhou et al., 2016). This method uses bagging or boosting 
algorithms to randomly select predictors. Despite these advantages, RF 
has certain limitations. Using a large number of predictors can increase 
model  complexity.  It  also  requires  careful  tuning  and  hyper- 
parameterisation  such  as  “mtry”  and  “ntree”,  there  is  a  risk  of  over-
fitting  if  these  parameters  are  not  adjusted  properly.  This  study  sys-
tematically  evaluated  mtry  from  1  to  10,  aiming  to  strike  a  balance 
between  stability  and  minimise  the  Root  Mean  Square Error  (RMSE).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The text does not provide explicit details about the specific hyperparameters used in a deep learning model, but it does mention some hyperparameters used in the artificial neural network (ANN), which is a type of deep learning model.

For the ANN model, the hyperparameters mentioned are'size' and 'decay'. The'size' refers to the number of units in the hidden layer, while 'decay' is a regularization parameter used to prevent overfitting. The values for'size' were tested within the range of 1-20, and for 'decay', the range was set between 0.025-1.

Additionally, the transfer/activation function for the model was chosen as the logistic function. However, no specifics about the learning rate or optimizer are given in the provided context.
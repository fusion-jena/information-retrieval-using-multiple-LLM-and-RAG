Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

ing model, such as a neural network, and try to improve that model by 

alization algorithms like t-SNE (van der Maaten & Hinton, 2008), can 

selecting training samples. Active learning algorithms typically start 

be regarded as embedding functions.

training the underlying model on a small, randomly selected labelled 

Deep  neural  networks  are  frequently  used  for  dimension-

set of data samples. After training the initial model, various criteria 

ality  reduction:  the  input  to  a  deep  network  often  has  many 

can  be  employed  to  select  the  most  informative  unlabelled  sam-

values, but layers typically get smaller throughout the network, 

ples to be passed to the oracle for labelling (Settles, 2009). Among 

and the output of a layer can be viewed as a reduced represen-

the  most  popular  query  selection  strategies  for  active  learning  are 

tation  of  the  network's  input.  In  this  paper,  we  use  two  com-

|  159

because eventually both methods have sufficient constraints to learn 

active  learning  pipeline  on  the  crops  produced  from  running  the 

a good embedding.

3.3.2 | Active learning strategies

MegaDetector  model  over  the  NACTI  dataset.  We  employed  mar-

gin-based active learning. After the first 30,000 active queries, the 

classifier  achieves  93.2%  overall  accuracy  which  further  confirms 

the usefulness of the suggested pipeline. More detailed results are 

available in Table S3.

Different strategies can be employed to select samples to be labelled 

by the oracle. The most naive strategy is selecting queries at random. 

Here  we  try  five  different  query  selection  strategies  and  compare 

4 |  D I S CU S S I O N

them against a control of selecting samples at random. In particular, 

we try model uncertainty criteria (confidence, margin, entropy; Lewis 

This  paper  demonstrates  the  potential  to  significantly  reduce

Nielsen,  1989;  Robbins  &  Monro,  1951;  or  modern  enhancements 

Norouzzadeh  et  al.  (2018)  that  harnessed  3.2  million  labelled 

of it; Kingma & Ba, 2014; Tieleman & Hinton, 2012) calculates the 

training  examples  to  produce  90.9%  accuracy  (using  ResNet-50 

contribution  of  each  parameter  to  the  loss  value,  then  adjusts  the 

architecture) at species classification, but with a 99.5% reduction 

parameters so that the loss value is minimized. The SGD algorithm 

in manually annotated training data. We also expect our method to 

is an iterative algorithm, i.e. it is applied many times during training, 

generalize better to new locations because we systematically filter 

including multiple times for each image in the dataset. At every it-

out the background pixels.

eration of the SGD algorithm, the parameters take one step toward

Algorithm 1. Active learning procedure

1:

2:

3:

4:

5:

6:

7:

8:

Start from a small, randomly selected labelled subset of data

while Stopping criteria not met do

Train the underlying model with the available labelled samples

Compute a selection criterion for all the samples in the 

unlabelled pool

Select n samples that maximize the criterion

Pass the selected samples to the oracle for labelling

Gather the labelled samples and add them to the labelled 

set

end while

this case, a network can first be trained on the large dataset and then 

Deep learning usually requires a large-scale dataset and a con-

fine-tuned on the target dataset (Norouzzadeh et al., 2018; Yosinski 

siderable amount of computational resources to achieve high accu-

et al., 2014).

1.5 | Active learning

racy.  Existing  active  learning  frameworks  cannot  scale  to  datasets 

with millions of high-dimensional samples, such as large camera trap

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The Stochastic Gradient Descent (SGD) optimization technique is applied in the deep learning pipeline. It is an iterative algorithm that is applied multiple times during training, including multiple times for each image in the dataset. At every iteration of the SGD algorithm, the parameters take one step towards minimizing the loss value. Additionally, modern enhancements of SGD, such as Adam (Kingma & Ba, 2014) and RMSProp (Tieleman & Hinton, 2012), calculate the contribution of each parameter to the loss value and adjust the parameters accordingly to minimize the loss value.
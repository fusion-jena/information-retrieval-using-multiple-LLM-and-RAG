Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.3.2.3. ANN: artificial neural networks classification. In classifying with 
the MLP ANN model, we used the package “nnet”, which provides pos-
sibilities for adjusting weighted decay and size, thereby countering the 
effects of model overfitting. We used an MLP ANN architecture with 1 
hidden layer established as a default setting within the “nnet” package, 
and with 6 neurons defined for our model inputs. The number of neurons 
in the input layer was equal to the number of used bands (6), and the 
output  layer  had  8  neurons  (representing  8  LULC  classes).  A  back 
propagation learning algorithm was used during the training phase of 
the model. Size and decay were used to define the primary model tuning 
parameters, and the control () function was used to control for model 
runs. As with the kNN approach, we defined the LULC classes of the test 
datasets  as  target  variables  and  the  band  reflectance  values  as  pre-

Control() function helped to evaluate these tuning parameters for model 
performance. Table S3 shows the parameterization settings (i.e. model 
type,  number  of  tuning  parameters/iterations,  tuning  methods  and 
description) for each of the four ML algorithms.

and  Kappas,  2017).  The  kNN  model  is  a  non-parametric  model  that 
performs  LULC  classification  based  on  the  distance  between  k  closest 
samples drawn from training datasets. The approach depends on thor-
ough image (predictor) pre-processing so as to reduce sampling bias and 
ensure equal treatment of predictors when computing distance (Kuhn 
and Johnson, 2016). The SVM model uses support vectors (i.e. based on 
a subset of training data points closest to decision boundaries) to locate 
optimal decision boundaries that separate two LULC classes (Cortes and 
Vapnik, 1995; Kuhn and Johnson, 2016). The ANN model is a mathe-
matical model developed as an analogy of the human brain. Using an 
interconnected  group  of  responsive  and  conducting  nodes,  the  ANN 
model  mimics,  in  a  very  simplified  fashion,  the  functionality  of  the 
human brain for knowledge acquisition, recall, synthesis and problem

solving  (Kubat,  1999;  Yang,  2009).  In  LULC  classification,  the  Multi- 
Layer Perceptron (MLP) type of ANN has been used most often (Silva 
et al., 2020). MLP carries out backpropagation of training samples to 
accurately classify LULC. The RF was developed as an ensemble of ML 
models that use bootstrap techniques to build many single decision tree 
models (Breiman, 2001; Mellor et al., 2013; Rodriguez-Galiano et al., 
2012). The RF model uses subsets of predictor variables (e.g. Landsat 
bands) to split observation datasets into subsets of homogenous samples 
to build each decision tree (Mellor et al., 2013).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information about specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being applied in the deep learning pipeline. However, it mentions that the MLP ANN model used a backpropagation learning algorithm during its training phase. Backpropagation is a widely used optimization technique in deep learning pipelines, where the error calculated by comparing the predicted output with the actual output is propagated backwards through the network layers to update the weights and biases using gradient descent. This helps minimize the loss function and improve the accuracy of predictions. Additionally, the context also mentions that the 'nnet' package was used, which allows for adjusting weighted decay and size to counteract the effects of model overfitting. Weight decay is another optimization technique commonly used in deep learning models to prevent overfitting by adding a penalty term to the loss function.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Fig. 2. Diagram showing the procedure used to build models. (a) Obtain presences (red circles) and generate background points (black triangles). (b) Separate data 
into folds. (c) Set aside one fold to use as an external evaluation dataset and keep the remaining folds as training data. (d) Perform scale optimization for envi-
ronmental covariates: calculate focal statistics for each covariate using a moving window of radius equal to each scale to be tested, then compare scales using t-tests. 
(e) Subsample background points in the training data to create three datasets: one with the number of background points equal to the number of presences (1×), one 
with a number of background points equal to 10 times the number of presences (10×), and 10,000 background points (10K). Keep the number of presences the same. 
(f) Train component models separately using each background point subsample. (g) Evaluate component models using the evaluation dataset (using AUC and TSS).

points. Then, to obtain presences and background points to use during 
external evaluation, we set aside one fold. We then used the remaining 
four folds for internal cross-validation during model training.

2.7. Subsetting background points 

We  made  three  random  selections  of  background  points  from  the 
training dataset for use during model construction. First, we made the 
“1×” selection, which selected the same number of background points as 
presences in the training dataset. Second, we made the “10×” selection, 
which selected a number of background points equal to ten times the 
number of presences in the training dataset. Third, we made the “10 K” 
selection, which selected 10,000 background points from the training 
dataset.  We  used  this  approach  to  maintain  consistency  across  back-
ground  points  used  to  train  each  model  so  there  was  no  variation  in 
model results due to variation in the response variable. 

2.8. Training individual models

We  have  provided  further  evidence  that  individual  modeling  ap-
proaches perform better when the number of background points used to 
train each model is optimized for that particular modeling algorithm. 
Given the popularity of ensemble modeling approaches in the scientific 
literature,  we  went  further  and  demonstrated  how  the  number  of 
background  points  used  to  train  individual  models  can  influence  the 
performance of ensemble models. We tested ensemble models built with 
four combinations of background points and found that the ensemble 
performed slightly better (based on AUC) when all individual models 
used  to  train  the  ensemble  were  built  with  their  optimal  number  of 
background points (Fig. 4). Although there is only a small increase in 
performance, this potentially highlights the ability of ensemble models 
to balance the strengths and weaknesses of the component models, even

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The provided context does not directly discuss the specific preprocessing steps involved before training a deep learning model such as normalization, scaling, or cleaning. However, it does mention some relevant steps taken prior to building models. These include obtaining presence and generating background points, separating data into folds, setting aside one fold for external evaluation, performing scale optimization for environmental covariates, subsampling background points, and training component models separately using each background point subsample. While these steps may not be considered traditional preprocessing techniques for deep learning models, they do represent important preparatory work done before constructing and evaluating models. It should also be noted that the type of preprocessing required often depends on the nature of the data and the specific requirements of the model being trained. Therefore, without more information about the specific deep learning model and its input data, it would be difficult to provide a comprehensive list of preprocessing steps.
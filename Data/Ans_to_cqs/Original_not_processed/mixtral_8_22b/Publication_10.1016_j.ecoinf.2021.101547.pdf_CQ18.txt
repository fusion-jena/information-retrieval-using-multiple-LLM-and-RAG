Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Details of the parameters and settings we used for these two training 
stages are summarized in Table 1.

other class(es). The network then performs a new round of learning on 
the  mini-batch  of  images—focusing  relatively  more  on  images  with 
higher  loss  scores—and  updates  its  internal  parameters,  before  pro-
ceeding to the next training iteration with a new mini-batch of images. A 
full cycle through the entire set of training images is referred to as an 
‘epoch’,  and  the  number  of  iterations  per  epoch  is  equal  to  the  total 
number of training images divided by the mini-batch size.

The  mini-batch  size  is  user-adjustable  (typically  in  binary-number 
increments,  e.g.  2,  4,  8,  16,  32,  etc.)  depending  on  the  data  set.  Too 
small  a  mini-batch  size  may  provide  insufficient  image  data  for  the 
network  to  meaningfully  learn from  some or  all  of  the  classes  within 
each iteration, while too large a mini-batch size may cause the network 
to  overfit  to  the  training  images.  Because  of  the  high  computational 
complexity  of  the  deep  learning  process,  the  mini-batch  size  is  ulti-
mately limited by the amount of memory available to the GPU (in our 
case 8 GB), factoring in the size of the images (in our case 224 × 224 
pixels × 3 bands) and the number of internal parameters in the network 
(25.6 million in the case of ResNet-50). Thus, we were limited to mini-

detection with region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell. 39 
(6), 1137–1149. 

Sasse, D.B., 2003. Job-related mortality of wildlife workers in the United States, 

1937–2000. Wildl. Soc. Bull. 31 (4), 1015–1020. 

Shahinfar, S., Meek, P., Falzon, G., 2020. “How many images do I need?” Understanding 
how sample size per class affects deep learning model performance metrics for 
balanced designs in autonomous wildlife monitoring. Ecol. Informa. 57, 101085. 

Stapleton, S., 2013. Alternative Methods for Monitoring Polar Bears in the North 

American Arctic. Doctoral Dissertation. University of Minnesota. University of 
Minnesota Digital Conservancy. https://hdl.handle.net/11299/162524. 

Stapleton, S., Atkinson, S., Hedman, D., Garshelis, D., 2014. Revisiting Western Hudson 
Bay: using aerial surveys to update polar bear abundance in a sentinel population. 
Biol. Conserv. 170, 38–47.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
During the training stage, the strategy implemented to monitor the model performance involves performing a new round of learning on a mini-batch of images. This focuses relatively more on images with higher loss scores, which helps the network adjust its internal parameters accordingly. After this, it proceeds to the next training iteration with a new mini-batch of images. A full cycle through the entire set of training images is called an 'epoch'. The number of iterations per epoch equals the total number of training images divided by the mini-batch size.

The mini-batch size can be adjusted based on the dataset and the resources available. It should not be too small, as it might not provide enough image data for the network to learn effectively from each class. On the other hand, if it's too large, the network could potentially overfit to the training images. Due to the high computational complexity of deep learning processes, the mini-batch size is ultimately limited by the amount of memory available to the GPU.

In summary, the strategy to monitor the model performance during training includes focusing on images with higher loss scores, updating the network's internal parameters, and adjusting the mini-batch size according to the dataset and available resources.
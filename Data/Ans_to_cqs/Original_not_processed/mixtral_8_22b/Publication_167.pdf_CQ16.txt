Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The data of a certain year are used as the test set, and the remaining data are the training and
validation set. The training and validation set ia divided into ten parts using the 10-fold cross-validation
method, in which nine of the parts are used as a training set and one is used for the validation set.
The percentage of the training, validation, and testing sets is 5.4:0.6:1. The training set is the data
sample used for model ﬁtting. The validation set is a set of samples set aside separately during the
model training. It can be used to adjust the hyperparameters of the model and to conduct a preliminary
evaluation of the model’s capabilities. The testing set is used to evaluate the generalization ability of
the ﬁnal model. However, it cannot be used as a basis for algorithm-related selection such as parameter
tuning and selection of features. The mean value of the ten results is used to estimate the accuracy of

(3) Training set: grid with neighborhood features of six years (1986, 2005, 2008, 2010, and 2013).
The features include the LU type, soil, topography, elevation, slope, aspect, distance to settlements,
distance to roads, distance to rivers in this grid unit, and the 4 neighborhood grid units.

(4) Constraint Factors: The statistical data and the quantity simulated by the backtracking of the
Markov model are combined, and the numbers of diﬀerent LU types in 2000 after modiﬁcation by
the simulation numbers are as follows (Table 1):

Table 4. Prediction correctness and error (Appendix A).

2013

2010

2008

2005

2000

1986

Average

hits

2.02%

2.75%

1.38%

3.96%

4.11%

2.51%

2.79%

null success

93.20% 91.59% 91.67% 91.35% 88.32% 88.21%

90.72%

misses

0.62%

false alarms

4.17%

hits

1.88%

1.63%

4.02%

2.92%

1.88%

5.07%

2.33%

0.57%

4.12%

4.30%

2.01%

5.57%

4.29%

3.61%

5.67%

2.22%

1.72%

4.77%

2.99%

null success

86.53% 85.82% 85.75% 83.61% 82.08% 82.78%

84.43%

misses

0.75%

1.47%

0.93%

0.23%

1.83%

3.90%

1.52%

false alarms

10.84%

9.79%

10.99% 11.86% 11.80% 11.10%

11.06%

hits

0.13%

0.23%

0.30%

0.22%

1.18%

0.70%

0.46%

DLURM

HLURM

CA-Markov

null success

89.68% 88.89% 89.30% 86.33% 79.80% 77.71%

85.29%

misses

2.50%

false alarms

7.69%

4.16%

6.72%

2.96%

7.44%

4.29%

4.92%

5.40%

4.04%

9.15%

14.09% 16.19%

10.21%

Remote Sens. 2020, 12, 3314

14 of 22

Figure 11. Prediction correctness and error.

Good performance on the training set, poor performance on the test set, and high variance are a
sign of overﬁtting. To prevent overﬁtting of the model, the regularization method is used, and the
dropout operation is added during the training stage. The neural network model of the dropout is
shown in Figure 6. In this paper, the 6th layer dropout rate is 0.1, and for the other layers, it is 0.

Figure 6. Neural network model of dropout.

Left: standard neural network with 2 hidden layers.
Right: An example of a reﬁned network generated by applying dropout to the network on the left.
The cross symbol indicates that this unit is not activated. When training the neural network, some
neurons in the neural network can be deleted, as shown in the ﬁgure, but all of the neurons should be
activated during the test.

In the model training, the cross-entropy is used as the loss function, and the Adam optimization

algorithm is used to update the weights of the neural network.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there isn't a direct mention of specific criteria used to determine when training is complete. However, we can infer from common machine learning practices that the criteria could involve monitoring the validation loss or accuracy. Typically, training continues until the validation loss plateaus or starts increasing, indicating possible overfitting. This suggests that the model might have started memorizing the training data instead of learning general patterns. Additionally, early stopping can also be employed, where training stops if the validation metric doesn't improve for a specified number of epochs. In this case, the 10-fold cross-validation method was used, suggesting that the average performance across multiple folds may serve as another criterion for determining completion.
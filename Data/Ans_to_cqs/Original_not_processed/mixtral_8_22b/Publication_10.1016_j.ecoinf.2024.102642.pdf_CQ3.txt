Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

directly  to  datasets  where  the  relevance  of  annotations  has  not  been 
evaluated.

During  the  campaign,  methods  are  proposed  to  assess  inter- 
annotator  variability  and  aggregate  annotation  sets.  Those  methods 
will help to define the minimal number of annotators needed [Dubus 
et al., 2023; Walter et al., 2022]. Increasing the number of annotators 
per sample could be helpful if the variability is too high. Finally, the two 
grouping  methods:  majority  voting  (Section  2.2.3)  and  soft  labeling 
(Section 2.3.2) can also be used to produce relevant annotation sets for 
deep learning. 

6. Conclusion 

In this study, new approaches were explored to improve the accuracy 
and  efficiency  of  cetacean  vocalization  detection  using  deep  learning 
models. Leveraging multi-annotation campaigns involving both expert 
and novice annotators aimed to address the challenges associated with 
the scarcity of ground truth data in underwater PAM studies.

platform  is  not  user-friendly,  annotators  will  become  disengaged  and 
cease  annotating  the  files.  As  annotation  campaigns  generally  aim  to 
annotate large datasets, online access to the data is recommended. It is 
necessary for the procedure to be automatic, from the first observation 
of data by the annotators to the gathering of all annotations. Moreover, 
the platform must unify the data representation for all users, in order to 
reduce  the  variability.  To  reduce  the  over-representation  of  the  first 
annotations, it is also important to keep all the annotations independent. 
We also strongly believe that the temporal context is an important factor 
for many underwater sound events: song context, periodic emission of 
vocalization  [Madhusudhana  et  al.,  2021].  Thus,  the  spectrogram 
should be proposed to the annotator in a chronological order, or be long 
enough  to  capture  the  long-term  temporal  context  of  targeted  sound

distinctive  annotation  profiles  influenced  by  the  complexity  of  vocalizations  and  the  annotators’  strategies, 
ranging from conservative to permissive. To address the challenges of annotation discrepancies, Convolutional 
Neural Networks (CNNs) are trained on annotations from both novices and the expert. The results show varia-
tions in model performance. Our work highlights the importance of annotation guidelines encouraging a more 
conservative  approach  to  improve  overall  annotation  quality.  In  an  effort  to  optimize  the  potential  of  multi- 
annotation  and  mitigate  the  presence  of  noisy  labels,  two  annotation  aggregation  methods  (majority  voting 
and  soft  labeling)  are  proposed  and  tested.  The  results  demonstrate  that  both  methods,  particularly  when  a 
sufficient number of annotators are involved, significantly improve model performance and reduce variability:

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide specific details about the data annotation techniques used in the deep learning pipeline, such as bounding box annotation or instance segmentation. However, it mentions several aspects related to data annotation. During the campaign, methods like majority voting and soft labeling are proposed to assess inter-annotator variability and aggregate annotation sets. These methods help determine the minimum number of annotators required. Additionally, increasing the number of annotators per sample may be beneficial if the variability is too high. The goal of these grouping methods is to create relevant annotation sets for deep learning.

Moreover, the study emphasizes the need for a user-friendly annotation platform to engage annotators and maintain their interest in annotating large datasets. Online access to data is suggested, along with automation of the process from initial data observation to collection of all annotations. Unifying data representation for all users is essential to minimize variability. Furthermore, preserving the independence of all annotations helps prevent over-representation of early annotations.

Lastly, the text discusses the use of Convolutional Neural Networks (CNNs) trained on annotations from both novices and experts. Results indicate variations in model performance based on different annotation strategies. Annotation guidelines promoting a more conservative approach are encouraged to enhance overall annotation quality. Two annotation aggregation methods—majority voting and soft labeling—are proposed and tested to optimize multi-annotation and decrease the impact of noisy labels. Both methods, especially when enough annotators participate, significantly improve model performance and lower variability.
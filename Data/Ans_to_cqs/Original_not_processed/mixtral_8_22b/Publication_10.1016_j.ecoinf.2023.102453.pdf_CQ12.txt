Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Vision Transformers (ViT) (Dosovitskiy et al., 2021) are based on an 
alternative attention-based model design rather than CNN. The trans-
former  architecture was first applied on Natural Language Processing 
(NLP)  problems.  The  recent  ground-breaking  research  outputs  of 
Generative  Pre-trained  Transformer  (GPT)  and  Bidirectional  Encoder 
Representations from Transformers (BERT) are all based on the trans-
former architecture. The concepts of attention were later applied to the 
image classification problems as Vision Transformers (Dosovitskiy et al., 
2021). The transformers use the relationship between words, termed as 
attention, whereas the vision transformers determine relationships be-
tween patches of images. ViT have since achieved performance similar 
to convolution-based architectures. There are many vision transformer 
architectures for image classification but we chose MobileViT (Mehta 
and Rastegari, 2022) models proposed as a small model for embedded

Neural architecture search (NAS) was used for developing the Effi-
cientNetV2B0 model, and for the optimization of parameter efficiency 
and  training  speed  (Tan  and  Le,  2021).  The  model  performance  was 
improved  using  progressive  learning  to  adapt  regularization  to  the 
image size (Tan and Le, 2021). EfficientNetV2 is a family of models (B0 
to B7), and B0 is the smallest variant in the family (Keras, n.d), with the 
other  models  progressively  increasing  in  performance  going  up  to 
EfficientNetV2B7.

For model deployment to an edge device, e.g., RPi, the model needs 
to be converted to the TensorFlow Lite format, which although reduces 
its size, but also reduces the modelâ€™s accuracy. The model can then be 
used on an edge device for classification of as yet unseen images. The 
TensorFlow or TensorFlow Lite model may also optionally be optimized 
for size, or latency depending on the application requirements and the 
edge device specifications. The RPi or similar edge devices can also be 
used to  run XAI  techniques such as  Grad-CAM  for generating  visuali-
zations through heatmaps. 

3.3. Transfer learning approaches for image classification

3.5. Hyperparameter optimization 

The model training process determines the values for the trainable 
parameters  of  a  model,  e.g.,  Table  4 shows  the  size  of  the  model pa-
rameters. In addition, a DNN model also has other parameters that need 
to be selected, e.g., batch size, that determines the model performance. 
The  model  parameters  define  the  model  and  are  termed  as  hyper-
parameters. Each hyperparameter has a range of values, from which an 
optimum  selection  can  improve  the  model  performance.  The  model 
hyperparameters could be in hundreds, presenting a very large search 
space, but these differ in their relative importance. The common ones for 
a DNN are learning rate, batch size, and dropout.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In this pipeline, two types of deep learning models are used - Vision Transformers (ViT) and EfficientNetV2B0.

Vision Transformers (ViT) are based on an alternative attention-based model design rather than Convolutional Neural Networks (CNN). They are inspired by the transformer architecture, which was initially developed for natural language processing tasks. However, they have been adapted for image classification problems. Specifically, MobileViT models are chosen due to their smaller size, making them suitable for embedded systems like Raspberry Pi.

On the other hand, EfficientNetV2B0 is part of the EfficientNetV2 family of models, ranging from B0 to B7. It was developed using neural architecture search (NAS) and optimized for parameter efficiency and training speed. This model uses progressive learning to adapt regularization to the image size.

Therefore, both Vision Transformers (specifically MobileViT) and EfficientNetV2B0 are employed in this pipeline.
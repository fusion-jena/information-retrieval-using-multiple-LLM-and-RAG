Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Hyperparameter optimization techniques make it simpler to search 
for an optimum hyperparameter configuration, however, it can take a 
significant amount of time depending on the number of hyperparameter 
values being searched (Nazir et al., 2020a). We selected the Keras Tuner 
(O'Malley et al., 2019) which provides Random Search, Hyperband, and 
Bayesian  Optimization  algorithms.  In  this  paper,  we  used  Hyperband 
and Bayesian Optimization as these two algorithms are superior to the 
other search algorithms available in the Keras Tuner.

3.5. Hyperparameter optimization 

The model training process determines the values for the trainable 
parameters  of  a  model,  e.g.,  Table  4 shows  the  size  of  the  model pa-
rameters. In addition, a DNN model also has other parameters that need 
to be selected, e.g., batch size, that determines the model performance. 
The  model  parameters  define  the  model  and  are  termed  as  hyper-
parameters. Each hyperparameter has a range of values, from which an 
optimum  selection  can  improve  the  model  performance.  The  model 
hyperparameters could be in hundreds, presenting a very large search 
space, but these differ in their relative importance. The common ones for 
a DNN are learning rate, batch size, and dropout.

In future work, we will investigate model optimization techniques 
for deploying the deep learning models to even smaller microcontroller 
devices. Such low power devices can further economize the batteries and 
run autonomously to provide near real-time ecological insights from the 
field.  We  would  also  research  other  model  fine-tuning  techniques  to 
improve the model performance by investigating model pre-training on 
large datasets from ecological studies. 

Funding 

This  research  did  not  receive  any  specific  grant  from  funding 

agencies in the public, commercial, or not-for-profit sectors. 

Declaration of competing interest 

The authors declare that they have no known competing financial 
interests or personal relationships that could have appeared to influence 
the work reported in this paper. 

Data availability 

Data will be made available on request.  

Appendix A. Appendix  

Table A1 
Class wise statistics for the Florida-Part1 dataset.  

Technique

Shahinfar, S., Meek, P., Falzona, G., 2020. “How many images do I need?” Understanding 
how sample size per class affects deep learning model performance metrics for 
balanced designs in autonomous wildlife monitoring. Eco. Inform. 57, 101085. 

Simoes, F., Bouveyron, C., Precioso, F., 2023. DeepWILD: wildlife identification, 

localisation and estimation on camera trap videos using deep learning. Eco. Inform. 
75, 102095. 

Stancic, A., Vyrouba, V., Slijepcevic, V., 2022. Classification efficiency of pre-trained 
deep CNN models on camera trap images. J. Imaging 8, 20. https://doi.org/ 
10.3390/jimaging8020020. 

Sun, C., Shrivastava, A., Singh, S., Gupta, A., 2017. Revisiting unreasonable effectiveness 
of data in deep learning era. In: 2017 IEEE International Conference on Computer 
Vision. 

Tan, M., Le, Q.V., 23 Jun 2021. EfficientNetV2: Smaller Models and Faster Training. 

arXiv:2104.00298v3 [cs.CV].

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there is no explicit information given about handling randomness in the deep learning pipeline, such as managing random seed values, within the provided context. However, one strategy commonly employed to manage randomness in deep learning pipelines involves setting fixed seeds for various libraries, including NumPy and TensorFlow, before running experiments. This ensures reproducibility across different runs and enables researchers to compare results accurately. Additionally, some practitioners use deterministic algorithms instead of stochastic ones when possible, reducing the impact of randomness on outcomes. Lastly, employing multiple trials with different initializations and averaging results can help mitigate the effects of randomness on overall performance.
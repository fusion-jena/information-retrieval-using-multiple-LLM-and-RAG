Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.3.  Model training 

2.3.1.  Training data preprocessing 

The  CNN  model  used  in  this  study  requires  equally  sized  input 
images. We chose a time-frequency input window size of 2 s as it is near 
the mean and median template duration across target call types (Fig. 2). 
Most  call  types  have  a  duration  below  1  s,  and  for  those  above  2  s,

learning, wherein a model already optimized for a similar dataset is re- 
trained with new data. We used a ResNet50 model, pre-trained on the 
ImageNet dataset, which contains over one million photo images across

1000 classes (Deng et al., 2009). Although ImageNet does not contain 
spectrograms,  models  pre-trained  with  the  dataset  learn  a  variety  of 
image features that have been successfully tuned to spectrogram clas-
sification  previously  (Lasseck,  2019;  Florentin  et  al.,  2020).  While 
previous  studies  have  found  optimal  performance  using  ensembles  of 
multiple CNN models (Kahl et al., 2017; Lasseck, 2019; Florentin et al., 
2020), in the interest of prediction efficiency and reasonable memory 
requirements, we chose to evaluate a single ResNet50 model.

Our implementation only included the feature extraction layers of 
ResNet50,  excluding  the  remaining  layers  used  for  ImageNet  classifi-
cation,  referred  to  as  the  network  “top”.  The  network  top  consists  of 
fully  connected  (FC)  layers  for  learning  a  predictive  model  from  the 
input  features.  An  FC  layer  consists  of  a  set  of  nodes,  each  of  which 
takes  a  weighted  sum  of  the  input's  values  and  passes  it  through  a 
transfer function. The weights of an FC layer are learnable. In our case, 
we used two FC layers separated by a drop-out layer. The drop-out layer 

6

J. LeBien, et al.   

Ecological Informatics 59 (2020) 101113

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there is no explicit mention of specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being applied in the deep learning pipeline. However, it can be inferred that some form of optimization technique was employed during the training process since the text mentions that the model was trained and optimized for a similar dataset. Additionally, the use of fully connected (FC) layers suggests that backpropagation was likely utilized for updating the weights of these layers during training. Nevertheless, without further information, it cannot be definitively stated what specific optimization techniques were used in the deep learning pipeline.
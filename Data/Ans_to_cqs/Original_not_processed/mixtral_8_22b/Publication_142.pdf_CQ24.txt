Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

For any given research question, ecologists and data scien-
tists should carefully consider the steps that might be required
to ensure the relevance and accuracy of AI-generated data for
any given research question. To aid this we have summarized
our experience into an eight-point list of questions which we
recommend researchers ask themselves when using AI classiﬁer
naturalists:

1. Does the spatial distribution of images ﬁt your needs? Im-
ages from social media are often aggregated in areas of
high population density or tourist hotspots. If the distribu-
tion is biased in some way, could this be accounted for in
subsequent analyses?

2. Can you ﬁlter images before classiﬁcation? For example,
ﬁltering can be done by carefully selecting your source

Access to the Pl@ntNet classiﬁcation tool is provided through a dedicated
API available at my.plantnet.org. The main feature of this API is a RESTful
JSON-based web service that can accept one to ﬁve images of a plant and re-
turns a list of likely species. The species are associated with classiﬁcation
scores (the softmax output of the CNN), as well as a list of matching images
retrieved from the database. To facilitate the implementation of future studies
based on the methodology of this paper, we have developed the ‘‘plantnet’’51
R-package allowing users to query the Pl@ntNet API. The package is available
online at https://github.com/BiologicalRecordsCentre/plantnet.

Biases in our data arise in part from differences between the
aims of the original data collectors (i.e., the photographers)
and our aims as biodiversity researchers and ecologists. For
example, the spatial distribution of our images was biased to-
ward areas where extensive managed gardens or other displays
exhibited large collections of ﬂowering plants (Figure 2). These
biases could be addressed by choosing alternative sources,
changing the search terms used, or pre-ﬁltering images. For
example, choosing to collect images from social media targeted
at outdoor enthusiasts (e.g., hikers), such as specialist Facebook
groups or ‘‘subreddits,’’ would be less likely to return images
from formal gardens. Images may also be biased taxonomically

reliably identiﬁed

In Brief
We apply newly developed AI image
classiﬁers to large social media image
datasets in order to assess whether new
datasets of biodiversity observations can
be generated in this way. We explore
biases in both the dataset of images as
well as in the ability of the AI image
classiﬁer to make accurate identiﬁcations
and propose a checklist of questions
researchers should ask themselves when
considering this approach to data
generation.

August et al., 2020, Patterns 1, 100116
October 9, 2020 ª 2020 The Authors.
https://doi.org/10.1016/j.patter.2020.100116

ll

ll

OPEN ACCESS

Article
AI Naturalists Might Hold the Key to Unlocking
Biodiversity Data in Social Media Imagery

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not directly address the handling of randomness in the deep learning pipeline, including the use of random seed values. However, it discusses the application of AI image classifiers to large datasets of social media imagery for generating new datasets of biodiversity observations. It highlights potential biases in these datasets and proposes a checklist of questions for researchers to consider when employing this approach.

To specifically address the query about handling randomness in the deep learning pipeline, common strategies include setting a fixed random seed value for reproducibility, using techniques like cross-validation to reduce variability due to random sampling, and applying ensemble methods to combine multiple models trained with different initializations. Additionally, hyperparameter tuning and regularization techniques can help mitigate overfitting caused by random fluctuations during training.
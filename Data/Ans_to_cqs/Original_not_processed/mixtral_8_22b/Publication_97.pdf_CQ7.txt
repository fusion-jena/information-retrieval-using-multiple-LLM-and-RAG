Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Remote Sens. 2020, 12, 1145

6 of 17

Training data were generated as batches of size (m, 256, 256, 3), where m is the batch size (m =
20 in our experiments). The batches consisted of sub-images of size 256 × 256 that were randomly
cropped out from the original satellite images presented in Table 1. We had a stream (internally, a
Python generator) of almost never repeated sub-images; these sub-images were combined into batches
and used for the neural network training. Satellite images for sites #1, #3, #5, #7–10 were used for
training and #2, #4, #6 for validation. Corresponding batches of mask data had shape (20, 256, 256, 1).
The network training assessment was performed on sub-images generated from image #2 (Table 1).
Images #4 and #6 were used for visualization and demonstration of the algorithm eﬃcacy.

58. Yu, L.C.; Sung, W.K. Understanding geometry of encoder-decoder CNNs. arXiv 2019, arXiv:1901.07647v2.
Ioﬀe, S.; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate
59.
shift. arXiv 2015, arXiv:1502.03167v3.
Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. Dropout: A simple way to prevent
neural networks from overﬁtting. J. Mach. Learn. Res. 2014, 15, 1929–1958.

60.

61. Evaluation of the CNN Design Choices Performance on ImageNet-2012. Available online: https://github.

com/ducha-aiki/caﬀenet-benchmark (accessed on 24 March 2020).

62. Abadi, M.; Agarwal, A.; Barham, P.; Brevdo, E.; Chen, Z.; Citro, C.; Corrado, G.S.; Davis, A.; Dean, J.;
Devin, M.; et al. TensorFlow: Large-scale machine learning on heterogeneous systems. arXiv 2016,
arXiv:1603.04467v2.

Therefore, having a batch size of 20 and performing typically up to 1500 epochs for training the

network, we used almost 30,000 diﬀerent augmented images of size 256 × 256.

2.4. Artiﬁcial Neural Network Architecture

The problem of forest damage identiﬁcation is a semantic segmentation problem. Eﬀorts to solve
such problems have recently made signiﬁcant progress due to artiﬁcial neural networks of complex
architecture, which are closely related to a general term referred to as deep learning [55–57].

Semantic segmentation is a pixel-wise classiﬁcation problem aimed at determining the class of
the particular pixel of the image it belongs to. It is usually handled by means of convolutional neural
networks (CNNs). As was noted in the introduction, one of such CNNs is U-Net, which is of an
encoder-decoder architecture [58].

High-intensity ﬂuctuations of the loss function shown in Figure 2, which corresponds to the best
set of parameter values, are caused by the speciﬁcity of the algorithm used at the training stage. We
did not have a prebuilt set of images to use for the training. We instead generated batches of training
images randomly, on the ﬂy, from source images presented in Table 1 (we randomly cropped source
images to 256 × 256 resolution and applied augmentation), and we never showed exactly the same

Remote Sens. 2020, 12, 1145

8 of 17

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In this study, the authors used satellite images for various sites to train their deep learning model. Specifically, they used satellite images for sites #1, #3, #5, #7-10 for training and #2, #4, #6 for validation. They did not mention any specific criteria used to split the data for training and validation sets. However, it can be inferred that the selection of sites for each set was based on the availability and quality of the satellite images. Additionally, the authors mentioned using a stream of almost never repeated sub-images, which suggests that they may have used some form of random sampling or shuffling to generate the training and validation datasets. Overall, while the exact criteria used to split the data were not explicitly stated, it appears that the authors used a combination of site selection and random sampling techniques to create their training and validation datasets.
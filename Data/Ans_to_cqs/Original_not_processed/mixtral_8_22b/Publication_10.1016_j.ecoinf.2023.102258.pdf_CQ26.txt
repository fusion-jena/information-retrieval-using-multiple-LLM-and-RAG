Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Many machine learning systems have pragmatic design constraints 
that  limit  the  range  of  durations  they  can  consider.  Our  template- 
matching method uses ranges directly inherited from the 5 annotated 
events, although there remain practical limits on very large templates, 
such as computer memory. In deep learning, long audio files are usually 
divided into shorter chunks (with fixed durations of e.g. 3 or 10 s), so 
that a whole batch can fit inside the limited memory of GPUs. To detect 
long  events,  detections  that  span  these  chunks  are  joined  together  in 
post-processing. This as well as other considerations meant that post- 
processing  of  outputs  was  an  important  aspect  of  all  strongly- 
performing systems.

Baselines 

Prototypical 

Mel +PCEN 

Systems submitted 
to the public 
challenge 

Template matching 

Yang et al. (2021) 

Lin 

Mel 

Tang et al. (2021) 

Lin + PCEN 

Du_NERCSLIP 

Mel +PCEN 

Liu_Surrey 

Mel +PCEN & 
delta-MFCC 

CNN 

n/a 

CNN 

CNN 

CNN 
framewise 

CNN 

n/a 

x-ent 

Proto 

x-ent 

Proto 
(modifed) 

Wu_SHNU (+Wu 
2023 ICASSP) DFSL 
Moummad_IMT 

Other 

Wolters 2021 arxiv 
Perceiver 

You et al. (2023) 
(ICASSP 2023) 

Mel 

Mel 

Mel 

Mel +PCEN 

CNN (ResNet) 

x-ent 

DFSL attentive 

No 

Pseudo-pos 

– 

Proto 

Dist:Proto 

TI, Retrain 

5 

Between-the-5 +
Pseudo-neg 
(SpecSim) 
Pseudo-neg 

CNN (ResNet) 

SCL 

CNN + CRNN 
+Perceiver 

Proto +RPN 
(R-CRNN) 

Posterior 

Finetune 

Between-the-5 

Dist:Proto 

No 

n/a 

5 

5 

AST 

Proto 

Proto 

Dist:Proto 

Finetune, TI 

Between-the-5 

5 + aug 

New 
templates 
Retrain (new 
pos + neg) 
Proto 

Finetune last 
layer 

DFSL 
attentive 
Finetune last 
layer 
Proto

The next system in rank, Liu_Surrey, implements a novel approach 
designed to optimise the contrast between positive events and negative 
prototypes. This, together with an adaptive segment length dependent 
on each target class, works well across all the evaluation sets. 

The problem of very different lengths of events across target classes 
was also directly addressed by other submissions. Both Martinsson_RISE 
and Zgorzynski_SRPOL implemented an ensemble approach where each 
individual model focuses on a different input size range. In Liu_BIT-SRCB 
this is explored through a multi-scale ResNet, and in Willbo_RISE with a 
wide ResNet containing many channels. Also in XuQianHu_NUDT_BIT, 
they implement a novel adaptive mechanism - squeeze/excitation block 
- designed to assign different weights to different channels of the feature 
map.

• Transductive  few-shot  learning  -  Meta  learning  methods  aim  to 
learn on scarce data in order to generalise to unseen tasks,  which 
makes the problem fundamentally difficult. In order to mitigate the 
difficulty, transductive based methods utilise the information present 
in the unlabeled examples from the query set to adapt the model and 
improve its predictions. In Liu et al. (2018), the samples in support 
and query set are jointly modelled as nodes of a graph and the pre-
diction on query set is conducted by label-propagation algorithm. In 
Hou  et  al.  (2019),  a  cross-attention  based  map  is  learnt  between 
support set and query set in order to make predictions on individual 
query examples.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The provided context does not explicitly mention any specific techniques for addressing data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, it discusses various approaches employed by different systems to improve their performance in handling events of varying durations and lengths. These include dividing long audio files into shorter chunks, joining detections spanning these chunks in post-processing, using adaptive segment lengths depending on each target class, implementing ensemble approaches focusing on different input size ranges, and employing multi-scale ResNets or wide ResNets containing many channels. Additionally, some systems use transductive few-shot learning methods like joint modeling of support and query set nodes as a graph or learning a cross-attention based map between support and query sets to improve predictions on individual query examples.
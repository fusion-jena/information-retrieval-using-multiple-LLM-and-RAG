Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We implemented a deep neural network algorithm
(Goodfellow et al. 2016)—IUC-NN—as an R package
(IUCNN, available at https://github.com/azizka/IUCNN)
based on the Python (version 3.7) TensorFlow (https://
www.tensorflow.org) library to predict the conservation
status of unevaluated orchid species. We based the pre-
dictions on 4 groups of features, derivable from digitally
available occurrence records: geographic (mean latitude
and longitude, longitudinal and latitudinal range, extent
of occurrence [EOO], area of occupancy [AOO], number
of locations [sensu IUCN], and occurrence records); cli-
matic (mean of 19 bioclim variables [Karger et al. 2017]);
biome (presence in 14 biomes [Olson et al. 2001]); and

anthropogenic (mean Human Footprint Index [Wildlife
Conservation Society & International Earth Science In-
formation Network 2005]).

We trained IUC-NN on all species with an IUCN RL as-
sessment and available occurrence records. Prior to the
training, we randomly split the data set into a training
set (90% of the entries) and a test set (10%). We used
20% of the training set for validation. Because the size
of the data set was comparatively small, we performed
cross-validation by shifting the validation set 5 times to
quantify the average validation cross-entropy loss and ac-
curacy. We then used the neural network with the lowest
cross-entropy loss across a range of models with differ-
ent numbers of hidden layers and subsets of features to
predict the conservation status of all orchid species at
2 levels: binary (possibly threatened vs. not threatened)
and detailed (CR, EN, VU, NT, and LC). See Supporting
Information for details on the network architecture and
training. We combined the IUC-NN assessments with dis-
tribution data from WCSP to show the number and pro-

Keywords: biodiversity, data quality, IUC-NN, IUCN Red List, machine learning, Orchidaceae, sampling bias

Evaluación Automatizada de la Conservación de la Familia Orchidaceae mediante Aprendizaje Profundo

Address correspondence to: A. Zizka, email alexander.zizka@idiv.de
Article impact statement: An automated conservation assessment with deep learning reveals global centers of orchid extinction risk.
Paper submitted June 11, 2020; revised manuscript accepted August 17, 2020.

This is an open access article under the terms of the Creative Commons Attribution-NonCommercial License, which permits use, distribution
and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes.

Conservation Biology, Volume 35, No. 3, 897–908
© 2020 The Authors. Conservation Biology published by Wiley Periodicals LLC on behalf of Society for Conservation Biology
DOI: 10.1111/cobi.13616

897

898

Orchid Conservation Assessment

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions that they used a deep neural network algorithm (IUC-NN), which could potentially include various regularization techniques such as dropout or L2 regularization. Cross-validation was also employed to quantify the average validation cross-entropy loss and accuracy, which can help detect and mitigate overfitting. Nevertheless, without further details regarding the implementation of these methods, it cannot be confirmed if dropout, L2 regularization, or other techniques were specifically utilized in this study.
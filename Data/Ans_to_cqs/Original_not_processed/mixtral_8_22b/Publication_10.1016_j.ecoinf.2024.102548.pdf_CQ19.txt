Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

5.1.4. Total model parameters 

Total  parameters  in  a  neural  network encompass  the  aggregate of 
trainable weights and biases, serving as a direct measure of the model’s 
architectural complexity. This parameter count is pivotal in defining the 
model’s capacity for feature extraction and pattern recognition. High- 
parameter  models  are  adept  at  discerning  intricate  data  patterns  but 
are  susceptible  to  overfitting,  particularly  in  scenarios  with  limited 
training samples. Furthermore, such models escalate computational and 
memory  requirements,  potentially  impeding  deployment  in  resource- 
constrained environments. 

5.1.5. Total layers in the model

3.2.4. ResNet-50 

ResNet-50 is a widely used deep residual neural network architecture 
that addresses the challenges of training very deep networks. It employs 
residual blocks with skip connections to enable the flow of gradients and 
facilitate the training of deeper models. ResNet-50 has been pre-trained 
on  large-scale  datasets,  allowing  it  to  capture  complex  patterns  and 
achieve strong performance. 

3.2.5.

InceptionV3 

InceptionV3 is an advanced CNN architecture that incorporates the 
concept of inception modules. These modules utilize parallel convolu-
tions  of  different  kernel  sizes  to  capture  multi-scale  features.  Incep-
tionV3  has  been  pre-trained  on 
large-scale  datasets  and  has 
demonstrated  excellent  performance  in  various  image  classification 
tasks. 

3.2.6.

InceptionResNetV2

4.3. Parameter setting 

The experiments were performed using GPU acceleration to expedite 
computations.  The  models  were  initialized  with  pre-trained  weights 
obtained from the ‘imagenet’ dataset. To tailor the models to the specific

In contrast, ResNet50, with a staggering 76,019,586 parameters and 
a  training  time  of  952.39  s,  exhibited  relatively  inferior  performance 
when  compared  to  the  MobileNet-based  models.  It  has  a  substantial 
number of parameters (68,495,586) and requires 1022.48 s for training, 
reflecting its complexity. ResNet50 is a deeper and more complex ar-
chitecture specifically designed to address the challenge of training very 
deep neural networks. While it has achieved remarkable performance in 
various computer vision tasks (Mukti and Biswas, 2019), its structure 
may not have been optimized for the specific classification task in this 
study. The increased depth and complexity of ResNet50 may have made 
it more difficult for the model to capture important features and pat-
terns, resulting in lower accuracy and higher loss.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given context, there isn't any explicit information about which specific deep learning framework was used to build the models mentioned - ResNet-50, InceptionV3, or InceptionResNetV2. However, these architectures can be implemented using popular deep learning libraries like TensorFlow and PyTorch. For instance, both TensorFlow and PyTorch provide implementations of ResNet-50, InceptionV3, and InceptionResNetV2 in their respective Model Zoos. Therefore, without further context, we cannot definitively state which framework was used to build these models.
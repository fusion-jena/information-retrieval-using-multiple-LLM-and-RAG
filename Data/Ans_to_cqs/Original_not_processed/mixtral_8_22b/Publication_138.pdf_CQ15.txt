Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The size of output result is 17,280. 

Eleventh layer: This layer is the full connected layer with 100 neurons. 
Twelfth layer: This layer is the Softmax activation function layer with 2 neurons. 
In  our  ICSNet,  we  adopt  the  inception  module  from  GoogLeNet,  but  only  three  inception 
modules  are  built  in  ICSNet.  And  we  also  add  more  convolution  layers  than  AlextNet,  but  the 
number of convolution layers is less than VGGNet and ResNet. Consequently, the tradeoff between 
accuracy and time consuming can be achieved by our ICSNet. This is basic idea of our work. 

Mathematical Biosciences and Engineering 

Volume 18, Issue 2, 1121–1135. 

 
1127 

3.  Experiments 

3.1.  Data description

(1) LeNet is firstly proposed by LeCun et al. in the 1990s, which includes three convolution layers, 
two  sampling  layers,  two  fully  connected  layers.  At  that  time,  it  is  difficult  to  run  LeNet  due  to  the 
limit of computation and memory capacity [21]. 

(2) AlexNet is proposed by Alex et al. and won the ILSVRC 2012 [22]. AlexNet achieves higher 
identifying accuracy than all the traditional machine learning algorithms. It is significant breakthrough 
for machine learning for classification. 

(3) GoogLeNet is proposed by Christian of Google and is the winner of ILSVRC 2014 [23], in 
which inception layers, including different receptive areas with different kernel sizes capturing sparse 
correlation patterns, are integrated into CNN.

(4) VGG is proposed by Simonyan et al. in 2014, in which the convolution filter is a 3 × 3 filter 
and  the  stride  is  2.  VGG-11,  VGG-16  and  VGG-19  respectively  include  11,  16  and  19  layers.  The 
Softmax layer is the final layer for classification [24]. 

(5) ResNet is proposed by Kaiming et al. in 2015 to address to the vanishing gradient problem and 
shows the excellent ability of classification [25]. The popular ResNet consists of 49 convolution layers. 
All  the  algorithms  in  our  experiments  are  coded  in  Matlab  R2019a.  The  values  of  initial 
parameters  are  randomly  generated  for  networks.  The  parameters  are  set  as  follows:  the  number  of 
epochs, learning rate drop factor, learning rate drop period, batch size are 200, 0.0004, 0.6, 30 and 128, 
respectively. The operating system of PC is Window 7 and the GPU is a NVIDIA with 8 GB memory. 

3.4. 

Results and discussions

We also see that, the more the number of the layers of networks, the larger the min-batch size is. 
But  the  VGG-19  is  just  the  opposite  due  to  its  too  much  layers.  The  inception  module  induces  the 
number  of  parameters  and  computing  cost  of  the  GoogLeNet  and  ICSNet,  and  remains  more  high 
identifying ability. 

Mathematical Biosciences and Engineering 

Volume 18, Issue 2, 1121–1135. 

 
1130 

Table 1. The results of six algorithms. 

Algorithms 

Accuracy 
(%) 

Precision 
(%) 

Recall 
(%) 

F1 
(%) 

AUC 

Min-batch 
size 

Time(s) 

The number of 
Layers 

ICSNet 

ICSNet_n 

LeNet 

AlexNet 

VGG-19 

GoogLeNet 

ResNet-18 

ResNet-50 

ResNet152 

93.00 

80.50 

80.50 

85.00 

91.00 

92.50 

91.10 

92.15 

93.00 

93.20 

81.60 

81.42 

84.35 

92.30 

92.64 

92.42 

92.50 

93.45 

93.30 

80.50 

80.64 

84.41 

92.00 

92.50 

92.15 

92.15 

93.30 

93.00 

80.50 

80.50 

84.38 

92.15 

93.00 

92.24 

92.50 

93.00 

94.00 

81.44

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there is no explicit mention of any specific optimization technique used in the deep learning pipeline such as Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), or others. However, some general training settings are mentioned, like the number of epochs, learning rate drop factor, learning rate drop period, and batch size. These hyperparameters are crucial for optimizing the model during training. For instance, the learning rate determines how quickly or slowly the weights are updated, while the batch size affects the efficiency and stability of the training process. Therefore, although the exact optimization algorithm isn't specified, these settings suggest that some form of optimization was employed to train the models effectively.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

10 

20/40/60/80 

20/40/60 

11 

20/40/60/80 

20/40/60 

128 
128 
128 
128 
128 
128 
64 
256 
512 
64/256/ 
512 
64/256/ 
512 

1024 
1024 
1024 
1024 
1024 
1024 
1024 
1024 
1024 

512 

2048  

Table 3 
Combination  of  the  convolutional  neural  network  deep  learning  model 
parameters.  

No 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 

Model 

Optimizer 

Learning Rate 

Without Dropout 

With Dropout 

RMSProp 

Adam 

SGD 

RMSProp 

Adam 

SGD 

0.001 
0.01 
0.001 
0.01 
0.001 
0.01 
0.001 
0.01 
0.001 
0.01 
0.001 
0.01  

EcologicalInformatics80(2024)1024958E. Joelianto et al.                                                                                                                                                                                                                               

Fig. 8. Performance test of each configuration.  

Table 4 
The standard deviation values for each performance metric.  

Configurations 

Accuracy 
(%) 

Precision 
(%)

Fig. 2. Model of deep learning layer.  

EcologicalInformatics80(2024)1024954E. Joelianto et al.                                                                                                                                                                                                                               

In a CNN, a convolution layer performs convolution operations be-
tween the pixels on the input and several kernels or filters. The math-
ematical equation of the convolution process is as follows, where f is the 
input data and h is the kernel size. 

G[m, n] = (f *h)[m, n] =

h[j, k]f [m (cid:0)

j, n (cid:0) k]

(6) 

∑

∑

j

k

Fig. 6. Feature extraction flowchart.  

EcologicalInformatics80(2024)1024957E. Joelianto et al.                                                                                                                                                                                                                               

of the FFT window, 1024. The feature extraction parameters that yielded 
the best performance were employed during the feature extraction phase 
in optimizing the CNN deep learning model. The results of model opti-
mization  were  acquired  by  employing  identical  performance  metrics 
used in optimizing the feature extraction parameters (Fig. 13).

The standard deviation values for each performance indicator were 
compared  to  determine  the  optimal  configuration  (Table  4).  Configu-
ration 8 was determined as the ideal parameter (number of bands: 60, 
number of frames: 40, hop length: 256, and length of the FFT window: 
1024)  owing  to  its  reduced  standard  deviation  value.  The  feature 
extraction parameters that yielded the best performance were employed 
during the feature extraction phase in the optimization process of the 
CNN  deep  learning  model.  The  model  optimization  outcomes  were 
derived  using  performance  criteria  identical  to  those  employed  to 
optimize  feature  extraction  parameters.  Fig.  9  illustrates  the  perfor-
mance evaluation of each model. Additionally, Model 1 exhibited the 
most favorable average performance outcome, with an 89.43% accuracy 
rate, 91.38% precision, 95.52% recall, and 0.816 F1-Score. The obtained

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there isn't any specific detail about the deployment process of the trained deep learning model. However, it can be assumed that after training the model, the authors might have serialized the model using popular libraries like TensorFlow or PyTorch. This would allow them to save the model architecture and weights into a file which could then be loaded later for prediction purposes. As for platform selection, depending upon their use case, they may choose cloud platforms such as AWS, Google Cloud, Azure etc. or edge devices based on hardware constraints. But without explicit details, this remains speculative.
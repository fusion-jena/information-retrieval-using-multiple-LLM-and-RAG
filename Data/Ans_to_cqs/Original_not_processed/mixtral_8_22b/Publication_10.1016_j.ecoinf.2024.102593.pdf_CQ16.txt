Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.6. Input selection in active learning 

Active learning enhances and facilitates the labour-intensive process 
of data generation, important in this field given the scarcity of labelled 
data or the need to generate training data to target certain species (Allen 
et  al.,  2021).  It  progressively  increases  the  training  data  by  selecting 
those  samples  most  likely  to  improve  model  performance  from  pre-
dictions that a model generates over unlabelled data. Iteratively training 
new models with additional labelled data selected from their predictions 
reduces  the  extent  of  manual annotation  (Shuyang  et  al.,  2020). In  a 
similar process as the example of Tolkova et al. (2021) discussed above, 
our proposed method of using model embeddings and dimensionality 
reduction to display model predictions in EV can contribute to selecting 
the best sample candidates for manual annotation. 

4.7. Closing in on rare events

4.2. Model selection 

The  most  apparent  use  of  EV  is  as  an  aid  in  selecting  the  most 
appropriate trained model for an intended bioacoustic task or selecting 
base models for use in transfer learning. It is common practice to train 
multiple  models  on  the  same  dataset,  but  there  is  little  else  beyond 
manual  review  and  performance  metrics  (statistical  comparisons  be-
tween the model predictions and known classes from a held-out part of 
the training dataset or an unrelated labelled datasets) to guide decision 
making. We propose to complement these metrics with the use of EV to 
assess and compare characteristics and suitability of trained models.

purposes all spectrograms use a minimum sound extract of 1.8 s that, in 
the case of short-labelled calls, encompasses the whole labelled section 
(Fig. 3c, labelled extents are noted between vertical red lines).

deployment, when a trained model is run through field recordings, we 
can again rely on EV to select possible calls of the Coxenâ€™s fig-parrot.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The provided context does not explicitly mention any specific criteria used to determine when training is complete. However, it does discuss some aspects related to model training and evaluation. For instance, it mentions iterative training of new models with additional labeled data selected from their predictions, which suggests that the training process might continue until satisfactory results are achieved. Additionally, it refers to statistical comparisons between model predictions and known classes from a held-out part of the training dataset or an unrelated labeled dataset to guide decision making. This implies that evaluating the model's performance against such datasets could potentially serve as a criterion for determining when training is complete. Nevertheless, without more explicit information regarding the completion criteria, one cannot provide a definitive answer based solely on the given context.
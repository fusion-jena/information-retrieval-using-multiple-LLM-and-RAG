Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

For the ViT models, we adapted the approach of pretraining deep
bidirectional transformers for language understanding (BERT) by Devlin
et al. (2018) for birdsong classification. Table 3 provides an overview of
the investigated ViT models with their respective parameterizations.
ViT-S/16, for example, denotes a ViT variant of reduced complexity with
an input patch size of 16 × 16. The sequence length of the transformer
models is inversely proportional to the square of the provided patch size.
In the following, ViT-B/16 is further investigated as it strikes a balance
between general model complexity and classification performance.

ViT models are often computationally costly compared to conven-
tional SED models. However, our 5 s long log-mel spectrograms are more
feasible to process. The influences of class imbalances were alleviated by
utilizing the binary cross entropy based focal loss as well as the so-called
scaled focal loss (Arunodhayan Sampathkumar, 2021). Our training
setup includes the Adam optimizer (Kingma and Ba, 2015) with a
concatenating cosine-annealing linear scheduler with an initial learning
rate of 0.0001, decaying by a factor of 0.001⋅learning rate, and a batch
size of 32. For validation, we used a 5-fold cross validation. Given the
class imbalance, we additionally deployed a data set sampler by
upsampling and duplicating randomly selected samples and augmenting
them. Our models were trained for 50 epochs without mixing up aug-
mentations as well as for 100 epochs with mixing up augmentations.
Early stopping was introduced to prevent overfitting when no further

preadapted to inhabit noisy urban areas? Behav. Ecol. 20, 1268–1273. URL: https://
academic.oup.com/beheco/article/20/6/1268/200758.

Mühling, M., Franz, J., Korfhage, N., Freisleben, B., 2020. Bird species recognition via
neural architecture search. In: Cappellato, L., Eickhoff, C., Ferro, N., N´ev´eol, A.
(Eds.), Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum,
CEUR-WS.org, Thessaloniki, Greece, pp. 1–13. URL: https://ceur-ws.org/Vol-2696/
paper_188.pdf.

Huang, G., Liu, Z., Maaten, L.V.D., Weinberger, K.Q., 2017. Densely connected

Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., Dosovitskiy, A., 2021. Do vision

convolutional networks. In: In: 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). IEEE, Honolulu, HI, USA, pp. 4700–4708. URL: https://
ieeexplore.ieee.org/document/8099726.

Iqbal, T., Helwani, K., Krishnaswamy, A., Wang, W., 2021. Enhancing audio

In order to alleviate the effects of present class imbalances, the focal
loss was introduced to the training process. To reduce the impact of
computational costs within the context of the BirdCLEF challenge in
terms of model runtime, our augmentation techniques were applied to
the training samples with a probability factor 50%. Therefore, future
investigations should also assess the differences in classification capa-
bilities when different probabilities are compared with each other.
When further assessing the resulting training and testing times in
Table 3, it is evident that the improved performance of ViT-B/16 comes
at a cost of increased training times by a factor of about 2.3 as well as
increased testing times by a factor of about 3.9. The observed data
augmentation trends – for increasing and decreasing classification ca-
pabilities – are merely robust against the investigated models. There-
fore, future investigations focusing on augmentation strategies should

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information regarding the method used for hyperparameter optimization, such as grid search or random search. However, it mentions specific values for certain hyperparameters, including an initial learning rate of 0.0001, a batch size of 32, and a concatenating cosine-annealing linear scheduler. These details suggest that some form of hyperparameter tuning has been performed, but the exact method remains unspecified.
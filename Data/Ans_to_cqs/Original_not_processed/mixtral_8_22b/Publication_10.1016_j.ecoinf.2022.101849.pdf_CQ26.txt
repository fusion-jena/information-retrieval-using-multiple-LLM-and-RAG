Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

by a random percentage.  

2.  Random reduction. We reduce each target in the composition by a 
random fraction. In this case, some targets can be reduced to 0.  
3.  Non-random (biased) reduction. We reduce targets randomly, as 
in the random reduction. However, only a few selected targets are 
being reduced. This  way, we imitate  the biased incompleteness of 
only some target labels. 

Then, we compare the mean absolute error (MAE) left in the data 
after we employ baseline approaches from Section 2.2 (taking no action 
and applying normalization) and partial imputation algorithm (Section 
2.3)  to  this  artificially  incomplete  data.  MAE  is  averaged  over  1000 
repetitions for each experiment. 

In the partial imputation algorithm, we use two types of clusters:

1. Introduction 

Target variables are usually fully labeled in the classical supervised 
machine learning setting. In real-world predictive tasks, however, labels 
are often scarce and/or noisy. Various definitions and terms are used in 
the  literature  to  describe  variants  of  noise  and  scarceness  of  labels 
(Allison, 2001; Xie and Huang, 2018; Nikoloski et al., 2021; Sun et al., 
2010; Gao et al., 2017; Alarc´on and Destercke, 2021; Van Engelen and 
Hoos, 2020), and each setting requires tailored approaches for exploit-
ing such target labels. In this study, we formulate a new computational 
setting for regression, where target labels are structurally incomplete. We 
computationally study this task via a case study in predictive modeling 
of global vegetation cover. 

1.1. The vegetation modelling task

Table 1 
An example training instance for different weakly-labeled settings. Here yj indicates the targets of an instance. Red indicates an 
error in the training data (deviation from ground truth) – it is not known to the user where errors occur. Note that in the weak 
j=1yj = 1 and yj⩾0). In our setting 
label setting, only 0s may be errors. Shading represents a constraint (in this example, that 
(structurally incomplete), the constraint may not be met, but errors are neither random deviations. Mixtures of these settings 
are possible, e.g., we may have structurally incomplete with missing labels, etc. 

∑
5

EcologicalInformatics72(2022)1018492R. Beigait˙e et al.

Various other terms have been used to describe weakly supervised 
learning tasks in the literature (Table 1). Weak labels (Sun et al., 2010) 
or  partial labels (Xie  and  Huang, 2018)  are  mainly  considered  in  the 
context  of  binary  labels.  They  often  include  different  types  of  noise 
coming from the labeling process and data sources and lacks constraints. 
Distribution learning (Gao et al., 2017) entails the constraint that the 
outputs  must  sum  to  unity,  but  this  constraint  is  already  met  in  the 
training data, unlike in our structural incompleteness setting.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide information on specific techniques used to address data bias during preprocessing of the deep learning pipeline, such as stratified splitting, oversampling, undersampling, or diverse data collection. Instead, it discusses three methods for creating artificial incompleteness in datasets: random reduction, random reduction with some targets reduced to zero, and non-random (biased) reduction. These methods are used to simulate various forms of label scarcity and noise found in real-world predictive tasks. After applying these methods, the authors evaluate the performance of their partial imputation algorithm using Mean Absolute Error (MAE) compared to baseline approaches.

However, based on general knowledge about addressing data bias, one could mention that stratified splitting involves dividing the dataset into subsets while maintaining the original class proportions. Oversampling increases the number of instances in minority classes, while undersampling reduces the number of instances in majority classes. Diverse data collection aims to gather data from multiple sources to ensure representation across all relevant categories.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, et al. 2019. “PyTorch: An Imperative Style, High-Performance Deep Learning Li-
brary.” In Advances in Neural Information Processing Systems 32, edited by H. Wallach, H.
Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett, 8024–35. Curran Associates,
Inc. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-
learning-library.pdf.

Rota, Christopher T., Marco A. R. Ferreira, Roland W. Kays, Tavis D. Forrester, Elizabeth L. Kalies,

27

William J. McShea, Arielle W. Parsons, and Joshua J. Millspaugh. 2016. “A Multispecies Occu-
pancy Model for Two or More Interacting Species.” Methods in Ecology and Evolution 7 (10):
1164–73. https://doi.org/https://doi.org/10.1111/2041-210X.12587.

Then we get the model architecture. For the sake of illustration, we use a resnet18 here, but we used
a resnet50 to get the full results presented in the main text.

learn <- cnn_learner(dls = dls,

arch = resnet18(),
metrics = list(accuracy, error_rate))

Now we are ready to train our model. Again, for the sake of illustration, we use only 2 epochs here,
but used 20 epochs to get the full results presented in the main text. With all pictures and a resnet50,
it took 75 minutes per epoch approximatively on a Mac with a 2.4Ghz processor and 64Go memory,
and less than half an hour on a machine with GPU. On this reduced dataset, it took a bit more than
a minute per epoch on the same Mac. Note that we save the model after each epoch for later use.

one_cycle <- learn %>%

fit_one_cycle(2, cbs = SaveModelCallback(every_epoch = TRUE,

fname = 'model'))

0.00%

train_loss

epoch
------ -----------
Epoch
1/2
|
Epoch
|
Epoch
|
Epoch
|

valid_loss
-----------
:
[0/36
:
[1/36
:
[2/36
:
[3/36

Yosinski, Jason, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. “How Transferable Are Features in
Deep Neural Networks?” In Proceedings of the 27th International Conference on Neural Information
Processing Systems - Volume 2, 3320–28. NIPS’14. Cambridge, MA, USA: MIT Press.

Zimmermann, Fridolin, Christine Breitenmoser-Würsten, Anja Molinari-Jobin, and Urs Breitenmoser.
2013. “Optimizing the Size of the Area Surveyed for Monitoring a Eurasian Lynx (Lynx Lynx)
Population in the Swiss Alps by Means of Photographic Capture-Recapture.” Integrative Zoology
8 (3): 232–43. https://doi.org/10.1111/1749-4877.12017.

28

This result calls for further work on the trade-offs between time and resources allocated to train
models with deep learning and our ability to correctly answer key ecological questions with camera-
trap surveys. In other words, while a computer scientist might be keen on spending time training
models to achieve top performances, an ecologist would rather rely on a model showing average
performances and use this time to proceed with statistical analyses if, of course, errors in computer-
annotated images do not make ecological inference flawed. The right balance may be found with
collaborative projects in which scientists from artificial intelligence, statistics and ecology agree on
a common objective, and identify research questions that can pick the interest of all parties.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about the specific hyperparameters such as learning rate or optimizer used in the deep learning model. However, it is mentioned that the fit\_one\_cycle function was used, which suggests that cyclical learning rates were employed during training. This method involves varying the learning rate over each cycle, starting with a minimum value, increasing it linearly until reaching a maximum value, then decreasing it back down to the minimum before beginning the next cycle. Additionally, the number of epochs used for training is specified as 2 for illustration purposes, but 20 epochs were actually used to obtain the final results.
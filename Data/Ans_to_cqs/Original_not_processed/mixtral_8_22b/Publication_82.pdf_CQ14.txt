Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

To summarize, we have a total of 60 experimental

conﬁgurations, which vary on the following parameters:

1. Choice of deep learning architecture:

AlexNet,
GoogLeNet.

2. Choice of training mechanism:

Transfer Learning,
Training from Scratch.

3. Choice of dataset type:

Color,
Gray scale,
Leaf Segmented.

4. Choice of training-testing set distribution:

Train: 80%, Test: 20%,
Train: 60%, Test: 40%,
Train: 50%, Test: 50%,
Train: 40%, Test: 60%,
Train: 20%, Test: 80%.

this paper, we have used the notation of

Throughout
Architecture:TrainingMechanism:DatasetType:Train-Test-
Set-Distribution to refer
experiments. For
to particular
instance,
to refer to the experiment using the GoogLeNet
learning
architecture, which was
on the gray-scaled PlantVillage dataset on a train—test
set distribution of
the notation
GoogLeNet:TransferLearning:GrayScale:60–40.

trained using transfer

60–40, we will use

Measurement of Performance
To get a sense of how our approaches will perform on new unseen
data, and also to keep a track of if any of our approaches are
overﬁtting, we run all our experiments across a whole range
of train-test set splits, namely 80–20 (80% of the whole dataset
used for training, and 20% for testing), 60–40 (60% of the whole
dataset used for training, and 40% for testing), 50–50 (50% of

the whole dataset used for training, and 50% for testing), 40–60
(40% of the whole dataset used for training, and 60% for testing)
and ﬁnally 20–80 (20% of the whole dataset used for training,
and 80% for testing). It must be noted that in many cases, the
PlantVillage dataset has multiple images of the same leaf (taken
from diﬀerent orientations), and we have the mappings of such
cases for 41,112 images out of the 54,306 images; and during all

Frontiers in Plant Science | www.frontiersin.org

3

September 2016 | Volume 7 | Article 1419

Mohanty et al.

So far, all results have been reported under the assumption
that the model needs to detect both the crop species and the
disease status. We can limit the challenge to a more realistic
scenario where the crop species is provided, as it can be expected
to be known by those growing the crops. To assess this the
performance of the model under this scenario, we limit ourselves
to crops where we have at least n > = 2 (to avoid trivial
classiﬁcation) or n > = 3 classes per crop. In the n > = 2 case,
dataset 1 contains 33 classes distributed among 9 crops. Random
guessing in such a dataset would achieve an accuracy of 0.225,

while our model has an accuracy of 0.478. In the n > = 3 case, the
dataset contains 25 classes distributed among 5 crops. Random
guessing in such a dataset would achieve an accuracy of 0.179,
while our model has an accuracy of 0.411.

of classes in our dataset), which feeds the softMax layer. The
softMax layer ﬁnally exponentially normalizes the input that it
gets from (fc8), thereby producing a distribution of values across
the 38 classes that add up to 1. These values can be interpreted
as the conﬁdences of the network that a given input image is
represented by the corresponding classes. All of the ﬁrst 7 layers
of AlexNet have a ReLu non-linearity activation unit associated
with them, and the ﬁrst two fully connected layers (fc{6, 7})
have a dropout layer associated with them, with a dropout
ratio of 0.5.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper does not provide explicit information about the optimization method used for tuning the hyperparameters of the models. Therefore, I cannot give a definitive answer regarding the specific methods like grid search or random search employed for hyperparameter optimization. However, considering common practices in machine learning research, popular techniques include manual tuning, grid search, random search, Bayesian optimization, and others. Without further details, one can only speculate about the exact approach used in this study.
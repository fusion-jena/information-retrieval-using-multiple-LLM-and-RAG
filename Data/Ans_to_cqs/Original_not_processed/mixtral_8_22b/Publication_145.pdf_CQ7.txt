Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

the validation dataset and the model segmented 1195 objects.

as a sum of two terms: binary cross-entropy and Dice coefﬁcient-related loss of the three predicted
masks [50–52]. We used the optimizer RMSprop (unpublished, adaptive learning rate method proposed
by Geoff Hinton here http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
with an initial learning rate of 0.0001. We trained our network for 300 epochs with 24 images per batch.

∼

80(cid:176)W70(cid:176)W60(cid:176)W50(cid:176)W40(cid:176)W10(cid:176)S0(cid:176)NBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBrazilian Amazon forestBRAZIL05001000 km(a)01020 kmN62(cid:176)W61.8(cid:176)W61.6(cid:176)W61.4(cid:176)W61.2(cid:176)W61(cid:176)W10.3(cid:176)S10.2(cid:176)S111111111222222222333333333444444444555555555666666666777777777888888888999999999Forest FormationsOther Non Forest Natural FormationAgriculture or PastureWater bodiesregions of interestGeoEye image extents(b)Remote Sens. 2020, 12, 2225

5 of 20

2.3. Forest Cover Mask and Clear-Cut Deforestation History from PRODES

×

×

3.7. Algorithm

The model was coded in the programming language R [48] with Rstudio interface to Keras [51,52]
2–20 hours using GPU of an Nvidia
and Tensorﬂow backend [53]. The training of the models took
RTX2080 graphics card with 8 GB of dedicated memory. Prediction using GPU of a single tile of 16,384

∼

16,384 pixels (

∼

×

67.10 km2) took approximately 6 min.

Remote Sens. 2020, 12, 2225

8 of 20

4. Results

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there is no explicit mention of how the data was split into training, testing, and validation sets for the deep learning model. However, it can be inferred that some form of splitting was done since the text mentions a validation dataset and the use of a validation set during training is common practice in machine learning. Typically, datasets are randomly divided into three subsets: training, validation, and testing. The training set is used to fit the model, while the validation set is used to tune hyperparameters and prevent overfitting. Finally, the testing set is used to evaluate the final performance of the model. In this case, we cannot provide specific percentages or ratios used for splitting the data due to insufficient information.
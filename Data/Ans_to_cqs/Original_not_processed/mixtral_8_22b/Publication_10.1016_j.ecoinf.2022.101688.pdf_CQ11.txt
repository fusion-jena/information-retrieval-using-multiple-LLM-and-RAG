Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We  used  four  experiments  to  investigate  the  effects  of  transfer 
learning,  especially  within  the  context  of  data  scarcity.  More  specif-
ically, we attempt to answer the question “can a CNN be successfully 
trained on very few verified calls?”. By answering this, this would enable 
bioacoustics researchers to spend less effort in manually labelling calls 
prior to training CNNs. Typically, a researcher would need to manually 
label a large quantity of data – a labour intensive and time consuming 
task. We randomly selected a subset of 25, 50, 100, and 200 spectro-
grams within the presence class. In order to overcome class imbalance 
issues we randomly augmented the presence spectrograms by applying a 
time-shift  operation  to  generate  enough  synthetic  spectrograms  such 
that the number of spectrograms in both classes were equal. The time- 
shifting  operation  involved  taking  the  starting  time  of  a  spectrogram

dataset)  with  46  softmax  units.  The  spectrograms  were  duplicated  to 
meet  the  input  of  ResNet50  which  expects  3  channels.  Zhong  et  al. 
(2020) compared VGG16 that was randomly initialised to a ResNet50 
model that was  pre-trained  on ImageNet.  In both  cases, a  colour mel 
spectrogram was input into the network. The spectrograms were resized 
to match the network's input of 224 by 224. Their models were applied 
to bird and amphibian vocalisations. ResNet50 pre-trained on ImageNet 
was also used by LeBien et al. (2020) whereby the pre-trained feature 
extractor was used and then two fully connected layers were added to 
the CNN. Zhong et al. (2021) applied ResNet50 to a birdsong dataset 
that contained three classes (two bird presence and one absence). The 
CNN was  pre-trained on ImageNet and  the fully connected layer, fol-
lowed by a dropout and an output layer was added to the CNN that was

Since CNNs require a fixed input size, we studied the vocalisations 
within the presence class for each dataset to determine the character-
istics of the calls which would allow us to create fixed input, which we 
refer to as segments. For example, the Hainan gibbon calls vary from 2 to 
9 s (Dufourq et al., 2021), and thus a suitable input size was 4 s to ensure 
that the smallest call would fit within the segment. A longer input (>4 
seconds) would result in CNNs with more network parameters, an un-
desirable consequence as this would increase the chances to overfit. A 
shorter input (<4 s) would not contain enough information, especially 
in cases where the individual pulses that make up a call are long. A short 
input could omit parts of the call. Preliminary experiments were con-
ducted on the different datasets to minimise the input length as much as 
possible,  thus  minimising  network  parameters.  The  characteristics  of

this case a CNN – will consistently perform well across all applications 
and datasets. Table 4 shows that, VGG16 for example, obtained the best 
results on three configurations, but also did not achieve equally as good 
results  on  other  configurations.  ResNet101V2  and  ResNet152V2  on 
average  performed  well  across  nearly  all  configurations  and  we  thus 
recommend that either of these architectures are used as a starting point 
for  researchers  wanting  to  use  pre-trained  CNNs  for  bioacoustics 
research. 

The experiments presented in Fig. 6 reveal that if very little data is 
available – in our case 25 examples – that pre-trained ResNet152V2 with 
the feature extractor frozen can yield good performance (up to 82% F1 
score  on  the  lemurs,  77%  on  the  gibbons  and  75%  on  the  whydah 
dataset).  This  suggests  that  when  conducting  an  acoustic  survey,  a

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given context, there is no explicit mention or reference to any specific data repository link for the deep learning pipeline discussed in the text. Therefore, I cannot provide a direct answer to your query. It might be helpful to refer back to the original source material or contact the authors directly for further clarification.
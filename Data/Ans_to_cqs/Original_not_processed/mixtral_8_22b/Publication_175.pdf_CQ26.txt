Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Comput. Vis. 211–252. 

Salman, A., et al., 2016. Oceanography : Methods Fish Species Classification in 

Unconstrained Underwater Environments Based on Deep Learning, pp. 570–585. 

Shiu, Y., et al., 2020. Deep Neural Networks For Automated Detection of Marine 

Mammal Species, pp. 1–12. 

Stuart-smith, R.D., et al., 2013. Integrating abundance and functional traits reveals new 

global hotspots of fish diversity. Nature 501 (7468), 539–542. 

Sun, Q., Chua, Y.L.T., 2018. Meta-transfer learning for few-shot learning. Conf. Comput. 

Vis. Pattern Recognit. 403–412.

Wong, S.C., Mcdonnell, M.D., Adam, G., Victor, S., 2016. Understanding data 

augmentation for classification : when to warp ?. In: 2016 International Conference 
on Digital Image Computing: Techniques and Applications (DICTA), pp. 1–6. 
Yanbin, Liu, et al., 2019. Learning to Proagate Labels: Transductive Propagation Network 

for Few-shot Learning. arXiv preprint arXiv:1805.10002, pp. 1–14. 

Young, H.S., Mccauley, D.J., Galetti, M., Dirzo, R., 2016. Patterns, causes, and 

consequences of anthropocene defaunation. Annu. Rev. Ecol. Evol. Syst. (August), 
333–358. 

Zhuang, P., Wang, Y., Qiao, Y., 2018. Wildfish : a large benchmark for fish recognition in 
the wild. In: Proceedings of the 26th ACM international conference on Multimedia, 
2, pp. 1301–1309. 

Zintzen, V., Anderson, M.J., Roberts, C.D., Harvey, E.S., Andrew, L., 2017. Effects of 

latitude and depth on the beta diversity of New Zealand fish communities. Sci. Rep. 7 
(July), 1–10. 

EcologicalInformatics63(2021)1013206

raw data instead of augmented data in few-shot learning paper is that 
with very few training samples and few conditions, the risk of overfitting 
by using the same image modified multiple times is far greater than in 
classic approaches with important datasets with many conditions.

1)  We  trained  a  classic  DL  algorithms  architecture  with  our  biggest 

dataset AT0 as a baseline for the DL accuracy;  

2)  We  trained  the  same  DL  architecture  with  the  same  hyper- 
parameters (e.g. model architecture and training process) but on a 
much  more  limited  dataset  (AT1).  Hyper-parameters  are  the  pa-
rameters  defining  the  architecture (number of  layers, number and 
size of convolutions, connections between layers) and the training 
process  of  a  Deep  Model  (learning  rate,  neurone  activation,  back- 
propagation compotation).;  

3)  We trained the same DL architecture with limited datasets obtained 
by  subsampling  T0  to  250  and  500  images  per  class  (here  after 
“species” when we are referring to our experiments), corresponding 
to 2500 and 5000 thumbnails in AT0;

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, based on general knowledge, some common techniques used to mitigate data bias include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into subsets or strata, ensuring each subset maintains the original distribution of classes or labels. This technique helps maintain balance among different categories within the dataset.

Oversampling is a method where additional copies of minority class examples are added to the dataset to achieve better balance. Conversely, undersampling reduces the majority class by randomly removing instances from it until the desired balance is achieved. Both methods aim to create a balanced dataset, which can improve the performance of machine learning models.

Lastly, diverse data collection focuses on gathering data from various sources, environments, and conditions to ensure the dataset represents real-world scenarios accurately. This approach can help reduce biases introduced due to limited or skewed data representation.

While these techniques are commonly employed to tackle data bias, the given context does not explicitly mention their use in this particular study.
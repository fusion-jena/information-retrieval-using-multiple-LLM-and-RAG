Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Build a three-layer back propagation (BP) artificial neural network 
using MATLAB R2014b (Lin et al., 2021). The first layer (i.e. the input 
layer) corresponded to the aCDOM(254) and Fn(355) respectively. The 
intermediate layer (i.e. hidden layers) and the last layer (i.e. the output 
layer)  comprised  a  single  neuron,  that  corresponded  to  the  predictor 
(TSI). Set the maximum training times to 10,000 and the training error 
to 0.001. 64 water samples were utilized for model calibration, with the 
remaining  used  for  validation.  Validation  accuracy  was  assessed  by 
calculating the coefficient of determination (R2) and root-mean-square 
error (RMSE) between the measured and predicted TSI. The RMSE was 
determined using Eq. (6). 

RMSE =

√

∑n

̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅
1
n

[Mea.TSI (cid:0) Pre.TSI]2

×

1

(6)  

where Mea,TSI is the measured TSI and Pre.TSI is the TSI predicted using 
the neural network model. 

3. Results

Based on the Trophic State Index, which ranges from 31 to 67, all 

Fn(355) values ranged from 1.16 to 2.97, with the average increasing 

Table 1 
The physio-chemical parameters of water samples classified by trophic state from the study area.  

Trophic state 

pH 

◦
Water temperature (

C) 

+
NH4

-N (mg/L) 

(cid:0)
NO3

-N (mg/L) 

TN (mg/L) 

TP (mg/L) 

Chl-a 
(μg/L) 

DOC (mg/L) 

Mesotrophic 
Light-eutrophic 
Middle-eutrophic 

8.55 ± 0.36 
8.58 ± 0.27 
8.42 ± 0.18 

16.53 ± 4.46 
16.47 ± 5.33 
12.71 ± 2.61 

0.06 ± 0.03 
0.04 ± 0.02 
0.14 ± 0.07 

0.15 ± 0.14 
0.09 ± 0.06 
0.05 ± 0.04 

0.66 ± 0.26 
0.99 ± 0.30 
1.73 ± 0.34 

0.04 ± 0.02 
0.05 ± 0.02 
0.08 ± 0.02 

11.06 ± 6.17 
25.57 ± 13.30 
67.39 ± 24.67 

5.71 ± 1.59 
6.60 ± 0.96 
7.30 ± 2.70

To quantify the relative parameter contributions of the parameters to 
the characteristics of DOM, the least absolute shrinkage and selection 
operator (LASSO) model was used to investigate the multicollinearity 
between the variables, and the analysis with lars() took place in the R- 
studio lars package. By imposing a penalty on the size of coefficients, the 
LASSO technique shrinks less important variables’ coefficients to zero, 
thus performing variable selection and allowing for the identification of 
the most relevant variables (Mukherjee et al., 2019). This model used 
+
environmental factors (pH, water temperature), nutrients (TP, TN, NH4
- 
(cid:0)
N, and NO3
-N), and phytoplankton (Chl-a) as predictors as predictors, 
with Fn355 as the response variable.

3. Results 

3.1. Water quality characteristics 

(cid:0)
-N,  NO3

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In the given context, there is no explicit mention of specific hyperparameters such as learning rate or optimizer. However, we can infer some hyperparameters based on the information provided.

The text mentions building a three-layer backpropagation (BP) artificial neural network using MATLAB R2014b. The number of layers and their respective functions (input, hidden, and output) can be considered as hyperparameters. In this case, the input layer corresponds to aCDOM(254) and Fn(355), while both the intermediate (hidden) and output layers consist of a single neuron corresponding to the predictor TSI.

Additionally, two other hyperparameters mentioned are the maximum training times set to 10,000 and the training error threshold set to 0.001. These values determine when the training process should stop, either after reaching the specified number of iterations or achieving the desired level of accuracy.

However, without further details about the implementation, it is not possible to provide more specific hyperparameter values like learning rate or optimizer type.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.2. Data 

The data used here was provided as part of the EarthNet2021 Chal-
lenge and consists of 23,904 training datacubes located across Europe

Recurrent architectures are not the only means for capturing time 
dependencies effectively. In recent years, Transformer-based architec-
tures  (Vaswani  et  al.,  2017)  have  led  to  remarkable  successes  in 
numerous applications - besides natural language processing (Jaderberg 
et al., 2015; Meinhardt et al., 2022; Zhao et al., 2021), including vision 
tasks (Dosovitskiy et al., 2021). Since transformers are not conceived in 
a  sequential manner, they exhibit multiple advantages over  recurrent 
architectures, including a more direct gradient flow, a higher level of 
parallelizability  (Zeyer  et  al.,  2019)  and  allowing  for  effective  self- 
supervised pre-training schemes (Devlin et al., 2018). In our efforts to 
use  a  Transformer  version  for  video  prediction,  similar  to  Liu  et  al. 
(2020),  we  encountered  significant  memory  limitations  even  after 
decreasing the hidden channel dimension and resorting only to single

Convolutional LSTM network: A machine learning approach for precipitation 
nowcasting. In: Advances in Neural Information Processing Systems, pp. 802–810. 
Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y., 
2015. Show, attend and tell: Neural image caption generation with visual attention. 
In: International Conference on Machine Learning. PMLR, pp. 2048–2057. 
Yu, X., Orth, R., Reichstein, M., Bahn, M., Klosterhalfen, A., Knohl, A., Koebsch, F., 

Migliavacca, M., Mund, M., Nelson, J.A., Stocker, B.D., Walther, S., Bastos, A., 2022. 
Contrasting drought legacy effects on gross primary productivity in a mixed versus 
pure beech forest. Biogeosciences 19 (17), 4315–4329. 

Yuan, Z., Zhou, X., Yang, T., 2018. Hetero-convlstm: A deep learning approach to traffic 

accident prediction on heterogeneous spatio-temporal data. In: Proceedings of the 
24th ACM SIGKDD International Conference on Knowledge Discovery & Data 
Mining, pp. 984–992.

In this work, we resorted to purely deterministic variants of these 

architectures.  The  models  were  implemented  using  the  deep  learning 
framework PyTorch Lightning (Falcon, 2019) which is built on top of 
PyTorch  (Paszke  et  al.,  2019)  and  enables  improved  scalability.  The 
hyperparameters were tuned using an Optuna-based (Akiba et al., 2019) 
hyperparameter optimization procedure.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't any information available regarding the specific data augmentation techniques applied in the deep learning pipeline mentioned in the given context. Therefore, I cannot provide a helpful answer to this query based on the provided information.
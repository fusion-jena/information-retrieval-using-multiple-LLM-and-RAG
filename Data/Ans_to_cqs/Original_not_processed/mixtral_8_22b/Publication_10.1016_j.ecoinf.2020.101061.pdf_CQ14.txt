Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.3. Model evaluation

forest type and used to predict another, performance generally decreased, with better performance when forests
were more similar in structure. However, when models were pretrained on data from other sites and then ﬁne-
tuned using a relatively small amount of hand-labeled data from the evaluation site, they performed similarly to
local site models. Most importantly, a model ﬁt to data from all sites performed as well or better than individual
models trained for each local site.

To assess generalization among sites, we performed three types of
experiments that used diﬀerent combinations for hand-annotations and
pretraining data (Fig. 2). The ﬁrst experiment is to use pretraining and
hand-annotated data to predict the evaluation data from the same site
(‘within-site’). The next setup is to use the pretraining data and hand-
annotated from the same site to predict the evaluation data from a
diﬀerent site (‘cross-site’). For example, using each of the within-site
models, we can test the ability for a model to predict tree conditions in
each of the other geographic sites, creating a matrix of cross-site pre-
dictions. To assess generalization without local pretraining data, we
tested a model training using pretraining data from all other sites, but
hand annotations from the same site as the evaluation data (‘transfer-
learning’). For example, the transfer learning model for Oak Woodland
used the hand-annotations from Oak Woodland, but the pretraining

Ecological Informatics 56 (2020) 101061

Fig. 3. For each site, results of our proposed workﬂow for the evaluation data. The proposed model is compared to three existing LiDAR-only implementations from
the commonly used lidR package. The proposed approach was evaluated at each of the 0.1 probability score intervals between 0 and 1. The probability threshold of
the best performing model in our approach, calculated by f-score, is shown in black. Error bars show the variance in recall and precision based on ﬁve runs of hand-
annotation training for each probability cutoﬀ at each site.

4. Discussion

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide information on how the hyperparameters of the model are optimized. Therefore, it is not possible to determine if grid search, random search, or any other method is used for this purpose.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Remote Sens. 2019, 11, x FOR PEER REVIEW 5 of 22  These results will guide future large-scale spatial wetland inventory efforts in Canada. If deep 154 learning techniques are found to generate better products, new wetland inventories projects should 155 adopt deep learning workflows. If shallow learning methods still produce comparable results, they 156 should continue to be used in wetland inventory workflow; however, deep learning architectures 157 should still remain an active area of research with regard to methods for wetland mapping. 158 2. Methods  159 2.1. Study Area 160 Our study area includes the Boreal Natural Region (BNR) of Alberta, Canada, along with parts 161 of the Canadian Shield, Parkland, and Foothills Natural Regions (Figure 2). The study area comprises 162 60% (397,958 km2) of the total area of Alberta. Elevations range from 150 m above sea level in the 163 northeast to 1100 m near the Alberta–British Columbia border [53]. 164  Figure 2. Location of the study area

The shallow learning classiﬁcation model was done with the XGBoost algorithm [75]. XGBoost
was used since it has been shown to be one of the better performing shallow learning models in
machine learning competitions [1], although it has limited use in remote sensing literature. It has been
the most popular shallow learning algorithm in Kaggle competitions since 2014 [1]. Early work on this
project showed XGB models slightly out performing random forest and boosted regression tree models.
We used the xgboost package [75] in R Statistical software [76]. The inputs into the XGBoost model
were: Anthocyanin Reﬂectance Index (ARI), delta Normalized Diﬀerence Vegetation Index fall–spring
(dNDVI), POLr, Red Edge Inﬂection Point (REIP), Topographic Position Index (TPI), Topographic
Wetness Index (TWI), Multi Resolution Index of Valley Bottom Flatness (VBF), VH, dVH (Table 1).
These inputs were the indices shown to be important for wetland class mapping, while also having low

region. The insets zoom in on important wetland landscapes in the Boreal region. Top-left: Zama Lake wetland area; bottom-right: Utikuma Lake; top-right: The Peace Athabasca Delta; and bottom-right: the McClelland Lake fen/wetland complex. 4. Discussion 381 This study produced two large-scale wetland inventory products using a fusion of open-access 382 satellite data, and machine learning methods. The two machine learning approaches that were 383 compared—convolutional neural networks and XGBoost—demonstrate a decent ability to predict 384 wetland classes and upland habitat across a large region. Some wetland classes, such as bog and 385 swamp, proved to be much harder to map. This is made clear in the relative F1-scores of the wetland 386 classes (Figure 3). In the comparisons to the photo-interpretation validation data sets (Table 3), it is 387 clear that the CNN model outperforms the XGB model in terms of overall accuracy, Kappa statistic, 388 and per-class F1-score. As expected,

Remote Sens. 2020, 12, 2

4 of 20

data sets: Two generated from photo interpretation and one ﬁeld validation data set. These results
will guide future large-scale spatial wetland inventory eﬀorts in Canada. If deep learning techniques
are found to generate better products, new wetland inventories projects should adopt deep learning
workﬂows. If shallow learning methods still produce comparable results, they should continue to
be used in wetland inventory workﬂow; however, deep learning architectures should still remain an
active area of research with regard to methods for wetland mapping.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, there isn't any explicit information about handling randomness in the deep learning pipeline, specifically regarding random seed values. However, some common strategies to manage randomness in deep learning pipelines include:

1. Fixing the random seed: By setting a fixed random seed value, you can ensure reproducibility in your experiments. This means that every time you run your code, you get the same results, which makes debugging easier and allows others to reproduce your findings.

2. Using multiple seeds: To avoid overfitting or bias towards specific initial conditions, you can train your model with different random seeds and average their performance. This helps in understanding the variability of your model's performance due to random initialization.

3. Stratified sampling: When dealing with imbalanced datasets, stratified sampling ensures that each class is represented proportionally during training, reducing the impact of randomness on model performance.

4. Cross-validation: Performing cross-validation by splitting your dataset into k folds and iteratively training and testing your model on different subsets can help reduce the effect of randomness on model evaluation.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

72 
72 
128 
64 

DN-3 
72 
72 
256 
64 

36 
36 
256 
128 

DN-4 
36 
36 
512 
128 

18 
18 
512 
256 

DN-5  
18  
18  
1024  
256   

9 
9 
512 
256         

networks often fail in extracting global information from shallow layers 
because of the small receptive fields (Liu et al., 2019b; Liu et al., 2021). 
For  creating  feature  maps  with  much  global  information,  multiple 
dilated convolutions are used for shallow layers (Zhao et al., 2020)— 
which, however, entail more computation resources. U2-Net defines a 
two-level  nested  model  (i.e.,  a  stack  of  nested  encoder-decoder)  to 
capture the contextual information in different scales at a moderate level 
of computation cost.

2.3. Model training 

Our study can be categorized as a binary classification. We supervise 
TrunkNet’s training with the Binary Cross-Entropy (BCE) loss function, 
which  is  a  commonly  employed  loss  function  in  supervised  deep 
learning (Zhao et al., 2019b). The BCE loss function measures the dif-
ference between the predicted probability and the actual value. Mini-
mizing this difference indicates a better training effect. Specifically, the 
BCE loss function in use is expressed with 

L BCE = (cid:0)

∑H

∑W

x=1

y=1

G(x, y)logSpre(x, y) + (1 (cid:0) G(x, y))log(1 (cid:0) Spre(x, y)),

(3)  

where (x,y) is the pixel coordinate, H and W are the height and width of 
the image, respectively, G is the ground-truth, and Spre  is the final pre-
dicted saliency map.

We  also  conduct  a  comparative  analysis  of  total  training  time, 
average  inference  time,  model parameters,  and  FLOPs  between  Trun-
kNet and the other 10 SOTAs, aiming to evaluate the time and compu-
tational performance. The experimental results are presented in Table 5. 
Noticeably, TrunkNet is built entirely on the proposed MIF block that 

needs to be trained from scratch, so necessitates a longer training time in 
comparison to models built on pre-trained backbones. Since TrunkNet 
prioritizes competitive performance, there may be a limitation in terms 
of  time  efficiency.  However,  TrunkNet  achieves  an  average  inference 
time of only 58.41 ms per image, that is 17.12 frames per second (FPS), 
which  is  still  above  average.  Furthermore,  TrunkNet  showcases

2.2.1. Overall architecture 

As indicated in Fig. 4, the presented TrunkNet model can be divided 
into two parts (or stages): the feature extraction and aggregation. The 
two parts are armed with novel designs: the Multiscale Information Fusion 
(MIF) block and the Texture Attention (TA) module. 

In the feature extraction part, we employ U2-Net (Qin et al., 2020) as 
our  base  network.  For  SOD  applications,  conventional  CNN-based 

EcologicalInformatics77(2023)1022155R. Li et al.                                                                                                                                                                                                                                        

Table 2 
Configurations of MIF block’s local-feature extractor.   

EN-1 

EN-2 

EN-3 

EN-4 

EN-5 

EN-6 

H 
W 
C 
′ 
C

H 
W 
C 
′ 
C

288 
288 
3 
32 

DN-1 
288 
288 
64 
32 

144 
144 
64 
32 

DN-2 
144 
144 
128 
32 

72 
72 
128 
64 

DN-3 
72 
72 
256 
64

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The pipeline uses a Convolutional Neural Network (CNN)-based model called U2-Net as its base network for feature extraction. This choice was made due to the effectiveness of CNNs in handling spatial data such as images, which makes them suitable for Salient Object Detection (SOD) applications. Additionally, the paper introduces novel components like the Multiscale Information Fusion (MIF) block and the Texture Attention (TA) module to enhance the overall architecture of their proposed model, named TrunkNet.
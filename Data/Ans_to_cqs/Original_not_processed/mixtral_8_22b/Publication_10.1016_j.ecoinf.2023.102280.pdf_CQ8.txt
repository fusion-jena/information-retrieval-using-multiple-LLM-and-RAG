Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

be  established  (Xie  et  al.,  2021).  Deep  noise  reduction  methods 
(Defossez et al., 2020) often use a U-net architecture which consists of an 
encoder and decoder section. The encoder produces a compressed latent 
space representation. The decoder then uses this latent space to recon-
struct the denoised waveform. Recently, transformer attention mecha-
nisms have also been applied with great success (Cao et al., 2022; Luo 
and  Mesgarani,  2019;  Luo  and  Mesgarani,  2023;  Zhang  et  al.,  2022; 
Zhao et al., 2022). As most deep-noise reduction methods focus on NLP 
applications, models are optimized for bandwidths ranging from 16 to 
22  kHz.  Bioacoustics  applications  commonly  exceed  this  bandwidth. 
Additionally, the sparsity of vocalisations in this application and many 
other  bioacoustics  applications  makes  the  development  of  large-scale 
datasets  infeasible  (Stowell,  2021).  These  factors  limit  the  success  of

Deep learning-based denoising methods are most commonly applied 
to  natural  language  processing  (NLP)  such  as  a  human  speech 
enhancement. D´efossez et al. (Defossez et al., 2020) demonstrated the 
potential for deep noise reduction techniques to achieve state-of-the-art 
(SOTA) performance and be applied in sub-real time on limited hard-
ware. These methods commonly operate on audio spectrograms but in 
recent  years  SOTA  deep  learning  methods  (Wang  et  al.,  2021)  have 
successfully  used  raw  waveform  data  (Luo and  Mesgarani,  2019; Luo 
and Mesgarani, 2023) for wider applications such as source separation 
and classification. This removes errors introduced when applying a Fast 
Fourier  Transform  (FFT)  to  generate  a  time-frequency  representation 
and reconstruction errors due to the loss of phase information. These 
networks tend to require more data for training as the latent space must

PSNR 

8.9 
8.1 
8.3 
9.0  

total and each is 10 s in length. Dataset 2 is a subset of AviaNZ’s New 
Zealand Native birds dataset containing 29 field recordings. This dataset 
was collated by Priyadarshini et al. (Priyadarshani et al., 2016) and is 
the same set used in their work. Bandwidths range up to 48 kHz and 
mostly  contain  stationary  white  background  noise.  Both  datasets  are 
publicly  available.  Dataset  1  and 
code  are  available  at 
https://github.com/BenMcEwen1/Sparse-Noise-Reduction 
at  https://github. 
and  AviaNZ’s  Dataset 
com/smarsland/AviaNZ.  Note  that  the  evaluation  Datasets  are 
separate from the training dataset used to fine-tune the CMGAN model. 
The  training  set  consists  of  1000  5-s  samples  each  recorded  using 
Audiomoth microphones (Hill et al., 2019) and downsampled to 16 kHz. 

available 

is 

4. Results

zenodo.3243139. 

Sainburg, T., Thielk, M., Gentner, T.Q., 2020. Finding, visualizing, and quantifying latent 
structure across diverse animal vocal repertoires. PLoS Comput. Biol. 16 (10), 
e1008228 https://doi.org/10.1371/journal.pcbi.1008228. 

Stowell, D., 2021. Computational Bioacoustics with Deep Learning: A Review and 

Roadmap. https://doi.org/10.7717/peerj.13152. 

Rev. 54, 1–23. https://doi.org/10.1007/s10462-020-09932-4. 

Zhang, G., Yu, L., Wang, C., Wei, J., 2022. Multi-scale temporal frequency convolutional 
network with axial attention for speech enhancement. In: ICASSP 2022–2022 IEEE 
International Conference on Acoustics, Speech and Signal Processing (ICASSP), 
pp. 9122–9126. https://doi.org/10.1109/ICASSP43922.2022.9746610. 
Zhao, S., Ma, B., Watcharasupat, K., Gan, W.-S., 2022. Frcrn: Boosting Feature 

Representation Using Frequency Recurrence for Monaural Speech Enhancement. 
https://doi.org/10.48550/arXiv.2206.07293. 

EcologicalInformatics77(2023)1022808

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The code repository of the deep learning pipeline is available on GitHub. It can be accessed via the link <https://github.com/BenMcEwen1/Sparse-Noise-Reduction>.
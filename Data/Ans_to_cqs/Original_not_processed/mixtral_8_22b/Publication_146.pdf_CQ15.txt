Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Model training was carried out on a NVIDIA Quadro M4000 GPU with 8 GB of memory,
with a training time of 4.9 min per epoch. With the adaptive learning rate algorithms usually yielding
higher model accuracies than static ones [16], the Adam learning rate optimizer was chosen over the
static stochastic gradient descent (SGD). Both the basic SGD and its further adaptive developments are
popular in neural network applications [17], but given the ﬁndings of [18,19], which proved Adam’s
usefulness on relatively small datasets (less than 1000 images), the Adam optimizer is chosen for
the model of this study. Hyperparameter setting was completed based on studies who successfully
implemented Yolo in remote sensing cases. The author of the Yolo-based “Yolt” model [14] suggests
implementing the same hyperparameters as the Yolo model. The default parameters of YoloV3 are
0.001, 0.9 and 0.0005 for the learning rate momentum and weight decay, respectively, and we decided

for deep learning. arXiv 2019, arXiv:1910.05446.

17. Ruder, S. An overview of gradient descent optimization algorithms. arXiv 2016, arXiv:1609.04747.
18. Taqi, A.M.; Awad, A.; Al-Azzo, F.; Milanova, M. The impact of multi-optimizers and data augmentation on
TensorFlow convolutional neural network performance. In Proceedings of the 2018 IEEE Conference on
Multimedia Information Processing and Retrieval (MIPR), Miami, FL, USA, 10–12 April 2018; pp. 140–145.
19. Olson, M.; Wyner, A.; Berk, R. Modern neural networks generalize on small data sets. In Proceedings of
the Advances in Neural Information, Neural Information Processing Systems 31 (NeurIPS 2018), Montreal,
BC, Canada, 3–8 December 2018; pp. 3619–3628.

20. Ammar, A.; Koubaa, A.; Ahmed, M.; Saad, A. Aerial images processing for car detection using convolutional

neural networks: Comparison between faster r-cnn and yolov3. arXiv 2019, arXiv:1910.07234.

Keywords: deep learning; object detection; ocean objects; synthetic aperture radar; classiﬁcation;
YoloV3

1. Introduction

2.2. YoloV3 Model Architecture

The YoloV3 algorithm implemented in this study is based on Darknet-53, a 53-layer deep
convolutional neural network (CNN) with residual connections. The traditional CNN object detectors
function as two stage detectors that ﬁrst have to identify individual regions of interest in the image
and then carry out bounding box detection within these regions. The two-stage detection performs
at competitive levels but at slow speeds, and while the improvements to speed have been made in
further developments such as the Fast R-CNN and Faster R-CNN, they cannot achieve equal training
time to the single stage detection. Yolo is a single stage detector that does not need to divide the image
into separate regions, but instead handles the full image at once and hence the name: You Only Look
Once. This is achieved by creating feature maps consisting of grid cells through a 3 level pyramid-like

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the deep learning pipeline used in this study, the Adam learning rate optimizer was chosen over the static stochastic gradient descent (SGD). Both the basic SGD and its further adaptive developments are popular in neural network applications. However, given the findings of previous studies, which proved Adam’s usefulness on relatively small datasets, the Adam optimizer was selected for the model of this study. These findings suggest that adaptive learning rate algorithms like Adam often yield higher model accuracies than static ones like SGD. Therefore, the use of the Adam optimizer can be seen as an optimization technique applied in the deep learning pipeline of this study.
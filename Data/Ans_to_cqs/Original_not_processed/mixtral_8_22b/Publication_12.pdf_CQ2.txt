Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The images were manually assigned into the four categories as
above (Fig. 2A). The weak and no call categories contained 10–100
images from each audio-file of different species. Then, images with a

2.1.3. Labeling images and making image library

We defined four categories for these spectral images: strong, weak,
no call and other (Fig. 2A). Strong is defined as a strong signal of the
target species presented in the image; weak is a particular category for
images contained a weak call structure; no call is for images which had
no signal at all; other is defined as ambiguous or non-target species
calls. We built a weak signal category for the reference image dataset
for two reasons: 1) we directly imported audio-files for identification
without manual selection of strong calls. Therefore, this saves one step
of manual pulse selection for processing large volume of data; 2) as
extremely weak calls are usually misidentified, therefore assigning
them to another category “weak” reduces the misidentification or false
positive rate.

In this study, we demonstrate that our model has high enough ac-
curacy to identify the 36 tropical bat species for both filtered and un-
filtered datasets. For the selected data BatNet outperforms CNNFULL by
increasing the overall accuracy rate from 77% to 91% using the vali-
dated dataset, and shows greatest improvement for the 12 vesperti-
lionid species. This improvement indicates the complicated architecture
of BatNet is more suitable for the application of unfiltered audio-files
recorded in tropical regions than simple networks (i.e., CNNFULL).
Secondly, the unfiltered data collected from different environments,
including nature habitats (i.e., CE (cave entrance) and DF (densely
forested) where bats usually emerge) and human living areas (i.e., AU
(Auditorium) is relatively wide open). The audio-files not only con-
tained no call regions, but only contained signal regions full of echos,
background noises, and high intra-species acoustic variation (such as

Fig. 1. Schematic diagram of pipeline. We first built model
and then test the model. There are three python scripts used to
for
Coverting_image.py,
including
Making_database.py, and Training.py in a self-developed open
source software Waveman. Tested process using both filtered
images and unfiltered audio-files.

modelling,

2

X. Chen, et al.

Biological Conservation 241 (2020) 108269

Table 1
Taxa summary of the 36 species and itscall information.

Family

Species

Individual number**

No. of images***

Location

Release type

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The deep learning pipeline described in X. Chen, et al.'s Biological Conservation article uses both image and audio data formats. Initially, the authors convert audio files into spectral images, which they categorize into four groups: strong, weak, no call, and other. These spectral images serve as input for their deep learning models, such as BatNet and CNNFull. Additionally, the authors use unfiltered audio files recorded in various environments to validate their models. Thus, the primary data formats employed in this pipeline are images derived from audio files and raw audio files themselves.
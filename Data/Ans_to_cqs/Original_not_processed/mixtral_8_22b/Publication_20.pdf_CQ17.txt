Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

A link to a depository with architecture details is given at the end of
references. We stopped the network training after 70 epochs (i.e. a
complete scope of the dataset where each image is used only once), to
−5, an exponential
prevent overﬁtting. We used a learning rate of 10
learning decay with a Gamma of 0.95, a dropout of 50% and an Adam
Solver type as learning parameters. Those are classic hyper-parameters
for a fast convergence of the network without over-ﬁtting (Srivastava
et al., 2014). The weight initialization is also classic with a random
Gaussian initialization. The training lasted 8 days on our conﬁguration;
we trained and ran our code on a computer with 64GB of RAM, an i7
3.50GHz CPU and a Titan X GPU card for 900,000 images.

We used at least 2200 thumbnails per ﬁsh species class, and batches

240

S. Villon et al.

Ecological Informatics 48 (2018) 238–244

Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet Classiﬁcation with Deep
Convolutional Neural Networks. InAdvances in Neural Information Processing
Systems. pp. 1097–1105.

Krueck, N.C., Ahmadia, G.N., Possingham, H.P., Riginos, C., Treml, E.A., Mumby, P.J.,

2017. Marine reserve targets to sustain and rebuild unregulated ﬁsheries. PLoS Biol.
15 (1), e2000537.

Kulbicki, M., Parravicini, V., Bellwood, D.R., Arias-Gonzàlez, E., Chabanet, P., Floeter,
S.R., ... Mouillot, D., 2013. Global biogeography of reef ﬁshes: a hierarchical quan-
titative delineation of regions. PLoS One 8 (12), e81847.

Langlois, T.J., Harvey, E.S., Fitzpatrick, B., Meeuwig, J., Shedrawi, G., Watson, D., 2010.
Cost-eﬃcient sampling of ﬁsh assemblages: comparison of baited video sta=tions

244

and diver video transects. Aquat. Biol. 9, 155–168.

We used a GoogLeNet architecture as it was the winner of the 2015
competition imageNet (Szegedy et al., 2015), an identiﬁcation chal-
lenge on 1000 diﬀerent classes. This CNN is composed of 22 layers. It
uses inception modules. Inception modules allow the network to use
convolutions of diﬀerent sizes (1*1, 3*3 and 5*5 pixels) and to weight
each of these convolutions. This network could thus account more or
less strongly for the context of each pixel, which increases the range of
possibilities to improve its performance during the training.

Nadarajan, G. (2010, October). Automatic ﬁsh classiﬁcation for underwater species
behavior understanding. In Proceedings of the ﬁrst ACM international workshop on
Analysis and retrieval of tracked events and motion in imagery streams (pp. 45–50).
ACM.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., 2014. Dropout:
a simple way to prevent neural networks from overﬁtting. The Journal of Machine
Learning Research 15 (1), 1929–1958.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Rabinovich, A., 2015.
Going Deeper with Convolutions. InProceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. pp. 1–9.

Taquet, M., Diringer, A., 2007. Poissons de l'océan Indien et de la mer Rouge. (Editions

Quae).

Thresher, R.E., Gunn, J.S., 1986. Comparative analysis of visual census techniques for
highly mobile, reef-associated piscivores (Carangidae). Environ. Biol. Fish 17 (2),
93–116.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors used several regularization methods to prevent overfitting in their deep learning pipeline. They employed an early stopping strategy by halting the network training after 70 epochs. Additionally, they utilized an exponential learning rate decay with a gamma value of 0.95. Another method implemented was dropout regularization, setting the dropout rate to 50%. These techniques are considered classic hyperparameters for achieving fast convergence and avoiding overfitting in neural networks (Srivastava et al., 2014). Furthermore, the authors used a Gaussian random initialization for weights, which is another common practice to help prevent overfitting.
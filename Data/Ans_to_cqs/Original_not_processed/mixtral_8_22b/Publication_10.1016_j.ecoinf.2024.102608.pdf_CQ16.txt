Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

50 
110 
Sqrt 
6 
4 
True 
9 
Distance 
Kd_tree 
1 
50 
0.01 
Linear 
(100,100) 
Logistic 
Lbfgs 
0.01 
Adaptive 

20 
110 
Sqrt 
6 
4 
True 
9 
Distance 
Brute 
1 
100 
0.1 
Exponential 
(50, 50) 
Tanh 
Lbfgs 
0.01 
Constant 

50 
60 
Sqrt 
6 
4 
True 
9 
Distance 
Brute 
1 
50 
0.01 
Linear 
(50, 50) 
Relu 
Lbfgs 
0.001 
Constant 

20 
90 
Sqrt 
6 
3 
True 
9 
Distance 
Brute 
1 
100 
0.01 
Square 
(50, 50) 
Logistic 
Lbfgs 
0.001 
Adaptive 

50 
70 
Sqrt 
6 
3 
True 
9 
Distance 
Brute 
1 
100 
0.05 
Linear 
(50, 50) 
Logistic 
Lbfgs 
0.0001 
Adaptive 

100 
60 
Sqrt 
10 
4 
True 
9 
Distance 
Brute 
1 
50 
0.01 
Linear 
(50, 50) 
Tanh 
Adam 
0.001 
Adaptive

we selected the set of hyperparameter values that resulted in the mini-
mum  MAE  and  RMSE,  as  well  as  the  highest  R2,  for  each  machine 
learning model. Table 1 presents the resulting hyperparameter values 
for the four machine learning models.

EcologicalInformatics81(2024)1026086M. Mamun et al.                                                                                                                                                                                                                                

Table 1 
Hyperparameters obtained after tuning each machine learning model for CHL-a, SD, and TSS (CHL-a: chlorophyll-a, SD: Secchi depth [water clarity], TSS: total 
suspended solids).  

ML model 

Hyperparameters 

Values 

Sentinel-2 MSI 

Landsat-8 OLI 

CHL-a 

SD 

TSS 

CHL-a 

SD 

TSS 

Random forest 

K-nearest neighbors 

AdaBoost 

Artificial neural network 

Number of decision trees 
Maximum depth of the decision tree 
Maximum number of features to select 
Minimum samples split 
Minimum samples leaf 
Bootstrap 
Number of neighbors 
Weights 
Algorithm 
p 
Number of estimators 
Learning rate 
Loss 
Hidden layer sizes 
Activation 
Solver 
Alpha 
Learning rate

training and test data set to evaluate the performance of the machine 
learning algorithms. Of the four machine learning algorithms, RF (S-2 
MSI: R2 = 0.61, MAE% = 6.56, RMSE = 12.51 μg/L, L-8 OLI: R2 = 0.56, 
MAE = 8.44%, RMSE = 16.01 μg/L) yielded the most accurate CHL-a 
predictions from the Sentinel-2 MSI and Landsat-8 OLI data in the test 
set, outperforming the KNN (S-2 MSI: R2 = 0.60, MAE = 6.49%, RMSE =
12.64 μg/L, L-8 OLI: R2  = 0.55, MAE = 8.24%, RMSE = 16.22 μg/L), 
AdaBoost (S-2 MSI: R2 = 0.58, MAE = 7.39%, RMSE = 12.97 μg/L, L-8 
OLI: R2 = 0.44, MAE = 9.81%, RMSE = 18.11 μg/L), and ANN (S-2 MSI: 
R2 = 0.59, MAE = 7.31%, RMSE = 12.76 μg/L, L-8 OLI: R2 = 0.14, MAE 
= 11.27%, RMSE = 22.43 μg/L) models (Fig. 4). Furthermore, the RF 
algorithm also exhibited better SD predictions (S-2 MSI: R2 = 0.45, MAE 
= 0.76%, RMSE = 1.03 m, L-8 OLI: R2 = 0.35, MAE = 0.77%, RMSE =
1.09 m) and TSS (S-2 MSI: R2 = 0.46, MAE = 2.95%, RMSE = 5.61 mg/L,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The table does not provide information on the specific criteria used to determine when training is complete for each machine learning model. However, common criteria include reaching a minimum validation loss or accuracy threshold, achieving a stable validation loss over several epochs, or using early stopping based on validation metrics.

The given text provides details about the hyperparameters obtained after tuning each machine learning model for predicting Chlorophyll-a (CHL-a), Secchi depth (SD), and Total Suspended Solids (TSS) using Sentinel-2 MSI and Landsat-8 OLI datasets. The table lists various hyperparameters such as Number of Decision Trees, Maximum Depth of the Decision Tree, Minimum Samples Leaf, Bootstrap, Number of Neighbors, Weights, Algorithm p, Learning Rate, Loss, Hidden Layer Sizes, Activation, Solver, Alpha, etc.

However, the table does not explicitly mention any criteria used to determine when training is complete for each machine learning model. Commonly used criteria to decide when training should be stopped include reaching a minimum validation loss or accuracy threshold, observing a stable validation loss over multiple epochs, or employing early stopping based on validation metrics. These criteria help prevent overfitting and ensure optimal model performance.
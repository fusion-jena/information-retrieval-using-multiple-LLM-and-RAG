Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.2. Model performance MLMs comparisons 

The performance of each MLM was assessed using several metrics, 
such as AUC, TSS, Kappa, and phi. The outcomes of these evaluations are 
presented  in  Table  3.  Additionally,  Fig.  4  visually  illustrates  the  pre-
dictive performance of each model with AUC values for both training 
and test datasets. Upon analysing the four performance metrics, it was 
evident that the selected models exhibited high consistency, except for 
the  CART.  The  performance  of  the  remaining  models  displayed  only 
slight variations within a small range, as shown in Table 3. Additionally, 
there was no discernible variation in the classifiers’ prediction accuracy 
according to the Kruskal-Wallis statistic (p > 0.05).

Writing – review & editing. Sunil Nautiyal: Conceptualization, Inves-
tigation, Supervision, Writing – review &  editing, funding acquisition, 
writing management recommendation in supplementary material.

Mriganka Shekhar Sarkar: Conceptualization, Methodology, Soft-
ware,  Data  curation,  Formal  analysis,  Validation,  Writing  –  original 
draft, writing management recommendation in the supplementary ma-
terial.  Bishal  Kumar  Majhi:  Conceptualization,  Data  curation,  Soft-
ware, Formal analysis, Writing – review & editing, writing management 
recommendation  in  supplementary  material.  Bhawna  Pathak:  Data 
curation, Formal analysis, Writing – review & editing. Tridipa Biswas: 
Data  curation,  Writing  –  review  &  editing,  writing  management 
recommendation  in  supplementary  material.  Soumik  Mahapatra: 
Writing  management  recommendation  in  supplementary  material. 
Devendra  Kumar:  Writing  –  review  &  editing.  Indra  D.  Bhatt: 
Conceptualization, Project administration, Funding acquisition, Super-
vision, Writing – review & editing. Jagadish C. Kuniyal: Conceptuali-
zation,  Project  administration,  Funding  acquisition,  Supervision,

Validation of MLMs is a critical step in assessing each model’s pre-
dictive capabilities, as Mohammadi et al. (2014) emphasized. To eval-
uate  the  accuracy  and  reliability  of  each  MLM,  we  employed  robust 
statistical  measures  provided  by  the  ‘sdm  1.1–8’  package.  These  are 
kappa (Allouche et al., 2006), true skill statistics (TSS; Allouche et al., 
2006), and area under the receiver operator characteristics (ROC) curve 
(AUC; Hanley and McNeil, 1982). Model complexity was evaluated for 
each case by computing the difference between the AUC (train) and AUC 

(CV), represented as Δ AUC (Jarnevich et al., 2018). The Δ AUC scores 
from each model offer insights into their sensitivity to the training data 
(Jarnevich  et  al.,  2018).  Moreover,  the  Kruskal-Wallis  test  was  con-
ducted to determine whether a significant difference in prediction ac-
curacy exists among the classifiers. 

2.5. Ensemble model

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training process, there are several postprocessing steps that can be taken to evaluate the performance of machine learning models (MLMs). According to the given context, the performance of each MLM was assessed using several metrics such as AUC, TSS, Kappa, and phi. These evaluations were presented in Table 3 and visualised in Figure 4. Additionally, the model complexity was evaluated by computing the difference between the AUC (train) and AUC (CV), represented as Δ AUC. This score provides insight into the sensitivity of the models to the training data. Furthermore, the Kruskal-Wallis test was conducted to determine if there was a significant difference in prediction accuracy among the classifiers.

In addition to these statistical measures, other common postprocessing techniques include creating saliency maps, calculating confusion matrices, and generating ROC curves. Saliency maps highlight the areas of input images that contribute most significantly to the output predictions made by the model. Calculating confusion matrices allows researchers to understand how well the model performs on different classes of data. Generating ROC curves enables researchers to compare the performance of multiple models across various thresholds.

Overall, postprocessing steps play a crucial role in validating the predictive capabilities of MLMs. By employing robust statistical measures and visualisation tools, researchers can gain valuable insights into the strengths and weaknesses of their models.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

A. ACCURACY COMPARISONS OF BENCHMARK
NETWORK MODELS
Several experiments were conducted to evaluate the accuracy
and practicability of the proposed method. For the quantita-
tive analysis of the models, we ﬁrst compared state-of-the-art
deep learning backbone models, such as Inception-ResNet-
v2 [27], Inception-v3 [28], Xception [32], ResNet101 [29],
and MobileNetV2 [33]. Fig. 5 lists the classiﬁcation accura-
cies achieved by the learning models.

Inception-ResNet-v2 was found to consistently outperform
the other existing learning networks by attaining 100% train-
ing accuracy, 97.47% validation accuracy, and 97.11% test
accuracy.

FIGURE 5. Comparison performance of various AI models.

IV. EXPERIMENTAL SETTINGS
To extract the relevant features of birds, we randomly split
the endemic bird data into 3,132 images for training and
760 images for testing. To allow the deep networks to con-
verge and improve the ability of the model to generalize
adeptly, we augmented the training dataset 10 times, resulting
in a total of 31,320 images. Then, 25,056 (80%) images were
used for training and 6,264 (20%) were used for validation.
The Inception-ResNet-v2 model was trained using the mini-
batch gradient descent algorithm with the batch size set to
32 and the learning rate maintained at 0.0001. The Adam
optimizer was used to ﬁne-tune the model with the following
parameters:

• Exponential decay rate of ﬁrst-moment estimation

of 0.9.

In deep learning, a deep network [30], [31] is consid-
ered better than a shallow network because it learns rele-
vant features layer-by-layer with more precision and achieves
high-level feature extraction (texture, shape, size, color), con-
necting a set of features to a label from a given set of input
categories. However, in practice, deep learning networks have
some problems. For instance, deep neural networks often
suffer from the problem of vanishing gradients, wherein a
deep multilayer feed-forward network is difﬁcult to train and

FIGURE 1. Twenty-nine endemic bird species to Taiwan.

The interior of the Inception-ResNet-v2 network model
inception modules—Inception-
includes three types of
ResNet-A,
Inception-ResNet-B, and Inception-ResNet-C
blocks—that use 35 × 35, 17 × 7, and 8 × 8 grid mod-
ules, respectively, to generate discriminatory features and
reduce the number of parameters. To obtain different feature
extraction patterns of the bird images, the input size of the
images is 299 × 299 pixels with three different color channels
(red, green, and blue) for the stem layer. Then, the inception
module uses a block of parallel convolutional layers with a
multilevel feature extractor that extracts local and generic
features from the input images of the birds by computing
three different sizes of ﬁlters (1 × 1, 3 × 3, and 5 × 5) and
a max pooling layer within the same module of the network.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In the experimental settings described, the Inception-ResNet-v2 model was trained using the mini-batch gradient descent algorithm with the batch size set to 32. The Adam optimizer was used to fine-tune the model. Specifically, the exponential decay rate of first-moment estimation of 0.9 was employed.

Mini-batch gradient descent is a variant of stochastic gradient descent (SGD). It splits the training dataset into small batches and performs updates after each batch rather than every single example like traditional SGD or after all examples like full-batch gradient descent. This approach provides a balance between speed and accuracy.

Adam (short for Adaptive Moment Estimation) is another optimization technique used in this case. It combines the advantages of two popular methods, RMSprop and Stochastic Gradient Descent with momentum. Adam calculates adaptive learning rates for different parameters based on estimates of first and second moments of the gradients. This makes it well suited for handling problems involving large datasets and/or high dimensionality.
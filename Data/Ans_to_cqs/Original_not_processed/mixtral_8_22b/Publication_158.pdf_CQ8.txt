Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

the ensemble results. All CNNs were trained using the TensorFlow backend on a Tesla
M100 graphic processing unit.

Figure 3. Schematic of the CNN used in the study. Input image size was 32 by 32 pixels shown as the
large grey square. Numbers represent the number of features output from a convolution or dense
layer, except for the cropping layers where the crop dimension is shown.

There are numerous aspects to consider for enhancing the deep-learning classiﬁer,
speciﬁcally regarding the input image size; network conﬁguration, depth, and width; and
training strategy. In initial experimentation, several standard forms of deep-learning CNN
architectures were tested based largely on ResNet [46] and Inception [63], but these did
not perform well due to the range of image objects size/scales encountered. To account
for objects represented as a single pixel up to larger assemblages extending beyond the
32 by 32 pixel input image, the ensemble with varying input image sizes was selected.
Segmentation-based approaches were also considered as an alternative, but sufﬁcient
quality training data for segmentation was a limitation.

3.3.3. Change Detection

For classiﬁer development, samples across the prairie region were acquired for train-
ing, veriﬁcation, and ﬁnal independent testing. The veriﬁcation sample was used to
determine training convergence of the deep-learning classiﬁer and the test sample for inde-
pendent assessment. To generate these, a systematic approach was used where samples
were collected following a grid of sample blocks of 9 by 9 km with 15 km spacing between
blocks. Blocks were assigned as training, validation, and test. For the test sample a column
of the sample grid was randomly selected and every fourth column from that added. Only
every second row for the selected columns were included in the test sample. Figure 2
shows the distribution of these sample sets. In total there were 39,520 (60%) training
samples, 23,934 (30%) veriﬁcation samples and 6509 (10%) test samples. Here we focused
the majority of the samples for training and veriﬁcation in an effort to maximize model

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information about the location of the code repository for the deep learning pipeline. It only mentions that all CNNs were trained using the TensorFlow backend on a Tesla M100 GPU. However, it does not specify where the code can be found. Therefore, I do not have enough information to determine where the code repository is located.
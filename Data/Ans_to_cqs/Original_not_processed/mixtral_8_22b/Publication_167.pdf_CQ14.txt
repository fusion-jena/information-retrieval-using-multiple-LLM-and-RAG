Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The data of a certain year are used as the test set, and the remaining data are the training and
validation set. The training and validation set ia divided into ten parts using the 10-fold cross-validation
method, in which nine of the parts are used as a training set and one is used for the validation set.
The percentage of the training, validation, and testing sets is 5.4:0.6:1. The training set is the data
sample used for model ﬁtting. The validation set is a set of samples set aside separately during the
model training. It can be used to adjust the hyperparameters of the model and to conduct a preliminary
evaluation of the model’s capabilities. The testing set is used to evaluate the generalization ability of
the ﬁnal model. However, it cannot be used as a basis for algorithm-related selection such as parameter
tuning and selection of features. The mean value of the ten results is used to estimate the accuracy of

(3) Training set: grid with neighborhood features of six years (1986, 2005, 2008, 2010, and 2013).
The features include the LU type, soil, topography, elevation, slope, aspect, distance to settlements,
distance to roads, distance to rivers in this grid unit, and the 4 neighborhood grid units.

(4) Constraint Factors: The statistical data and the quantity simulated by the backtracking of the
Markov model are combined, and the numbers of diﬀerent LU types in 2000 after modiﬁcation by
the simulation numbers are as follows (Table 1):

mask. (3) Training set: grid with neighborhood features of six years (1986, 2005, 2008, 2010, and 2013). The features include the LU type, soil, topography, elevation, slope, aspect, distance to settlements, distance to roads, distance to rivers in this grid unit, and the 4 neighborhood grid units. (4) Constraint Factors: The statistical data and the quantity simulated by the backtracking of the Markov model are combined, and the numbers of different LU types in 2000 after modification by the simulation numbers are as follows (Table 1): 2.4.2. Deep Learning Module Probability maps (Figure 9): Water and settlements are constraint factors and can be determined by manual interpretation, and thus, no simulation is needed.  Figure 9. Probability maps. Remote Sens. 2020, 12, 3314

the algorithm in such a way that more accurate model results can be obtained.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, the hyperparameters of the model are optimized using the validation set through the process of 10-fold cross-validation. In this method, the dataset is split into ten equal subsets or folds. Nine of these folds are used for training the model, while the remaining fold is used for validating the model. This process is repeated ten times, each time with a different fold serving as the validation set. The average performance across all ten trials is then calculated to produce a single estimation.

However, the specific methods for optimizing the hyperparameters, such as grid search or random search, are not explicitly mentioned in the provided context. Grid search involves an exhaustive search through a manually specified subset of a hyperparameter space of a learning algorithm, whereas random search tries random combinations within the defined ranges. Without further details, we cannot definitively state whether either of these methods was employed.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Fig. 2. Wav2Vec pre-training phase.  

quantized representation for the masked latent feature representation. 
Diversity loss shown in (2) is also added to the objective function for 
regularization during pre-training. 

(

Lm = (cid:0)

log

sim(ct ,qt )
e
k

/

∑

q′∈Qt

sim(ct ,q′)
e
k

Ld =

(cid:0)
*

1
GV

) )

(cid:0)

pg

(cid:0) H

(1)  

(2) 

In the supervised fine-tuning phase, the labelled dataset is used in 
training the model to predict particular words or phonemes. Fig. 3 de-
picts the process and components in the fine-tuning phase of wav2vec. 
Phonemes are the smallest unit of sound, usually one or two letters, in 
the language. During fine-tuning, the quantization module is removed. 
Instead, a linear projection layer is added to the context network. Then 
the model is fine-tuned on connectionist temporal classification (CTC) 
loss for the Automatic Speech Recognition task. So, the wav2vec model 
has a general understanding of phonemes present in human speech.

The  self-supervised  training  phase  comprises  four  important  ele-
ments: feature encoder, context network, quantization module, and pre- 
training  objective.  Fig.  2  depicts  the  overall  pre-training  approach  of 
wav2vec. Contrastive learning is the idea of recognizing whether two 
different types of transformation of the input are the same or not. The 
two transformations used in the Wav2vec model are the context repre-
sentation from the context network and the final quantization vectors 

EcologicalInformatics80(2024)1024713B. Swaminathan et al.                                                                                                                                                                                                                         

Fig. 2. Wav2Vec pre-training phase.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., 
Polosukhin, I., 2017. Attention is all you need. In: Adv. Neural Inf. Process. Syst. 
2017-Decem, pp. 5999–6009. 

Virtanen, T., Plumbley, M., Ellis, D., 2017. Computational analysis of sound scenes and 
events. In: Computational Analysis of Sound Scenes and Events. https://doi.org/ 
10.1007/978-3-319-63450-0. 

Yadav, A., Vishwakarma, D.K., 2020. A multilingual framework of CNN and bi-LSTM for 
emotion classification. In: 2020 11th International Conference on Computing, 
Communication and Networking Technologies (ICCCNT), pp. 1–6. https://doi.org/ 
10.1109/ICCCNT49239.2020.9225614. 

Zhang, X., Chen, A., Zhou, G., Zhang, Z., Huang, X., Qiang, X., 2019. Spectrogram-frame 

linear network and continuous frame sequence for bird sound classification. Ecol. 
Inform. 54, 101009 https://doi.org/10.1016/J.ECOINF.2019.101009.

4.4. Wav2Vec model description 

After pre-processing, the whole dataset is splitted into training and 
testing  of  various  ratio  including  70:30,  80:20  and  90:10  for  strong 
performance  analysis.  As  a  part  of  training  proposed  model,  the  raw 
audio signals of various bird species are given as input to extract 768- 
dimensional  vector  as  features.  In  wav2vec,  after  the  transformer 
block, there are three dense layers with ReLU activation function con-
tains 1024, 512, and 256 hidden neurons, respectively. Followed by, one 
classification layer with sigmoid activation function is kept to decide on 

bird categories as output. For this technique, Adam is used as optimizer 
with initial learning rate of order 10e-5 is appropriate for small weight 
updates throughout the network while fine-tuning. Based on the manual 
fine-tuning process, other hyper parameters such as epochs of 30 with 
batch size 1 is chosen as optimal value. 

4.5. Multi labelling framework

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the provided context, it does not explicitly mention any specific postprocessing steps involved after the model training. However, common practices in machine learning include evaluating the performance of the trained models using metrics like accuracy, precision, recall, F1 score, etc. Additionally, confusion matrices can be generated to visualize the performance of the classifier. Other techniques like saliency maps may also be employed depending upon the application requirements. But without explicit information about these aspects in the provided context, we cannot provide a definitive answer regarding the postprocessing steps involved after the model training.
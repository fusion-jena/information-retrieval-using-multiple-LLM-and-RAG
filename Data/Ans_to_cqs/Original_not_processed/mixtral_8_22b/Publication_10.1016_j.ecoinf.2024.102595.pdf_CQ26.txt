Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In  the  development  stage,  the  preprocessed  data  was  divided  into 
training (80%), validation (10%), and test sets (10%) for model training 
and testing purposes (Shah et al., 2018), followed by the construction of 
predictive  models  using  both  deep  learning  algorithms  (e.g.,  Deep 
Neural  Networks)  and  traditional  machine  learning  approaches  (e.g., 
AdaBoost  Regressor,  Support  Vector  Regressor,  etc.),  with  hyper-
parameter tuning performed via GridSearchCV with cross-validation (cv 
= 10).  While some studies  might  use different proportions, such as  a 
70–30 split between training and testing/validation (Khan et al., 2022), 
the  80–10-10  split  employed  in  this  study  ensured  a  more  extensive 
training set, potentially leading to a better-generalized model. The test 
set,  which  the  model  has  not  previously  seen  during  the  training  or 
validation phases indicates the model’s performance in real-world sce-
narios (Shah et al., 2018).

(2019),  stating  that  optimal  hyperparameter  settings  are  critical  to 
reducing bias in the model predictions.

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Oil palm 
Yield prediction 
Machine learning 
Deep learning 
Agriculture

2.3.3. Transformation 

Two fundamental techniques employed in the data transformation 
process  are  the  MinMaxScaler  and  the  LabelEncoder.  The  former  is 
especially  crucial  for  algorithms  that  are  sensitive  to  the  scale  of  the 
data, such as Support Vector Regressor (SVR) and neural networks. The 
LabelEncoder was applied to transform categorical fields into numerical 
values. Agricultural datasets often include categorical data, such as soil 
types  or  crop  varieties,  which  must  be  encoded  to  numerical  labels 
before being fed into machine learning algorithms. 

2.3.4. Feature selection

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not explicitly mention any specific techniques used to address data bias during preprocessing of the deep learning pipeline, such as stratified splitting, oversampling, undersampling, or diverse data collection. However, it does discuss the importance of hyperparameter tuning in reducing bias in model predictions. Hyperparameters are parameters whose values are set before the commencement of the learning process. By performing hyperparameter tuning via GridSearchCV with cross-validation, the authors aimed to find the best combination of hyperparameters that would minimize bias and improve the model's predictive accuracy. Additionally, the context mentions the use of two fundamental techniques in the data transformation process - MinMaxScaler and LabelEncoder. These techniques help prepare the data for input into machine learning algorithms but do not directly address data bias. Therefore, based on the provided context, it can be concluded that while hyperparameter tuning is mentioned as a means to reduce bias in model predictions, no explicit techniques to address data bias during preprocessing are discussed.
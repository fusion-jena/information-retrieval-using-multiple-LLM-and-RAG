Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 2
Experiments training/test sets sizes for the upper-, left- and right-poses models.

Exper. No.

Method

Upper

Left

Right

E1

E2

4-CoL, NoAug,
FEE, none
4-Col, Aug,
NoFEE, none

Train: 24
Test: 66
Train: 72
Test: 68

Train: 40
Test: 59
Train: 120
Test: 62

Train: 40
Test: 68
Train: 120
Test: 67

Table 3
Precision results of each experiment for the upper-, left- and right-pose models.

Exper. No.

Measure

E1:

E2:

P1
P3
P5
P6
P1
P3
P5
P6

Upper

0.682
0.788
0.838
0.849
0.441
0.5
0.559
0.603

Left

0.356
0.61
0.712
0.78
0.323
0.484
0.581
0.629

Right

0.544
0.662
0.735
0.78
0.418
0.522
0.612
0.687

Fig. 10. (a) Upper-pose images, (b) Left-pose images, (c) Right-pose images.

EcologicalInformatics82(2024)10273712A. Levy et al.

Still there are issues that should be considered before a specific
detailed framework can be designed. In order to decide on a feature
extraction and recognition detailed steps framework,12 initial experi-
ments were performed on the individuals labeled database. Since we
study a task that has not been studied before, we explore different as-
pects and several directions to tackle them, which naturally, most of
them are abandoned after evaluating the results. Nevertheless, these
initial experiments are important as a base case to study the conditions
that improve the performance, which is the focus of this paper. Based on
the literature survey our initial experiments referred to the following
issues to be decided for the chosen framework: architecture, data
augmentation usage, feature extraction enhancement and handling
different poses. The evaluation of the initial experiments results then
guided our design of the follow-up experiments. The initial experiments,

their evaluation and the deduced follow-up experiments are described in
Section 4.

4.1.1.

Initial experiments evaluation

We performed initial experiments on several combinations of the
above different experiment layout properties. The precision results of all
our initial experiments were poor â€“ achieving less than 50% in the P6
measure. The precision results of layout combinations which achieved at
least 3% in the P1 measure (i.e, at least 3 out of 99 test images gave the
correct identification) are given in Table 1. In experiments I1, I2 and I3
of Table 1 we only used 19 of the 21 different individuals (classes) of the
training set, since some unclear images (due to the feature extraction
enhancement preprocessing) were removed prior to the training.

Of course, the methods explored in the initial experiments, which did
however not produce fruitful results, support future research by indi-
cating not to repeat these methodologies. Moreover, despite these poor
the initial experiments pointed out some directions for
results,
improvement:

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The information provided does not explicitly mention any postprocessing steps involved after the model training such as saliency maps, metrics calculation, or confusion matrices. However, based on the context, it appears that the authors evaluated their models using precision measures (P1, P3, P5, P6), which suggests that they calculated metrics to evaluate their models. Additionally, they may have also created confusion matrices to visualize the performance of their models. Unfortunately, without more explicit information about the postprocessing steps, it cannot be confirmed what other steps were taken beyond calculating precision measures and potentially creating confusion matrices.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

must  be  rescued  by  attending  to  the  body  of  the  text  message  and 
complemented  with  the  associated  multimedia  content.  On  the  other 
hand,  it  is  also  necessary  to  extract  the  location  associated  with  this 
information, while not invading the privacy of the user. The increasing 
number of messages shared on social networks and their short lifespan 
make  it  necessary  to  automate  the  process  of  capturing  information 
(Toivonen et al., 2019) and subsequent automated classification to filter 
messages  before  they  are  human  validated.  This  implies  the  develop-
ment of algorithms that combine natural language processing and image 
analysis techniques. In addition, a major challenge is to integrate this 
information with existing international open-access biodiversity obser-
vation  databases  in  order  to  increase  the  return  on  this  kind  of 
initiatives.

(INDITIC Consortium, 2023). This kind of initiatives would benefit of 
the implementation of the current methodology to reduce human effort. 
Although the usefulness of this methodology has been proven, there 
are still certain limitations that cannot be ignored. The main one cor-
responds to the nature of the tweets themselves, with a low number of 
words  accompanying  the  messages,  not  always  well  written,  which 
makes  it  difficult  to  obtain  useful  information.  This  leads  to  a  high 
number of tweets being lost in the filtering process. Furthermore, using 
exclusively textual information from tweets to obtain a training dataset 
would  only  lead  to  the  creation  of  a  deficient  model.  In  addition, 
biodiversity observations only make sense when accompanied by their 
location, allowing, among other applications, to alert about the presence 
of  invasive  species  or  reuse  the  information  in  species  distribution

The  information  retrieved  from  this  social  network,  as  a  general 
characteristic to big data, are defined by a large volume of data and a 
large heterogeneity that requires cleaning and filtering processes that 
allow  focusing  on  relevant  data  (Poorthuis  and  Zook,  2017).  Fig.  1 
shows  a  flowchart  of  the  data  acquisition  and  processing  in  order  to 
reduce  invalid  tweets.  After  the  acquisition  of  original  tweets  (not 
retweets), a first filter consists in eliminating those tweets that contain 
images in GIF format and that in most cases corresponds to memes (http 

Fig. 1. Simplified flowchart of the tweet detection algorithm.  

EcologicalInformatics80(2024)1025153P. Otero et al.

In this study we have used the ‘Full-archive search endpoint’ to ac-
cess  the  entire  dataset  published  on  Twitter  from  its  beginning  in 
2006–2007 until the end of 2022. To handle the data in JSON format 
returned  by  the  Twitter  API,  we  used  the  twarc2  library  (https:// 
twarc-project.readthedocs.io/).  This  library  simplifies  the  handling  of 
the quota limits imposed by the Twitter API, as well as the pagination 
resulting from a single query. 

The API works by searching for words, hashtags or combinations of 
both  in  the  text  of  tweets,  much  like  the  Twitter  Advanced  Search 
interface  (https://twitter.com/search-advanced?).  To  find  out  how  to 
run the search, in an initial phase of the study, some public tweets with 
observations  were  provided  by  the  social  media  manager  of  the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, the data formats used in the deep learning pipeline primarily involve text and images. Specifically, the study uses the 'Full-archive search endpoint' to access the entire dataset published on Twitter, which includes both text and images. The text data is handled in JSON format returned by the Twitter API, utilizing the twarc2 library for easier management. Additionally, the study mentions the use of natural language processing techniques, further emphasizing the importance of text data. Regarding images, the study discusses the need to eliminate tweets containing images in GIF format, suggesting that non-GIF images may be part of the analysis. However, the context does not explicitly mention any specific format for these images. Therefore, based on the available information, the primary data formats used in the deep learning pipeline appear to be text (JSON) and images.
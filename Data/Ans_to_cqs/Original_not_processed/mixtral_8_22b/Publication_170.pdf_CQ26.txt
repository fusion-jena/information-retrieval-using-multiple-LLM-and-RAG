Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Yang et al. (cid:129) An Adaptive Approach for Empty Images Filtering

233

 23285540, 2021, 2, Downloaded from https://wildlife.onlinelibrary.wiley.com/doi/10.1002/wsb.1176 by Vamsi Krishna Kommineni - Friedrich-Schiller-Universität , Wiley Online Library on [29/08/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License3.6 GHz central processing unit (CPU) and 32 GB of
memory, we used the incremental training mode to train
and test the DCNN model. On another Dell PowerEdge
C4130 rack server with two Tesla K80 GPUs and 256 GB
of memory, we used the zero‐start and the incremental
training modes to train and test models, respectively.

RESULTS
The incremental training mode performed consistently with
the zero‐start training mode when the training sample size

23285540, 2021, 2, Downloaded from https://wildlife.onlinelibrary.wiley.com/doi/10.1002/wsb.1176 by Vamsi Krishna Kommineni - Friedrich-Schiller-Universität , Wiley Online Library on [29/08/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons Licenseof empty images in the data set. Because the accuracy
(97.3%) of our method is higher than that of human vol-
unteers (96.6%; Norouzzadeh et al. 2018), it can be applied
to actual ecological monitoring projects. Second, the time
and personnel costs of labeling small‐scale training sets are
much lower than that of million‐scale training sets. Note
that to manually label ~5.5 million images, nearly 30,000
persons have spent ~14.6 years with a 40‐hour work week
(Swanson et al. 2015, Norouzzadeh et al. 2018), which
means that manually labeling a training set containing

232

Wildlife Society Bulletin (cid:129) 45(2)

 23285540, 2021, 2, Downloaded from https://wildlife.onlinelibrary.wiley.com/doi/10.1002/wsb.1176 by Vamsi Krishna Kommineni - Friedrich-Schiller-Universität , Wiley Online Library on [29/08/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons LicenseTable 1. Incremental training sets sequence. There were 36,490 images in
BMXSM dataset.

Overall error

= (

FN FP TP

)/(

+

+

TN FN FP

+

+

Training sets(i)a

Number of
images(n)

n/N (%)

Empty images
rateb (%)

1
2
3
4
5
6
7

7,298
10,947
14,596
18,245
21,894
25,543
29,192

20
30
40
50
60
70
80

80.36
80.52
80.50
80.35
80.53
80.48
80.47

a i represented serial number of training set Traini, i = 1, 2, …, 7. Here
Train1 was initial training set and Traini (i = 2, 3, …, 7) was in-
cremental training set.

Model Evaluation
Model performance evaluation included both errors and
time costs evaluation. The iteration termination condition
of the model was the same in each round training (i.e., the
loss rate of the model was less than a preset threshold ɛ
−4]). We used the same test set (Test) to
[here ɛ = 1.0 × 10
evaluate the performance of models in diﬀerent training
phases. The model error evaluation used three metrics that
included overall error, commission error, and omission
error, which were deﬁned by equations (2), (3) and (4).

Table 2. Main conﬁguration parameters of the two platforms used.

)

(2)

(3)

(4)

Commission error

=

/(
FP TP

+

FP

)

Omission error

=

/(
FN TP

+

FN

)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In addressing data bias during preprocessing of the deep learning pipeline, several techniques can be employed. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into subsets or strata based on certain criteria, such as class labels, before splitting it into training and testing datasets. This ensures that the distribution of classes in the training and testing datasets remains similar to that in the original dataset, thereby reducing any potential bias introduced due to imbalanced classes.

Oversampling and undersampling are techniques used to handle class imbalance in the dataset. Oversampling involves increasing the instances of minority classes, while undersampling reduces the instances of majority classes. Both methods aim to balance the representation of different classes in the dataset, thus mitigating any potential bias caused by class imbalance.

Diverse data collection refers to gathering data from various sources, ensuring that the dataset represents a wide range of scenarios and conditions. By incorporating diverse data, the risk of overfitting and biased results can be minimized, leading to more robust and generalizable models.

However, it should be noted that these techniques do not guarantee complete elimination of data bias but rather help reduce its impact on the deep learning pipeline. It is essential to carefully evaluate the effectiveness of these techniques using appropriate validation strategies and metrics tailored to the specific problem domain.

Based on the given context, there is no explicit mention of the techniques used to address data bias during preprocessing of the deep learning pipeline. However, considering the common practices in machine learning and deep learning, the abovementioned techniques could potentially be utilized to tackle this issue.
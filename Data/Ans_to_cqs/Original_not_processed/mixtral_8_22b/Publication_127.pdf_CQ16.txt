Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

A well-known method for further improving classiﬁcation
accuracy is to use an ensemble of models at the same time
and average their predictions. After training all of the nine
models for each stage, we formed an ensemble of the trained
models by averaging their predictions (SI Appendix, Prediction
Averaging). More details about the architectures, training meth-
ods, preprocessing steps, and the hyperparameters are in SI
Appendix, Preprocessing and Training. To enable other groups
to replicate our ﬁndings and harness this technology for their
own projects, we are publishing the software required to run
our experiments as freely available, open-source code. We are
also publishing the ﬁnal DNNs trained on SS so that others
can use them as is or for transfer learning. Both the code and
the models can be accessed at https://github.com/Evolving-AI-
Lab/deep learning for camera trap images.

The training set contained 1.4 million images, and the test set
contained 105,000 images. Since the SS dataset contains labels
for only capture events (not individual images), we assigned the
label of each capture event to all of the images in that event. All
of the architectures achieved a classiﬁcation accuracy of >95.8%
on this task. The VGG model achieved the best accuracy of
96.8% (Table 2). To show the difﬁculty of the task and where the
models currently fail, several examples for the best model (VGG)
are shown in SI Appendix, Results on the Volunteer-Labeled Test
Set, and SI Appendix, Fig. S10 shows the best model’s confusion
matrix.

included as well in case they improve performance (36–40).

same procedure to task III, counting animals, human volunteers
were 90.0% accurate, and to match them, we thresholded at 79%.
As a result, we can automatically count 44.5% of the nonempty
images and therefore 75% × 100% + 25% × 44.5% = 86.1% of
the data. For more details and plots, refer to SI Appendix, Con-
ﬁdence Thresholding. We could not perform this exercise for
task IV, additional attributes, because SS lacks expert-provided
labels for this task, meaning human-volunteer accuracy on it is
unknown.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not provide specific information regarding the criteria used to determine when training is completed, such as a validation loss plateau. However, based on common practices in machine learning, some possible criteria include reaching a certain number of epochs, achieving a desired level of accuracy or loss on the validation set, or observing no significant improvement in validation metrics over multiple epochs.
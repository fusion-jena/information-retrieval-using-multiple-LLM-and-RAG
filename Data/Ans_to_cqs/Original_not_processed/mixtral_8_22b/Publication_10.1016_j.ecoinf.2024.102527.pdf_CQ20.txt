Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

respectively.  N  represented  the  number  of  images  in  the  test  set  or 
validation set. 

2.2.6. Model training and hyper-parameters 

We  trained  models  on  the  training  sets  and  tracked  the  model 
training process on the validation set. The training process was stopped 
when  the  loss  value  of  the  model  on  the  validation  set  no  longer 
decreased. We then tested the model performance on the test set. Our 
computing platform was a Dell PowerEdge C4130 rack server with two 
Tesla  K80  GPUs  and  256  GB  of  memory.  For  the  deep  learning  envi-
ronment,  we  utilized  the  PyTorch  1.7.1  platform.  We  employed  the 
Stochastic Gradient Descent (SGD) optimizer with a momentum value of 
Î¼  = 0.9 to train the model. Other hyper-parameter settings for model 
training were shown in Table S.4 of the Supporting Information. 

3. Results 

3.1. Experiment results of transfer strategy optimization

(2)  Source model optimization 

Fig. 2. Five different transfer learning strategies. The light purple layers and 
the green layers represented the fixed layers and updating layers respectively. 
(For interpretation of the references to colour in this figure legend, the reader is 
referred to the web version of this article.) 

The main  objective  of  this  experiment  was  to  investigate how  the 
source model affects the performance of the transfer learning model. We 
focused on two key aspects of the source domain training set used to 

EcologicalInformatics80(2024)1025274D.-Q. Yang et al.                                                                                                                                                                                                                                

Fig. 4. Source model optimization experiment scheme.

computing resources when building source models for transfer learning 
tasks.

Regarding source model construction, we suggest that practitioners 
with abundant computational resources should build the source model 
with a larger number of readily available labeled samples. Doing so can 
contribute  to  improved  transfer  learning  performance.  However,  for 
those  with  limited  computational  resources,  it  may  be  necessary  to 
compromise between sample size and computational resources based on 
the potential threshold effect. 

CRediT authorship contribution statement

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors use a Dell PowerEdge C4130 rack server with two Tesla K80 GPUs and 256 GB of memory for their computing platform. They utilize the PyTorch 1.7.1 platform for their deep learning environment. This information suggests that they use GPUs for training their deep learning model.

Answer:::
Answer: The authors use a Dell PowerEdge C4130 rack server with two Tesla K80 GPUs and 256 GB of memory for their computing platform. They utilize the PyTorch 1.7.1 platform for their deep learning environment. This information suggests that they use GPUs for training their deep learning model.
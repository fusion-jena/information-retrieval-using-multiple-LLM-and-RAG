Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(cid:0)

̂ηs(x) = exp
exp

∑

j

)

θ(x)
f s
(
f j
θ(x)

)

where θ is the set of parameters of the neural network to be optimised by 
minimizing the loss function of Eq. (2). 

Using this very common deep learning framework, it is possible to 
show that the species assemblage predictor Sλ(x) of Eq. (1) is consistent 
λ(x) when  the 
(Lorieul,  2020),  i.e.  it  tends  towards  the  optimal  set  S*
number  of  training  samples  increases.  In  other  words,  our  species 
assemblage predictor is as simple as training a deep neural network with 
a cross-entropy loss function on the presence-only samples and thresh-
olding  the  output  softmax  probabilities  to  get  the  assemblage  of  pre-
dicted species. 

◦

2.2.1. Definition 

The objective is to optimise a model returning likely species assem-
blages worldwide while being learned on a set of presence-only obser-
vations. To do so, we optimise a deep species distribution model (Botella 
et  al.,  2018b)  and  further  calibrate  it  to  return  species  assemblages 
including the initial species observed with very high confidence. This 
method is derived from what is called set-valued prediction (or set-valued 
classification) in the machine learning community (Chzhen et al., 2021; 
Mortier et al., 2021). The model is trained on presence-only data, all 
species combined (multi-species SDM), and is then used to predict a set 
of  labels  by  thresholding  the  SDM  output  categorical  probabilities 
associated to species.

Contents lists available at ScienceDirect 

Ecological Informatics 

journal homepage: www.elsevier.com/locate/ecolinf 

Mapping global orchid assemblages with deep learning provides novel 
conservation insights 

Joaquim Estopinan a, b, c,*, Maximilien Servajean b,d, 1, Pierre Bonnet c, 1, Alexis Joly a, b, 1, 
François Munoz e,1 
a INRIA, Montpellier, France 
b LIRMM, Univ. Montpellier, CNRS, Montpellier, France 
c UMR AMAP, CIRAD, Montpellier, France 
d AMIS, Univ. Paul Val´ery Montpellier, Univ. Montpellier, CNRS, Montpellier, France 
e LIPHY, Univ. Grenoble Alpes, CNRS, Grenoble, France   

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Spatial indicator 
Species assemblage 
Deep learning 
Species distribution modelling 
IUCN status 
Orchids

We believe that predictors of large spatial patterns may play a sig-
nificant role in the regional diversity of orchids, and that the computer 
vision model can learn such information. The model’s strength is to rely 
on the best possible input set and exploit complex interactions in order 
to be as predictive as possible. The trade-off is interpretability, but the AI 
community  is  investing  heavily  in  this area  and  our  understanding is 
getting  finer  (Linardatos  et  al.,  2021).  For  example,  deep-SDMs  have 
been shown to construct a feature space with structured functional traits 

and bioclimatic preferences, even though only remote sensing data were 
provided (Deneu et al., 2022). 

4.2. Error and uncertainty quantification

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, there isn't any explicit mention of strategies employed to handle randomness in the deep learning pipeline, specifically related to random seed values. However, some commonly used techniques to manage randomness in deep learning pipelines include:

1. Seed Fixing: One popular method for handling randomness is fixing the seed value before running experiments. By setting a fixed seed, one ensures reproducibility across different runs of the same experiment. This technique helps in debugging and comparing results between multiple runs.

2. Ensemble Methods: Another strategy to deal with randomness involves using ensemble methods. These methods combine predictions from multiple models, each initialized with different seeds, to improve overall performance and reduce variance caused by random initialization.

3. Data Augmentation: Randomness can also be introduced through various forms of data augmentation. Techniques like rotation, scaling, cropping, etc., help create new training examples and increase the robustness of the model against variations in the input data.

4. Dropout Regularization: Dropout regularization is another way to introduce controlled randomness during training. It randomly drops out neurons during training, which helps prevent overfitting and improves generalization capabilities.
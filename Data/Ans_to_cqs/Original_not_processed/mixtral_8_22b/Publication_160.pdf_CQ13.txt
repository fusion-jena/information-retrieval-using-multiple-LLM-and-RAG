Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

60.64
63.82
65.77
66.30

60.17
63.57
65.54
65.94

Table 3
Performances of different ResNet architectures on validation and test sets. ResNetX-Y is written so that X indicates the network's depth and Y the input size. In bold
the best value for each metric.

Network -patch Size

Batch size

Validation set

Test set

Macro-F1

Top-1 accuracy

Micro-F1

Macro-F1

Top-1 accuracy

Micro-F1

ResNet152–224
ResNet50–128
ResNet18–128
ResNet18–224
Ensemble

16
128
128
200
128

37.45
52.04
51.40
54.93
60.56

62.38
64.07
63.90
66.70
70.60

60.46
63.85
63.88
66.44
70.35

38.26
52.27
51.62
53.93
60.38

61.71
64.35
63.60
66.30
70.54

60.09
63.89
63.44
65.94
70.37

from underwater video using neural networks. Opt. Express 13, 8766. https://doi.
org/10.1364/OPEX.13.008766.

Masters, D., Luschi, C., 2018. Revisiting Small Batch Training for Deep Neural Networks.
McGill, B.J., Dornelas, M., Gotelli, N.J., Magurran, A.E., 2015. Fifteen forms of biodi-

versity trend in the Anthropocene. Trends Ecol. Evol. 30, 104–113. https://doi.org/
10.1016/j.tree.2014.11.006.

Mehdipour Ghazi, M., Yanikoglu, B., Aptoula, E., 2016. Open-Set Plant Identification

Using an Ensemble of Deep Convolutional Neural Networks. Working Notes of CLEF.
Mishkin, D., Sergievskiy, N., Matas, J., 2016. Systematic evaluation of CNN advances on
the ImageNet. Comput. Vis. Image Underst. 161, 11–19. https://doi.org/10.1016/j.
cviu.2017.05.007.

Niculescu-Mizil, A., Caruana, R., 2005. Predicting good probabilities with supervised

6.2.1. Simple CNNs and ensemble network

All validation performances consistently increased with the patch
size up to 224 × 224, which achieved 65.94 micro-F1 (see Table 2). As
for training curves, the gap between patch size performances tended to
decrease quickly. It is worth emphasizing that macro-F1 (with all
classes weighted equally) was consistently lower than micro-F1 (all
classes weighted according to their proportion in the dataset).

The ResNet18 and ResNet50 trained with the same hyperparameters

Table 1
Classification accuracy of hosts (n = 4) and visitors (n = 20) on the Moorea
dataset (Beijbom et al., 2012). Values are given in mean percentage ± standard
deviation.

Subject

Macro-F1

Top-1 accuracy

Micro-F1

hosts
visitors

74.86 ± 3.63
59.4 ± 4.15

79.21 ± 2.35
64.98 ± 6.31

80.32 ± 2.32
68.31 ± 4.64

the limit.

Implementation of selective classification requires a good calibra-
tion of the network, i.e. the network should output a probability score
equivalent to the empirical accuracy; for an output score of 0.8 for
example, the network should predict the right class in 80% of the cases.
However, a recent study showed that deep neural networks are not
always well calibrated, and they tend to output over-confident predic-
tion scores (Guo et al., 2017). Hyperparameters, such as depth, batch
normalization, and weight decay, have been shown to influence cali-
bration. Scaling the logits passed to the softmax function can also help
calibrate the network.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide explicit information about the specific hyperparameters used in the deep learning models, such as learning rates or optimizers. However, it mentions that ResNet18 and ResNet50 were trained with the same hyperparameters. Additionally, it discusses how certain hyperparameters like depth, batch normalization, and weight decay can impact the calibration of the network. Unfortunately, without more detailed information, we cannot determine the exact values of these hyperparameters.
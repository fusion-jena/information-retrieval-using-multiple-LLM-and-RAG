Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

For the training process, a batch size of 20 images and an RMSprop optimiser with a learning rate of 0.001 and 
a learning rate decay of 0.0001 was used. The chosen loss function was mean squared error, while the prediction 
accuracy was quantified by the MAE of the respective dataset. The MAE of the validation dataset was computed 
after each epoch. Models were trained until the validation MAE did not further improve compared to the preced-
ing epochs and diverged from the training MAE (’overfit’). The trained model was then applied to the test dataset.
All CNN were implemented using the Keras API version (2.3.0.0)54 and the TensorFlow backend (version 
2.2.0)55 in R (version 3.6.3)23. Model training was undergone on a workstation with two CUDA-compatible 
NVIDIA GPUs (GeForce RTX 2080 Ti, CUDA version 11.0).

Vol.:(0123456789)www.nature.com/scientificreports/Training  process  and  hyperparameters. 
In  order  to  build  upon  a  pre-existing  knowledge  base,  we 
employed  ’transfer  learning’  by  using  pre-trained  layer  weights  (the  storage  of  the  model’s  knowledge)  from 
a classification task on a dataset on www. image- net. org38 for all CNN models used in this study. The regressor 
following the basic CNN consisted of a global average pooling layer followed by two dense layers with 512 and 1 
output units. The latter forces the CNN to output exactly one prediction (trait) value. In case of the mixed data 
model (setups (3) and (4)), the CNN consisted of parallel branches to incorporate the different input data types. 
The branch processing the bioclimatic data consisted of three dense layers with 64, 32 and 4 output units, and 
the last layer of the CNN regressor contained 4 output units. After concatenating the two branches (image and

extracted a random sample of 10% of the dataset of each trait before model training. This ‘test dataset’ was not 
involved in the training process and exclusively served for the independent evaluation of the trained models. 
The remaining data was split into ’training dataset’ and ’validation dataset’ by a ratio of 4:1 (Table 1). The train-
ing dataset was employed to train the weights of the CNN model, whereas the validation dataset indicated the 
training progress after each full training cycle (’epoch’).

53.  Hernández-García, A. & König, P. Further advantages of data augmentation on convolutional neural networks. ICANN 95–103, 

(2018).

 54.  Allaire, J. & Chollet, F. keras: R Interface to ’Keras’. R package version 2.3.0.0 (2020).
 55.  Allaire, J. & Tang, Y. tensorflow: R Interface to ’TensorFlow’. R package version 2.2.0.9000 (2020).
 56.  Hijmans, R. J. raster: Geographic Data Analysis and Modeling. R package version 3.4–5 (2020).
 57.  Wei, T. & Simko, V. corrplot: Visualization of a Correlation Matrix. R package version 0.84 (2017).

Author contributions
T.K. conceived the study and assisted in conducting the experiments. C.S. planned and conducted the experi-
ments, performed the analyses, and wrote the manuscript. T.K. and S.S. assisted in the analysis and in writing 
the manuscript. C.B. and A.M.-M. contributed data and helped to interpret the results. All authors provided 
edits and reviewed the manuscript.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there is no explicit mention of specific regularization methods such as dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. However, it can be inferred that some form of regularization has been implemented indirectly through the use of certain techniques and parameters during the training process.

Firstly, the choice of a relatively small batch size of 20 images could potentially act as a form of implicit regularization. Smaller batch sizes introduce more noise into the gradient estimation, which can have a similar effect to adding explicit regularization terms like weight decay or dropout.

Secondly, the use of a learning rate decay of 0.0001 also suggests that some form of regularization might be taking place. Learning rate decay reduces the learning rate over time, which can help prevent overfitting by making the optimization process more stable and reducing the chance of getting stuck in local minima.

Lastly, the fact that models were trained until the validation MAE did not further improve compared to the preceding epochs and diverged from the training MAE ('overfit') indicates that early stopping was employed as a form of regularization. Early stopping involves monitoring the performance of the model on a validation set during training and halting the training process once the validation performance stops improving. This helps prevent overfitting by avoiding unnecessary iterations that may cause the model to learn noise instead of generalizable patterns.

However, without explicit mentions of dropout, L2 regularization, or other common regularization techniques, it cannot be definitively stated what specific regularization methods were used in this particular deep learning pipeline.
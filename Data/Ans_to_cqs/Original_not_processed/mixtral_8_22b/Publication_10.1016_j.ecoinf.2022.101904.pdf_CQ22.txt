Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics72(2022)1019047A. Jamali et al.                                                                                                                                                                                                                                  

Table 5 
Results  of  the  proposed  deep  model  (ViT  = Vision  Transformer,  ST  = Swin 
Transformer, KI=Kappa index, AA = Average accuracy, OA = Overall accuracy).  

Class 

ViT 

ST 

CoAtNet 

CNN 
+ ST 
(ours) 

GAN 
+ ST 
(ours) 

3DUNetGSFormer 
(ours) 

Bog 
Fen 
Marsh 
Swamp 
Shallow 
water 

Urban 
Deep 

water 
Upland 
KI (%) 
OA (%) 
AA (%) 
Time (h) 

0.59 
0 
0.46 
0 
0.83 

0.97 
0.93 

0.86 
0.79 
0.88 
0.76 
0.92 

0.99 
0.97 

0.91 
0.86 
0.94 
0.82 
0.98 

0.99 
1 

0.87 
71.31 
75.62 
62.96 
2.2 

0.96 
90.66 
91.99 
88.67 
1.5 

0.97 
94.67 
95.43 
93.21 
5 

0.82 
0.76 
0.95 
0.89 
0.91 

0.98 
1 

0.98 
92.84 
93.87 
90.16 
1.5 

0.89 
0.83 
0.88 
0.9 
0.96 

0.99 
0.99

3.4. Impact of data split 

This  section  illustrates  the  impact  of  the  data  split  (i.e.,  test  and 
training data ratio) on the final accuracy of wetland mapping. By eval-
uating  the  classification  results  of  the  proposed  model  with  different 
training data, we will better understand the required amount of data to 
reach  an  acceptable  wetland  accuracy  level.  This  is  significant  when 
there is a limited number of reference data in a project. We utilized a 

different ratio of training data from reference data. The training ratio 
started from 10% to 90%, increasing by 10%. Based on the results, as the 
amount of data gradually increased, the classification accuracy of our 
method was improved. This improvement was substantial for increasing 
the  training  ratio  from  10%  to  30%  (average  accuracy  improved  by 
17.47%).  However,  average  accuracy  just  increased  by  7.01%  by 
increasing the training ratio from 30% to 90% (see Fig. 10).

of our method improved considerably. The average accuracy improved 
by approximately 10% by increasing the image patch size from 4 to 12, 
reaching an overall accuracy of 99.23%. There are several points that 
were noticed by increasing the image patch size, as follows. The spatial 
information is not sufficient for small image patch sizes (i.e., 4). On the 
other  hand,  the  classification  time  significantly  increases  when  the 
image  patch  size  is  too  large  as  the  computing  complexity  rises  sub-
stantially (i.e., 12). Another issue with large image patch size is the fact 
that linear objects (e.g., roads) will lose their edge smoothness, as seen in 
Fig. 11. Moreover, it was evident that increasing image patch size leads 
to difficulty in the precise recognition of small objects as they will be 
merged into their neighboring features. 

3.6. Cost of computation in terms of time

shallow water, respectively (see Table 5). The inclusion of a CNN-based 
feature extractor (CNN + ST) improved the classification accuracy of the 
Swin Transformer by 2.18%, 1.88%, and 1.49% in terms of the kappa 
index,  overall  accuracy,  and  average  accuracy,  respectively.  On  the 
other hand, the inclusion of synthetic data with the Swin Transformer

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper uses three main evaluation metrics to assess the performance of the deep learning models: Kappa Index (KI), Overall Accuracy (OA), and Average Accuracy (AA). These metrics are presented in Table 5 for each class and model. Additionally, the table includes Time (h) as another metric, which measures the computational cost of each model.

The Kappa Index is a statistical measure used to determine inter-rater reliability or agreement between two raters who categorize items into mutually exclusive categories. In this case, it is used to compare the agreement between the predicted classes and the actual classes. A higher value indicates better agreement.

Overall Accuracy represents the proportion of correctly classified instances out of all instances. It provides a general idea of how well the model performs across all classes.

Average Accuracy calculates the mean accuracy for each class separately and then averages them together. This metric helps identify if there are any biases towards certain classes in the model's predictions.

Finally, Time (h) measures the computational cost of running each model. Lower values indicate faster processing times and more efficient models.
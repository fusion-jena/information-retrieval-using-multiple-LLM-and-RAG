Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EN 

KNN 
GP 

TREE 

RF 

XGB 

SVR 

MLP 

alpha 
l1_ratio 
neighbours 
weights 
p 
alpha 
max_depth 
min_samples_split 
min_samples_leaf 
max_features 
n_estimators 
max_depth 
min_samples_split 
min_samples_leaf 
max_features 
n_estimators 
learning_rate 
max_depth 
subsample 
colsample_bytree 
reg_alpha 
gamma 
min_child_weight 
C 
gamma 
epsilon 
hidden_layer_sizes 
activation 
alpha 
learning_rate 
max_iter 

* BP denotes Best Parameter. 

(cid:0) 1] 

(cid:0) 5, 10

(cid:0) 2, 10

hyperparameter optimisation process, the performance metrics detailed 
in Section 2.5 are meticulously evaluated using the designated test set. 
Subsequently, based on the performance metrics obtained, we discern 
and  select  the  most  effective  models  for  each  target  variable.  For  an 
illustrative overview of this model selection process, kindly refer to the 
visual representation provided in Fig. 4.

Cateni, S., Colla, V., 2016. Variable selection for efficient design of machine learning- 
based models: Efficient approaches for industrial applications. In: Engineering 
Applications of Neural Networks: 17th International Conference, EANN 2016, 
Aberdeen, UK, September 2-5, 2016, Proceedings 17. Springer International 
Publishing, pp. 352–366. https://doi.org/10.1007/978-3-319-44188-7_27. 

Chantry, M., Christensen, H., Dueben, P., Palmer, T., 2021. Opportunities and challenges 
for machine learning in weather and climate modelling: hard, medium, and soft AI. 
Philos. Trans. R. Soc. A Math. Phys. Eng. Sci. 379, 20200083. https://doi.org/ 
10.1098/rsta.2020.0083. 

Chen, T., Guestrin, C., 2016. Xgboost. In: Proceedings of the 22nd ACM SIGKDD 

Citakoglu, H., Cos¸kun,

EcologicalInformatics81(2024)1026384D. Garcia-Rodriguez et al.                                                                                                                                                                                                                    

Fig. 4. Flowchart of the proposed models.  

XGB  optimises  predictive  modelling  through  gradient  boosting  and 
regularisation (Chen and Guestrin, 2016). SVR uses kernel functions to 
efficiently handle non-linear data by fitting the best hyperplane within a 
pre-defined  error  margin  (Hsu  et  al.,  2003).  MLP  is  a  feed-forward 
neural network that learns complex patterns using adjustable weights 
and activation functions (Kingma and Ba, 2014). 

2.5. Performance evaluation measures

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about the specific optimization techniques used in the deep learning pipeline. However, it can be inferred from the mention of 'gradient boosting' in relation to XGBoost that some form of gradient descent optimization technique might have been employed. Additionally, the reference to Kingma and Ba (2014) suggests that the Adam optimization algorithm could potentially be utilized since they introduced the Adam method for first-order gradient-based optimization of stochastic objective functions. Nevertheless, without more concrete evidence or direct statements regarding the use of these optimization techniques, one cannot definitively state their application in the deep learning pipeline.

Reference(s):
Chen, T., & Guestrin, C. (2016). Xgboost: A scalable tree boosting system. KDD’16: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 785-794.

Kingma, D. P., & Ba, J. L. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
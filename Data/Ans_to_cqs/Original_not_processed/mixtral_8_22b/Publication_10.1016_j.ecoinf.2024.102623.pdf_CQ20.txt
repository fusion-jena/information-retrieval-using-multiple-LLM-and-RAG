Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

vancements in deep learning to enhance performance. Specifically, our 
MLP architecture consists of four layers, each containing 512 neurons 
and connected with residual connections (He et al., 2016). We employ 
batch normalization (Ioffe and Szegedy, 2015) and the Rectified Linear 
Unit (ReLU) activation function in all layers except the final one, where 
instead a sigmoid function is used to enable multi-label classification. 
The model is trained with a batch size of 256 for 30 epochs using the 
AdamW optimizer (Loshchilov and Hutter, 2017). Both the weight decay 
and learning rate are set to 0.0001. Additionally, we employ a learning 
rate scheduler with exponential decay of 0.95, and introduce dropout

Table B.4 
Mean AUC performance of other alternative ML loss functions. The performance is averaged over 10 random seeds.   

CE 
weighted CE 
Focal CE (γ = 0.5) 
Focal CE (γ = 1) 
Focal CE (γ = 2) 
Focal CE (γ = 5) 
Focal full weighted (γ = 0.5) 
Focal full weighted (γ = 1) 
Focal full weighted (γ = 2) 
Focal full weighted (γ = 5) 
LDAM (C = 0.1) 
LDAM (C = 1) 
LDAM (C = 10) 
DB loss (λ = 3) 
DB loss (λ = 5) 
Entmax (α = 0.01) 
Entmax (α = 0.05) 
Entmax (α = 0.1) 
Full weighted (λ2 = 0.5) 
Full weighted (λ2 = 0.8) 
Full weighted (λ2 = 1) 

AWT 

0.663 
0.670 
0.663 
0.663 
0.662 
0.660 
0.680 
0.686 
0.689 
0.682 
0.663 
0.662 
0.648 
0.669 
0.668 
0.665 
0.680 
0.676 
0.698 
0.704 
0.696 

CAN 

0.718 
0.706 
0.718 
0.718 
0.718 
0.719 
0.684 
0.682 
0.678 
0.665 
0.718 
0.718 
0.719 
0.704 
0.698 
0.687 
0.697 
0.683 
0.650 
0.696 
0.714 

NSW

We  adopt  a  configuration  similar  to  the  multi-species  model  pre-
sented in Zbinden et al. (2023). Instead of developing individual models 
for each species, we train a single multi-layer perceptron (MLP) model for 
each region (details about the six regions considered are given in Section 
3.4). These models predict suitability scores for every species within the 
region following a multi-label classification approach. MLPs are a type 
of neural network consisting of multiple layers of fully-connected neu-
rons  with  non-linear  activation  functions  between  layers  (Gorishniy 
et al., 2021), enabling them to capture complex interactions among the 
input environmental covariates. This architecture is well-suited for the 
tabular format of the dataset in use (see Section 3.4). For the model and 
training hyperparameters, we stick to standard values (Gorishniy et al., 
2021;  Mac  Aodha  et  al.,  2019),  while  also  incorporating  recent  ad-

Cambridge University Press. 

Gorishniy, Y., Rubachev, I., Khrulkov, V., Babenko, A., 2021. Revisiting deep learning 

models for tabular data. Adv. Neural Inf. Proces. Syst. 34, 18932–18943. 
Grinsztajn, L., Oyallon, E., Varoquaux, G., 2022. Why do tree-based models still 

outperform deep learning on typical tabular data?. In: In Thirty-sixth Conference on 
Neural Information Processing Systems Datasets and Benchmarks Track. 
Guisan, A., Tingley, R., Baumgartner, J.B., Naujokaitis-Lewis, I., Sutcliffe, P.R., 

Tulloch, A.I., Regan, T.J., Brotons, L., McDonald-Madden, E., Mantyka-Pringle, C., 
et al., 2013. Predicting species distributions for conservation decisions. Ecol. Lett. 16 
(12), 1424–1435. 

EcologicalInformatics81(2024)10262313R. Zbinden et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there is no information available regarding the specific hardware resources used for training the deep learning model in this case. The text only provides details about the model architecture, such as the number of layers, neurons, and the types of connections, along with the optimization techniques employed during training. However, it does not mention any specific hardware resources like GPUs or TPUs.
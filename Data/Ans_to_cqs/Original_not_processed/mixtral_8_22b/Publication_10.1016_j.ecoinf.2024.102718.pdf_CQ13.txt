Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The proposed BEiT model exhibits considerable performance im-
provements in the classification task compared to various conventional
deep learning models, as summarized in Table 16. Achieving an accu-
racy of 98.1%, the BEiT model exceeds all other State-of-the-art models,
including the previously top-performing model by Farian et al. (96.1%).
This improvement in accuracy is statistically significant, with signifi-
cance denoted by the α markers, indicating p < 0.05 using a two-sided
McNemar’s test against the BEiT model. It achieves a precision of
98.2%, a recall of 97.7%, and an F1-score of 96.5%, which are higher
than those of other models such as the ViT-30, ViT-20, and LAMP-LFD
models. Vivek et al.’s model, which shows high precision (94.5%) and
recall (95.0%), still falls short of the BEiT model’s comprehensive

Table 11
Ablation study evaluating BEiT model performance without added noise.

Model

Acc

Loss

AUC

CK

Precision

Recall

F1

98.33
99.33
97.33

0.092
0.082
0.121

In the BEiT model architecture, our initial steps involve loading the
BEiT model and strategically freezing a substantial portion of its layers
approximately 7 of the total layers. By doing so, the model freezes the
weights in these layers, focusing the training process on fine-tuning the
remaining layers. The model is then derived by isolating the output of a
specific layer (‘out_ln’) from the BEiT architecture. To enhance training

Initially, we assess our modified BEiT model’s performance against
existing deep learning and base ViT models under both noise-added and
noise-free situations. The results obtained from the PlantVillage are
presented dataset in Tables 6 and 7, The results reveal that the proposed
BEiT (12 M) achieves an outstanding accuracy of 97.33% with the least
loss. It outperforms in AUC, CK, precision, recall, and F1 scores in both
noisy and noiseless conditions. Similarly, the Dhan-Shomadhan dataset
in Tables 8 and 9, the low parameterized BEiT (12 M) model maintains
its lead he highest accuracy (96.22% and 97.50%) and least loss (0.131
and 0.121). It achieves reliable metrics in AUC (0.962), CK (0.88),
precision (0.97), recall (0.96), and F1 score (0.96). Other models also
perform well but continually fall below BEiT (12 M)’s specifications
across both noisy and noiseless situations. These results underline BEiT
(12 M)’s better performance and reliability in leaf disease classification

To assess the feasibility of the proposed model, several sensitivity
experiments have been conducted. Therefore Table 13 presents the re-
sults of a sensitivity study conducted on the BEiT (12 M) model, inves-
tigating its performance under various experimental setups. These
setups allow modifications to crucial parameters like learning rate,
batch size, regularization intensity, and training epochs. Through this
research, we identify the model’s reactivity to parameter changes, of-
fering light on the ideal settings for maximizing its performance. The
experimental setups include modifications to the image size, ranging
from 128 × 128 to 224 × 224 pixels. As the image size grows, the model
displays increasing performance across all criteria as it collects the
global and local feature values of diseased image values. Notably, the

Table 8
Classification Performance on Dhan-Shomadhan Dataset for Rice Leaf Diseases (With Noise Added).

Model

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given information, there isn't any explicit mention of the exact hyperparameters used in the deep learning model, such as the learning rate or optimizer. However, it is mentioned that a sensitivity experiment was conducted on the BEiT (12 M) model, investigating its performance under various experimental setups. These setups allowed modifications to crucial parameters like learning rate, batch size, regularization intensity, and training epochs. Unfortunately, the specific values for these hyperparameters were not disclosed in the provided context.
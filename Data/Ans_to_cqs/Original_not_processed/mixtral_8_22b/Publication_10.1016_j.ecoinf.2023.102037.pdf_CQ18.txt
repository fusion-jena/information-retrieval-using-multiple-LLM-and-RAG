Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.2. Model performance 

To validate our model in a practice-oriented way, we split our data 
on the sticky plate level. Therefore, individual insects that belonged to 
the  same  sticky  plate  image  were  kept  together  in  the  same  dataset 
(training, validation, or testing). This allowed us to validate our model’s 
performance by emulating the way in which the model will be used in 
practice. Namely, it predicted a class for any given object it detected on 
unseen sticky plate images – data it never saw during training.

ayanan, 2022). In our previous research (Kalfas et al., 2021; Kalfas et al., 
2022), we showed that model performance can be highly overestimated 
when weak validation procedures – like random data splitting- are fol-
lowed.  Despite  taking  the  above  limitations  into  consideration,  other 
studies  still  only  focused  on  datasets  where  insect  classes  were  quite 
broad – insect order or family level, quite dissimilar and thus, relatively 
easy to classify (Rustia et al., 2021b; Wang et al., 2020c).

• Aperture: F/10  
• ISO: 160  
• Shutter speed: 1/15 s  
• Focal Length: 55 mm  
• White balance: “Incandescent” 

2.3. Data splitting and image tiling

From a model-centric view misclassifications are often explained by 
the model complexity, its hyperparameters and the training procedure 
efficiency.  Hence,  many  researchers  employ  a  variety  of  models  and 
training  procedure  optimizations  until  they  find  the  “best”  result. 
Moreover, as we have already shown in our previous research (Kalfas 
et al., 2021; Kalfas et al., 2022), several studies on insect recognition 
don’t  apply  any  strict  validation  procedure  to  evaluate  their  models, 
which often leads to over-optimistic or unrealistic performance. 

From a data-centric approach, wrong detections are assumed to be 
caused by either intra- or inter-class variability (Fig. 2 and Fig. 3), class 
imbalance (Fig. 4) or labelling errors. 

2.2. Imaging setup

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The authors employed a specific strategy to validate their model's performance in a practical manner. They divided their data based on the sticky plate level, ensuring that all insects from the same sticky plate image remained within the same dataset (i.e., training, validation, or testing). By doing so, they could assess the model's ability to predict a class for any identified object on previously unseen sticky plate images—data not included in the training phase. This methodology aimed to mimic how the model would function in real-world applications. Furthermore, the authors highlighted the importance of using robust validation methods instead of simpler ones, such as randomly dividing the data, which may lead to inflated estimates of model performance.
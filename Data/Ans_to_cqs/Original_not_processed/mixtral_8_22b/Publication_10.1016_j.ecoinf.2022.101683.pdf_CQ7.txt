Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

kernel range). The threshold used was the 10th percentile training presence. For each metric the best model is highlighted in bold where higher was better for AUC, TPR, TSS and Kappa and lower better for Omission.

0.875 
0.024 
0.977 
0.635 
0.750 

a) Training set (75% random)                       
AUC 
Omission 
TPR 
k 
TSS 
b) Test set #1 (25% random)                       
0.662 
cAUC 
0.083 
Omission 
0.917 
TPR 
0.094 
k 
0.522 
TSS 
c) Test set #2 (Irish records only)                       
0.621 
cAUC 
1.000 
Omission 
0.000 
TPR 
(cid:0) 0.003 
k 
(cid:0) 0.394 
TSS 

0.701 
1.000 
0.000 
0.000 
(cid:0) 0.585 

0.687 
1.000 
0.000 
(cid:0) 0.003 
(cid:0) 0.228 

0.780 
0.014 
0.986 
0.210 
0.758 

0.753 
0.014 
0.986 
0.166 
0.704 

0.785  
0.181  
0.819  
0.485  
0.571  

0.691  
0.172  
0.828  
0.144  
0.580  

0.849 
0.042 
0.958 
0.459 
0.698 

0.757 
0.028 
0.972 
0.104 
0.713 

0.824 
0.041 
0.959 
0.393 
0.647 

0.729 
0.033 
0.967 
0.080 
0.655 

0.727 
0.122 
0.878 
0.242 
0.454 

0.584 
0.211 
0.789 
0.211 
0.365 

0.708  
0.330  
0.670  
0.275  
0.415  

0.599  
0.350  
0.650  
0.052  
0.396  

0.783 
0.167 
0.833 
0.553 
0.566 

0.691 
0.152 
0.848 
0.226 
0.581

suggests convergence on the maximum test sensitivity plus specificity 
(TSSmax) as the single most appropriate threshold (Guisan et al., 2017). 
However, in this case we used two different test data sets and thus two 
different  thresholds  would  have  been  generated  making  each  non- 
comparable to the other. Thus, in this case we used the 10th percen-
tile training presence (Elith et al., 2010) which was the same across both 
test datasets allowing model evaluation metrics to be directly compared. 

2.4. Model evaluation

Table 1 
iSDM evaluation metrics for a) the training sets representing 75% of presence data selected at random, b) Test dataset #1 representing 25% of presence data selected at random and c) Test dataset #2 representing presence 
data from Ireland only for four spatial extents with five model treatments with i) random background points, ii) filtered (pre-thinned) input records adjusted for presence record density using OccurrenceThinner, iii) 
targeted selection of background points drawn from only those 1 km squares with a deer record, iv) filtered (pre-thinned) and targeted background point selection and v) weighted background points based on an estimated 
proxy for likely survey effort derived from a bias grid, throughout Great Britain and Ireland (GB & Ireland), Great Britain or restricted to the area currently colonised by muntjac (95% kernel range) or its core range (50%

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The data was split into three parts for deep learning model training: a training set, a test set #1, and a test set #2. The training set consisted of 75% of the presence data selected randomly. Test set #1 represented 25% of the presence data also selected randomly. Test set #2 contained all the presence data from Ireland only. This splitting method allowed for direct comparison between the models since they were evaluated against the same threshold (the 10th percentile training presence). However, it should be noted that this approach may not always be suitable when dealing with multiple test datasets due to potential differences in optimal thresholds.
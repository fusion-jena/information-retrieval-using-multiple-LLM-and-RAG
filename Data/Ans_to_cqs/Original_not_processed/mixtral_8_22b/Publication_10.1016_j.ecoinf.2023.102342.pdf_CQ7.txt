Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.1. Operational workflow 

The whole process of data acquisition and processing, reported in the 
following  section,  is  called  Knowledge  Discovery  in  Databases  (KDD) 
(Kotu and Deshpande, 2015); it is a process of identifying patterns or 
relationships  within  a  dataset  useful  to  make  important  decisions 
(Hammad and AbouRizk, 2014). This process does not involve applying 
data-mining  techniques  but  requires  more  actions  to  reach  the  final 
result. 

The KDD sequence applied in our case study included the following 

steps:  

• identification of the aim for a specific study area;  
• pre-selection of useful data to achieve the objective;  
• analytical determinations for the detection of PPPs in Puglia surface 

water bodies;  

• optimisation and elaboration of the obtained results through:

in Fig. 2. 

The  numerical  values  of  some  parameters  (Tan  et  al.,  2019)  were 
calculated and provided as measures of the association values to explain 
the data mining results. The support parameter measures the abundance 
or  frequency  (often  interpreted  as  important)  of  a  set  of  items  in  a 
database. The value of the support parameter is given by the ratio be-
tween  the  number  of  times  an  item  appears  and  the  total  number  of 
items. We refer to a set of molecules as a “set of frequent molecules” if 
support  exceeds  a  specified  minimum  threshold  we  can  identify.  In 
general, the threshold depends on the size of the dataset.

In short, data mining ensures that starting from “cryptic” information 
scattered without apparent order in a database, we arrive at knowledge 
that can be exploited for various purposes (Kotu and Deshpande, 2015). 
It is also possible to implement (Online document, 2022b) an automatic 
elaboration  workflow  with  open-source  software  without  using  an 
operator who reprocesses the data each time. Data mining can be sup-
ported  by well-known  and  widely applied  data  processing techniques 
such as verifying or interpreting results (Hamdi et al., 2022).

The Apriori algorithm consists of a sequence of steps for identifying 
the most frequent itemset in a given database (Fig. 2). Two main pro-
cesses  are  performed  iteratively  to  locate  the  most  frequent  itemsets: 
joining and pruning. As the name indicates, join represents the union of 
itemsets,  and  pruning  is  a  data  compression  technique  in  machine 
learning and search algorithms that reduces the size of decision trees by 
removing  sections  of  the  tree  that  are  non-critical  and  redundant  to 
classify instances. Pruning reduces the complexity of the final classifier 
and  improves  the  predictive  accuracy  by  reducing  overfitting  (Zhao 
et al., 2009).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the provided context, there isn't explicit mention of the criteria used to split the data for deep learning model training such as train, test, and validation sets. However, I can provide general guidelines on how this splitting is typically done.

Data splitting is crucial in building robust and reliable models. Typically, the entire dataset is divided into three subsets: training, testing, and validation datasets.

1. Training Set: This subset is used to train the model, i.e., to fit the parameters of the model. It is usually the largest portion of the dataset.

2. Testing Set: After the model has been trained, it needs to be tested to see how well it performs on unseen data. This is where the testing set comes in. It helps evaluate the performance of the model and estimate its generalization error.

3. Validation Set: Also known as the development or dev set, this subset is used during the model development phase to fine-tune the model's hyperparameters. It provides feedback on the model's performance while tuning the parameters.

The common practice is to use 60-80% of the data for training, 10-20% for testing, and another 10-20% for validation. However, these percentages may vary depending on the size of the dataset and the problem at hand. For instance, when dealing with small datasets, one might opt for cross-validation instead of maintaining separate validation sets.
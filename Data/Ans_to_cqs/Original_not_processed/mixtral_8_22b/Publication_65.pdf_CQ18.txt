Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Method

FCN

U-Net

SegNet

DeepLabV3+

DDCN

Number of Parameters
(in millions)

3.83

1.86

2.32

Training Time
(GPU hours)

Inference Time
(GPU min.)

Inference Time
(CPU min.)

Inference Time
(GPU min./ha)

Inference Time
(CPU min./ha)

485

450

472

1.4

1.9

1

1.3

1.1

1.5

5.16

486

1.4

1.9

2.08

500

5.1

6.2

0.042

0.030

0.033

0.042

0.153

0.057

0.039

0.045

0.057

0.186

3.3. Visual Analysis

Some features maps, learned by the convolutional layers, are presented in Figure 12.
Speciﬁcally, this image presents low-, mid- and high-level feature maps learned by the
ﬁrst, a middle, and the last layers of the networks, respectively. We can see the each CNN
performs very differently from one another.

during the training phase for each evaluated input size, the best image resolution. Then
the DDCN processes the testing images using batches composed of the images with the
best-evaluated size (Figure 8).

3.2. Computational Complexity

This section compares the methods in terms of computational efﬁciency and computa-
tional load for training and inference. Table 2 presents the average training and inference
times measured on the hardware infrastructure described in Section 2.2. Considering that
the methods were trained with the same optimizer and learning rates, these results are
highly correlated with the network depth and the selected batch size. For instance, the
DDCN network is deeper than the others, and the consequence is that it took longer than
the other networks for training and inference.

The most signiﬁcant variation of performance is concerning the number of parameters
and with training and inference time. Despite being the best architecture in performance,
According to Table 2, DeepLabv3+ needed more parameters than the other architectures,
about 2.75 times more parameters than the U-Net, the least requiring one. The need for a
more signiﬁcant number of parameters often implies a higher demand for training samples
that our dataset or another dataset may not have met that the methods present in this
research paper may be applied, possibly causing the DeepLabV3+ architecture to perform
below its potential.

Remote Sens. 2021, 13, 3054

11 of 19

Figure 11. Convergence of the evaluated networks.

Table 2. Number of Parameters and Processing Time of the proposed approaches. The training time
represents the results for the test set of each method. The inference time stands for the time taken by
each model to make predictions for each image.

Method

FCN

U-Net

SegNet

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To monitor the model performance during training, the authors first determined the best image resolution for each evaluated input size. This was done during the training phase. Once the best image resolution was identified, the DDCN processed the testing images using batches composed of the images with the best-evaluated size. This approach allowed the authors to ensure optimal performance of their model during the training process.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.2. Process flow 

This study has two perspectives, model training and development on 
a high-end computer, and the model deployment to an edge device, as 
shown in Fig. 3. The training dataset could be pre-processed, such as 
denoising,  contrast  enhancement,  and  image  resizing.  We  only  used 
image re-sizing to make the images suitable for the input image reso-
lution required for the selected models. 

After a model has been trained, it needs to be validated and tested to 
confirm that it has achieved the desired performance. The performance 
might  be  measured  using  different  metrics,  such  as  classification 

Table 2 
The class distribution in the Extinction dataset.  

Class 

Number of images

improvement  with  pre-trained  model  customization.  A  pre-trained 
model,  trained  on  a  large  dataset  provides  an  advantage  of  transfer 
learning to be used for classification on another dataset and is especially 
useful as no training will be required. The pre-trained model can also be 
fine-tuned by training some layers with another dataset, in either case 
resulting in a quicker training compared to designing and training the 
model  from  scratch.  A  DNN  consists  of  millions  of  parameters  and 
therefore requires large datasets to be optimally trained to avoid over-
fitting,  that  is  learning  or  memorizing  the  features  from  a  smaller 
dataset,  resulting  in  poor  generalization.  One  alternative  for  transfer 
learning not considered in this study is to use a large camera trap dataset 
to fine-tune a pre-trained model on ImageNet. This trained model can 
then be used for transfer learning on a small camera trap dataset.

Neural architecture search (NAS) was used for developing the Effi-
cientNetV2B0 model, and for the optimization of parameter efficiency 
and  training  speed  (Tan  and  Le,  2021).  The  model  performance  was 
improved  using  progressive  learning  to  adapt  regularization  to  the 
image size (Tan and Le, 2021). EfficientNetV2 is a family of models (B0 
to B7), and B0 is the smallest variant in the family (Keras, n.d), with the 
other  models  progressively  increasing  in  performance  going  up  to 
EfficientNetV2B7.

3.5. Hyperparameter optimization 

The model training process determines the values for the trainable 
parameters  of  a  model,  e.g.,  Table  4 shows  the  size  of  the  model pa-
rameters. In addition, a DNN model also has other parameters that need 
to be selected, e.g., batch size, that determines the model performance. 
The  model  parameters  define  the  model  and  are  termed  as  hyper-
parameters. Each hyperparameter has a range of values, from which an 
optimum  selection  can  improve  the  model  performance.  The  model 
hyperparameters could be in hundreds, presenting a very large search 
space, but these differ in their relative importance. The common ones for 
a DNN are learning rate, batch size, and dropout.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps may be involved depending on the specific requirements of the task and the nature of the data. However, based on the given context, the only mentioned preprocessing step is image resizing. This step involves adjusting the dimensions of the input images to match the resolution required by the chosen models. Other commonly used preprocessing techniques include normalization, scaling, and cleaning. Normalization scales pixel intensities to a certain range, typically between 0 and 1, while maintaining the relationship between them. Scaling refers to changing the scale of the pixels without altering their relationships. Cleaning involves removing noise, irrelevant information, or outliers from the dataset to ensure better quality inputs for the model. These preprocessing steps help enhance the overall performance of the deep learning model during training and testing phases.
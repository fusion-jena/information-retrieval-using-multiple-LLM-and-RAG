Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

hyperparameters for model architectures.  

2.  Partial training: Models are partially trained (on a small fraction of 
the data) to quickly eliminate underperforming configurations.  
3.  Successive Halving: The remaining configurations are grouped into 
sets of different sizes, and the associated models are trained further. 
The best-performing configurations in each set are promoted to the 
next  stage,  while 
the  under-performing  configurations  are 
eliminated. 

4. Repeat: Steps 2 and 3 are repeated until only one configuration re-
mains, which is then considered the best configuration found.

described in the next section. 

During the training of the model, the data is then standardized. In 
section 4, one of the three datasets will be defined as a training base, on 
which the standardizaion parameters will be calculated and applied to 
the  other  two  datasets  considered  as  test  base.  The  best  result  of  the 
optimisation will allow us to determine the base that will be used for 
training. 

4. Model optimization: Determination hyperparameters and 
parameter training 

4.1. Optimisation algorithm and experimental design

For these studies, it is proposed to use the MAE (mean absolute error) 
metric  and  the  NSE  (Nash-Sutcliffe  efficiency)  score  for  the  training 
phase: 

NSE = 1 (cid:0)

∑T
∑T

t=1(yt (cid:0) ̂yt)2
t=1(yt (cid:0) yt)2

et MAE =

∑T

t=1(|yt (cid:0) ̂yt|)
T 

Two elements will be determined here:  

1.  The best training base between Rhˆone-Vallabregues, Loire-Ouzouer 

and Loire-Muides  

2.  The optimal size of the selected training base 

4.1.1.

Influence of the training base 

The influence of the training base on the results is seen by swapping 

EcologicalInformatics80(2024)1024637K. Pel´e et al.

A  clear  improvement  of  the  predictions  and  a  stabilisation  of  the 
error  can  be  observed  when  the  training  size  reaches  50%,  corre-
sponding to data from April 1983 to February 1995. Over this period of 
time, two time intervals can be distinguished: the first one from 1983 to 
1992 where high concentrations of 137Cs are found in the SPM of Rhˆone- 
Vallabregues station which is the result of the fallout from Chernoby in l 
1986  as  well  as  an  important  period  of  release  from  Marcoule.  The 
second  period  from  1992  to  1995  is  characterised  by  a  very  strong 
decrease  in  137Cs  concentrations  due  to  the  STEL  facility  installed  in 
1992  that  improved  the  treatment  of  the  nuclear  wastes.  It  can  be 
assumed that the post-1995 information, which is also characterised by 
lower  concentrations  does  not  provide  more  information  on  the 
behaviour of the system, which explains the stabilisation of the error.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't explicit mention of specific criteria used to determine when training is complete. However, we can infer some possible methods based on the described process.

One method could involve using partial training and successive halving techniques. This involves initially training models with a small portion of the data and eliminating those that perform poorly. The remaining models are then trained further, and again, the worst performing ones are eliminated. This process continues iteratively until only one configuration remains, which would be considered the best configuration found.

Additionally, the context mentions the use of Mean Absolute Error (MAE) and Nash-Sutcliffe Efficiency (NSE) metrics during the training phase. These metrics could potentially serve as indicators for determining when training should stop. For instance, if the MAE or NSE scores reach a certain threshold or show no significant improvement over several iterations, it might suggest that the model has reached its maximum performance capacity, indicating that training could be stopped.

However, without explicit confirmation from the source material, these interpretations remain speculative. Therefore, I cannot definitively state what criteria are used to determine when training is complete.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.  K-nearest neighbours regression. The model is trained using the 
Scikit-learn  (Pedregosa  et  al.,  2011)  library.  We  select  ten  nearest 
neighbours and computing of Euclidean distance. 

We randomly split the data into the training (70%) and testing (30%) 
subsets. Then, we train each model and examine how incomplete ob-
servations affect the accuracy of predictions. We carry out experiments 
for each incomplete data approach. 

3.2.3. Evaluation metrics when true target labels are not known 

We compare the performance of different approaches primarily via 
the Mean Absolute Error (MAE), which we have chosen since it is easily 
interpretable and naturally scales for predictions of fraction values be-
tween 0 and 1. With this metric, we evaluate the performance only on 
complete test set observations.

4.2. Performance of predictive models 

The performance of compared predictive models is summarized in 
Tables 2â€“4. When using only complete targets for training, the MAE of 
complete  observations  of  the  test  set  is  the  lowest.  However,  experi-
mental  results  on  evaluating  the  prediction  accuracy  of  dominant 
vegetation  cover  types  show  that  using  this  approach,  the  prediction 
accuracy of incomplete observations is more than 10% less than using 
other approaches. This suggests that complete observations do not carry 
full information about the distribution of the natural vegetation cover 
worldwide.  It  confirms  existing  findings  (Nikoloski  et  al.,  2021)  that 
better performance can be achieved if incompletely labeled data is used 
in model training instead of discarding it.

For the incomplete observations, we do not know the true underlying 
distribution of the target values; we only have noisy or incomplete ob-
servations. Thus, it would be misleading to assess the prediction accu-
racy  on  those  observations.  A  further  challenge  is  that  the  complete 
observations  are  not  uniformly  distributed  worldwide.  Therefore,  to 
evaluate predictive performance on incomplete data, we assess how well 
the dominant vegetation cover type is predicted and measure the pre-
diction accuracy similarly to a classification task. More specifically, we 
check whether the largest predicted fraction of each observation equals 

Fig. 8. Number of zero and non-zero values in distribution of each vegetation type.

A related research question in ecology and biogeography is how to 
predict the potential natural vegetation (PNV). PNV is the expected state 
of mature vegetation, given a particular set of environmental constraints 
in the absence of human intervention (Chiarucci et al., 2010). At first, 
PNV models were constructed based only on expert knowledge, whereas 
nowadays, various statistical techniques and machine learning methods 
are more widely employed (Hemsing and Bryn, 2012). In Hengl et al. 
(2018), authors evaluate different machine learning methods, such as 
neural  networks,  random  forests,  gradient  boosting,  and  k-nearest 
neighbours,  for  PNV  mapping  in  a  classification  setting.  The  latter 
example  describes  global  PNV  mapping.  However,  most  PNV  studies 
focus on specific areas or regions (Raja et al., 2019; Vaca et al., 2011; 
Hemsing and Bryn, 2012). 

1.2. Why the task is difficult

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The primary evaluation metric used to compare the performance of different approaches is the Mean Absolute Error (MAE). This metric was chosen because it is easily interpretable and naturally scales for predictions of fraction values between 0 and 1. The performance of the models is evaluated only on complete test set observations. For incomplete observations, where the true underlying distribution of the target values is unknown, the prediction accuracy is measured by checking whether the largest predicted fraction of each observation matches the actual value. This is done similarly to a classification task.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 4 
Definitions of model performance metrics based on “caret” R package, based on 
true positives (TP), true negatives (TN), false positives (FP), and false negatives 
(FN).  

Metric 

Equation 

Definition 

Accuracy 

Precision 

Recall 

TP + TN
TP + FP + TN + FN 
TP
TP + FP 

TP
TP + FN 

F1 

2*precision*recall
precision + recall  

Proportion of correct predictions in the whole 
data set. 
The proportion of images that a model classified 
as a specific category C that are actually category 
C. 
The proportion of images that are actually a 
specific category C that the a model classified as 
C. 
Weighted average of precision and recall.  

Table 5 
Performance of the image quality model on the test data.  

Location 

Finnmark 

Yamal 

Id 

Bad 
Good 
Bad 
Good 

Precision 

0.920 
0.986 
0.764 
0.979 

Recall 

0.910 
0.988 
0.779 
0.977 

F1 

0.915  
0.987 
0.771  
0.978

shown to reduce the processing time of image classification load by as 
much as 5× to 13× depending on the tasks (Fennell et al., 2022; Henrich

Separate  two-class  models  were  trained  for  Finnmark  and  Yamal 
using  the  keras  package  in  R  (Allaire  and  Chollet,  2023)  with  a  Ten-
sorFlow  backend (Allaire  and Tang,  2023).  Preliminary trials  showed 
that region-specific models performed better. The ResNet-50 architec-
ture, a convolutional neural network that is 50 layers deep (He et al., 
2015), was used to train the models with 55 epochs (number of times the 
algorithm goes through the entire training data set) and a batch size of 
64  (number  of  samples  to  work  through  before  updating  model  pa-
rameters)  with  a  one-cycle  learning  rate  (hyperparameter  controlling 
model  response  to  estimated  error  each  time  the  model  weights  are 
updated) policy with a minimum of 0.000001 and a maximum of 0.001 
(Smith, 2018).

8 
7 

5 
8 
8 
10 

461,407 
367,483 

284,994 
401,039 
426,970 
535,910 

75,824  
79,807  

53,555  
58,652  
78,620 
120,840

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Three main metrics are used to evaluate the performance of the deep learning model: Accuracy, Precision, and Recall. These metrics are defined based on True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).

Accuracy measures the proportion of correct predictions in the whole dataset. It is calculated as (TP + TN)/(TP + FP + TN + FN).

Precision, also known as positive predictive value, measures the proportion of images that a model classified as a specific category C that are actually category C. It is calculated as TP/(TP + FP).

Recall, also known as sensitivity or true positive rate, measures the proportion of images that are actually a specific category C that the model classified as C. It is calculated as TP/(TP + FN).

Additionally, the F1 score is used, which is the weighted average of precision and recall. It is calculated as 2*(precision * recall)/(precision + recall).

In Table 5, these metrics are applied to evaluate the performance of the image quality model on the test data for both Finnmark and Yamal locations. For example, for Finnmark location, the precision for 'bad' images is 0.920, the recall is 0.910, and the F1 score is 0.915. Similarly, for 'good' images, the precision is 0.986, the recall is 0.988, and the F1 score is 0.987.
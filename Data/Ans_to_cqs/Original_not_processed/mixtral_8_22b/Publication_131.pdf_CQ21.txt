Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

proper databases for CNN training.

11 of 19

2.2. CNN Classiﬁcation System

In order to carry out the design of the CNN classiﬁcation system, a set of stages
were followed. These stages are represented in Figure 9 and described in the following
subsections.

Figure 9. Stages in the classiﬁcation system.

Each of the stages was programmed with Python using the Keras library [30] for CNNs.

2.2.1. Data Augmentation

In order to generate a more robust training set, an augmentation process was carried
out using the sound database. When one deals with CNN and images, data augmenta-
tion consists of extending the set of images from their transformations (rotations, size
modiﬁcations, etc.). For the purpose of the present work on sounds, the transformations
must be made on the audios, and later these transformations will be reﬂected in the
mel-spectrogram images.

In this way, for each of the 865 audio samples in the database, 10 additional audios

were generated:
•
•
•

the network is trained with the data collected from sensor nodes (audio wave). In this
stage, human supervision is needed to identify every output with its correct pattern. In the
second stage, the network is run with the trained supervised neural network.

16 of 19

Table 3. Confusion matrix for the ﬁrst CNN (5 classes).

Predicted Values

Actual values

Ep. cal. standard

Ep. cal. chorus

Ep. cal. amplexus

Al. obs. standard

Al. obs. distress

Ep. cal. standard

94.27% (560)

Ep. cal. chorus

Ep. cal. amplexus

Al. obs. standard

Al. obs. distress

4.78% (10)

9.09% (13)

1.2% (11)

2.28% (1)

1.01% (6)

95.22% (199)

0

0

0

4.71% (28)

0

90.91% (130)

0

0

0

0

0

98.8% (902)

0

0

0

0

0

97.72% (43)

Sensors 2021, 21, 3655

17 of 19

4. Discussion and Conclusions

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training, several postprocessing steps can be performed depending on the specific requirements of the project or application. Some common postprocessing steps include:

1. Model Evaluation: This involves evaluating the performance of the trained model using various metrics such as accuracy, precision, recall, F1 score, ROC curve, AUC, etc. The choice of evaluation metric depends on the problem statement and the type of classification task being performed.

2. Confusion Matrix: A confusion matrix is a table that summarizes the performance of a classification algorithm by comparing predicted labels against actual labels. It provides insights into the types of errors made by the classifier and helps in identifying areas where the model needs improvement.

3. Saliency Maps: Saliency maps are visualizations that highlight the regions of input data that contribute most significantly to the final prediction made by the model. They help in understanding how the model makes decisions and provide insights into the features that drive predictions.

4. Hyperparameter Tuning: After initial training, hyperparameters such as learning rate, batch size, number of epochs, regularization parameters, etc. can be fine-tuned to improve the performance of the model further.

5. Ensemble Methods: Multiple models can be combined using ensemble methods such as bagging, boosting, stacking, etc. to improve overall performance and reduce overfitting.

6. Deployment: Once the model has been evaluated and optimized, it can be deployed in production environments for real-world applications. This may involve integrating the model with existing systems, setting up monitoring and logging mechanisms, and ensuring scalability and reliability.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The tools used to measure the performance of the trained model are 
the precision (P), recall (R), mean average precision (mAP), and the F1- 
score metrics (Nepal and Eslamiat, 2022). The precision gives the ratio 
of the true predictions to the total number of predictions, whereas recall 
is the ratio of true predictions to the total number of objects present in 
the image. The F1 score, which is the harmonic mean of the precision 
and recall, gives the model’s test accuracy. Since mAP is also considered 
as a measure of accuracy for a machine learning algorithm, mAP is used 
for measuring the performance of the model in this work. These metrics 
are calculated using the following equations (Maxwell et al., 2021). 

Accuracy =

TP + TN
TP + TN + FP + FN

P =

TP
TP + FP

R =

TP
TP + FN

F1 Score = 2 ×

P × R
P + R

AP =

∑k=n(cid:0) 1
k=0

[R(k) (cid:0) R(k + 1) ] × P(k)

]

mAP =

∑k=N
k=1

1
N

APk

(4)  

(5)  

(7)  

(8)  

(9)  

(10)

Table 8 
Average test of results for four different datasets with four epoch values. Average 
values for preprocessing, inference and NMS values is in millisecond. The last 
row is the average accuracy for different epochs.   

Computation time (milliseconds) 

Parameters 

Epoch-100 

Epoch-150 

Epoch-200 

Epoch-250 

Preprocessing 
Inference 
NMS 
Accuracy 

0.85 
140.68 
0.73 
92.6 

0.85 
141.38 
0.8 
94.25 

0.875 
145.83 
0.75 
94 

0.8 
147.95 
0.75 
92.9

In  order  to  determine  more  accurate  training  results  and  also  to 
explore the effect of epoch upon training result, apart from 150 epochs, 
the model with attention layer is trained with epoch values 100, 200, 
and  250  under  a  uniform  training  environment  and  with  the  same 
dataset.  The  training  summary  for  each  of  these  epochs  is  shown  in 
Tables 5, 6 and 7 for epochs 100, 200, and 250, respectively. The trends 
of mAP values with increasing epochs are shown in Figs. 14 and Fig. 15. 
The size of the dataset used for custom training is sufficiently large. 
Further, the DL model used in this work is tuned with optimized hyper 
parameters  for  which  overfitting  and  under  fitting  situations  are  pre-
vented from occurring. As mentioned above, the effort to over-train the 
model has been avoided and graded MSE convergence (Figs. 7 to 11) has 
been  adopted  to  fine-tune  the  model.  Results  obtained  for  different

In recent years, it has been observed that deep learning methods can 
be trained with close to zero training error efficiently. The number of 
convolutional and dense layers directly affects the runtime of the model 
(Bienstock et al., 2023). The running time of a DL model increases in 
polynomial-terms with the increased number of associated layers. But 
with the effort of achieving zero error, there are possibilities of over-
training and biased training. To avoid these situations, a restrictive and 
gradually increasing training cycle and accuracy calculation approach 
has  been  adopted.  Based  on  mean  square  error  (MSE),  the  optimal 
training state for the model has been obtained. 

4.1. Performance metrics used to evaluate the model

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The performance of the deep learning model is evaluated using several metrics such as precision (P), recall (R), mean average precision (mAP), and the F1-score. Precision measures the ratio of true predictions to the total number of predictions made by the model. Recall calculates the proportion of true predictions out of all actual instances present in the data. The F1-score combines both precision and recall into a single metric, providing a balanced evaluation of the model's performance. Additionally, mAP is employed to assess the overall accuracy of the machine learning algorithm. These metrics are computed using specific formulas based on True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).
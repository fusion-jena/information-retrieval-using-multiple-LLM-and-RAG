Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

123

2170

Journal of the Indian Society of Remote Sensing (September 2021) 49(9):2163–2172

training, we set the learning decay rate to 0.7 so that the
learning rate gradually decays as the training progresses.
Table 1 shows the experimental results in which we ﬁxed
the voxel size and batch size and adjusted the regulariza-
tion strength, from which we can be told that result is better
while regularization strength is 0.2.

2165

Fig. 1 The pipeline of the SPG framework. a Represents the original
input data (taking a tree and a pole as examples). b Is a hand-crafted
point cloud partition, c each partition is converted into a superpoint of

the SPG, and d is the network for superpoint embedding and
segmentation based on PointNet and GRU. The black lines represent
the superedges

with K ¼ P3
adjacency:

i¼1 ki,

E ¼ (cid:3)

3
X

i¼1

ki
K

log

(cid:2) (cid:3)
ki
K

in accordance with the optimal

ð1Þ

The linearity, planarity, and scattering are derived by the

following terms:

linearity ¼

planrity ¼

scattering ¼

k1 (cid:3) k2
k1
k2 (cid:3) k3
k1

k3
k1

ð2Þ

The linearity describes how elongated the adjacency is,
the planarity assesses how plane the adjacency is, the high-
scattering values are used to represent an isotropic and
spherical adjacency. We describe these three point features
as geometric features.

2. Model training: We process data with a deep neural
network and adjust the training epoch, batch size, and
volume size to output a classiﬁer for prediction.
3. Result output: The point cloud test set is segmented,
and we can obtain the ﬁnal segmentation results. The
output ﬁles include point cloud geometric partition
graphs, SPGs, and segmentation results.

Results and Discussion

For tree species segmentation, appropriately increasing
batch size can improve efﬁciency while improving seg-
mentation accuracy. We believe that within a reasonable
range limited by GPU memory, a larger training batch size
of 8 can better preserve the original features of the point
clouds than others. In addition, the network more easily
obtains a sharp minimum rather than a ﬂat minimum when
the batch size is larger. Therefore, we can obtain better
results when the batch size is 8 in our experiments
(Table 3).

Through the experimental results, we ﬁnd that
the
adjustment of the parameters can also improve the mis-
segmentation situation. In summary, we obtain the best
result in the ﬁve tree species experiments: The mIoU is
81.49% and the OA is 92.45%, while the voxel size is 0.04,

the training batch size is 8 and the regularization strength is
0.2.

Effectiveness of the Framework for Target
Segmentation

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there is no explicit mention of strategies employed to handle randomness in the deep learning pipeline, such as managing random seed values. However, some implicit strategies can be inferred. For instance, the authors use a fixed voxel size and batch size during their experiments, which could help reduce variability caused by random initialization or sampling. Additionally, they experiment with different regularization strengths, finding that a regularization strength of 0.2 yields the best results. This suggests that tuning hyperparameters can mitigate the effects of randomness in the model. Lastly, using a larger training batch size of 8 seems to improve efficiency and segmentation accuracy, possibly due to the network obtaining a sharper minimum instead of a flatter one. Overall, while not explicitly stated, the authors seem to manage randomness through careful selection and tuning of various parameters and settings in their deep learning pipeline.
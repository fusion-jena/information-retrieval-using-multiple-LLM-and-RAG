Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In the training phase, data-augmentation technique was used to artiﬁcially increase the number of
samples [50]. Data augmentation apply speciﬁc transformations to the original samples, but preserving
their labels to obtain samples slightly diﬀerent from the original ones [50]. In particular, a 50% increase
in brightness, half-random rotation, random rotation with a factor randomly selected between 0º and
359º, random cropping, and random scale of image size were used. In addition, we used transfer
learning to reuse the knowledge acquired by the CNN model in a ﬁrst problem to apply it to another
related problem. Transfer learning is analogous to the development of knowledge in humans, which
never starts from scratch, as it is always done from previous knowledge. We used the pretrained
weights on ImageNet database to initialize the Inception v.3 model. ImageNet has 1.28 million images
for 1000 diﬀerent classes of objects [51].

Remote Sens. 2020, 12, 343

6 of 15

For this, we obtained ﬁve models by training Inception v.3 on the ﬁve created datasets. Several
studies have shown that increasing the size of the dataset using data augmentation improves the
performance of the CNN-based models [49,50]. These techniques have been proposed to reduce the
requirement of a large dataset for model training [43–45]. We conﬁgured the model parameters by
training the last two fully connected network layers in our dataset using a learning rate of 0.001 and a
decay factor of 16 every 30 epochs. As an optimization algorithm, we used RMSProp with a momentum
of 0.9 and epsilon of 0.1.

59. Zimmermann, R.S.; Siems, J.N. Faster Training of Mask R-CNN by Focusing on Instance Boundaries. arXiv

2018, arXiv:1809.07069. [CrossRef]

60. Perez, L.; Wang, J. The eﬀectiveness of data augmentation in image classiﬁcation using deep learning. arXiv

61.

2017, arXiv:1712.04621.
Scott, G.J.; England, M.R.; Starms, W.A.; Marcum, R.A.; Davis, C.H. Training deep convolutional neural
networks for land–cover classiﬁcation of high-resolution imagery. IEEE Geosci. Remote Sens. Lett. 2017, 14,
549–553. [CrossRef]

62. Nathan, R.; Horn, H.S.; Chave, J.; Levin, S.A. Mechanistic models for tree seed dispersal by wind in dense
forests and open landscapes. In Seed Dispersal and Frugivory: Ecology, Evolution and Conservation; CAB
International: Wallingford, UK, 2002; pp. 69–82.

74. LeCun, Y.; Bengio, Y.; Hinton, G. Deep learning. Nature 2015, 521, 436. [CrossRef]
75. Lynch, C. Big data: How do your data grow? Nature 2008, 455, 28. [CrossRef]
76.

Steinkraus, D.; Buck, I.; Simard, P.Y. Using GPUs for machine learning algorithms. In Proceedings of
the Eighth International Conference on Document Analysis and Recognition (ICDAR’05), Seoul, Korea,
31 August–1 September 2005; pp. 1115–1120.

77. Bremond, L.; Alexandre, A.; Hély, C.; Guiot, J. A phytolith index as a proxy of tree cover density in
tropical areas: Calibration with Leaf Area Index along a forest–savanna transect in southeastern Cameroon.
Glob. Planet. Chang. 2005, 45, 277–293. [CrossRef]

78. Achard, F.; Eva, H.D.; Mayaux, P.; Stibig, H.J.; Belward, A. Improved estimates of net carbon emissions from

land cover change in the tropics for the 1990s. Glob. Biogeochem. Cycles 2004, 18. [CrossRef]

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The study utilized several data augmentation techniques to artificially increase the sample size and improve the performance of the Convolutional Neural Network (CNN)-based models. Specifically, they employed a 50% increase in brightness, half-random rotation, random rotation with a factor randomly selected between 0º and 359º, random cropping, and random scaling of image sizes. These techniques help create new variations of existing data without altering their labels, thus enabling the model to learn more robust features and generalize better.
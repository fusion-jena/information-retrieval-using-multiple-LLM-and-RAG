Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Labeling Time 
Training Time 
Turnaround 

Time 
Accuracy 

Very Short 
1.0 h 

4 h 

89.4% 

Short 
6.9 h 

22 h 

94.0% 

Very Long 
~3.5 h 

94 h 

98%  

4. Discussions 

The  development  of  deep  learning  methodologies  continues  to 
advance at an astonishing rate and be applied to various applications 
ranging from biomedical (Azghadi et al., 2020), hydrological processes 
in river channels (Talukdar et al., 2023) and agricultural (Olsen et al., 
2019) systems, to marine (Laradji et al., 2021; Saleh et al., 2022b), and 
environmental (Jahanbakht et al., 2022a) sciences. The application of 
deep learning technologies has been also used in profiling the ecosystem 
services of estuarine habitats by community members (Yee et al., 2023). 
In this paper, we extend the application of deep learning methodologies 
to advance state-of-the-art underwater fish video processing techniques 
applied to turbid waters.

2.3. Weakly-supervised DNN ensemble 

A second approach to counteract the problem of limited labeled data 
availability is to combine weak supervision (Laradji et al., 2021) and 
transfer learning. Transfer learning is a technique that provides an op-
portunity to ensure that we make the best use of available labeled data. 
For instance, in this work, we have used the open-source fully-labeled 
ImageNet dataset to pre-train two well-known DNNs, i.e., EfficientNet 
(Tan and Le, 2019) and ViT (Dosovitskiy et al., 2020), with a great ca-
pacity  for  image  classification,  while  requiring  fewer  computational 
resources compared to other DNNs. ImageNet is a huge visual dataset 
with fourteen million images, designed for object recognition tasks (Lab, 
2023). This free dataset has been hand-annotated with bounding boxes 
to indicate what objects are present and where in the images.

We collected the FishInTurbidWater dataset and quickly and weakly 
labeled  it  to  contribute  to  the  first  weakly-supervised  fish  dataset  in 
turbid  waters.  We,  then  used  this  dataset  to  develop  two  novel  deep 
learning networks, one using semi-supervised contrastive learning for 
significantly  accelerated  model  deployment  time,  and  one  weakly- 
supervised  model  to  shorten  deployment  time,  while  providing  high 
accuracy. 

For  semi-supervised  contrastive  learning,  we  first  trained  a  self- 
supervised  contrastive  learning  model  and  then  fine-tuned  it  on  our

weakly  labeled  dataset.  This  semi-,  weakly-supervised  approach  re-
quires only a small portion of our weakly labeled dataset, which makes 
the  development  cycle  from  data  labeling  to  final  model  inferencing 
very fast (i.e., 4 h) while producing a relatively high accuracy of 89%. 
For  the  ensembling  approach,  we  first  trained  two  state-of-the-art 
DNNs on ImageNet. We then performed transfer learning of these two 
DNNs  on  our  weakly  labeled  dataset.  This  generated  two  weakly- 
supervised  DNNs,  which  we  ensembled using  the  XGBoost  technique. 
This novel ensemble technique significantly improves the overall accu-
racy compared to the two weakly-supervised individual DNNs.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, there is no explicit mention of specific regularization methods such as dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. However, the text does discuss two approaches to address the issue of limited labeled data availability, which can lead to overfitting. These approaches include combining weak supervision and transfer learning, as well as developing a semi-supervised contrastive learning model and a weakly-supervised model.

Transfer learning involves using pre-trained models on large datasets like ImageNet to improve performance on smaller, task-specific datasets. In this case, two well-known DNNs, EfficientNet and ViT, were pre-trained on ImageNet before being fine-tuned on the custom FishInTurbidWater dataset. This process helps reduce overfitting by leveraging the knowledge gained from training on a larger dataset.

Additionally, the semi-supervised contrastive learning model was developed to further mitigate the risk of overfitting. Contrastive learning is a type of self-supervised learning that encourages representations of similar inputs to be close together in the embedding space, while dissimilar inputs should be far apart. By applying this technique, the authors aimed to create a more robust and generalizable model that could perform well even when trained on a limited amount of weakly labeled data.

While not explicitly mentioned, it is possible that some form of regularization, such as dropout or weight decay (L2 regularization), may have been employed during the training process to further combat overfitting. However, without additional information, it cannot be confirmed whether these specific techniques were utilized.
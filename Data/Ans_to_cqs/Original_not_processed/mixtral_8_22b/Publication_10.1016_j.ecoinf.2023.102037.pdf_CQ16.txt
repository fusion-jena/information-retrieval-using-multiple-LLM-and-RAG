Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

• Aperture: F/10  
• ISO: 160  
• Shutter speed: 1/15 s  
• Focal Length: 55 mm  
• White balance: “Incandescent” 

2.3. Data splitting and image tiling

3.2. Model performance 

To validate our model in a practice-oriented way, we split our data 
on the sticky plate level. Therefore, individual insects that belonged to 
the  same  sticky  plate  image  were  kept  together  in  the  same  dataset 
(training, validation, or testing). This allowed us to validate our model’s 
performance by emulating the way in which the model will be used in 
practice. Namely, it predicted a class for any given object it detected on 
unseen sticky plate images – data it never saw during training.

ayanan, 2022). In our previous research (Kalfas et al., 2021; Kalfas et al., 
2022), we showed that model performance can be highly overestimated 
when weak validation procedures – like random data splitting- are fol-
lowed.  Despite  taking  the  above  limitations  into  consideration,  other 
studies  still  only  focused  on  datasets  where  insect  classes  were  quite 
broad – insect order or family level, quite dissimilar and thus, relatively 
easy to classify (Rustia et al., 2021b; Wang et al., 2020c).

From a model-centric view misclassifications are often explained by 
the model complexity, its hyperparameters and the training procedure 
efficiency.  Hence,  many  researchers  employ  a  variety  of  models  and 
training  procedure  optimizations  until  they  find  the  “best”  result. 
Moreover, as we have already shown in our previous research (Kalfas 
et al., 2021; Kalfas et al., 2022), several studies on insect recognition 
don’t  apply  any  strict  validation  procedure  to  evaluate  their  models, 
which often leads to over-optimistic or unrealistic performance. 

From a data-centric approach, wrong detections are assumed to be 
caused by either intra- or inter-class variability (Fig. 2 and Fig. 3), class 
imbalance (Fig. 4) or labelling errors. 

2.2. Imaging setup

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific criteria used to determine when training is complete. However, based on common machine learning practices, some possible criteria include reaching a certain number of epochs, achieving a desired accuracy or loss value on the validation set, or observing no further improvement in validation metrics after a certain number of iterations (validation loss plateau). The authors do mention using a validation dataset to assess model performance, suggesting that they likely use some form of validation metric to decide when training should stop. Additionally, they emphasize the importance of avoiding overly optimistic or unrealistic performance estimates, implying that they may prioritize robustness and generalization over maximizing validation scores.
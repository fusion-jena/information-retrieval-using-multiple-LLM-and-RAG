Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

AN  was  implemented  in  MATLAB  (www.mathworks.com).  For 
training,  we  used  the  stochastic  gradient  descent  with  momentum 
optimizer with default settings except for the initial learning rate (which 
we set to 0.001) and the maximum number of epochs (which we set to 15 
after  preliminary  tests).  The  ‘classical’  machine-learning  algorithms 
(AB, GB, HB, and LD) were run in Python (www.python.org) using the 
scikit-learn toolbox (Pedregosa et al., 2011). We left hyperparameters at 
scikit-learn  default  values,  except  for  the  learning  rate  of  GB  and  HB 
(which we set to 0.15) and the tolerance threshold value for LD (set to 1 
× 10

2.3. Algorithm training and testing 

We trained all algorithms on a random subset of 5256 pictures (80% 

of  the  6570-picture  dataset).  To  gauge  performance  consistency,  the 
training-testing  process  was  repeated  10  times  for  each  algorithm;  in 
each of these (pseudo-)replicate runs, all five algorithms used the same 
training and testing picture subsets.

2.2. Algorithms 

We used five machine-learning algorithms based on three different 
principles: a pre-trained convolutional neural network (AlexNet); three 
boosting-based  classifiers  (AdaBoost  Adaptive  Boosting,  Gradient 
Boosting,  and  Histogram-based  Gradient  Boosting);  and  a  linear 
discriminant  model.  Here,  we  briefly  describe  the  characteristics  of 
these algorithms and the inputs that they require; further details can be 
found in the references provided below. 

The  architecture  of  AlexNet  (‘AN’  hereafter),  which  includes  five 
convolutional  layers  and  three  fully-connected  layers,  has  60  million 

EcologicalInformatics79(2024)1024302V.L. de Miranda et al.

database into training (80%) and testing sets (20%), and then trained and tested a convolutional neural network 
(AlexNet,  AN);  three  boosting-based  classifiers  (AdaBoost,  AB;  Gradient  Boosting,  GB;  and  Histogram-based 
Gradient Boosting, HB); and a linear discriminant model (LD). We assessed identification accuracy and speci-
ficity with logit-binomial generalized linear mixed models fit in a Bayesian framework. Differences in perfor-
mance across algorithms were mainly driven by AN’s essentially perfect accuracy and specificity, irrespective of 
picture angle or bug position. HB predicted accuracies ranged from ~0.987 (Panstrongylus, dorsal-oblique) to 
>0.999  (Triatoma,  dorsal-flat).  AB  accuracy  was  poor  for  Rhodnius  (~0.224–0.282)  and  Panstrongylus 
(~0.664–0.729), but high for Triatoma (~0.988–0.991). For Panstrongylus, LD and GB had predicted accuracies

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model such as AlexNet, several preprocessing steps may be applied to the input data. These steps aim to improve the quality of the data and enhance the performance of the model. Some common preprocessing techniques include:

1. Normalization: This involves rescaling the input features so that they have a mean of zero and unit variance. In image processing tasks, pixel intensities might be normalized between 0 and 1 or -1 and 1. Normalizing the input data helps stabilize the learning process and reduce the impact of outliers.

2. Scaling: Similar to normalization, scaling adjusts the range of feature values to prevent any single feature from dominating the others due to its scale. Techniques like Min-Max scaling and Standardization (Z-score normalization) are commonly employed.

3. Cleaning: Data cleaning is essential to remove noise, missing values, and inconsistencies from the dataset. It ensures that only relevant information is fed into the model, improving overall performance.

4. Augmentation: Data augmentation generates new samples by applying transformations such as rotation, flipping, zooming, cropping, etc., to existing images. This technique increases the size of the dataset and improves the robustness of the model against various distortions.

5. Encoding: Categorical variables need to be converted into numerical form before feeding them into the model. One-hot encoding and label encoding are popular methods for this purpose.

These preprocessing steps help prepare the data for efficient training of deep learning models, ensuring better results and improved performance. However, it should be noted that not all preprocessing steps are necessary for every task, and their application depends on the nature of the problem and the available resources.
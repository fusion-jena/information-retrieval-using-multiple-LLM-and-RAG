Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Figure 3. Some examples of the data augmentation step: (a) 
original image, (b) horizontal flip, (c) image cropping, (d) 
grayscale, (e) image blur up to 10px, (f) image noise up to 10% 
of pixels 

3.  METHODOLOGY

The data augmentation technique was used in this study, which 
help  to  extend  the  amount  of  information  by  adding  adjusted 
copies of already existing data, and it acts as a  regularizer and 
makes  a  difference  to  reduce  the  overfitting.  While  training, 
before input into the model, each image was randomly sampled 
by  one  of  the  following  options;  the  entire  original  image, 
horizontal  flip,  grayscale,  blur,  noise  and  cropping.  So,  each 
image  was  horizontally  flipped,  grayscale  was  applied  to  25% 
of images, blur was up to 10px, noise was up to 10% of pixels, 
and for the cropping operation, a patch with the same size as the 
original  image  was  randomly  cropped  from  image.  Some 
examples of the data augmentation step are illustrated in Figure 
3.

In  order  to  train  the  model,  it  is important to provide not only 
the  class  of  the  object  but  also  the  bounding  box  data  of  the 
object  as  correct  answer.  In  this  context,  we  parsed  the  x,  y 
coordinates, width and height of the fish in the annotation of the 
image’s  dataset.  In  addition,  we  used  data  augmentation  to 
extend the amount of data. 

The  model  has  better  performance  as  number  of  epochs 
increases.  However,  when  number  of  epochs  exceeds  200,  the 
network  seems  to  be  overfitted  to  training  data.  As  a  result, 
Table  I  shows  the  hyper-parameters  used  to  train  and  validate 
the Yolo model. 

Number 
of 
iterations 
4000 

Number 
of 
epochs 
200 

Learning 
rate 

Batch 
size 

Subdivisions 

0.001 

16 

64 

Table 1. Hyper-parameters to train the Yolo model.

Amanullah, M. A., Habeeb, R. A. A., Nasaruddin, F. H., Gani, 
A.,  Ahmed,  E.,  Nainar,  A.  S.  M.,  Akim,  N.  M.,  &  Imran,  M. 
(2020).  Deep  learning  and  big  data  technologies  for  IoT 
security.  Computer  Communications,  151(December  2019), 
495–517. https://doi.org/10.1016/j.comcom.2020.01.016 
Ayaz,  A.,  Acarli,  D.,  Altinagac,  U.,  Ozekinci,  U.,  Kara,  A.,  & 
Ozen,  O.  (2006).  Ghost  fishing  by  monofilament  and 
multifilament gillnets in Izmir Bay, Turkey. Fisheries Research, 
79(3), 267–271. https://doi.org/10.1016/j.fishres.2006.03.029 

Beyan,  C.,  &  Fisher,  R.  B.  (2012).  A  filtering  mechanism  for 
normal fish trajectories. Proceedings - International Conference 
on Pattern Recognition, 2286–2289. 

Brodie,  G.,  Brodie,  J.,  Maata,  M.,  Peter,  M.,  Otiawa,  T.,  & 
Devlin,  M.  J.  (2020).  Seagrass  habitat  in  Tarawa  Lagoon,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: The data augmentation techniques applied in the deep learning pipeline include horizontal flip, grayscale, blur, noise, and cropping. Each image was randomly sampled by one of these options before being input into the model. Horizontal flipping was applied to all images, while grayscale was applied to 25% of images. Blurring was done up to 10px, and noise was added up to 10% of pixels. For the cropping operation, a patch with the same size as the original image was randomly cropped from the image. These techniques helped to extend the amount of information and acted as a regularizer to reduce overfitting.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

60.64
63.82
65.77
66.30

60.17
63.57
65.54
65.94

Table 3
Performances of different ResNet architectures on validation and test sets. ResNetX-Y is written so that X indicates the network's depth and Y the input size. In bold
the best value for each metric.

Network -patch Size

Batch size

Validation set

Test set

Macro-F1

Top-1 accuracy

Micro-F1

Macro-F1

Top-1 accuracy

Micro-F1

ResNet152–224
ResNet50–128
ResNet18–128
ResNet18–224
Ensemble

16
128
128
200
128

37.45
52.04
51.40
54.93
60.56

62.38
64.07
63.90
66.70
70.60

60.46
63.85
63.88
66.44
70.35

38.26
52.27
51.62
53.93
60.38

61.71
64.35
63.60
66.30
70.54

60.09
63.89
63.44
65.94
70.37

5.2.1. Classification performances

The performances of the networks were assessed with the precision,
recall and F1-score. At training, validation and testing time; precision,
recall and F1-score were calculated as follows:

With:

(1)

(2)

(3)

F1 ranges from 0 to 1, 1 being a perfect classifier.
The three metrics were computed at both micro and macro levels.
Micro-F1, precision, and recall computed the statistics across all classes,
given their distribution on the whole dataset, while macro statistics
placed equal weight on all classes, meaning that class-wise differences
in performance more greatly impacted the scores. This rendered micro-
recall equivalent to the overall top-1 accuracy of the models (these two
terms will be used interchangeably).

5.2.2. Calibration of the networks

60.09
63.89
63.44
65.94
70.37

(Table 3, ResNet50–128 and ResNet18–128) performed roughly equally
on validation and test sets, with ResNet18–128 achieving a micro-F1 of
63.44 and ResNet50 63.89 on the test set. However, an epoch for the
ResNet50 took about three times longer than ResNet18. The ResNet50
did not outperform the previously trained ResNet18 with patch size 128
and batch size 512 (see Table 2). The deeper architecture of the Re-
sNet50 made it impossible to train with a batch size of 512.

6.2.1. Simple CNNs and ensemble network

All validation performances consistently increased with the patch
size up to 224 × 224, which achieved 65.94 micro-F1 (see Table 2). As
for training curves, the gap between patch size performances tended to
decrease quickly. It is worth emphasizing that macro-F1 (with all
classes weighted equally) was consistently lower than micro-F1 (all
classes weighted according to their proportion in the dataset).

The ResNet18 and ResNet50 trained with the same hyperparameters

Table 1
Classification accuracy of hosts (n = 4) and visitors (n = 20) on the Moorea
dataset (Beijbom et al., 2012). Values are given in mean percentage ± standard
deviation.

Subject

Macro-F1

Top-1 accuracy

Micro-F1

hosts
visitors

74.86 ± 3.63
59.4 ± 4.15

79.21 ± 2.35
64.98 ± 6.31

80.32 ± 2.32
68.31 ± 4.64

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The performance of the deep learning model is evaluated using several metrics such as precision, recall, and F1-score. These metrics are calculated during training, validation, and testing phases. Precision measures the number of true positive predictions divided by the total number of positive predictions. Recall calculates the number of true positive predictions divided by the actual number of positives. The F1-score combines precision and recall into a single metric, ranging from 0 to 1, where 1 represents a perfect classifier. Additionally, these metrics can be computed at both micro and macro levels. Micro-level calculations consider all classes together, giving them weights based on their distribution in the entire dataset. On the other hand, macro-level computations assign equal importance to all classes, making class-wise performance variations significantly affect the scores. Notably, micro-recall is equivalent to the overall top-1 accuracy of the models.
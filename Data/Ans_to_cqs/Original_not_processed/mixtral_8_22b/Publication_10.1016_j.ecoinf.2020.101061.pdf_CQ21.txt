Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.3. Model evaluation

Collecting a suﬃcient number of training samples will often be a
bottleneck in developing supervised methods in airborne imagery. It is
therefore useful to test the number of local training samples needed to
achieve maximum performance. We performed a sensitivity study by
training models using diﬀerent proportions of training data. We se-
lected 5%, 25%, 50% and 75% of the total hand-annotations to com-
pare to the full dataset for the within-site results for each site. We reran
this experiment ﬁve times to account for the random subsampling of
annotations. In addition, we ran the evaluation plots for the pretraining
model only (i.e. 0% hand-annotated data) to assess whether the addi-
tion of hand-annotated data improved the within-site pretraining.

3. Results

To assess generalization among sites, we performed three types of
experiments that used diﬀerent combinations for hand-annotations and
pretraining data (Fig. 2). The ﬁrst experiment is to use pretraining and
hand-annotated data to predict the evaluation data from the same site
(‘within-site’). The next setup is to use the pretraining data and hand-
annotated from the same site to predict the evaluation data from a
diﬀerent site (‘cross-site’). For example, using each of the within-site
models, we can test the ability for a model to predict tree conditions in
each of the other geographic sites, creating a matrix of cross-site pre-
dictions. To assess generalization without local pretraining data, we
tested a model training using pretraining data from all other sites, but
hand annotations from the same site as the evaluation data (‘transfer-
learning’). For example, the transfer learning model for Oak Woodland
used the hand-annotations from Oak Woodland, but the pretraining

Fig. 6. A sample evaluation plots from the Mixed Pine site predicted by
a model built from training data from the same site, from each other
site, and a universal model. Ground truth boxes are shown in green (A).
Individual trees with a predicted probability greater than 15% are
shown in black. B) Predictions from the model trained on mixed-pine
annotations. C) Predictions from the model trained Oak Woodland
annotations. D) Predictions from the model
trained on Eastern
Deciduous annotations. E) Predictions from the model trained on
Alpine annotations. F) Predicted from the model trained on annota-
tions from all sites. The universal model (F) built from all annotations
slightly outperformed all other models, including the model trained
only from the Mixed Pine site. For example, the boulder in the bottom
right corner is incorrectly classiﬁed as a tree by the models trained
from Mixed Pine, Alpine, and Eastern Deciduous sites, but is correctly

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about specific postprocessing steps involved after the model training. However, based on common practices in machine learning, some possible postprocessing steps could include:

1. Evaluation metrics: Calculating various evaluation metrics such as accuracy, precision, recall, F1 score, ROC curves, etc., to measure the performance of the model.

2. Confusion Matrix: Creating a confusion matrix to visualize the performance of the algorithm and understand where it might be making mistakes.

3. Saliency Maps: Generating saliency maps or heatmaps to highlight important features contributing to the model's predictions. This helps in understanding how the model makes decisions and identifying potential biases.

4. Cross-validation: Performing cross-validation techniques like k-fold cross-validation to ensure the robustness and reliability of the model.

5. Hyperparameter tuning: Adjusting hyperparameters to optimize the model's performance further.

In this case, the authors have mentioned performing a sensitivity study by training models using different proportions of training data and running evaluations five times to account for random subsampling of annotations. They also mention testing a model training using pretraining data from all other sites but hand annotations from the same site as the evaluation data ('transfer-learning'). These indicate that they may have employed some form of cross-validation and possibly hyperparameter tuning during their experiments.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Collecting a suﬃcient number of training samples will often be a
bottleneck in developing supervised methods in airborne imagery. It is
therefore useful to test the number of local training samples needed to
achieve maximum performance. We performed a sensitivity study by
training models using diﬀerent proportions of training data. We se-
lected 5%, 25%, 50% and 75% of the total hand-annotations to com-
pare to the full dataset for the within-site results for each site. We reran
this experiment ﬁve times to account for the random subsampling of
annotations. In addition, we ran the evaluation plots for the pretraining
model only (i.e. 0% hand-annotated data) to assess whether the addi-
tion of hand-annotated data improved the within-site pretraining.

3. Results

To assess generalization among sites, we performed three types of
experiments that used diﬀerent combinations for hand-annotations and
pretraining data (Fig. 2). The ﬁrst experiment is to use pretraining and
hand-annotated data to predict the evaluation data from the same site
(‘within-site’). The next setup is to use the pretraining data and hand-
annotated from the same site to predict the evaluation data from a
diﬀerent site (‘cross-site’). For example, using each of the within-site
models, we can test the ability for a model to predict tree conditions in
each of the other geographic sites, creating a matrix of cross-site pre-
dictions. To assess generalization without local pretraining data, we
tested a model training using pretraining data from all other sites, but
hand annotations from the same site as the evaluation data (‘transfer-
learning’). For example, the transfer learning model for Oak Woodland
used the hand-annotations from Oak Woodland, but the pretraining

Tesla K80 GPU for 5 epochs. To align these unsupervised classiﬁcations
with the ImageNet pretraining weights, we normalized the RGB chan-
nels by subtracting the ImageNet mean from each channel. We then
retrained the network using the hand-annotated data for 40 epochs. For
more details of this approach see Weinstein et al. (2019). Data aug-
mentation of random ﬂips and translations was tested and found to
have little eﬀect on the ﬁnal score.

Fig. 2. Approaches to geographic generalization in model training: 1) ‘Within-
site’ training in which training data from site 1 is used to predict site 1; 2)
‘Cross-site’ training in which training data from site 1 is used to predict site 2; 2)
‘Transfer learning’ in which a model is ﬁrst trained on site 1 data, followed by
ﬁnetuning on site 2 training data, and 3) ‘Universal’ model in which training
data from both site 1 and site 2 are used to predict evaluation data from site 2.

compared this model with each of the within-site model to test whether
the addition of data from other sites improved predictions of trees from
the same site.

2.5. Sensitivity to the number of hand-annotations

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific criteria used to split the data for deep learning model training into train, test, and validation sets. However, it mentions several experiments conducted to evaluate the performance of the models under different scenarios. These include:

1. Within-site: Using pretraining and hand-annotated data to predict evaluation data from the same site. This suggests that the data might have been split based on geographical locations or sites.

2. Cross-site: Using pretraining data and hand-annotated data from one site to predict evaluation data from another site. This also indicates that the data could have been divided according to their respective sites.

3. Transfer learning: Training a model using pretraining data from all other sites except the target site, along with hand-annotations from the target site. This implies that the data might have been partitioned based on the source and target sites.

Additionally, the text discusses a sensitivity analysis where models were trained using varying proportions of training data (5%, 25%, 50%, and 75%) compared to the full dataset. This further supports the idea that the data might have been segmented based on the amount of available training data.

In conclusion, while the exact criteria used to split the data for deep learning model training remain unknown, the experiments mentioned suggest that the data may have been divided based on geographical location, source/target sites, and the proportion of available training data.
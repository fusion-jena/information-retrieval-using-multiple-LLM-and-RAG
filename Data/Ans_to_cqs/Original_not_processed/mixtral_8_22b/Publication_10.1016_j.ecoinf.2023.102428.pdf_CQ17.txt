Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 7 
MLP parameters and performance.  

S/N 

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14 

Variables 

Input layer neurons 
Hidden layer nodes 
Output layer neurons 
Requested samples per class 
Start learning rate 
End learning rate 
Momentum factor 
Sigmoid constant 
Iterations 
Acceptable RMS 
Training RMS 
Testing RMS 
Accuracy Rate 
Skill measure (%) 

Results 

8 
10 
8 
1688 
0.0008 
0.0001 
0.5 
1.0 
10,000 
0.01 
0.1430 
0.1446 
88.79% 
0.8719

2.5. Model description 

2.5.1. MLPnn-MC modelling 

MLPnn-MC  modelling  integrates  the  multi-layer  perceptron  (MLP) 
algorithm  and  Markov  chain,  which  unravels  robust  future  LULC 
simulation (Gebresellase et al., 2023; Nasiri et al., 2019). The novelty of 
this model lies in the fact that it utilises deep machine learning to learn 
the model in automated mode in less calibration time through complex, 
multivariate, and non-linear relationships to create several transitions 
and predict unseen data within the model at a single time by compre-
hending the hidden relationships (Kumar and Agrawal, 2023). MLP is 
the  most  prevalent  machine  learning  algorithm  in  ANN  (Artificial 
Neural Network) modelling (Alla et al., 2021; Band et al., 2020), which 
uses  back  propagation  (BP)  or  the  reverse  method  of  automatic

2.4. LULC classification and accuracy assessment

EcologicalInformatics79(2024)1024283P. Verma et al.                                                                                                                                                                                                                                  

Fig. 1. Location map.  

differentiation as a feedforward propagation (FP) algorithm to train the 
neural networks by employing supervised learning based on stochastic 
gradient descent (SGD). Backward pass always improves the efficiency 
of MLP neural networks. Generally, MLP consists of three main layers of 
nodes: an input layer, an output layer, and one or more hidden layers in- 
between them (Kumar and Agrawal, 2023). Apart from the input layer,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information about the specific regularization methods used to prevent overfitting in the deep learning pipeline for this particular study. However, some common regularization techniques used in deep learning include dropout, L1 regularization, and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the network more robust. L1 regularization adds a penalty term to the loss function equal to the absolute value of the weights, while L2 regularization adds a penalty term equal to the square of the weights. Both L1 and L2 regularization help prevent overfitting by discouraging large weight values. Other regularization techniques include early stopping, data augmentation, and batch normalization. It is important to note that the choice of regularization technique depends on the specific problem and dataset being studied.

Reference(s):
None required for helpful answer.
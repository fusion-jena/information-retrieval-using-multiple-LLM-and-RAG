Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

models, pre-trained on 1.4M images from 1000 classes. First,
the intermediate layer of all the models was selected to use
for feature extraction. For this purpose, the output of the last
layer (bottleneck layer) is commonly used prior to the ﬂatten
operation. The reason is that all the fully connected layers
are specialized to the task network was trained on; thus, the
features learned by these layers will not be useful for the new
task. The bottleneck features are more generic. Therefore, the
base models were frozen, loaded with weights trained on the
ImageNet dataset and used as a feature extractor. We added
a dropout layer with a dropout rate of 0.2 to reduce the
overﬁtting of the model. Finally, the top classiﬁer softmax
layer was added for predicting the class for an input image.
The model with the base model of MobileNetV2 was compiled
and optimized with Adam optimizer. The other with the base
model of MobileNetV3 were optimized with Gradient descent

TABLE I
COMPARISON OF TRAINED MNV3/MNV2 MODELS WITH OTHER CNN-BASED APPROACHES

Model

Dataset

Overall Iterations

Accuracy % Loss %

[22] CNN
[23] CNN
[25] CNN
[25] CNN
Our work
MobileNetV2
Our work
MobileNetV3-Large
Our work
MobileNetV3-Small

Flavia
Flavia
LeafSnap
Flavia

LeafSnap

LeafSnap

LeafSnap

12,000
50,000
200,000
30,000

20

20

20

94.69
98.69
79.96
98.69

86

89

86

0.7-2.5
-
-
-

0.4

0.3-0.4

0.5-0.6

Training Time
(minutes)
-
252
1920
-

40

40

16

Size

-
-
-
-

9.86 MB

4.88 MB

1.99 MB

much reduced, which makes them ideal to be inserted in
handheld devices for ofﬂine plant and tree species recognition.

V. MOBILE APP IMPLEMENTATION

D. Model Training

Feature Extraction: The hardware to train the model included
a Lenovo laptop equipped with an 8265U CPU at 1.80 GHz
of Intel Core i5, 8 GB of RAM running on a Windows 10 64-
bit system. The software tools included Annaconda3 Jupyter
Notebook with Python 3.8, where the Tensorﬂow, OpenCV-
python3 [37] and Keras [38] libraries were used. The dataset
was divided in a ratio of 75:25 into a training set of 5789
images and a test set of 1930 images. To obtain the pre-
trained weight parameters, we built the base models from the
MobileNetV2, MobileNetV3-Large and MobileNetV3-Small

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on August 29,2024 at 05:51:17 UTC from IEEE Xplore.  Restrictions apply.

CNN comes with the disadvantages of a huge number
of labelled training samples requirement for weighting pa-
rameters and powerful GPUs for a faster learning process.
With limited training samples, learning too many parameters
directly will result in overﬁtting even after using over-ﬁtting
prevention techniques such as Dropout [7]. Overﬁtting of a
machine learning model includes more terms and uses more
complicated approaches than necessary [8]. To make deep
CNN ﬁt for small datasets while keeping high accuracy is
a challenge. To overcome this challenge, transfer learning [9]
offers the solution, which implies reusing previously attained
knowledge in similar tasks. Some studies analyzed the use
of transfer learning for many deep learning algorithms. These
evaluations [10] concluded that transfer learning improves the
accuracy of deep learning models, which apply deep features
and ﬁne-tuning.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning pipelines mentioned in the given context utilize two main datasets - Flavia and LeafSnap. The Flavia dataset is used in conjunction with both MobileNetV2 and MobileNetV3-Large models, whereas the LeafSnap dataset is employed alongside MobileNetV2, MobileNetV3-Large, and MobileNetV3-Small models. It should be noted that the exact number of iterations varies between different models and datasets, ranging from 12,000 to 200,000 overall iterations. However, no specific information regarding the usage of popular benchmark datasets like MNIST, CIFAR, or ImageNet has been provided within the given context.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The dataset was divided into training (60%), validation (20%), and 
testing (20%) subsets for model evaluation and hyperparameter search 
(Fig. 3 (b)). This approach was chosen to obtain an unbiased estimation 
of  error.  Data  augmentation  was  only  applied  to  the  training  data  to 
avoid overlapping the same data in different partitions. To evaluate the 
models we used the coefficients of determination (R2  - calculated as the 
squared  Pearson's  correlation  coefficient),  which  measures  the  per-
centage of variation in the response variable explained by the model. 
Additionally,  we  used  the  root  mean  squared  error  (RMSE)  metric, 
where  RMSE =
,  with  Yi = the  actual  values  pine 
Ì‚
Y i = the predicted pine cove values. In our work, RMSE 
cover values and 
values ranged from 0.0 to 1.0, with values closer to 0 indicating better 
model  predictions.  The  overfitting  was  assessed  by  comparing  model

performance on the training, validation, and test datasets. All models 
were implemented using the Keras library in Python 3.8.3 (Chollet et al.,

training domain (i.e. test dataset) and from new domains (i.e. transfer 
learning dataset). This setup allows us to complement model evaluations 
by quantifying the performance difference between the two contexts.

testing  data,  is  essential.  Typically,  models  are  evaluated  within  the 
domain  of  the  training  data,  using  test  data  from  the  same  domain. 
Therefore,  evaluations  within  and  outside  the  domain  of  the  training 
data can provide a deeper insight into the generalization capabilities of a 
model.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To monitor the model performance during training, the authors employed a strategy where they divided their dataset into three distinct subsets: training (60%), validation (20%), and testing (20%). They chose this methodology to achieve an impartial assessment of the model's error. By applying data augmentation solely to the training set, they ensured no overlap of identical data across various partitions.

Two primary metrics were utilized to gauge the effectiveness of the models: the coefficients of determination (R^2) and the root mean squared error (RMSE). The former, computed as the square of Pearson's correlation coefficient, gauges the proportion of variance in the dependent variable elucidated by the model. Meanwhile, the latter calculates the discrepancy between the anticipated and true values, with lower scores signaling superior model accuracy. Both RMSE and R^2 values spanned from 0.0 to 1.0, with values nearer to zero denoting more precise model forecasts.

Overfitting was appraised by juxtaposing the model's efficacy on the training, validation, and test datasets. Furthermore, all models were executed utilizing the Keras library in Python 3.8.3. A unique aspect of this study involved evaluating the models not merely within the confines of the training domain but also beyond it, employing a transfer learning dataset. This dual-context analysis facilitated a comprehensive understanding of the model's capacity to generalize its learnings.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

H. ovalis) with a max stride of 32. It is worth noting that data augmen-
tation is a well-proven and common technique to avoid overfitting and 
to render a model generalised. We applied random cropping, horizontal/ 
vertical flipping, image rotation, image translation, zooming, brightness 
adjustment,  contrast  adjustment,  hue  and  saturation  adjustment, 
gaussian  noise,  random  erasing  both  of  our  models  training,  which 
helped increase the size and variability of our dataset.

Recently,  attention  has  focused  on  machine  learning  and  deep- 
learning algorithms to achieve these goals. Deep learning is a branch 
of machine learning and uses algorithms inspired by the function and 
structure of neural networks in the human brain (LeCun et al., 2015). An 
important  part  of  machine  learning  is  feature  engineering,  which  in-
creases  accuracy  and  requires  expertise  in  the  specific  domain  of  the 
problem. The deep learning approaches can perform feature engineering 
independently  by  itself  (Goodfellow  et  al.,  2016;  Jalali  et  al.,  2022). 
Deep-learning are providing state-of-the-art performances of computer 
vision tasks such as classification, detection or segmentation (Kamilaris 
and Prenafeta-Boldu, 2018; Pouyanfar et al., 2018).

1.  Unbounded fusion 

∑

O =

wi.Ii

(1) 

i

where wi  is a learnable weight that can be a scalar (per feature), a 
vector (per channel) or a multi-dimensional tensor (per pixel) and Ii  is 
the  number  of  layers.  However,  the  unbounded  scalar  weight  could 
cause  training  instability,  Therefore,  weight  normalisation  is  used  to 
bound the value range of each weight.  

2.  Softmax-based fusion 

∑

O =

ewi

∑

ewj

i

j

.Ii

(2) 

The  softmax  activation  function  is  applied  to  each  weight  to 
normalise  all  weights  to  be  a  probability  within  a  range  of  0  to  1. 
However,  the  extra  softmax  function  leads  to  a  slowdown  on  GPU 
hardware resources.  

3.  Fast normalized fusion 

where  P7

td  presents  the  intermediate  feature  at  level  6  on  the  top- 
out  presents the feature at level 6 on the bottom- 

down pathway and P7
up pathway.

Neuderth, K., 2018. Looking for seagrass: Deep learning for visual coverage 
estimation. In: 2018 OCEANS - MTS/IEEE Kobe Techno-Oceans (OTO), pp. 1–6. 
Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y., 2013. Overfeat: 
Integrated recognition, localization and detection using convolutional networks. In: 
arXiv preprint arXiv:1312.6229. 

McKenzie, L., 2003. Guidelines for the rapid assessment of seagrass habitats in the 

Tan, M., Pang, R., Le, Q.V., 2020. Efficientdet: Scalable and efficient object detection. In: 

western pacific. In: The State of Queensland, Department of Primary Industries, 
43pp.  

Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 
Recognition, pp. 10781–10790. 

Moniruzzaman, M., Islam, S.M.S., Lavery, P., Bennamoun, M., 2019. Faster r-cnn based 
deep learning for seagrass detection from underwater digital images. In: 2019 Digital 
Image Computing: Techniques and Applications (DICTA), pp. 1–7.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, there is no explicit mention of regularization methods like dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. However, it does discuss another commonly used method to avoid overfitting and improve the model's generalization – data augmentation. Data augmentation techniques such as random cropping, horizontal/vertical flipping, image rotation, image translation, zooming, brightness adjustment, contrast adjustment, hue and saturation adjustment, gaussian noise, and random erasing were applied during the training process. These techniques help increase the size and variability of the dataset, making the model more robust and less prone to overfitting. While not explicitly mentioned, other popular regularization methods like dropout and L2 regularization may also have been employed but are not discussed in this particular context.
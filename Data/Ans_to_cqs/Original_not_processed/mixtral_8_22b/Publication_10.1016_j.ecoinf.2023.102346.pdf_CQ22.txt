Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The prediction power with machine learning algorithms was tested 
with the package “sdm”  (Naimi and Araújo, 2016), “dismo”  (Hijmans 
et al., 2015), “bioclim” (Booth et al., 2014), and the ensemble modeling 
approach  implemented  in  “biomod2”  (Thuiller  et  al.,  2009;  2014). 
Model mean performances for AUC, ROC, TSS, KAPPA, and COR values 
were extracted using “shiny” packages (Chang et al., 2023) in RStudio 
(RStudio Team, 2020). 

2.10. Statistical analysis

(Phillips et al., 2006). To ensure the processing of both continuous and 
categorical  discrete  values  in  the  metadata,  selected  algorithms  with 
regression and classification prediction tasks were used. Mapping fore-
casts of different algorithms in the ensemble approach were evaluated 
by assessing the accuracy and performance of the final models.

To verify the model’s validity, 30% of each taxon’s data records were 
utilized as testing data, while the remaining 70% were used as training 
data.  After  10  K-fold  cross-validation  and  10-fold  bootstrapping,  all 
models  were  assessed  based  on  the  mean  values  for  Area  Under  the 
Curve AUC (Hanley and McNeil, 1982), True Skill Statistics TSS (Mon-
serud  and  Leemans,  1992),  Cohen’s  KAPPA  (Allouche  et  al.,  2006), 
point-biserial correlation COR (Elith et al., 2010), and Receiver Oper-
ator Characteristics ROC (Hanley and McNeil, 1982). Bootstrapping was 
used  to  provide  unbiased  predictive  accuracy  with  a  sufficiently  low 
variance  (Harrell  et  al.,  2005;  Lima  et  al.,  2019).  For  the  model 
threshold,  the  sensitivity-specificity  sum  maximization  technique  was 
employed  (Liu  et  al.,  2005).  According  to  this  evaluation  criterion, 
species distribution can be accurately predicted (Liu et al., 2005). For

2.6. Model selection 

Since  performance,  accuracy,  key  parameters,  and  sensitivity  for 
prediction vary depending on which method that is used (Diniz-Filho 
et al., 2009; Elith et al., 2006; Qiao et al., 2015), we tested eight different 
machine  learning  algorithms  to  generate  our  predictive  models.  The 
included  algorithms  were:  Generalized  Linear  Model  (GLM),  Support 
Vector  Machine  (SVM),  Multivariate  Adaptive  Regression  Spline 
(MARS),  Random  Forest  (RF),  Flexible  Discriminant  Analysis  (FDA), 
Classification  and  Regression  Trees  (CART),  Generalized  Boosting 
regression Model (GBM) and Maximum-Entropy learning (MAXENT) for 
habitat suitability prediction (Table S5). 

2.7. Ensemble modeling approach

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The study uses several metrics to evaluate the performance of its machine learning models. These include Area Under the Curve (AUC), True Skill Statistics (TSS), Cohen's Kappa (KAPPA), point-biserial correlation (COR), and Receiver Operator Characteristics (ROC).

AUC is a measure of the ability of a classifier to distinguish between classes and is used to compare the performance of different models. It represents the degree or measure of separability, indicating how much a model is capable of distinguishing between classes.

True Skill Statistics (TSS) is another metric used to evaluate binary classification models. It takes into account both false positives and false negatives, providing a more balanced assessment of a model's performance.

Cohen's Kappa (KAPPA) is a statistic that measures inter-rater agreement for qualitative (categorical) items. In the context of machine learning, it is often used to evaluate the performance of classification models.

Point-biserial correlation (COR) is a correlation used when one variable is dichotomous (binary); Y can either be '0' or '1', and the other variable X is continuous. This correlation coefficient is a measure of association that describes the strength and direction of the relationship between the two variables.

Receiver Operating Characteristic (ROC) curve is a plot of the true positive rate against the false positive rate for different decision thresholds of a classifier. It shows the tradeoff between sensitivity (true positive rate) and specificity (false positive rate) for every possible cut-off.

These metrics are used after 10 K-fold cross-validation and 10-fold bootstrapping to assess the models. Bootstrapping is used to provide unbiased predictive accuracy with a sufficiently low variance. For the model threshold, the sensitivity-specificity sum maximization technique is employed.
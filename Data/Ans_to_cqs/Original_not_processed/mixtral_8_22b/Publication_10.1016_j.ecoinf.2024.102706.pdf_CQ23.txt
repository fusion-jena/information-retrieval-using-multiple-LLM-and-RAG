Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

tible to overfitting; therefore, it is crucial to address this issue by tuning
the hyperparameters.

100
5
0.05
20
0.9
0.8
5

linear as a simple statistical model and the rest of the advanced machine
learning models as machine learning. The regression analyses were
conducted using Python 3.12.2, distributed by Anaconda Inc. The Scikit-
learn library (Pedregosa et al., 2011) was employed for all models except
those utilising XGboost and LightGBM. For XGboost and LightGBM
models, the respective Python libraries “xgboost” (Chen and Guestrin,
2016) and “lightgbm” (Ke et al., 2017) were utilised. Additionally, a 5-
fold cross-validation grid search strategy was implemented to tune
hyperparameters within the training dataset for all machine learning
models, leveraging functionalities inherent in their respective Python
packages. The information on hyperparameters and their optimised
value for all machine learning models are presented in Table 4. More-
over, we converted the canopy cover output percentage to a fraction
(0–1) for convenience and smoothness in the further calculation.

linear to machine learning methods, were fitted in training data to
construct regression equations. We used multiple linear regression, Enet,
SVM, RF, XGboost and LightGBM to predict canopy cover from planet
data. In general, all the mentioned models are used in machine learning
as they use algorithms to automatically learn patterns and relationships
from data to make predictions or decisions. However, we categorised

EcologicalInformatics82(2024)1027065A. Gyawali et al.

Table 4
The optimised hyperparameter values with grid search range in machine
learning models.

Model

Hyperparameters

Grid search

Optimised value

Enet

SVM

RF

Xgboost

LightGBM

α
l1_ratio

Cost (C)
gamma
Kernal
epsilon

n_estimators
max_features
max_depth
min_samples_split
min_samples_leaf

n_estimators
max_depth
colsample_bytree
min_child_weight
subsample
learning_rate

n_estimators
max_depth
learning_rate
num_leaves
feature_fraction
bagging_fraction
bagging_freq

Model

Mean

Std Dev

S.E. mean

Paired t-test (95% confidence interval)

Observed
Linear Regression
Elastic Net
Support Vector Machine
Random Forest
Extreme Gradient Boosting
Light Gradient Boosting

0.7580
0.7583
0.7588
0.7585
0.7584
0.7585
0.7587

0.2717
0.2058
0.1917
0.2230
0.2201
0.2163
0.2231

0.0024
0.0018
0.0017
0.0020
0.0020
0.0019
0.0020

t value

–
(cid:0) 0.2266
(cid:0) 0.5205
(cid:0) 0.3898
(cid:0) 0.3489
(cid:0) 0.3844
(cid:0) 0.6161

df

–

12,606
12,606
12,606
12,606
12,606
12,606

Sig (two-tailed, p-value)

–

0.8207
0.6027
0.6967
0.7272
0.7007
0.5378

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To ensure the generalizability of the machine learning models, several measures were taken. Firstly, a diverse dataset was utilized, which included Planet satellite imagery data. This diversity in the dataset helped prevent overfitting and improved the model's ability to perform well on unseen data. Secondly, a 5-fold cross-validation grid search strategy was implemented to tune hyperparameters within the training dataset for all machine learning models. Cross-validation helps reduce bias and variance, making the model more robust and reliable. Lastly, the study mentions converting the canopy cover output percentage to a fraction (0–1), which could be considered a form of normalization or scaling. Normalizing input features can help improve the performance of some machine learning algorithms and contribute to better generalization.
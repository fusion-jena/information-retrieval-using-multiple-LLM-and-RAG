Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

by a running average of its recent magnitude to speed
up training (Tieleman and Hinton 2012). The batch size
is ﬁxed to 32. We check the training loss every 50 epochs
and stop the training when the value of the loss function
fails to decrease in two consecutive checks. The model
with the optimal objective function value is chosen.

{

=

=

n
i

n
i

Di}
{

DR
i }

1 and

Evaluation on Simulated Data Sets
DEPP training and parameter sensitivity.—We start by
evaluating DEPP on simulated data sets, testing the
ability to train the CNN model in reasonable times. As
the training epochs advance, the loss function (1) drops
rapidly and stabilizes after around 500 epochs in a typical
case (Fig. S1 of the Supplementary material available on
Dryad). Here, training, which is a one-time process for
each reference tree, ﬁnished in around 20 min for the 200-
taxon data set and 260 minutes for 10,000-taxon data set,
on a machine with one 2080Ti NVIDIA GPU and 8 CPU
cores. Placement of 1000 queries took 4 seconds for the
200-taxon and 30 s for the 10,000-taxon data sets using a
single CPU core. On the small 200-taxon data set, EPA-ng
has an advantage in terms of running time. However, in
the larger HGT data set (10,000-taxon), DEPP placements
are faster than the alternatives with half the running time
of EPA-ng. In terms of the memory usage, APPLES+JC

−

Our choices of hyperparameters such as k and the
training parameters such as the stopping criterion were
based on preselected values that were not ﬁne-tuned
on any data set. In extensive simulations, this simple
procedure was necessary due to computational reasons.
When applying in practice on real data, it is possible to
ﬁne-tune all the hyperparameters using a validation set.
We can ﬁrst randomly select a subset of the reference
species as the validation set, use these as testing data
to tune the parameters for the given data set, and then
train one last time with all the data with the ﬁne-tuned
parameters. Note that such a procedure will require
repeated training and can be slow, and our preliminary
results (Table S2 of the Supplementary material available
on Dryad) show that the gain in accuracy obtained can
be small.

Embedding size. The Layer and Rhodes (2017) (LR)
formulation requires n
1 dimensions, which introduces
some challenges. According to the theory, the number of
dimensions needs to increase by one after inserting the
query. Our supervised learning formulation does not
allow that (the embedding size is ﬁxed after training).
Thus, there is no guarantee that the embeddings remain
correct after addition, even if they are before addition.
However, we note that, in LR embeddings, adding a
leaf would require simply dividing one of the n
1
dimensions into two dimensions, leaving the rest of the
embeddings intact. Thus, one can hope that having one
less dimension has a minimal practical impact. More
broadly, for large n, training models with n-dimensional
embedding is impractical. Thus, we often set k < n
1,
and the gap can be more than an order of magnitude for
some of our tests described below. In practice, we use a
rule of thumb to select the default k (which the user can

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about how the data is divided into training, testing, and validation sets. However, it mentions that the authors did not fine-tune their hyperparameters or adjust their stopping criterion based on any specific dataset. Instead, they chose preselected values for these parameters. This suggests that they may have used a single dataset for both training and validation purposes without explicitly separating them.

In general, when developing deep learning models, it is common practice to divide the available data into three distinct subsets: training, validation, and testing. The training set is used to fit the model, while the validation set is employed to optimize the model's hyperparameters and prevent overfitting. Finally, the testing set serves as an independent evaluation of the final model's performance.

Although the text does not specify the exact methodology used for splitting the data, it does mention that the authors considered fine-tuning their hyperparameters using a validation set when working with real datasets. This implies that they might follow standard practices for data partitioning when dealing with actual data.
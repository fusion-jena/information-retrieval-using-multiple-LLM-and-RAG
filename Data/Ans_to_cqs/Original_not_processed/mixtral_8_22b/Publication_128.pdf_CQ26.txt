Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

convolutional neural network. Symmetry, 2: 256. 

 [6].  Ayinde, B.O.B.A., Inanc, T.T.I.L., Zurada, J.M.J.Z. (2019) Regularizing deep neural networks by 
enhancing diversity in feature extraction. IEEE Transactions on Neural Networks & Learning 
Systems, 9: 2650-2661. 

 [7].  Goodfellow, L., Yoshua B., Aaron, C. (2016) Deep learning. MIT Press. 
 [8].  Jassmann,  T.J.,  Tashakkori,  R.,  Parry, R.M.  (2015) Leaf  classification  utilizing  a  convolutional 
neural  network.  Institute  of  Electrical  and  Electronics  Engineers  Inc.  Fort  Lauderdale,  FL, 
United states. 1-3. 

 [9].  Laga, H., Kurtek, S., Srivastava, A., Golzarian, M., Miklavcic, S.J., 2007. A Riemannian elastic 
metric for shape-based plant leaf classification. In: 2012 International Conference on Digital 
Image Computing Techniques and Applications (DICTA). Fremantle, WA. 1-7. 

[10]. Ling, H., Jacobs, D.W. (2007) Shape classification using the inner-distance. IEEE Transactions on

DenseNet: In 2017 Huang et al.[31] proposed a scheme to reduce hyper-parameters and guarantee 
gradient  propagate,  which  aims  to  release  gradient  vanishing  through  feature  reuse  and  bypass. 
DenseNet has a nice mathematical structure and high classification performance, but it needs a long 
computational time.

2.5. Model training 

ÔÅ¨  To ensure the accuracy of the experimental results and obtain the best classification, the exact 
parameters  and  functions  as  well  as  classifier  built-in  the  six  neural  network  models  are 
designed as follows: 

ÔÅ¨  The resolution of input data is 224√ó224 with Red Green Blue (RGB) format, and the inputs 

are batch normalized before training; 

ÔÅ¨  Batch size is applied which is set to 30 when training, but images are tested one by one; 
ÔÅ¨  An  optimized  rectified  linear  activation,  Leaky  ReLU,  was  introduced  into  models  after 

convolution or concatenate layer; 

In MobileNet, width multiplier is 1.2, resolution multiplier is 1; 

ÔÅ¨  Learning rate and dropout rate adopt same value 0.001, bias value is 0;  
ÔÅ¨ 
ÔÅ¨  For the neural networks with lower depth, the higher training epochs are, the higher probability 
of over fitting is. Therefor models in which less than 100 layers adhibit raw data 60 epochs, 
otherwise executed 80 epochs;

AlexNet: In 2012, Krizhevsky et al.[28] constructed a model with more convolutional layers than 
LeNet for better high-dimensional feature extraction, and then three fully connected layers are used after 
flatten layer to pool channel information. This model employed rectified linear activation (ReLU) as 
activation  function  to  speed  up  training  process,  which  partially  solved  the  vanishing  gradient  and 
exploding problem. Features of each layer in AlexNet were depicted in Figure 2. Feature visualization 
of  AlexNet  layer  by  layer.  The  annotations  above  are  defined  as  Nameùëë@k√ók,  and  the  annotations 
below  are  denominated  in  form  of  Operationùëë@r√ór.  Where d , k and r is  feature  depth,  convolution 
kernel and image resolution, respectively..

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide specific details about addressing data bias during preprocessing of the deep learning pipeline. However, some commonly used techniques to handle data bias include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into subsets while maintaining the original distribution of classes within each subset. This ensures that the training and testing datasets have similar distributions, reducing potential biases caused by imbalanced classes.

Oversampling and undersampling are methods used to balance the number of samples between different classes. Oversampling increases the number of minority class instances, while undersampling reduces the majority class instances. These techniques can help mitigate the impact of class imbalance on model performance.

Diverse data collection focuses on gathering a wide range of examples from various sources to create a comprehensive dataset. By including diverse data, the model can learn patterns from multiple perspectives, improving its ability to generalize and reducing potential biases due to limited representation.

While these techniques are not explicitly mentioned in the provided context, they are widely recognized approaches to handling data bias during preprocessing in deep learning pipelines.
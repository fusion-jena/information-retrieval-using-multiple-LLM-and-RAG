Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

propagating errors. Cognitive modeling, 5(3).

19. Nair, V. & Hinton, G. (2010). Rectiﬁed linear units improve restricted boltzmann machines.
Proceedings of the 27th international conference on machine learning (ICML-10), 807–814.
20. Ioffe, S. & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by
reducing internal covariate shift. International Conference on Machine Learning. 448–456.
21. Dutrève, B. & Robert, S. (2016). INPN - Données ﬂore des CBN agrégées par la FCBN.
Version 1.1. SPN - Service du Patrimoine naturel, Muséum national d’Histoire naturelle, Paris.
Occurrence Dataset https://doi.org/10.15468/omae84 accessed via GBIF.org on 2017-08-30.
22. Karger, D. N., Conrad, O., Bohner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W. & Kessler,
M. (2016). Climatologies at high resolution for the earth’s land surface areas. arXiv preprint
arXiv:1607.00217.

average pooling which seems intuitively more relevant to evaluate an abundance
(=concentration). The different kinds of operations and their succession in our CNN
model are illustrated in Fig. 10.2c.

shift and help to regularize our model as well. It consists of a parameterized
centering and reduction of pre-activations. This facilitates optimization and enables
to raise the learning rate leading to a quicker convergence. At the same time, it has
a regularization effect because the centering and reduction of a neuron activation
is linked to the mini-batch statistics. The mini-batch selection being stochastic at
every iteration, a neuron activation is stochastic itself, and the model will not rely
on it when it has no good effect on prediction.

• CNN is composed of two hidden convolutional layers and one last layer fully
connected with 200 neurons, exactly similar to previous ones. The ﬁrst layer is
composed of 64 convolution ﬁlters of kernel size (3, 3) and 1 line of 0 padding.
The resulting feature maps are batch-normalized (same normalization for every
pixels of a feature map) and transformed with a Relu. Then, an average pooling
with a (8, 8) kernel and (8, 8) stride is applied. The second layer is composed of
128 convolution ﬁlters of kernel size (5, 5) and 2 lines of padding, plus Batch-
Normalization and ReLU. After, that a second average pooling with a (8, 8)
kernel and (8, 8) kernel and (8, 8) stride reduces size of the 128 feature maps
to one pixel. Those are collected in a vector by a ﬂattening operation preceding
the fully connected layer. This architecture is not very deep. However, considered
the restricted number of samples, a deep CNN would be very prone to over ﬁtting.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't any explicit mention or discussion about addressing data bias during preprocessing of the deep learning pipeline using techniques such as stratified splitting, oversampling, undersampling, or diverse data collection. Therefore, I cannot provide a direct answer based solely on the information available. However, these techniques are commonly employed to handle class imbalance and ensure fair representation of all classes in the dataset.

Stratified splitting involves dividing the dataset into training and testing sets while maintaining the original class distribution. Oversampling increases the frequency of minority class examples, whereas undersampling decreases the frequency of majority class examples. These methods can help mitigate the impact of class imbalance on model performance. Additionally, collecting diverse data from various sources can also contribute to reducing data bias and improving overall model generalizability.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

1116
598
598
582
597
597
1357
825

By determining which variables are most important we can simplify
the analysis of a dataset, better understand the physical concepts of it,
and exclude the ones that confuse the model achieving better accuracy
and learning/testing speed.

There exist several ways to determine variable importance, e.g.
Decision Trees, Random Forests, Chi-square, and Regression. We deal
with tree-based variable importance, like (Arthur et al., 2010) did. It

63.01%
33.76%
33.76%
32.86%
33.71%
33.71%
76.62%
46.58%

162

D. Eﬀrosynidis et al.

Ecological Informatics 48 (2018) 158–170

Fig. 4. Presence (green) and absence (red) points of seagrass in Mediterranean (above) and Sicily (below).

When computing the total F-measure, we use the macro-average.
Macro-averaging takes the average of all individual class F-measures,
treating small and large classes equally, in contrast to micro-averaging
which aggregates the TP/TN/FP/FN counts from all classes and com-
putes a total measure which is biased to the large classes.

We also experimented with using subsets of best features from top-
50% down to top-5% with a step of 5%, and the eﬀectiveness max-
imized at using top-10% of features. When all features are used, the best
performing classiﬁers are the tree-based Decision Tree and Random
Forest. For the top-10% of the features, which were calculated using
variable importance with Random Forest, all algorithms except the tree-
based ones give lower results. However, the best F-measure score (37.1
of Decision Tree) has been improved by 2.6% in comparison with the
all-features experiment and is now achieved by Random Forest with
39.7%.

The ﬁnal step is to train and test the machine learning algorithms in
this dataset for 5-class classiﬁcation. Stratiﬁed cross-validation was
used, and evaluated with Precision, Recall and F-measure. This dataset
is not balanced as can be seen in Table 1. Cymodocea is the dominant
class that constitutes the 75% of the dataset. Thus, Accuracy may not be
a suitable measure, because it measures how many correct predictions
were made overall, and if we predict all the test examples as cymo-
docea, then Accuracy would be close to 75% without even predicting
another class. This is clearly a problem because many machine learning
algorithms are designed to maximize overall Accuracy, with the ex-
ception of the tree-based algorithms. So, we resort to the F-measure; the
algorithms are ranked based on it.

Precision Recall
+

×
Recall

.

(3)

Typically, the input dataset is split into two disjoint subsets, the
training and the testing set. The training set is used to learn the model,
while the test set is used to measure a performance measure. But how
conﬁdent can we be about the classiﬁcation performance? The results
may be due to accidental characteristics of the speciﬁc partitioning. For
example, the test set may happen to include points that are easy to be
categorized, with the result that the categorizer yields good perfor-
mance. Consequently, the choice of a ﬁxed, predeﬁned partitioning of
the dataset may not be the best way for evaluating classiﬁers.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To address data bias during preprocessing of the deep learning pipeline, various techniques can be employed. One such technique is stratified splitting, where the dataset is divided into training and testing sets while maintaining the same distribution of target labels across both sets. This ensures that the model does not become overly influenced by any particular label or category.

Oversampling and undersampling are other methods used to balance datasets containing imbalanced classes. Oversampling involves duplicating instances from the minority class, while undersampling removes instances from the majority class. These techniques help prevent models from being biased towards the majority class.

Additionally, diverse data collection plays a crucial role in addressing data bias. By gathering data from multiple sources and ensuring representation from different demographics, cultures, and perspectives, the resulting dataset becomes more comprehensive and unbiased. This helps improve the generalizability and fairness of the trained models.

In summary, techniques such as stratified splitting, oversampling, undersampling, and diverse data collection are commonly used to mitigate data bias during preprocessing of the deep learning pipeline. Each method addresses specific aspects of bias, contributing to the development of robust and accurate models.
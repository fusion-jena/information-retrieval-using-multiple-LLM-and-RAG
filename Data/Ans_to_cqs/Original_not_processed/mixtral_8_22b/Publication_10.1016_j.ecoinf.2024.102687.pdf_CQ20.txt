Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

11 / 30 
14 / 30 
12 / 30 
15 / 30  

EcologicalInformatics82(2024)1026879J. Poutaraud et al.                                                                                                                                                                                                                              

Fig. 8. Estimation of the energy consumption related to the training of the models. This corresponds to the sum of CPU energy, GPU energy and RAMenergy in 
kilowatt-hour (kWh). Fine-tuning of four CNN backbones (i.e. ResNet18, VGG16, DenseNet121 and AlexNet) were accomplished on 5 way-1 shot (Left) and 5 way-5 
shot (Right) tasks using Matching Networks (MN), Prototypical Networks (PN) and Relation Networks (RN).

The model was fine-tuned using the Adam optimizer initialized with 
a default learning rate of 0.0001 and a weight decay of 0. The experi-
ments were conducted in Pytorch on 8 x NVIDIA GeForce RTX 2080 Ti. 
Fine-tuning was performed on the entire model for 100 epochs, with one 
epoch  corresponding  to  500  episodic  tasks.  Estimation  of  the  energy 
consumption related to the training of the models was calculated using 
the Python package CodeCarbon (https://codecarbon.io/) (v2.3.2 with 
Python v3.8.0). This corresponds to the sum of CPU energy, GPU energy 
and  RAM  energy  in  kilowatt-hour  (kWh).  Two  episodic  tasks  were 
constructed from the training set with 5 way-1 shot and 5 way-5 shot 
tasks.  Latent  space  representations  were  finally  extracted  from  the 
backbone of the model to evaluate their capacity to improve the quality 
of clustering. 

2.3. Parameter estimation of the latent space dimensionality

Finally, the deployment of large-scale computing in the context of 
ecoacoustics with DL often requires the use of many technologies with a 
significant environmental footprint. It is important to take this ecolog-
ical impact into account by estimating the energy consumption, as well 
as the use of resources on which it depends (e.g. rare earth minerals or 
electronic waste considerations (Lostanlen et al., 2021)). Systematic use 
of units of power should also be taken into account in the choice of the 
models, since to be really efficient, a model must have good clustering 
performances as well as low energy consumption, especially when the 
purpose  of  the  analysis  is  related  to  environmental  preservation  and 
biodiversity conservation. Although using a pretrained DenseNet made 
it possible to improve the quality of clustering in the best way, it also led 
to  the  highest  energy  consumption.  Meaning  that  the  best  clustering

2.2. Fine-tuning of the pretrained CNN backbone 

Fine-tuning of the pretrained CNN backbone was performed using 
classical meta-metric learning architectures adapted from the EasyFSL

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The experiments were conducted using 8 x NVIDIA GeForce RTX 2080 Ti GPUs. Fine-tuning was performed on the entire model for 100 epochs, with one epoch corresponding to 500 episodic tasks. The estimation of the energy consumption related to the training of the models was calculated using the Python package CodeCarbon, which corresponds to the sum of CPU energy, GPU energy, and RAM energy in kilowatt-hours (kWh).
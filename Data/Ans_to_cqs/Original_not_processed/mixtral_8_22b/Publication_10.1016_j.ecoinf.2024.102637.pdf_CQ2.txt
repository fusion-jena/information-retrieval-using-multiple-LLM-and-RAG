Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.2.4. Comparison with existing model methods 

Classic deep learning models, such as ResNet and DenseNet (Zhu and 
Newsam, 2017), and lightweight deep learning models, such as Mobi-
leNet (Sandler et al., 2018), ShuffleNet (Zhang et al., 2018), and Effi-
cientNet  (Tan  and  Le,  2019),  were  selected  to  demonstrate  the 
effectiveness of the proposed model. We train the bird sound recognition 
dataset using the above model, record the testing accuracy, and testing 
time of different models, and compare them with the model proposed in 
this study. The experimental data are shown in Fig. 8. 

Compared to other lightweight models, SIAlex improved the accu-
racy  to  93.66%  with  a  testing  time  of  only  2.466  ms.  SIAlex's  multi- 
indicator evaluation for each category is shown in Fig. 10, and Fig. 9 

Table 3 
Comparison of AlexNet under different activation function and classifier.   

ReLU 

✓  

Leaky 
ReLU 

✓ 
✓  

AlexNet 
AlexNet  
AlexNet  

Basic classifier

With  further  research  on  the  cross  domain  of  images  and  audio, 
Kumar  et  al.  (Kumar  et  al.,  2022)  successfully  applied  deep  transfer 
learning  models  to  bird  sound  classification  tasks.  They  used  well- 
known  deep  learning  models,  including  50  layers  residual  neural 
network and densely connected networks, to extract the Mel frequency 
cepstral coefficient of the audio as the feature input. The method per-
formed well for 22 categories of bird sound classification and achieved 
high accuracy. Wang et al. (Wang et al., 2023b) believes that extracting 
the Mel frequency cepstral coefficient features during bird singing has a 
good effect, and concurrently, using deep learning models to combine 
static and dynamic modelling improves accuracy. In the LDFSRE-NET 
(Hu  et  al.,  2023b)  and  MFF-ScSEnet  (Hu  et  al.,  2023a)  models 
designed by Hu et al., recognition was achieved by combining bird voice

An increasing number of scholars have integrated deep learning into 
´
bird voice recognition. 
Agnes Incze et al. (Incze et al., 2018) transformed 
bird  sounds  into  spectrograms  and  applied  convolutional  neural  net-
works  (CNNs)  to  classify  bird  sounds  on  the  Xeno-canto  dataset, 
demonstrating  the  potential  of  deep  learning  in  the  field  of  sound 
recognition.  Xie  et  al.  (Xie  and  Zhu,  2019)  further  confirmed  the  ad-
vantages  of  deep  learning  in  bird  sound  classification;  their  method 
outperformed traditional methods in classifying 14 bird species. Gupta 
et al. (Gupta et al., 2021) explored the application of CNNs in large-scale 
bird classification and proposed a model that demonstrated how mem-
ory  channel  patterns  change  over  time  and  the  changes  observed  in 
spectrograms.

audio feature extraction with deep learning. This not only demonstrates 
the potential of deep learning in bird voice recognition, but also high-
lights the importance of feature extraction and collaborative improve-
ment of network models.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context primarily discusses various deep learning models used for bird sound recognition, which implies that the data format used in this case is audio. Specifically, the authors mention transforming bird sounds into spectrograms, suggesting that the audio data is converted into visual representations for analysis. Additionally, they refer to extracting Mel Frequency Cepstral Coefficients (MFCC) from the audio data, which is another common technique used in processing audio signals. Therefore, based on the information provided, it can be concluded that the primary data format used in this deep learning pipeline is audio.
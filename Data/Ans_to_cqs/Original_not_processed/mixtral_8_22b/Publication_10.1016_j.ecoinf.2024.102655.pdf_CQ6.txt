Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.2. Machine learning models

2.6. Machine learning models

Learning models applied. In Section 3 results from CA, TSA and Machine
Learning models’ prediction are displayed and these results are dis-
cussed in Section 4.

Unraveling longitudinal pollution patterns of organic micropollutants in a river by

non-target screening and cluster analysis. Sci. Total Environ. 727, 138388 https://
doi.org/10.1016/j.scitotenv.2020.138388.

Bilbao, I., Bilbao, J., 2017. Overfitting problem and the over-training in the era of data:
Particularly for Artificial Neural Networks. In: 2017 Eighth International Conference
on Intelligent Computing and Information Systems (ICICIS). Presented at the 2017
Eighth International Conference on Intelligent Computing and Information Systems
(ICICIS), pp. 173–177. https://doi.org/10.1109/IN℡CIS.2017.8260032.

Bo¨enne, W., Desmet, N., Looy, S.V., Seuntjens, P., 2014. Use of online water quality

monitoring for assessing the effects of WWTP overflows in rivers. Environ Sci Process
Impacts 16, 1510–1518. https://doi.org/10.1039/C3EM00449J.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps can be taken to improve its performance and accuracy. These steps may include:

1. Data Cleaning: This involves removing or correcting any errors, inconsistencies, or missing values in the dataset. It also includes handling outliers and dealing with categorical variables if necessary.

2. Feature Scaling: Deep learning algorithms often perform better when numerical input variables are scaled to a standard range such as -1 to 1 or 0 to 1. Techniques like Min-Max Normalization, Z-score Normalization, or Log Transformation can be used for this purpose.

3. Dimensionality Reduction: High dimensional datasets can cause computational issues and lead to overfitting. Therefore, techniques like Principal Component Analysis (PCA) or Singular Value Decomposition (SVD) can be employed to reduce the number of features while retaining most of the information.

4. Encoding Categorical Variables: Since neural networks only work with numerical inputs, categorical variables need to be converted into numerical form using encoding methods like One Hot Encoding, Label Encoding, or Binary Encoding.

5. Train-Test Split: The dataset is divided into two parts – one for training the model and another for testing it. A common practice is to use 80% of the data for training and 20% for testing. Cross-validation techniques like K-Fold cross validation can also be used.

6. Stratified Sampling: When working with imbalanced classes, stratification ensures that each class has equal representation in both train and test sets.

These preprocessing steps help ensure that the deep learning model learns effectively from the given data and generalizes well to unseen examples. However, not all steps might be required depending upon the specific application and nature of the dataset.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1. Number of samples in the training, validation and test sets for Amazon database.

Set

Tiles

Available Def.
Samples

Available No-def.
Samples

Balanced Samples
(per Class)

Total Samples

Training

Validation

1, 7, 9, 13

5, 12

2706

963

78,431

39,697

Test

2, 3, 4, 6, 8, 10, 11, 14, 15

40,392

1,675,608

8118

2889

-

16,236

5778

1,716,000

The EF network architecture consisted of three convolutional layers (Conv) including the
Rectiﬁed Linear Unit (ReLU), two Max-pooling layers (MaxPool), and two Fully Connected layers
(FC), with a softmax layer at the end with two outputs, corresponding to “deforestation” and
“no-deforestation” classes. The ﬁlter and output size of each layer are summarized in Table 3.

132456789101112131415Remote Sens. 2020, 12, 910

11 of 28

Table 2. Number of samples in the training, validation and test sets for Cerrado database.

Set

Tiles

Available Def.
Samples

Available No-def.
Samples

Balanced Samples
(per Class)

Total Samples

2.3.3. Generation of Feature Maps

In this stage, the input patch pairs are convolved with the learned SVM ﬁlters to generate the
feature maps, which are fed to a pooling layer followed by a non-linear activation function. The output
is the input to the next convolution layer (see Figure 4). The procedure is repeated until the desired
number of layers is reached.

2.3.4. Classiﬁcation

Layer

Filter Size Output Size

Parameters

Input
Conv1
MaxPool1
Conv2
MaxPool2
Conv3
MaxPool3
Total params
Treinable params

-
3 × 3
1 × 1
3 × 3
1 × 1
3 × 3
1 × 1
-
-

15 × 15 × 16
13 × 13 × 12
11 × 11 × 12
9 × 9 × 12
7 × 7 × 12
5 × 5 × 12
3 × 3 × 15
-
-

-
1740
-
1308
-
1308
-
4356
4356

The buffer of both references was obtained applying the morphological dilation, using as
structuring elements a disk of radius 2. This operation expanded the boundaries of the deforested
polygons. Then, a difference between the dilated and original images was performed, resulting in the
outer edge, and the patches with the central pixel in these regions were not considered for training,
validation or test.

Remote Sens. 2020, 12, 910

13 of 28

2.6. Inﬂuence of the Number of Training Samples

The patch size was selected experimentally as 15 − by − 15. Then, the input of EF and CSVM was
a tensor of size of 15-by-15-by-16, for SN a tensor of size of 15-by-15-by-8 in each subnetwork and for
SVM a vector of size of 15 × 15 × 16. The procedure of the patch extraction was applied following the
overlapping sliding windows with stride equal to three. The size of the patch and stride were selected
empirically. In all methods, the input was an image patch, and the classiﬁcation outcome was assigned
to the patch central pixel.

Similar to [43], the images of Amazon and Cerrado databases were divided into 15 tiles, as shown
in Figures 7 and 8, respectively. From each image, four tiles were selected for training, two tiles for
validation, and nine tiles for testing.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there is no explicit mention of any specific data augmentation techniques such as flipping, rotation, scaling, etc., used in the deep learning pipeline. Therefore, it cannot be determined what data augmentation techniques have been applied in the study.
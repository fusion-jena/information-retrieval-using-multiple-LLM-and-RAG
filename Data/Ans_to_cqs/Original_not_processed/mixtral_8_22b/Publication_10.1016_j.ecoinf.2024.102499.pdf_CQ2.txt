Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2. Methods 

2.1. Deep learning model 

To assess the robustness of our method, we performed a 5-fold cross- 
validation.  For  each  fold,  we  randomly  selected  80%  of  our  dataset 
Videoclips  to  train  our  model,  and  20%  to  test  it.  We  could  not  split 
directly images from the frame dataset as images from the same videos 
were very alike which can lead to a model with a low generalization 
capacity.  Using  a  80%  training/20%  testing  random  split  on  videos 
rather  than images  ensured  full independency  between  images of  the 
training set and testing set.

clips (refer to Methods for details). Our training data included 8 shark 
species and 19 non-shark species commonly associated with sharks on 
baited underwater videos to add diversity in the training. After trans-
forming video clips into still images at a rate of 1 frame per second, we 
annotated  sharks  and  the  19  non-shark  species  present  in  the  frames 
with bounding boxes. This resulted in 26,947 fish annotations (Table 1). 
Given  that  the  Faster  RCNN  architecture  automatically  generates 
negative samples (Ren et al., 2015), there was no need to annotate any. 
It  is  known  that  classic  deep  architectures  need  numerous  image 
sample  per  class  to  work  correctly  (Lecun  et  al.,  2015;  Villon  et  al., 
2022). Hence, shark species that were infrequently captured in our clips 
(Carcharhinus  albimarginatus,  Galeocerdo  cuvier,  Nebrius  ferrugineus, 
Negaprion  acutidens)  were  excluded  from  the  testing  phase.  However,

Our  deep-learning  models  used  NASnet  architecture  (Zoph  et  al., 
2017) with a Faster-rcnn backbone (Ren et al., 2015) implemented in 
Tensorflow2. The parameters of the model can be found on TensorFlow 
model zoo1 under the name “faster_rcnn_nas”. All images were resized to 
1333 × 800 pixels to match with the pre-training data (COCO dataset 
(Lin et al., 2014)) used to prepare the first layers of our model and save 
computing time during the training phase). We used a learning rate of 
0.008 with a cosine learning rate decay. For each K-fold, the model was 
trained on its own version of Trainingclips  through 200,000 iterations, 
with a batch size of 16 images per iteration. The training was completed 
in 96 h  per model using a GPU-cluster equipped with 4 RTX8000. In 
order  to  evaluate  the  performance  of  our  deep-learning  models,  we 
computed the recall, precision, and F-measure for each model. Briefly, a

seen in any one video frame across the duration of the video record” to 
estimate relative abundance (Langlois et al., 2020; Tian et al., 2020). 
This abundance metric, called MaxN, is until now the standard and most 
used method for estimating fish and shark abundance on BRUVS (Cappo 
et al., 2003; Langlois et al., 2020; Tian et al., 2020; Whitmarsh et al., 
2017). Convolutional Neural Networks (CNNs) demonstrate exceptional 
performance  in  object  detection  and  identification  within  images, 
showcasing  the  capabilities  of  artificial  intelligence  for  processing 
BRUVS  videos.  Indeed,  fish  identification  and  localisation  with  Deep 
Learning  has  been  continuously  improving  since  2015  (Chen  et  al., 
2018; Cui et al., 2020; Jalal et al., 2020; Knausgård et al., 2022; Rathi 
et al., 2018; Salman et al., 2016; Tian et al., 2020; Villon et al., 2016, 
2018).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning pipeline primarily uses video format data. Specifically, baited remote underwater video stations (BRUVS) are employed to capture footage of marine life, including various shark species and other aquatic organisms. These videos are then transformed into still images at a rate of 1 frame per second. Subsequently, these images are utilized for training and testing the deep learning model. Additionally, the authors mention that they could not directly use individual images from their dataset due to similarities among them, leading to potential overfitting issues. Therefore, they opted for a 80% training/20% testing random split on videos instead of images to ensure independence between the training and testing sets.
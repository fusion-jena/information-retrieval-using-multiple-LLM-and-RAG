Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The training data was acquired by sending query parameters through 
the  application  programming  interface  (API)  of  Xeno-Cano  which 
returned a JSON object containing recording metadata. The recordings 
were downloaded according to four query parameters: (i) audio quality, 
corresponding  to  the  highest  audio  quality  (i.e.  A  quality  level),  (ii) 
duration,  corresponding  to  recordings  lasting  from  20  to  60  s,  (iii) 
maximum number of recordings allowed per species, which was set to 
100 to avoid large imbalanced between the classes and ensure accept-
able  computing  power,  and  (iv)  geographic  coordinates  of  the  re-
cordings  that  surrounded  the  Equator  line  in  America.  Geographical 
coordinates were defined according to the latitude of the Tropics, with 
◦
26 ‘10.6”N and 
the Tropic of Cancer in the Northern Hemisphere at 23
26′ 10.6”S. 
the Tropic of Capricorn in the Southern Hemisphere at 23

The model was fine-tuned using the Adam optimizer initialized with 
a default learning rate of 0.0001 and a weight decay of 0. The experi-
ments were conducted in Pytorch on 8 x NVIDIA GeForce RTX 2080 Ti. 
Fine-tuning was performed on the entire model for 100 epochs, with one 
epoch  corresponding  to  500  episodic  tasks.  Estimation  of  the  energy 
consumption related to the training of the models was calculated using 
the Python package CodeCarbon (https://codecarbon.io/) (v2.3.2 with 
Python v3.8.0). This corresponds to the sum of CPU energy, GPU energy 
and  RAM  energy  in  kilowatt-hour  (kWh).  Two  episodic  tasks  were 
constructed from the training set with 5 way-1 shot and 5 way-5 shot 
tasks.  Latent  space  representations  were  finally  extracted  from  the 
backbone of the model to evaluate their capacity to improve the quality 
of clustering. 

2.3. Parameter estimation of the latent space dimensionality

2.2. Fine-tuning of the pretrained CNN backbone 

Fine-tuning of the pretrained CNN backbone was performed using 
classical meta-metric learning architectures adapted from the EasyFSL

◦

3.1.2. Testing data acquisition 

The testing data was acquired in the same manner as the training 
data, except that it used (i) both high and low quality for the Xeno-Canto 
recordings  (i.e.  from  A  (highest  quality)  to  E  (lowest  quality)  quality 
levels), and (ii) no restrictions for the length of the recording in seconds. 
The query included audio recordings of 30 nocturnal and crepuscular 
tropical bird species that were not present in the training data (Table 1). 
These  species  belong  to  nocturnal  and  crepuscular  tropical  bird 

Fig.  2. Schematic  representation  of  a  Harmonic-Percussive-Derivative  (HPD) 
sound image. Left: HPD image. Right: Harmonic component (top), percussive 
component (middle), and first derivative (bottom) of the original spectrogram 
in dB.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given information, there isn't a direct mention of the specific criteria used to determine when training is complete. However, we can infer some details about the training process.

The model was fine-tuned using the Adam optimizer with a default learning rate of 0.0001 and a weight decay of 0. The experiments were conducted in PyTorch on 8 x NVIDIA GeForce RTX 2080 Ti. Fine-tuning was performed on the entire model for 100 epochs, with one epoch corresponding to 500 episodic tasks.

From this, we can deduce that the training process was determined based on the number of epochs rather than a validation loss plateau or other similar metrics. In machine learning, an epoch refers to one cycle through the full training dataset. Therefore, in this case, the training was completed after going through the full dataset 100 times.

However, without explicit information regarding the use of validation loss plateaus or other stopping criteria, it cannot be definitively stated what criteria were used to determine when training was complete beyond the specified number of epochs.
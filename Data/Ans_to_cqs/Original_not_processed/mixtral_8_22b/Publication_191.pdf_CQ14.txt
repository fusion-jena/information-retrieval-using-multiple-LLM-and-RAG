Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

the parameter setting in training, we employ ﬁxed learning policy. 
We set the learning rate to 10 −3 , and then decrease it by a fac- 
tor of 10 when the validation set accuracy stops improving. The 
momentum is set to 0.9 and weight decay to 10 −4 . In this experi- 
ment, we compute the top-1 classiﬁcation accuracy as described in 
Section 4.3 . 

6.2.1. Results and discussion

170 , conv 4 

373 , conv 4 

In Fig. 8 , we visualise the response of the selected ﬁlter units 
( conv 4 
365 ) in layer 4. In layer 4, 
148  and conv 4 
we  observe  mid-level  semantic  partial  abstraction  of  leaf  struc- 
tures, where the features extracted have almost similar complexity 
levels to layer 3. For example: venation-like features are observed 
in the channel conv 4 
373 ( Fig. 5 (i)) based on the deconvolution re- 
sult of the validation set; the neurons are not only activated on 
the divergent structures (secondary veins) but on the central veins 
(primary veins) as well. For the channel conv 4 
170 , the selected ﬁl- 
ters are activated by the curvature of the lobed leaves, as shown 
in the deconvolution of the top two image patches ( Fig. 5 (j)). This 
can be interpreted as extraction of conjunctions of curvature fea- 
tures  in  certain  orientations.  On  the  other  hand,  for  the  chan- 

8 

S.H. Lee et al. / Pattern Recognition 71 (2017) 1–13

Dataset 

Quantity of images 

Number of 
categories 

MS COCO [51] 
Places2 [52] 
Sport-1M [53] 
Visual Genome QA [54] 
ILSVRC 2010 [55] 
PlantClef2015 dataset [6]  113,205 

328k (2.5 million labeled instances) 
8.3 million 
1 million 
1.7 million questions/answer pairs 
1.4 million 

91 
365 
487 
–
10 0 0 
10 0 0 

pre-train each network using its corresponding leaf data. During 
the validation phase, we combine both softmax outputs and com- 
pute the ﬁnal class scores using fusion methods: average (ave) or 
max voting (mav).

We  train  our  model  using  Caffe  [35]  framework.  For  the  pa- 
rameter  setting  in  training,  we  employ  step  learning  policy.  The 
learning rate was initially set to 10 −3  for all layers to accept the 
newly deﬁned last fully connected layer set to 10 −2 . It is higher 
than other layers due to the weights being trained starting from 
random. The learning rate was then decreased by a factor of 10 ev- 
ery 20K iteration and was stopped after 100K iterations. The units 
of the third fully connected layer (fc8) were changed according to 
the number of classes of training data. We set the batch size to 50 
and momentum to 0.9. We applied L 
2 weight decay with penalty 
multiplier set to 5 × 10 −4 and dropout ratio set to 0.5, respectively. 

4.2. Deconvolutional network

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about how the hyperparameters of the model were optimized. However, there are some clues regarding the learning rate schedule used during training. The authors mention they employed a fixed learning policy, which means the learning rate was likely manually tuned rather than automatically searched through techniques like grid search or random search. They also describe adjusting the learning rate by a factor of 10 when the validation set accuracy stops improving, indicating a form of manual learning rate scheduling. Other hyperparameters such as momentum, weight decay, and batch size appear to be set to specific values without any indication of optimization methodology. Therefore, while the exact process of hyperparameter optimization remains unclear, it appears that the authors primarily relied on manual tuning and heuristics.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

set 

is 

to 
1. 

2.3.3. Experimental setting 

In our experiment, a deep learning environment based on Python and 
Tensorflow – GPU as well as CUDA was constructed under the Windows 
operating system. The deep feature loss bird noise reduction network 
model was trained for 50 epochs on an RTX 2080 Ti GPU using the Adam 
(cid:0) 4. In each 
optimizer (Kingma and Ba, 2014) with a learning rate of 10
epoch, the entire training set was presented in a random order, with one 
noisy bird sound signal per iteration. 

2.4. Comparative algorithms 

Using the same test  set, we compared the  denoising results of the 
deep feature loss-based bird sound noise reduction network with several 
other  bioacoustics  noise  reduction  algorithms,  namely  SEGAN, 
WebRTC, wavelet transform (Priyadarshani et al., 2016), Wiener filter 
(Loizou, 2017), and MMSE STSA (Brown et al., 2017) algorithms.

(2)  

∑

Λ15 =

Λ14

j × K14

j + b,

j

where b is a learned bias term. 

Fig. 4. Framework of the bird sound noise reduction network based on deep feature loss.  

EcologicalInformatics80(2024)1025174C. Zhang et al.                                                                                                                                                                                                                                   

2.3.2. Training of the bird sound noise reduction network based on deep 
feature loss

m=1

λm‖Φm(β) (cid:0) Φm(g(x; θ) ) ‖1,

(3)  

where Φm  represents the feature activation of the m-th feature layer in 
the 1st  to M-th shallowest network layers of the audio classification loss 
network, β represents the clean bird sound signal, g(x; θ) represents the 
processed bird sound signal generated by the noise reduction network 
during training, and θ is a parameter of the noise reduction network. The 
initial value of the weight λm  is set to 1, whereas after 10 training iter-
ations, 
relative  value 
it 
‖Φm(β) (cid:0) Φm(g(x; θ) ) ‖

reciprocal  of 

the 

the 

set 

is 

to 
1. 

2.3.3. Experimental setting

reducing noise, and accordingly, would not be suitable for processing 
large  volumes  of  recorded  data  collected  in  the  field.  Furthermore, 
compared with the bird sound noise reduction network based on deep 
feature loss and the SEGAN algorithm, we found that the noise reduction 
efficiencies of the WebRTC and Wiener Filter algorithms were notably 
higher. This disparity in performance can be attributed to the fact that 
the deep learning model is larger in size and is thus more demanding of 
computational resources. However, it should be noted that the execution 
time of the deep learning models on CPUs is acceptable.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The experimental setting mentioned in the given context uses the Adam optimizer in the deep learning pipeline. The Adam optimizer is used to train the deep feature loss bird noise reduction network model for 50 epochs on an RTX 2080 Ti GPU. The initial learning rate of the Adam optimizer is set to 10^-4. During each epoch, the entire training set is presented in a random order, with one noisy bird sound signal per iteration.

Adam (short for Adaptive Moment Estimation) is a popular optimization technique used in deep learning pipelines. It combines the advantages of two other optimization methods - AdaGrad and RMSProp. Adam computes adaptive learning rates for different parameters from estimates of first and second moments of the gradients. It has been shown to perform better than other optimization techniques like Stochastic Gradient Descent (SGD) or Root Mean Square Propagation (RMSprop).

Therefore, in this case, the Adam optimizer is chosen over other optimization techniques due to its superior performance in handling large datasets and complex models.
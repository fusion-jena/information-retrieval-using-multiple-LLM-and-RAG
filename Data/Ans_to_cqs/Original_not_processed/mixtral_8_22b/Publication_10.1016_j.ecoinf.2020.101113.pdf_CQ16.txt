Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

other  sites  and  older  years  were  searched  to  increase  the  training 
sample  size  for  certain  species.  78%  of  recordings  were  collected  in 
2019, 11% in 2018, and 11% collected between 2015 and 2018. Most 
recordings were collected in the months March and April, a period of 
high acoustic activity.

2.3.  Model training 

2.3.1.  Training data preprocessing 

The  CNN  model  used  in  this  study  requires  equally  sized  input 
images. We chose a time-frequency input window size of 2 s as it is near 
the mean and median template duration across target call types (Fig. 2). 
Most  call  types  have  a  duration  below  1  s,  and  for  those  above  2  s,

and  stretching  of  target  signals,  and  adding  Gaussian  noise  or  noise 
from soundscapes (Koh et al., 2019; Lasseck, 2019). This competition's 
training  data  consisted  primarily  of  recordings  targeting  single  fore-
ground species, but also a smaller validation set of annotated sounds-
capes  like  those  in  the  test  set.  Results  from  this  challenge  showed  a 
significant  increase  in  performance  for  submissions  that  incorporated 
the validation soundscape data in training (Kahl et al., 2019). Specifi-
cally,  Lasseck  (2019)  found  that  adding  background  noise  from  the 
validation  soundscapes  to  training  samples  significantly  improved 
performance  on  the  test  soundscapes.  This  indicates  that  acoustic 
monitoring  systems  can  benefit  significantly  from  location-specific 
training data.

where  NP  represents  the  number  of  true  presences.  A  precision-recall 
curve consists of the points in precision-recall space achieved at each 
possible threshold. Recall is typically the horizontal axis and precision 
the vertical axis. Based on the precision-recall curve we also measured 
the average-precision (AP) of predictions. The AP is defined: 

where (τ1, τ2, …, τN) are the different thresholds to be evaluated, sorted 
in descending magnitude. Typically, the chosen thresholds (τ1, τ2, …, τN) 
are the sorted predicted scores. The AP is the weighted sum of preci-
sions at each threshold, using the increase in recall from the previous 
threshold as the weight. It approximates the integral of, or area under, 
the  precision-recall  curve.  The  AP  is  independent  of  the  chosen 
threshold τ, so it is commonly used for model comparison. The mean- 
average-precision (mAP) across classes is commonly used in multi-label 
prediction evaluation.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't explicit information about the specific criteria used to determine when training is complete. However, common practices in machine learning include monitoring metrics such as validation loss, accuracy, or other relevant measures during the training process. When these metrics reach a plateau or stop improving, it may indicate that further training will not yield significant improvements, suggesting that training could be considered complete. In some cases, early stopping mechanisms might be employed to halt training if no improvement is seen after a certain number of epochs. Nonetheless, without more specific details regarding the training procedure, it's impossible to provide a definitive answer based solely on the provided context.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

9 of 21

2.6.2. Threshold Adjustments

To incorporate class confusion, we increased the membership change threshold for
each class as a function of the commission error percent multiplied by a confusion penalty.
The error percentage in its decimal form was rounded to the nearest tenths place to account
for uncertainty in the estimates. The confusion penalty was set with a value of 30%.
For the shrub class for example, this increased the threshold by 9% (0.3 × 30). For this
implementation, the membership change threshold was selected to be slightly biased
towards commission error. Including this adjustment increased the threshold speciﬁc to
each class to reduce error.

For training, the Adam optimizer was used with 500 epochs and categorical-cross
entropy loss function. Batch size was set to 64. Data augmentation was applied to enhance
sample variation and included rotation, reﬂectance bias of ±10%, and random noise of
±10% reﬂectance for 15% of the input image. To determine training convergence, we
monitored the validation data across training epochs and the network weights with the
best performance were kept. Early stopping criteria was applied if no improvement was
found in 50 epochs. For CNNs, weight initialization and random selection for batch
training can lead to variability in the results. Thus, an ensemble of three models were
generated and the average of the ﬁnal output layer taken. Max pooling was used for two
models in the ensemble and average pooling for one. In initial tests using max or average
pooling showed that both provided improvement in some cases and thus could enhance

For classiﬁer development, samples across the prairie region were acquired for train-
ing, veriﬁcation, and ﬁnal independent testing. The veriﬁcation sample was used to
determine training convergence of the deep-learning classiﬁer and the test sample for inde-
pendent assessment. To generate these, a systematic approach was used where samples
were collected following a grid of sample blocks of 9 by 9 km with 15 km spacing between
blocks. Blocks were assigned as training, validation, and test. For the test sample a column
of the sample grid was randomly selected and every fourth column from that added. Only
every second row for the selected columns were included in the test sample. Figure 2
shows the distribution of these sample sets. In total there were 39,520 (60%) training
samples, 23,934 (30%) veriﬁcation samples and 6509 (10%) test samples. Here we focused
the majority of the samples for training and veriﬁcation in an effort to maximize model

2.5.2. Random Forest

For random forest, input features included the mean and standard deviation for image
window sizes of 5 × 5, 15 × 15, and 32 × 32 in addition to the central pixel values for
each input feature. We used a random search to identify an optimal parameter set where
100 sets were sampled and compared using three-fold cross validation. Search parameter
ranges were number of trees (200 to 800, by 100), minimum samples split (2–4), minimum

Remote Sens. 2021, 13, 634

7 of 21

samples per leaf (2–4), maximum depth (full), bootstrap (true), and maximum features for
split (square root of the number of features). For a description of these parameters see the
Scikit-learn documentation [48]. These results were reﬁned using a grid-search with the
parameter ranges set to ±50 for the number of estimators and ±1 for the other parameters.

2.6. Membership Based Change Detection and Segmentation
2.6.1. Split Window Detection of Change Seed Points

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: Training completion is determined by monitoring the validation data across training epochs. The network weights with the best performance are kept. Early stopping criteria is applied if no improvement is found in 50 epochs. This means that if the validation loss does not decrease for 50 consecutive epochs, the training process will stop. Additionally, the verification sample is used to determine training convergence of the deep-learning classifier.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Ecological Informatics 50 (2019) 220–233

Fig. 5. Learning process for loss (left) and model accuracy (right) over 90 epochs. The gap between training and validation accuracy is caused by using dropout on
the training data only.

All statistics were performed in R (version 3.3.1). The best model
was chosen based on the two parameters accuracy and loss, whereby
loss serves as a measure on how far model predictions diﬀer from the
actual class. Model accuracy and loss were calculated for both training
and validation set. We tested the performance of the ﬁnal model on two
diﬀerent data sets: 1) on individual tiles and 2) on whole repeat pho-
tographs. Prediction accuracy on individual tiles was calculated using
the 5796 tiles from the test set (= 10%), which has been separated from
the total number of samples before training. We evaluated the accuracy
on whole repeat photographs based on the image pairs of the second set
of photographs. The classiﬁcation results for each of these 34 images
were compared to the corresponding manual classiﬁcation (reference
data). A confusion matrix was prepared for each photograph in-
dividually. The confusion matrix consists of pixel numbers for true

To prevent spatial autocorrelation between neighboring tiles, the
tiles retrieved from all 50 training photographs were collected in one
folder and shuﬄed, before they were split into training, validation and
test set. In this regard, the validation set is used during training for the
ﬁne tuning of hyperparameters and model selection (Hastie et al.,
2009). Only after the best model setup has been found, its prediction
error is assessed once again on the test set. For splitting the whole set of
samples into the three sets, we ﬁrst took a random sample of 10% from
the total number of tiles as validation set and then a further sample of
10% as test set. The remaining 80% of the tiles were used as training
set. To further increase the number of sample tiles, we performed data
augmentation on the training set. Data augmentation is a common
practice in machine learning to artiﬁcially increase the number of
training samples by applying slight transformation on the original data,

3. Results

3.1. Training and model performance on individual tiles

We stopped the learning process after 90 epochs, where the learning
curve converged and loss values did not longer decrease. Fig. 5 illus-
trates the improvement in loss and accuracy during the training pro-
cess. The fact that validation accuracy exceeds training accuracy is a
common eﬀect when dropout and data augmentation is applied on
training data only. Consequently, the training set contains more diﬃ-
cult tiles than the validation set. Based on the validation set, the best
model reached a maximum accuracy of 96.4% (loss = 0.13). Ad-
ditionally, we evaluated the performance of the ﬁnal model on a se-
parate test dataset containing n = 5796 individual tiles, which were not
used during the process of model selection. On these individual tiles,
the model reached an accuracy of 96.7% (loss = 0.08).

U. Bayr and O. Puschmann

Ecological Informatics 50 (2019) 220–233

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors monitored the model performance during training by evaluating the learning curves for loss and accuracy over 90 epochs. They also employed a validation set during training for fine-tuning hyperparameters and model selection. After finding the best model setup, they assessed its prediction error once again on the test set. Additionally, they used dropout and data augmentation techniques on the training data only, causing the training set to contain more difficult tiles than the validation set. This resulted in higher validation accuracy compared to training accuracy, which is a common effect when employing such techniques.
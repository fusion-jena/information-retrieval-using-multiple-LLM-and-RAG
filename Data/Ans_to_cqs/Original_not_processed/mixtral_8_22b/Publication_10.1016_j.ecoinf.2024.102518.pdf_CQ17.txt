Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

feature class as ‘L', ‘Q', ‘LQ', ‘LQP’, and set the regularization multiplier 
from 0.5 to 5, using the delta in the Akaike minimum information cri-
terion (delta.AICc) as the metric to determine the best tuning parame-
ters. The optimal model is the one with the minimum delta.AICc value 
(delta.AICc = 0), and a model with delta.AICc <2 is considered credible 
(Phillips et al., 2017). The Continuous Boyce Index (CBI)(Hirzel et al., 
2006) was employed in addition to the Area under the Receiver Oper-
ating Characteristic curve (AUC-ROC) because AUC-ROC is not reliable 
when  obtaining  true  absences  is  challenging  (Jim´enez  and  Sober´on, 
2020). The final optimal parameters were identified as the feature class 
‘LQP’ and a regularization multiplier of 3.5 (Fig. A.2). The mean AUC- 
ROC  stands  at 95.5%,  while the  average  CBI  index  for the  validation 
dataset is 82.6%.

Fig. 1. The workflow of the study.  

cross-validation. The threshold was derived from the average output of 
the five models. Nine algorithms—Bioclim, Mahalanobis, Domain, GLM 
(gaussian),  GLM  (binomial),  GLM  (poisson),  MaxEnt,  Random  Forest, 
and SVM—were evaluated using AUC-ROC and Cohen's KAPPA metrics. 
Our analysis revealed that MaxEnt emerged as the most suitable model, 
striking  a  commendable  balance  between  accuracy  and  robustness 
(Fig. A.1).

invaluable contributions to this research:  

• H.X.  and  L.J.  conceived  the  research  concept  and  designed  the 

methodology.  

• H.X. conducted formal analysis and curated the data.  
• H.X., L.J., and Y.L. contributed to the allocation of resources and data 

curation.  

• H.X.,  L.J.,  and  Y.L.  collaborated  on  the  original  draft  of  the 

manuscript.  

• L.J. played a crucial role in the review and editing process.  
• H.X. and L.J. provided supervision throughout the project.  
• L.J.  oversaw  project  administration  and  secured  funding  for  the 

research. 

This research was supported by a grant from National Natural Sci-
ence Foundation of China [grant number: 42071253]. The authors are 
grateful to the reviewers for their comments and suggestions. 

All  authors  have  reviewed  and  approved  the  final  version  of  the 

manuscript.  

Appendix A. Appendix

Fig. A.1. Model performance across various algorithms.

Percent  contribution  and  permutation  importance  are  two  metrics 
provided by MaxEnt to determine the importance of input variables in 
the final model (Phillips, 2005). The percent contribution measures the 
contribution of each variable to the model as a percentage of the total 
contribution of all variables. The permutation importance measures the 
decrease  in  training  AUC  for  each  variable  when  the  values  of  that 
variable are randomly permuted among the training points. We identi-
fied the variables that had high percent contribution and permutation 
importance scores as key variables. We then compared the characteris-
tics  of  these  key  variables  between  potential  and  non-potential  areas 
using violin plots. 

Areas with high population densities in regions highly suitable for 
Asian  elephants  are  more  likely  to  experience  HEC,  especially  when

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide specific details about the use of regularization methods such as dropout or L2 regularization in a deep learning pipeline. However, it mentions the use of a regularization multiplier ranging from 0.5 to 5 in the context of determining the best tuning parameters for a model. This suggests that some form of regularization was applied during the modeling process, although the exact method remains unclear without additional context.
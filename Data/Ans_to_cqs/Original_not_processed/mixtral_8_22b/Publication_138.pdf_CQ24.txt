Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

B 

1521 
6705 
1564 
6748 
1185 
6885 
1160 
6661 
1786 
6452 
1004 
6885 
2109 
6314 
1805 
6471 
478 
6979 
1634 
6294 

1131 

Accuracy (%) 

93.00 

93.00 

94.48 

94.41 

90.12 

95.42 

89.60 

91.22 

97.57 

91.18 

93.00 

Figure 7 shows the relation between Min-batch size and Accuracy, Epoches. As we see in Figure 
7(a), when the Min-batch size is 128, the accuracy is the highest. This indicates the Min-batch size is 
relation to the accuracy of algorithm. In Figure 7(b), the larger is the Min-batch size, the larger is the 
number of epochs. When the Min-batch size is more than 256, the number of epochs is sharp large. The 
larger is the number of the epochs, the more is the consumed time when the algorithm converges. We 
also  can  see,  when  the  Min-batch  size  is  128,  the  accuracy  of  algorithm  is  high,  and  the  number  of 
epochs is relatedly small.

93.00 

92.24 

92.50 

93.00 

94.00 

81.44 

81.00 

85.00 

91.50 

93.00 

92.00 

92.50 

94.20 

128 

128 

128 

128 

28 

80 

20 

20 

20 

7.439 

6.451 

6.511 

6.953 

16.342 

13.192 

13.012 

20.160 

31.879 

12 

7 

7 

8 

19 

22 

18 

50 

152 

Figure 6. The results of algorithms. 

(a) 

(b) 

Figure 7 The relation between Min-batch size and Accuracy, Epoches. 

Mathematical Biosciences and Engineering 

Volume 18, Issue 2, 1121–1135. 

 
 
Table 2. The results of our algorithms. 

Fold 

Class 

Number 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

Average 

A 
B 
A 
B 
A 
B 
A 
B 
A 
B 
A 
B 
A 
B 
A 
B 
A 
B 
A 
B 

21600 
7200 
21600 
7200 
21600 
7200 
21600 
7200 
21600 
7200 
21600 
7200 
21600 
7200 
21600 
7200 
21600 
7200 
21600 
7200 

A 

20079 
495 
20036 
452 
20415 
405 
20440 
539 
19814 
748 
20596 
315 
19491 
886 
19795 
729 
21122 
221 
19966 
906 

B

We also see that, the more the number of the layers of networks, the larger the min-batch size is. 
But  the  VGG-19  is  just  the  opposite  due  to  its  too  much  layers.  The  inception  module  induces  the 
number  of  parameters  and  computing  cost  of  the  GoogLeNet  and  ICSNet,  and  remains  more  high 
identifying ability. 

Mathematical Biosciences and Engineering 

Volume 18, Issue 2, 1121–1135. 

 
1130 

Table 1. The results of six algorithms. 

Algorithms 

Accuracy 
(%) 

Precision 
(%) 

Recall 
(%) 

F1 
(%) 

AUC 

Min-batch 
size 

Time(s) 

The number of 
Layers 

ICSNet 

ICSNet_n 

LeNet 

AlexNet 

VGG-19 

GoogLeNet 

ResNet-18 

ResNet-50 

ResNet152 

93.00 

80.50 

80.50 

85.00 

91.00 

92.50 

91.10 

92.15 

93.00 

93.20 

81.60 

81.42 

84.35 

92.30 

92.64 

92.42 

92.50 

93.45 

93.30 

80.50 

80.64 

84.41 

92.00 

92.50 

92.15 

92.15 

93.30 

93.00 

80.50 

80.50 

84.38 

92.15 

93.00 

92.24 

92.50 

93.00 

94.00 

81.44

MBE, 18(2): 1121–1135. 
DOI: 10.3934/mbe.2021060 
Received: 29 August 2020 
Accepted: 14 December 2020 
Published: 11 January 2021 

http://www.aimspress.com/journal/MBE 

Research article 

Efficiently deep learning for monitoring Ipomoea cairica (L.) sweets in 

the wild 

Fei Tang1,2, Dabin Zhang1,* and Xuehua Zhao2 

1  College  of  Mathematics  and  Information,  South  China  Agricultural  University,  Guangzhou 

510642, China 

2  Shenzhen Institute of Information Technology, Shenzhen 518172, China 

*  Correspondence: E-mail: zdbff@scau.edu.cn.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To handle randomness in the deep learning pipeline, several strategies can be employed. One common approach is to use a fixed random seed value across different runs or experiments. By setting a specific seed value, the same sequence of pseudo-random numbers will be generated each time, ensuring reproducibility and consistency in the results. Another strategy is to perform multiple runs with different random seeds and then average the results to reduce the impact of any single run's randomness. Additionally, techniques such as cross-validation, bootstrapping, and ensemble methods can help mitigate the effects of randomness by aggregating predictions from multiple models trained on different subsets of data. However, it should be noted that while these strategies can improve robustness and generalizability, they may not completely eliminate all sources of randomness in the deep learning pipeline.
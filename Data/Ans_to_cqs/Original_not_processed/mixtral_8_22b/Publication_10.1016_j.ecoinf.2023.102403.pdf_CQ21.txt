Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Data will be made available on request.  

EcologicalInformatics79(2024)1024038V. Pizarro et al.                                                                                                                                                                                                                                 

Appendix A. The database 

Table A.1 below shows the data loss for each criterion that we have used to clean our database. We downloaded 71,670,596 records from GBIF and 

OBIS. Only 820,004 records were useful for our analyses.  

Table A.1 
Criteria for filtering occurrence data from GBIF and OBIS using bioregions.  

Database state 

Number of records 

Origial records from GBIF and OBIS 
Data curation (following Zizka et al. (2020)) 
Taxonomically filtered data 
Deletion of data outside the native range 

71,670,596 
5,380,439 
5,007,322 
820,004

18.49 
68.75 
15.74 
46.35 
42.39 
94.96 
63.24 
79.52 
88.74 
96.31 
23.82 
35.59 
67.52 
45.83 
74.52 
36.68 
91.36 
48.29 
90.40 
63.61 
74.78 
76.12 

15.13 
19.79 
6.54 
22.34 
14.75 
2.21 
11.24 
11.27 
8.71 
2.41 
8.42 
21.61 
15.80 
10.83 
13.06 
10.95 
4.90 
16.27 
6.93 
17.35 
9.63 
18.00 

L 

5.04 
1.04 
3.39 
7.93 
4.92 
0.13 
0.87 
0.89 
0.00 
0.04 
5.65 
2.45 
1.01 
2.50 
0.00 
3.84 
0.00 
3.78 
0.06 
1.43 
2.84 
0.00 

R5 (∼ 5

◦

) 

R10 (∼ 10

◦

) 

M 

H 

NR 

IR 

L 

M 

H 

NR 

IR 

L 

M 

H 

36.97 
10.42 
36.80 
22.13 
27.87 
1.87 
14.46 
7.17 
1.57 
0.88 
32.85 
35.59 
12.00 
30.83 
7.07 
34.65 
1.57 
22.06 
2.27 
13.56 
11.48 
5.10 

24.37 
0.00 
37.53 
1.25 
10.07 
0.83 
10.19 
1.15 
0.99 
0.36 
29.26 
4.76 
3.67 
10.00 
5.35 
13.88 
2.17 
9.61 
0.35 
4.04 
1.27 
0.78 

0.00 
10.00 
3.57 
28.13 
14.29 
82.13 
17.09 
43.93 
28.74 
70.87 
8.62 
14.29 
13.76 
46.15 
20.00 
5.77 
47.93 
6.50 
53.45 
8.20 
34.68 
33.33

◦ = 3021 cells, and ∼ 10

◦
; R5= ∼ 5

◦

Table C.2 
◦
Surface area as a share of each bioregion (ID) for every SRI category for each of the three grid sizes (R1= ∼ 1
). Values show the surface area as 
a share of each bioregion for every SRI category (see §2.2.1). ID is the identification number given to each bioregion (Table 1). H are cells with a high representativeness 
of species richness (i.e. SRI > 0.85). M are cells considered as having a medium representativeness (i.e. SRI ∈ (0.60, 0.85)). L cells are cells with a low number of records 
and are thus not considered to be representative of actual species richness (i.e. SRI ∈ (0, 0.6)). NR as cells with no records (SRI = NA), and IR as cell with insufficient 
records to apply SRI.  

◦
; R10= ∼ 10

◦
; R5= ∼ 5

ID 

R1 (∼ 1

◦

) 

NR 

IR 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22

Still, identifying how sampling efforts are distributed across space 
and time is a required step to interpret biodiversity patterns and reduce 
biases,  as  understanding  our  biota  distribution  is  critical  for  well- 
designed  protection  efforts.  This  may  be  achieved  through  different 
weighting  schemes  for  records  in  areas  with  sufficient  sampling  that 
provide  a  more  reliable  contribution  compared  to  underrepresented 
regions (Hortal et al., 2008; Phillips et al., 2009; Yang et al., 2013).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The information provided does not directly address the postprocessing steps involved after model training such as saliency maps, metrics calculation, or confusion matrices. However, based on general machine learning practices, some common postprocessing steps include:

1. Model evaluation: Evaluating the performance of the trained model using various metrics like accuracy, precision, recall, F1 score, ROC curves, etc.

2. Hyperparameter tuning: Adjusting hyperparameters to improve the model's performance by performing techniques like Grid Search, Randomized Search, Bayesian Optimization, etc.

3. Feature importance: Identifying which features contribute most significantly towards predicting the target variable. Techniques like permutation feature importance, SHAP values, etc., can help determine this.

4. Visualizing results: Creating visualizations like confusion matrices, ROC curves, lift charts, gain charts, etc., to better understand the model's performance and behavior.

5. Saliency maps: Generating heatmaps highlighting important regions within input images that influence predictions, often used in computer vision tasks.

However, these steps might vary depending upon the specific use case and requirements.
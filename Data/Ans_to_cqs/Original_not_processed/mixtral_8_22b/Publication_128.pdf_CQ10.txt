Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Figure 2. Feature visualization of AlexNet layer by layer. The annotations above are defined as 
Nameùëë@k√ók, and the annotations below are denominated in form of Operationùëë@r√ór. Where d , k 
and r is feature depth, convolution kernel and image resolution, respectively. 

ResNet: In 2016, He et al.[30] proposed a short connection from low layer to the second higher layer, 
which  increases  network  layers  but  avoids  degradation  at  the  same  time.  The  residual  block  idea  of 
ResNet significantly speeds up the learning process between input data and output data space, which 
makes it easily extended to very deeper learning layers (could be thousands of layers).

AlexNet: In 2012, Krizhevsky et al.[28] constructed a model with more convolutional layers than 
LeNet for better high-dimensional feature extraction, and then three fully connected layers are used after 
flatten layer to pool channel information. This model employed rectified linear activation (ReLU) as 
activation  function  to  speed  up  training  process,  which  partially  solved  the  vanishing  gradient  and 
exploding problem. Features of each layer in AlexNet were depicted in Figure 2. Feature visualization 
of  AlexNet  layer  by  layer.  The  annotations  above  are  defined  as  Nameùëë@k√ók,  and  the  annotations 
below  are  denominated  in  form  of  Operationùëë@r√ór.  Where d , k and r is  feature  depth,  convolution 
kernel and image resolution, respectively..

InceptionNet: In 2014, Szegedy et al.[29] proposed a new architecture with poly depth pathway, 
which enhances computational resources usage efficiency. The InceptionNet includes four sub-paths for 
convolution and each pathway has different convolutional kernel and operation layers: 1) convolution 
path with kernel size 1√ó1; 2) max-pooling with step 3√ó3 followed by convolution with kernel 1√ó1; 3) 
two convolutions with 1√ó1 and 3√ó3, respectively; 4) two convolutions with kernel size 1√ó1 and 5√ó5. 
The network is constructed over 27 layers with ReLU, which speeds up training time significantly and 
has better batch-normalization.  

3

 
 
 
 IWECAI 2021
Journal of Physics: Conference Series

1873 (2021) 012002

IOP Publishing
doi:10.1088/1742-6596/1873/1/012002

Abstract. Plant identification and classification play a key role in understanding, protecting and 
conserving  biodiversity.  Traditional  plant  taxonomy  needs  long  time  intensive  training  and 
experience, which limited others to identify plant categories. With the development of automated 
image-based  classification,  machine  learning  (ML)  is  becoming  a  popular  tool.  Image 
classification, especially plant images taxonomy, has achieved great improvement in these years 
by  deep  learning  (DL)  methods.  In  this  study,  we  first  reviewed  current  deep  learning 
applications  in  the  field  of  plant  image  classification,  and  then  we  tested  six  deep  learning 
methods  in  four  public  plant  image  datasets.  In  order  to  test  the  classification  power  of  DL 
methods  at  cultivar  level,  we  prepared  a  Camellia  sasanqua  Thunb.  dataset,  which  is  called

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given context, there is no mention or reference to any specific code repository link for the deep learning pipelines discussed in the paper. Therefore, I cannot provide the requested information. It would be best to refer to the original research papers mentioned in the text or contact the authors directly for further details regarding their respective implementations.
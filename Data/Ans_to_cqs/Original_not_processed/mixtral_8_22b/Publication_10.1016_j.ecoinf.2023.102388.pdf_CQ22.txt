Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

• No-overfitting. In this setting, the optimization algorithm of CNNs is 
Stochastic Gradient Descent (SGD), with a learning rate of 0.0001 
and a weight decay of 0.5. The batch size is set to 32, the number of 
training epochs to 200 and finally the batch-norm and dropout (0.5) 
are used to reduce the overfitting level.  

• Overfitting.  We  use  the  same  hyperparameters  setting  as  the  no- 
overfitting  but  we  remove  the  use  of  batch-norm,  weight  decay 
and dropout techniques to ensure that the model overfits.

Strengthening the ability of trained models to generalize effectively 
poses  a  significant  challenge  within  the  domain  of  deep  learning. 
Indeed, generalization refers to the variation in a model’s performance 
when applied to data it has been trained on, as opposed to data it en-
counters for the first time during testing. This issue is of fundamental 
importance and has far-reaching implications for applications employ-
ing  deep  neural  networks.  In  particular,  this  challenge  becomes 

Table 7 
Discrimination  accuracy  (%)  of  ensemble  MIA  and  SNN  for  the  GREMM 
dataset.  

Approach 

Siamese Neural Network (SNN) 
Ensemble MIA 

Discrimination (%) 

65.86 
86.22  

EcologicalInformatics79(2024)10238814V.M. Araújo et al.

0.62 s 
0.66 s 
1.57 s  

2021) 

computer equipped with a NVIDIA TITAN X Pascal GPU featuring 12GB 
of memory. More precisely, the training time corresponds to the time 
required to train the attack model while discrimination time refers to the 
average computational time for a membership inference on a single data 
point. Nonetheless, metric-based MIAs can often achieve a performance 
that is not too far from the best  attack. For instance, on the GREMM 
dataset,  the  attack  performance  of  ResNet50  + LabelOnly  is  0.744 
against  0.695  for  ResNet50  + Yeom.  Therefore,  if  the  adversary  has 
limited  computation  resources,  metric-based  MIAs  may  be  a  more 
appropriate choice than LabelOnly MIA.

To  enhance  the  learning  capacity  of  the  SNN  model,  the  distance 
scores  are  normalized  within  the  range  of  0  to  1.  Subsequently,  a 
contrastive  loss  function  is  applied  to  evaluate  the  loss  and  accuracy 
based on the prediction and ground truth. This ensures that the model’s 
weights are trained in a manner that minimizes the distance between 
two similar images (of the same beluga) while maximizing the distance 
between dissimilar images (of different belugas). Once the model has 
converged  to  an  optimal  point  over  time,  the  generalized  network  is 
subsequently employed to assess the discrimination performance across 
the entire dataset. In Section 5.5, we provide a detailed account of the 
implementation and the result of the evaluation. 

4. Experimental setting 

In this section, we present the experimental setting used to validate 

Fig. 3. Ensemble MIA.  

Fig. 4. Siamese network architecture.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention which specific metrics were used to evaluate the performance of the deep learning model. However, it does state that a contrastive loss function was applied to evaluate the loss and accuracy based on the prediction and ground truth. Additionally, the term 'discrimination accuracy' is mentioned when discussing the results of the experiment, suggesting that accuracy might have been one of the metrics used to measure the model's performance. Unfortunately, without further information, I cannot confirm whether other common metrics such as precision or recall were also utilized.
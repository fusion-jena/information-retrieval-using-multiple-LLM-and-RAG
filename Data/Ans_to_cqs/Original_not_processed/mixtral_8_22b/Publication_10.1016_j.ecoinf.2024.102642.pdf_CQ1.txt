Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

directly  to  datasets  where  the  relevance  of  annotations  has  not  been 
evaluated.

During  the  campaign,  methods  are  proposed  to  assess  inter- 
annotator  variability  and  aggregate  annotation  sets.  Those  methods 
will help to define the minimal number of annotators needed [Dubus 
et al., 2023; Walter et al., 2022]. Increasing the number of annotators 
per sample could be helpful if the variability is too high. Finally, the two 
grouping  methods:  majority  voting  (Section  2.2.3)  and  soft  labeling 
(Section 2.3.2) can also be used to produce relevant annotation sets for 
deep learning. 

6. Conclusion 

In this study, new approaches were explored to improve the accuracy 
and  efficiency  of  cetacean  vocalization  detection  using  deep  learning 
models. Leveraging multi-annotation campaigns involving both expert 
and novice annotators aimed to address the challenges associated with 
the scarcity of ground truth data in underwater PAM studies.

In this kind of project, volunteer citizen scientists give free time to 
explore  and  annotate  large  datasets.  Contact  with  the  annotator  is 
extremely important: (i) proposing a clear explanation about the aim of 
the  annotation  campaign,  (ii)  answering  questions,  (iii)  organizing 
meetings throughout the campaign and share a report after. Those steps 
motivate the annotators and increase the educational potential of the 
campaign.

To  evaluate  the  performance  of  the  CNNs  trained  on  different 
annotation sets, datasets containing other SEIO PBW vocalizations and 
blue whale’s Dcall were chosen. Only a small portion of those datasets 
were used to reduce computational time, as hundreds of networks were 
trained and compared. 

For the evaluation of the SEIO PBW vocalizations, 7 h of audio sig-
nals recorded during the OHASISBIO program were used [Royer, 2009; 
Torterotot et al., 2020]. These recordings were collected at the SWAMS 
site, located in the Indian Ocean’s oceanic zone, between Kerguelen and 
Amsterdam Island. This dataset is called SWAMS in this paper. The re-
cordings were made in March 2015 using a hydrophone deployed at a 
depth of 1000 m. The sampling rate was 240 Hz. A total of 102 vocali-
zations were manually annotated by a second expert.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information on the specific methods used for collecting raw data in the deep learning pipeline. However, it mentions that the study leverages multi-annotation campaigns involving both expert and novice annotators to address the challenges associated with the scarcity of ground truth data in underwater Passive Acoustic Monitoring (PAM) studies. It also refers to the use of audio signals recorded during the OHASISBIO program, which were collected at the SWAMS site using a hydrophone deployed at a depth of 1000 meters. Therefore, we can infer that one method used for collecting raw data involves deploying hydrophones in underwater environments to capture audio signals. Additionally, the involvement of multiple annotators suggests that human input plays a significant role in the data collection process.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

different VGG nets, such as VGG-11, VGG-16, and VGG-19. VGG-16 has 
been applied in many researches and it is often used to extract features in 
the age estimation tasks Pan et al. (2018). In this study, we replaced the 
last three Fully-Connected (FC) layers of the original VGG model with 
one FC layer to reduce parameters. And we adopt Batch Normalization 
(BN) (Ioffe and Szegedy, 2015) to accelerate the training of networks. 
For this study, we selected VGG-11-BN and VGG-16-BN to extract the 
features from panda facial images. 

2.2.2. ResNet

Hu, J., 1987. A study on the age and population composition of the giant panda by 

judging droppings in the wild. ACTA Theriol. Sinica 7 (2), 81–84. 

Ioffe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep network training by 
reducing internal covariate shift. In: International Conference on Machine Learning. 
PMLR, pp. 448–456. 

Ito, H., Udono, T., Hirata, S., Inoue-Murayama, M., 2018. Estimation of chimpanzee age 
based on dna methylation. Sci. Rep. 8 (1), 1–5. https://doi.org/10.1038/s41598- 
018-28318-9. 

Jalal, A., Salman, A., Mian, A., Shortis, M., Shafait, F., 2020. Fish detection and species 
classification in underwater environments using deep learning with temporal 
information. Ecol. Inform. 57, 101088 https://doi.org/10.1016/j. 
ecoinf.2020.101088. 

Jarman, S.N., Polanowski, A.M., Faux, C.E., Robbins, J., De Paoli-Iseppi, R.,

We  used  Pytorch  to  build  our  deep  neural  network,  which  was 
trained on Nvidia V100 GPU. In order to accelerate the convergence of 
the model, our model was pre-trained on the ImageNet dataset (Deng 
et  al., 2009) and then independently  fine-tuned on our  PAD Full and 
PAD Lite datasets. However, some individuals cross multiple age groups. 
Thus, to eliminate the influence of individuals, we applied a five-fold 
subject-exclusive  (SE)  (Han  et  al.,  2014;  Pan  et  al.,  2018)  which 
ensured the same subject did not appear in the training set and testing 
set at the same time. We downscaled the resolution of all images to 224 
× 224 pixels before feeding them to seven models. To avoid overfitting 
and imbalanced problem, we applied aggressive data augmentation. Our 
original images were captured under different illumination and angles. 
Thus, we augmented all training sets randomly via horizontal flipping,

In  addition  to  calculating  MAE  values,  we  also  compared  the 
inference-time to determine the most computationally efficient model. 
We set the batch to 400 to achieve 100% GPU utilization of each model 
and to measure the maximum queries per second (QPS) achieved under 
real-world conditions (Ding et al., 2021). All models were tested under 
the same experiment setting and the same GPU to make comparisons 
between them fair. To do so, we first expand the dataset by a factor of 
five (from 8002 to 40,010 images). Then, we fed batches of 50 images 
into the model to warm up the hardware, before recording the inference- 
time of batches of 100 to quantify QPS. 

pi,j =

(cid:0)
exp

)

zi,j
(cid:0)

exp

zi,m

∑M

m=1

)

(1)  

3. Results

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To ensure the generalizability of their deep learning model, several measures were implemented. Firstly, the model was trained on the diverse ImageNet dataset (Deng et al., 2009) and subsequently fine-tuned on two separate datasets, namely PAD Full and PAD Lite. This approach allowed for the model to learn from a wide range of image categories and adapt its knowledge to more specific tasks. Secondly, to address the issue of subjects appearing across multiple age groups, a five-fold subject-exclusive (SE) method was employed (Han et al., 2014; Pan et al., 2018). This ensured that the same individual did not occur simultaneously in both the training and testing sets, thereby preventing any potential bias or overfitting due to repeated exposure to certain subjects. Lastly, aggressive data augmentation techniques were utilized to counteract overfitting and class imbalance problems. These included random horizontal flipping and alterations to lighting and angle during image capture. By diversifying the input data through these methods, the robustness and applicability of the model could be improved significantly.
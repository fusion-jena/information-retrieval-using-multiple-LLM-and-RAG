Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Beltagy, I., Lo, K., Cohan, A., 2019. SciBERT: a pretrained language model for scientific 
text. In: Proceedings of the 2019 Conference on Empirical Methods in Natural 
Language Processing and the 9th International Joint Conference on Natural 
Language Processing (EMNLP-IJCNLP), pp. 3615–3620. 

Benıtez-L´opez, A., Santini, L., Schipper, A.M., Busana, M., Huijbregts, M.A., 2019. Intact 
but empty forests? Patterns of hunting-induced mammal defaunation in the tropics. 
PLoS Biol. 17 (5), e3000247. 

Blasi, D., Anastasopoulos, A., Neubig, G., 2022. Systematic inequalities in language 
technology performance across the world’s languages. In: Proceedings of the 60th 
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 
pp. 5486–5505. https://doi.org/10.18653/v1/2022.acl-long.376. 

Bojanowski, P., Grave, E., Joulin, A., Mikolov, T., 2017. Enriching word vectors with 

subword information. Trans. Assoc. Comput. Linguist. 5, 135–146.

Expanding digital data sources, including social media, online news articles and blogs, provide an opportunity to 
understand better the context and intensity of human-nature interactions, such as wildlife exploitation. However, 
online searches encompassing large taxonomic groups can generate vast datasets, which can be overwhelming to 
filter for relevant content without the use of automated tools. The variety of machine learning models available 
to  researchers,  and  the  need  for  manually  labelled  training  data  with  an  even  balance  of  labels,  can  make 
applying  these  tools  challenging.  Here,  we  implement  and  evaluate  a  hierarchical  text  classification  pipeline 
which brings together three binary classification tasks with increasingly specific relevancy criteria. Crucially, the 
hierarchical approach facilitates the filtering and structuring of a large dataset, of which relevant sources make

Settles, B., Craven, M., 2008. An analysis of active learning strategies for sequence 
labeling tasks. In: Proceedings of the 2008 Conference on Empirical Methods in 
Natural Language Processing, pp. 1070–1079. 

Seymour, E., Damle, R., Sette, A., Peters, B., 2011. Cost sensitive hierarchical document 

classification to triage PubMed abstracts for manual curation. BMC Bioinform. 12 
(1), 1–11. 

Shinyama, Y., 2020. Pdfminer. https://pdfminersix.readthedocs.io/en/latest/. 
Stringham, O., Moncayo, S., Hill, K., Toomes, A., Mitchell, L., Ross, J.V., Cassey, P., 

2021a. Text classification to streamline online wildlife trade analyses. PLoS One 16 
(7) e0254007.584.  

Stringham, O., Toomes, A., Kanishka, A., Mitchell, L., Heinrich, S., Ross, J., Cassey, P., 
2021b. A guide to using the internet to monitor and quantify the wildlife trade. 
Conserv. Biol. 35 (4), 1130–1139.

ventional  ‘flat’  classification  has  been  previously  demonstrated  as  a 
useful  tool  for  filtering  standardised  datasets,  such  as  academic  ab-
stracts,  data  collection  from  online  platforms  often  generates  large 
volumes of highly variable textual data which, as demonstrated by our 
study,  can  necessitate  using  a  hierarchical  classification  approach. 
Although we focused on collecting social media posts and web articles, 
the models and pipeline developed in this study can be applied to other 
forms of textual data, such as listings from E-commerce platforms.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not contain any direct reference or link to a code repository containing the deep learning pipeline used in the research. Therefore, it is not possible to determine the exact code repository link based solely on the given information. It would require further investigation or additional resources to locate the desired code repository.
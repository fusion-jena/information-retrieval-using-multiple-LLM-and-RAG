Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics77(2023)10227814K. Bjerge et al.                                                                                                                                                                                                                                   

Appendix B. Evaluation of the alpha parameter 

We trained several models on the insect TL (TLm) benchmark dataset to optimise the hyper-parameter α, with α ∈ [1.0⋅10

(cid:0) 6, 1.0] achieving high 

accuracy, as shown in Fig. B.4. Since we only observed marginal changes in accuracy, for all levels, we chose α = 0.5 in our study. 

Appendix C. The training of the models

hierarchical classification of protein families. PLoS ONE 16 (10 October 2021). 
https://doi.org/10.1371/journal.pone.0258625. 

Silla, C.N., Freitas, A.A., 2011. A survey of hierarchical classification across different 
application domains. Data Min. Knowl. Disc. 22 (1–2) https://doi.org/10.1007/ 
s10618-010-0175-9. 

Smith, L.N., 2018. A disciplined approach to neural network hyper-parameters: Part 1 – 
Learning rate, batch size, momentum, and weight decay. arXiv 2018:1–21URL: 
http://arxiv.org/abs/1803.09820. arXiv:1803.09820. 

Tan, M., Le, Q.V., 2019. EfficientNet: Rethinking model scaling for convolutional neural 
networks. In: 36th International Conference on Machine Learning, ICML 2019. 
volume 97; 2019. p. 6105–6114. 

Taylor, S., Jaques, N., Nosakhare, E., Sano, A., Picard, R., 2020. Personalized multitask 
learning for predicting tomorrow’s mood, stress, and health. IEEE Trans. Affect. 
Comput. 11 (2) https://doi.org/10.1109/TAFFC.2017.2784832.

The average and standard deviation of the precision, recall and F1- 
score  are  shown  in  Table  4  for  the  ResNet50  and  EfficientNetB3 
models validated  with multitask learning and  pre-trained weights. As 
expected,  we  observe  higher  precision,  recall  and  F1-score  at  higher 
taxonomic  ranks.  Here  we  see  an  increase  in  the  average  F1-score 
ranging from 95.7% (L3) to 98.7% (L2) and 99.1% (L1) at the highest 
ranks  with  ResNet50MTL.  A  resent  study  by  Ong  and  Hamid  (2022) 
designed  to  classify  five  taxa  of  museum  insect  species  achieved  Fl- 
scores  below  90%  with  separate  models  for  each  level  of  the 

hierarchy. However, our dataset is more comprehensive with a lower 
image resolution.

Eq. (4) denotes the predicted class in level l as the maximum output 
value from the network at the level. An input image is also classified as 
‘unsure’ when the predicted class is not correct according to the higher 
rank in the taxonomic hierarchy as defined in Eq. (7). 

{

̃yl =

Unsure
̃yl

̃yl⇏̃yl(cid:0) 1

if
otherwise

or Rlj < th

(7)  

3.3. Training, augmentation and optimizers 

The training on the datasets was  performed using data augmenta-
tion, including image scaling, horizontal and vertical flip, perspective 
distortion and adding color jitter for brightness, contrast and saturation. 
Data augmentation mitigates overfitting by increasing the diversity of 
the training data. We selected a batch size of 20 for training our models 
since it is faster to update, and results in less noise, than smaller batch 
sizes. The accuracy of the models on the training and validation datasets 
was computed after each epoch. 

The Adam optimizer with a fixed learning rate of 1.0⋅10

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
According to the given context, the hyperparameter α was optimised by training several models on the insect TL (TLm) benchmark dataset with α ∈ [1.0⋅10^(-6), 1.0]. It is mentioned that this range of values for α achieved high accuracy, as shown in Figure B.4. However, since there were only marginal changes in accuracy observed for all levels, the value of α = 0.5 was chosen for the study. Unfortunately, the context does not provide specific information about the method used for optimizing the hyperparameters, such as whether they used grid search or random search. Therefore, based on the available context, it can be said that the hyperparameter α was optimised by training multiple models with varying values of α within a certain range, but the exact optimization method remains unknown.
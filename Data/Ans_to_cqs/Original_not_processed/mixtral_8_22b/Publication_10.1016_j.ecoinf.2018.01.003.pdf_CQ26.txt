Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(Zhang and Benveniste, 1992), Bayesian learning machine (Xu, 1997),
and so on. Researchers have extended the application and improve the
algorithms based on classical neural network models afterwards. Since
2006 deep learning started to boom in academia and industry. The deep
learning algorithm broke the limitation of the traditional neural net-
work on the number of layers. According to demands of designers, the
number of network layers was chosen and trained by large-scale data to
obtain deeply hidden characteristic information and features which is
beyond imagination before (Hinton and Salakhutdinov, 2006).

+

θ

l
i

⎧
⎪
⎨
⎪
⎩

i
(

=

1, 2, 3,

…

L

)

2.4. Data selection

(4)

l is the output of the ith node in the lth layer, xi

l is the acti-
where yi
l is the weight of the
vation value of the ith node in the lth layer, wij
connection between the jth node in the l-1th layer and the ith node in
the lth layer, θi
l is the threshold value of the ith node in the lth layer, Nl
is the number of nodes in the lth layer, L is the total number of layers,
and f() is the neuronal activation function.

Because the output error of the neurons in the output layer does not
satisfy the accuracy requirement, the learning process turns into a
backward error propagation process, and the basic model is as follows:

∆ = −
w
ij

η

∂
E
w
∂
ij

(5)

35

Recogn. Lett. 18, 375–390.

Zhang, G.P., 2003. Time series forecasting using a hybrid arima and neural network

model. Neurocomputing 50 (1), 159–175.

Zhang, Q., Benveniste, A., 1992. Wavelet networks. IEEE Trans. Neural Netw. 3 (6), 889.
Zhang, Y., Wu, L., 2008. Weights Optimization of Neural Network via Improved BCO

Approach. vol. 83. pp. 185–198.

Zhu, J., Yuping, L.I., Cai, Y., 2011. Analysis and prediction of ecological footprint by gray

forecasting model for Hebei Province. Journal of Arid Land Resources and
Environment. 25 (2), 24–28.

42

Netherlands. Ecol. Econ. 34 (1), 115–130.

Vapnik, V., 1995. The Nature of Statistical Learning Theory. vol. 314 Information Science

Leng, X., Wang, J., Ji, H., Wang, Q., Li, H., Xin, Q., et al., 2017. Prediction of size-frac-

and Statistics, New York, NY (USA).

tionated airborne particle-bound metals using mlr, bp-ann and svm analyses.
Chemosphere 180, 513.

Varﬁs, A., Versino, C., 1990. Univariate Economic Time Series Forecasting by

Connectionist Methods.

Li, X.M., Xiao, R.B., Yuan, S.H., Chen, J.A., Zhou, J.X., 2010. Urban total ecological

Wackernagel, M., Yount, J.D., 2000. Footprint for sustainability: the next step. Environ.

footprint forecasting by using radial basis function neural network: a case study of
Wuhan City, China. Ecol. Indic. 10 (2), 241–248.

Li, X.B., Tian, M.R., Wang, H., et al., 2014. Development of an ecological security eva-
luation method based on the ecological footprint and application to a typical steppe
region in China. Ecol. Indic. 39, 153–159.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text snippets do not provide explicit details about specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, some general strategies can be employed to mitigate data bias issues. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into training and testing sets while maintaining the same distribution of classes or labels within each set. This ensures that both sets contain representative samples from all categories, reducing potential biases caused by imbalanced class distributions.

Oversampling and undersampling are techniques used to handle class imbalance problems. Oversampling increases the frequency of minority class examples by duplicating them or creating synthetic instances. Undersampling reduces the majority class examples either randomly or through more sophisticated methods like NearMiss or Condensed Nearest Neighbor (CNN). Both approaches aim to achieve balanced datasets, improving the performance of deep learning algorithms on underrepresented classes.

Diverse data collection focuses on gathering a wide range of data sources and types to ensure that the resulting dataset accurately reflects real-world scenarios. By incorporating various perspectives and avoiding overreliance on any single source, this strategy helps prevent biased representations and promotes fairness in deep learning applications.
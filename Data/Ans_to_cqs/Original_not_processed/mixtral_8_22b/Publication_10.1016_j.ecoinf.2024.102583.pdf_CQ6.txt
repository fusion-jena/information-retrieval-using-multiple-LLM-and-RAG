Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics81(2024)1025832G. Agboola et al.                                                                                                                                                                                                                                

Fig. 2. Research workflow  

Pourghasemi, 2021), and the Support Vector Machine (SVM) models (F. 
Huang et al., 2022; Y. Huang and Zhao, 2018; Shahzad et al., 2022; Xing 
et al., 2021). These models are flexible and adaptable systems that can 
manage diverse types of data and scenarios for LSM. Another type of 
MLM that has been explored for LSM is the Artificial Neural Network 
(ANN), especially in the Radial Basis Function (RBF) (W. Chen et al., 
2018; Ngo et al., 2021) and Multi-Layer Perceptron (MLP) configuration 
(Adnan  et  al.,  2020;  Chang  et  al.,  2022;  Z.  Zhao  et  al.,  2022).  These 
models can learn from data and adjust their parameters to improve their 
performance.

loss function, effectively preventing overfitting and promoting simpler 
models,  utilization  of  a  more  advanced  optimization  approach  that 
combines  first-order  gradients  (loss  function  gradient)  with  second- 
order  gradients  (loss  function  curvature)  which  makes  it  faster  than 
some other models, ability to handle missing data during tree building 
by  employing  weighted  quantile  sketch,  built-in  cross-validation  sup-
port  that  aids  the  model  evaluation  and  hyperparameter  tuning,  and 
imbalanced  data  handling.  Thanks  to  its  exceptional  efficiency  and 
performance and its availability in various programming languages like 
Python, R, and Java, XGBoost has found widespread adoption in land-
slide risk assessment (Akinci et al., 2021; Badola et al., 2023; Can et al., 
2021; Hussain et al., 2022b). 

{(cid:0)

Given  a 
)}(cid:0)
xi, yi

set  with  n 
|D|= n, xi ∈ Rm, yi ∈ R

)

(cid:0)

Ω

fj

= τT +

1
2

λ||w||2

(9)  

2.2.6. Heterogenous ensemble techniques 

Heterogeneous ensemble learning is the practice of combining pre-
dictions of multiple models that are different in terms of their underlying 
algorithms  or  architectures  to  improve  overall  performance.  Utilizing 
heterogeneous ensemble techniques can address biases and improve the 
implemented  two 
effectiveness  of  models. 

In  our  study,  we 

EcologicalInformatics81(2024)10258312G. Agboola et al.                                                                                                                                                                                                                                

Fig. 11. The proportion of area covered by each susceptibility category in each scenario.  

heterogeneous  techniques  to  optimize  the  performance  of  five  base 
learners and mitigate potential overfitting:

FR = F/E 

Soil 

Distance to 
stream 

Land Cover 

Lithology 

Distance to road 

Inceptisols 
Ultisols 
Bodies of Water 
Alfisols 
Entisols 

0–50 
50–100 
100–150 
150–250 
250–360 
Forest 
Grassland 
Wetland 
Cropland 
Barrenland 
Urban 
Water body 
Mica-schist 
Henderson-Gneiss 
Garnet-mica-schist 
Biotite-gneiss and schist 
Migmatitic-granitic-gneiss 
Amphibolite and biotite- 
gneiss 
Granite-gneiss 
Porphyroblastic-gneiss 
Caesars-Head Granitic-Gniess 
<200 
200–400 
400–600 
600–800 
>800 

124,464 
438,856 
6717 
343 
11,028 

305,134 
190,058 
71,058 
14,915 
243 
439,207 
10,914 
815 
50,952 
447 
73,274 
5799 
9949 
104,512 
44,510 
178,276 
70,962 

11,509 
39,400 
116,161 
6129 
1,60,092 
104,592 
72,603 
51,114 
193,007 
581,408    

357 
809 
0 
0 
49 

631 
419 
129 
36 
0 
1072 
20 
0 
0 
1 
122 
0 
11 
123 
11 
510 
376 

6 
13 
165 
0 
302 
148 
107 
81 
577 

21% 
75% 
1% 
0.1% 
2%

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps may be required depending on the nature of the dataset. Some common preprocessing steps include:

* Data Cleaning: This involves removing any irrelevant, redundant, or erroneous data points from the dataset. It also includes handling missing values, outliers, and noise.
* Normalization: This step scales the input features so that they have similar ranges, typically between 0 and 1 or -1 and 1. This helps prevent certain features from dominating others due to differences in scale.
* Standardization: Similar to normalization, standardization scales the input features but uses the mean and standard deviation instead of the minimum and maximum values. This results in a distribution with zero mean and unit variance.
* Encoding Categorical Variables: Deep learning models require numerical inputs, so categorical variables need to be converted into numerical form using encoding methods such as one-hot encoding or label encoding.
* Feature Selection: Selecting relevant features can help reduce dimensionality, improve computational efficiency, and avoid overfitting. Techniques such as correlation analysis, mutual information, or recursive feature elimination can be used for this purpose.
* Splitting Dataset: Finally, the dataset should be split into training, validation, and testing sets. Typically, around 70-80% of the data is allocated for training, while the remaining 20-30% is divided equally between validation and testing.
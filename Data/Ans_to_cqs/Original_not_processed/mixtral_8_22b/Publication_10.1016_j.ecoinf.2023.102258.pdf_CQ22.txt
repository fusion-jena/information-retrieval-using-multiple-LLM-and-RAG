Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4 
2 

Evaluation (95% CI) 

61.83 (61.23–62.32) 
60.22 (59.66–60.70) 
48.52 (48.18–48.85) 
47.97 (47.48–48.40) 
44.98 (44.44–45.42) 
44.26 (43.85–44.62) 
40.93 (40.48–41.30) 
37.71 (36.98–38.23) 
37.32 (36.82–37.74) 
33.24 (32.69–33.69) 
26.79 (26.13–27.29) 
25.66 (25.40–25.91) 
23.74 (23.14–24.17) 
21.67 (21.32–21.97) 
19.20 (18.88–19.51) 
18.29 (18.01–18.56) 
17.22 (16.82–17.55) 
15.49 (15.16–15.77) 
13.31 (12.83–13.67) 
12.35 (11.52–12.75) 
5.3 (5.1–5.2) 
4.34 (3.74–4.56) 
2.82 (2.76–2.87) 

Validation 

75.6 
74.4 
50.03 
60 
61.76 
64.77 
53.88 
63.94 
63.46 
57.2 
36.6 
43.89 
81.52 
47.94 
51.99 
54.63 
54.53 
47.88 
62.64 
3.37 
28.45 
44.17 
–  

incorporated this frame-level embedding idea into a multi-task learning 
framework, that also includes a speaker voice activity detection branch. 
This modifications are responsible for a score improvement of almost 2% 
points.

The next system in rank, Liu_Surrey, implements a novel approach 
designed to optimise the contrast between positive events and negative 
prototypes. This, together with an adaptive segment length dependent 
on each target class, works well across all the evaluation sets. 

The problem of very different lengths of events across target classes 
was also directly addressed by other submissions. Both Martinsson_RISE 
and Zgorzynski_SRPOL implemented an ensemble approach where each 
individual model focuses on a different input size range. In Liu_BIT-SRCB 
this is explored through a multi-scale ResNet, and in Willbo_RISE with a 
wide ResNet containing many channels. Also in XuQianHu_NUDT_BIT, 
they implement a novel adaptive mechanism - squeeze/excitation block 
- designed to assign different weights to different channels of the feature 
map.

Then, for each sample xq  from the query set, a distance function is 
used  to  calculate  the  Euclidean  distance  of  xq  from  each  prototype, 
following which a softmax function over the distances produces a dis-
tribution over the classes. This directly implies that training the neural 
network to optimise these distances should move prototypes and their 
corresponding  query  points  closer  together  in  the  embedding  space 
created  by  fϕ,  and  further  away  from  non-matching  points.  In  other 
words, the training procedure creates a general representation in which 
similar  sounds  are  close  to  each  other.  Nearest-neighbour  algorithms 
such as k-means can then be used to label future data points—even those 
from novel categories, after a simple procedure of calculating the pro-
totype of a novel category as the centroid of its k shots.

in  few-shot  learning,  nor  by  pretrained  networks;  yet  we  report  very 
strong progress through the public challenges. We also consider that our 
chosen evaluation measure—an event-based F-score—has good external 
validity,  since  it  aligns  well  with  expert  evaluations  of  automatic 
transcripts.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The performance of the deep learning models mentioned in the given context is evaluated using the Event-Based F-Score metric. This metric is considered to have good external validity because it aligns well with expert evaluations of automatic transcripts. However, there is no mention of other common evaluation metrics such as accuracy, precision, or recall being used in this specific case.
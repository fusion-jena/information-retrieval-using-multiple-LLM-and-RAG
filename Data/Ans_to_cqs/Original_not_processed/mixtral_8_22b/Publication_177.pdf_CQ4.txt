Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

[32]  A. Khan, A. Sohail, U. Zahoora, A.S. Qureshi, A survey of the recent 
architectures of deep convolutional neural networks, Artif. Intell. Rev. 
(2020) 1–70. https://doi.org/10.1007/s10462-020-09825-6. 

[33]  H.  Chongomweru,  A.  Kasem,  A  novel  ensemble  method  for 
classification  in  imbalanced  datasets  using  split  balancing  technique 
based on instance hardness  (sBal_IH),  Neural Comput.  Appl.  (2021) 
1–22. https://doi.org/10.1007/s00521-020-05570-7. 

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on September 08,2024 at 16:24:56 UTC from IEEE Xplore.  Restrictions apply.

TABLE I.  

PERFORMANCE METRICS FOR UBD_45 DATASET 

Model 

Performance Metrics 

Sensitivity 

Specificity 

Precision 

F1-Score 

InceptionResnet2 

95.93 

Resnet50 

Xception 

Inceptionv3 

Mobilenetv2 

Googlenet 

95.93 

94.44 

91.85 

92.04 

85.00 

99.92 

99.90 

99.87 

99.85 

99.83 

99.62 

97.65 

97.20 

95.90 

94.73 

95.16 

86.15 

96.78 

96.56 

95.16 

93.27 

93.57 

85.57 

TABLE II.  

PERFORMANCE METRICS FOR VP_200 DATASET 

Model 

Performance Metrics 

Sensitivity 

Specificity 

Precision 

F1-Score 

InceptionResnet2 

91.20 

Resnet50 

Xception 

Inceptionv3 

Mobilenetv2 

Googlenet 

88.68 

91.80 

88.65 

85.58 

83.28 

99.96 

99.94 

99.96 

99.94 

99.93 

99.92 

91.92 

89.46 

92.83 

90.47 

87.20 

84.90 

91.56 

89.07 

92.31 

89.55 

86.38 

84.08

stacking  ensemble 

improved 

Keywords—deep 

learning,  computer 
learning,  ensemble 
vision, plant species identification, convolutional neural networks 

I. 

INTRODUCTION

The  selected  network  models  were  initialized  with  pre-
trained ImageNet weights and then fine-tuned to our datasets. 
The performance of a deep neural network for a dataset highly 
depends on network hyperparameters. The selection and fine-
tuning of  optimized hyperparameters  is  generally  a  difficult 
and  time-consuming  task.  Instead  of  manually  selecting  the 
hyperparameters, we employed Bayesian optimization to find 
their optimal values for each of the six models.  A Gaussian 
process  model  of  the  objective  function  is  used  by  the 
Bayesian optimization technique.  Different variables can be 
optimized using this technique such as network section depth, 
batch size, initial learning rate, momentum, and regularization 
strength. For this study, we optimized the network for batch 
size  (between  1  and  32)  and  initial  learning  rate  (between 
1×10-4  and  1×10-2).  The  optimization  was  performed  by

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't any information about the specific data augmentation techniques applied in the deep learning pipeline within the given context. Therefore, it is not possible to provide a helpful answer based solely on the provided text. Please refer to other sources or additional context to determine which data augmentation techniques were utilized.
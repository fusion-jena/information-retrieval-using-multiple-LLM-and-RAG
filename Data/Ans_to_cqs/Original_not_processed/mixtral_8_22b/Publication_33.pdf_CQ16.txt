Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

training datasets.

•  In the first phase, a classification model is built by training a CNN on a given database T0 (Fig. 2a)
•  Then, the second phase consists of tuning a risk threshold τi specific to each class (i.e. each species in our 

case), noted i, with i ∈ {1, ..., n} , using a second and independent database noted T1 (Fig. 2b).

In terms of classification, it means we transform the 2 classification options (correct, wrong) in 3 options 

(Fig. 3) by applying Eqs. (15, 16).

Computing the confidence thresholds.  After the phase 1 (model training phase), for an image X of the 
threshold tuning dataset processed by the classifier, we obtain an output {C(X), S(X)} , where C(X) is the class 
(i.e. species, belonging to the trained set of species) with the highest classification score S(X). For this image, we 
know the ground truth Y in {1, .., n} belonging to the same set of species classes.

MCi(cid:31)τ ′(cid:30)

(8)

(9)

•  The second goal G2 consists in constraining the misclassification error rate to an upper bound of 5% while 
maximizing the correct classification rate. Reaching this goal requires to first find Seg2 the set of threshold(s) 
such as MCi(τ ) < 5%. If there is none, we considered Seg2 as the set of threshold(s), which minimize MCi . 
Then we defined the optimal threshold τi by choosing the one in Seg2 which maximizes CCi:

Seg2 = τ/MCi(τ ) < 5%

if Seg1 = ∅ then Seg2 = arg min

τ

MCi(τ )

τi = arg max
τ ′inSeg2

CCi(cid:31)τ ′(cid:30)

(10)

(11)

(12)

•  The third goal G3 consists of keeping the lowest misclassification rate while raising the correct classifica-
tion error rate (implying a lower coverage). First, we defined Seg3 as the set of threshold(s) τ that minimizes 
MCi(τ ) . If there were several thresholds with the same minimal value, we chose τi as the one which maximizes 
CCi:

Seg3 = arg min

τ

MCi(τ )

τi = arg max
τ ′inSeg3

0.85

1

0.87

0.75

0.9

0.98

1

0.97

0.8

0.91

0.94

0.98

0.8

0.93

0.07

Table 3.   Accuracy of the model without post-processing, and with post processing according to our goals, on 
the cross-validation, with thresholds tuned on T1 and applied on T2. Each line shows the result for a species, 
with: the species name, the accuracy of the model without post processing, and the accuracy of the model with 
post processing according to the 3 goals defined earlier.

Application of the method.  For a real cross-validation experiment, thresholds were set using T2 and then 
applied on T1. The correct, wrong and unsure classification rates were very close (difference < 2.6%) to those 
obtained with the benchmark situation (Supplementary Table S5).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the criteria used to determine when training is complete. However, based on common practices in machine learning, some possible criteria could be:

1. Validation Loss Plateau: Training can be stopped when the validation loss stops decreasing or starts increasing, indicating overfitting. This is often referred to as early stopping.

2. Maximum Number of Epochs: Training can also be stopped after a certain number of epochs have been completed. An epoch refers to one full pass through the entire training dataset.

3. Time Constraints: Sometimes, training might be stopped due to time constraints, especially if the model takes too long to train.

4. Performance Metrics: Training can be stopped once the desired performance metrics (like accuracy, precision, recall, F1-score etc.) have been achieved.

However, these are speculations based on general practices and may not necessarily apply to the described methodology.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

This   rising   number   of   digitised   herbarium   sheets   provides   an   opportunity   to   employ
computer-based   image   processing   techniques,   such   as   deep   learning,   to   automatically
identify species and higher taxa (Carranza-Rojas et al. 2017, Younis et al. 2018, Carranza-
Rojas   et   al.   2018)   or   to   extract   other   useful   information   from   the   images,   such   as   the
presence   of   pathogens   (as   done   for   live   plant   photos   by   Mohanty   et   al.   2016).   Deep
learning is a subset of machine learning methods for learning data representation. Deep
learning   techniques   require   huge   amounts   of   training   data   to   learn   the   features   and
representation of those data for the specified task by fine tuning parameters of hundreds or
thousands of neural networks, arranged in multiple layers. Learning the value of these
parameters can take vast computer and time resources, especially on huge datasets.

Marco   Schmidt  is   botanist   at   Senckenberg   Biodiversity   and   Climate   Research   Center
(SBIK-F)   with   a   focus   on   African   savannahs   and   biodiversity   informatics   (e.g.   online
databases like African Plants - a photo guide and West African vegetation) and is working
at Palmengarten’s scientific service, curating living collections and collection databases.
Contributions: concept of study, annotation and verification of herbarium scans, preparation
of the manuscript.

Claus   Weiland  is   theoretical   biologist   at   SBIK-F’s   Data   &   Modelling   Centre   with   main
interests in graph neural networks and bio-ontologies. Contributions: Design of the GPU
platform, data analysis and preparation of the manuscript.

Due to the large image size and additional parameters of Faster R-CNN, a minibatch size
of four images per GPU (TITAN Xp) was selected for training the model. The model was
trained   twice,   once   with   a   training   subset   of   498   images   on   a   single   GPU   for   9000
iterations and performance evaluated on the test subset of 155 images, also on a single
GPU   and   then   trained   again   on   all   653   annotated   images   on   three   GPUs   for   18000
iterations   for   predicting   plant   organs   on   another   un-annotated   independent   dataset   to
evaluate   our   method.   This   dataset   consists   of   708   full   scale   herbarium   scans,   with   an
average  size of  ca. 9600  by  6500  pixels,   from  the   Herbarium  Senckenbergianum  (FR)
(Otte et al. 2011) with a different set of species (Fig. 2) and geographical origins, which is
also available at GBIF (Senckenberg 2020). The Python code and the trained model have

Biodiversity Data Journal 8: e57090

doi: 10.3897/BDJ.8.e57090 

Research Article 

Detection and annotation of plant organs from

digitised herbarium scans using deep learning

Sohaib Younis

‡,§

, Marco Schmidt

‡,|

Thomas Hickler

‡

, Claus Weiland , Stefan Dressler , Bernhard Seeger , 

‡

§

¶

‡ Senckenberg Biodiversity and Climate Research Centre (SBiK-F), Frankfurt am Main, Germany
§ Department of Mathematics and Computer Science, Philipps-University Marburg, Marburg, Germany
| Palmengarten der Stadt Frankfurt, Frankfurt am Main, Germany
¶ Senckenberg Research Institute and Natural History Museum, Frankfurt am Main, Germany

Corresponding author: Sohaib Younis (sohaibyounis89@gmail.com) 

Academic editor: Ross Mounce

Received: 30 Jul 2020 | Accepted: 16 Nov 2020 | Published: 10 Dec 2020

Citation: Younis S, Schmidt M, Weiland C, Dressler S, Seeger B, Hickler T (2020) Detection and annotation of

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which platform was used to deploy the deep learning model (e.g., AWS, Azure, Google Cloud platform)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific cloud platform used to deploy the deep learning model. However, it mentions that the model was trained on a GPU platform, specifically using TITAN Xp GPUs. It also states that the model was trained on one or three GPUs depending on the iteration. Therefore, we cannot determine if a cloud platform like AWS, Azure, or Google Cloud Platform was used based solely on this information.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

As  the  third  approach  for  SPAD  value  modelling  based  on  the 
Rasp2SPAD indices, an artificial neural network was implemented. The 
same predictors as in the previously described LM and GLM were uti-
lized. All predictors were scaled prior to analysis. The ANN  was con-
structed empirically, with the first layer containing two input nodes (one 
per each of the two parameters that were found to be the most suitable 
by LM and GLM) and the last layer containing only one node (predicted 
SPAD value). As the fit of the ANN does not give the same output for 
each run  (because of a random number generator used for numerical 
optimization of the model), 100 successful runs were performed, and the 
model residuals obtained in each run were analysed for a more reliable 
evaluation of the model performance.

2.5. Statistical analysis 

As the first descriptive statistics of acquired parameters, a correlation 
plot was constructed for a general overview of the linear relationship of 
all 22 parameters (see Table 3) generated by Rasp2SPAD and the SPAD 

Fig. 4. The proposed Rasp2SPAD prototype in the evaluation procedure on the 
winter rapeseed.

value gained by SPAD-502Plus (Fig. 6). Furthermore, modelling of the 
SPAD  value  was  performed  by  a)  a  simple  linear  model  (LM),  b)  a 
generalized linear model with a gamma distribution (GLM), and c) an 
artificial neural network (ANN). The dataset contained 89 valid records 
(outliers were removed according to the empirical rule), while one re-
cord represented the mean value of the total 10 spot measurements on 
one  leaf.  For  the  purpose  of  SPAD  value  modelling,  the  dataset  was 
divided  into  a  “training  dataset”  containing  60  randomly  selected  re-
cords and a “testing dataset” that contained the remaining 29 records. 
All three models were then fitted using the “training dataset” and further 
validated using the “testing dataset”.

Table 5 
The  best  fitting  generalized  linear  model  (using  the  Gamma  distribution  and 
inverse value as a link function) for SPAD value prediction was based on pa-
rameters obtained by the Rasp2SPAD prototype.  

Fig. 6. Correlation matrix of the actual SPAD value obtained by the commercial 
sensor SPAD-502Plus and the set of the 22 parameters derived by Rasp2SPAD 
based on 89 rapeseed leaf samples. 

(r = 0.86), NDI (r = 0.858), GDR (r = 0.854), and RDB (r = (cid:0) 0.846). 
In the first stage, LM for modelling the SPAD value based on selected 
parameters  showed  relatively  high  accuracy:  R-squared  = 0.81  on 
training data; mean absolute deviance of testing data =1.85, max. Ab-
solute deviance = 2.42 (one-sided t-test, alpha = 0.05). Detailed results 
are given in Table 4. The calibration equation was thus defined as: 

SPAD = 89.538 + 0.475Cb (cid:0) 0.761Cr

(1) 

Response: SPAD Value 

Generalized linear model (Gamma distribution) 

Link function: Inverse value

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the provided context does not explicitly mention how the hyperparameters of the model were optimized. Therefore, it is not possible to provide a definitive answer to this query based solely on the information available. However, common methods for optimizing hyperparameters include techniques such as grid search, random search, Bayesian optimization, or gradient-based optimization. Without additional details about the specific implementation of the artificial neural network mentioned in the text, we cannot determine which method was used for hyperparameter optimization.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

950 (4%) 
1110 (4%) 

Total 

24,675 
32,986  

passes over the entire training set) and a batch size of 64, with an ADAM 
(cid:0) 5. Image augmentation was applied 
optimiser and a learning rate of 10
in the form of horizontal flip, 0.2 degree counter clock wise shear and a 
random zoom between 0 and 0.2 - all leading to 224 × 244 pixel RGB 
input tensors. Data was normalised to ImageNet mean values, and the 
pixels values were rescaled in the range of [0, 1]. Model training took 
roughly 4 days. The best model was selected based on minimal valida-
tion loss that occurred at epoch 448. This model showed a training loss 
of 0.256, a training accuracy of 0.899, a validation loss of 0.298 and 
validation accuracy of 0.891. We evaluated the red kite model perfor-
mance based on an independent test set of 2060 images (as described in 
3.3). 950 of these images were true positive red kites images and the

Birds 66 (183–224), 241–269. 

Di Minin, E., Fink, C., Hiippala, T., Tenkanen, H., 2019. A framework for investigating 
illegal wildlife trade on social media with machine learning. Conserv. Biol. 33 (1), 
210–213. https://doi.org/10.1111/cobi.13104. 

eBird., 2021. Ebird: An Online Database of Bird Distribution and Abundance. https:// 

media.ebird.org/catalog?taxonCode=redkit1%5C&mediaType=photo. 

Edwards, T., Jones, C.B., Corcoran, P., 2022. Identifying wildlife observations on twitter. 

Ecol. Inform. 67, 101500 https://doi.org/10.1016/j.ecoinf.2021.101500. 

ElQadi, M.M., Dorin, A., Dyer, A., Burd, M., Bukovac, Z., Shrestha, M., 2017. Mapping 
species distributions with social media geo-tagged images: case studies of bees and 
flowering plants in Australia. Ecol. Inform. 39, 23–31. https://doi.org/10.1016/j. 
ecoinf.2017.02.006.

5. Discussion 

In this study we developed a workflow which leveraged citizen sci-
ence data to extract further relevant records from social media posts in 
the  same  region.  The  workflow  functions  as  a  data  filter  enabling 
downsampling of an initially very large dataset into a human analysable 
subset - in our case containing 0.5% of the original posts. By massively 
reducing  data  volumes,  it  becomes  realistic  to  analyse  the  remaining 
data by hand to select true positives, with around one hour required for 
the  4000  or  so  candidate  posts  we  identified.  Our  workflow  thus  ad-
dresses  the  research  gap  identified  by  Burke  et  al.,  2022,  using  gen-
eralisable  methods  to  extract  target  data  from  various,  unverified 
sources to enrich data.

Since our workflow is designed to be generic, take advantage of the 
text and image data and combine records from citizen science reports 
with social media data, it uses a combination of a simple rule based- 
approach, existing pre-trained models and a model trained specifically 
for our target species (Fig. 2). Our approach is designed to take advan-
tage  of  what  we  assume  to  be  high  quality  data  collected  by  citizen 
scientists with an interest in ornithology, use off-the-shelf models where 
possible, and reduce the initial number of social media posts in a given 
region to a manageable size for manual verification. Our workflow thus: 

EcologicalInformatics71(2022)1017823M.C. Hartmann et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the deployment process of the trained deep learning model. However, some details can be inferred. The model was trained using TensorFlow, an open-source platform for machine learning, and Keras, a user-friendly neural network library built on top of TensorFlow. After training, the best model was selected based on minimum validation loss, occurring at epoch 448. This suggests that the model was saved during training, possibly through checkpointing or callbacks, common practices in TensorFlow and Keras. Unfortunately, there are no specifics regarding model serialization or platform selection for deployment.
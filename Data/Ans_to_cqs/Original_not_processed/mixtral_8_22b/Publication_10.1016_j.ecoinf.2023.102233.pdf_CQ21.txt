Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Active  learning  iterations  used  a  batch  size  of  64,  ten  epochs  and 
learning rate of 0.001. We used a grid search technique (Mohri et al., 
2018) to tune hyperparameters of the final model including the number 
of epochs, batch size and learning rate. 

2.3.3. Active learning framework 

We  applied  an  active  learning  approach  to  iteratively  train  and 
improve  the  CNN  model.  The  active  learning  approach  is  depicted 
within Fig. 3 and described below.

logit; (Monarch, 2021; Roh et al., 2019). Through this active learning 
driven annotation process we grew the number of audio frames in the 
training data to 1073 audio frames containing target calls and 5786 non- 
target  audio  frames  within  five  iterations  (Table  1).  All  labels  were 
reviewed by a primary annotator (J.V.O.) with 5 years’ experience with 
SBTF prior to the next iteration of model training.

An active learning strategy requires consideration of biases that may 
be  introduced  during  annotation  (Monarch,  2021).  While  annotation 
bias is a consideration for all ML datasets, the active learning framework 
allows ML models to be generated from smaller datasets and potentially 
developed  by  fewer  annotators  compared  to,  for  example,  a  publicly 
available  call  database  such  as  Xeno-canto  (Vellinga  and  Planqu´e, 
2015). In our study, the final model’s predictions most closely reflected 
the primary annotator’s labels, compared to the expert’s labels, which 
suggests that the model is replicating biases of the training data (Koe-
necke et al., 2020; Roh et al., 2019). Performance improvements may 
therefore  be  achieved  through  the  inclusion  of  labelled  training  data

Sankupellay, M., Konovalov, D., 2018. Bird call recognition using deep convolutional 

neural network, ResNet-50. In: Proceedings of Acoustics. 

Sekercioglu, C.H., et al., 2008. Climate change, elevational range shifts, and bird 

extinctions. Conserv. Biol. 22 (1), 140–150. 

Settles, B., Craven, M., 2008. An analysis of active learning strategies for sequence 

labeling tasks. In: Proceedings of the 2008 Conference on Empirical Methods in 
Natural Language Processing, pp. 1070–1079. 

Teixeira, D., et al., 2022. Fledge or fail: Nest monitoring of endangered black-cockatoos 
using bioacoustics and open-source call recognition. Ecol. Inform. 69, 101656. 
Thakur, A., Thapar, D., Rajan, P., Nigam, A., 2019. Deep metric learning for bioacoustic 

classification: Overcoming training data scarcity using dynamic triplet loss. 
J. Acoust. Soc. Am. 146 (1), 534–547. 

Threatened Species Scientific Committee (TSSC), 2005. Commonwealth Listing Advice

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about the specific postprocessing steps involved after the model training. However, based on common practices in machine learning, some possible postprocessing steps could include calculating metrics like accuracy, precision, recall, and F1 score to evaluate the performance of the trained model. Additionally, creating confusion matrices can help visualize the performance of the classifier and identify any potential issues with imbalanced classes. Other techniques like saliency maps might also be employed to understand how the model makes its decisions and what features it focuses on when making predictions. Nevertheless, without further details regarding the specific implementation, these suggestions remain speculative.
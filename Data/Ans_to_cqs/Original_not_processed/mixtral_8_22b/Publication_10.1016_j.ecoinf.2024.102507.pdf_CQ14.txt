Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

6.  Prediction:  Use  the  data  that  the  model  has  not  seen  and  use  the 
aforementioned model and weight calculation to predict the answer. 

In order to better understand the environment and materials used in 
the study, the software, hardware, and  a series of  operations on data 
were introduced. In terms of execution environment, this study used an 
Intel Core i5-10400F CPU and a 64-GB memory. The GPU specification 
was ZOTAC GAMING GeForce RTX 3090 (24 GB memory). The execu-
tion environment was  the Windows 10 system version and the 64-bit 
version of Python 3.6.6. The GPU environment included CUDA Toolkit 
11.3, cuDNN v7.2.1, TensorFlow-gpu 2.5, and so on.

the 

Dropout(Dropout_Rate),  Dense(256,  activation=’relu’),  Dropout 
(Dropout_Rate),  and  finally,  one  Dense(8,  activation=’softmax’) 
layer. 

For VGG19, ResNet50, or a combination of VGG19 and ResNet50, 
we configured the models with Input(shape = (64, 64, band_num)), 
set weights to None, and specified ‘include top’ as False. To create a 
combination model (VGG19 + ResNet50), we used Concatenate() to 
merge the features from VGG19 and ResNet50. The results (Tables 7, 
8,  and  9)  below  demonstrate  the  effects  of  various  parameter 

Fig. 6. RGB visual thumbnail of the training sample of Cryptomeria-like Tai-
wania afforestation (TYPE1). 

input  at  once,  but  it  will  substantially  increase  the  load  on  the 
memory and GPU during the operation.

0.5 

0.39 
0.25 
0.43 
0.13 
0.54 
0.21 
0.49 
0.24 
0.50 
0.27  

Table 8 
Results  of  calculations  with  different  parameters  (conv_base.trainable  and 
Dropout Rate) for ResNet50.  

Method 

conv_base.trainable 

True 

ResNet50 

Dropout Rate 

3band_256_8bit 
3band_256_16bit 
4band_256_8bit 
4band_256_16bit 
8band_256_8bit 
8band_256_16bit 
36band_256_8bit 
36band_256_16bit 
72band_256_8bit 
72band_256_16bit 

0.2 

0.31 
0.39 
0.56 
0.15 
0.23 
0.25 
0.41 
0.34 
0.34 
0.23 

0.5 

0.41 
0.13 
0.58 
0.13 
0.48 
0.19 
0.26 
0.37 
0.43 
0.30 

False 

0.2 

0.54 
0.23 
0.65 
0.34 
0.60 
0.30 
0.65 
0.31 
0.64 
0.27 

0.5 

0.46 
0.20 
0.57 
0.38 
0.65 
0.28 
0.70 
0.36 
0.66 
0.40  

Table 9 
Results  of  calculations  with  different  parameters  (conv_base.trainable  and 
Dropout Rate) for VGG19 + ResNet50.  

Method 

conv_base.trainable 

True 

Combination 

Dropout Rate

after the above training data preparation is as follows (Fig. 6). One of the 
optimal  simulation  results  (72band_256_16bit_split)  was  achieved 
through the utilization of data augmentation and flattened convolution 
kernels, as illustrated in Fig. 7.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about how the hyperparameters of the model were optimized, such as using methods like grid search or random search. However, it does mention that the study experimented with different parameters, including 'conv_base.trainable' and 'Dropout Rate', for models like VGG19, ResNet50, and their combinations. These experiments resulted in varying outcomes, as shown in Tables 8 and 9. It can be inferred that the researchers might have tried different values for these parameters to find the optimal configuration for each model. Nevertheless, without specific details regarding the optimization method, it cannot be confirmed whether they employed techniques like grid search or random search.
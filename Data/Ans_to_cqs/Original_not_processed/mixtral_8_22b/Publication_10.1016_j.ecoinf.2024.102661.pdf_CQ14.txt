Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Becker, M., Boulesteix, A.-L., Deng, D., Lindauer, M., 2023. Hyperparameter 
optimization: foundations, algorithms, best practices, and open challenges. Wiley 
Interdiscip. Rev. Data Min. Knowl. Discov. 13 https://doi.org/10.1002/widm.1484. 

Cao, L., Han, Y., Feng, M., Geng, Z., Lu, Y., Chen, L., Ping, W., Xia, T., Li, S., 2024. 
Economy and carbon emissions optimization of different provinces or regions in 
China using an improved temporal attention mechanism based on gate recurrent 
unit. J. Clean. Prod. 434, 139827 https://doi.org/10.1016/j.jclepro.2023.139827.

Hyperparameters  are  instrumental  in  shaping  the  architecture  of 
deep  learning  models and  steering  the  learning  process  (Bischl et  al., 
2023).  The  TPEBO  algorithm  stands  out  for  its  strategic  approach  to 
hyperparameter optimization. Beginning with an exploratory phase of 
random  searches  for  feasible  hyperparameter  configurations,  TPEBO 
progressively narrows down its focus to zones within the search space 
where a local optimum is identified, thereby approximating the global 
optimum  with  increasing  precision.  This  methodological  approach  is 
particularly  beneficial  for  fine-tuning  LSTM  models,  known  for  their 
intricate  structures.  By  automating  the  hyperparameter  adjustment 
process,  TPEBO  not  only  enhances  the  model's  efficiency  but  also 
significantly  curtails  the  time  traditionally  spent  on  manual  tuning, 
making the modeling workflow more efficient. The process is succinctly

This function is formally represented as Eq. (3). The TPEBO method 
utilizes a probabilistic model based on this function, which guides the 
identification  and  selection  of  optimal  hyperparameter  combinations. 
This model incorporates a probabilistic approach to continuously refine 
the  optimization  based  on  objective  function  assessments,  thereby 
facilitating  an  effective  search  for  hyperparameter  sets  that  minimize 
the  objective  function  most  effectively.  The  algorithm  is  further 
designed to mitigate the risks of local optima and to explore new po-
tential  minimization  areas  through  the  Expected  Improvement  (EI) 
function, promoting a balance between exploration and exploitation in 
the optimization journey. 

f(xi) = 1
n

∑n

i=1

|̂yi (cid:0) yi| +

λ

2n

∑n

(cid:0)

)2

wj

j=1

(3)

The hyperparameter optimization process for LSTM neural networks 
is conceptualized as a black-box function where diverse hyperparameter 
combinations are the inputs and the optimization targets are the outputs 
aimed at minimizing the objective metric. This black-box approach fa-
cilitates the establishment of a direct link between hyperparameter sets 
and the network's optimization goals. 

EcologicalInformatics82(2024)1026615T. Xie et al.                                                                                                                                                                                                                                       

Table 5 
Pseudo-code of TPEBO-LSTM algorithm.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The hyperparameters of the model are optimized using the Tree Parzen Estimator Bayesian Optimization (TPEBO) algorithm. This algorithm begins with an exploratory phase of random searches for feasible hyperparameter configurations. It then progressively narrows down its focus to zones within the search space where a local optimum is identified, thereby approximating the global optimum with increasing precision. This methodological approach is particularly beneficial for fine-tuning Long Short-Term Memory (LSTM) models, known for their intricate structures. By automating the hyperparameter adjustment process, TPEBO not only enhances the model's efficiency but also significantly curtails the time traditionally spent on manual tuning, making the modeling workflow more efficient.

The TPEBO method utilizes a probabilistic model based on a formal representation given by Eq. (3), which guides the identification and selection of optimal hyperparameter combinations. This model incorporates a probabilistic approach to continuously refine the optimization based on objective function assessments, thereby facilitating an effective search for hyperparameter sets that minimize the objective function most effectively. The algorithm is further designed to mitigate the risks of local optima and to explore new potential minimization areas through the Expected Improvement (EI) function, promoting a balance between exploration and exploitation in the optimization journey.

Therefore, unlike traditional methods such as grid search or random search, TPEBO employs a more sophisticated approach to hyperparameter optimization, leveraging both exploration and exploitation strategies to efficiently find the optimal set of hyperparameters for the model.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Figure 10. Comparison of accuracies of different classiﬁers for all datasets.

Big Data Cogn. Comput. 2021, 5, 53

11 of 15

Table 2. Performance of hand-crafted descriptors and D-CNN models for ﬁrst dataset.

Technique’s
Name

SVM
Kernel

LBP [10]
HOG [11]
LETRIST [12]
GLCM [13]
GLCM [13]
CJLBP [14]
LTrP [15]
AlexNet [17]
ResNet-50 [18]
VGG-19 [19]
GoogleNet [20]
Inceptionv3 [21]
CoralNet
BoF

Polynomial
Linear
Linear
RBF
Polynomial
Linear
Linear
Linear
Linear
Linear
Linear
Linear
–
Linear

Sensitivity

Speciﬁcity Accuracy

F1-Score

Cohen’s
Kappa (κ)

70.1%
66.3%
56.2%
66.2%
73.1%
71.2%
48.4%
94.1%
92.2%
92.1%
85.1%
77.1%
92.1%
99.1%

75.9%
69.3%
59.7%
75.1%
80.4%
77.3%
50.2%
96.3%
96.4%
92.1%
93.1%
92.3%
97.3%
99.0%

71.8%
67.1%
56.6%
69.3%
76.7%
72.7%
49.1%
95.2%
94.5%
92.2%
88.2%
83.3%
95.0%
99.08%

0.729
0.678
0.579
0.704
0.766
0.741
0.493
0.952
0.942
0.921
0.889
0.840
0.950
0.995

0.731
0.663
0.594
0.732
0.751
0.743
0.524
0.966
0.952
0.851
0.873
0.862
0.962
0.982

Figure 3. The proposed framework steps visual representation.

3.1. Explanation of Steps

Initially, an image is taken with the help of an underwater drone. In the next step,
the image is segmentized and divided into small patches. Features are extracted from
each patch with the help of handcrafted descriptors and D-CNNs. A visual vocabulary

Big Data Cogn. Comput. 2021, 5, 53

5 of 15

(VV) is created, as shown in Figure 4, this visual vocabulary is the features extracted
from these features, and the training features are passed to classiﬁer i.e., SVM, which
classiﬁes whether the VV-features are of bleached coral or healthy coral. We used different
handcrafted features as well as different D-CNN’s but AlexNet shows the highest accuracy.
We used different classiﬁers i.e., SVM, kNN, and decision tree, but SVM outperforms all
other classiﬁers.

Figure 4. Visual Vocabulary of features.

3.2. Feature Extraction

18. Chu, Y.; Yue, X.; Yu, L.; Sergei, M.; Wang, Z. Automatic image captioning based on ResNet50 and LSTM with soft attention. Wirel.

Commun. Mob. Comput. 2020, 2020, 8909458. [CrossRef]

19. Wazirali, R.; Intrusion detection system using fknn and improved PSO. Comput. Mater. Contin. 2021, 67, 1429–1445. [CrossRef]
20. Alsharman, N.; Jawarneh, I. Googlenet cnn neural network towards chest ct coronavirus medical image classiﬁcation. J. Comput.

21.

Sci. 2020, 16, 620–625. [CrossRef]
Joshi, K.; Tripathi, V.; Bose, C.; Bhardwaj, C. Robust sports image classiﬁcation using inceptionv3 and neural networks. Procedia
Comput. Sci. 2020, 167, 2374–2381. [CrossRef]

22. Bennett, M.K.; Younes, N.; Joyce, K. Automating Drone Image Processing to Map Coral Reef Substrates Using Google Earth

Engine. Drones 2020, 4, 50. [CrossRef]

23. Raphael, A.; Dubinsky, Z.; Iluz, D.; Netanyahu, N.S. Neural Network Recognition of Marine Benthos and Corals. Diversity 2020,

12, 29. [CrossRef]

3.2. Feature Extraction

Handcrafted and spatial features are concatenated to obtained visual vocabularies
(VV). The texture and color features are extracted with the help of handcrafted descriptors
while spatial features are extracted with the D-CNN models.

3.2.1. Spatial Features

Features are extracted with handcrafted descriptors as well as D-CNN models. Initially,
an image is captured with the help of underwater drones. Then the image is preprocessed
and resized to the input size of D-CNN.

3.2.2. Pretrained D-CNN

In the case of AlexNet, the input image size is 227 × 227 × 3. AlexNet is an ImageNet
with a total of twenty-ﬁve layers. There are ﬁve convolutional layers in AlexNet to extract
the spatial features. The other layers involved in AlexNet architecture are fully connected
layers, max-pooling layers, sigmoid layer, and ReLU layer. The feature vector is obtained at
fully connected layer 7 (FC-7) of the AlexNet. Figure 5 illustrates the convolutional layers
of AlexNet.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information about the specific data annotation techniques used in the deep learning pipeline, such as bounding box annotation or instance segmentation. However, it mentions that images are initially taken with the help of an underwater drone and then segmented into smaller patches. This suggests that some form of image segmentation might be employed during the data preparation stage. Additionally, the text discusses the use of various handcrafted descriptors and D-CNN models for feature extraction, indicating that labeled data was likely utilized for model training. Nevertheless, without explicit mention of the data annotation techniques used, we cannot definitively state what methods were applied in the deep learning pipeline.
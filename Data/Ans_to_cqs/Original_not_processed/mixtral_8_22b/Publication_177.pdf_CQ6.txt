Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The  selected  network  models  were  initialized  with  pre-
trained ImageNet weights and then fine-tuned to our datasets. 
The performance of a deep neural network for a dataset highly 
depends on network hyperparameters. The selection and fine-
tuning of  optimized hyperparameters  is  generally  a  difficult 
and  time-consuming  task.  Instead  of  manually  selecting  the 
hyperparameters, we employed Bayesian optimization to find 
their optimal values for each of the six models.  A Gaussian 
process  model  of  the  objective  function  is  used  by  the 
Bayesian optimization technique.  Different variables can be 
optimized using this technique such as network section depth, 
batch size, initial learning rate, momentum, and regularization 
strength. For this study, we optimized the network for batch 
size  (between  1  and  32)  and  initial  learning  rate  (between 
1×10-4  and  1×10-2).  The  optimization  was  performed  by

images  which  do  not  belong  to  a  known  class  (specie)  of  a 
plant. 

The highest testing accuracy was found to be around  96.7% 
using  the  InceptionResNetV2  network.  An  example  of  an 
estimated objective function value for different learning rates 
and batch sizes for this model is shown in Fig. 6 with the best 
batch size of 6 and the learning rate of 0.0014593. On the other 
hand, for VP_200, the maximum testing accuracy was found 
to be 91.8% using Xception net with the second closest one as 
of 91.2% using InceptionResNetV2. Though this dataset was 
more  balanced  and  had  a  large  number  of  samples,  the 
validation  and  testing  accuracies  were  comparatively  lower 
than  the  UBD_45  dataset.  One  potential  reason  may  be  the 
presence of a large number of classes and the nature of images 
in the dataset. 

Fig. 6.  An example of estimated objective function value using Bayesian 
optimization for InceptionResNetV2 network 

TABLE I.

learning  models  namely  GoogleNet,  ResNet,  and  ResNeXT 
with different configurations [25]. In another study, Ghazi et 
al.  [26]  ensembled  the  results  of  GoogleNet  and  VGG 
networks  to  obtain  an  accuracy  of  80.18%  on  a  validation 
dataset of the LifeCLEF 2015 dataset. A bagging ensemble of 
three ResNet152-layered CNN networks was used by Šulc and 
Matas  to  obtain  a  performance  of  78.8%  mean  average 
precision (MAP) on a Plant CLEF 2016 dataset [27].

training while 40% of the images were used for testing. In 
order to develop and compare the models for both datasets, 
we split the original 40% testing data into validation (20%) 
and testing (20%). So, the final distribution of the dataset 
was  60%  for  training,  20%  for  validation,  and  20%  for 
testing. 

Agathis Bomeensis  Bauhinia coccinea 

Clitoria Ternatea  Shorea Albida 

Fig. 3.  Example plants from the UBD_45 dataset 

B.  Deep Learning Models

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not provide information about any specific preprocessing steps involved before training the deep learning models. However, it is common practice in machine learning and deep learning to perform various preprocessing techniques such as normalization, scaling, cleaning, augmentation, etc., depending on the type of data being used. Normalization typically involves rescaling the input features so they fall within a certain range, often between 0 and 1 or -1 and 1. Scaling can also help ensure that all features have similar magnitudes, preventing those with larger scales from dominating the learning process. Cleaning refers to handling missing or corrupted data points, either by removing them entirely or imputing appropriate values. Data augmentation is a strategy commonly applied when working with image data, where existing images are modified through operations like rotation, flipping, zooming, cropping, etc., to artificially increase the amount of available training data.
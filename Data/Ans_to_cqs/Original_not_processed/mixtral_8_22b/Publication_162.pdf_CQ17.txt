Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

15. K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask R-
CNN,” in Proceedings of the IEEE International Confer-
ence on Computer Vision (ICCV) (2017), pp. 2961–2969.
16. K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual
learning for image recognition,” in IEEE Conference on
Computer  Vision  and  Pattern  Recognition  (CVPR)
(2016), pp. 770–778.

17. E. J. Howe, S. T. Buckland, M. L. Després-Einspen-
ner, and H. S. Kühl, “Distance sampling with camera
traps,” Methods Ecol. Evol. 8 (11), 1558–1565 (2017).
18. A. K. Kalan et al., “Towards the automated detection
and  occupancy  estimation  of  primates  using  passive
acoustic monitoring,” Ecol. Indic. 54, 217–226 (2015).
19. L.  Keselman,  J.  Iselin  Woodfill,  A.  Grunnet-Jepsen,
and A. Bhowmik, “Intel real-sense stereoscopic depth
cameras,”  in  Proceedings  of  the  IEEE  Conference  on
Computer  Vision  and  Pattern  Recognition  Workshops
(CVPR-WS) (2017), pp. 1–10.

results in an unchanged expected activation in the fol-
lowing  convolutional  layers.  Furthermore,  we  nor-
malize all input channels (red, green, blue, and depth)
by subtracting the respective mean and dividing by the
respective standard deviation over the whole dataset.
We also introduce a feature fusion module to combine
the extracted features from both backbones using one
3 × 3 convolution per FPN scale and reduce the num-
ber of channels from 2 × 256 to 256. This allows us to
use weights pre-trained on the Microsoft COCO data-
set [24] for the region proposal network (RPN) classi-
fier and mask head. Fig. 4 illustrates the general archi-
tecture of the resulting D-Mask R-CNN architecture.
We  restrict  our  evaluation  of  D-Mask  R-CNN  to
instances of deers as it is the most common species in our
RGB-D dataset. We quantify the results of D-Mask R-
CNN using the average precision (AP), AP50, and AP75
metrics as defined by the Microsoft COCO dataset [24].

he 

is 

fauna and flora using deep learning.

Dimitri Korsch is a Research Associ-
ate in the Computer Vision Group at
Friedrich  Schiller  University  Jena,
Germany.  He received his BSc and
MSc  degree  in  IT-Systems  Engi-
neering from University of Potsdam
in  2013  and  2016,  respectively.  His
research  interests  include  unsuper-
vised  machine  learning,  reinforce-
ment learning as well as fine-grained
visual categorization.

20. D. Korsch and J. Denzler, “In defense of active part se-
lection for fine-grained classification,” Pattern Recog-
nit. Image Anal. 28 (4), 658–663 (2018).

21. D. Korsch, P. Bodesheim, and J. Denzler, “Classifica-
tion-specific  parts  for  improving  fine-grained  visual
categorization,” in German Conference on Pattern Rec-
ognition (GCPR) (2019), pp. 62–75.

22. A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Ima-
geNet  classification  with  deep  convolutional  neural
networks,” in Advances in Neural Information Processing
Systems (NIPS) (2012), pp. 1097–1105.

23. T. Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan,
and S. Belongie, “Feature pyramid networks for object
detection,”  in  Proceedings  of  the  IEEE  Conference  on
Computer  Vision  and  Pattern  Recognition  (2017),
pp. 2117–2125.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The given context does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline mentioned in the paper. However, based on common practices in deep learning, some possible regularization techniques could be employed such as Dropout or L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust and reducing its reliance on any single neuron. On the other hand, L2 Regularization adds a penalty term to the loss function, encouraging smaller weight values and thus preventing overfitting due to large weights.

While these methods are commonly used, without further details from the authors or the original research, it cannot be confirmed if either of these techniques were applied in this particular case.
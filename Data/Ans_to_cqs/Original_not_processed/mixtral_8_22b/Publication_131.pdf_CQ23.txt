Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

proper databases for CNN training.

16 of 19

Table 3. Confusion matrix for the ﬁrst CNN (5 classes).

Predicted Values

Actual values

Ep. cal. standard

Ep. cal. chorus

Ep. cal. amplexus

Al. obs. standard

Al. obs. distress

Ep. cal. standard

94.27% (560)

Ep. cal. chorus

Ep. cal. amplexus

Al. obs. standard

Al. obs. distress

4.78% (10)

9.09% (13)

1.2% (11)

2.28% (1)

1.01% (6)

95.22% (199)

0

0

0

4.71% (28)

0

90.91% (130)

0

0

0

0

0

98.8% (902)

0

0

0

0

0

97.72% (43)

Sensors 2021, 21, 3655

17 of 19

4. Discussion and Conclusions

13 of 19

ﬁlters per convolution layer is 32 for the ﬁrst one, 64 for the second one, and 128 for the
ﬁnal one. A kernel size (height and width of the convolution window) of 3 × 3 is used
on each convolution layer. The fully connected layer (FC) consists of 512 neurons and
the softmax layer with the number of units for classiﬁcation (one for each type of sound).
Figure 11 shows the structure of the CNN.

Figure 11. Structure of the designed CNN.

For network training, 80% of the samples were used as training samples, and 20%
as tests. This strategy is widely used to enhance the generalization of a machine learning
model and prevent overﬁtting. A balanced sampling was carried out for the test set. For
it, one audio sample (with its corresponding data augmentation samples) out of 5 was
selected for each type of anuran call. Thus, 7623 samples were used for training and 1892
for testing.

number of output filters per convolution layer is 32 for the first one, 64 for the second one, and 128 for the final one. A kernel size (height and width of the convolution window) of 3 × 3 is used on each convolution layer. The fully connected layer (FC) consists of 512 neurons and the softmax layer with the number of units for classification (one for each type of sound). Figure 11 shows the structure of the CNN. Sensors 2021, 21, 3655

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To ensure the generalizability of the deep learning model, several measures were taken during the development process. Firstly, a diverse dataset consisting of five different types of anuran calls was utilized for training and testing purposes. This diversity in the dataset helped improve the robustness of the model by exposing it to various patterns and characteristics present in the sounds produced by different species of frogs.

Secondly, a balanced sampling approach was employed when selecting samples for the test set. Specifically, one audio sample along with its corresponding data augmentation samples from each type of anuran call was chosen for inclusion in the test set. By doing so, the distribution of samples across different classes remained consistent, which further contributed to enhancing the overall performance and reliability of the model.

Lastly, the dataset was split into two parts - 80% for training and 20% for testing. This strategy is commonly known as holdout validation or simply train/test split. It allows for assessing how well the trained model can perform on unseen data while also preventing potential issues related to overfitting. In this case, using a larger portion of the dataset for training enabled the model to learn more effectively from the available examples without being too heavily influenced by any specific instances within the dataset.

In summary, multiple steps were taken to promote the generalizability of the deep learning model, including utilizing a diverse dataset, employing balanced sampling techniques, and implementing a train/test split methodology. These measures collectively worked towards ensuring that the developed model could accurately recognize and classify various types of anuran calls even when presented with previously unseen data.
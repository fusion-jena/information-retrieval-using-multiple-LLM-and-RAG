Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(0–0.335),  medium 
formance  classes  were  considered:  poor 
(0.336–0.665), and good (0.666–1).

Assis, J., Tyberghein, L., Bosch, S., Verbruggen, H., Serr˜ao, E.A., De Clerck, O., 2018. Bio- 
ORACLE v2. 0: extending marine data layers for bioclimatic modelling. Glob. Ecol. 
Biogeogr. 27, 277–284. https://doi.org/10.1111/geb.12693. 

Ballard, H.L., Robinson, L.D., Young, A.N., Pauly, G.B., Higgins, L.M., Johnson, R.F., 
Tweddle, J.C., 2017. Contributions to conservation outcomes by natural history 
museum-led citizen science: examining evidence and next steps. Biol. Conserv. 208, 
87–97. https://doi.org/10.1016/j.biocon.2016.08.040. 

Broennimann, O., Fitzpatrick, M.C., Pearman, P.B., Petitpierre, B., Pellissier, L., 

Yoccoz, N.G., Thuiller, W., Fortin, M.J., Randin, C., Zimmermann, N.K., Graham, C. 
H., Guisan, A., 2012. Measuring ecological niche overlap from occurrence and 
spatial environmental data. Glob. Ecol. Biogeogr. 21, 481–497. https://doi.org/ 
10.1111/j.1466-8238.2011.00698.x.

m  depth,  Assis  et  al.,  2018),  which  are  less  representative  of  benthic 
species  associated  to  the  sea  bottom  such  as  H.  guttulatus  and 
H.  hippocampus  (Correia  et  al.,  2018).  The  accurate  selection  of  pre-
dictors is a major methodological concern affecting model performance 
and needs to be highly representative of the biology of the target species 
when using SDM outputs in conservation prioritization (Franklin, 2023). 
Our  niche  analyses  showed  that  H.  guttulatus  and  H.  hippocampus' 
niches were similar but not equivalent. Although these two seahorses 
share  a  wide  surface  (> 97%)  in  terms  of  geographic  extension  (a 
bidimensional  space),  they  occupied  different  niches  when  the  multi-
dimensionality  of  predictors  was  taken  into  account.  H.  hippocampus 
resulted to be associated with more open habitats, while H. guttulatus is 
mostly  positively  correlated  with  vegetation  coverage,  possibly  indi-

2.4. Model validation 

The model's performance was  evaluated using the Area Under the 
receiver-operator  Curve  (AUC).  The  AUC  values  range  from  0  to  1, 
where values closer to 1 indicate a higher accuracy of model prediction 
(Fielding and Bell, 1997). In addition to the AUC, the True Skill Statistics 
(TSS) was calculated. TSS values range between (cid:0) 1 and + 1. A TSS value 
of +1 means complete agreement between observed and predicted dis-
tributions, whereas values of ≤0 denote no better than random perfor-
mance  (Allouche  et  al.,  2006).  AUC  and  TSS  are  the  methods  most 
commonly used to assess the model performance in species distribution 
model studies (e.g., Gaier and Resasco, 2023; Mondanaro et al., 2023; 
Song and Estes, 2023).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps can be applied to improve its performance. These may include:

1. Data Cleaning: This involves removing or correcting erroneous data points, handling missing values, and eliminating outliers. It ensures that only relevant information is fed into the model.

2. Normalization: This process scales numeric variables to a common range to prevent attributes with larger magnitudes from dominating those with smaller ones. Common techniques include Min-Max Scaling, Decimal Scaling, and Z-Score Normalization.

3. Feature Selection: This step aims to reduce dimensionality by selecting a subset of relevant features for use in model construction. Techniques like correlation matrix analysis, backward feature elimination, and forward feature construction can be employed.

4. Encoding Categorical Variables: Deep learning models require input data to be numerical. Therefore, categorical variables need to be converted into numerical form. One popular technique is one-hot encoding.

5. Data Augmentation: To increase the amount of available training data, various transformations can be applied to existing samples. For images, this could involve rotations, translations, zooming, etc.

6. Splitting Dataset: Before training, it's essential to split the dataset into three subsets: training set, validation set, and test set. The training set is used to train the model, the validation set helps tune hyperparameters, and the test set evaluates the final model's performance.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

http://orcid.org/0000-0002-2176-7935 

R E F E R E N C E S

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., … Kudlur, M. 
(2016). TensorFlow: A system for large- scale machine learning. OSDI, 
16, 265–283.

Anderson, T. M., White, S., Davis, B., Erhardt, R., Palmer, M., Swanson, A., 
… Packer, C. (2016). The spatial distribution of African savannah her-
bivores: Species associations and habitat occupancy in a landscape 
context.  Philosophical  Transactions  of  the  Royal  Society  B:  Biological 
Sciences, 371, 20150314. https://doi.org/10.1098/rstb.2015.0314
Babaee,  M.,  Dinh,  D.  T.,  &  Rigoll,  G.  (2017).  A  deep  convolutional  neu-
ral  network  for  background  subtraction.  Pattern  Recognition,  76, 
635–649.

download  for  Mac  and  Windows  with  the  pre- trained  humming-

bird model. In addition, I provide reproducible scripts for local and 

3 |  R E S U LT S

Google  cloud  environments  to  allow  users  to  train  new  models, 

which can then be used in the local software.

2.2 | Test dataset

Feature  extraction  of  the  fixed  inception  layers  completed  in  1 hr 

and  26 min  on  15  CPUs.  Training  of  the  new  layers  completed  in 

27 min on a single CPU. Model evaluation on the 70 test videos com-

pleted in 4 hr and 38 min on 30 CPUs with an average frame rate of 

My  collaborators  and  I  have  been  studying  hummingbird  ecol-

17 frames/s. On average, a video contained  545.84 candidate  mo-

ogy  using  time- lapse  cameras  in  the  Ecuadorian  Andes  since  2013 

tion frames (2.5% of total frames) that were sent for classification by 

(Weinstein & Graham, 2017). Cameras turn on at dawn, off at dusk,

|  1437

F I G U R E   2  The front screen of the DeepMeerkat GUI. A user 
can select a file or directory of videos to screen using a pre- trained 
model. The path to the model is set under “Advanced settings”

creasing false positives.

majority of hummingbird visitation events (Weinstein, 2015). For the 

For training the fine- tuned neural network, I collected images for 

purposes  of  this  article,  I  assumed  that  all  events  are  captured  by 

each class and trained with a batch size of 100 for 20,000 steps. To 

motion  detection  and  were  passed  to  the  neural  network  for  clas-

reduce training time, the feature vectors for the frozen layers were 

sification.  While  quantitative  benchmarks  are  needed  to  validate 

extracted in parallel using Google Cloud DataFlow. These features 

model performance, they provide a coarse description of the errors 

were then the basis for retraining the new fine- tuned layers. To fit

AC K N OW L E D G E M E N T S

Thanks to Segment for supporting this work as part of their Open 

Data Fellowship. Thanks to the many Google engineers who shared 

code samples and answering questions on deploying machine learn-

ing models. The author declares no conflict of interest.

DATA ACC E S S I B I L I T Y

A DeepMeerkat executable for Mac and Windows is available with 

cloud systems. In 2017 IEEE/ACM second international conference on 
internet-of-things  design  and  implementation  (IoTDI),  Pittsburgh,  PA, 
2017 (pp. 247–258).

Gregory, T., Carrasco Rueda, F., Deichmann, J., Kolowski, J., & Alonso, A. 
(2014).  Arboreal  camera  trapping:  Taking  a  proven  method  to  new 
heights.  Methods  in  Ecology  and  Evolution,  5,  443–451.  https://doi.
org/10.1111/2041-210X.12177

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, it appears that the deep learning model was trained using multiple CPUs. Specifically, feature extraction of the fixed inception layers was completed in 1 hour and 26 minutes on 15 CPUs, while training of the new layers took 27 minutes on a single CPU. Additionally, model evaluation on the 70 test videos was done on 30 CPUs with an average frame rate of 17 frames per second. However, there is no explicit mention of any specific hardware resources such as GPUs or TPUs being used for training the deep learning model. Therefore, based solely on the provided context, we cannot definitively state which hardware resources were used for training the deep learning model beyond the use of CPUs.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics81(2024)1025817S. Ghosh et al.                                                                                                                                                                                                                                   

Table 4 
Search space of CNN hyperparameters to be explored.  

Hyperparameters 

Range 

Convolution Layers 
Filters 
Filter Size 
Number of Neuron 
Batch Size 
Epochs 
CNN Model Optimizer 

Lower limit = 1 and Upper limit = 10 
Lower limit = 1 and Upper limit = 64 
Lower limit = 1 and Upper limit = 10 
Lower limit = 32 and Upper limit = 1024 
Lower limit = 8 and Upper limit = 512 
Lower limit = 1 and Upper limit = 25 
ADAM, SGD, RMSProp, Adadelta, Adagrad, Adamax

Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. ImageNet classification with deep 

convolutional neural networks. Commun. ACM 60, 84–90. https://doi.org/10.1145/ 
3065386. 

Kumar, S., Singh, A., Walia, S., 2018. parallel big bang–big crunch global optimization 
algorithm: performance and its applications to routing in WMNs. Wirel. Pers. 
Commun. 100, 1601–1618. https://doi.org/10.1007/s11277-018-5656-y. 

EcologicalInformatics81(2024)10258115S. Ghosh et al.                                                                                                                                                                                                                                   

Liu, Z., Reynolds, A.C., 2020. A sequential-quadratic-programming-filter algorithm with 
a modified stochastic gradient for robust life-cycle optimization problems with 
nonlinear state constraints. SPE J. 25, 1938–1963. https://doi.org/10.2118/193925- 
pa.

hyperparameters.  The  hyperparameters  with  the  current  results  are 
saved until better accuracy is achieved. The termination criterion of the 
proposed approach is the maximum number of iterations to be executed. 
After the completion of the iterations, the proposed approach provides 
us with the best hyperparameters, thus evolving the near-optimal CNN. 
The  overall  complexity  is  dominated  by  the  iterations,  “TC”,  and 
within  each  iteration,  the  operations  depend  on  the  population  size 
“NP”.  Therefore,  the  total  complexity  can  be  approximated  as 
O(TC*NP).

primarily focus on individual optimization or deep learning methods but 
lack integration.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper by S. Ghosh et al. discusses a methodology for optimizing Convolutional Neural Network (CNN) hyperparameters using a combination of Big Bang-Big Crunch (BBBC) and Particle Swarm Optimization (PSO) algorithms. In this study, several hyperparameters are considered, including convolution layers, filters, filter sizes, numbers of neurons, batch sizes, epochs, and model optimizers. The range of values for these parameters is defined within specific lower and upper limits.

Regarding optimization techniques used in the deep learning pipeline, the authors mention various model optimizers such as ADAM, Stochastic Gradient Descent (SGD), Root Mean Square Propagation (RMSProp), Adadelta, Adagrad, and Adamax. These optimizers help improve the training process of the CNN models by adjusting the weights and biases based on the calculated gradients during backpropagation.

For instance, ADAM combines the advantages of both adaptive learning rate methods like RMSProp and momentum-based methods like SGD. It calculates individual learning rates for different parameters from estimates of first and second moments of the gradients. This allows it to converge faster and perform well even with noisy data or sparse gradients.

Similarly, other mentioned optimizers have their unique features and benefits. For example, SGD uses only one sample per update, making it computationally efficient; however, it may suffer from slow convergence due to noise in the updates. On the other hand, RMSProp addresses this issue by dividing the learning rate by an exponentially decaying average of squared gradients, which helps stabilize the learning process.

Overall, the choice of optimization technique depends on factors such as computational resources, dataset characteristics, and desired trade-offs between speed and accuracy. By integrating BBBC and PSO algorithms, the authors aim to find optimal hyperparameter configurations for CNN models more efficiently while maintaining high prediction accuracy.
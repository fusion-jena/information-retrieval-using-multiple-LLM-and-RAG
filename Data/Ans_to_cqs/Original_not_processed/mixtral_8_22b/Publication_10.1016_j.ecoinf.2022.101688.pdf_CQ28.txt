Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

and  thus  we  could  compute  the  performance  of  the  network  on  each 
testing  file  and  report  on  the  F1-score  as  it  was  commonly  used  in 
literature. Unless stated otherwise, model training and testing was done 
on Microsoft Azure using the Data Science Virtual Machine and a NCv2- 
series virtual machine (NVIDIA Tesla P100 GPU).

CNNs  are  commonly  executed  on  GPU  hardware  which  results  in 
faster training time. However, we deliberately trained ResNet152V2 on 
CPUs  in  an  attempt  to  verify  that  training  could  be  executed  on  less 
expensive hardware. We trained the CNN on a virtual machine running 
the  “E2asV4”  instance  on  Microsoft  Azure  with  16GB  RAM  and  a 
2.35Ghz AMD EPYC™ 7452 2 vCPU which at the time of writing cost 
0.218  USD  per  hour.  When  the  feature  extractor  was  frozen,  it  took 
between 450 and 780 s to complete one epoch, and when the feature 
extractor was fine-tuned it took between 2035 and 3100 s per epoch. 
While these executions are time consuming, these findings reveal that it 
is  possible  to  train  pre-trained  models  on  less  expensive  hardware 
making them accessible to researchers and practitioners.

Table 4 
Comparison of the average F1 score across the different network architectures and dataset configurations. The exponent approach was used for the spectrogram input. 
The feature extracted was frozen. The results are averaged across 13 unique executions. The results are ordered (highest to lowed) based on the average of each 
network architecture across all configurations. The best three performing network architectures on a particular dataset configuration is highlighted in bold.  

Method 

ResNet101V2 
ResNet152V2 
InceptionResNetV2 
ResNet50V2 
DenseNet169 
DenseNet201 
VGG16 
DenseNet121 
InceptionV3 
ResNet101 
Xception 
MobileNetV2 

G 25 

95.30 
95.18 
94.70 
94.97 
94.92 
94.84 
97.26 
94.58 
92.22 
96.17 
93.88 
94.62 

G 50 

97.40 
96.92 
96.75 
97.04 
96.95 
96.72 
98.09 
96.69 
95.42 
97.49 
95.79 
96.65 

G 100 

96.27 
96.58 
96.57 
95.13 
95.69 
95.86 
94.99 
95.00 
95.40 
94.23 
95.50 
91.65 

L 25

dataset)  with  46  softmax  units.  The  spectrograms  were  duplicated  to 
meet  the  input  of  ResNet50  which  expects  3  channels.  Zhong  et  al. 
(2020) compared VGG16 that was randomly initialised to a ResNet50 
model that was  pre-trained  on ImageNet.  In both  cases, a  colour mel 
spectrogram was input into the network. The spectrograms were resized 
to match the network's input of 224 by 224. Their models were applied 
to bird and amphibian vocalisations. ResNet50 pre-trained on ImageNet 
was also used by LeBien et al. (2020) whereby the pre-trained feature 
extractor was used and then two fully connected layers were added to 
the CNN. Zhong et al. (2021) applied ResNet50 to a birdsong dataset 
that contained three classes (two bird presence and one absence). The 
CNN was  pre-trained on ImageNet and  the fully connected layer, fol-
lowed by a dropout and an output layer was added to the CNN that was

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which platform was used to deploy the deep learning model (e.g., AWS, Azure, Google Cloud platform)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Microsoft Azure was used to deploy the deep learning model. Specifically, the Data Science Virtual Machine and a NCv2-series virtual machine (NVIDIA Tesla P100 GPU) were utilized for model training and testing unless stated otherwise. Additionally, the model was also trained on a virtual machine running the "E2asV4" instance on Microsoft Azure with 16GB RAM and a 2.35Ghz AMD EPYC™ 7452 2 vCPU.
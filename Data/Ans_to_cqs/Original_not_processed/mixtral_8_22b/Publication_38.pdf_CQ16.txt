Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

As these data contained a significantly greater proportion of negative (non-bat calls) as
compared to positive examples (bat calls), standard error metrics used for classification such
as overall accuracy were not suitable for evaluating detection. Instead, we report the interpo-
lated average precision and recall of each method displayed as a precision-recall curve [42].
Precision was calculated as the number of true positives divided by the sum of both true and
false positives. We consider a detection to be a true positive if it occurred within 10ms of the
expert annotated start time of the search-phase echolocation call. Recall was measured as the
overall fraction of calls that were present in the audio that were correctly detected. Curves
were obtained by thresholding the detection probabilities from zero to one and recording the
precision and recall at each threshold. Algorithms that did not produce a continuous output

S2 Fig. Spectrogram annotation interface from Bat Detective. Boxes represent example user
annotations of sounds in a spectrogram of a 3840ms sound clip, showing annotations of two
sequences of search-phase echolocation bat calls (blue boxes), and an annotation of an insect
call (yellow box).
(TIF)

S3 Fig. Example search-phase bat echolocation calls from iBats Romania & Bulgaria train-
ing dataset. Each example is represented as a spectrogram of duration 23 milliseconds and fre-
quency range from 5–115 kHz using the same FFT parameters as the main paper, and contains
examples of different search-phase echolocation call type, but also a wide variety of back-
ground non-bat biotic, abiotic and anthropogenic sounds.
(TIF)

assumption that they belonged to the same call. We measured the top user’s annotation accu-
racy on the test set from Romania and Bulgaria compared to the expert curated ground truth.
This resulted in an average precision of 0.845 (computed from 455 out of 500 test files this user
had labelled). This is in contrast with the second most prolific annotator who had an average
precision of 0.67 (based on 311 out of 500 files).

many more unused annotations in the Bat Detective dataset that could potentially increase our
training set size. However, we found some variability in the quality of the citizen science user
annotations, as in other studies [61]. To make best use of these annotations, we need user
models for understanding which annotations and users are reliable [62, 63]. The Bat Detective
dataset also includes annotations of particular acoustic behaviours (feeding buzzes and social
calls), which in future can be used to train detection algorithms for different acoustic behav-
iours [e.g., 64].

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not explicitly mention any specific criteria for determining when training is complete, such as a validation loss plateau or another commonly used metric. Therefore, I cannot provide a definitive answer based solely on the given information. It would require additional details about the training process and evaluation methods employed in this study to accurately address the query.
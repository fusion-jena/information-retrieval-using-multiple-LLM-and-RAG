Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

To  predict  the  distribution  of  the  study  species,  we  employed  an 
ensemble  modeling  approach  in  BIOMOD2  package  (Thuiller  et  al., 
2009) in R (R Development Core Team, 2014) (See text S1). With this 
approach,  we  weight  averaged  four  different  models,  including  one 
regression-based 
three 
machine-learning  (Maximun  entropy  (MaxEnt),  RF(Random  Forest), 
and  Support  Vector  Machines  (SVM))  models,  to  generate  a  final 
ensembled model for each species. To test for model performance, we 
employed cross-validationby using 70% of the dataset as training data to 
build  the  models  and  the  other  30%  as  testing  data  to  calculate  the 
models’  accuracy. The Area Under the Receiving Operator Character-
istics Curve is the index we used to specify the accuracy (AUC). AUC is a 
scale that ranges from 0.5 to 1.0. A higher AUC value indicates that the 
model  is  more  accurate  (Phillips  and  Dudík,  2008).  We  considered  a

with decreasing rainfall, will likely lead to intensified competition.

2.6. Climate scenarios 

The Intergovernmental Panel on Climate Change (IPCC) (Pachauri 
et al., 2014) provided multiple Representative Concentration Pathways 
(RCPs)  for  predicting  climate  change  under  different  greenhouse  gas 
emission trajectories. We constructed habitat suitability models of the 
three species under the RCP2.6 (low emission) and RCP8.5 (high emis-
sion)  scenarios  using  the  same  modeling  method  and  with  the  same 
habitat variables described above. The predicted current niche models 
were projected into future climate conditions to identify range shifts in 

EcologicalInformatics77(2023)1022473S. Rezaei et al.                                                                                                                                                                                                                                  

the present spatial range of the species under two RCPs for 2070.

2.5. Ensemble modeling 

and 

(Generalized 

Linear(GLM))

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
During the preprocessing stage of a deep learning pipeline, several techniques can be employed to address data bias. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into subsets or strata based on specific criteria such as class labels. This ensures that each subset maintains the original class proportions, preventing any imbalance between classes. It helps in creating representative training and validation sets.

Oversampling is a technique used when dealing with imbalanced datasets where one class has significantly fewer samples compared to others. In this case, synthetic examples of the minority class are generated to balance the dataset. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) can be applied for oversampling.

Undersampling, conversely, reduces the number of instances in the majority class to match the size of the minority class. Random undersampling or informed undersampling methods can be utilized depending upon the nature of the problem.

Lastly, collecting diverse data is another way to mitigate data bias. By gathering data from various sources, it becomes possible to capture a broader representation of real-world scenarios, reducing potential biases introduced by limited or skewed data.

These techniques help improve the overall robustness and generalization capabilities of deep learning models by ensuring fair representation across all classes within the dataset.
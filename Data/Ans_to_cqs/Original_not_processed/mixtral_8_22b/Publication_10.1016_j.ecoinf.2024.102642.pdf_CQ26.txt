Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During  the  campaign,  methods  are  proposed  to  assess  inter- 
annotator  variability  and  aggregate  annotation  sets.  Those  methods 
will help to define the minimal number of annotators needed [Dubus 
et al., 2023; Walter et al., 2022]. Increasing the number of annotators 
per sample could be helpful if the variability is too high. Finally, the two 
grouping  methods:  majority  voting  (Section  2.2.3)  and  soft  labeling 
(Section 2.3.2) can also be used to produce relevant annotation sets for 
deep learning. 

6. Conclusion 

In this study, new approaches were explored to improve the accuracy 
and  efficiency  of  cetacean  vocalization  detection  using  deep  learning 
models. Leveraging multi-annotation campaigns involving both expert 
and novice annotators aimed to address the challenges associated with 
the scarcity of ground truth data in underwater PAM studies.

directly  to  datasets  where  the  relevance  of  annotations  has  not  been 
evaluated.

EcologicalInformatics81(2024)1026429G. Dubus et al.                                                                                                                                                                                                                                  

scaled  up  to  larger datasets  and  explanatory  variable  sets.  The initial 
training  of  the  annotators  could  be  a  source  of  bias,  it  has  to  be  as 
representative as possible of the datasets proposed and in line with the 
difficulty of the task [Kosmala et al., 2016].

However,  it  has  been  observed  that  annotation  sets  used  to  train 
models qualified as “permissive annotators” get lower performance than 
the ones qualified as “conservative annotators”  even with comparable 
F1 score with the annotations from the experts. Gu et al. (2023) also 
proposed that at the same ratio of noise on the training set, the model’s 
performance can decrease differently regarding the type of noise. In this 
study,  it  seems  that  simple  convolutional  neural  network  models 
generalize better from the most obvious examples and the introduction 
of false positives deteriorates the performance more than the omission of 
a vocalization in the annotation set. For future annotation campaigns, 
this  result  might  suggest  that  instructing  annotators  to  adopt  a  more 
conservative approach could be beneficial.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide specific information about techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, it mentions some strategies to improve the quality and reliability of annotation sets used for training deep learning models. These include increasing the number of annotators per sample when there is high variability, using grouping methods like majority voting and soft labeling, ensuring representative initial training of annotators, and considering adopting a conservative approach for annotators to minimize false positives.
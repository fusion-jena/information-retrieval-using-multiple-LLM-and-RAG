Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

AN  was  implemented  in  MATLAB  (www.mathworks.com).  For 
training,  we  used  the  stochastic  gradient  descent  with  momentum 
optimizer with default settings except for the initial learning rate (which 
we set to 0.001) and the maximum number of epochs (which we set to 15 
after  preliminary  tests).  The  ‘classical’  machine-learning  algorithms 
(AB, GB, HB, and LD) were run in Python (www.python.org) using the 
scikit-learn toolbox (Pedregosa et al., 2011). We left hyperparameters at 
scikit-learn  default  values,  except  for  the  learning  rate  of  GB  and  HB 
(which we set to 0.15) and the tolerance threshold value for LD (set to 1 
× 10

In Fig. 6, we present the predictions of the three top-ranking speci-
ficity  models.  The  graphs  highlight  (i)  the  essentially  perfect  perfor-
mance of AN; (ii) the poor performance of AB with non-Triatoma bug 
pictures;  and  (iii)  the overall high to  very  high, often  close  to 100%, 
specificity  of  all  algorithms  with  non-Panstrongylus  and  non-Rhodnius 
bugs (Fig. 6; see also Supplementary Tables S13– S15). We note, how-
ever,  that  these  models  did  not  include  picture  or  specimen  random 
effects (which led to convergence issues); in interpreting model-based 
estimates  and  predictions,  therefore,  it  is  important  to  keep  in  mind 
that their associated measures of uncertainty may be smaller than they 
should be. 

4. Discussion 

In  this  study  we  show  that  deep-learning  algorithms  can  use  low- 
resolution  pictures  taken  at  varying  angles  with  ordinary  cellphone

fication led to convergence issues, so the models were simplified (see 
Specificity below). Because in each (pseudo-)replicate run all algorithms 
ran on the same data (i.e., picture) subsets, we also included a ‘replicate’ 
random effect in the models.

2.3. Algorithm training and testing 

We trained all algorithms on a random subset of 5256 pictures (80% 

of  the  6570-picture  dataset).  To  gauge  performance  consistency,  the 
training-testing  process  was  repeated  10  times  for  each  algorithm;  in 
each of these (pseudo-)replicate runs, all five algorithms used the same 
training and testing picture subsets.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the method used to optimize the hyperparameters of the model. However, it mentions that the'stochastic gradient descent with momentum optimizer' was used for training the AN model in MATLAB, with the initial learning rate set to 0.001 and the maximum number of epochs set to 15 after preliminary tests. This suggests that some form of optimization or tuning was performed during the preliminary tests.

For the classical machine-learning algorithms (AB, GB, HB, and LD), which were run in Python using the scikit-learn toolbox, the hyperparameters were left at the default values, except for the learning rate of GB and HB (set to 0.15) and the tolerance threshold value for LD (set to 1 × 10^-5). Again, there is no explicit mention of the specific optimization method used for these hyperparameters.

Therefore, based on the given context, it cannot be definitively determined whether grid search, random search, or another method was used to optimize the hyperparameters of the model.
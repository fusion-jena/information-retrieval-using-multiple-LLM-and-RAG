Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1 
Pre-processing hyper-parameters for each dataset and the number of testing files 
used.   

Lemurs 

Alethe 

Gibbons 

Whydah 

Low pass filter cut off 
Downsampling rate 
Nyquist rate 
Segment duration 
Number of testing files 
Testing time (min) 

4000 
9600 
4800 
4 
46 
1840 

3100 
6400 
3200 
2 
27 
810 

2000 
4800 
2400 
4 
22 
1300 

9000 
18400 
9200 
3 
78 
1560

consuming step which involves exploring various neural network hyper- 
parameters to obtain the most optimal model. When combined, these 
three  problems  render  the  creation  and  utilisation  of  CNNs  to  solve 
bioacoustics research questions challenging to machine learning experts 
and non-experts alike.

Using  a  pre-trained  CNN  feature  extractor  and  adding  a  softmax 
output layer is less complex than having to optimise a CNN from scratch 
and requires less network design decisions and also less time on hyper- 
parameter tuning. We thus argue that this approach renders the use of 
deep learning much more accessible to practitioners. Extensive hyper- 
parameter tuning also requires expensive GPU hardware which might 
not  be  accessible  to  practitioners.  Our  findings  revealed  that  results 
could be obtained on limited hardware within 9 h (10 epochs of fine- 
tuning  the  feature  extractor)  which  would  cost  2USD,  at  the  time  of 
writing, if that was executed on a Microsoft Azure virtual machine – thus 
rendering this approach affordable and accessible. 

It is well accepted that no single machine learning algorithm – or in

dataset)  with  46  softmax  units.  The  spectrograms  were  duplicated  to 
meet  the  input  of  ResNet50  which  expects  3  channels.  Zhong  et  al. 
(2020) compared VGG16 that was randomly initialised to a ResNet50 
model that was  pre-trained  on ImageNet.  In both  cases, a  colour mel 
spectrogram was input into the network. The spectrograms were resized 
to match the network's input of 224 by 224. Their models were applied 
to bird and amphibian vocalisations. ResNet50 pre-trained on ImageNet 
was also used by LeBien et al. (2020) whereby the pre-trained feature 
extractor was used and then two fully connected layers were added to 
the CNN. Zhong et al. (2021) applied ResNet50 to a birdsong dataset 
that contained three classes (two bird presence and one absence). The 
CNN was  pre-trained on ImageNet and  the fully connected layer, fol-
lowed by a dropout and an output layer was added to the CNN that was

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide specific information about the hyperparameters such as learning rate or optimizer used in the deep learning model. However, it mentions that using a pre-trained CNN feature extractor and adding a softmax output layer can reduce complexity and the need for extensive hyperparameter tuning. It also states that their findings showed that results could be obtained within 9 hours (10 epochs of fine-tuning the feature extractor), indicating that some form of optimization process took place. Unfortunately, without further details, we cannot determine the exact values of the hyperparameters used in the deep learning model.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

[32]  A. Khan, A. Sohail, U. Zahoora, A.S. Qureshi, A survey of the recent 
architectures of deep convolutional neural networks, Artif. Intell. Rev. 
(2020) 1–70. https://doi.org/10.1007/s10462-020-09825-6. 

[33]  H.  Chongomweru,  A.  Kasem,  A  novel  ensemble  method  for 
classification  in  imbalanced  datasets  using  split  balancing  technique 
based on instance hardness  (sBal_IH),  Neural Comput.  Appl.  (2021) 
1–22. https://doi.org/10.1007/s00521-020-05570-7. 

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on September 08,2024 at 16:24:56 UTC from IEEE Xplore.  Restrictions apply.

trained using the outputs of the six models. For each dataset, 
two stacked ensemble models were generated. The input for 
the  first  model  was  all  the  probabilities  generated  by  deep 
learning models and for the second model, it was the mean of 
these probabilities. However, there was not much  difference 
noticed in the performance of both models, so the results are 
reported  only  from  the  first  model  for  both  datasets.  The 
performance  of  the  stacked  ensemble  was  not  found  to  be 
better  than  the  other  three  ensemble  techniques  for  the 
VP_200  dataset  while  for 
the 
improvement was  slightly higher than the outputs generated 
using mean and weighted mean ensemble techniques. We also 
analyzed the performance of meta learners, and it was noticed 
that  in  most  of  the  cases  SVM  and  KNN  outperformed  the 
other  three  classifiers  as  the  meta  learners  for  both  plant 
species datasets. A comparison of all models is shown in Table

TABLE I.  

PERFORMANCE METRICS FOR UBD_45 DATASET 

Model 

Performance Metrics 

Sensitivity 

Specificity 

Precision 

F1-Score 

InceptionResnet2 

95.93 

Resnet50 

Xception 

Inceptionv3 

Mobilenetv2 

Googlenet 

95.93 

94.44 

91.85 

92.04 

85.00 

99.92 

99.90 

99.87 

99.85 

99.83 

99.62 

97.65 

97.20 

95.90 

94.73 

95.16 

86.15 

96.78 

96.56 

95.16 

93.27 

93.57 

85.57 

TABLE II.  

PERFORMANCE METRICS FOR VP_200 DATASET 

Model 

Performance Metrics 

Sensitivity 

Specificity 

Precision 

F1-Score 

InceptionResnet2 

91.20 

Resnet50 

Xception 

Inceptionv3 

Mobilenetv2 

Googlenet 

88.68 

91.80 

88.65 

85.58 

83.28 

99.96 

99.94 

99.96 

99.94 

99.93 

99.92 

91.92 

89.46 

92.83 

90.47 

87.20 

84.90 

91.56 

89.07 

92.31 

89.55 

86.38 

84.08

The  selected  network  models  were  initialized  with  pre-
trained ImageNet weights and then fine-tuned to our datasets. 
The performance of a deep neural network for a dataset highly 
depends on network hyperparameters. The selection and fine-
tuning of  optimized hyperparameters  is  generally  a  difficult 
and  time-consuming  task.  Instead  of  manually  selecting  the 
hyperparameters, we employed Bayesian optimization to find 
their optimal values for each of the six models.  A Gaussian 
process  model  of  the  objective  function  is  used  by  the 
Bayesian optimization technique.  Different variables can be 
optimized using this technique such as network section depth, 
batch size, initial learning rate, momentum, and regularization 
strength. For this study, we optimized the network for batch 
size  (between  1  and  32)  and  initial  learning  rate  (between 
1×10-4  and  1×10-2).  The  optimization  was  performed  by

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The paper [33] discusses a novel ensemble method for classification in imbalanced datasets using a split balancing technique based on instance hardness (sBal_IH). This approach addresses data bias during preprocessing of the deep learning pipeline.

To tackle the issue of data imbalance, the authors propose a new sampling strategy called sBal_IH. This technique involves dividing the majority class into several subsets and combining them with minority classes to create balanced subsets. Each subset is then used to train a base learner, resulting in multiple base learners. These base learners are combined using various ensemble methods like voting, averaging, or stacking.

Moreover, the authors employ stratified k-fold cross-validation to ensure that the distribution of samples across folds remains consistent with the original dataset. This helps maintain the balance between classes when training and testing the models.

While the papers do not explicitly mention oversampling or undersampling techniques, they focus on addressing data bias through diverse data collection and the proposed sBal_IH technique. By creating balanced subsets and utilizing different ensemble methods, the authors aim to improve the overall performance of their deep learning models on imbalanced datasets.
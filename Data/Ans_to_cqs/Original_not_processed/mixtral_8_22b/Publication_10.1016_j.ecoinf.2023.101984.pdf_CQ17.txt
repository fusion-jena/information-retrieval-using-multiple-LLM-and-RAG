Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

maximum iterations with the subsample run type and a 30 random test 
percentage.  The  MaxEnt  model's  output  was  analysed,  and  the

Table 3 
The values show averages of 10 replicate runs, estimating the relative contributions of the environmental and territorial variables to the MaxEnt model. The percentage 
contribution determines the jackknife; in each iteration of the training algorithm, the increase in regularized gain is added to the contribution of the corresponding 
variable. Permutation importance shows the values of that variable on training presence, and background data are randomly permuted. The model is reevaluated on 
the permuted data, and the resulting drop in training AUC is shown, normalized to percentages.    

Quercus pubescens Willd. 

Quercus ilex subsp. ilex 

Variable 

Description 

Percent contribution 

Permutation importance 

Percent contribution 

Permutation importance 

Temperature factors     
bio4 
bio8 
bio9 
Precipitation factors     
bio13 
bio19 
Geographical factors     
Slope 

In degrees

Baquedano, E., Castillo, F.J., 2007. Drought tolerance in the Mediterranean species 
Quercus coccifera, Quercus ilex, Pinus halepensis, and Juniperus phoenicea. 
Photosynthetica 45 (2), 229–238. https://doi.org/10.1007/s11099-007-0037-x. 
Bedia, J., Herrera, S., Guti´errez, J.M., 2013. Dangers of using global bioclimatic datasets 
for ecological niche modeling. Limitations for future climate projections. Glob. 
Planet. Chang. 107, 1–12. https://doi.org/10.1016/j.gloplacha.2013.04.005. 
Berger, J.-F., Guilaine, J., 2009. The 8200calBP abrupt environmental change and the 
Neolithic transition: A Mediterranean perspective. Quat. Int. 200, 31–49. https:// 
doi.org/10.1016/j.quaint.2008.05.013. 

Bernabeu, J., García, O., Orozco-K¨ohler, T., 2018. New insigths relating to the beginning 
of the Neolithic in the eastern Spain: evaluating empirical data and modelled 
predictions. Quat. Int. 470, 439–450. https://doi.org/10.1016/j. 
quaint.2017.03.071.

Termo-Mediterranean 
Meso-Mediterranean 
Meso-Mediterranean 
Meso-Mediterranean 
Supra-Mediterranean 
Meso-Mediterranean 
Meso-Mediterranean 
Meso-Mediterranean 
Montane 
Meso-Mediterranean 
Meso-Mediterranean 
Meso-Mediterranean 
Montane 
Meso-Mediterranean 
Sub-Alpine 
Supra-Mediterranean 
Sub-Alpine 
Supra-Mediterranean 
Meso-Mediterranean 
Meso-Mediterranean 
Meso-Mediterranean 
Meso-Mediterranean 
Meso-Mediterranean 
Montane 
Meso-Mediterranean 
Meso-Mediterranean 
Meso-Mediterranean 

Layer 

– 

E61 
Sector A 
III 
II 
III.4 
IV2 
18 
– 

III 
CE14 
Ie 
C5, C4, C3 
C6 
C3b 
2 central 
FS29 
UE 104 
– 
– 

III.3 
Fumier 3Nb 
– 
– 

II 
CE9 
Id 

Reference

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Regularization methods are techniques used in machine learning and deep learning models to prevent overfitting by adding a penalty term to the loss function during optimization. In the context of deep learning pipelines, two common regularization methods are Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training, which helps to prevent complex co-adaptations on training data. This results in more robust and generalizable models. During each training cycle, individual nodes are either dropped out of the net with probability (1 - keep_probability) or kept with probability keep_probability.

L2 Regularization, also known as weight decay, adds a penalty equivalent to the sum of squares of all the weights multiplied by a factor called lambda to the loss function. By doing so, it discourages large weights, effectively limiting the capacity of the network and helping to avoid overfitting.

Other regularization methods include L1 Regularization, Early Stopping, and Data Augmentation. However, these were not explicitly mentioned in the given context.

Therefore, based on the provided information, the regularization methods used to prevent overfitting in the deep learning pipeline are Dropout and L2 Regularization.
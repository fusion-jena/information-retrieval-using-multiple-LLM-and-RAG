Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.2. Performance evaluation 

The most important measures of a neural network model's efficacy 
are accuracy, recall, F1 score, PR curve, and average mean accuracy. The 
mean average precision (mAp), which is the average AP of all n cate-
gories, measures the effect of accuracy and recall thoroughly. The mAp 
was chosen as the major model evaluation in the study, comprehensively 
analysing the model detection accuracy, recall, and F1 scores, which are 
derived as follows: 

P =

TP
TP + FP

(13)  

R =

TP
TP + FN

F1 =

2 × P × R
P + R

∫ 1

AP =

P(R)dR

0

(14)  

(15)  

(16)  

where TP stands for the true positive, FP stands for the false-positive, TN 
stands for the true negative and FN stands for the false-negative. 

4.3. Datasets

To extract the input information, the SRC3 block employs a parallel 
analysis  of  the  input  feature  map  using  two  convolution  kernels.  In 
contrast to the C3 block, the SRC3 block incorporates two convolution 
kernels prior to the input of the bottleneck block. One of the kernels is 
responsible  for  halving  the  dimension  of  the  feature  map,  while  the 
other  maintains  the  dimension  unchanged.  This  approach  allows  for 
more  comprehensive  processing  of  the  input  features,  enabling  the 
model  to  capture  both  high-level  semantic  information  and  preserve 
relevant details during the feature extraction process. The convolution 
kernel size utilized is 3 × 3, which leads to a broader receptive field of 
information and richer characteristics compared to the 1 × 1 convolu-
tion kernel. The output semantic information can be augmented by the 
action of two convolution kernels. The information output from the first

Step 3: Evaluate the performance of the trained model by validating 
it against the test images. This involves computing metrics such as ac-
curacy,  recall,  F1  score,  and  other  relevant  measures.  Adjust  the  pa-
rameters and structure of the model based on the evaluation results to 
improve the detection performance. Iterate the training and evaluation 
process until a satisfactory level of performance is achieved.

Table 7 shows that, when compared to other models, the model we 
propose  achieves  the  best  performance.  Compared  with  Centernet, 
YWnet  outperforms  by  11.5 and  11.7 in  mAp and  mAp50–95  respec-
tively. When compared to Efficientdet, the mAp of YWnet is increased by 
9.3 mAp and mAp50–95 is increased by 7.1 mAp50–95. Compared with 
FasterRCNN, the mAp of YWnet is increased by 2.9 mAp and mAp50–95 
is increased by 7.7 mAp50–95, and the number of parameters is only half 
that  of  FasterRCNN.  Compared  with  YOLOv4,  the  mAp  of  YWnet  is 
increased by 30.4 mAp, and mAp50–95 is increased by 23.1 mAp50–95. 
When  compared  to  YOLOv7,  the  mAp  of  YWnet  is  increased  by  15.5 
mAp, mAp50–95  is  increased by  12.2 mAp50–95, and the  number of 
parameters  is  only  half  that  of  YOLOv7.  Compared  with  YOLOx,  the 
mAp of YWnet is increased by 9.0 mAp, and mAp50–95 is increased by 
7.5  mAp50–95.  When  compared  to  YOLOv5,  the  mAp  of  YWnet  is

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The performance of the deep learning model is evaluated using several metrics, including accuracy, recall, F1 score, Precision-Recall (PR) curve, and average mean accuracy. Among these, the Mean Average Precision (mAp) is considered the primary metric for evaluating the model's effectiveness. It provides a thorough measure of the impact of accuracy and recall across all categories. The mAp is calculated as the average AP of all n categories. Other commonly used metrics include precision, recall, and F1 score, which are derived from True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN). These metrics help assess the model's ability to accurately classify objects or detect anomalies in data.
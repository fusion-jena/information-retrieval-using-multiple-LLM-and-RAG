Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Active  learning  iterations  used  a  batch  size  of  64,  ten  epochs  and 
learning rate of 0.001. We used a grid search technique (Mohri et al., 
2018) to tune hyperparameters of the final model including the number 
of epochs, batch size and learning rate. 

2.3.3. Active learning framework 

We  applied  an  active  learning  approach  to  iteratively  train  and 
improve  the  CNN  model.  The  active  learning  approach  is  depicted 
within Fig. 3 and described below.

Sankupellay, M., Konovalov, D., 2018. Bird call recognition using deep convolutional 

neural network, ResNet-50. In: Proceedings of Acoustics. 

Sekercioglu, C.H., et al., 2008. Climate change, elevational range shifts, and bird 

extinctions. Conserv. Biol. 22 (1), 140–150. 

Settles, B., Craven, M., 2008. An analysis of active learning strategies for sequence 

labeling tasks. In: Proceedings of the 2008 Conference on Empirical Methods in 
Natural Language Processing, pp. 1070–1079. 

Teixeira, D., et al., 2022. Fledge or fail: Nest monitoring of endangered black-cockatoos 
using bioacoustics and open-source call recognition. Ecol. Inform. 69, 101656. 
Thakur, A., Thapar, D., Rajan, P., Nigam, A., 2019. Deep metric learning for bioacoustic 

classification: Overcoming training data scarcity using dynamic triplet loss. 
J. Acoust. Soc. Am. 146 (1), 534–547. 

Threatened Species Scientific Committee (TSSC), 2005. Commonwealth Listing Advice

Hüllermeier, E., Waegeman, W., 2021. Aleatoric and epistemic uncertainty in machine 
learning: an introduction to concepts and methods. Mach. Learn. 110, 457–506. 
Kahl, S., et al., 2021. BirdNET: a deep learning solution for avian diversity monitoring. 

Ecol. Inform. 61, 101236. 

Karimi, D., et al., 2020. Deep learning with noisy labels: exploring techniques and 

remedies in medical image analysis. Med. Image Anal. 65, 101759. 

Kingma, D.P., Ba, J., 2014. Adam: a method for stochastic optimization. Proceedings of 
the 3rd International Conference on Learning Representations. arXiv preprint arXiv: 
1412.6980, pp. 1–15. 

Koehler, J., et al., 2017. The use of bioacoustics in anuran taxonomy: theory, 

terminology, methods and recommendations for best practice. Zootaxa 4251 (1), 
1–124. 

Koenecke, A., et al., 2020. Racial disparities in automated speech recognition. Proc. Natl. 

Acad. Sci. 117 (14), 7684–7689.

3.1. CNN model development with active learning 

The CNN model performance stabilised within five active learning 
iterations (Fig. 4). The majority of model performance improvement was 
achieved within the first two iterations, with a 34.3% decrease in macro- 
averaged MAE between iterations one and three, compared to a 12.1% 
decrease in macro-averaged MAE between iterations three and the final 
model (Fig. 4). Most of the training data added between iterations one 
and  three  consisted  of  non-target  calls  (Table  1),  which  led  to  rapid 
improvements in the model’s ability to differentiate non-target sounds, 
resulting in a reduction in false positives.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After reviewing the given context, it appears there isn't any explicit mention of specific regularization methods such as dropout or L2 regularization being employed to prevent overfitting in the deep learning pipeline. However, some general practices can be observed from the information provided.

Firstly, the authors utilized a grid search technique to fine-tune the hyperparameters of their final model, including the number of epochs, batch size, and learning rate. This process helps optimize the model's performance by finding the most suitable combination of these parameters, thus potentially reducing overfitting.

Secondly, they implemented an active learning approach to improve the Convolutional Neural Network (CNN) model iteratively. Active learning involves selecting the most informative samples for labeling and adding them to the training set, allowing the model to learn more effectively from fewer examples. By focusing on the most valuable data points, this strategy can help mitigate overfitting.

Lastly, the authors mentioned that the CNN model performance stabilized after five active learning iterations, indicating that further training would not significantly improve its accuracy. This suggests that the model did not suffer from severe overfitting during the training process.

In conclusion, while the text does not explicitly state the use of common regularization methods like dropout or L2 regularization, the application of grid search for hyperparameter tuning and the implementation of an active learning approach could contribute to preventing overfitting in the deep learning pipeline.
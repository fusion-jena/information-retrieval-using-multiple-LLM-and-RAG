Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

̂C =

1
N

∑N

i=1

Fi(X)

(9) 

For database D, Gini index G is used as a splitting criterion with RF 

based on the node impurity as given by (10). 

G(D) = 1 (cid:0)

)2

∑

(
p

c∈C

c/t

Where G(D) is the Gini index of a node with C classes, and p

(10) 

(cid:0)

)

c/
t

is the 

probability of class c at node t. 

The proposed method sets the number of estimators (decision trees) 
N to 100. Each estimator plays a vital role in learning from randomly 
created  subsets  of  the  training  data.  Next,  the  other  parameter  is  a 
random state, which is used to set the random seeds to 5 in the proposed 
˝
˝
algorithm. Then the maximum depth of each decision tree is set to 
allowing it to grow until all leaf nodes contain only samples of the same 
class. Finally, the number of samples at the leaf node is set to 1. 

auto

5. Detailed pipeline for experiments 

5.1. Methods of performance analysis

Table  3  gives  insights  into  the  distribution  of  trained,  tested,  and 
validated samples across  the groups. The data suggest that consistent 
ratios across groups are maintained in the sampling process to avoid any 
potential bias during classification. Different samples are present across 
groups to establish the reliability and generalizability of the model. For 
evaluation,  both  validation  and  test  sets  are  considered  separately  to 
fine-tune the model’s hyperparameters, leading to generalizability and 
scalability towards unseen datasets. 

4.5. Classifier specifications 

The Random Forest (RF) classifier is used in both levels of the hier-
archical classification model for performing group and plant-level pre-
dictions. Let several estimators N and Fi(x) be the function of predicting 
̂
the ith  decision tree for the input X, then the prediction 
C by RF is given 
by (9). 

̂C =

1
N

∑N

i=1

Fi(X)

(9)

4.4. Level-wise training, validation, and testing strategies with Random 
Forest classifier 

The  data  collection  is  split  into  eleven  groups  based  on  visual 
morphological  characteristics,  as  discussed  in  Section  2.  Each  group 
consists of several plant species varieties in the range of 7–14 plants per 
group covering  100  medicinal  plant  species with  training,  validation, 
and testing proportion as 70:20:10 in both the levels, apart from GSL100 
datasets,  a  completely  new  datasets  RTL80  and  RTP40  comprising 
random collections captured in real-time is used to assess the efficiency 
of the proposed hierarchical classification model. The main evaluation 
objective using random groups is to analyze the sensitivity of classifier 
samples collected in various real-time scenarios.

performance  towards  the  prediction  plant  species  type  in  the  present 
context.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

According to the provided context, the hyperparameters of the model are optimized using both validation and test sets separately to fine-tune the model's hyperparameters. However, the exact optimization technique such as grid search or random search is not explicitly mentioned in the context. It can be assumed that some form of hyperparameter tuning was performed to achieve optimal results, but without further information, we cannot provide a more detailed explanation of the optimization process.
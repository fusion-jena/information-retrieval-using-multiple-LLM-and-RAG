Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.6. Model selection 

Since  performance,  accuracy,  key  parameters,  and  sensitivity  for 
prediction vary depending on which method that is used (Diniz-Filho 
et al., 2009; Elith et al., 2006; Qiao et al., 2015), we tested eight different 
machine  learning  algorithms  to  generate  our  predictive  models.  The 
included  algorithms  were:  Generalized  Linear  Model  (GLM),  Support 
Vector  Machine  (SVM),  Multivariate  Adaptive  Regression  Spline 
(MARS),  Random  Forest  (RF),  Flexible  Discriminant  Analysis  (FDA), 
Classification  and  Regression  Trees  (CART),  Generalized  Boosting 
regression Model (GBM) and Maximum-Entropy learning (MAXENT) for 
habitat suitability prediction (Table S5). 

2.7. Ensemble modeling approach

The prediction power with machine learning algorithms was tested 
with the package “sdm”  (Naimi and Araújo, 2016), “dismo”  (Hijmans 
et al., 2015), “bioclim” (Booth et al., 2014), and the ensemble modeling 
approach  implemented  in  “biomod2”  (Thuiller  et  al.,  2009;  2014). 
Model mean performances for AUC, ROC, TSS, KAPPA, and COR values 
were extracted using “shiny” packages (Chang et al., 2023) in RStudio 
(RStudio Team, 2020). 

2.10. Statistical analysis

(Phillips et al., 2006). To ensure the processing of both continuous and 
categorical  discrete  values  in  the  metadata,  selected  algorithms  with 
regression and classification prediction tasks were used. Mapping fore-
casts of different algorithms in the ensemble approach were evaluated 
by assessing the accuracy and performance of the final models.

Jiang, Y., Luo, J., Huang, D., Liu, Y., Li, D., 2022. Machine learning advances in 

microbiology: A review of methods and applications. Front. Microbiol. 13, 925454 
https://doi.org/10.3389/fmicb.2022.925454. 

Jim´enez-Valverde, A., 2012. Insights into the area under the receiver operating 

characteristic curve (AUC) as a discrimination measure in species distribution 
modelling: insights into the AUC. Glob. Ecol. Biogeogr. 21 (4), 498–507. https://doi. 
org/10.1111/j.1466-8238.2011.00683.x. 

Jones, C.M., Hallin, S., 2010. Ecological and evolutionary factors underlying global and 

local assembly of denitrifier communities. ISME J. 4 (5) https://doi.org/10.1038/ 
ismej.2009.152. Article 5.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The given context does not provide explicit information about the criteria used to split the data for deep learning model training such as train, test, or validation sets. However, it discusses various aspects related to machine learning and statistical analysis.

In general, when working with machine learning models like those mentioned in the text (Generalized Linear Model, Support Vector Machine, etc.), data is often divided into three subsets: training set, validation set, and testing set. This division helps evaluate the model's performance and prevent overfitting.

The training set is used to fit the model, i.e., to learn from the data. The validation set is used during the model development phase to fine-tune hyperparameters and select the best model. It provides feedback on how well the model is performing while it is being developed. Lastly, the testing set is used after the model has been fully specified to estimate its performance on unseen data.

Common ratios for splitting the data include 70% for training, 15% for validation, and 15% for testing, but these can be adjusted based on the specific requirements and size of the dataset. Cross-validation techniques may also be employed to further improve the robustness of the model evaluation process.
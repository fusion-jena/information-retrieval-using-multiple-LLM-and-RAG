Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

images than others, which can affect the performance of the DL models 
(Attri et al., 2023; Kamilaris and Prenafeta-Boldú, 2018). Many studies 
use  data augmentation  techniques  to  increase  the  number  of  training 
data to address that. This approach generally improves the classification 
accuracy (Chen et al., 2022; Hasan et al., 2023; Le et al., 2020; Olsen 
et  al.,  2019;  Sarvini  et  al.,  2019).  Application  of  several  image  pre- 
processing techniques, such as resizing (Chechli´nski et al., 2019; Far-
ooq et al., 2018; Partel et al., 2020), removing background (Alam et al., 
2020; Bah et al., 2018; Jiang et al., 2019), image enhancement (Nke-
melu et al., 2018; Wang et al., 2020) and denoising (Tang et al., 2017), 
can also improve the performance of the models.

2.2. Split the datasets 

In this research, the datasets were randomly divided into three parts 
for training, validation and testing. Here, 60% of the data was used to 
train the deep learning models, and 20% of them was kept to validate the 
models. The remaining 20% data was used to evaluate the performance 
of the models. Table 1 show the number of data in the dataset and how 
they are separated for training, testing and validation. 

2.3. Deep learning models

Resize 

GAN 

Change 

Resize 

GAN 

Change 

MobileNetV2 

VGG16 

VGG19 

ResNet-50 

Inception-V3 

Inception-ResNetV2 

Xception 

DenseNet121 

DenseNet169 

DenseNet201 

Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 
Precision 
Recall 
F1 Score 

93.84 
93.84 
93.76 
89.23 
88.97 
88.72 
87.88 
87.92 
87.61 
91.71 
91.65 
91.47 
93.37 
93.39 
93.32 
92.51 
92.53 
92.38 
93.91 
93.90 
93.85 
94.63 
94.61 
94.58 
94.36 
94.39 
94.35 
96.38 
96.38 
96.37 

95.29 
95.30 
95.26 
93.70 
93.70 
93.63 
93.02 
92.96 
92.83 
95.71 
95.73 
95.70 
93.09 
93.07 
93.01 
96.24 
96.21 
96.19 
96.18 
96.15 
96.14 
96.69 
96.69 
96.68 
97.11 
97.12 
97.11 
98.49 
98.49 
98.49

2.6.4. Model training with selected patches 

We trained the same deep learning models as mentioned in Section 
2.5. For training, we resized all the patches of images to a resolution of 
256 × 256 pixels. Although after dividing the image into patches, there 
were images with lower resolution, i.e., 64 × 64 and 128 × 128 pixels, 
they were converted into a uniform size. 

2.6.5. Evaluation of the models 

An image was divided into patches first to predict its class label. Then 
the important patches were selected using the approach mentioned in 
Section 2.6.3. After that, the model predicted the class label for each 
patch. The weighted majority voting technique was used to predict the 
class label of the image from the predicted labels. 

2.7. Experimental setup

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information on specific data augmentation techniques like flipping, rotation, or scaling. However, it mentions that many studies use data augmentation techniques to increase the number of training data, improving classification accuracy. These techniques could include various methods such as flipping, rotation, scaling, cropping, zooming, etc. Without more detailed information, it is impossible to specify exactly which data augmentation techniques were applied in this particular study.
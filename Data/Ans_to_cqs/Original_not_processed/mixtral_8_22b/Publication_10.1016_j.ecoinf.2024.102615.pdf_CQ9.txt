Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

an input layer with the 4200 dimensions of the spectral reflectance data; 
ii) the first hidden layer containing 2048 neurons, which uses the ReLu 
activation function to learn the nonlinear expression of the data; iii) a 
batch normalization layer followed by a dropout layer (random dropout

imbalanced small datasets and the application of machine learning algorithms to 
predict total phosphorus concentration in rivers. Eco. Inform. 76, 102138 https:// 
doi.org/10.1016/j.ecoinf.2023.102138. 

Atkinson, P.M., Tatnall, A.R., 1997. Introduction neural networks in remote sensing. Int. 

J. Remote Sens. 18, 699–709. https://doi.org/10.1080/014311697218700. 
Attri, I., Awasthi, L.K., Sharma, T.P., Rathee, P., 2023. A review of deep learning 

techniques used in agriculture. Eco. Inform. 77, 102217 https://doi.org/10.1016/j. 
ecoinf.2023.102217. 

Awadallah, M.A., Abu-Doush, I., Al-Betar, M.A., Braik, M.S., 2023. Metaheuristics for 
optimizing weights in neural networks. In: Mirjalili, S., Gandomi, A.H. (Eds.), 
Comprehensive Metaheuristics, 1st ed. Elsevier, Palestine, pp. 359–377. 
Bashir, O., Bangroo, S.A., Shafai, S.S., Senesi, N., Kader, S., Alamri, S., 2024.

the percentage value of the dropout layers, the learning rate, and the 
batch size, the optimal model was obtained with an R value of 0.76.

EcologicalInformatics81(2024)1026153M. Zhou et al.                                                                                                                                                                                                                                   

Fig. 3. The performance optimization of the MLP model.  

Table 1 
Hyperparameters tuning ranges.  

Hyperparameters 

Type 

Tuning range 

The number of neural network layers 
The number of neurons per layer 
Dropout 
Learning rate 
Batch size 

Integer 
Integer 
Float 
Float 
Integer 

1–5 
64, 128, 256, 512, 1024, 2048 
0.1, 0.25, 0.35, 0.5 
0.001, 0.0001, 0.00001 
32, 64, 128, 256  

Table 2 
the optimal MLP architecture and parameters.  

Layer type 

Input 
Dense 
Batch normalization 
Dropout 
Dense 
Batch normalization 
Dropout 
Dense(output) 
Total parameters 

Layer size 

4200 
2048 
2048 
2048 
1024 
1024 
1024 
1 
10,715,137  

Parameters 

0 
8,603,648 
8192 
0 
2,098,176 
4096 
0 
1025

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given information, there is no explicit mention or reference to where the data repository of the deep learning pipeline is available. It could be possible that the authors did not provide this information in their paper or it may have been omitted from the provided excerpts. Therefore, without further details, it is impossible to determine the exact location of the data repository. However, common repositories for such data include platforms like Zenodo, Figshare, Dryad, and GBIF. To find out more about the specific data repository, one would need to refer back to the original source material or contact the study's corresponding author directly.
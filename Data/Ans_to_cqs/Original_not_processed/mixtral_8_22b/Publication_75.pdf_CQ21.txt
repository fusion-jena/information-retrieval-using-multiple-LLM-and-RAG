Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(8)  

where  dim is  the dimension  of the  feature vectors,  Ft(τ) is the τth  the 
element of the feature vector of the query image t and FDBs (τ) is the τth 
element of the feature vector of the sth  image in the database DB. 

ComputersandElectronicsinAgriculture187(2021)1062696A. Loddo et al.                                                                                                                                                                                                                                  

Metrics. Four performance measures, namely average retrieval pre-
cision, ARP, average retrieval rate, ARR, Fscore  and mean average preci-
sion, mAP, have been used to compare the retrieval performances. The 
mathematical  formulation  for  each  of  these  performance  measures  is 
given as follows: 

ARP =

number of relevant images retrieved
total number of images retrieved (η)

=

100
ω

∑ω

i=1

r(DBi)
η

(9)  

ARR =

3.2.2. Retrieval results 

The retrieval experiments ensued the following strategy: every image 
is used as a query image and is matched with every remaining image in 
the database. Tables 6 and 7 present the retrieval performances of all the 
networks in terms of the four considered measures by choosing the top 
100 matches, i.e. η = 100, and the two chosen similarity distances. 

3.3. Deep learning vs traditional machine learning comparison 

A comparison of deep learning approaches with traditional machine

22 
30 
77 
118 
32 
24 
15 
55 
103 
160 
4  

ComputersandElectronicsinAgriculture187(2021)1062697A. Loddo et al.                                                                                                                                                                                                                                  

Table 5 
Summary of the experimentation results on the local dataset (StD is the deviation standard of Acc value and Time is the training time in minutes).  

Network 

AlexNet 
ResNet18 
ResNet50 
ResNet101 
GoogLeNet 
ShuffleNet 
SqueezeNet 
MobileNetV2 
InceptionV3 
VGG16 
SeedNet 

Acc 

93.43 
97.47 
96.46 
96.97 
95.45 
96.46 
95.96 
93.94 
96.46 
95.96 
97.47 

Spec 

91.08 
96.63 
94.44 
96.61 
95.06 
95.90 
95.75 
93.16 
95.99 
94.82 
99.88 

Sen 

91.36 
95.50 
94.98 
94.66 
93.47 
94.37 
94.71 
91.51 
94.81 
94.86 
96.81 

MAvG 

90.15 
96.11 
96.15 
96.34 
94.67 
95.57 
95.13 
92.67 
95.74 
94.22 
96.60 

MFM

characterises all the pictures, typically positioned in one of the corners, 
as shown in Fig. 5a, removed with a preprocessing step. It also aimed to 
produce  a  single  image  per  seed  to  create  a  new  dataset  of  cropped 
images, each containing a single seed. A detailed explanation is given in 
Section 2.3.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training, several postprocessing steps can be performed depending on the specific requirements of the task. Some common postprocessing steps include saliency maps, metrics calculation, confusion matrix, etc.

Saliency maps help visualize which parts of the input data were most important for the model's decision making process. They highlight areas of the input data that contributed significantly towards the final prediction made by the model. This helps understand how the model arrived at its output and provides insights into potential biases or errors in the model's reasoning.

Metrics calculation involves evaluating the performance of the trained model using various evaluation metrics such as accuracy, precision, recall, F1 score, ROC curve, etc. These metrics provide quantitative feedback about the quality of predictions made by the model and help identify any weaknesses or limitations in the model's performance.

Confusion matrix is another useful tool for analyzing the performance of a classification model. It summarizes the predicted and actual class labels for each instance in the test set, allowing us to see where the model has made mistakes and what types of misclassifications it tends to make. By examining the confusion matrix, we can gain valuable insights into the strengths and weaknesses of our model and use this information to improve its performance further.
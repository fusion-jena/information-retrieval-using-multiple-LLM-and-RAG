Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In contrast, ResNet50, with a staggering 76,019,586 parameters and 
a  training  time  of  952.39  s,  exhibited  relatively  inferior  performance 
when  compared  to  the  MobileNet-based  models.  It  has  a  substantial 
number of parameters (68,495,586) and requires 1022.48 s for training, 
reflecting its complexity. ResNet50 is a deeper and more complex ar-
chitecture specifically designed to address the challenge of training very 
deep neural networks. While it has achieved remarkable performance in 
various computer vision tasks (Mukti and Biswas, 2019), its structure 
may not have been optimized for the specific classification task in this 
study. The increased depth and complexity of ResNet50 may have made 
it more difficult for the model to capture important features and pat-
terns, resulting in lower accuracy and higher loss.

3.2.4. ResNet-50 

ResNet-50 is a widely used deep residual neural network architecture 
that addresses the challenges of training very deep networks. It employs 
residual blocks with skip connections to enable the flow of gradients and 
facilitate the training of deeper models. ResNet-50 has been pre-trained 
on  large-scale  datasets,  allowing  it  to  capture  complex  patterns  and 
achieve strong performance. 

3.2.5.

InceptionV3 

InceptionV3 is an advanced CNN architecture that incorporates the 
concept of inception modules. These modules utilize parallel convolu-
tions  of  different  kernel  sizes  to  capture  multi-scale  features.  Incep-
tionV3  has  been  pre-trained  on 
large-scale  datasets  and  has 
demonstrated  excellent  performance  in  various  image  classification 
tasks. 

3.2.6.

InceptionResNetV2

Transfer learning is an efficient technique in deep learning that le-
verages  pre-trained  models  to  address  data  scarcity  and  accelerate 
model  training.  Our  study  employed  several  well-known  pre-trained 
models,  namely  MobileNet,  VGG-19,  EfficientNet,  ResNet-50,  Incep-
tionV3, and InceptionResNetV2, to harness their learned representations 
and adapt them to our specific classification task. These models have 
been extensively trained on large-scale datasets, allowing them to cap-
ture general features that are useful across various domains (Weiss et al., 
2016).  Transfer  learning  involves  two  main  phases:  fine-tuning  and 
feature extraction. In the feature extraction phase, the pre-trained model 
is used as a fixed feature extractor. The input image is passed through 
the modelâ€™s convolutional layers, and the output features are obtained. 
These features capture meaningful information from the image, forming

3.2.6.

InceptionResNetV2 

InceptionResNetV2  is  a  fusion  of  the  Inception  and  ResNet  archi-
tectures,  combining  the  benefits  of  both  networks.  It  incorporates  re-
sidual  connections  and  parallel  convolutions  to  capture  fine-grained 
details and high-level features simultaneously. InceptionResNetV2 has 
been  pre-trained  on  extensive  datasets,  enabling  it  to  learn  compre-
hensive feature representations and achieve high performance. 

4. Materials and methods 

This section provides a comprehensive overview of the data acqui-
sition process, augmentation techniques employed, parameter settings, 
and the proposed model. The system-level design of our model is illus-
trated  in  Fig.  1.  The  methodology  incorporates  a  range  of  transfer 
learning  models,  including  MobileNet,  EfficientNet,  InceptionV3,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: Unfortunately, based on the given context, there is no mention or reference to any code repository links related to the deep learning pipeline. Therefore, I cannot provide the requested information.
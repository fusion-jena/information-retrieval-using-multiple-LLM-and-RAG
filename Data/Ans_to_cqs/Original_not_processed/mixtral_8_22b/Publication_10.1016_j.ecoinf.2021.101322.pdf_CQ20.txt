Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Given abundant GPU resources an ideal approach would also be to 
combine predictions from an ensemble of architectural similar or even 
identical  CNNs.  These  CNNs  could  use  the  same  training  data  but 
altering the training process slightly by shuffling the sequence of data or 
by using different batch sizes. This approach of using multi-model en-
sembles  to  produce  more  reliable  predictions  have  been  explored  in 
modelling studies within various fields (Kindt, 2018; Liao et al., 2014; 
Olsen et al., 2016). Future studies could exploit this, and it is expected 
that the group average prediction from several CNNs would outperform 
the performance of any single CNN in the very same group.

2.3. Implementation and training 

The  CNN  was  implemented  using  the  Keras  (Chollet,  2015)  and 
TensorFlow  (Abadi  et  al.,  2016)  software  packages  implemented  in 
Python, and computation was performed using CUDA version 9.1 and 
CuDNN  with  Nvidia  (Nvidia  Corp.,  Santa  Clara,  California)  P100 
accelerator cards with 12 GB of GPU memory. We used a Keras imple-
mentation of EfficientNetâ€“B4 called EfficientNet V1.1.0 (https://github. 
com/qubvel/efficientnet).  The  pre-trained  weights  used  for  transfer 
learning were available through this API.

2. Methods 

2.1. CNN training data

2.2. Convolutional neural network architecture

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The text mentions that the implementation and training of the CNN were done using the Keras and TensorFlow software packages implemented in Python. For computation, CUDA version 9.1 and CuDNN were used along with Nvidia P100 accelerator cards with 12 GB of GPU memory. Therefore, it can be concluded that the hardware resource used for training the deep learning model is GPU.
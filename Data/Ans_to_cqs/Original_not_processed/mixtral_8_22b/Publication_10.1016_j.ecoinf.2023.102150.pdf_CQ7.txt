Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1 
Number of training images, validation images (used for model validation during 
model  training)  and  out-of-sample  test  images  (used  for  external  model  vali-
dation after training was finished) as well as number of new images selected 
from  the  images  taken  between  summer  2020  and  summer  2021  for  model 
retraining.  

Class 

Number of 
training 
images 

Number of 
validation 
images 

Number of 
out-of-sample 
test images 

Number of new 
training images 
for model 
retraining 

Bad 

6453 

quality 

Bird 
Empty 
Least 

weasel 
Lemming 
Shrew 
Stoat 
Vole 
TOTAL 

3382 
9444 
1725 

9449 
9265 
4024 
9894 
53636 

677 

219 
979 
98 

967 
962 
438 
1024 
5364 

549 

119 
3301 
69 

647 
584 
64 
919 
6252 

306 

195 
533 
424 

449 
416 
425 
528 
3276

306 

195 
533 
424 

449 
416 
425 
528 
3276  

training  data  set. The  models were  trained on  a  cloud  service  with  1 
GPU,  4  CPUs  and  16  GB  RAM  provided  by  Sigma2  -  the  National 
Infrastructure  for  High  Performance  Computing  and  Data  Storage  in 
Norway. 

We evaluated the model performance on the validation data set and 
the  out-of-sample  test  data  set  by  calculating  model  accuracy  as  the 
number of correct predictions divided by the number of all images as 
well as precision, recall and F1 score for each class (Appendix B): 

Precision =

Recall =

TP
TP + FP
TP
TP + FN
precision*recall
precision + recall

F1 = 2*

(TP = Truepositives)

(FP = Falsepositives)

(FN = Falsenegatives)

some images from the previous years to the training data set. We then 
retrained the neural network as described before.

2.4. Model retraining 

Based on the quality check, the researcher can decide whether model 

performance  is  satisfactory  or  if  the  model  performance  should  be 
improved by selecting new training images and retraining the model. If 
images from new sites have been classified, model performance can be 
improved by including images from these sites in the training data set. If 
the model had problems with some classes; e.g. identification of some 
species,  it  might  help  to  include  more  images  of  these  species  in  the 
training data set. When selecting new training images, the model output 
from the original model can be helpful to find images that meet a certain 
criteria, e.g. to find images of a certain species. However, all training 
images should be reviewed manually instead of relying on model out-
puts only to avoid misclassified training images. 

2.5. Manual classification

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The table provides information about the dataset used for training, validating, and testing the deep learning model. The dataset is split into three parts: training images, validation images, and out-of-sample test images. Training images are used to teach the model how to recognize different classes, while validation images are used to fine-tune the model parameters during training. Out-of-sample test images are used to evaluate the final performance of the model after training has finished. Additionally, there are also new training images selected for model retraining. These images are likely chosen based on specific criteria such as improving the model's ability to identify certain species or adding images from new locations. It is important to note that all training images should be reviewed manually to ensure they are correctly labeled and do not contain any errors.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

classification tasks. The evaluation of model performance was based on 
the  accuracy  metric.  Each  model  underwent  training  for  10  epochs, 
utilizing  a  batch  size  of  32.  The  class  mode  was  set  to  ‘categorical,’ 
indicating  that  the  labels  were  encoded  using  one-hot  encoding  for 
multi-class  classification  tasks.  These  generalized  parameter  settings 
were consistently applied across all models, ensuring comparability and 
standardization throughout the study experiments.

Fig. 6. Training accuracy vs validation accuracy - MobileNet-BiLSTM.  

EcologicalInformatics81(2024)102548824681000.20.40.60.81EpochsAccuracyTrainingAccuracyValidationAccuracyLoss24681000.20.40.60.81EpochsLossTrainingLossValidationLoss24681000.20.40.60.81EpochsAccuracyTrainingAccuracyValidationAccuracyLossT. Chawla et al.                                                                                                                                                                                                                                 

GRU, LSTM, and BiLSTM consistently demonstrated superior results as 
shown  in  Figs.  4  to  9,  with  MobileNet-GRU  emerging  as  the  highest 
performing model. This model obtained an accuracy of 99.27% with a 
minimal loss of 0.02, and it was also efficient in terms of training time 
(890.16 s), parameter count (11,864,770), and a moderate number of 
layers (6).

5.1.4. Total model parameters 

Total  parameters  in  a  neural  network encompass  the  aggregate of 
trainable weights and biases, serving as a direct measure of the model’s 
architectural complexity. This parameter count is pivotal in defining the 
model’s capacity for feature extraction and pattern recognition. High- 
parameter  models  are  adept  at  discerning  intricate  data  patterns  but 
are  susceptible  to  overfitting,  particularly  in  scenarios  with  limited 
training samples. Furthermore, such models escalate computational and 
memory  requirements,  potentially  impeding  deployment  in  resource- 
constrained environments. 

5.1.5. Total layers in the model

Fig. 7. Training loss vs validation loss - MobileNet-BiLSTM.  

Fig. 8. Training accuracy vs validation accuracy - MobileNet-LSTM.  

Fig. 9. Training LOSS VS VALIDATION LOSS - MobileNet-LSTM.  

EcologicalInformatics81(2024)102548924681000.20.40.60.81EpochsLossTrainingLossValidationLossT. Chawla et al.                                                                                                                                                                                                                                 

Table 2 
Comparison of performance metrics of different models in terms of accuracy, 
binary cross entropy loss, model training time, total parameters and total layers.  

significantly. The results across different metrics of six transfer learning 
models, three RNNs and three hybrid models implemented are presented 
in Table 2. 

Total 
parameters 

No. of 
layers 

5.3. Error analysis 

Model 

Accuracy 
(%) 

Loss

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The strategy implemented to monitor the model performance during training was based on the accuracy metric. Each model underwent training for 10 epochs, utilizing a batch size of 32. The class mode was set to 'categorical,' indicating that the labels were encoded using one-hot encoding for multi-class classification tasks. These generalized parameter settings were consistently applied across all models, ensuring comparability and standardization throughout the study experiments.

Answer:::

Unhelpful Answer:
I do not have enough information to provide an accurate response.

Answer:::

Answer:
The strategy implemented to monitor the model performance during training was based on the accuracy metric. Each model underwent training for 10 epochs, utilizing a batch size of 32. The class mode was set to 'categorical,' indicating that the labels were encoded using one-hot encoding for multi-class classification tasks. These generalized parameter settings were consistently applied across all models, ensuring comparability and standardization throughout the study experiments.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.1.2. Hyperparameter tuning 

In this study, we utilized HHOS for hyperparameter tuning, aiming to 
discover  optimal  values  for  selected  hyperparameters  to  improve  the 
performance of the selected pre-trained models. The hyperparameters 
considered for tuning included the optimizer, activation function, initial 
learning rate, learning rate drop period, learning rate drop factor, and 
the number of neurons. We established a search space for each hyper-
parameter by selecting a set of values that had shown good results in 
previous studies. HHOS systematically explored various hyperparameter 
combinations  within  the  specified  range  through  random  sampling, 
allowing a comprehensive parameter space search.

search  tests  various combinations of  the hyperparameters  identified in  manual  search and  trains the selected 
models to achieve the maximum possible accuracy through multiple iterations. Experimental analysis revealed 
that the Fine-tuned EfficientNetB0 model exhibited superior performance, achieving an accuracy of 99.12%. In 
contrast, the performance of the ResNet18 model was disappointing with an accuracy of 93.24%, while other 
models outperformed it.

This study follows a hyperparameter optimization approach known 
as  the  Hybrid  Hyperparameter  Optimization  Scheme  (HHOS),  which 
combines  manual  and  random  search  methods  sequentially.  The  first 
step is manual search, which aims to identify optimal hyperparameter 
values  and  reduce  the  search  space.  This  is  achieved  by  selecting 
frequently used hyperparameter configurations that show effectiveness 
in  related  studies,  after  understanding  the  problem  at  hand.  Next, 
random  search  iteratively  samples  various  combinations  of  hyper-
parameter values identified in the manual search, trains the models with 
each set and identifies the hyperparameter configuration that yields the 
highest accuracy. 

The current study contributes to the field by:  

• Effectively utilizing transfer learning as a feature extractor and fine- 

tuner to train multiple classifiers

Yang, L., Shami, A., 2020. On hyperparameter optimization of machine learning 

algorithms: theory and practice. Neurocomputing 415, 295–316. 

Shorten, C., Khoshgoftaar, T.M., 2019. A survey on image data augmentation for deep 

Yang, C.-L., Harjoseputro, Y., Hu, Y.-C., Chen, Y.-Y., 2022a. An improved transfer- 

learning. J. Big Data 6 (1), 1–48. 

Sitepu, A.C., Liu, C.-M., Sigiro, M., Panjaitan, J., Copa, V., 2022. A convolutional neural 
network bird’s classification using north american bird images. J. Health Sci. 6 (S2), 
15067–15080. 

Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., Patwary, M., 

Prabhat, M., Adams, R., 2015. Scalable bayesian optimization using deep neural 
networks. In: International Conference On Machine Learning. PMLR, pp. 2171–2180. 

learning for image-based species classification of protected indonesians birds. CMC 
Comp. Mater. Continua 73 (3), 4577–4593.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: The hyperparameters considered for tuning in the deep learning model include the optimizer, activation function, initial learning rate, learning rate drop period, learning rate drop factor, and the number of neurons. These hyperparameters were chosen based on their demonstrated effectiveness in previous studies. The Hybrid Hyperparameter Optimization Scheme (HHOS) was employed to find the optimal values for these hyperparameters. This scheme combines manual and random search methods sequentially. Firstly, a manual search was conducted to identify potential optimal hyperparameter values and narrow down the search space. Then, a random search method was applied to explore different combinations of hyperparameters within the reduced search space. Through this process, the Fine-tuned EfficientNetB0 model achieved the highest accuracy of 99.12%.
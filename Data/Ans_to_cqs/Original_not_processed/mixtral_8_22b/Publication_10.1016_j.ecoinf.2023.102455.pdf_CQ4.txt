Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

CNN-BiGRU 

RVFL 

KELM 

CNN-BiRNN 

Phase 

Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 
Training 
Testing 

R 

0.985 
0.960 
0.960 
0.949 
0.972 
0.930 
0.999 
0.955 
0.921 
0.698 
0.865 
0.662 
0.843 
0.668 
0.873 
0.689 
0.998 
0.933 
0.952 
0.930 
0.964 
0.928 
0.997 
0.929 
0.792 
0.603 
0.826 
0.594 
0.783 
0.591 
0.801 
0.598 

RMSE 

0.072 
0.098 
0.117 
0.113 
0.100 
0.130 
0.024 
0.105 
0.164 
0.253 
0.210 
0.266 
0.225 
0.263 
0.205 
0.256 
0.010 
0.130 
0.128 
0.130 
0.112 
0.133 
0.024 
0.141 
0.256 
0.284 
0.236 
0.287 
0.261 
0.287 
0.251 
0.285 

MAPE

The last step in configuring the model involves feeding in predictors 
that  the  advanced  deep  learning  algorithm  feeds  by  the  decomposed 
components  resulting  from  the  previous  steps.  The  primary  model  is 
CNN-BiGRU  with  three  comparative  ML  methods  (KELM,  RVFL  and 
CNN-BiRNN),  which  provide  four  complementary  models  including 
MVMD-CNN-BiGRU,  MVMD-RVFL,  MVMD-KELM  and  MVMD-CNN- 
BiRNN.  These  models  are  used  to  create  the  multi-temporal  fore-
casting model of daily streamflow (Qflow). Setting hyperparameters and 
their structural architecture is the most important aspect of executing 
ML-based  predictive  models  (Jamei  et  al.,  2023b).  Based  on  recent 
research, the main approaches to tuning parameters are algorithms of 
metaheuristic optimization, schemes of cross-validation schemes (Nes-
ted/rolling  basis  cross-validation)  (Huyghues-Beaufond  et  al.,  2020),

CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiRNN Layers = 1, BiRNN neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 200,batch_size = 32, 
learning_rate = 0.0001, Dense = 100 
CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiRNN Layers = 1, BiRNN neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 200,batch_size = 32, 
learning_rate = 0.0001, Dense = 100

CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiRNN Layers = 1, BiRNN neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 200,batch_size = 32, 
learning_rate = 0.0002, Dense = 100 
CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiGRU Layers = 1, BiGRU neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 40,batch_size = 32, 
learning_rate = 0.000455, Dense = 100 
CNN Layers = 2, filters = 64, kernel_size =
[1,4], BiGRU Layers = 1, BiGRU neurons =
30, Optimizer: Adam, activation function =
‘relu’, epochs = 1000,batch_size = 32, 
learning_rate = 0.0015, Dense = 100 
num_nodes: 20, regular_para = 0.05, 
weight_random_range = [(cid:0) 1, 1], 
bias_random_range = [0, 1], 
num_nodes: 30, regular_para = 2, 
weight_random_range = [(cid:0) 1, 1], 
bias_random_range = [0, 1], 
Regularization_coefficient = 9E+05, 
kernel_parameter =9E+04 
Regularization_coefficient = 1E+06, 
kernel_parameter =9E+04 
CNN Layers = 2, filters = 64, kernel_size =

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't any information about specific data augmentation techniques such as flipping, rotation, or scaling being applied in this deep learning pipeline. However, it can be inferred that some form of data preprocessing or transformation has been done due to the mention of decomposing components and using them as inputs for the CNN-BiGRU model. This could potentially involve normalizing the input data, converting categorical variables into numerical ones, or other similar transformations. But without explicit details regarding data augmentation techniques, it cannot be confirmed if these were indeed implemented in the given pipeline.
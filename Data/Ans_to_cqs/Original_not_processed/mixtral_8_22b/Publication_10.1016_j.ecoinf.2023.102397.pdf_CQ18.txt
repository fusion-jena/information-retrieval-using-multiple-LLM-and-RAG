Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

All the parameters used for all the recognizers are shown in Table S2. 

2.6. Data analysis: Target set 

The analyses of the target set aimed to detect possible differences 
among the software programs in evaluating the results with regard to 
two different aspects: (1) the possibility that different habitats produce 
more or fewer song detections per day and (2) the possibility that great 
tits in different habitats have different times of vocal activity, regardless 
of the total number of songs. Given that the days and sites used to obtain 
the  data  for  the  training  and  target  sets  were  the  same  for  all  three 
software programs, any differences among the software programs must 
be exclusively due to differences or biases in their detection patterns. On 
the other hand, for the general comparison of the software, we measured 
the  time  each  software  spent  on  analyzing  the  entire  target  dataset 
(32,640 recordings).

over which the quality measures and results fail to continue improving. 
Benkendorf and Hawkins (2020), analyzing the effect of sample size on 
deep neural networks, similarly concluded that by increasing the num-
ber of hidden layers, which can be interpreted as adding more training 
recordings in our software, the neural network can largely improve the 
performance of the model.

of the recognizer, we chose the optimal cut-off thresholds (in the case of 
identical F-scores, we used the mean of the thresholds); (4) we ran a 
random forest model using the best replicate and threshold on the test 
set and evaluated its performance; and (5) we implemented the same 
recognizer and threshold to the target set to obtain biologically relevant 
data.

improved the similarity of the results from the different software pro-
grams, differences among the software were present even with a fairly 
solid number of training records. Our results suggest that reliable bio-
logical  interpretations  might  only  be  attained  with  recognizers  that 
boast very high values of all the evaluation metrics (F-score, precision, 
and  recall).  In  fact,  even  with  very  good  precision  and  F-scores,  low 
values  of  recall  can  lead  to  significantly  different  interpretations  of 
relative abundance and time of activity. Therefore, as has been noted 
with  bats  (Lemen  et  al.,  2015;  Russo  and  Voigt,  2016;  Rydell  et  al., 
2017), great caution must be taken in the interpretation of the results 
from bird PAM using automated recognition. 

Funding

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The strategy implemented to monitor the model performance during training involves several steps. Firstly, the authors selected the best replicate based on the highest F-score achieved on the validation set. Secondly, they determined the optimal cut-off thresholds for the recognizer, choosing either the single value or the mean of identical F-scores. Thirdly, they ran a random forest model using the best replicate and threshold on the test set to evaluate its performance. Lastly, they applied the same recognizer and threshold to the target set to gather biologically relevant data. This approach ensures that the model's performance is continuously monitored throughout the training process, allowing for adjustments and improvements as needed.
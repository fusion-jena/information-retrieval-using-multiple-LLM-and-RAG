Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Our "shallow learning" approach  [16]  is  based  on dietionary  learn 
ing.  We  use the spherical k-means  algorithm  [18]  to learn a dietio 
nary  of representative code words, and  then encode our data against 
the learned dictionary. In this variant ofthe k-means c1ustering al go 
rithm  [19]  the centroids are constrained  to  have  unit  L2  norm  (pre 
venting  them  from  becoming arbitrarily  large  or  small), and  repre 
sent  the  distribution  of meaningful  directions  in  the  data.  The  al 
gorithm is  efficient and  scalable, competitive with  slower and  more 
complex  techniques  such  as  sparse  coding,  and  it  has  been  shown 
that its resulting set of centroids can  be  used as  bases  (a dietionary) 
for  mapping  new  data  into  a  feature  space  which  reflects  the  dis 
covered regularities [20,  18, 8].  Spherical k-means (SKM) has been 
exploited  for  classifying  musie  [21],  birdsong  [8],  urban  (environ

The  automated  classification  of  migrating  birds'  ftight  calls  has 
the  potential  to  yield  new  biologieal  insights  and  conservation  ap 
plications  for  birds  that  vocalize  during  migration.  In  this  paper 
we  explored  two  state-of-the-art classification  techniques  for large 
vocabulary  bird  species  classification  from  flight  calls:  a  "shallow 
learning" unsupervised dietionary learning  method and  a deep con 
volutional  neural  network  combined  with  data  augmentation.  The 
models  were evaluated on a dataset of 5428  ftight  calls from 43  dif 
ferent  species, and  were  compared  against  a  baseline  model  based 
on  MFCCs.  We  showed  that  the  two  models  perform  comparably, 
yielding  a  mean  classification  accuracy  of 0.94  and  significantly 
outperforming  the  MFCC  baseline  (0.85).  We  also  compared  the 
performance of the CNN model  with  and  without augmentation and

During training the model optimi zes cross-entropy loss via mini-
batch  stochastic  gradient descent  [26].  We  use  a constant learning 
rate  of 0.01  and  apply  dropout [27]  to  the  input of the  last two lay 
ers  with  probability 0.5.  L2-regularization  is  applied to  the  weights 
of the last two layers  with  a penalty  factor  of 0.001.  The  model  is 
trained for  100 epochs and is  checkpointed after each epoch.  A val 
idation  set  is  used to  identify  the  parameter setting  (epoch)  achiev 
ing the  highest classification accuracy.  The CNN is  implemented  in 
Python using Lasagne [28] , and data stream  multiplexing (for train 
ing)  is implemented using Pescador [29].

Index  Terms- Convolutional  neural  networks,  bioacoustics, 

flight calls, deep learning, data augmentation. 

1.  INTRODUCTION

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't any explicit information about where the code repository of the deep learning pipeline is available in the given context. It could be possible that the authors did not provide their code publicly or they might have mentioned it elsewhere outside of the provided text. Therefore, I cannot determine the exact location of the code repository.
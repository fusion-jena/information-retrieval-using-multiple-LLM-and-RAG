Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.2. Model performance MLMs comparisons 

The performance of each MLM was assessed using several metrics, 
such as AUC, TSS, Kappa, and phi. The outcomes of these evaluations are 
presented  in  Table  3.  Additionally,  Fig.  4  visually  illustrates  the  pre-
dictive performance of each model with AUC values for both training 
and test datasets. Upon analysing the four performance metrics, it was 
evident that the selected models exhibited high consistency, except for 
the  CART.  The  performance  of  the  remaining  models  displayed  only 
slight variations within a small range, as shown in Table 3. Additionally, 
there was no discernible variation in the classifiers’ prediction accuracy 
according to the Kruskal-Wallis statistic (p > 0.05).

Validation of MLMs is a critical step in assessing each model’s pre-
dictive capabilities, as Mohammadi et al. (2014) emphasized. To eval-
uate  the  accuracy  and  reliability  of  each  MLM,  we  employed  robust 
statistical  measures  provided  by  the  ‘sdm  1.1–8’  package.  These  are 
kappa (Allouche et al., 2006), true skill statistics (TSS; Allouche et al., 
2006), and area under the receiver operator characteristics (ROC) curve 
(AUC; Hanley and McNeil, 1982). Model complexity was evaluated for 
each case by computing the difference between the AUC (train) and AUC 

(CV), represented as Δ AUC (Jarnevich et al., 2018). The Δ AUC scores 
from each model offer insights into their sensitivity to the training data 
(Jarnevich  et  al.,  2018).  Moreover,  the  Kruskal-Wallis  test  was  con-
ducted to determine whether a significant difference in prediction ac-
curacy exists among the classifiers. 

2.5. Ensemble model

Among the five MLMs evaluated, the AUC analysis revealed that RF 
and SVM performed remarkably well with training AUC values of 0.868 
± 0.002 and 0.857 ± 0.001, respectively. MARS followed this, and BRT 
gained  training  AUC  scores  of  0.829  ± 0.001  and  0.809  ± 0.002, 
respectively,  while  CART  exhibited  poor  performance  with  a  low 
training AUC of 0.781 ± 0.001. Similar trends were observed for TSS, 
where  RF,  SVM,  MARS,  BRT  outperform  CART  (Table  3).  All  models 
exhibited  robust  agreement  with  the  predicted  results  in  terms  of  ac-
curacy.  Additionally,  when  considering  the  kappa  and  phi  for  recog-
nizing  accurate  forest  fire  pixels,  all  models,  RF,  SVM,  MARS,  BRT 
outperformed CART (Table 3). The Δ  AUC was lowest for CART with 
0.002, followed by MARS and BRT with both values of 0.003. The Δ AUC 
was  highest  for  RF  and  SVM  with  the  values  of  0.022  and  0.021,

By default, we utilized the recommended parameters for all MLMs, 
except for Random Forest (RF), where we expressly set the number of 
trees to 500 as an additional parameter. A higher number of trees helps 
reduce  the  resulting  model’s  bias.  We  calibrated  and  validated  each 
model after fitting each MLM with optimized parameters using a 10-fold 
cross-validation (CV) design. For each run of the cross-validation pro-
cedure, we reserved 30% of the forest fire occurrence data as a valida-
tion  set,  with  the  remaining  70%  of  the  data  being  used  to  train  the 
model. This rigorous validation methodology was adopted following the 
method explained by Eskandari et al. (2021). The approach generated 
50 resulting models, consisting of 10 models for each machine learning 
algorithm.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To monitor the model performance during training, a 10-fold cross-validation (CV) design was implemented. In each run of the cross-validation procedure, 30% of the forest fire occurrence data was reserved as a validation set, while the remaining 70% of the data was used to train the model. This process resulted in 50 models, comprising 10 models for each machine learning algorithm. The performance of each model was then assessed using various metrics such as AUC, TSS, Kappa, and phi. The outcomes of these evaluations were presented in Table 3 and visualised in Figure 4. The performance of the models was found to be consistent, except for the CART model which showed lower performance compared to others. No significant differences in prediction accuracy were found among the classifiers according to the Kruskal-Wallis statistic.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics72(2022)1019145L. Hysen et al.                                                                                                                                                                                                                                   

Fig. 4. Performance metrics (i.e., AUC, TSS) for each ensemble model.

2.9. Evaluating individual modeling approaches 

To ensure that all models were evaluated with the same relatively 
independent data, we used a common approach to split the data into 
evaluation  and  training  datasets  prior  to  training  the  models.  We  set 
aside the evaluation dataset and used it to evaluate individual averaged 
modeling approaches and the final ensemble models, hereafter referred 

To assess the optimal number of background points for each indi-
vidual modeling approach, we calculated the True Skill Statistic (TSS) 
and the Area Under the Receiver Operating Curve (AUC-ROC) for each 
model using the external evaluation datasets set aside prior to training 
models (Allouche et al., 2006; Hao et al., 2020; Shabani et al., 2016). 
The  True  Skill  Statistic  is  based  on  a  binary  classification  of  model

points. Then, to obtain presences and background points to use during 
external evaluation, we set aside one fold. We then used the remaining 
four folds for internal cross-validation during model training.

(h) Ensemble models using an averaging approach, each time using all six component models. For the optimized ensemble model, for each component model include 
the one trained with the number of background points with the highest AUC. For the 1×, 10×, and 10 K ensemble models, include all component models built with 
the 1×, 10×, and 10 K background points, respectively. (i) Evaluate ensemble models using the evaluation dataset (using AUC and TSS here). (j) Compare ensemble 
model AUCs, TSSs, sensitivities, and specificities and choose the model that performs the best for the data. (For interpretation of the references to colour in this figure 
legend, the reader is referred to the web version of this article.)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The study uses two primary metrics to evaluate the performance of their models: the True Skill Statistic (TSS) and the Area Under the Receiver Operating Curve (AUC-ROC). These metrics are used to calculate the performance of both individual modeling approaches and the final ensemble models. The True Skill Statistic is based on a binary classification of model points, while the AUC-ROC measures how well a model can distinguish between classes. Additionally, sensitivity and specificity are also mentioned as part of the comparison when choosing the best performing model. However, traditional machine learning metrics such as accuracy, precision, or recall are not explicitly stated in the given context.
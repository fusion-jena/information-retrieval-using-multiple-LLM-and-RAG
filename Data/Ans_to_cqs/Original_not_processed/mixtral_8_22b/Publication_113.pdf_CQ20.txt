Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

ing model, such as a neural network, and try to improve that model by 

alization algorithms like t-SNE (van der Maaten & Hinton, 2008), can 

selecting training samples. Active learning algorithms typically start 

be regarded as embedding functions.

training the underlying model on a small, randomly selected labelled 

Deep  neural  networks  are  frequently  used  for  dimension-

set of data samples. After training the initial model, various criteria 

ality  reduction:  the  input  to  a  deep  network  often  has  many 

can  be  employed  to  select  the  most  informative  unlabelled  sam-

values, but layers typically get smaller throughout the network, 

ples to be passed to the oracle for labelling (Settles, 2009). Among 

and the output of a layer can be viewed as a reduced represen-

the  most  popular  query  selection  strategies  for  active  learning  are 

tation  of  the  network's  input.  In  this  paper,  we  use  two  com-

Xu, Z., Yu, K., Tresp, V., Xu, X., & Wang, J. (2003). Representative sampling 
for  text  classification  using  support  vector  machines.  In  European 
conference on information retrieval (pp. 393–407). Springer.

Wiley & Sons.

Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learn-
ing with neural networks. In Advances in neural information processing 
systems (pp. 3104–3112).

Swanson, A., Kosmala, M., Lintott, C., Simpson, R., Smith, A., & Packer, 
C.  (2015).  Snapshot  serengeti,  high-frequency  annotated  camera 
trap images of 40 mammalian species in an african savanna. Scientific 
Data, 2, 150026.

Nielsen,  1989;  Robbins  &  Monro,  1951;  or  modern  enhancements 

Norouzzadeh  et  al.  (2018)  that  harnessed  3.2  million  labelled 

of it; Kingma & Ba, 2014; Tieleman & Hinton, 2012) calculates the 

training  examples  to  produce  90.9%  accuracy  (using  ResNet-50 

contribution  of  each  parameter  to  the  loss  value,  then  adjusts  the 

architecture) at species classification, but with a 99.5% reduction 

parameters so that the loss value is minimized. The SGD algorithm 

in manually annotated training data. We also expect our method to 

is an iterative algorithm, i.e. it is applied many times during training, 

generalize better to new locations because we systematically filter 

including multiple times for each image in the dataset. At every it-

out the background pixels.

eration of the SGD algorithm, the parameters take one step toward

1.  Every  deep  learning  algorithm  has  numerous  hyperparameters, 

subset of unlabelled samples such that the loss value of the selected 

options  selected  by  the  data  scientist  before  machine  learning 

subset  is  close  to  the  ‘expected’  loss  value  of  the  remaining  data 

begins.  For  this  paper,  we  used  well-known  values  of  hyperpa-

points (Sener & Savarese, 2017). At 14,000 labels, we match the ac-

rameters  to  train  our  models.  Tuning  hyperparameters  is  likely 

curacy of Norouzzadeh et al. for the same architecture; compared to 

to  improve  results.  In  particular,  we  only  used  the  ResNet-50 

the 3.2 million labelled images they trained with, our results represent 

architecture  for  embedding  and  a  simple  two-layer  architecture 

over a 99.5% reduction in labelling effort to achieve the same results.

for  classification.  Further  probing  of  the  architecture  space 

may  improve  results.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide specific details about the hardware resources used for training the deep learning model. However, based on common practices in deep learning, it is highly probable that either Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs) were utilized due to their superior performance in handling large-scale computations required for training deep learning models. These specialized processors accelerate matrix operations, which form the backbone of deep learning algorithms, enabling faster training times and improved efficiency. Nonetheless, without explicit mention in the provided context, it cannot be definitively stated whether GPUs, TPUs, or other hardware resources were employed for training the deep learning model.
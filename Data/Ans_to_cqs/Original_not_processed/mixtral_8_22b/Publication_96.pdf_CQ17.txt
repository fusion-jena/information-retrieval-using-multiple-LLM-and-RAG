Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

More recently, there has been a great effort to automate the process 
of classification/mapping of specific targets using Deep Learning (Lecun 
et al., 2015) algorithms, such as the Deep Neural Network (DNN), which 
includes  a  learning  algorithm  based  on  an  artificial  neural  network 
(Langford et al., 2019). In this study, we developed a semi-automatic 
methodological approach for mapping burned areas in the entire Cer-
rado  biome  in  Brazil  using  Deep  Learning  techniques  available  on 
Google Cloud computing and Landsat 8 imagery, which consist of eleven 
spectral bands with a spatial resolution of 30 meters for bands 1-7 and 9, 
15 meters for band 8, and 100 meters for bands 10 and 11. We did not 
use  Landsat-7  imagery  because  of  the  data  gaps  observed  images  ac-
quired after 2003, which would substantially affect the spatial coverage 
of  our  analysis.  We  assessed  accuracies  and  compared  our  mapping 

2. Material and methods 

2.1. Study region

Since  deep  learning  methods  require  a  powerful  computational 
processing, we conducted our analysis using graphics processing units 
(GPUs)  and  specialized  hardware  components  for  running  parallel 
arithmetic operations (Goodfellow et al., 2016). The access to GPUs in a 
virtual  machine  environment  was  implemented  on  the  Google  Cloud 
suite  of  cloud 
Platform 

(https://console.cloud.google.com),  a 

3.1. Optimal period of burn scar mapping 

The fire hotspot data analysis provided by INPE (http://www.inpe. 
br/queimadas/bdqueimadas)  indicates  that  most  of  the  fire  events  in 
the  Cerrado  biome  occurred  between  May  and  December,  showing  a 
peak in September 2017 (Fig. 4). Based on it, we focused our mapping 
analysis of burn scars on the period between May and December, which 
comprised 98% of fire hotspots detected in 2017 by INPE. 

3.2. Burned areas in 2017

We used Landsat 8 Operational Land Imager (OLI) satellite images 
and Deep Neural Network models to detect and map burned areas within 
the  Cerrado  biome.  The  image  processing  and  classification  followed 
four steps as follows: (1) collecting spectral training samples of burned 
and non-burned areas for the entire study area, well distributed in space, 
(2) training the deep learning models, (3) developing model prediction, 
and (4) model assessment (validation and concordance analysis). Fig. 2 
uses  step-by-step  bases  to  show  all  details  of  our  methodological 
approach for detecting and mapping burned areas in the Cerrado biome 
in 2017. 

2.4.1. Training samples dataset 

The spectral training samples of burned area (BA) and non-burned

2.4. Burned area mapping 

Deep  Neural  Network  (DNN)  is  a  learning  algorithm  based  on  an 
artificial neural network with multiple hidden layers (Langford et al., 
2019).  The  DNN  models  use  hierarchical  data  processing,  where  the 
input data in each module (called the hidden layer) results in an output 
that  is  an  input  for  the  next  module,  connected through  weights  and 
biases  whose  values  were  learned  during  the  training  of  the  network 
(Bramhe et al., 2018).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, no explicit mention of regularization methods used to prevent overfitting in the deep learning pipeline is mentioned. However, some common regularization methods used in deep learning pipelines include Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training, which helps to avoid overfitting by preventing complex co-adaptations on training data. This can be thought of as a way to automatically and adaptively regularize neural networks by adding noise to its inputs (Srivastava et al., 2014).

On the other hand, L2 Regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the magnitude of the weights. This encourages smaller weights, effectively limiting the complexity of the model and reducing overfitting (Ng, 2017).

These regularization methods could have potentially been applied in the development of the Deep Neural Network (DNN) models mentioned in the context, but without further information, their usage cannot be confirmed.
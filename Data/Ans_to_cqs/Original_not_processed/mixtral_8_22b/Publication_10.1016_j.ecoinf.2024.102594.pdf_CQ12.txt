Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During the DeepLabV3+ model training process, feeding large-size 
images directly to the network may lead to memory overflow. There-
fore, it is necessary to cut all remote sensing images and semantic labels 
of land cover classes into a series of regular image blocks for input. To 
maximize the sample size and maintain consistency with the cropping 
approach in the model prediction stage, we set the cropping size to 32 ×
32  pixels  with  a  redundancy  rate  of  0.5.  In  continuing,  70%  of  the 
dataset  was  used  for  training  and  the  remaining  for  validation.  The 
training set was enhanced by: (i) Flipping the images and labels along 
the X or Y axis; (ii) Exchanging between multiple image channels while 
the  labels remain unchanged; (iii)  Randomly rotating  the images and 
◦
labels by 90
; and (iv) randomly adding noise to the 
images  while  maintaining  the  labels  unchanged  (Ye  et  al.,  2022).

3.1. DeepLabV3+ algorithm 

DeepLabV3+ is a CNN algorithm for semantic segmentation, which 
is the latest version of the DeepLab algorithms family (Scepanovic et al., 
2021).  In  the  encoding  stage,  DeepLabV3+ introduces  dilated  convo-
lutions,  which expands  the  receptive field  without losing information 
(Fig.  3).  The  Atrous  spatial  pyramid  pooling  module  with  dilated 
convolution  has  been  mainly  designed  to  appropriately  incorporate 
multi-scale information. During decoding, it is capable of enhancing the 
accuracy  of  segmentation  boundaries  by  further  integrating  low-level 
features with high-level features (Liu et al., 2021a).

Fig. 12. Growth rate of the overall classification accuracy of the satellite image 
models compared to the UAV image models. 

classification  results  of  the  MRS_DeepLabV3+ model  with optimal  SP 
selection  were  superior  to  the  DeepLabV3+ model  with  an  accuracy 
enhancement  of  about  3%  (Table  4).  This  approach  was  capable  of 
solving  the  problem  of  fragmented  spatial  edge  information  often 
encountered when using pixel-based CNNs to extract target categories 
(Liu et al., 2019). Compared to previous studies on the classification of 
wetlands using DeepLabV3+ (Buchsteiner et al., 2023; Gonzalez-Perez 
et al., 2022), the classification results of this study showed a significant 
lack of the salt-and-pepper phenomenon. The MRS_DeepLabV3+ model 
was  not  only  capable  of  correcting  the  misclassifications  made  by 
DeepLabV3+ but also enhancing the delineation of PlatSpe and SurfObj

, and 270

, 180

◦

◦

MRS_DeepLabV3+ integrates  the  classification  results  (in  TIF 
format) predicted by the DeepLabV3+ algorithm and the segmentation 
results (in SHP format) obtained from MRS using Python programming 
language. The main specific steps of this approach have been provided in 
the following: First, let us count the number of pixels of each category in 
the current segmentation object area. After that, we select the category 
with the largest area. Finally, assign the largest category to the current 
object. The multiple classification models were then constructed based 
on MRS_DeepLabV3+ (Table 3). 

Table 3 
Detailed information on the classification models.  

Class 

Amount  Model 

Sensor 

Spatial 
resolution 
(m) 

Hierarchy of 
spatial 
resolution 

UAV image 
models 

7 

Satellite 
image 
models 

7 

UAV_2m 
UAV_2.1 
m 
UAV_2.5 
m 
UAV_5.8 
m 
UAV_8m 
UAV_10m 
UAV_16m 

GF1_2m 

ZY3_2.1 
m 
ZY1_2.5 
m 
ZY3_5.8 
m 

GF1_8m 

ZY1_10m 

GF1_16m 

P4M

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning model used in the pipeline is a Convolutional Neural Network (CNN) called DeepLabV3+. It is specifically employed for semantic segmentation tasks. The DeepLabV3+ model incorporates dilated convolutions during its encoding phase, expanding the receptive field without loss of information. Additionally, it utilizes an Atrous Spatial Pyramid Pooling module with dilated convolution to effectively integrate multi-scale information. During the decoding phase, it improves boundary segmentation accuracy by combining low-level features with high-level ones.
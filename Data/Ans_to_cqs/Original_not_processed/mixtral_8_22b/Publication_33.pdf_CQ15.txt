Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

to calibrate Deep Learning models using a single parameter for all  classes35. This parameter is used, instead of 
the traditional softmax function, to convert the vector output from the neural network into a real probability.

https ://arXiv .org/1712.04621 .

 42.  Goodfellow, I., Bengio, Y., Courville, A. & Bengio, Y. Deep Learning (MIT Press, Cambridge, 2016).
 43.  Abadi, M. et al. Tensorflow: A system for large-scale machine learning. OSDI 16, 265–283 (2016).
 44.  He, K., Zhang, X., Ren, S., & Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer 

Vision and Pattern Recognition 770–778 (2016).

 45.  Sarle, W. S. Stopped training and other remedies for overfitting. Computing Science and Statistics, 352–360 (1996).
 46.  Hintze, J. L. & Nelson, R. D. Violin plots: A box plot-density trace synergism. Am. Stat. 52(2), 181–184 (1998).
 47.  Catford, J. A., Bode, M. & Tilman, D. Introduced species that overcome life history tradeoffs can cause native extinctions. Nat. 

Commun. 9(1), 2131 (2018).

they apply convolutions, i.e. they modify the value of one pixel according to a linear weighted combination of the 
values of the neighbor pixels. In our case, each image used to train the CNN is coded as 3 matrices with numeric 
values describing the color component (R, G, B) of the pixel. The optimization of the parameters of the CNN is 
achieved during the training through a process called back-propagation. Back-propagation consists of automati-
cally changing parameters of the CNN through the comparison between its output and the correct class of the 
training element to eventually improve the final classifications rate. Here we used a 100-layer CNN based on the 
 TensorFlow43 implementation of  ResNet44. The ResNet architecture achieved the best results on ImageNet Large

Building  the  convolutional  neural  network.  Convolutional  neural  networks  (CNNs)  belong  to  the 
class  of  DLAs.  For  the  case  of  species  identification,  the  training  phase  is  supervised,  which  means  that  the 
classes  to  identify  are  pre-defined  by  human  experts  while  the  parameters  of  the  classifier  are  automatically 
optimized  in  order  to  accurately  classify  a  “training”   database24.  CNNs  are  composed  of  neurons,  which  are 
organized in layers. Each neuron of a layer computes an operation on the input data and transfers the extracted 
information to the neurons of the next layer. The specificity of CNNs is to build a descriptor for the input image 
data and the classifier at the same time, ensuring they are both optimized for each  other42. The neurons extract-
ing the characteristics from the input data in order to build the descriptors are called convolutional neurons, as

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention any specific optimization technique such as Stochastic Gradient Descent (SGD) or Adam. However, it does refer to the use of back-propagation for optimizing the parameters of the CNN. Back-propagation is an algorithm used to efficiently calculate gradients in a neural network, which are then used to update the weights of the network via some form of gradient descent. Therefore, although not directly stated, it can be inferred that some form of gradient descent optimization technique was likely employed in conjunction with back-propagation.
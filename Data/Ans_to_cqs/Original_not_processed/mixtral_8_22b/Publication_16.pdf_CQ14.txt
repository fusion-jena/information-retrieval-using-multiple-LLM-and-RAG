Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The results presented in this section were obtained over 100 
epochs. Figure 9 shows the loss of the model using both the 
test  and  validation  data  during  model  training.  The  figure 
shows  that  there  was  no  overfitting  during  training  and  that 
the dropout layers helped with model regularisation. Although 
model convergence was achieved early in the training session 
the loss shows continuing decreases throughout the specified 
epochs. 

Figure 10. Train and Validation Accuracy During the 
Training Session. 

Table 1 shows the performance metrics obtained using the 
test data. The best performing class was the Eurasian Collard 
Dove achieving a Sensitivity of 0.86 and a Specificity of 0.90. 
The  worst  performing  class  was 
the  Lesser  Spotted 
Woodpecker  where  the  model  attained  a  Sensitivity  of  0.58 
and a Specificity of 0.91. 

Table 1. Performance Metrics for Test Set 

Species 

Sensitivity 

Specificity

III. 

MATERIALS AND METHODS 

In  this  section  the  dataset  used  in  the  study  is  presented 
along with the modelling approach taken and the evaluation 
metrics used to evaluate the trained model. The section also 
discusses data pre-processing using the Librosa library. Keras 
and TensorFlow 2.2 are utilised as the backend and an Nvidia 
2070 super GPU with 8GB of memory is utilised to accelerate 
model training. In addition, the proposed inferencing pipeline 
is discussed along with the associated technologies. 
A.  Data Collection and Description

(3) 

The MLP is configured with a filter size of 2 and is used 
with Backpropagation as the learning algorithm and Adam as 
the optimiser. A dropout value of 50% has been used in the 
first  three  layers  to  improve  generalisation  and  reduce 
overfitting. The first three layers are composed of 256 nodes 
while the final layer is equal to the number of classes in our 
dataset. The model summary is shown in figure 5. 

overfitting.  This  section  concludes  the  methods  used  in  this 
paper to train the model. 

The  performance  of  the  trained  model  is  measured  using 
Sensitivity,  Specificity,  Precision  and  Accuracy.  The 
Sensitivity  describes 
the 
Specificity describes the true negative rate. Precision is used 
to show the number of correctly classified species. 
E.  Model Inferencing 

true  positive  rate  while 

the

V. 

DISCUSSION 

In this paper we proposed a methodology and pipeline for 
the  classification  of  five  common  UK  birds  â€“  a  Common 
Wood  Pigeon,  Eurasian  Collared  Dove,  Great  Tit,  House 
Sparrow  and  Lesser  Spotted  Woodpecker.  By  extracting 
features using MFCC and an MLP for classification we were 
able to achieve encouraging results using a restricted amount 
of data with limited pre-processing. The results show that bird 
species can be detected and classified with a reasonable degree 
of  accuracy  to  rapidly  speed  up  the  time  taken  to  manually 
classify acoustic data. The performance values across all bird 
classes  are  encouraging  and  in  many  cases  are  capable  of 
detecting birds with high Sensitively and Specificity values. 

are 

there 

to  both

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given information, it does not explicitly state how the hyperparameters of the model were optimized. There is no mention of techniques such as grid search or random search being employed. However, some details about the configuration of the model are mentioned. For instance, the Multilayer Perceptron (MLP) uses a filter size of 2, backpropagation as the learning algorithm, and Adam as the optimizer. Additionally, a dropout value of 50% is applied in the first three layers to enhance generalization and mitigate overfitting. These specifications suggest that some form of optimization might have occurred, but without explicit confirmation, it cannot be definitively stated which method was utilized.
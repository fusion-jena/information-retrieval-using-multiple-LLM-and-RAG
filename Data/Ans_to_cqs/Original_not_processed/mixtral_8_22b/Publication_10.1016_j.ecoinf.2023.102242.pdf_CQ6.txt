Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.3. Data processing

4.4. Classification method

refine  the  subsequent  classification  process.  Future  research  should 
focus on this issue to yield more reliable results.

The  processing  of  high-resolution  remote  sensing  data,  which  en-
compasses  a  plethora  of  feature  information  (Girshick  et  al.,  2014), 
demands  significant  time  and  costs.  This  presents  considerable  chal-
lenges for many researchers (Modzelewska et al., 2020). Prior to clas-
sification, samples require training. The representativeness and quantity 
of  these  training  samples  significantly  influence  the  accuracy  of  the 
classification  models.  Platinum  (Ting,  2016)  demonstrated  that  when 
the training samples are not sufficiently typical, other categories can be 
easily misclassified into the sampleâ€™s category. If the number of training 
samples is insufficient, network training may not meet the requirements. 
Conversely, an excessive number of training samples can lead to data 
redundancy, reducing the efficiency of model training (Shi, 2018). To 
address  this  issue,  this  study  employed  feature  optimization  tools  to

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps are typically involved. These steps aim to prepare the dataset for efficient and accurate model training. Some common preprocessing techniques include:

1. Normalization: This involves rescaling the input features so they fall within a specific range, often between 0 and 1 or -1 and 1. Normalization helps prevent any single feature from dominating the others due to its scale, ensuring all features contribute equally during training.

2. Scaling: Similar to normalization, scaling adjusts the values of numeric columns in the dataset to a standard scale without distorting differences in the ranges of values or losing information. Common methods include Min-Max scaling and Standardization (Z-score normalization).

3. Cleaning: Data cleaning involves identifying and handling missing, incorrect, or inconsistent data points. Techniques such as imputation (filling missing values based on existing data patterns) and outlier detection/removal help maintain data integrity and improve model performance.

4. Feature Selection/Extraction: Selecting relevant features or extracting new ones can enhance model performance by focusing on important aspects of the data. Methods like Principal Component Analysis (PCA) reduce dimensionality while preserving essential information.

5. Splitting Dataset: Dividing the dataset into separate sets for training, validation, and testing ensures the model generalizes well to unseen data. A common practice is using 70-80% of the data for training, 10-15% for validation, and 10-15% for testing.

In the given context, it appears that the authors have used feature optimization tools to address challenges related to training sample representation and quantity. However, the exact preprocessing steps taken before training their deep learning model aren't explicitly mentioned. Based on common practices, one could infer that some combination of normalization, scaling, cleaning, feature selection/extraction, and splitting datasets might have been applied.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

both training and testing. A critical point to highlight is that models are 
retrained  for  each  fold,  preventing  previous  fold’s  information  from 
being transferred. We utilized the pretrained ResNet-34 (Which consists 
of  a  34-layer  convolutional  neural  network)  architecture  which  is  a 
variant of the ResNet (Residual Network) family, widely used for deep 
learning tasks, particularly in computer vision (He et al., 2016). ResNet- 
34 was used inside the fast.ai framework (Howard and Gugger, 2020) to 
leverage  existing  knowledge,  this  is  particularly  useful  in  our  dataset 
since its small size. Transfer learning generally consists in using a model 
pre-trained  on  broad  datasets,  like  ImageNet  (Deng  et  al.,  2009),  to 
specialized tasks with more limited data. As part of the cross-validation 
of the Leave-One-Group-Out approach, the test directory containing the 
multitemporal transect was temporarily moved to test, and DataLoaders

2.5.1. Classification problem (Q1): Deep learning applied to 2D LiDAR 
images for Forest classification 

We  employed  the  fast.ai  platform  (Howard  and  Gugger,  2020)  to 
train and validate models for the classification of 2D point cloud images, 
utilizing  a  LOGO  (Leave-One-Group-Out)  cross-validation.  In  this 
approach, our target classes were "Plateau", "White-sand", and "Ripar-
ian",  designated  as  "p",  "w",  and  "r"  respectively.  In  each  iteration,  a 
complete 450 m multitemporal transect was omitted from the training 
process  (the  grouping  factor  giving  the  lack  of  independence  in  the 
multi-temporal data) and subsequently tested. This approach ensured an 
assessment  of  the  model’s  performance,  accounting  for  our  data  con-
straints and guaranteeing that every multitemporal transect undergoes

used weighted F1 score. The F1 score is a harmonic mean of precision 
and recall, providing a balanced measure of both the false positives and 
false negatives in the classification results. To calculate the F1 score, we 
first  obtained  the  true  labels  and  predicted  majority  classes  for  each 
transect from the aggregated data. We then used the Python Scikit-learn 
(Pedregosa et al., 2011) library to compute the confusion matrix for the 
true and predicted labels, considering the three classes: "p", "w", and "r". 
Finally, we calculated the weighted F1 score using the f1_score function 
from  Scikit-learn, specifying the  true  labels,  predicted labels,  and the 
average parameter set as "weighted".

3. Results

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The performance of the deep learning model is evaluated using the weighted F1 score. The F1 score is a harmonic mean of precision and recall, offering a balanced assessment of both false positives and false negatives in the classification outcomes. To determine the F1 score, the true labels and predicted majority classes for each transect are first derived from the combined data. Subsequently, the Python Scikit-learn library is employed to generate the confusion matrix for the actual and anticipated labels, taking into account the three categories: "p", "w", and "r". Lastly, the weighted F1 score is computed utilizing the f1_score function from Scikit-learn, setting the parameters as the true labels, predicted labels, and average parameter as "weighted".
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Sensors 2021, 21, 343

11 of 18

7,047,754 learnable parameters gave a F1-score of 84.93% which is even lower. CNN
architectures with many parameters (more than 20,000,000) such as ResNetV50 [38] and
InceptionNetV3 [39] gave a high training accuracy, but a lower validation F1-score of 69.1%
and 81.7%, respectively. This result indicates overﬁtting and that more training data are
needed when such large deep learning networks are used. A very high F1-score of 96.6%
was ﬁnally achieved by transfer learning on ResNetV50 using pretrained weights and only
training the output layers. This indicates that the state-of-the-art was able to outperform
our proposed model, but requires pretrained weights with many more parameters.

2.2.4. Summary Statistics

To ﬁnd the best CNN architecture for species classiﬁcation, different hyperparameters
were adjusted as described in Section 2.2.3. A total of 64 architectures were trained using a
dropout probability of 0.3 after the second to last hidden layer. The average F1-score for all
classes was used as a measure for a given architecture’s performance.

The ﬁve best architectures had high F1-scores, which only varied by 0.02, but had a
varying number of learnable parameters (Table 2). Compared to SGD, Adam turned out to
be the superior optimizer for training of all models. In the end, the architecture that had
a rating among the three highest F1-score but the lowest amount of learnable parameters
(2,197,578) was chosen. The reason for this is that an architecture with many parameters
and few training data would increase the risk of overﬁtting the neural network.

The chosen model shown in Figure 5 had an F1-score of 92.75%, which indicated that
the trained CNN was very accurate in its predictions. This ﬁnal architecture was chosen
because it achieved average precision, recall, and an F1-score of 93%, which indicated a
suitable model classiﬁcation.

Sensors 2021, 21, 343

10 of 18

Table 2. Ranking of the CNN architectures with highest and lowest F1 classiﬁcation scores. Rank 1 to
32 were trained using the Adam optimizer. Rank 33 to 64 were trained using the SGD optimizer. The
hyperparameters column shows values of {kernel size layer 1, kernel size last layer, convolutional
depth layer 1, convolutional depth last layer, fully connected size}.

Rating

Hyperparameters

Learnable
Parameters

F1/-Score

1.
2.
3.
4.
5.
...
31.
32.

33.
34.
...
62.
63.
64.

3, 3, 32, 128, 512
5, 1, 32, 128, 512
5, 3, 32, 64, 512
3, 3, 32, 64, 512
5, 3, 32, 128, 512

5, 1, 64, 64, 512
5, 3, 32, 32, 512

5, 3, 64, 128, 512
5, 3, 32, 128, 512

2.2.3. Moth Species Classiﬁcation

In the ﬁeld of deep learning, speciﬁc architectures of CNNs have provided particularly
positive results in many areas of computer vision [31]. CNNs use both pixel intensity
values and spatial information about objects in the image. It was a challenging task to ﬁnd
a suitable CNN architecture for classiﬁcation of moth species. Based on an investigation
of several CNN architectures [32,33], a customized network was designed inspired by the
work in [34]. Hyperparameters of the architecture were explored to ﬁnd the optimal
network architecture to classify moth species. The model was designed to be light and fast
for the purpose of being able to be executed on the embedded Raspberry Pi computer used
in the light trap.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper describes the process of finding the best CNN architecture for species classification by adjusting various hyperparameters. However, the specific hyperparameters mentioned in the text are related to the structure of the CNN rather than the training process itself. These include kernel sizes for the first and last layers, convolutional depth for the first and last layers, and the size of the fully connected layer. The table lists these hyperparameters for the top five performing architectures.

Regarding the optimization algorithm, the paper mentions that Adam was found to be superior compared to Stochastic Gradient Descent (SGD), although no further details about their respective configurations or learning rates are provided. Therefore, while we can infer that Adam was used as the optimizer, the exact learning rate remains unknown based on the provided context.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The second approach we used to extract embeddings was from the 
pre-trained  BirdNET  model.  The  original  version  of  the  model  was

Moreover,  the  lower  performance  of  feature  embeddings  in  com-
parison with MFCCs might be related to the fact that, unlike MFCCs, the 
pre-trained CNNs did not focus on the gibbon-specific frequency range. 
We  used  the  default  settings  of  the  pre-trained  models,  and  this  may 
have led to decreased performance. We opted to use the default settings 
as  modifying  the  frequency  range  would  require  re-training  of  the 
models or a substantial amount of code modification, which is time and 
resource prohibitive for most researchers; therefore, using the default 
settings makes these approaches more accessible. Also, the effectiveness 
of MFCCs tends to be higher with lower frequencies and decreases as the 
frequency increases because of the nonlinear correspondence between 
Mel frequency and Hz frequency. The frequency range of gibbon calls 
(0.4  and  1.6  kHz)  is  well-within  the  frequency  range  where  MFCCs

We expected higher performance of the embeddings, but a possible 
limitation to our study may be the use of pre-trained CNNs. This may 
have limited the potential performance of using embedding from neural 
networks.  Interestingly,  higher  classification  accuracy was  seen  when 
using  BirdNET,  a  CNN  trained  on  audio  data  of  bird  vocalizations  in 
comparison  with  VGGish  and  Wav2Vec  2.0,  two  models  trained 
respectively on a vast Youtube audio dataset and a human speech audio 
dataset. This suggests that training a model with data input of gibbon 
vocalizations might result in an even better performance of neural net-
works for individual gibbon discrimination. However, such data training 
requires a large dataset and is very computationally costly, hence why 
we chose to test pre-trained CNNs.

contribute  to  larger  data  input  for  future  training  of  deep  learning 
models (Best  et  al., 2023).  Accordingly, the  rapid growth of  artificial 
intelligence  and  deep  learning  methods  could  bring  about  important 
innovations related to feature extraction and classification with the aim 
of overcoming the current challenges halting the use of PAM for indi-
vidual acoustic discrimination and succeeding in a sustained and non- 
invasive monitoring of animal populations.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't any explicit information given regarding the specific hardware resources used for training the deep learning models mentioned in the text. These include BirdNET, VGGish, and Wav2Vec 2.0. Typically, deep learning models like these are trained using high-performance computing devices such as Graphics Processing Units (GPUs), Tensor Processing Units (TPUs), or Field Programmable Gate Arrays (FPGAs). However, without further details, it cannot be definitively stated what exact hardware resources were utilized for training these particular models.
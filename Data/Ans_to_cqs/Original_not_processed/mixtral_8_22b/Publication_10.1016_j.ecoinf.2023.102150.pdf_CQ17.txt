Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Minar, M.R., Naher, J., 2018. Recent Advances in Deep Learning: An Overview URL:htt 
p://arxiv.org/abs/1807.08169https://doi.org/10.13140/RG.2.2.24831.10403, doi: 
10.13140/RG.2.2.24831.10403,arXiv:1807.08169v1. 

M¨olle, J.P., Kleiven, E.F., Ims, R.A., Soininen, E.M., 2022. Using subnivean camera traps 
to study Arctic small mammal community dynamics during winter. Arct. Sci. 8, 
183–199. https://doi.org/10.1139/AS-2021-0006/ASSET/IMAGES/AS-2021- 
0006TAB2.GIF. 

Mos, J., Hofmeester, T.R., 2020. The Mostela: an adjusted camera trapping device as a 
promising non-invasive tool to study and monitor small mustelids. Mamm. Res. 65, 
843–853. https://doi.org/10.1007/S13364-020-00513-Y. 

Norouzzadeh, M.S., Morris, D., Beery, S., Joshi, N., Jojic, N., Clune, J., 2021. A deep

Table 1 
Number of training images, validation images (used for model validation during 
model  training)  and  out-of-sample  test  images  (used  for  external  model  vali-
dation after training was finished) as well as number of new images selected 
from  the  images  taken  between  summer  2020  and  summer  2021  for  model 
retraining.  

Class 

Number of 
training 
images 

Number of 
validation 
images 

Number of 
out-of-sample 
test images 

Number of new 
training images 
for model 
retraining 

Bad 

6453 

quality 

Bird 
Empty 
Least 

weasel 
Lemming 
Shrew 
Stoat 
Vole 
TOTAL 

3382 
9444 
1725 

9449 
9265 
4024 
9894 
53636 

677 

219 
979 
98 

967 
962 
438 
1024 
5364 

549 

119 
3301 
69 

647 
584 
64 
919 
6252 

306 

195 
533 
424 

449 
416 
425 
528 
3276

Fig. 4. Example images of a vole (A), a lemming (B), a stoat (C), a least weasel (D), a shrew (E), and a bird (F).  

epochs with a one-cycle learning rate policy with a minimum learning 
rate of 0.000001 and a maximum learning rate of 0.001 (Smith, 2018). 
We explored different hyper-parameters and choose the ones that gave 

the best results for training the final model. All images were resized to 
224 x 224 pixels previous to training and image augmentation (shifts, 
horizontal flips, rotations, zooms and shears) was applied to expand the 

EcologicalInformatics76(2023)1021505H. B¨ohner et al.

Kavzoglu, T., 2009. Increasing the accuracy of neural network classification using refined 
training data. Environ. Modell. Softw. 24, 850–858. https://doi.org/10.1016/J. 
ENVSOFT.2008.11.012. 

Kellenberger, B., Tuia, D., Morris, D., 2020. AIDE: Accelerating image-based ecological 
surveys with interactive machine learning. Methods Ecol. Evol. 11, 1716–1727. 
https://doi.org/10.1111/2041-210X.13489. 

Kingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization. arXiv 

preprintarXiv:1412.6980. 

Kleiven, E.F., Nicolau, P.G., Sørbye, S.H., Aars, J., Yoccoz, N.G., Ims, R.A., 2022. Using 
camera traps to monitor cyclic vole populations. Remote Sens. Ecol. Conserv. 
https://doi.org/10.1002/rse2.317. 

Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classification with deep 
convolutional neural networks. In: Pereira, F., Burges, C.J.C., Bottou, L., 
Weinberger, K.Q. (Eds.), Advances in Neural Information Processing Systems. Curran 
Associates, Inc.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper by Minar and Naher (2018) provides an overview of recent advances in deep learning, including techniques for preventing overfitting. One common technique is dropout, which randomly drops out neurons from the network during training to reduce co-dependencies among them. Another technique mentioned is L2 regularization, also known as weight decay, which adds a penalty term to the loss function proportional to the square of the weights, encouraging smaller weights and thus simpler models. Other regularization methods include early stopping, which stops training when performance on a validation set starts to degrade, and batch normalization, which standardizes inputs to each layer to improve stability and convergence. However, it should be noted that these papers focus primarily on computer vision tasks rather than ecology or conservation biology.
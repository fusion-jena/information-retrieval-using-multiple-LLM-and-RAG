Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.2.5. K-nearest neighbors

It is an non-parametric ‘lazy’ learning algorithm. This means that it
does not make any assumptions on the underlying data distribution and
that it does not use the training data points to do any generalization. It
does not attempt to construct a general internal model, but simply
stores instances of the training data. Classiﬁcation is computed from a
simple majority vote of the nearest neighbors of each point: a query
point is assigned the data class which has the most representatives
within the nearest neighbors of the point.

3.2.6. Decision tree

The Decision Tree Classiﬁer is a simple and widely used classiﬁca-
tion technique. It applies a straightforward idea to solve the classiﬁ-
cation problem by posing a series of carefully crafted questions about
the attributes of the test record. Each time it receives an answer, a
follow-up question is asked until a conclusion about the class label of
the record is reached.

Table 5
Binary classiﬁcation eﬀectiveness, per classiﬁer, using all variables.

Classiﬁer

Accuracy

Precision

Recall

F-measure

Passive-Aggressive
k-Nearest Neighbors
Logistic Regression
Ridge
Linear SVC
Decision Tree
Random Forest

59.4
68.5
61.2
61.2
75.4
92.5
93.4

45.4
73.3
61.9
61.6
76.6
95.7
98.1

66.6
71.4
98.7
98.8
88.5
91.0
90.4

53.8
72.1
75.2
75.2
80.5
93.2
93.8

(in terms of comparative presence–absence item counts), Accuracy is a
suitable measure for model performance evaluation. Nevertheless, all
applied algorithms are ranked based on the F-measure (from worst to
best), for reasons to be explained in Section 4.1.1 below.

In this paper, we employ machine learning (ML) techniques to ex-
amine the presence–absence of seagrass meadows in the Mediterranean
Sea, and the environmental relationship among seagrasses at family
level. To achieve these, we combine data from a broad and diverse
range of databases, such as EMODnet, UNEP, and CMEMS, aiming to
determine the most appropriate variables aﬀecting the distribution of
seagrasses. We used static and temporal variables and chose the most
important ones with variable importance method by the random forest
algorithm. The temporal variables have additional features such as the
values for each month, along with the year min, max and average for
surface and seabed, totaling 217 variables. In order to perform binary
classiﬁcation we propose a method to automatically generate an ab-
sence dataset based on the presence dataset. For both binary and multi-
class classiﬁcation, 7 diﬀerent classiﬁers are compared and their results
are discussed.

A very important component of machine learning is model selection
(also known as feature selection) and is mandatory in order to reach the
best model from other alternative ones. Arthur et al. (2010); Li and
Heap (2011) suggest that model selection is important for the popular
random forest algorithm and thus, researchers have to focus to the most
important variables.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't enough information in the given context to determine how the hyperparameters of the models were optimized. The text mentions several types of classifiers used for binary classification, including k-nearest neighbors, decision trees, logistic regression, ridge, linear support vector classifier (SVC), random forests, and passive-aggressive classifier. However, it doesn't provide details on how the hyperparameters of these models were tuned or optimized. Common methods for hyperparameter optimization include grid search, random search, Bayesian optimization, and others. Without further information, I cannot specify which method was used for hyperparameter optimization in this case.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.2. Split the datasets 

In this research, the datasets were randomly divided into three parts 
for training, validation and testing. Here, 60% of the data was used to 
train the deep learning models, and 20% of them was kept to validate the 
models. The remaining 20% data was used to evaluate the performance 
of the models. Table 1 show the number of data in the dataset and how 
they are separated for training, testing and validation. 

2.3. Deep learning models

2.7. Experimental setup 

The experiments were conducted using a desktop computer with an 
Intel Core i9-9900× processor, 128 GB of RAM, and an NVIDIA GeForce 
RTX 2080 Ti graphics card. The deep learning models were developed 
using Python 3.8 and Tensorflow 2.4 library. The models were initialised 
with the pre-trained weights trained on the ImageNet dataset. Since the 
models were trained to classify 1000 classes of images, the classification 
layer of the models was replaced for fine-tuning them on the crop-weed

DenseNets (DenseNet121, DenseNet169, and DenseNet201) promote 
feature  reuse  by  connecting  each  convolutional  layer  to  every  other 
layer in a feed-forward fashion. Compared to other deeper architectures, 
these  memory-efficient  models  achieve  better  performance  on  image 
classification tasks using fewer parameters (J´egou et al., 2017). Finally, 
MobileNetV2  is  chosen  as  a  lightweight  model,  which  is  an  efficient 
architecture for real-time applications. The model balances speed and 
accuracy  well  using  limited  computational  resources  (Wang  et  al., 
2022). 

Several studies used these models on crop-weed datasets and ach-
ieved better performance. For instance, Olsen et al. (2019) used ResNet- 
50 and Inception-V3. Hasan et al. (2023) compared the performance of 
VGG16, ResNet-50, Inception-V3, InceptionResNetV2 and MobileNetV2

VGG16 and VGG19 are classical architectures that are well known 
for  simplicity  and  uniformity.  These  models  are  suitable  for  smaller 
datasets and can provide better accuracy by fine-tuning the pre-trained 
network (Sukegawa et al., 2020). The models have several drawbacks, 
such  as  vanishing  gradient  problems  and  loss  of  fine-grained  spatial 
information (Pan et al., 2020). On the other hand, ResNet-50 contains 
residual  connections,  which  can  overcome  the  vanishing  gradient 
problem and enable training very deep networks. The model performs 
well on both large and small datasets (Al-Masni et al., 2020).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: The experimental setup mentioned in the given context indicates that the deep learning models were trained using a desktop computer equipped with an Intel Core i9-9900x processor, 128 GB of RAM, and an NVIDIA GeForce RTX 2080 Ti graphics card. This suggests that the primary hardware resource utilized for training the deep learning models was a GPU, specifically the NVIDIA GeForce RTX 2080 Ti. No mention of any other specialized hardware resources like TPUs or additional GPUs was made in the provided context.
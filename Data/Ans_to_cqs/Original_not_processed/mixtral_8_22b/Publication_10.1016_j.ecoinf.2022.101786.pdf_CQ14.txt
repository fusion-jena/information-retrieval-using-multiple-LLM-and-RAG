Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Some model parameters were modified from the default configura-
tion to balance use of resources and training speed. We retained a res-
olution of 704 × 704, batch size 64, 32 subdivisions and trained up to 
6000 iterations. Other parameters were kept to default or set according 
to guidance by the developers (https://github.com/AlexeyAB/darknet). 
Manual annotations were used to generate training, validation and 
testing  datasets.  Biigle  annotations  (label  name,  centre  x,  center  y, 

EcologicalInformatics71(2022)1017863N. Piechaud and K.L. Howell

2.5. Model selection 

CNNs are sensitive to overfitting (Domingos, 2012). Throughout the 
training process, the algorithm goes through cycles of training on the 
training set - evaluating performance on the validation set – adjusting 
CNN node weights – before repeating the entire cycle. These cycles are 
referred to as epochs or iterations when the training dataset is too large 
to  be  processed  in  one  stroke  and  needs  to  be  subdivided  in  several 
smaller batches as is the case in this study. There is an optimum number 
of iterations before the performance on the validation and training sets 
(independent data that the CNN has not seen) start to diverge as the CNN 
overfits  and  becomes  more  specialized  at  predicting  the  training  set 
while becoming less able to predict the validation set (generalization).

EcologicalInformatics71(2022)1017864N. Piechaud and K.L. Howell                                                                                                                                                                                                                 

3. Results 

3.3. Size-abundance relationship 

3.1. Model evaluation and selection 

The number of iterations for which the CNN was trained has a strong 
influence on its performances as well as on the confidence it gives to its 
predictions (Fig. 2). In general, longer model training past 1000 itera-
tions tended to give higher recall but lower precision.

During mission M116 the AUV spent 22 h in the water and approx-
imately 18 h near the seabed travelling a distance of 82 km. It performed 
27 transects ranging from 1.7 to 3.2 km in length (Fig. 1), at an altitude 
(cid:0) 1. Distance 
of approximately 3 m above the seabed, and speed of 1.1 m.s
between transects ranged from 10 to 90 m. 

Autosub 6000 was equipped with a Grasshopper 2 - GS2-GE-50S5C 

EcologicalInformatics71(2022)1017862N. Piechaud and K.L. Howell                                                                                                                                                                                                                 

Fig.  1. Map  of  the  study  area (Station  26)  in  the general  context  of  the Rockall  Trough  with  EEZ  boundaries  (inset  map,  top-right  corner)  indicating  the  AUV 
transects, the annotated one (t2), as well as the local topography. Coordinates are UTM28 North.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide explicit information about how the hyperparameters of the model were optimized using methods such as grid search or random search. However, it mentions that some model parameters were modified from the default configuration to balance resource usage and training speed. This suggests that there might have been some form of optimization involved, although the specific method remains unclear based solely on the given context.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 4 
Average performance (Avg) and standard deviation (SD) for five trained models. Average precision, recall and F1-score for trained ResNet50 and EfficientNetB3 
(EffNetB3) models modified for multitask learning (MTL) with transfer learning using pre-trained weights from ImageNet. The models are trained and validated on the 
TLm  dataset. The models ResNet50, EfficientNetB3 are trained without MTL.  

Model 

Level 

ResNet50MTL 
EffNetB3MTL 

ResNet50MTL 
EffNetB3MTL 

ResNet50MTL 
EffNetB3MTL 

ResNet50 
EffNetB3 

L1 Order 
L1 Order 

L2 Family 
L2 Family 

L3 Species 
L3 Species 

Species 
Species 

Avg 

0.990 
0.986 

0.987 
0.984 

0.955 
0.948 

0.955 
0.953 

Precision 

SD (10

(cid:0) 3) 

(1.0) 
(4.4) 

(0.8) 
(3.1) 

(4.3) 
(5.2) 

(3.3) 
(2.5) 

Avg 

0.991 
0.993 

0.986 
0.988 

0.961 
0.966 

0.957 
0.966 

Recall 

SD (10

(cid:0) 3) 

(1.1) 
(0.5) 

(0.9) 
(0.7) 

(9.8) 
(5.1) 

(7.3) 
(2.5) 

Avg 

0.991 
0.989 

0.987 
0.986 

0.957 
0.956 

0.955 
0.959

(cid:0) 4 was finally 
chosen after comparing with stochastic gradient descent (SGD) with the 
(cid:0) 4  and learning rate of 
(cid:0) 4, weight decay of 1.0⋅10
momentum of 1.0⋅10
(cid:0) 3. SGD was tested with the parameters as recommended by L. 
1.0⋅10
Smith  (Smith,  2018),  but  achieved  a  1%–2%  decrease  in  accuracy 
compared to using the Adam optimizer. 

The  ResNet50  architecture  (He  et  al.,  2016)  was  modified  and 
trained with transfer learning using pre-trained weights from ImageNet 
(Smith,  2018).  Models  trained  with  transfer  learning  outperformed 
models trained with random weights with an increase of 4% in the F1-

We simplified the network architecture and hierarchical dependency 
loss  proposed  by  Gao  (2020)  and  rewrote  the  code  from  Ugenteraan 
(2020), which will be explained in the following sections. 

3.1. A multitask CNN for hierarchical classification 

Our proposed network architecture is shown in Fig. 2. The first part 
of the architecture is a CNN such, as ResNet (He et al., 2016) or Effi-
cientNet (Tan and Le, 2019). It is given an input image X transformed by 
the network NCNN(θCNN), where θCNN  means the trainable parameters of 
the CNN network. The network output is viewed as a root representation 
R0, 

R0 = NCNN (X, θCNN )

(1) 

For each level l we add two FC layers and an activation function to 
perform a non-linear transformation of the input R0  to the level output 
Rl. 

Rl = (Wl2*σrelu(Wl1*R0 + bl1)) + bl2)

(2)

pendency  in  “flat”  networks.  Both  methods  only  focus  on  the  loss 
function  and  do  not  incorporate  the  class  hierarchy  into  the  CNN 
architecture.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In this study, the authors use Convolutional Neural Networks (CNNs) as their primary deep learning model. Specifically, they employ two types of CNN architectures - ResNet50 and EfficientNetB3. These models have been modified for multitask learning (MTL) and utilize transfer learning with pre-trained weights from ImageNet. Additionally, the authors mention modifying the ResNet50 architecture and training it with transfer learning using pre-trained weights from ImageNet. Therefore, the main deep learning models used in this pipeline are CNNs, specifically ResNet50 and EfficientNetB3.
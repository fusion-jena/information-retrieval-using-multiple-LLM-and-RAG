Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

training and testing the model is provided, too.

4.5. Implementation details

We have implemented our models in PyTorch (Paszke et al., 2017).
We trained ğ‘€ = 5 models with batch size ğµ = 64 and a base learning
rate ğ›¼ = 10âˆ’4. The learning rate is automatically reduced by a factor
of 0.1 when the validation loss has not improved for 15 consecutive
epochs. We apply weight decay to control the strength of the unit
Gaussian prior, with an empirically chosen magnitude of 10âˆ’3 that
is inversely proportional to the hyperparameter ğœ† from Eq. (1). We
chose ğ›½1 = 0.9, ğ›½2 = 0.999 and ğœ– = 10âˆ’8 as hyper-parameters for the
Adam optimizer. Each neural network was trained on a single Nvidia
RTX2080Ti GPU for âˆ¼14 days.

5. Experimental results and discussion

Learn. Res. 814â€“822.

Rasmussen, C.E., 2004. Gaussian processes in machine learning. In: Bousquet, O.,
von Luxburg, U., RÃ¤tsch, G. (Eds.), Advanced Lectures on Machine Learning: ML
Summer Schools 2003, Canberra, Australia, February 2 - 14, 2003, TÃœBingen,
Germany, August 4 - 16, 2003, Revised Lectures. Springer Berlin Heidelberg, Berlin,
Heidelberg, pp. 63â€“71.

Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You only look once: Unified,
real-time object detection. In: 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). pp. 779â€“788.

Shimizu, K., Ota, T., Mizoue, N., Saito, H., 2020. Comparison of multi-temporal
PlanetScope data with Landsat 8 and Sentinel-2 data for estimating airborne LiDAR
derived canopy height in temperate forests. Remote Sens. 12 (11).

Simonyan, K., Zisserman, A., 2015. Very deep convolutional networks for large-scale
image recognition. In: 3rd International Conference on Learning Representations,
ICLR.

4.3. Loss function

The loss function, which is optimized during training, measures
the quality of a set of network parameters ğœ½ w.r.t. the training data
îˆ° = {(ğ’™ğ‘–, ğ’šğ‘–)}ğ‘
ğ‘–=1, under some regularizing prior assumptions. We use a
standard loss function îˆ¸(îˆ°; ğœ½) whose minimization corresponds to max-
imizing the posterior probability of the parameters given the training
data. As it is commonly done in machine learning (see e.g. Goodfellow
et al., 2016), we assume a zero-mean isotropic Gaussian prior over
the network parameters (corresponding to îˆ¸
2 regularization) and a
Gaussian likelihood function with mean Ì‚ğğ‘– âˆ¶= Ì‚ğ(ğ’™ğ‘–; ğœ½) âˆˆ R5 and
diagonal covariance matrix with logarithmic elements Ì‚ğ’”ğ‘– âˆ¶= Ì‚ğ’”(ğ’™ğ‘–; ğœ½) âˆˆ
R5:

îˆ¸(îˆ°; ğœ½) = ğœ† â€–ğœ½â€–

2
2 +

âˆ‘

[ Ì‚ğ‘ ğ‘–ğ‘— + exp(âˆ’ Ì‚ğ‘ ğ‘–ğ‘— )( Ì‚ğœ‡ğ‘–ğ‘— âˆ’ ğ‘¦ğ‘–ğ‘— )2]

ğ‘–,ğ‘—

(1)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't any explicit information about the postprocessing steps involved after the model training in the provided context. However, based on common practices in machine learning, one could infer that possible postprocessing steps might include generating saliency maps, calculating metrics such as accuracy or F1 score, creating confusion matrices, or performing other forms of analysis to evaluate the performance of the model. These steps would typically be used to assess how well the model generalizes to unseen data and to identify areas where the model may need improvement.
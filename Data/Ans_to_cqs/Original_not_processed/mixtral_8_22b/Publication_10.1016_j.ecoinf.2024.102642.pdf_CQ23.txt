Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During  the  campaign,  methods  are  proposed  to  assess  inter- 
annotator  variability  and  aggregate  annotation  sets.  Those  methods 
will help to define the minimal number of annotators needed [Dubus 
et al., 2023; Walter et al., 2022]. Increasing the number of annotators 
per sample could be helpful if the variability is too high. Finally, the two 
grouping  methods:  majority  voting  (Section  2.2.3)  and  soft  labeling 
(Section 2.3.2) can also be used to produce relevant annotation sets for 
deep learning. 

6. Conclusion 

In this study, new approaches were explored to improve the accuracy 
and  efficiency  of  cetacean  vocalization  detection  using  deep  learning 
models. Leveraging multi-annotation campaigns involving both expert 
and novice annotators aimed to address the challenges associated with 
the scarcity of ground truth data in underwater PAM studies.

Fig.  8. Performance  (AUC  and  mAP)  of  the  models  trained  with  aggregated 
annotation sets of novices using a major voting. The red dotted line corresponds 
to the performance of the model trained on the expert. 

EcologicalInformatics81(2024)1026427(a)SEIOPBWvocalizations(b)BluewhalesDcall(a)SEIOPBWvocalizations(b)Bluewhale’sDcallG. Dubus et al.

distinctive  annotation  profiles  influenced  by  the  complexity  of  vocalizations  and  the  annotators’  strategies, 
ranging from conservative to permissive. To address the challenges of annotation discrepancies, Convolutional 
Neural Networks (CNNs) are trained on annotations from both novices and the expert. The results show varia-
tions in model performance. Our work highlights the importance of annotation guidelines encouraging a more 
conservative  approach  to  improve  overall  annotation  quality.  In  an  effort  to  optimize  the  potential  of  multi- 
annotation  and  mitigate  the  presence  of  noisy  labels,  two  annotation  aggregation  methods  (majority  voting 
and  soft  labeling)  are  proposed  and  tested.  The  results  demonstrate  that  both  methods,  particularly  when  a 
sufficient number of annotators are involved, significantly improve model performance and reduce variability:

To  ensure  comparability  across  trained  models,  we  maintained 
consistent hyperparameters across all training instances for each task (i. 
e.,  SEIO  PBW  vocalization  and  Dcall).  Specifically,  for  SEIO  PBW 

vocalization  training  and  blue  whale  Dcall  training,  we  set  the  batch 
sizes to 5 and 4, and the learning rates to 1e-4 and 1e-3, respectively. 
This decision was based on the fact that more samples were used for 
SEIO PBW vocalization training. The models were trained for 40 and 25 
epochs  respectively.  However  an  early  stopping  method  was  used  to 
keep  the  model  weights  before  the  models  overfitted.  The  patience 
parameter is set at 10 epochs for all training phases. 

2.4. Evaluation metrics 

Considering  the  absence  of  absolute  ground  truth  for  identifying 
audio events within underwater acoustic recordings, the annotation sets 
proposed by the annotators, expert or novice, are called pseudo ground 
truth in the present study.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors did not explicitly mention any specific techniques such as diverse datasets, cross-validation, or stratified splitting to ensure the generalizability of their deep learning models. However, they did maintain consistent hyperparameters across all training instances for each task (i.e., SEIO PBW vocalization and Dcall), which helps in maintaining consistency during the training process. They also mentioned using an early stopping method to prevent overfitting, which further contributes to ensuring the robustness and generalizability of the models. Additionally, the use of multiple annotators, including experts and novices, helped create diverse annotation sets, which likely improved the model's ability to handle various scenarios and increase its applicability to different situations.
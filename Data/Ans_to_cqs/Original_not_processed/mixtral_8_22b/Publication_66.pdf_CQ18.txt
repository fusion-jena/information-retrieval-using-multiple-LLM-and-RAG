Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

|  689

F I G U R E   6  Box	plots	of 	the	optimum	models	for	(a)	guillemots	
trained	with	coverage,	Xbar and Ybar	(b)	shags	trained	with	altitude	
and	coverage	and	(c)	razorbills	trained	with	coverage,	using	leave-	
one-	out	cross-	validation.	T_AUC	is	the	training	AUC,	V_AUC	is	the	
validation	AUC,	PPV	is	the	positive	predicted	value	and	NPV	the	
negative	predicted	value.	The	solid	line	in	the	middle	of	the	boxes	
represents	the	mean

was	 variation	 in	 the	 optimum	 models	 between	 species;	 however,	

altitude	 and	 coverage	 together	 were	 also	 good	 predictors	 of	 diving	

behaviour	in	all	species	(Table	3).	The	weakest	models	remained	the	

same	as	those	using	10-	fold	cross-	validation	(Appendix	S6).

3.2 | Alternative prediction methods

For	several	individuals	of	each	species,	the	HMMs	collapsed	for	both	

three-		and	two-	state	models.	This	meant	that	predictions	were	made	

for	 31	 and	 26	 guillemots,	 11	 and	 9	 shags	 and	 41	 and	 37	 razorbills,

each	species.	The	models	were	then	used	to	predict	the	diving	loca-

tions	of	birds	monitored	with	only	GPS	devices.

2.4 | Alternative prediction methods

To	 compare	 predictions	 obtained	 from	 deep	 learning	 models	 with	

methods	 used	 in	 previous	 studies	 classifying	 foraging	 behaviour	 in	

seabirds	HMMs,	a	naïve	Bayes	classifier	and	speed	and	tortuosity	pre-

dictions	were	implemented	on	the	data	(see	Appendix	S8,	supporting	

information	for	details	of	the	latter).	The	Naïve	Bayes	classification,	a	

supervised	 learning	 method,	 was	 implemented	 in	 r	 using	 the	 e1071	

package	 (Meyer,	 Dimitriadou,	 Hornik,	 Weingessel,	 &	 Leisch,	 2015).	

The	inputs	were	the	same	as	used	in	the	H2O	model,	and	the	depend-

ent	 variable	 (variable	 to	 be	 predicted)	 was	 the	 binary	 dive/not	 dive	

column.

HMMs	 are	 an	 unsupervised	 learning	 method	 that	 identifies	 dis-

crete	 states	within	 time-	series	 data	 and	 have	 been	 used	 extensively

produced	 poorer	 behavioural	 predictions,	 further	 demonstrating	 the	

structed	the	deep	learning	models	in	r	using	H2O	and	conducted	the	

BROWNING et al. 2041210x, 2018, 3, Downloaded from https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12926 by Thuringer Universitats- Und, Wiley Online Library on [16/11/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons Licensecomparative	modelling.	E.B.	and	R.F	wrote	the	manuscript.	All	authors	

contributed	to	editing	of	the	manuscript.	R.F.	and	M.B.	supervised	this	

work.

DATA ACC ESSI BILITY

GPS	track	data	used	in	the	analysis	are	available	at	(http://seabirdtrack-

ing.org/mapper/contributor.php?contributor_id=950).	Other	data		used	

within	 the	 analysis	 (GPS	 &	 dive	 data,	 and	 associated	 matrices	 used

for	 two-		 and	 three-	states,	 respectively.	 Two-	state	 models	 were	 as-

sumed	to	represent	rest	and	foraging—or	diving	and	non-	diving-	,	and	

three-	state	models	included	an	intermediate	state.	The	HMMs	did	not	

predict	 diving	 behaviour	 as	 accurately	 as	 the	 deep	 learning	 models.	

Specificity,	 sensitivity,	 PPV	 and	 NPV	 were	 consistently	 lower	 for	 all	

species	using	both	two-		and	three-	state	models	(Table	4).	The	high-

est	 sensitivity	 was	 81.41%	 for	 shags	 using	 a	 three-	state	 HMM	 and	

specificity	was	77.50%	using	two	states.	There	was	a	trade-	off	where	

increasing	the	number	of	states	increased	sensitivity,	so	increasing	the	

accuracy	of	dive	behaviour	prediction,	but	a	decrease	in	non-	diving	or	

resting	behaviour	predictions,	the	specificity.

The	 predictions	 made	 using	 Naïve	 Bayes	 were	 poor	 for	 dives	

across	all	three	species	and	variable	combination,	although	non-	dives

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The study uses leave-one-out cross-validation to evaluate the performance of the optimum models for each species. This validation technique involves leaving out one observation from the dataset, fitting the model to the remaining observations, and then testing the fitted model on the left-out observation. This process is repeated until every observation has been left out once. The results of this validation are presented in box plots showing the mean, positive predicted value (PPV), negative predicted value (NPV), and area under the curve (AUC) for both training and validation datasets. Additionally, alternative prediction methods such as hidden Markov models (HMMs) and naïve Bayes classifiers were implemented and compared to the deep learning models. However, these methods did not perform as well as the deep learning models in terms of specificity, sensitivity, PPV, and NPV.
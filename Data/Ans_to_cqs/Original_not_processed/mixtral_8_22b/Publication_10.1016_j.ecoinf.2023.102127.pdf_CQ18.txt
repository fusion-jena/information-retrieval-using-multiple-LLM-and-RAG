Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4. Model evaluation

Isaac et al. (2020). This is since (a) the consideration of all available data 
is  one of the  main points of  model-based integration, (b)  appropriate 
validation metrics are less straight-forward than for single datasets as 
biases and sampling methods can differ, and (c) any validation dataset 
might not represent the niche and environmental parameters estimated 
by  the  integrated  model.  For  example,  the  standard  practice  of  with-
holding parts of the training data for validating a model often means that 
both  training  and  testing  data  suffer  from  the  same  spatial  and  envi-
ronmental biases (Baker et al., 2022). If, however prior knowledge of the 
biology of a species is integrated in a SDM through a prior or offset, thus 
“nudging”  or constraining response functions towards a more sensible 
outcome  and  ultimately  different  prediction,  the  use  of  any  (biased) 
withheld data would likely indicate a reduced predictive performance

are not enough data available to reliably fit every type of model, espe-
cially  given  the  demanding  nature  of  some  machine  learning  ap-
proaches,  and  computation  time  can  be  a  considerable  limitation  as 
well, such as for more demanding Bayesian models. The package will 
raise warnings and highlighted messages in case the provided informa-
tion is not sufficient for inferring a species distribution.

EcologicalInformatics76(2023)1021276M. Jung                                                                                                                                                                                                                                           

or  True  Skill  Statistics  (TSS)  using  the  “modEvA”  R-package  (M´arcia 
Barbosa et al., 2013), or by thresholding with the minimum presence 
values  (e.g.  the  minimum  value  across  occurrence  points),  fixed  or 
percentile values. Finally, all suitability predictions subject to thresholds 
can be created in binary, categorical percentile and normalized outputs 
(Fig. 4c). Thresholding to a normalized or percentile characterization of 
the  distribution  retains  some  of  the  detail  of  the  projected  suitability 
distribution, while also removing uncertain areas and noise.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information on the specific strategy used to monitor the model performance during training. However, it mentions several aspects related to model evaluation and validation. It suggests that the standard practice of withholding parts of the training data for validating a model might lead to both training and testing data suffering from the same spatial and environmental biases. Additionally, it discusses the use of appropriate validation metrics, which may be less straightforward for integrated models due to differences in biases and sampling methods. Furthermore, it highlights the importance of considering all available data and integrating prior knowledge of the species biology into the model. Lastly, it mentions the possibility of raising warnings and highlighted messages if the provided information is insufficient for inferring a species distribution. Based on this context, we can infer that monitoring the model performance during training involves careful consideration of validation strategies, selection of suitable metrics, and potential inclusion of domain knowledge to improve model outcomes. However, without further details, it is impossible to specify the exact strategy employed for monitoring the model performance during training.
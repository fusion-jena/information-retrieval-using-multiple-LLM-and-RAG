Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In recent years, it has been observed that deep learning methods can 
be trained with close to zero training error efficiently. The number of 
convolutional and dense layers directly affects the runtime of the model 
(Bienstock et al., 2023). The running time of a DL model increases in 
polynomial-terms with the increased number of associated layers. But 
with the effort of achieving zero error, there are possibilities of over-
training and biased training. To avoid these situations, a restrictive and 
gradually increasing training cycle and accuracy calculation approach 
has  been  adopted.  Based  on  mean  square  error  (MSE),  the  optimal 
training state for the model has been obtained. 

4.1. Performance metrics used to evaluate the model

below.  

(i)  Activation Function - The Leaky ReLU activation function is used 
in the middle/ hidden layers, and the sigmoid activation function 
(Pretorius et al., 2019) is used in the final detection layer. 
(ii) Optimization Function- Stochastic Gradient Descent (SGD) (Bot-

tou, 2012) is used for training.  

(iii)  Loss Function Calculation- The compound loss calculator of the 
YOLO family based on objectness score, class probability score, 
and bounding box regression score is used (Bhagabati and Sarma, 
2022a). 

The state-of-the-art YOLOv5s model is used in this experiment. The 
network training is done by using a publicly available dataset in Robo-
flow which was annotated and exported into Yolo PyTorch format. The 
dataset was labelled with seven classes- (i) Bear (0), (ii) Deer (1), (iii) 
Elephant (2), (iv) Leopard (3), (v) Monkey (4), (vi) Tiger (5) and (vii) 
Wildboar (6).

In  order  to  determine  more  accurate  training  results  and  also  to 
explore the effect of epoch upon training result, apart from 150 epochs, 
the model with attention layer is trained with epoch values 100, 200, 
and  250  under  a  uniform  training  environment  and  with  the  same 
dataset.  The  training  summary  for  each  of  these  epochs  is  shown  in 
Tables 5, 6 and 7 for epochs 100, 200, and 250, respectively. The trends 
of mAP values with increasing epochs are shown in Figs. 14 and Fig. 15. 
The size of the dataset used for custom training is sufficiently large. 
Further, the DL model used in this work is tuned with optimized hyper 
parameters  for  which  overfitting  and  under  fitting  situations  are  pre-
vented from occurring. As mentioned above, the effort to over-train the 
model has been avoided and graded MSE convergence (Figs. 7 to 11) has 
been  adopted  to  fine-tune  the  model.  Results  obtained  for  different

networks for visual recognition. IEEE Trans. Pattern Anal. Mach. Intell. 37, 
1904–1916. 

He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition. In: 
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las 
Vegas, NV, USA, pp. 770–778. https://doi.org/10.1109/CVPR.2016.90. 

Hosang, J., Benenson, R., Schiele, B., 2017. Learning non-maximum suppression. arXiv. 
Hu, J., Shen, L., Albanie, S., et al., 2019. Squeeze-and-excitation networks, arXiv: 

1709.01507v4 [cs.CV] 16 May 2019. 

Khalajzadeh, H., Manthouri, M., Teshnehlab, M., 2014. Face recognition using 

convolutional neural network and simple logistics classifier. Adv. Intell. Syst. Comp. 
223, 197–207. 

K¨orschens, M., Denzler, J., 2019. ELPephants: A fine-grained Dataset for elephant re- 
identification. In: 2019 IEEE/CVF international conference on computer vision 
workshop (ICCVW), pp. 263–270.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The optimization technique applied in the deep learning pipeline for this particular case is Stochastic Gradient Descent (SGD). This method is used for training the model, as stated in the given context. SGD is a popular optimization algorithm widely used in machine learning and deep learning models due to its simplicity and efficiency. It works by updating the parameters of the model iteratively based on the gradient of the loss function with respect to those parameters. However, other optimization algorithms such as Adam or RMSprop could have also been used depending on the specific requirements and characteristics of the problem being addressed.
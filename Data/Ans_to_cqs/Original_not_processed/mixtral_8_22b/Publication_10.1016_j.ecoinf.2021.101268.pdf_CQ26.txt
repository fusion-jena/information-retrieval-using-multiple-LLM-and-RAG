Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

H . 
. (4c) Adaptive equalization of the Lightness - IRGB
ICanny. (4f) Final. (For interpretation of the references to color in this figure legend, the 

S

Table 3 
Architectures comparison: AlexNet, VGG-19, ResNet-101, and DenseNet-201.  

Network 

AlexNet 
VGG-19 
ResNet-101 
DenseNet-201 

Year 

2012 
2014 
2016 
2017 

Depth 

#parameters 

8 
19 
101 
201 

60 M 
144 M 
44.8 M 
20 M  

validation samples are selected randomly from the first group. The data 
distribution is 70.12% for training, 1.69% validation, and 28.19% for 
testing. The model feeds off with 16 elements per mini-batches using the 
ADAM optimizer (Kingma and Ba, 2014) with a learning rate of 1e-3. To 
run our experiments, we use Pytorch 1.3 framework in a PC with the 
following specifications: 4.0 GHz Intel Core i9 processor, 32 GB 3000 
MHz DDR4 memory, and NVIDIA Titan RTX. 

5. Results

(13)  

4.2. Architecture configurations 

Convolutional neural networks (CNN) outstand over DL techniques 
by disentangling high-level representations across multiple processing 
layers. CNN’s process data on two levels: a convolutional block for the 
automatic  feature  extraction,  and  fully  connected  layers  to  establish 
feature-output  correlation.  The  convolutional  block  comprises  con-
volutional,  ReLU,  and  max-pooling  layers.  Each  set  of  convolutional 
layers diversify features by applying a set of parallel filters that process 
local sections of the input space. The feature vector integrates low-level 
local features from the first layers and higher-level representations from 
the latest ones.

Model 

AlexNet 
VGG-19 
ResNet-101 
DenseNet-201 

Accuracy 

Raw 

Train 

98.75% 
96.77% 
82.25% 
93.71% 

Validation 

97.16% 
98.30% 
89.04% 
91.30% 

Test 

96.16% 
95.15% 
83.30% 
86.48% 

Pre-processed  

Train 

98.21% 
96.94% 
77.25% 
91.61% 

Validation 

97.92% 
97.92% 
79.02% 
87.33% 

Test 

95.98% 
96.52% 
75.44% 
86.29%  

Table 5 
Accuracy of the models swapping the testing sets (source → target).  

Model 

Accuracy 

Raw → Pre-processed 

Pre-processed → Raw 

AlexNet 
VGG-19 
ResNet-101 
DenseNet-201 

82.35% 
82.70% 
69.22% 
65.26% 

54.76% 
78.87% 
29.56% 
33.97%  

Fig. 5. Confusion matrices for the VGG-19 architecture.  

et al., 2017) and SmoothGrad (Smilkov et al., 2017) methods over each 
model. These methods plot a point cloud, where the density denotes the 
input space relevance. Thus, a higher density in a region suggests that 
the network ponderates it the most when classifying.

The  pre-processing  stage  reduces  noises  and  standardizes  inputs, 
which  enhances  metrics.  Nevertheless,  real-world  data  challenges 
robustness for any model due to registration conditions are not in con-
trol.  Therefore,  we  run  an  experimental  ablation  in  the  background 
removal algorithm to measure if the model learns by itself how to focus 

5.2. Qualitative evaluation 

Like Lee et al. (2017) and Barr´e et al. (2017), the qualitative evalu-
ation consists of a visual interpretation of features. Instead of visualizing 
features  per  layer,  we  apply  the  Integrated  Gradients  (Sundararajan 

EcologicalInformatics62(2021)1012686G. Vizcarra et al.                                                                                                                                                                                                                               

Table 4 
Accuracy of the models w/wo pre-processing.   

Model

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there is no explicit mention of techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, it can be inferred that some form of preprocessing was performed since the results show accuracy comparisons between raw and preprocessed datasets. Additionally, the table shows that the data distribution is split into 70.12% for training, 1.69% for validation, and 28.19% for testing, which could indicate some form of stratification. Nonetheless, without further details about the exact preprocessing steps taken, it cannot be confirmed what specific techniques were used to address data bias.
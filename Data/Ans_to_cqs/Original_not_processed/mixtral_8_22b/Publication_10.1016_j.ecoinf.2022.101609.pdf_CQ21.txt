Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

this layer is MLOps - model maintenance in the production environment. 
MLOps encompasses automation and monitoring steps such as contin-
uous  integration,  deployment  and  training  on  data  collected  in 
production.

by  subtracting  the mean  and dividing by  the standard deviation.  The 
“add datepart” helper function of the library allows the specification of 
the date column which generates additional predictors such as “Year”, 
“DayofWeek”,  “DayOfYear”,  “Is  Month_End”  and  so  on.  Appendix  B 
highlights the list of categorical and continuous variables in the dataset 
after processing. 

4.3.2. Validation set creation and training architecture 

Model  training  is  typically  initiated  by  splitting  the  dataset  into 
training, test and validation datasets. As the name implies, training data 
is used for training, while validation data is used for selecting the model 
that works best after verification using the test data. It is customary to 
randomise the dataset before splitting when there is a class imbalance - 
stratification; but since this problem is like a time-series problem where

Suitable optimisers, loss functions and activation functions had to be 
selected from an array of available options. Series of experimentation 
were  carried  out  on  popular  optimisation  functions  such  as  SGD, 
RMSProp,  LAMB,  LARS  and  Adam  and  regression  loss  functions  like 

EcologicalInformatics69(2022)10160911T.D. Akinosho et al.                                                                                                                                                                                                                            

Fig. 10. Auto-SQL generation to pre-process the dataset. An SQL command which generates 3-hour and 6-hour pollutant averages from the preceding readings 
is depicted. 

Fig.  11. The  model's  training  loss  against  the  learning  rate  to  determine  the 
appropriate learning rate. The learning rate was fixed at the point where the 
(cid:0)
plot started dipping (i.e., 10

4).

2. Research methodology

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not explicitly mention any specific postprocessing steps involved after the model training. However, based on general machine learning practices, some common postprocessing steps include metrics calculation, confusion matrix, saliency maps, and model interpretation techniques. These steps help evaluate the performance of the trained model and provide insights into its predictions.

Metrics calculation involves computing various evaluation metrics such as accuracy, precision, recall, F1 score, ROC curve, etc., depending on the type of problem being solved. This helps quantify the performance of the model and compare it with other models or baselines.

A confusion matrix is a table layout that visualizes the performance of a classification algorithm. Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class. This provides a more detailed view of the model's performance beyond simple accuracy.

Saliency maps are visualizations that highlight the regions of input data that contribute most significantly to the output prediction. They can be useful for understanding how the model makes decisions and identifying potential biases or errors.

Other postprocessing steps may involve interpreting the model's weights, coefficients, or activations to gain insight into its inner workings. This can help identify important features, interactions between features, or unexpected patterns in the data.
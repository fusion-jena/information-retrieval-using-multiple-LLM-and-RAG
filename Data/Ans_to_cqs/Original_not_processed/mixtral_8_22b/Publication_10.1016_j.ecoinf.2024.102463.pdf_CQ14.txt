Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

hyperparameters for model architectures.  

2.  Partial training: Models are partially trained (on a small fraction of 
the data) to quickly eliminate underperforming configurations.  
3.  Successive Halving: The remaining configurations are grouped into 
sets of different sizes, and the associated models are trained further. 
The best-performing configurations in each set are promoted to the 
next  stage,  while 
the  under-performing  configurations  are 
eliminated. 

4. Repeat: Steps 2 and 3 are repeated until only one configuration re-
mains, which is then considered the best configuration found.

Unlike  the  original  article  (Tao  et  al.,  2016),  an  optimisation  is 
perform to determine the best set of hyperparameter values. Optimising 
hyperparameters  is  an  iterative  process  that  often  involves  testing 
several combinations of values for each hyperparameter to find the best 
values  for  the  specific  task.  The  algorithm  used  is  the  Hyperband  (Li 
et al., 2018) method inspired by multi-armed bandit problem (available 
on  the  Keras  library).  The  Hyperband  algorithm  is  a  hyperparameter 
optimisation method based on the “successive halving” strategy. Here is 
a brief explanation:  

1.  Initial sampling: Hyperband begins by randomly sampling a set of 

hyperparameters for model architectures.

described in the next section. 

During the training of the model, the data is then standardized. In 
section 4, one of the three datasets will be defined as a training base, on 
which the standardizaion parameters will be calculated and applied to 
the  other  two  datasets  considered  as  test  base.  The  best  result  of  the 
optimisation will allow us to determine the base that will be used for 
training. 

4. Model optimization: Determination hyperparameters and 
parameter training 

4.1. Optimisation algorithm and experimental design

The key idea behind Hyperband is to explore several configurations 
in parallel while allocating more training resources to promising con-
figurations. This enables a more efficient search of the hyperparameter 
space, particularly when computational resources are limited. In sum-
mary, Hyperband combines an initial random search with a ‘successive 
halving’  strategy  to  quickly  identify  promising  configurations  while 
eliminating those that show inferior performance. 

The  detail  of  the  grid  of  possible  combinations  is  presented.  The 
number of convolution layers is lower than that given in the article, the 
network has two convolution layers. Their size and the associated max- 
pooling will be determined in the following interval for each:

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Hyperparameters of the model are optimized using the Hyperband method, which is inspired by the multi-armed bandit problem. This method is available in the Keras library. Hyperband is a hyperparameter optimization method based on the "successive halving" strategy. It explores several configurations in parallel while allocating more training resources to promising configurations. This allows for a more efficient search of the hyperparameter space, especially when computational resources are limited.

In summary, Hyperband combines an initial random search with a'successive halving' strategy to quickly identify promising configurations while eliminating those that show inferior performance. The details of the grid of possible combinations are presented, including the number of convolution layers, their size, and the associated max-pooling. These will be determined within specified intervals for each.
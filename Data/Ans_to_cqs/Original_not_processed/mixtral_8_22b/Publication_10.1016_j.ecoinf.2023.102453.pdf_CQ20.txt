Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In future work, we will investigate model optimization techniques 
for deploying the deep learning models to even smaller microcontroller 
devices. Such low power devices can further economize the batteries and 
run autonomously to provide near real-time ecological insights from the 
field.  We  would  also  research  other  model  fine-tuning  techniques  to 
improve the model performance by investigating model pre-training on 
large datasets from ecological studies. 

Funding 

This  research  did  not  receive  any  specific  grant  from  funding 

agencies in the public, commercial, or not-for-profit sectors. 

Declaration of competing interest 

The authors declare that they have no known competing financial 
interests or personal relationships that could have appeared to influence 
the work reported in this paper. 

Data availability 

Data will be made available on request.  

Appendix A. Appendix  

Table A1 
Class wise statistics for the Florida-Part1 dataset.  

Technique

3.5. Hyperparameter optimization 

The model training process determines the values for the trainable 
parameters  of  a  model,  e.g.,  Table  4 shows  the  size  of  the  model pa-
rameters. In addition, a DNN model also has other parameters that need 
to be selected, e.g., batch size, that determines the model performance. 
The  model  parameters  define  the  model  and  are  termed  as  hyper-
parameters. Each hyperparameter has a range of values, from which an 
optimum  selection  can  improve  the  model  performance.  The  model 
hyperparameters could be in hundreds, presenting a very large search 
space, but these differ in their relative importance. The common ones for 
a DNN are learning rate, batch size, and dropout.

Neural architecture search (NAS) was used for developing the Effi-
cientNetV2B0 model, and for the optimization of parameter efficiency 
and  training  speed  (Tan  and  Le,  2021).  The  model  performance  was 
improved  using  progressive  learning  to  adapt  regularization  to  the 
image size (Tan and Le, 2021). EfficientNetV2 is a family of models (B0 
to B7), and B0 is the smallest variant in the family (Keras, n.d), with the 
other  models  progressively  increasing  in  performance  going  up  to 
EfficientNetV2B7.

6. Conclusion and future work 

In this paper, we have shown that through transfer learning with fine 
tuning,  the  performance  of  state-of-the-art  deep  learning  models  can 
improve  animal  classification  results  in  the field  on-board  low  power 
customized  camera  trap  on  edge  devices.  The  Deep  Neural  Network 
model  selection  has  the  trade-offs  concerning  the  model  size,  model 
performance,  model  training  and 
inference  time,  computations 
complexity,  and  ease  of  use.  Therefore,  the  studies  can  benefit  by 
considering these trade-offs to select a model depending on the appli-
cation performance requirements. The model comparison showed that 
EfficientNetB0  had  the  best  performance  amongst  the  selected  small 
sized models. The need for a human-in-the-loop would always remain to 
validate the model performance and predictions.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not explicitly mention the hardware resources used for training the deep learning model. It only discusses the model optimization techniques, such as neural architecture search (NAS) and hyperparameter optimization, along with the use of EfficientNetV2B0 model. However, it does not specify whether GPUs, TPUs, or other hardware resources were utilized during the training process.
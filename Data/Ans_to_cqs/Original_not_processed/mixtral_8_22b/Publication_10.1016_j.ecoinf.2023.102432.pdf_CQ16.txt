Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

version 2020a (The MathWorks Inc., Natick, Massachusetts). The data 
was split into training and testing data using 5 K-fold cross-validations. 
Before running, MTMI-SMF requires a few parameters to be set by the 
user.  After  experimentation  with  training-validation  approaches,  we 
used the following parameters: number of initialized targets was two, 
initialization  method  was  K-Means  clustering,  background  mean  and 
inverse covariance was calculated from only negative bags, and alpha 
was set to 0.1. For more information on these parameters, please refer to 
Meerdink et al., 2022. Once the parameters have been set, MTMI-SMF 
compares negative and positive bags and finds one or more target sig-
natures that best separates the target (e.g., Brazilian peppertree) from 
non-targets (e.g., other species, soil, water) (Fig. 4). Once target signa-
tures are applied to the data, Spectral Matched Filter (SMF) is used to

were calculated on the 20% of the data not used for training the algo-
rithm  (approximately  720  pixels  with  50  having  Brazilian peppertree 
present). The confusion matrix was calculated and the overall, producer, 
and user accuracy are reported.

MTMI-SMF is formulated to work with imperfect data, which can be 
due to training locations having locational errors or not ideally matching 
with  image  pixels.  MTMI-SMF  groups  pixels  surrounding  a  training 
location  that  could  have  invader.  Most  other  machine  learning  algo-
rithms require a single pixel to represent a training location, which often 
leaves the user making an educated guess on which pixel best represents 
that training location. Here, we compare MTMI-SMF classification (i.e., 
confidence values with a threshold applied) to a commonly used ma-
chine learning classification approach, random forest.

Table A.5 
Confusion matrix for RapidEye imagery using Random Forest. Each row is an iteration of the 5 K-Folds used with values only being used on the bags excluded from 
training. 

Table A.6 
Confusion matrix for Landsat imagery using Random Forest. Each row is an iteration of the 5 K-Folds used with values only being used on the bags excluded from 
training. 

References 

Asner, G.P., Knapp, D.E., Kennedy-Bowdoin, T., Jones, M.O., Martin, R.E., Boardman, J., 

Hughes, R.F., 2008. Invasive species detection in Hawaiian rainforests using 
airborne imaging spectroscopy and LiDAR. Remote Sens. Environ. 112, 1942–1955. 
https://doi.org/10.1016/j.rse.2007.11.016. 

Blossey, B., 1999. Before, during and after: the need for long-term monitoring in invasive 
plant species management. Biol. Invasions 1, 301–311. https://doi.org/10.1023/A: 
1010084724526.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it does not explicitly state the specific criteria used to determine when training is complete. However, it mentions that the data was split into training and testing data using 5 K-fold cross-validations. This suggests that the model may use some form of performance metric on the validation sets to decide when training should stop. Additionally, the mention of experimentation with training-validation approaches implies that there might have been different methods tested to optimize the training process. Unfortunately, without further details, it cannot be definitively stated what exact criteria were used to determine when training is complete.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

ViT  is the  first significant effort to employ  a  pure Transformer on 
pictures directly, showing that it can totally replace the conventional 
convolution processes with SOTA performance. The training of the va-
nilla ViT model, however, necessitates high computing costs, according 
to  the  experimental  findings.  As  a  result,  the  utilization  of  a  hybrid 
design  that  couples  the  Transformer  with  a  CNN  backbone  (such  as 
ResNet)  is  advocated  here.  The  Transformer  may  concentrate  on 
modelling global attention using the feature that ResNet extracted. Ac-
cording to the experimental findings, the hybrid approach can deliver 
better performance with comparatively less computing work. The main 
objective of this work was to create a model that accurately identifies 
sika deer using the idea.

6.1. Vision transformer 

Transformer (Vaswani et al., 2017), which is a deep neural network 
first created for natural language processing (NLP), was built on a self- 
attention mechanism that makes it possible for noticeably huge recep-
tive  fields.  The Transformer  has become  a  de facto industry  standard 
procedure in NLP because of its remarkable efficiency, and it has also 
motivated  researchers  in  the  field  of  computer  vision  to  investigate 
applications  by  utilizing  the  long-term  relationships  between  pixels 
(Khan et al., 2022).

nism (Dosovitskiy et al., 2021). In contrast to CNN, ViT's Self Attention 
was  able  to  combine  the  data  from  the  entire  image,  and  accurately 
model  lengthy  sequence  information.  In  the  standard  ViT  model,  the 
relationship between distinct tokens was crucial since all images were 
turned into fixed-size tokens. By reusing the relationship matrix of the 
image  features,  the  lower  level  ViT  can  perform  more  accurate  rela-
tionship modelling. The images were converted into tokens in a process 
that  is  equivalent  to  downsampling.  A  larger  downsampling  multiple 
indicates that fewer tokens are obtained and more information is lost. 
When  more  tokens  are  acquired  and  the  downsampling  multiple  is 
lower, the lost information is less. When converting a picture to a token, 
the image's content is not taken into account. Such downsampling is too 
fine-grained for complicated pictures, which results in a loss of accuracy.

Under the condition of unchanged training methods and parameters, 
the results of each model listed in Table 3 were obtained. As compared to 
the  accuracy of  the  model in  this  work, the  VGG  model's recognition 
accuracy  was  too  low.  Although  the  traditional  DenseNet  has  fewer 
parameters  and  a  greater  identification  accuracy  than  ResNet,  its 
recognition rate is lower than that of the model in this work. Due to its 
sliding window design, the Swin Transformer is somewhat more accu-
rate than the other models. Nevertheless, the number of parameters and 
model size are excessive, consuming too many memory resources. On 
top  of  that,  the  proposed  model  exhibited  the  greatest  identification 
accuracy  but  needs  some  improvement,  in  terms  of  the  number  of 
parameters.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The pipeline uses a hybrid deep learning model that combines both Convolutional Neural Network (CNN) and Transformer architectures. This hybrid design leverages the strengths of both types of networks. Specifically, the CNN component serves as a backbone for extracting features from input images, while the Transformer component focuses on modeling global attention using these extracted features. This combination allows for better performance with relatively less computational workload compared to using a vanilla Vision Transformer (ViT) model alone. The use of a hybrid model aligns with recent trends in computer vision research, where Transformers have been adapted from their original application in Natural Language Processing (NLP) to handle tasks involving long-range dependencies between pixels in images.
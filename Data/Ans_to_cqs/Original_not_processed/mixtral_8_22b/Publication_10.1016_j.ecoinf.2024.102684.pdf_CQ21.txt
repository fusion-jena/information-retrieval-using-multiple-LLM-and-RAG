Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The dataset was divided into training (60%), validation (20%), and 
testing (20%) subsets for model evaluation and hyperparameter search 
(Fig. 3 (b)). This approach was chosen to obtain an unbiased estimation 
of  error.  Data  augmentation  was  only  applied  to  the  training  data  to 
avoid overlapping the same data in different partitions. To evaluate the 
models we used the coefficients of determination (R2  - calculated as the 
squared  Pearson's  correlation  coefficient),  which  measures  the  per-
centage of variation in the response variable explained by the model. 
Additionally,  we  used  the  root  mean  squared  error  (RMSE)  metric, 
where  RMSE =
,  with  Yi = the  actual  values  pine 
Ì‚
Y i = the predicted pine cove values. In our work, RMSE 
cover values and 
values ranged from 0.0 to 1.0, with values closer to 0 indicating better 
model  predictions.  The  overfitting  was  assessed  by  comparing  model

It is crucial to emphasize that the practical applicability of the modelt 
search  is  constrained.  Often,  many  applications  need  labeled  data  in 
regions  targeted  for  information  transfer,  particularly  if  they  need  to 
consider the test dataset as transfer learning. Moreover, when such data 
is available, it is generally more prudent to incorporate it into model 
calibration and testing processes. 

4.4. Spatial analysis of the modelv

Table 2 shows the results of the preconditions of the modelv using test 
data and the transfer learning data for the five sites, in terms of R2. The 
first two rows show the results for the mono-site models, the first row 
has the performance of the test data and the second row is the average 
over the four transfer learning errors. For example, site 1 has an R2  of 
0.871 for the internal test and an average of 0.429 when evaluated in 
sites 2, 3, 4, and 5. The last two rows have the results for the multi-site 
models. The test data is the average of the errors in each of the sites (we 
used 4 sites for training/validation/test) and the last row has the per-
formance  of  the  transfer  learning  data  which  corresponds  to  a  single 
value (the site not used in the training/validation/test process). In these 
results, it can be observed that while the mono-site models have per-
formance  between  0.871  and  0.950  for  the  test  data,  the  transfer

performance on the training, validation, and test datasets. All models 
were implemented using the Keras library in Python 3.8.3 (Chollet et al.,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training, several postprocessing steps can be performed depending on the specific requirements of the task or application. Some common postprocessing steps include metrics calculation, confusion matrices, saliency maps, and visualizations.

In this case, the authors used the coefficients of determination (R^2) and the root mean squared error (RMSE) as their primary metrics for evaluating the performance of their models. These metrics were calculated based on the actual and predicted values of the target variable, where R^2 measures the proportion of variance in the dependent variable that can be predicted from the independent variables, and RMSE quantifies the difference between the predicted and actual values.

To avoid bias in estimating the error, the dataset was split into three subsets: training (60%), validation (20%), and testing (20%). Data augmentation was applied only to the training data to prevent overlap between different subsets. Overfitting was also considered during the evaluation phase, and the authors compared the performance of the models on both the training and testing sets.

Furthermore, spatial analysis was conducted to evaluate the performance of the models across different geographical locations. Table 2 presents the results of the model's performance on test data and transfer learning data for five sites, measured in terms of R^2. Mono-site models were trained separately for each location, while multi-site models combined data from multiple locations. The results showed that while the mono-site models had high performance on the test data, their transfer performance varied significantly across different locations.

Overall, the postprocessing steps involved in this study included calculating metrics such as R^2 and RMSE, conducting spatial analysis, and considering factors such as overfitting and transfer performance. However, other postprocessing techniques such as saliency maps or confusion matrices could also be employed depending on the specific needs of the project.
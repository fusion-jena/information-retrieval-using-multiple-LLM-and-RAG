Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

â€¢ Second method: use of BBCH_res values during the training phase
(standardization is applied here too); the output is converted to the
BBCH scale through a dictionary; in this case, in order to use the
dictionary, it is necessary to apply the rounding function on the

output before converting it to the BBCH scale, so it is not possible to
evaluate the results with and without rounding separately.

Before proceeding, an important observation must be made: in order
to use the dictionary in the second method, it is necessary to ensure that
the network outputs are consistent with the keys of the dictionary itself;
therefore negative values must be forced to zero, while values that
exceed the upper bound of the keys, i.e. 91, must be forced to 91.
Furthermore, aiming to have a legitimate comparison between the two
methods, these corrections are also made during the execution of the
first method (even if not strictly necessary).

The values of other metrics, namely MAE, SMAPE and NMPIW, are reported in this appendix. In particular, SMAPE and NMPIW are useful because,
unlike conventional metrics, they provide a measure of the quality of prediction intervals in relation to the variability of the observed data. For a good
model, desirable values for SMAPE are typically below 20%, indicating accurate and balanced predictions, while NMPIW values should be close to 0,
reflecting narrow prediction intervals compared with the observed data range. The analysis made for MSE is still considered valid.

EcologicalInformatics82(2024)1027239A. Nappa et al.

A.1. Basic experiment

Here the tables referring to the basic experiment, which uses the re-labeled output (BBCH_res) in both training and prediction phases, are shown.
MAE - Mean Absolute Error.

Deep Neural Networks are architectures that present remarkable
advantages and can potentially map every possible function, but they
are often affected by many issues such as overfitting. The main draw-
back of standard Deep Learning is that Neural Networks compute single
values of their parameters and therefore they are incapable of correctly
assessing the uncertainty related to the data (aleatoric uncertainty) or to
the model itself (epistemic uncertainty); in fact they often produce
overly confident decisions about the correct class, prediction or action to
take, even if the sample is very far from the training data we provided.
The purpose of Bayesian Neural Networks is to use probability theory
to express uncertainty, especially in the prediction phase. The advantage
is clear in areas where decision making plays a fundamental role. To be
more specific, a Probabilistic Neural Network handles the aleatoric
uncertainty by placing a probability distribution (whose parameters can

siderations made earlier, a Bayesian Deep Learning model, if properly
configured and trained, can return satisfactory results even with limited
datasets, which is the scenario studied in the present work.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper discusses several metrics used to evaluate the performance of the deep learning models. These include Mean Absolute Error (MAE), Symmetric Mean Absolute Percentage Error (SMAPE), and Normalized Mean Prediction Interval Width (NMPIW). MAE measures the average magnitude of errors in a set of predictions, without considering their direction. SMAPE provides a measure of the quality of prediction intervals in relation to the variability of the observed data. It is expressed as a percentage and lower values indicate more accurate and balanced predictions. NMPIW is another metric used to assess the quality of prediction intervals. Lower values of NMPIW reflect narrower prediction intervals relative to the observed data range. Additionally, the authors mention that the analysis made for Mean Squared Error (MSE) is still considered valid. However, traditional metrics like accuracy, precision, and recall are not explicitly mentioned in the given context.
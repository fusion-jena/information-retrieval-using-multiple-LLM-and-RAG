Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

http://orcid.org/0000-0002-2176-7935 

R E F E R E N C E S

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., … Kudlur, M. 
(2016). TensorFlow: A system for large- scale machine learning. OSDI, 
16, 265–283.

Anderson, T. M., White, S., Davis, B., Erhardt, R., Palmer, M., Swanson, A., 
… Packer, C. (2016). The spatial distribution of African savannah her-
bivores: Species associations and habitat occupancy in a landscape 
context.  Philosophical  Transactions  of  the  Royal  Society  B:  Biological 
Sciences, 371, 20150314. https://doi.org/10.1098/rstb.2015.0314
Babaee,  M.,  Dinh,  D.  T.,  &  Rigoll,  G.  (2017).  A  deep  convolutional  neu-
ral  network  for  background  subtraction.  Pattern  Recognition,  76, 
635–649.

|  1437

F I G U R E   2  The front screen of the DeepMeerkat GUI. A user 
can select a file or directory of videos to screen using a pre- trained 
model. The path to the model is set under “Advanced settings”

creasing false positives.

majority of hummingbird visitation events (Weinstein, 2015). For the 

For training the fine- tuned neural network, I collected images for 

purposes  of  this  article,  I  assumed  that  all  events  are  captured  by 

each class and trained with a batch size of 100 for 20,000 steps. To 

motion  detection  and  were  passed  to  the  neural  network  for  clas-

reduce training time, the feature vectors for the frozen layers were 

sification.  While  quantitative  benchmarks  are  needed  to  validate 

extracted in parallel using Google Cloud DataFlow. These features 

model performance, they provide a coarse description of the errors 

were then the basis for retraining the new fine- tuned layers. To fit

classes. Dropout is a form of regularisation that randomly removes 

a  proportion  of  nodes  to  reduce  overfitting  (Srivastava,  Hinton, 

Krizhevsky, Sutskever, & Salakhutdinov, 2014). The fully connected 

layer reduces the vector of image features to the desired dimension-

ality of length two (foreground and background). The softmax layer 

normalises  this  vector  into  probabilities  that  sum  to  one  across  all 

classes.  DeepMeerkat  is  designed  to  be  conservative,  with  a  high 

threshold for retaining frames (acceptance value = 0.1). This means 

that the model must be more than 90% confident that a frame does 

not contain a foreground object to assign a background label. This 

prioritises minimising false negatives at the potential expense of in-

Methods in Ecology and Evolu(cid:13)on

    |  1437

tification. OCEANS 2016 MTS/IEEE Monterey, 1–5.

Nogueira, K., Penatti, O. A. B., & dos Santos, J. A. (2017). Towards bet-
ter  exploiting  convolutional  neural  networks  for  remote  sensing 
scene  classification.  Pattern  Recognition,  61,  539–556.  https://doi.
org/10.1016/j.patcog.2016.07.001

Pimm, S. L., Alibhai, S., Bergl, R., Dehgan, A., Giri, C., Jewell, Z., … Loarie, 
S.  (2015).  Emerging  technologies  to  conserve  biodiversity.  Trends 
in  Ecology  &  Evolution,  30,  685–696.  https://doi.org/10.1016/ 
j.tree.2015.08.008

Price  Tack,  J.  L.,  West,  B.  S.,  McGowan,  C.  P.,  Ditchkoff,  S.  S.,  Reeves, 
S.  J.,  Keever,  A.  C.,  &  Grand,  J.  B.  (2016).  AnimalFinder:  A  semi- 
automated  system  for  animal  detection  in  time- lapse  camera  trap 
images. Ecological Informatics, 36, 145–151. https://doi.org/10.1016/ 
j.ecoinf.2016.11.003

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Unfortunately, the given context does not explicitly mention any specific optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD), Adam, etc. However, it can be inferred from the context that some sort of optimization technique was employed during the training process of the fine-tuned neural network.

In the context, it is mentioned that the fine-tuned neural network was trained with a batch size of 100 for 20,000 steps. This suggests that some form of gradient descent optimization algorithm was likely used, as these algorithms typically involve iteratively updating the weights of the neural network based on the gradients calculated from batches of data. Additionally, dropout regularization was also utilized to prevent overfitting, which further indicates that some form of optimization was being performed.

However, without explicit information regarding the specific optimization technique used, we cannot definitively state whether SGD, Adam, or another optimization method was implemented in the deep learning pipeline.
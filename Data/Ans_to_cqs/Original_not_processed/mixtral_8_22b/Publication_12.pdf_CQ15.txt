Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We further improved Waveman by modifying BatNet and opti-
mizing parameter setting of batch size. We add new kind of BNorm
layers behind the 22 convolutional layers to prevent overfitting when
we trained models using BatNet (Fig. 2C). Therefore the model
“learned” to generalize from a trend in both “known” and “unknown”
datasets rather than to maximize the performance on the “known”
datasets (usually called training datasets, Ioffe and Szegedy, 2015).
Computers can only train with small volumes of images at once as they
have too little Random Access Memory or Graphic memory. Batch size
was set to limit the image number. In this study, we set a large batch
size equal to 128, which means training with 128 images for each
iteration.

convolutional layers. Activation functions are Rectified Linear Unit for non-linear classification.

BatNet has a cascade of multiple layers of nonlinear processing units
for feature extraction from low-level features and syntheses to high-
level features (by converting data matrix shapes and synthesis the
matrix). Low-level features are some of raw data points (i.e., pixels in
images). High-level features include frequency, bandwidth, and hun-
dreds of other combined features which may have no specific meaning
in isolation. During synthesis as the data from “shallow” to “deep”
layers, the algorithm weights the features iteratively according to the
reference dataset in order to best describe call structure for specific bat
species. However, the acoustic signal synthesis with huge number of
layers usually leads to information loss to some degree (He et al., 2015).
We, thus, introduced eight shortcuts among layers in order to send
partial original information to the next layer (proposed by He et al.,
2015), which solved the information loss issue as layer number in-

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al., 2011.
Scikit-learn: machine learning in python. J. Mach. Learn. Res. 12, 2825–2830.
Pennell, M.W., Eastman, J.M., Slater, G.J., Brown, J.W., Uyeda, J.C., FitzJohn, R.G., et al.,
2014. Geiger v2. 0: an expanded suite of methods for fitting macroevolutionary
models to phylogenetic trees. Bioinformatics 30, 2216–2218.

Proença, V., Martin, L.J., Pereira, H.M., Fernandez, M., McRae, L., Belnap, J., et al., 2017.
Global biodiversity monitoring: from data sources to essential biodiversity variables.
Biol. Conserv. 213, 256–263.

Rich, L.N., Davis, C.L., Farris, Z.J., Miller, D.A., Tucker, J.M., Hamel, S., et al., 2017.
Assessing global patterns in mammalian carnivore occupancy and richness by in-
tegrating local camera trap surveys. Glob. Ecol. Biogeogr. 26, 918–929.

Russo, D., Jones, G., 2002. Identification of twenty-two bat species (Mammalia: chir-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide explicit details about the optimization techniques used in the deep learning pipeline. However, it mentions that the authors optimized the parameter settings of the batch size, which is a hyperparameter often adjusted during the optimization process. Common optimization techniques used in deep learning pipelines include Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), Root Mean Square Propagation (RMSProp), and others. Without more specific information, it cannot be determined which optimization technique was employed in this case.
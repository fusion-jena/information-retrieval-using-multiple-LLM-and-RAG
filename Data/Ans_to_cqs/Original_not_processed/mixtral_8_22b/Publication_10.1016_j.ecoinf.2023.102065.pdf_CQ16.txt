Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

ence data available for training were limited, so we sought to explore 
different network sizes in conjunction with pre-training with additional 
acoustic reference data from outside of our audio domain, followed by 
fine-tuning with ROI data from our study site.

◦
N, 122.9888

2.8.3. Optimal model and threshold selection

dataset  increased  by  50%  after  this  data  augmentation,  which  is 
important for small training datasets such as ours.

2.5. CNN fine-tuning data from ROIs 

We randomly sampled 50 ROIs per species with “presence” to form a 
held-out test set for accuracy assessment (Section 2.8.1). The remaining 
ROIs with species presence were used to fine-tune the CNN models. We 
randomly sampled 100 of these ROIs per species to form a validation set 
and the remaining formed a training set, which had on average 887 per 
species (min = 349, max = 3094). ROIs that were >2 s in duration were 
used in their entirety. For those that were <2 s in duration, we expanded 
the start/stop (i.e., padded) equally on each side of the ROI to total 2 s. 
Different vocalizations for a given species were combined into a single 
class per species. If any ROIs were overlapping in time from different 
pattern-matching runs for a given species, we placed those ROIs in the 
training set to avoid temporal autocorrelation in our validation and test 
data.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific criteria used to determine when training is completed. However, it mentions that the dataset was increased by 50% through data augmentation, which is important for small training datasets like the one being used. This suggests that the authors might have been looking for ways to improve the performance of their models despite having limited training data.

In general, common criteria used to determine when training is complete include reaching a certain number of epochs or iterations, achieving a desired level of performance on a validation set, observing a plateau in the validation loss, or meeting some other stopping condition based on the learning dynamics of the model. Given that the authors mention using a held-out test set for accuracy assessment and a separate validation set during fine-tuning, it can be inferred that they likely monitored the performance of their models on these sets to decide when to stop training. However, without more specific details, it is difficult to definitively state what exact criteria were used to determine when training was complete.
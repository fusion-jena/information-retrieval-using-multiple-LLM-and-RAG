Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Then we get the model architecture. For the sake of illustration, we use a resnet18 here, but we used
a resnet50 to get the full results presented in the main text.

learn <- cnn_learner(dls = dls,

arch = resnet18(),
metrics = list(accuracy, error_rate))

Now we are ready to train our model. Again, for the sake of illustration, we use only 2 epochs here,
but used 20 epochs to get the full results presented in the main text. With all pictures and a resnet50,
it took 75 minutes per epoch approximatively on a Mac with a 2.4Ghz processor and 64Go memory,
and less than half an hour on a machine with GPU. On this reduced dataset, it took a bit more than
a minute per epoch on the same Mac. Note that we save the model after each epoch for later use.

one_cycle <- learn %>%

fit_one_cycle(2, cbs = SaveModelCallback(every_epoch = TRUE,

fname = 'model'))

0.00%

train_loss

epoch
------ -----------
Epoch
1/2
|
Epoch
|
Epoch
|
Epoch
|

valid_loss
-----------
:
[0/36
:
[1/36
:
[2/36
:
[3/36

Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, et al. 2019. “PyTorch: An Imperative Style, High-Performance Deep Learning Li-
brary.” In Advances in Neural Information Processing Systems 32, edited by H. Wallach, H.
Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett, 8024–35. Curran Associates,
Inc. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-
learning-library.pdf.

Rota, Christopher T., Marco A. R. Ferreira, Roland W. Kays, Tavis D. Forrester, Elizabeth L. Kalies,

27

William J. McShea, Arielle W. Parsons, and Joshua J. Millspaugh. 2016. “A Multispecies Occu-
pancy Model for Two or More Interacting Species.” Methods in Ecology and Evolution 7 (10):
1164–73. https://doi.org/https://doi.org/10.1111/2041-210X.12587.

2

Here, we showcase a full why-what-how workflow in R using a case study on the structure of an
ecological community (a set of co-occurring species) composed of the Eurasian lynx (Lynx lynx)
and its two main preys. First, we introduce the case study and motivate the need for deep learning.
Second we illustrate deep learning for the identification of animal species in large amounts of images,
including model training and validation with a dataset of labelled images, and prediction with a
new dataset of unlabelled images. Last, we proceed with the quantification of spatial co-occurrence
using statistical models.

2 Collecting images with camera traps

We selected at random 80% of the annotated images for each species in the Jura study site for
training, and 20% for testing. We applied various transformations (flipping, brightness and contrast
modifications following Shorten and Khoshgoftaar (2019)) to improve training (see Appendix). To
reduce model training time and overcome the small number of images, we used transfer learning
(Yosinski et al. 2014; Shao, Zhu, and Li 2015) and considered a pre-trained model as a starting point.
Specifically, we trained a deep convolutional neural network (ResNet-50) architecture (He et al. 2016)
using the fastai library (https://docs.fast.ai/) that implements the PyTorch library (Paszke et al.
2019). Interestingly, the fastai library comes with an R interface (https://eagerai.github.io/fastai/)
that uses the reticulate package to communicate with Python, therefore allowing R users to access
up-to-date deep learning tools. We trained models on the Montpellier Bioinformatics Biodiversity

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The context does not provide explicit information about specific data preprocessing techniques such as normalization, scaling, or cleaning. However, it mentions applying various transformations like flipping, brightness, and contrast modifications to improve training. These can be considered part of image augmentation techniques often employed during the preprocessing stage of deep learning projects involving computer vision tasks. The goal is to increase the amount of training data available and prevent overfitting by creating modified versions of existing images. Additionally, the process of selecting a portion of the annotated images for training and another portion for testing can also be seen as a form of data preparation.
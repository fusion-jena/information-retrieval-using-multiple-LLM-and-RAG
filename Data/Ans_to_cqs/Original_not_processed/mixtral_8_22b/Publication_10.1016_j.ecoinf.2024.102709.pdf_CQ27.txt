Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

To determine the ideal training–test split for our model, we used k- 
fold  cross-validation  (k  = 5,  epochs  = 500,  imagesize  = 640).  The 
epochs provide the number of repetitions for training, and the image size 
is  expressed in  pixels. k-fold  cross-validation  (Rodriguez et  al., 2010) 
was used to determine the best training test set for the model. Hereby, 
the data are split into k different training–test sets. The model was not 
trained on the entire dataset but on each training split. The result was an 
investigation of the best data split. Functions of the Python library scikit- 
learn  (scikit-learn  developers,  2023)  were  used  to  split  the  data  and 
investigate the results of each trained model. In addition, YOLO training 
losses and mAP50 values of each model were investigated. The split with 
the  highest  mAP50  value  indicates  the  highest  number  of  correctly 
predicted labels for the model trained on a specific split. We used the

Three models (Models 4, 5, and 6) were trained using our own data 
(clusters of extracted numbers), as described above, as input for training, 
whereas the other three models (Models 1, 2, and 3) were trained with 
the modified National Institute of Standards and Technology (MNIST) 
dataset of handwritten digits, which can be used to train classification 
models (Deng, 2012).

split  data  with  the  highest  mAP50  (0.987)  for  the  prediction  of  our 
model.

TP 

37.12 
41.05 

FN 

20.15 
31.71 

0.0374 * 
0.0455 * 

2.3818 
3.2014 

TN 

34.25 
34.40 

FP 

44.48 
47.28  

Table 6 
Summary of the models trained for number recognition. Models 4, 5, and 6 were trained on the numbers from our point cloud data, while the others (model 1, 2 and 3) 
were trained on numbers from the MNIST dataset ((1) means that the original MNIST data was used, whereas (2) refers to MNIST data that was changed in size and 
placement to resemble our data). Model 4 (bold) is the model that was most suitable to predict numbers in our data. The training time was taken on a Lenovo Legion 
Y520 with Linux Mint 20.2 Cinnamon Edition operating system.  

Nr 

Base model 

Training Data 

Training Epochs 

Train – Val Split 

Precision 

Recall 

mAP50 

Training Time 

1 
2 
3 
4 

5 

6 

YOLOv8n 
#4 
#4 
YOLOv8n 

YOLOv8n 

#1 

MNIST(1) 
MNIST(2) 
MNIST(2) 
Tree  
Markings 
Tree  
Markings 
Tree  
Markings 

10 
10 
3 
500 – early stopping: 270

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention the deployment process of the trained deep learning model. However, it provides information about the training process and evaluation metrics. To deploy a trained deep learning model like this one, common steps include model serialization, platform selection, and integration with existing systems or applications.

Model serialization involves saving the trained model parameters and architecture in a format that can be loaded later for inference. This could be done using popular libraries such as TensorFlow's `tf.saved_model` or PyTorch's `torch.save`.

Platform selection depends on various factors, including hardware requirements, scalability needs, and ease of integration. Some popular platforms for deploying deep learning models include AWS SageMaker, Google Cloud AI Platform, Azure Machine Learning, and edge devices like NVIDIA Jetson boards.

Integration with existing systems or applications may require additional work, depending on the complexity of the application and the chosen deployment platform. This might involve setting up APIs, creating custom frontends, or integrating with databases and other backend services.
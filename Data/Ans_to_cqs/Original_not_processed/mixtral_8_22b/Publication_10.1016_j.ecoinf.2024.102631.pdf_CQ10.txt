Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

structures with an increasing number of output channels, encouraging 
the network to learn complex representations. To enhance the network's 
capacity  for  capturing  fine-grained  features  and  context,  Spatial  and 
Channel Attention (SCA) modules are integrated into the architecture. 
These modules act as mechanisms for focusing on relevant spatial re-
gions  and  channel-wise  information,  respectively.  These  attention 
mechanisms are integrated at multiple stages in the network to enrich 
the feature representations. The decoder upscales the feature maps and 
fuses  information  from  the  encoder  using  skip  connections.  Each 
decoder block consists of a transposed convolutional layer to increase 
spatial  resolution,  followed  by  batch  normalization  and  ReLU  activa-
tion.  The  final  stage  of  the  network  involves  upsampling  the  feature 
maps, followed by a convolutional layer with 4 × 4 filters to produce the

The network takes images with dimensions of 256 × 256 pixels and 
three  color  channels  as  input.  It  follows  a  fully  convolutional  design, 
with  each  layer  applying  2D  convolutions  using  4  × 4  filters.  Batch 
Normalization and Leaky-ReLU activation functions are used after each 
convolution  layer  to  facilitate  network  training  and  stability.  The 
encoder consists of 5 blocks, which progressively reduce the spatial di-
mensions and learn features. It starts with a convolutional layer with 32 
output channels and strides of 2, followed by a residual block. The re-
sidual block comprises a Conv-ReLU-Conv structure, where a convolu-
tional layer is followed by a Rectified Linear Unit (ReLU) activation and 
another convolutional layer. This configuration is designed to effectively 
capture  and  enhance  image  features,  serving  as  a  critical  component 
within the network's architecture. The residual block output is then fed

(cid:0)

(cid:0)

⃦
⃦
X , Z))

G

j

(6) 

Where, Lper  represents the perceptual loss. j refers to the index of the 
chosen  layer  in  the  VGG-19  network.  Cj,  Hj,  and  Wj  are  the  channel 
number, height, and width of the feature map in the jth layer of VGG-19. 
Φj(G(X, Y) ) is the feature representation of the generated underwater 
image G(X, Y) in the jth layer. Φj(Y) is the feature representation of the 
ground truth image Y in the jth layer. In this formulation, the perceptual 
loss measures the dissimilarity between feature maps extracted from the 
VGG-19  network,  helping  to  guide  the  optimization  process  toward 
generating  images  that  align  more  closely  with  the  features  of  the 
ground truth images.

24.36 
0.885 

15.25 
0.656 

25.59 
0.893  

EcologicalInformatics81(2024)1026319A.B. Bakht et al.                                                                                                                                                                                                                                

Table 2 
Quantitative  comparison  on  C-60  dataset  using  UIQM,  UCIQE,  and  NIQE 
metrics.   

UIQM↑ 

UCIQE↑ 

NIQE↓ 

Raw_Images 
Fusion 
IBLA 
U-Transformer 
Funie-GAN 
Water-Net 
UWCNN 
Ours 

1.99 
2.78 
1.81 
2.65 
3.10 
2.57 
2.25 
3.12 

0.478 
0.512 
0.574 
0.534 
0.572 
0.578 
0.466 
0.591 

5.18 
4.74 
5.02 
4.94 
4.73 
4.88 
4.89 
4.67  

Table 3 
Quantitative comparison on U45 dataset using UIQM, UCIQE, and NIQE metrics.   

UIQM↑ 

UCIQE↑ 

Raw_Images 
Fusion 
IBLA 
U-Transformer 
Funie-GAN 
Water-Net 
UWCNN 
Ours 

2.44 
3.14 
1.60 
3.10 
3.21 
3.18 
2.82 
3.23 

0.481 
0.532 
0.579 
0.553 
0.602 
0.587 
0.471 
0.612 

NIQE↓

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given context, there is no explicit mention or reference to any specific code repository link for the described deep learning pipeline. Therefore, I cannot provide the requested information. Please refer to other sources or contact the authors directly for further details regarding the code repository link.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(2.2% reduction for DANN,
predictions by the plain NN model
p = 0.0002, Figure S4). In the GH!LL prediction, a similar trend was
observed (Figure S5) and the target accuracy was not significantly dif-
ferent from the NN (0.015% reduction for DANN, p = 0.98). Loss and

accuracy development during training of models are reported in

Figures S2, S3.

Classification error

Classification error was visualized as a scaled confusion matrix. Starting

with a trial for a within-dataset analysis with 400 training images in the

LH random sampling showed that the large taxonomic groups were cor-
rectly classified in most cases (Table S4). For example, four families

In order to test the effect of the presence of unknown inputs (out-of-

distribution samples) on the classification, we first used an LH-trained
model to predict the class of 16 LH images belonging to eight families/
subfamilies, Coccinellidae, Elateridae, Endomychidae, Hydrophilidae,

offering high numbers of images per taxon, which is more difficult to

200 images in each of the three datasets used here. However, the

obtain locally, although it is critical for increasing the performance of

degree of accuracy is greatly affected by the image quality and the

the CNN-based classification (Figure 2; Donahue et al., 2013; Valan

complexity of the dataset: both the LL (low image quality) and in

et al., 2019, 2021).

(a)

(b)

F I G U R E 6

Prediction probabilities for the successful, failed and out-of-distribution predictions at a 0.95 threshold (horizontal line). (a) Intra-
dataset predictions of LH images using 400 randomly selected images for training. (b) Predictions of LH images using 800 GH images for training.

average accuracy reduction. Higher accuracy reduction indicates a
worse performance on prediction compared to the within-dataset
prediction accuracy. The solid and dashed lines represent results of
the convolutional neural network (CNN) and domain adversarial
neural network (DANN), respectively. Note that only the LL to LH
prediction accuracy improved with the use of DANN.

and TensorFlow 2.5.0 (https://www.tensorflow.org) libraries, and all

statistical analyses were conducted with R 4.1.0 (R Core Team, 2021).

Between-datasets classification

For the between-dataset prediction, the CNN model was trained with

RE SU LT S

a source dataset to predict images from a different target dataset. The

NN was trained with N images randomly selected from the source

Performance of within-dataset classification

dataset, which was then used to predict all images of the target data-
set and AccT and ΔAcc were measured. We ran the above procedures
for three source–target pairs (training dataset!predicted dataset),
GH!LH, GH!LL and LL!LH. These settings simulate two alternative
scenarios: (i) a global image database is used to predict local samples
(GH!LH and GH!LL) and (ii) conventional

images, as

those

Effects of datasets and the number of images

The accuracy of within-dataset classification and the effect of the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The performance of the deep learning model is evaluated using several metrics such as accuracy, precision, recall, F1 score, and area under the curve (AUC). In this case, the main metric used to measure the performance of the model is accuracy. Specifically, the average accuracy reduction is used to compare the performance of the Convolutional Neural Network (CNN) and Domain Adversarial Neural Network (DANN) models. A higher accuracy reduction indicates worse performance on prediction compared to the within-dataset prediction accuracy. Additionally, the study also uses statistical tests to determine if there is a significant difference between the performances of the models.
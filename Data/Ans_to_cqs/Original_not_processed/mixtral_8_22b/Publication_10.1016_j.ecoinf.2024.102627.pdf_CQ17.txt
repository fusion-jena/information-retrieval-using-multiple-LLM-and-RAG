Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(cid:0)

̂ηs(x) = exp
exp

∑

j

)

θ(x)
f s
(
f j
θ(x)

)

where θ is the set of parameters of the neural network to be optimised by 
minimizing the loss function of Eq. (2). 

Using this very common deep learning framework, it is possible to 
show that the species assemblage predictor Sλ(x) of Eq. (1) is consistent 
λ(x) when  the 
(Lorieul,  2020),  i.e.  it  tends  towards  the  optimal  set  S*
number  of  training  samples  increases.  In  other  words,  our  species 
assemblage predictor is as simple as training a deep neural network with 
a cross-entropy loss function on the presence-only samples and thresh-
olding  the  output  softmax  probabilities  to  get  the  assemblage  of  pre-
dicted species. 

◦

Zizka, A., Andermann, T., Silvestro, D., 2022. Iucnn–deep learning approaches to 

approximate species’ extinction risk. Divers. Distrib. 28, 227–241. 

EcologicalInformatics81(2024)10262715

Finally,  we  can  chose  λ  so  as  to  minimize  the  average  species 
assemblage size E[|Sλ(X) |] - which is equivalent to maximize λ  - while 
guarantying that the average error rate is lower than an ϵ objective: 

argmin
λ∈[0,1]

E[|Sλ(X) |]

max
λ∈[0,1]

(λ)

⇔

s.t. ℰ(Sλ) ≤ ϵ

s.t. ℰ(Sλ) ≤ ϵ

(3) 

This is equivalent to what is called conformal prediction in machine 
learning (Fontana et al., 2023) and guarantees that the actual species is 
contained within the set with probability 1 (cid:0) ϵ. 

2.3. Conservation indices for species assemblages 

2.3.1. Definition of the indices 

In addition to the classical Shannon index ℐH , we define two novel 
indices characterizing the extinction risk of a predicted species assem-
blage, ℐc and ℐO . They respectively render the proportion of threat-
ened species in the assemblage and the most critical IUCN status in the 
assemblage. Let’s break down their construction. 

2.3.2.

IUCN status notations

Deep learning (DL) offers an unprecedented opportunity to charac-
terize complex, scale-dependent relationships between species and their 
environment (Deneu et al., 2021). In addition, the ever-increasing vol-
ume of data stemming from citizen science observations on one hand, 
and  from remote  sensing  characterization  of environmental heteroge-
neity on the other hand, requires adapted DL workflows (Borowiec et al., 
2022). DL models can learn from complex effects and interactions be-
tween  environmental  predictors  (Puglielli  and  P¨artel,  2023),  and  Cai 
et al. (2022) have shown that DL can help to isolate relationships be-
tween biodiversity and ecological drivers.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The given context does not provide explicit information about the specific regularization methods used in the described deep learning pipeline. However, based on general knowledge of deep learning practices, some commonly used regularization techniques to prevent overfitting include Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overreliance on any single feature. On the other hand, L2 Regularization adds a penalty term to the loss function, encouraging smaller weights and thus reducing complexity and preventing overfitting. Other popular regularization methods include early stopping, weight decay, and batch normalization.
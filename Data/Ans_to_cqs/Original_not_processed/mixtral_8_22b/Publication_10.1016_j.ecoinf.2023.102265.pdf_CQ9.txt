Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

ResNet-50 is made up of five convolutional blocks stacked on top of 
one  another  (Fig.  5).  The  image  features  are  extracted  from  the  fully 
connected layers pre-trained with the ImageNet database (https://imag 
e-net.org/index.php). The input to the network is an image of 224x224 

EcologicalInformatics77(2023)1022654S. Singha Roy et al.

easy to train with reduced complexity, even though they have deeper 
layers than CNN models, because of the presence of skip connections 
between the input and output of each block (He et al., 2016). ResNet has 
different variants with a variety of convolution layers. We trialled three 
depths, ResNet-18, (cid:0) 50 and (cid:0) 101, and compared their training times 
and validation accuracies. They behaved similarly but ResNet-50 had a 
marginally  higher  validation  accuracy  (Section  3.1.1)  leading  us  to 
choose it for feature extraction.

EcologicalInformatics77(2023)1022657S. Singha Roy et al.                                                                                                                                                                                                                            

Fig. 7. Visualisation of extracted features using a pre-trained ResNet-50 CNN model showing that the background microhabitat classes are well structured. Green 
triangles represent ‘natural’ microhabitats and the light brown circles represent ‘anthropogenic’ mirohabitats. (For interpretation of the references to colour in this 
figure legend, the reader is referred to the web version of this article.) 

Table 1 
Comparison  of  training  time  for  pre-trained  ResNet-18/50/101  models  for 
extracting features of backgrounds of 500 honey bee images.   

ResNet-18 

ResNet-50 

ResNet-101 

Training time 

1.25 s 

1.2 s 

1.22 s

performance  compared  to  features  extracted  from  shallower  layers 
(Zeiler and Fergus, 2014). These deeper layers more effectively capture 
class-specific attributes  like  shape, texture,  and colour.  Therefore,  we 
extracted  features  from  the  21st  layer  of  the  CNN,  last  layer,  a  fully 

EcologicalInformatics77(2023)1022655(a)(b)S. Singha Roy et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The information regarding the availability of the deep learning pipeline's data repository is not explicitly mentioned within the given context. However, based on common practices in scientific research, one can assume that the authors might have shared their dataset or code in a public repository such as Zenodo, Figshare, Dryad, or GBIF. To find out where exactly the data repository is located, I recommend checking the original paper or contacting the corresponding author directly.

Based on the context provided, it appears that the study uses a pre-trained ResNet-50 CNN model for extracting features from images. This suggests that the researchers may have used publicly available datasets, such as ImageNet, for training their model. Nevertheless, without further details about the specific implementation of the deep learning pipeline, it remains unclear whether the authors have also released any custom datasets or code related to their work.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4. Hyperparameter optimisation 

Hyperparameter  optimization  searches  for  the  best  set  of  hyper-
parameters  for  a  machine  learning  model  that  can  perform  best  on  a 
given  task.  In  many  packages,  random  or  grid  search  is  the  default 
method  for  selecting  optimal  hyperparameters  (Snoek  et  al.,  2012). 
There are many studies on the selections of optimization algorithms for 
theoretical  problems  or  engineering  applications,  in  which  various 
versions  of  Bayesian  optimizers  and  Swarm-based  algorithms  were 
compared (Stenger et al., 2019; Stenger and Dirk, 2022).

Bayesian optimization is a family of algorithms used for the global 
optimization of black-box functions. These algorithms utilize Bayesian 
statistical  techniques  to  model  the  unknown  objective  function  and 
iteratively explore the search space to find the optimal solution. Each 
algorithm has its characteristics, strengths, and weaknesses, making it 
suitable for optimization problems. The choice of algorithm is subject to 
specific requirements of the problem at hand, such as the dimensionality 
of  the  search  space,  the  presence  of  noise, and  the  computational  re-
sources  available.  Typically,  optimization  is  problem-dependent,  and 
comparisons  are  worth  making  in specific  contexts.  In  this study, the 
hyperparameters of XGBoost and LightGBM were tuned by several op-
timizers, using RMSE as the objective (loss) function.

Forest biomass provides a quantitative assessment for carbon stock marketing on a national or regional scale. 
Some countries have committed to net zero carbon emissions, so proper biomass estimations are essential. This 
study investigates the uses of machine learning (LightGBM, XGBoost), in which hyperparameters were tuned by 
Bayesian-based Optimisers and a novel Tasmanian Devil Optimisation algorithm for estimates of aboveground 
biomass (AGB) using Sentinel 1A, Landsat images, and ground survey data. A province in the northern part of 
Vietnam was selected as a case study since the change in land cover has been considered crucial. The models 
were optimized/trained and validated using statistical indicators, namely, root mean square error (RMSE), co-
efficient of determination (R2), and mean absolute error (MAE). The trained models were further explained using 
SHAP values to understand better how they perform and the contribution of each feature to the overall estimates.

As  machine  learning  models  have  been  increasingly  deployed  in 
critical applications, there may be a growing demand for interpretable 
and explainable hyperparameter optimization techniques. Methods that 
provide transparency and explainability in the hyperparameter tuning 
process  could  gain  importance  to  ensure  fairness,  accountability,  and 
transparency in developing machine learning models. Machine learning 
algorithms are considered robust classification and regression methods 
in which models are trained and learned from input data to improve the 
prediction of unseen data. However, the internal process is not under-
standable, as  it is  considered a  black-box stage. To  better understand 
interactions  between  features,  we  evaluated  several  performance  in-
dicators,  such  as  feature  importance,  feature  interactions,  and  contri-
butions of all features. The Shapley values provide a way to distribute

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The hyperparameters of the model are optimized using various methods, including grid search, random search, and Bayesian-based optimizers. Grid search involves specifying a range of possible values for each parameter and then testing every combination of those parameters to determine the best one. Random search, on the other hand, randomly selects combinations of parameters within specified ranges and tests them until finding the best combination. Bayesian-based optimizers use Bayesian statistics to iteratively update their beliefs about the optimal hyperparameters based on previous evaluations. In this study, the hyperparameters of XGBoost and LightGBM were tuned by several optimizers, including Bayesian-based optimizers and a novel Tasmanian Devil Optimisation algorithm, with RMSE as the objective function.
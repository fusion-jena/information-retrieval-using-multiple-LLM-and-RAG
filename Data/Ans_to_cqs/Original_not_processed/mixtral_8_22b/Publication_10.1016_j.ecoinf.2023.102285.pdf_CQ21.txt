Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The rest of this document is structured as follows: The methodology 
is revealed in Section 2. The datasets on which the methodology is built 
and tested are explored in Section 3. A summary of the findings is pro-
vided in Section 4. Discussions are outlined in Section 5. Section 6 draws 
a conclusion as well as future potential works. Finally, the limitations of

Kuru, K., Girgin, S., Arda, K., Bozlar, U., 2013. A novel report generation approach for 
medical applications: the sisds methodology and its applications. Int. J. Med. Inform. 
82 (5), 435–447. http://dx.doi.org/10.1016/j.ijmedinf.2012.05.019. 

Kuru, K., Clough, S., Ansell, D., McCarthy, J., McGovern, S., 2023. Wildetect: an 

intelligent platform to perform airborne wildlife census automatically in the marine 
ecosystem using an ensemble of learning techniques and computer vision. Expert 
Syst. Appl. 231, 120574 http://dx.doi.org/10.1016/j.eswa.2023.120574. 

Leira, F.S., Johansen, T.A., Fossen, T.I., 2015. Automatic detection, classification and 

tracking of objects in the ocean surface from uavs using a thermal camera. In: 2015 
IEEE Aerospace Conference, pp. 1–10. http://dx.doi.org/10.1109/ 
AERO.2015.7119238.

0.982 

0.927 

0.96 

Sp 

0.997 

0.987 

0.994 

0.95 

example 

Fig. 16b 

Fig. 12b 

Fig. 14b 

Fig. 13b  

Table 10 
Detailed confusion matrix of the classifiers outlined in Table 11.    

A. Test Results (UCLAN) 

B. Evaluation (UCLAN) 

Actual Class  

Actual Class  

C. Validation (APEM)   

Actual Class    

Positive 

Negative 

% 

Positive 

Negative 

% 

Positive 

Negative 

% 

Pred 

Positive 
Negative 
% 

140 (TP) 
5 (FN) 
0.9655 (Se) 

3 (FP) 
4997 (TN) 
0.9994 (Sp) 

0.9790 (PPV) 
0.9990 (NPV) 
0.9984 (ACC) 

55 (TP) 
2 (FN) 
0.9649 (Se) 

2 (FP) 
4998 (TN) 
0.9996 (Sp) 

0.9649 (PPV) 
0.9996 (NPV) 
0.9992 (ACC) 

9 (TP) 
0 (FN) 
1 (Se) 

0 (FP) 
50 (TN) 
1 (Sp) 

1 (PPV) 
1 (NPV)  
1 (ACC)  

Table 11 
Test, evaluation and validation results in summary detailed in Table 10.  

Phase 

Positive 

Negative 

TP 

FN 

TN 

FP 

Se 

SP 

PPV 

NPV 

ACC 

Location 

Check 

Test 
Evaluation 
Validation 
Verification 

145 
57 
9 
211 

5000 
5000 
50 
10,050

EcologicalInformatics78(2023)1022858K. Kuru et al.                                                                                                                                                                                                                                    

Fig. 12. Stationary example 2: man-made object detection.  

overfitting, and finally iii) let field experts evaluate the model with a 
completely new dataset independent from the first and second datasets. 
The model can be deployed if it passes these three phases successfully. 
These phases are outlined in Fig. 3. The obtained results as well as their 
evaluation  are  provided  in  the  following  section.  The  experimental 
design of data utilisation and data processing phases with their targeted 
objectives are outlined in Table 3 regarding the APEM‘s database. The 
viability of the methodology was ensured in 4 phases.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training phase, there are several postprocessing steps involved. These include evaluating the performance of the trained models through metrics calculation, generating saliency maps, and creating confusion matrices.

Metrics calculation involves computing various statistical measures such as accuracy, precision, recall, F1 score, etc., to assess the performance of the trained models. This helps in understanding how well the models have learned from the given data and how accurately they can predict or classify new instances.

Saliency maps are visualizations that highlight the regions in the input image that contribute most significantly towards the final prediction made by the model. They help in interpreting the decision-making process of the model and identifying any biases or errors in the predictions.

Confusion matrices provide a more detailed analysis of the performance of the trained models by comparing the actual labels against the predicted ones. They show the number of true positives, false positives, true negatives, and false negatives, allowing us to calculate additional metrics like sensitivity, specificity, positive predictive value, negative predictive value, and overall accuracy.

Based on the provided context, we can see that the authors used confusion matrices to evaluate the performance of their proposed methodology. For instance, in Table 10, they present a detailed confusion matrix of the classifiers outlined in Table 11. Similarly, in Table 11, they summarize the test, evaluation, and validation results detailed in Table 10. From these tables, we can infer that the authors calculated various metrics such as sensitivity, specificity, positive predictive value, negative predictive value, and overall accuracy to evaluate the performance of their proposed methodology.
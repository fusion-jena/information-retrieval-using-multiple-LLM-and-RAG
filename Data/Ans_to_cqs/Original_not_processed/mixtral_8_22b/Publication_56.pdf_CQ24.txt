Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

These analyses help to understand the behaviour of deep learning models in agricultural applications. In par-
ticular, in the CAP activities in which payments to farmers must be well-founded, the use of classification models 
providing explainable predictions are of great interest. The conducted work not only confirm well established 
knowledge in remote sensing science but also opens the door to new studies in the field of the comprehensibility 
of deep learning algorithms in agricultural and environmental applications.

sample with its corresponding remote sensing sequence of data. Finally, 70% of the data were used for training 
the algorithms whereas the remaining 30% were used only for validation.

In RS image classification, exploiting the temporal domain with RNNs is of paramount relevance. Conven-
tional RNNs, however, present instabilities and problems during the training phase because backpropagated 
gradients tend to fade over time, which produces difficulties with learning long-term dependencies. LSTM 
networks mitigate this by incorporating a series of steps to decide which information is going to be stored 
(“memorized”), and which deleted (“forgotten”). Thus, the network has a certain “memory”. Furthermore, if the 

Scientific Reports |        (2020) 10:17188  | 

https://doi.org/10.1038/s41598-020-74215-5

5

Vol.:(0123456789)www.nature.com/scientificreports/Figure 5.   Heatmap of the 2-BiLSTM activations for every date (top), and mean squared activations in the 
temporal domain for the four considered BiLSTM architectures (bottom).

Figure 6.   Activations observed in the 2-BiLSTM network for (top) natural vegetation and permanent crops, 
and (bottom) annual crops.

The use of deep learning (DL) approaches for the analysis of remote sensing (RS) data is rapidly 
increasing. DL techniques have provided excellent results in applications ranging from parameter 
estimation to image classification and anomaly detection. Although the vast majority of studies report 
precision indicators, there is a lack of studies dealing with the interpretability of the predictions. This 
shortcoming hampers a wider adoption of DL approaches by a wider users community, as model’s 
decisions are not accountable. In applications that involve the management of public budgets or 
policy compliance, a better interpretability of predictions is strictly required. This work aims to deepen 
the understanding of a recurrent neural network for land use classification based on Sentinel‑2 time 
series in the context of the European Common Agricultural Policy (CAP). This permits to address

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it appears that the focus of the research is on using deep learning methods, specifically Long Short-Term Memory (LSTM) networks, for classifying land use in the context of the European Common Agricultural Policy (CAP). However, the context does not provide any specific details about how randomness in the deep learning pipeline, such as the random seed value, was handled. Therefore, I cannot provide a direct answer to your query based solely on the given context.

However, generally speaking, there are several strategies that can be employed to handle randomness in the deep learning pipeline:

1. Fixed Random Seeds: One common approach is to set a fixed random seed value before running experiments. This ensures that the same initial conditions are used each time, making the results reproducible.

2. Multiple Runs: Another strategy is to run the experiment multiple times with different random seeds and then average the results. This helps to reduce the impact of any single random initialization.

3. Cross-Validation: Using cross-validation can also help to reduce the impact of randomness. By splitting the dataset into multiple folds and averaging the results across all folds, the variance due to randomness can be reduced.

4. Regularization Techniques: Regularization techniques like dropout or weight decay can also help to reduce the impact of randomness by preventing overfitting to the training data.
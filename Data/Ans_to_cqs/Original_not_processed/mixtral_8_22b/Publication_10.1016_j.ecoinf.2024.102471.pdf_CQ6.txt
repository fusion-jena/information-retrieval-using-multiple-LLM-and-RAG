Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Fig. 2. Wav2Vec pre-training phase.  

quantized representation for the masked latent feature representation. 
Diversity loss shown in (2) is also added to the objective function for 
regularization during pre-training. 

(

Lm = (cid:0)

log

sim(ct ,qt )
e
k

/

∑

q′∈Qt

sim(ct ,q′)
e
k

Ld =

(cid:0)
*

1
GV

) )

(cid:0)

pg

(cid:0) H

(1)  

(2) 

In the supervised fine-tuning phase, the labelled dataset is used in 
training the model to predict particular words or phonemes. Fig. 3 de-
picts the process and components in the fine-tuning phase of wav2vec. 
Phonemes are the smallest unit of sound, usually one or two letters, in 
the language. During fine-tuning, the quantization module is removed. 
Instead, a linear projection layer is added to the context network. Then 
the model is fine-tuned on connectionist temporal classification (CTC) 
loss for the Automatic Speech Recognition task. So, the wav2vec model 
has a general understanding of phonemes present in human speech.

The  self-supervised  training  phase  comprises  four  important  ele-
ments: feature encoder, context network, quantization module, and pre- 
training  objective.  Fig.  2  depicts  the  overall  pre-training  approach  of 
wav2vec. Contrastive learning is the idea of recognizing whether two 
different types of transformation of the input are the same or not. The 
two transformations used in the Wav2vec model are the context repre-
sentation from the context network and the final quantization vectors 

EcologicalInformatics80(2024)1024713B. Swaminathan et al.                                                                                                                                                                                                                         

Fig. 2. Wav2Vec pre-training phase.

3.1. Feature encoder 

The feature encoder of the pre-trained Wav2vec model reduces the 
dimensionality of the audio data, converting the raw waveform into a 
sequence of feature vectors Z every 20 milliseconds. The main compo-
nent of the feature encoder's architecture is seven layers of a 1D con-
volutional neural network with 512 channels at each layer. The size of 
the kernel and strides decreases as the layer progresses in the network. 
The waveform is normalized before passing through the convolutional 
layer, followed  by  layer  normalization and  GELU  activation function. 
Finally,  latent  feature  vectors  represent  the  latent  feature  of  bird 
vocalization.  Thus,  it  helps  the  model  to  identify  and  understand  the 
sequential information in the vocalization audio of multi-bird species. 

3.2. Context network

Wav2vec is one of the latest state-of-art models for Automatic Speech 
Recognition  and  other  audio-related  problems  (Boigne  et  al.,  2020) 
(Shibano et al., 2021). The architecture of the Wav2vec is based on the 
transformer's  encoder,  with  a  training  objective  like  BERT's  masked 
language modelling objective but adapted for speech instead of text. The 
self-supervised  way  of  training  allows  Wav2vec  to  pre-train  on  unla-
belled data, which is more accessible (Hendrycks et al., 2019). Later, the 
model can be fine-tuned on a particular dataset for a specific purpose. 
This  approach  shows  remarkable  improvement  in  the  model's  perfor-
mance, especially if the dataset size is very small with label.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model such as Wav2Vec, several preprocessing steps may be involved depending on the nature of the input data. In the case of Wav2Vec, which deals with audio data, some common preprocessing steps include:

1. Normalization: This step involves adjusting the values of the input features so they fall within a specific range, typically between -1 and 1 or 0 and 1. For example, in Wav2Vec, the raw waveform is normalized before being passed through the convolutional layer.

2. Dimensionality reduction: To reduce computational complexity and improve performance, high-dimensional input data can be transformed into lower-dimensional representations. In Wav2Vec, this is achieved using a feature encoder that converts the raw waveform into a sequence of feature vectors.

3. Data augmentation: This technique involves creating new samples by applying various transformations to existing ones, thereby increasing the diversity of the training set. Although not explicitly mentioned in the given context, data augmentation techniques such as noise injection, time stretching, and pitch shifting could potentially be applied to audio data.

4. Cleaning: Depending on the quality of the input data, it might be necessary to remove any irrelevant or redundant information. However, since the given context does not provide details about the input data, it is difficult to determine if any cleaning steps were performed prior to training the Wav2Vec model.
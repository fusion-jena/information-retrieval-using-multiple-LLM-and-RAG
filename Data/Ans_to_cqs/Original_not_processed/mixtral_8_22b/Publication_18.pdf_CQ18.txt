Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

trends. J. Exp. Mar. Biol. Ecol. 368, 1–8 (2009).

 54.  Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks. Nature 542, 115–118 (2017).
 55.  Silver, D. et al. Mastering the game of Go with deep neural networks and tree search. Nature 529, 484–489 (2016).
 56.  Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015).
 57.  Dodge, S. & Karam, L. Understanding how image quality affects deep neural networks. In 2016 Eighth International Conference on 

Quality of Multimedia Experience (QoMEX) 1–6, https://doi.org/10.1109/QoMEX.2016.7498955 (2016).

 58.  Kim, J., Lee, J. K. & Lee, K. M. Accurate Image Super-Resolution Using Very Deep Convolutional Networks. in Proc. CVPR IEEE 

1646–1654, https://doi.org/10.1109/CVPR.2016.182 (2016).

 59.  Tabik, S., Peralta, D., Herrera-Poyatos, A. & Herrera, F. A snapshot of image pre-processing for convolutional neural networks: case

resolution satellite imagery. Polar Biol. 35, 963–968 (2012).

 76.  Buchanan, G. M. et al. Free satellite data key to conservation. Science 361, 139–140 (2018).
 77.  Popkin, G. Technology and satellite companies open up a world of data. Nature 557, 745 (2018).
 78.  Pettorelli, N., Owen, H. J. F. & Duncan, C. How do we want Satellite Remote Sensing to support biodiversity conservation globally? 

Methods Ecol. Evol. 7, 656–665 (2016).

 79.  Abadi, M. et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. ArXiv160304467 Cs (2016).
 80.  Huang, J. et al. Speed/accuracy trade-offs for modern convolutional object detectors. ArXiv161110012 Cs (2016).
 81.  Krizhevsky, A., Sutskever, I. & Hinton, G. E. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in 
Neural Information Processing Systems 25 (eds Pereira, F., Burges, C. J. C., Bottou, L. & Weinberger, K. Q.) 1097–1105 (Curran 
Associates, Inc., 2012).

1

www.nature.com/scientificreportsnumber of annotated images to later make correct predictions on new images37. Although the construction of a 
dataset for training is costly, the learning of CNNs on small datasets can be boosted by data-augmentation, which 
consists of increasing the volume of the training dataset artificially, and additionally by transfer learning, which 
consists of starting the learning of the network from a prior knowledge rather from scratch38,39.

Table 3.  Accuracy indicator, equation, and interpretation of the performance assessment.

Step-2: Whale counting phase.  We built the second CNN-based model that counts whales by refor-
mulating the problem into an object detection task. We used the detection model Faster R-CNN based on 
Inception-Resnet v2 CNN architecture42,81, pre-trained on the well known COCO (Common Objects in Context) 
detection dataset, which contains more than 200,000 images organized into 80 object categories82. The two last 
fully connected layers of the network were retrained on our dataset using a learning rate of 0.001 and a decay 
factor of 16 every 30 epochs. As optimization algorithm, we used RMSProp with a momentum of 0.9 and epsilon 
of 0.1.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific strategy implemented to monitor the model performance during training. However, it mentions the use of a learning rate of 0.001 and a decay factor of 16 every 30 epochs, along with the RMSProp optimization algorithm with a momentum of 0.9 and epsilon of 0.1. These details suggest that the authors might have monitored the model performance during training by observing metrics such as loss or accuracy after each epoch and adjusting the learning rate and other hyperparameters accordingly. Additionally, they may have employed techniques like early stopping or validation set monitoring to prevent overfitting and ensure optimal model performance.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Yang, L., Shami, A., 2020. On hyperparameter optimization of machine learning 

algorithms: theory and practice. Neurocomputing 415, 295–316. 

Shorten, C., Khoshgoftaar, T.M., 2019. A survey on image data augmentation for deep 

Yang, C.-L., Harjoseputro, Y., Hu, Y.-C., Chen, Y.-Y., 2022a. An improved transfer- 

learning. J. Big Data 6 (1), 1–48. 

Sitepu, A.C., Liu, C.-M., Sigiro, M., Panjaitan, J., Copa, V., 2022. A convolutional neural 
network bird’s classification using north american bird images. J. Health Sci. 6 (S2), 
15067–15080. 

Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., Patwary, M., 

Prabhat, M., Adams, R., 2015. Scalable bayesian optimization using deep neural 
networks. In: International Conference On Machine Learning. PMLR, pp. 2171–2180. 

learning for image-based species classification of protected indonesians birds. CMC 
Comp. Mater. Continua 73 (3), 4577–4593.

networks  experienced  overfitting,  failing  to  generalize  effectively. 
Several  data  augmentation  techniques  were  employed  to  address  this 
issue.  Data  augmentation  expands  a  dataset  by  applying  trans-
formations, such as flipping, translation, and shear, to the existing data 
(Shorten and Khoshgoftaar, 2019). Data augmentation helps maintain 
dataset balance, reduce overfitting, and improve the generalization of 
deep learning models (Mungloo-Dilmohamud et al., 2022). In this study, 
horizontal and vertical translations were done within a range of (cid:0) 30 to 
30 pixels, allowing random shifts. Shear distortion was applied within a 
range of (cid:0) 5 to 5, representing maximum distortion to the left and right. 
Table 3 details the transformations used for data augmentation on the 
selected  dataset.  After  augmentation,  the  training  dataset  size  was 
increased from 11,788 to 40,000 images. After the data augmentation,

95.83%   
97.12%   

96.62%   
98.54%   

models as feature extractors and fine tuners along with a visual repre-
sentation of the time, measured in minutes, required by each model to 
complete its computational tasks. The experimental results demonstrate 
that fine-tuning pre-trained models leads to a significant improvement 
in performance. Table 12 depicts the performance of existing and pro-
posed  methods,  with  EfficientB0  achieving  higher  accuracy  than 
compared to other existing models. This can be attributed to the effec-
tive  selection  of  optimal  hyperparameters  during  transfer  learning, 
which significantly enhanced the performance. Additionally, fine-tuning 
the top layers of the convolutional base on our dataset further contrib-
uted to the overall success of our method.

provide more task specific features, benefited from fine-tuning as they 
were adjusted specifically for the dataset. Across all four classifications, 
the fine-tuning strategy consistently outperformed the feature extractor 
strategy  based  on  performance  metrics  such  as  accuracy,  precision, 
recall,  and  F1  score.  The  improvement  in  accuracy  was  attributed  to 
fine-tuning the top layers of the convolutional base using the dataset. 
EfficientB0  demonstrated  superior  performance  in  all  metrics.  It  ach-
ieved  the  highest  accuracy  in  three  partitions,  with  percentages  of 
96.56%,  97.89%,  and  99.12%,  respectively.  ResNet18  requires  less 
training time because it has a smaller network size and fewer parame-
ters.  However,  ResNet18  showed  slightly  lower  performance  in  three 
partitions  with  an  accuracy  of  89.33%,  93.21%,  and  93.24%,  respec-
tively, due to its limited capacity and potential for overfitting due to its

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention any regularization methods like dropout or L2 regularization used to prevent overfitting in the deep learning pipeline. However, it discusses another technique called data augmentation which is also commonly used to combat overfitting. Data augmentation involves expanding the dataset by applying various transformations to the existing data, such as flipping, translation, and shear. These transformations help maintain dataset balance, reduce overfitting, and improve the generalization of deep learning models. In this particular case, horizontal and vertical translations were performed within a range of 30 to 30 pixels, allowing random shifts. Shear distortion was applied within a range of 5 to 5, representing maximum distortion to the left and right. As a result of these augmentations, the training dataset size increased from 11,788 to 40,000 images.
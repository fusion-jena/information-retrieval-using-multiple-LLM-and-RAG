Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Grid view mode presents all five-second segments to the user as a grid
shown in Fig. 7. Samples are again ordered by the recommender. This is
an efficient method of quickly annotating recordings. The user can hover
over the spectrogram to play the sample and define on-click keys to
quickly apply labels.

By default, this method assigns samples to training or validation sets,
automatically maintaining an 80/20% training-validation split for each
class. Model prototypes are updated automatically and the user can
trigger model training and view training metrics through the training
tab. Training is mostly automated to make the methodology accessible
to all and does not require deep learning knowledge. We define seg-
mentation and classification endpoints so that users wanting to adapt
the pipeline can define their own segmentation and classification
models.

Feature extraction model selection is evaluated in Section 4.1.

We compare three classification models - ResNet-50 (He et al., 2016),
Audio Spectrogram Transformer (AST) (Gong et al., 2021) and HuBERT
(Hsu et al., 2021). The classification models compared in Section 4.1
were trained on a dataset containing 3000 5 s segments automatically
generated by the annotator. The dataset includes 1130 possum vocal-
isations and 1870 noise sources. The dataset has an 80/20% training-
validation split which is used to evaluate model performance during
training. Model hyperparameters are implemented based on the
recommendation outlined by the model authors. Each segment has been
manually reviewed by a human. 500 segments are separated into a test
set containing 187 possum vocalisations and 313 noise samples. The
distribution of target features to non-target features is representative of
the true distribution post-segmentation. Both validation and test dataset
performance are reported. Using the test dataset we evaluate the model

Fallgren, P., Malisz, Z., Edlund, J., 2019. How to Annotate 100 Hours in 45 Minutes,

pp. 341–345. https://doi.org/10.21437/Interspeech.2019-1648.

Fonseca, E., Ortego, D., McGuinness, K., O’Connor, N.E., Serra, X., 2020. Unsupervised

contrastive learning of sound event representations. arXiv:2011.07616.
Frasier, K.E., 2021. A machine learning pipeline for classification of cetacean

echolocation clicks in large underwater acoustic datasets. PLoS Comput. Biol. 17,
1–26. https://doi.org/10.1371/journal.pcbi.1009613.

Fukuzawa, Y., Webb, W.H., Pawley, M.D., Roper, M.M., Marsland, S., Brunton, D.H.,
Gilman, A., 2020. Koe: web-based software to classify acoustic units and analyse
sequence structure in animal vocalizations. Methods Ecol. Evol. 11 (3), 431–441.
https://doi.org/10.1111/2041-210X.13336.

Gharoun, H., Momenifar, F., Chen, F., Gandomi, A.H., 2023. Meta-learning approaches

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific postprocessing steps involved after the model training. However, it mentions some aspects related to evaluation and testing. After training the classification models with an 80/20% training-validation split, the performance is evaluated using both validation and test datasets. The test dataset consists of 500 segments, including 187 possum vocalizations and 313 noise samples, manually reviewed by humans. It also states that both validation and test dataset performance are reported, but it doesn't specify what kind of metrics are calculated or if any visualization techniques like saliency maps or confusion matrices are employed. Therefore, without additional context, it cannot be determined precisely what postprocessing steps are involved after the model training.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Fig. 7. Left movement.  

search  to  ascertain  the  configuration  that  minimizes  cross- 
validation error, thereby enhancing model precision.  

• The  selection  of  the  distance  metric,  typically  Euclidean,  was 
predicated on the data’s characteristics and the specific analytical 
prerequisites.  

was dictated by the explained variance ratio, to ensure the distilled 
dataset retained the majority of the original data’s variance.  
• The  SVM  model  was  thereafter  trained  on  this  dimensionally 
reduced  dataset,  with  hyperparameters  refined  as  delineated 
above. 

2.  For SVM  

• The  regularization  parameter  (‘C’)  and  the  kernel  type  (linear) 
were  calibrated  through  a  synergistic  application  of  grid  search 
and  cross-validation,  aiming  to  mediate  the  balance  between 
model complexity and its generalization prowess.

• In  instances  involving  non-linear  SVMs,  the  kernel  coefficient 
(‘gamma’) underwent optimization via grid search, ensuring the 
model’s attunement to the data distribution.  

3.  For PCA-SVM  

• Commencing  with  dimensionality  reduction  via  Principal 
Component Analysis (PCA), the number of components preserved 

5.3.2. Performance indicators

Corollary  4.2. Consider  the  decision-making  model  as  defined  in  eq. 
(4.4). Let the mappings V 1, V 2 : X→X  satisfy the condition (2) of Defi-
nition 4.1 with parameters ℓ1  and ℓ2, respectively, ensuring that ℓ1 ≤ ℓ2. 
Furthermore, assume that Ξ⋆
:= (3 + ℓ4)ℓ2 < 1, and there exists a point ξ ∈
2
[0, 1] for  which  V 1(ξ) = V 2(ξ).  Additionally,  let  there  be  a  non-empty 
subset ℰ of the space S := {P ∈ C |P(1) ≤ 1 }, which forms a Banach space 
with the structure (ℰ, ‖ ⋅ ‖) as defined in (4.3)). Under these conditions, a 
singular solution exists for the model (4.4). Moreover, the sequence {Pn}, 
starting from any P0  in ℰ, converges to a unique solution of the model (4.4). 
The iterative process for this convergence is given by: 

Pn(x) = f(x)Pn(cid:0) 1(V 1(x) ) + (1 (cid:0)

f(x) )Pn(cid:0) 1(V 2(x) ), ∀n ∈ ℕ.

4.4. Convergence analysis

4.3. Analytical solution 

In  this  subsection,  we  delve  into  the  analytical  resolution  of  the 
decision-making model as delineated in eq. (4.4). The ensuing theorems 
and  corollaries  are  instrumental  in  establishing  the  existence  and 
uniqueness  of  solutions  within  the  confines  of  our  model  framework. 
These results hinge on the intricate interplay between the mappings V 1 
and V 2, and the probabilistic function f(x), which collectively orches-
trate the decision-making process in our T-maze setup. 

Theorem 4.2. Consider the model (4.4). Let 

|V 1(μ) (cid:0) V 2(ν) | ≤ ℓ3|μ (cid:0) ν|,

(4.5)  

where ℓ3 ∈ [0, 1), ∀μ, ν ∈ X with μ ∕= ν, such that Ξ1 := [(1 + ℓ4)ℓ1 + ℓ2 
+ℓ3 ] < 1, and there exists a subset ℰ ∕= 0 of S := {P ∈ C |P(1) ≤ 1 } with 
the structure (ℰ, ‖ ⋅ ‖) of a Banach space (given in (4.3)). Then, a singular 
solution exists for the model (4.4). Additionally, the sequence {Pn} converges 
to a unique solution of (4.4), where P0 ∈ ℰ and

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: The hyperparameters of the Support Vector Machine (SVM) model are optimized using a combination of grid search and cross-validation techniques. Specifically, the regularization parameter 'C' and the kernel type (which is linear in this case) are calibrated through this approach. This method aims to strike a balance between model complexity and its ability to generalize well. Furthermore, when dealing with non-linear SVM models, the kernel coefficient 'gamma' also undergoes optimization via grid search to ensure the model fits the data distribution appropriately.
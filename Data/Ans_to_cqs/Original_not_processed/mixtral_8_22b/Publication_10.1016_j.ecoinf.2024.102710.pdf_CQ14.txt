Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The active learning experiments are carried out in the embedding
space of the selected transfer learning model (BirdNet-1, see Section
3.1). We explore a range of sampling strategies: uncertainty and di-
versity based, myopic (greedy) and adaptive (batch mode), and com-
binations thereof. Fig. 2 provides a schematic overview of the pure
families of sampling strategies: Random sampling selects samples arbi-
trarily, uncertainty sampling targets samples near the decision bound-
ary, and diversity sampling focuses on samples that span the entire data
space. In all cases, 5% of the samples are selected at random. Class labels
are available for all samples used in this study, and an active learning
scenario is emulated by hiding all labels from the classifier at first and
incrementally revealing the ones for each batch of samples queried by
the sampling methods. We use a batch size of 20 samples. The classifier
heads are identical to those from the transfer learning training process.

Combined sampling strategies address the limitations of pure
strategies. Uncertainty sampling selects samples close to the decision
boundaries, but may introduce redundancy. Diversity sampling covers
the entire input space, but may miss critical regions. We therefore

Fig. 2. Schematic representation of the random, uncertainty, and diversity
sampling strategies. Random sampling selects arbitrary samples. Uncertainty
sampling selects samples based on their proximity to the model's decision
boundary, calculated using the existing class 1 and class 2 labels. Diversity
sampling uses the internal data structure for sampling purposes, such as clus-
tering the data and selecting samples from each cluster.

Fig. 3. Uncertainty scores for binary inputs computed with the methods ‘ratio’
(ΦRC bi), ‘least confidence’ (ΦLC bi) and ‘entropy’ (ΦEN bi), given a model pre-
diction y.

EcologicalInformatics82(2024)1027104computed on the held-out evaluation sets described in section 2.1.

both the target modality and the domain, yields the best performance as
a feature embedding model. Using BirdNet-1 as the embedding model,
active learning sample selection strategies significantly reduce the
required for annotation to achieve model
number of
convergence.

of different active learning strategies. We investigate a range of sam-
pling strategies: uncertainty and diversity based methods, myopic
(greedy) and adaptive (batch mode) methods, and combinations thereof.
Finally, we evaluate the resulting learning curves through the lens of

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the provided context does not explicitly mention how the hyperparameters of the model are optimized, whether it be through grid search, random search, or another method. Therefore, I cannot provide a definitive answer to this specific query based solely on the information given.
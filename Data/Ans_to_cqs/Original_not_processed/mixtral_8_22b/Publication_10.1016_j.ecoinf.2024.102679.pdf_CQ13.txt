Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

with a 50% probability of occurrence. To avoid any risk of overfitting at 
each  training  stage,  we  selected  the  model  relative  to  the  epoch  that 
gave the best performance on the validation set. During inference, the 
patch  size  was  set  to  1024  Ã— 1,024  pixels  to  accelerate  the  process. 
Further information on the fine-tuning process is described in section 
2.5.2.

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Wildlife population estimation 
Aerial surveys 
Deep learning 
Biodiversity monitoring 
Conservation technology 
African savanna

surveys of PAs covering thousands of square kilometers and generating 
thousands of images can be delayed by several months using the OCC 
approach due to the slow but necessary manual processing of images. 
Recent  advances  in  machine  learning  have  propelled  the  perspec-
tives of remotely sensed imagery for wildlife conservation (Tuia et al., 
2022),  and  announced  good  prospects  for  the  automation  of  image 
processing from SRF-OCC surveys (Delplanque et al., 2023b; Eikelboom 
et al., 2019). Deep learning (DL) is a subgroup of machine learning and 
artificial intelligence (AI) where artificial neural networks are trained to 
achieve challenging tasks (e.g. detect animals in aerial imagery) through 
a complex multi-level representation of information learned from a large 
amounts of data (LeCun et al., 2015). In the last decade, DL has been 
widely  employed  to  (semi-)automate  the  detection  and  counting  of

The  pre-trained  model  underwent  inference  and  fine-tuning  for  4 
iterations using the entire SW stratum employing the SAL. This iterative 
process served to enhance the model's performance and gather samples 
pertaining to each key species present in the region. The training pro-
cedure for the two first fine-tuning iterations was the one proposed in 
the  original  paper  (Delplanque  et  al.,  2023a)  which  consisted  of  two 
main steps: 1) training the architecture using positive patches for 200 
epochs, and 2) collecting and including hard negative patches, which are 
patches  containing  false  positives,  to  further  train  the  model  for  50 
epochs in order to reduce the number of false positives. During the two 
last fine-tuning iterations, only the second step of the training procedure 
was used. Hard negative patches were created using false positives that 
emerged from the thumbnail classification (step 1 of the SAL). To avoid a

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not provide specific details about the hyperparameters used in the deep learning model, such as the learning rate or the type of optimizer. However, it does mention that the model was trained for 200 epochs during the initial phase and then for another 50 epochs with hard negative patches to improve its performance. It also states that the patch size was set to 1024 x 1024 pixels during inference to speed up the process.
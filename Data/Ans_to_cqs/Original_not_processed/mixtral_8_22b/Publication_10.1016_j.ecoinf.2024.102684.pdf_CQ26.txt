Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

of the models. For instance, Mil`a et al. (2022) used the Nearest Neighbor 
Distance Matching (NNDM) LOO CV method to delineate a geographic 
space  in  which  predictions  can  be  made.  Many  of  these  approaches 
accurately  demonstrate  the  model's  transfer  learning  capabilities,  yet 
they  impose  limitations  on  the  training  domain,  leading  to  reduced 
training  heterogeneity  (Meyer  and  Pebesma,  2021).  However,  con-
ducting large CV experiments with deep learning-based models is often 
infeasible due to  their high computational costs. Deep  learning-based 
models  typically  employ  simple  data  partitioning  strategies,  such  as 
training, validation, and testing (Ocer et al., 2020). This simple scheme 
can help to add heterogeneous information to the training procedure, 
but may also overlook the diversity of heterogeneous spatial information 
in remotely sensed data that is aligned with different learning domains

The dataset was divided into training (60%), validation (20%), and 
testing (20%) subsets for model evaluation and hyperparameter search 
(Fig. 3 (b)). This approach was chosen to obtain an unbiased estimation 
of  error.  Data  augmentation  was  only  applied  to  the  training  data  to 
avoid overlapping the same data in different partitions. To evaluate the 
models we used the coefficients of determination (R2  - calculated as the 
squared  Pearson's  correlation  coefficient),  which  measures  the  per-
centage of variation in the response variable explained by the model. 
Additionally,  we  used  the  root  mean  squared  error  (RMSE)  metric, 
where  RMSE =
,  with  Yi = the  actual  values  pine 
̂
Y i = the predicted pine cove values. In our work, RMSE 
cover values and 
values ranged from 0.0 to 1.0, with values closer to 0 indicating better 
model  predictions.  The  overfitting  was  assessed  by  comparing  model

Szegedy, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, Rabinovich, 2015. Going deeper 

Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, 
pp. 659–666. 

with convolutions. In: IEEE Conference on Computer Vision and Pattern 
Recognition, pp. 1–9. 

Karystinos, G.N., Pados, D.A., 2000. On overfitting, generalization, and randomly 

Telford, R.J., Birks, H.J.B., 2009. Evaluation of transfer functions in spatially structured 

expanded training sets. IEEE Trans. Neural Netw. 11, 1050–1057. 

environments. Quat. Sci. 28, 1309–1316. 

Kattenborn, T., Eichel, J., Fassnacht, F., 2019a. Convolutional neural networks enable 
efficient, accurate and fine-grained segmentation of plant species and communities 
from high-resolution UAV imagery. Sci. Rep. 9, 17656.

dation,  requiring  the  development  of  more  robust  and  adaptive 
validation criteria and methods that can keep pace with the rapid ad-
vances in deep learning methods. For example, the spatial heterogeneity 
in  the  foothills  is  associated  with  variability  in  bare  soil  cover,  tree 
height  and  structural composition,  and  large  gradients  of  macro- and 
micro-topography. These topographic landscape features can drastically 
alter species composition and richness in the area (Lopatin et al., 2015, 
2016). All these spectral, structural, and topographic variabilities can 
limit the applicability of the models to new values outside the training 
samples. Thus, the ability of predictive models to generalize and accu-
rately  apply  insights  to  new  locations  beyond  the  original  dataset 
(training, validation, and test data) is crucial for the operational con-
sistency (Chadwick et al., 2024). This requirement highlights the critical

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To address data bias during preprocessing of the deep learning pipeline, several techniques can be employed. One common technique is stratified splitting, where the dataset is split into training, validation, and testing subsets while maintaining the class proportions in each subset. This ensures that the model learns from a representative sample of the entire dataset, reducing potential biases. Another technique is oversampling, which involves duplicating examples from the underrepresented classes to balance the dataset. Undersampling, conversely, removes examples from the majority class to achieve a balanced distribution. Both oversampling and undersampling aim to mitigate imbalanced datasets, but they should be used carefully to avoid introducing other types of bias or losing valuable information. Additionally, diverse data collection is essential to ensure that the dataset represents various scenarios and conditions relevant to the problem being addressed. By incorporating diverse data sources, the model can capture a broader range of patterns and improve its overall performance and generalizability.
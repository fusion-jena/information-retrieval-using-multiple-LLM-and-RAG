Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.3.2.3. ANN: artificial neural networks classification. In classifying with 
the MLP ANN model, we used the package “nnet”, which provides pos-
sibilities for adjusting weighted decay and size, thereby countering the 
effects of model overfitting. We used an MLP ANN architecture with 1 
hidden layer established as a default setting within the “nnet” package, 
and with 6 neurons defined for our model inputs. The number of neurons 
in the input layer was equal to the number of used bands (6), and the 
output  layer  had  8  neurons  (representing  8  LULC  classes).  A  back 
propagation learning algorithm was used during the training phase of 
the model. Size and decay were used to define the primary model tuning 
parameters, and the control () function was used to control for model 
runs. As with the kNN approach, we defined the LULC classes of the test 
datasets  as  target  variables  and  the  band  reflectance  values  as  pre-

They employ supervised classification systems using training datasets to 
minimize  classification  errors  that  could  otherwise  be  caused  by  the 
internal structure of the algorithms (Bousquet et al., 2004; Hastie et al., 
2009). As a result, ML algorithms can be used to improve classification 
performance without needing to articulate the underlying mechanisms 
and assumptions of traditional statistical models (Clarke, 2013; Hastie 
et al., 2009). They can therefore, be trained using both balanced datasets 
(with the same amount or number of pixels sampled for each LULC) and 
imbalanced datasets (with different amount or number of pixels sampled 
for each LULC class) without major classification uncertainties. Here, we 
focus on four ML algorithms, kNN, SVM, ANN, and RF, which have been 
shown to be well suited to LULC classification and to outperform other 
algorithms such as MLC and DT (Khatami and Mountrakis, 2016; Noi

solving  (Kubat,  1999;  Yang,  2009).  In  LULC  classification,  the  Multi- 
Layer Perceptron (MLP) type of ANN has been used most often (Silva 
et al., 2020). MLP carries out backpropagation of training samples to 
accurately classify LULC. The RF was developed as an ensemble of ML 
models that use bootstrap techniques to build many single decision tree 
models (Breiman, 2001; Mellor et al., 2013; Rodriguez-Galiano et al., 
2012). The RF model uses subsets of predictor variables (e.g. Landsat 
bands) to split observation datasets into subsets of homogenous samples 
to build each decision tree (Mellor et al., 2013).

Machine learning (ML) models are a leading analytical technique used to monitor, map and quantify land use and 
land cover (LULC) and its change over time. Models such as k-nearest neighbour (kNN), support vector machines 
(SVM), artificial neural networks (ANN), and random forests (RF) have been used effectively to classify LULC 
types at a range of geographical scales. However, ML models have not been widely applied in African tropical 
regions  due  to  methodological  challenges  that  arise  from  relying  on  the  coarse-resolution  satellite  images 
available for these areas. In this study, we compared the performance of four ML algorithms (kNN, SVM, ANN 
and  RF)  applied  to  LULC  monitoring  within  the  Mayo  Rey  department,  North  Province,  Cameroon.  We  used 
satellite data from the Landsat 7 Enhanced Thematic Mapper Plus (ETM+) combined with 8 Operational Land

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions that the authors used the package "nnet" in R, which allows for adjusting weighted decay and size to counteract the effects of model overfitting. Weighted decay is a form of L2 regularization, also known as Ridge Regression, where a penalty term is added to the loss function to shrink the weights towards zero. This helps to reduce overfitting by preventing the model from becoming too complex. Additionally, the authors mention using a backpropagation learning algorithm during the training phase of the model, but they do not specify whether any other regularization techniques, such as dropout, were employed.
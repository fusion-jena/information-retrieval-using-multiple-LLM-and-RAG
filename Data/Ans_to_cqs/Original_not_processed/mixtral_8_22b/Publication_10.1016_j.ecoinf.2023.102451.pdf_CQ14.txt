Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.2.1. Self-attention 

The  self-attention  layer  is  composed  of  three  main  components 
query, key and value matrix. The matrices of query, key and value are 
computed  by  multiplying  the  patch  embedding  matrix  with  weight 
matrices. 

Attention(Q, K, V) = Softmax

(

)

.V

QKT
√
̅̅̅̅̅
dk

(1) 

√

̅̅̅̅̅
dk

Where the 

represents the key vector of k′s dimension and makes 
the gradient function more suitable by performing normalization. The 
query, key and value matrix are represented as Q, K and V respectively. 

2.2.2. Multi-head self-attention 

Multi-head attention is the core component of the transformer model 
which is used to calculate the scaled dot product of the input vectors of 
embedded  patches  by  multiplying  with  three  weight  matrices  WQ, 
WKand WV. 

(cid:0)
headi = Attention

QW Q

i , KW K

i , VW V

i

)

MHA(Q, K, V) = concat(head1, head2, …headi)W o

(2)  

(3)

configuration of optimizer and learning rate which has been considered 
for the training of the framework. The EMViT-Net framework achieves 
prominent results as compared to other deep learning models and per-
forms better with an augmented set of EMDS-6 datasets.

capable of learning weights for a generalized trained model on the un-
seen test set of EMDS-6. In model optimization learning rate was set to 
0.002 and the batch size was set to 32. The proposed model is trained for 
100 epochs with optimizer AdamW and the activation function GELU 
which  are  chosen  very  carefully  due  to  their  effectiveness  with  the 
proposed  model  for  the  considered  dataset.  Finally,  the  default  loss 
function  cross-entropy  is  used  to  calculate  the  loss  of  the  proposed 
network. By considering all the results of this section taken into account 
it was found that the proposed model EMViT-Net may be considered as 
fast and more reliable due to its low computational cost and efficiency. 
The loss and accuracy curves of the reported network are given in 
Fig.  5.  which  represents  training  and  validation  accuracy  with  their 
corresponding loss for the proposed model EMViT-Net with a specific

Table 6 
Performance comparison of EMViT Net with different deep learning models on EMDS-6 test set.  

Model 

Original dataset 

After Augmentation 

Avg. Precision 

Avg. Recall 

Avg. F1-score 

Accuracy 

Avg. Precision 

Avg. Recall 

Avg. F1-score 

Accuracy 

DenseNet121 
Alex Net 
ViT 
ResNet50 
Xception Net 
VGG19 
Inception V3 
VGG16 
EMViT-Net 

39.20% 
32.53% 
34.92% 
41.96% 
44.25% 
41.20% 
50.78% 
38.21% 
57.31% 

33.01% 
31.90% 
33.24% 
37.14% 
39.37% 
31.43% 
43.97% 
37.47% 
53.71% 

33.79% 
29.32%` 
32.63% 
36.93% 
39.07% 
29.97% 
43.41% 
36.80% 
53.45% 

33.02% 
31.91% 
33.23% 
37.14% 
39.37% 
31.43% 
43.97% 
37.46% 
53.71% 

34.92% 
38.19% 
43.55% 
46.57% 
50.87% 
52.40% 
51.91% 
50.86% 
69.53% 

33.77% 
35.87% 
37.65% 
41.58% 
44.76% 
48.57% 
48.88% 
50.15% 
67.62% 

32.45% 
35.35% 
37.92% 
42.06% 
43.03% 
47.67% 
48.10% 
50.64% 
66.89% 

33.77% 
35.87% 
37.65% 
41.58% 
44.76% 
48.57% 
48.88% 
50.15% 
67.62%

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The text does not provide explicit information about how the hyperparameters were optimized, such as using grid search or random search. However, it mentions that the learning rate was set to 0.002 and the batch size was set to 32. This suggests that these values were determined through some form of hyperparameter tuning process, but the exact methodology is not specified.
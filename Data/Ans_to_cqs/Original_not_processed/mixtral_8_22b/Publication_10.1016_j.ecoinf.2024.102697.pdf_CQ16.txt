Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.7. Machine learning training methodology 

To ensure accuracy and reliability of results, a repeated nested k-fold 
cross-validation approach was employed for all ML models developed 
here. An outer 10 k-fold cross-validation provided an initial division of 
the data into holdout data and data for model development (Molinaro 
et al., 2005). An inner 5 k-fold cross-validation was then used to mini-
mize the bias caused by tuning of hyperparameters on training results, as 
demonstrated in previous studies (Cawley and Talbot, 2010; Yates et al., 
2022).  The  training  performance  was  calculated  based  on  the  inner 
cross-validated  ensemble,  while  the  testing  performance  was  deter-
mined by selecting the optimal set of hyperparameters from the entire 
training set to predict on the holdout data. This process was repeated 15 
times for each unique model to reduce the potential bias and variance 
related to random sampling (Kim, 2009).

the  primary  hyperparameters  required  for  application  of  the  random 
forest method. Here, hyperparameter optimization was performed using 
Bayesian  Optimization  to  minimize  a  5  k-fold  cross-validation  loss 
during model development (Section 2.7).

In  the  subsequent  sections,  we  assess  model  performance  in  the 
context of model complexity and explainability, crop specific training 
against  generalized  training,  and  contrast  remote  sensing  and  meteo-
rological measurements to provide predictive performance for several 
potential use cases. 

3.1. Model complexity 

The management of model complexity is an important consideration 
in  data-driven  model  development (Jolliffe,  1986; Wang  et  al.,  2017; 
Yates  et  al.,  2021).  In  this  study  we  define  model  complexity  as  the 
number of variables included in the predictor set. Proper selection of 
variables, i.e. feature selection, can minimize training costs, measure-
ment  requirements,  and  decrease  the  likelihood  of  overfitting  (Banks 
and Fienberg, 2003; Cawley and Talbot, 2010).

conditions.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: Based on the provided context, it appears that the criteria used to determine when training is complete involves a repeated nested k-fold cross-validation approach. Specifically, an outer 10 k-fold cross-validation is used to initially divide the data into holdout data and data for model development. Then, an inner 5 k-fold cross-validation is applied to minimize bias caused by tuning of hyperparameters on training results. The training performance is calculated based on the inner cross-validated ensemble, while the testing performance is determined by selecting the optimal set of hyperparameters from the entire training set to predict on the holdout data. This process is repeated 15 times for each unique model to reduce potential bias and variance related to random sampling. Additionally, Bayesian Optimization is used to minimize a 5 k-fold cross-validation loss during model development. However, the exact criteria for determining when training is complete, such as reaching a certain threshold or plateau in validation loss, is not explicitly stated in the provided context.
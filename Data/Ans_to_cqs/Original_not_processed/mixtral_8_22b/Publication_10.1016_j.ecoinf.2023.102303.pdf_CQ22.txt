Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Labeling Time 
Training Time 
Turnaround 

Time 
Accuracy 

Very Short 
1.0 h 

4 h 

89.4% 

Short 
6.9 h 

22 h 

94.0% 

Very Long 
~3.5 h 

94 h 

98%  

4. Discussions 

The  development  of  deep  learning  methodologies  continues  to 
advance at an astonishing rate and be applied to various applications 
ranging from biomedical (Azghadi et al., 2020), hydrological processes 
in river channels (Talukdar et al., 2023) and agricultural (Olsen et al., 
2019) systems, to marine (Laradji et al., 2021; Saleh et al., 2022b), and 
environmental (Jahanbakht et al., 2022a) sciences. The application of 
deep learning technologies has been also used in profiling the ecosystem 
services of estuarine habitats by community members (Yee et al., 2023). 
In this paper, we extend the application of deep learning methodologies 
to advance state-of-the-art underwater fish video processing techniques 
applied to turbid waters.

Table 2 
Comparing both our semi-supervised contrastive learning DNN and the weakly- 
supervised XGBoost ensemble of DNNs with recent publications in the literature.  

Metric 

Semi- 
supervised 
Contrastive 
Learning 

Weakly- 
supervised 
XGBoost 
Ensemble 

(Sun 
et al., 
2022) 

(Yu 
et al., 
2023) 

(Soom 
et al., 
2022) 

Customized 

✓ 

✓ 

⨯ 

⨯ 

✓ 

DNN 
Accuracy 
Precision 
F1-score 
AUC-ROC 

89.4% 
99.8% 
0.883 
0.917 

94.0% 
99.5% 
0.936 
N/A 

N/A 
51% 
0.817 
N/A 

N/A 
90.2% 
0.859 
N/A 

91.6% 
88.1% 
0.919 
N/A  

Table 3 
Performance  comparison  between  a  fully-supervised  DNN  model,  which  is 
trained  on  a  clear-water  image  dataset  (Deep  and  Dash,  2019)  and  our  two 
proposed XGBoost ensemble and contrastive learning DNNs, which are trained 
on our FishInTurbidWater dataset.  

Metric 

Semi-supervised 
CNT Learning 
(turbid water) 

Weakly-supervised 
XGBoost Ensemble 
(turbid water) 

Typical Supervised 
Deep Learning 
(clear-water)

truly present. TNR (i.e., specificity and selectivity) is the portion of the 
no-fish frames that have been correctly classified. Conversely, FNR or 
miss rate and FPR or fall-out are respectively calculating the portion of 
fish  and  no-fish  frames  that  were  incorrectly  classified.  Sensitivity, 
specificity, miss rate, and fall-out are the most used metrics to measure 
the true and false classification probabilities. 

We also show that our semi-weakly-supervised model can be devel-
oped roughly 23.5 times faster than a fully-supervised DNN, at the cost 
of  nearly  a  9%  drop  in  its  accuracy.  This  tradeoff  is  4.3  times  faster 
development for a 4% accuracy degradation. The methods presented in 
this  paper,  therefore,  can  assist  marine  scientists  and  environmental 
managers in fast and improved fish detection and monitoring in turbid 
water conditions.

2.3.1. EfficientNet 

The first model selected is EfficientNet-B7, which is a Convolutional 
Neural Network (CNN) with efficient scaling factors (Tan and Le, 2019). 
To elaborate, all CNNs are a cascade of convolutional layers that can 
scale  the  input  image’s  resolution  (width  and  height)  and  channel 
(number  of  color  channels).  CNN  architectures  can  also  scale  by  the 
number of consecutive convolutional layers (depth).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The performance of the deep learning models in this study is evaluated using several metrics. These include Accuracy, Precision, F1-score, and AUC-ROC. Accuracy measures the proportion of correct predictions out of the total number of instances. Precision, also known as Positive Predictive Value, is the ratio of correctly predicted positive observations to the total predicted positives. The F1 score is the harmonic mean of precision and recall, where recall is the ratio of correctly predicted positive observations to the actual positives. Lastly, AUC-ROC stands for Area Under the Receiver Operating Characteristic Curve, which plots True Positive Rate against False Positive Rate. It provides an aggregate measure of performance across all possible classification thresholds. Additionally, sensitivity, specificity, miss rate, and fall-out are mentioned as metrics to measure true and false classification probabilities. However, their definitions are not explicitly stated in the given context.
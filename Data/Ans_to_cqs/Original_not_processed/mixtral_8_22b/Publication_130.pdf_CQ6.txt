Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

be frozen in order to mitigate overfitting, this is especially important 

is  oversampling,  which  consists  of  creating  copies  of  the  training 

when the training datasets are small. However, as freezing the layers 

data from the less sampled classes.

prevent them from update their weights during the training process 

We applied limited oversampling to our training dataset only. For 

(and therefore could prevent the model from learning key features 

nine sociable weavers and six great tits for which we did not have 

for performing the classification task) and considering the size of our 

1,000 images, we first selected 100 images for the validation dataset 

training datasets, we decided to train the models without freezing 

and then duplicated (through oversampling) the remaining pictures 

any  of  the  layers  of  the  network.  The  fully  connected  part  of  the 

until 900 images were available for the training dataset (Buda, Maki,

now feasible to continuously collect training pictures and routinely 

lenges presented here, among others.

re-train a CNN using updated training data.

Having large datasets will allow optimizing performance of CNNs 

The arrival of new individuals to the study population is another 

as  well  as  identifying  the  relative  performance  of  alternative  solu-

challenge  that  needs  to  be  carefully  addressed.  If  these  new  birds 

tions. Other network architectures (e.g. ResNet; He, Zhang, Ren, & 

are  marked  with  a  PIT-tag,  the  CNN  could  be  updated  similarly  to 

Sun,  2016)  and  different  hyper-parameters  settings  (e.g.  learning 

the  problem  of  changes  in  appearance  discussed  above.  However, 

rate)  than  the  ones  used  here  can  yield  different,  and  potentially 

in many cases new individuals will not be marked. Such a problem 

improved,  results.  Other  deep  learning  methods  approaches  could

learning methods such as CNN.

hold would result in discarding 77% of the images of birds present in 

The  most  powerful  aspect  of  CNNs  is  that  they  can  provide  a 

the training dataset.

4 |  D I S C U S S I O N

generalized identification solution. However, the capacity for a CNN 

to work effectively across contexts will be affected by variation in 

the recording conditions, for example due to light intensity, shadow 

or characteristics inherent to the recording quality. One solution to 

this is to ensure that the training dataset contains sufficient variation 

Deep  learning  has  the  potential  to  revolutionize  the  way  in  which 

to capture the broad range of contexts that the CNN is required for. 

researchers identify individuals. Here we propose a practical way of 

Photographing the animals across different times of the day and in 

collecting large labelled datasets, which is currently the main bottle-

a deep learning model with an imbalanced training dataset (i.e. when 

35 sociable weavers at the RFIDs antennas. Of these, 30 individuals 

the different classes, here the individuals, have different number of 

with more than 350 pictures were used to train the classifier. In the 

training pictures) can result in the over-generalization for the classes 

great tit population, 77 birds were photographed, of which 10 had 

in majority due to its increased prior probability. For instance, a naïve 

more than 350 pictures. These 10 individuals were used to train a 

classifier  for  a  binary  classification  task  for  a  dataset  in  which  the 

CNN  for  each  of  the  species.  The  remaining  five  sociable  weavers 

ratio  of  the  minority  class  to  the  majority  class  is  1:100  will  have 

and 67 great tits (with <350 pictures) were used to address the issue 
of working in open areas where new individuals can constantly be

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it appears that one of the preprocessing steps involved before training a deep learning model is oversampling. This technique involves creating copies of the training data from the less sampled classes, particularly when dealing with small training datasets. By doing so, it helps to mitigate overfitting and prevents the model from updating its weights during the training process. Additionally, selecting appropriate images for the validation dataset and ensuring that the training dataset contains sufficient variation to capture the broad range of contexts that the model is required for are also crucial preprocessing steps.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

deeper, its optimization capability can be further improved. How- 
ever, deep CNN networks require very large amounts of training 
data. Table 6 shows examples of existing well-known datasets and 
their size as quantity of images. The biggest plant database that 
we  have  found  is  the  PlantClef2015  dataset  [6]  which  has  only 
around 113,205 number of images. This is still far from matching 
the scale and variety of existing general major datasets for images 
[51,52,55] , videos [53] or languages [54] . In addition, we can see 
that the PlantClef2015 dataset [6] has one of the largest number 
of object categories but the least number of images. For example, 
compared to the ILSVRC 2010 dataset [55] , it has less than 10% of 
their total images but the same number of categories. Hence, to

We  train  our  model  using  Caffe  [35]  framework.  For  the  pa- 
rameter  setting  in  training,  we  employ  step  learning  policy.  The 
learning rate was initially set to 10 −3  for all layers to accept the 
newly deﬁned last fully connected layer set to 10 −2 . It is higher 
than other layers due to the weights being trained starting from 
random. The learning rate was then decreased by a factor of 10 ev- 
ery 20K iteration and was stopped after 100K iterations. The units 
of the third fully connected layer (fc8) were changed according to 
the number of classes of training data. We set the batch size to 50 
and momentum to 0.9. We applied L 
2 weight decay with penalty 
multiplier set to 5 × 10 −4 and dropout ratio set to 0.5, respectively. 

4.2. Deconvolutional network

Feature 

Classiﬁer 

From Deep CNN (D1) 
From Deep CNN (D1) 
From Deep CNN (D2) 
From Deep CNN (D2) 
LeafSnap [7] 
LeafSnap [7] 
HCF [8] 
HCF-ScaleRobust [8] 
Combine [8] 
SIFT [37] 

MLP 
SVM (linear) 
MLP 
SVM (linear) 
SVM (RBF) 
NN 
SVM (RBF) 
SVM (RBF) 
Sum rule (SVM (linear)) 
SVM (linear) 

Acc 

0.977 
0.981 
0.995 
0.993 
0.420 
0.589 
0.716 
0.665 
0.951 
0.588

Based on all the facts that support the eﬃciency of leaf fea- 
tures learned using CNN for species identiﬁcation, it now appears 
undeniable  that  CNN  is  a  key  tool  to  assist  researchers  to  dis- 
cover which the leaf features are most effective for plant species 
identiﬁcation. Nevertheless, we come up against a common ques- 
tion that is often arises in the ﬁeld of deep learning: how many 
convolutional layers are required in CNN to achieve the best opti- 
mization ability in modeling plant data? Is using only the AlexNet 
model suﬃcient? Based on numerous publications on object clas- 
siﬁcation  benchmarks,  we  observe  a  dramatic  increase  in  depth 
for CNN in achieving the state-of-the-art result. For example: from 
5 convolutional layers in AlexNet [32] to 16 in VGGNet [48] , 21 
in GoogleNet [49] , and then to 164 in ResNet [50] . This conveys 
the important message that when the network goes deeper and

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given text does not provide information about the specific hardware resources used for training the deep learning model. However, it mentions that the Caffe framework was used for training the model. Caffe is known to support both GPU and CPU computations, so it can be inferred that either GPUs or CPUs could have been used for training the model. Without explicit mention of the hardware resources used, it cannot be definitively stated whether GPUs, TPUs, or any other specific hardware resource was utilized for training the deep learning model.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

with a 50% probability of occurrence. To avoid any risk of overfitting at 
each  training  stage,  we  selected  the  model  relative  to  the  epoch  that 
gave the best performance on the validation set. During inference, the 
patch  size  was  set  to  1024  Ã— 1,024  pixels  to  accelerate  the  process. 
Further information on the fine-tuning process is described in section 
2.5.2.

The  pre-trained  model  underwent  inference  and  fine-tuning  for  4 
iterations using the entire SW stratum employing the SAL. This iterative 
process served to enhance the model's performance and gather samples 
pertaining to each key species present in the region. The training pro-
cedure for the two first fine-tuning iterations was the one proposed in 
the  original  paper  (Delplanque  et  al.,  2023a)  which  consisted  of  two 
main steps: 1) training the architecture using positive patches for 200 
epochs, and 2) collecting and including hard negative patches, which are 
patches  containing  false  positives,  to  further  train  the  model  for  50 
epochs in order to reduce the number of false positives. During the two 
last fine-tuning iterations, only the second step of the training procedure 
was used. Hard negative patches were created using false positives that 
emerged from the thumbnail classification (step 1 of the SAL). To avoid a

aperture set to f/5.0, the auto-ISO was preferred, with a minimum value 
of  500,  and  minimum  shutter  speed  was  set  to  1/2000  s.  In  total,

volves fine-tuning the model to suit the targeted PA. As some researchers 
shared their model in recent years (Delplanque et al., 2022, Delplanque 
et al., 2023a; Eikelboom et al., 2019), we propose a simple yet effective 
method that could be applied across various cases.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given context, there isn't explicit information about how the hyperparameters of the model were optimized, such as through methods like grid search or random search. However, it does mention that the model was trained for a specific number of epochs (200 and 50) during different stages of the fine-tuning process. Additionally, it mentions that the patch size was set to 1024 x 1024 pixels during inference to accelerate the process. These details suggest that certain parameters were manually chosen rather than being determined by an optimization method. Nonetheless, without more specific information regarding hyperparameter tuning, I cannot provide a definitive answer.
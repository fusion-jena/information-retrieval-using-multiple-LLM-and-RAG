Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Lee, S., Agrawal, A., Balaprakash, P., Choudhary, A., Liao, W., 2018b. Communication- 
efficient parallelization strategy for deep convolutional neural network training. In: 
Proceedings of MLHPC 2018 : Machine Learning in HPC Environments. 

Lin, T.-Y., et al., May 2014. Microsoft COCO: common objects in context. In: 13th 

European Conference in Computer Vision (ECCV), pp. 740–755 [Online]. Available: 
http://arxiv.org/abs/1405.0312. 

Liu, W., et al., 2016. SSD: single shot MultiBox detector. Europ. Conf. Comp. Vision 1, 

852–869. https://doi.org/10.1007/978-3-319-46448-0. 

Dong, X., Yan, S., Duan, C., Aug. 2022. A lightweight vehicles detection network model 

Liu, J., Zhang, L., Li, Y., Liu, H., 2023a. Deep residual convolutional neural network 

based on YOLOv5. Eng. Appl. Artif. Intell. 113 https://doi.org/10.1016/j. 
engappai.2022.104914.

tionally, our technique is compared with state-of-the-art deep learning models, and from the baseline model we 
recorded  a  17.65%  increase  in  FPS,  28.55%  model  parameters  reduction,  and  50.92%  in  FLOPs  reduction. 
Furthermore, our model has a faster model loading time, which is critical for deployment in remote areas. This 
enables real-time species recognition on basic hardware, aiding conservation efforts through rapid analysis. The 
model advances deep learning in ecology by balancing efficiency with performance.

Parameter amount = k2.Cin.Cout

(14)  

(15) 

Understanding  the  computational  cost  and  parameters  of  a  single 
layer allows for precise estimation and control of the total requirements 
of our model. Computational complexity is quantified in FLOPs, calcu-
lated considering the convolutional kernel size (k2), the number of input 
Cin  and output Cout  channels, and the feature map’s height (hout) and 
width  (wout).  FPS  measure  the  model’s  speed,  encompassing  pre-
processing, inference, and non-maximum suppression speeds, providing 
an overall assessment of the model’s efficiency in real-time applications. 

4.4. Results

4. Performance evaluation 

4.1. Experimental settings 

For the experiments, we utilized a Windows 11 Pro (64-bit)-based 
computational system with an Intel Core i7-8565U CPU @ 1.80GHz x 16 
GB  memory,  which  was  remotely  connected  to  Ubuntu  18.04.6  LTS 
(GNU/Linux  4.15.0–169-generic  x86  64),  NVIDIA-SMI  470.103.01, 
GeForce  GTX 1080 Ti,  using CUDA 10.2 for  GPU parallelization. The 
computer vision library used was Pytorch 1.9.0, where we utilized the 
pre-trained weights file of YOLOv5s (Jocher et al., 2020). This high-end 
equipment was chosen primarily to speed up the training and validation 

EcologicalInformatics80(2024)10254111S.R. Bakana et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors utilized a Windows 11 Pro (64-bit)-based computational system with an Intel Core i7-8565U CPU @ 1.80GHz x 16 GB memory. They were also remotely connected to Ubuntu 18.04.6 LTS (GNU/Linux 4.15.0-169-generic x86 64), NVIDIA-SMI 470.103.01, GeForce GTX 1080 Ti, using CUDA 10.2 for GPU parallelization. Therefore, it can be concluded that they used a combination of CPUs and GPUs for training their deep learning model.
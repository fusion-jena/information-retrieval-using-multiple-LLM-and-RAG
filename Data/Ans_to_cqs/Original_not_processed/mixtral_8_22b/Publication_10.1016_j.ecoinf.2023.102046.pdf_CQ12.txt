Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

ResNet-50 is a convolutional neural network model that uses skip 
connections,  which  made  it  possible  to  get  good  performance  with 
deeper models than was previously possible (He et al., 2015). Training 
deep neural networks can result in exploding gradients, and skip con-
nections  were  introduced  to  ameliorate  that  problem.  ResNet-50  was 
chosen  for  comparison  as  it  is  a  proven  CV  classification  model.  It  is 
commonly used and provides a well-performing baseline. 

Inception-V3  is  a  convolutional  neural  network  used  for  image 
classification, that has an auxiliary classifier that acts as a regularizer 
(Szegedy et al., 2015b). The Inception-V3 architecture is built on pre-
vious Inception models, with the aim of making the V3 computationally

Inception Architecture for Computer Vision [arXiv:1512.00567 [cs]]. https://doi. 
org/10.48550/arXiv.1512.00567. 

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., 
Polosukhin, I., 2017. Attention is all you need. Adv. Neural Inf. Proces. Syst. 30. 
Wang, C.-H., Walther, B.D., Gillanders, B.M., 2019. Introduction to the 6th international 

otolith symposium. Mar. Freshw. Res. 70 (12), i–iii. 

Xu, Z., Zhu, L., Yang, Y., 2016. Few-Shot Object Recognition from Machine-Labeled Web 
Images [arXiv: 1612.06152]. arXiv:1612.06152 [cs]. Retrieved January 6, 2022, 
from. http://arxiv.org/abs/1612.06152. 

EcologicalInformatics76(2023)10204614

The Inception-V3 model accepts images resized to (299, 299) pixels 
but ViT and ResNet resize to (224, 224) pixels. Dataset imbalance was 
dealt with by including class weights in the cross-entropy loss function. 
For the training we used AdamW optimizer for Hugging Face models, 
which is the default optimizer in the transformers library by Hugging-
Face. For Inception-V3, we used Adam since that was the optimizer used 
in  previous  work  (and  AdamW  is  not  available  in  Keras).  We  used  a 
(cid:0) 4  for 
batch  size  of  16  examples  and  the  learning  rate  was  set  to  10
(cid:0) 5  for  Vanilla  Inception-V3  and  ViT.  For  Deep  Otolith 
ResNet  but  10
(cid:0) 4 as suggested by Politikos 
Inception we used a learning rate of 4 × 10
et  al.  (2021).  For  every  fine-tuning  experiment,  the  model  with  the 

smallest validation loss was used for evaluation on the test set. 

2.3. Performance

Recent progress in few-shot learning has been based on transformers. 
These  models were first  applied  successfully  in  natural language  pro-
cessing (NLP), and are used in popular language models such as Bidi-
rectional  Encoder  Representations  from  Transformers  (BERT,  Devlin 
et al., 2019). They have also been implemented in vision models, such as 
the Vision Transformer (ViT) (Dosovitskiy et al., 2023). The transformer 
architecture,  introduced  by  Vaswani  et  al.,  2017,  uses  an  attention 
mechanism  such  that  the  model  can  learn  what  parts  of  the  input  it 
should attend to. This allows it to outperform state-of-the-art models in 
NLP  tasks  without  using  recurrence  in  the  model  architecture.  Doso-
vitskiy et al., 2023 showed that convolutions are not necessary for state- 
of-the-art  performance  in  image  classification  tasks,  and  that  trans-
formers can perform well on these tasks when applied to image patches.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The pipeline utilizes three types of deep learning models: Convolutional Neural Networks (CNNs), specifically ResNet-50 and Inception-V3, and a Transformer model called Vision Transformer (ViT).

ResNet-50 is a variant of the Residual Network (ResNet) family of architectures, which introduces skip connections or shortcut paths to enable better gradient flow during backpropagation. This makes it easier to train deeper networks, improving their performance. ResNet-50 consists of 50 layers and is widely used for computer vision tasks like image recognition and object detection.

Inception-V3 is another CNN architecture designed for image analysis tasks. It incorporates an auxiliary classifier that serves as a regularizer, helping prevent overfitting. Inception-V3 builds upon earlier Inception models, focusing on computational efficiency while maintaining high accuracy.

Lastly, the pipeline employs a Transformer model known as Vision Transformer (ViT). Unlike traditional CNNs, ViT applies self-attention mechanisms directly to image patches, allowing the model to weigh different parts of the input differently. This approach enables ViT to achieve competitive results in various image classification tasks without relying on convolution operations.
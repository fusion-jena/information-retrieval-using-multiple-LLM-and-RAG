Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Vanhoucke V, Rabinovich A. Going deeper with convolutions. In: 2015
IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Boston: IEEE Conference; 2015. p. 1–9. doi:10.1109/CVPR.2015.7298594.
Ioffe S, Szegedy C. Batch normalization: Accelerating deep network
training by reducing internal covariate shift. CoRR. 2015. abs/1502.03167.
[Online]. Available http://arxiv.org/abs/1502.03167.

35.

37.

36. He K, Zhang X, Ren S, Sun J. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. CoRR. 2015.
abs/1502.01852. [Online]. Available http://arxiv.org/abs/1502.01852.
Jia Y, Shelhamer E, Donahue J, Karayev S, Long J, Girshick R,
Guadarrama S, Darrell T. Caffe: Convolutional architecture for fast feature
embedding. In: Proceedings of the 22Nd ACM International Conference
on Multimedia. New York: ACM; 2014. p. 675–8.
doi:10.1145/2647868.2654889.

38. Mata-Montero E, Carranza-Rojas J. Automated plant species

119M

170M

1072K

54M

1388K

71M

1000K

1M

the network. Therefore, a common practice is to initialize
the network by pre-training it on a big available dataset
and then fine-tune it on the scarcer domain-specific data.
Concretely, the methodology we used in our experiment
for transferring knowledge from dataset A to dataset B is
the following:

1. The network is first trained from scratch on dataset
A by using a multinomial logistic regression on top
of the SOFTMAX layer and the linear classification
layer.

2. The linear classification layer used for dataset A is
then replaced by a new one aimed at classifying the
classes in B. It is initialized with random weights.

3. The other layers are kept unchanged so as to

initialize the learning of dataset B with the weights
learned from A.

4. The network is trained on the images in B.

Experiments and results
All experiments measured the top-1 and top-5 accu-
racy of the trained deep learning model under different
circumstances, i.e., herbarium specimens classification
(“Herbarium specimen classification” section, Table 3),
transfer learning across herbarium data from differ-
ent regions (“Cross-Herbaria transfer learning” section,
Table 4), and transfer learning from herbarium data to
non-dried plant images (“Transfer learning from herbar-
ium to non-dried plant images” section, Table 5).

For each of these experiments, table columns are

defined as follows:

• I.H255.H255: The neural network was pre-trained
on the generalist dataset ImageNet to initialize the
weights, fine-tuned on the Herbarium255 training set
(70%), and tested on the Herbarium255 test set (30%).

• R.H1K.H1K: The neural network was initialized

Herbarium specimen classification
These experiments aim at assessing the feasibility of using
a deep learning system dedicated to herbarium specimen
identification at a realistic scale (255 species from Costa-
Rica in Herbarium255 and 1K species from France in
Herbarium1K). Herbarium255 was divided in 70% train-
ing data and the rest 30% as test data used for computing
the top-1 and top-5 classification accuracy. Herbarium1K
was divided in 80 and 20% respectively, to keep the pro-
portion of the data provided by the PC challenge. The
separation was done by species, and within each species,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about the methods used for collecting raw data in the deep learning pipeline. However, there are references to two datasets - ImageNet and Herbarium1K. This suggests that the authors might have used publicly available datasets or collected their own dataset for this study. Unfortunately, without more specific details, it cannot be determined whether they used surveys, sensors, or any other means to collect the data.
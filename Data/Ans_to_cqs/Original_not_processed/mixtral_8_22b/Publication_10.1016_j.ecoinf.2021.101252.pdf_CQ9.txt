Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Reichstein, M., Camps-Valls, G., Stevens, B., Jung, M., Denzler, J., Carvalhais, N., 
Prabhat, 2019. Deep learning and process understanding for data-driven earth 
system science. Nature 566, 195–204. https://doi.org/10.1038/s41586-019-0912-1. 

Reside, A.E., VanDerWal, J.J., Kutt, A.S., Perkins, G.C., 2010. Weather, not climate, 

defines distributions of vagile bird species. PLoS One 5, e13569. https://doi.org/ 
10.1371/journal.pone.0013569. 

Ryo, M., Aguilar-Trigueros, C.A., Pinek, L., Muller, L.A.H., Rillig, M.C., 2019. Basic 

principles of temporal dynamics. Trends Ecol. Evol. 34, 723–733. https://doi.org/ 
10.1016/j.tree.2019.03.007. 

Schmidhuber, J., 2015. Deep learning in neural networks: an overview. Neural Netw. 61, 

85–117. https://doi.org/10.1016/j.neunet.2014.09.003.

hyperparameters  (i.e.,  ‘AutoML’;  He  et  al.,  2021).  This  represents  an 
important  advantage  for  non-experts  in  deep  learning,  as  it  does  not 
require  the  manual  assembly  of  the  models  and  definition  of  their 
hyperparameters. The AutoML procedure starts by generating a set of 
candidate models with architectures and hyperparameters (e.g. number 
of layers; learning rate) selected at random from a prespecified range of 
values (see Fig. 2). Each candidate model is trained using a small subset 
of the data (data partition At; Fig. 2) during a small number of epochs. 
After  training,  the  performance  of  the  candidate  models  is  compared 
using a left-out validation data set (Av; Fig. 2). The selected candidate 
model (usually the best performing among candidates) is then trained on 
the full training data (Bt; Fig. 2). In this step it is required to identify an 
optimal number of training epochs, to avoid under- or overfitting of the

Contents lists available at ScienceDirect 

Ecological Informatics 

journal homepage: www.elsevier.com/locate/ecolinf 

Deep learning for supervised classification of temporal data in ecology 

C´esar Capinha a, *, Ana Ceia-Hasse b, Andrew M. Kramer c, Christiaan Meijer d 
a Centro de Estudos Geogr´aficos, Instituto de Geografia e Ordenamento do Territ´orio - IGOT, Universidade de Lisboa, Rua Branca Edm´ee Marques, 1600-276 Lisboa, 
Portugal 
b Global Health and Tropical Medicine, Institute of Hygiene and Tropical Medicine, NOVA University of Lisbon, Rua da Junqueira 100, 1349-008 Lisboa, Portugal 
c Department of Integrative Biology, University of South Florida, Tampa, Florida, USA 
d Netherlands eScience Center, Science Park 140, 1098 XG Amsterdam, the Netherlands   

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Deep learning 
Ecological prediction 
Scalability 
Sequential data 
Temporal ecology 
Time series

applied to image data. The challenges arising from computational de-
mand appear harder to solve. Here we showed that ‘typical’ classifica-
tion tasks can take several hours to run on a standard desktop computer. 
Additionally, the computational expensiveness of deep learning is ex-
pected  to  grow  in  the  future  (Thompson  et  al.,  2020).  To  face  this 
challenge, ecologists will likely have to move in the same direction as 
their  fellow  computer  scientists  and  embrace  faster  hardware  (e.g. 
GPUs,  ‘tensor  processing  units’  and  large-resourced  cloud  computing 
services)  and  scalable  model 
implementations  (e.g.  distributed 
computing).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The information regarding the specific data repository used for the deep learning pipeline mentioned in the given context is not explicitly stated. Therefore, without additional details, it is impossible to determine where the data repository of the deep learning pipeline is available.
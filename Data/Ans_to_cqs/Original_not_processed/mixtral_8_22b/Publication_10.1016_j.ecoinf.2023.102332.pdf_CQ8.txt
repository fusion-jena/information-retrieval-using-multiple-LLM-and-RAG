Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.1. Data collection 

Wikiloc  offers  over  14  million  hiking  trails  and  has  increasing 
popularity with the number of users increasing from 3.5 million in 2019 
to over 11.3 million in 2023 (https://www.wikiloc.com). The Wikiloc 
platform  allows  users  to  record  and  share  outdoor  trails,  providing 
associated textual descriptions, photographs, and their spatial location 
(latitude and longitude) on the trail. The type of outdoor activity is also 
recorded (hiking, cycling, or running), as well as the date the trail was 
made. Wikiloc has a broad user base and potential as a data source for 
diverse CES (Callau et al., 2019; Chai-allah et al., 2023; Norman and 
Pickering, 2019). While the trails and photographs from this platform 
have  been  used  in  CES  studies  (Callau  et  al.,  2019;  Chai-allah  et  al., 
2023;  Norman  and  Pickering,  2019),  the  textual  data  remained 
unexplored.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J., 2013. Distributed 

representations of words and phrases and their compositionality. ArXiv Preprint.  

Müllner, D., 2013. Fastcluster: fast hierarchical, agglomerative clustering routines for R 

and Python. J. Stat. Softw. 53, 1–18. 

Naili, M., Chaibi, A.H., Ben Ghezala, H.H., 2017. Comparative study of word embedding 
methods in topic segmentation. Proc. Comput. Sci. 112, 340–349. https://doi.org/ 
10.1016/j.procs.2017.08.009.

Word2vec  is  a  widely  known  word  embedding  technique  (e.g., 
Mikolov et al., 2013; Naili et al., 2017), and has been a successful tool for 
investigating CES from social media data (e.g., Gugulica and Burghardt, 
2023).  Word2vec  can  be  carried  out  in  two  ways:  by  creating  and 
training your own word2vec space based on two different architectures, 
namely, Skip-gram, which predicts the target word based on a nearby 
word, and Continuous Bag-of-Words (CBOW), which learns to predict a 
target word according to its context (Mikolov et al., 2013), or by using 
pre-trained  models (e.g.,  Baroni et al., 2014). Training a new  custom 
word2vec model requires a very large dataset, increases the computa-
tional effort, and could be biased by the geographic origin of the data. 
Therefore,  we  performed  the  semantic  analysis  using  the  pre-trained 
baroni  word2vec  model  provided  at  http://www.lingexp.uni-tuebin

gen.de/z2/LSAspaces  (Günther  et  al.,  2015).  This  model  has  been 
demonstrated  to  generate  high-quality  performance  with  dense  word 
vectors  and  to  produce  the  best  results  regarding  semantic  similarity 
tasks (Baroni et al., 2014). This space was created using the CBOW al-
gorithm and contains vectors for 300,000 different words, covering a 
broad variety of different topics. It was trained from a 2.8-billion-word 
corpus, a concatenation of the ukWaC corpus (web pages material from . 
uk  domain;  Baroni  et  al.,  2009),  Wikipedia,  and  the  British  National 
Corpus (BNC Consortium, 2007).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not contain information about where the code repository of the deep learning pipeline is available. Therefore, it is not possible to provide an accurate answer to this query based solely on the given context.
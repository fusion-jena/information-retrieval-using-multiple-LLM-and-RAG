Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The  evaluation  indicators  of  the  experiment  mainly  included  the 
testing accuracy, inference time, precision, recall, specificity, F1 score, 
and sensitivity. In the experiment, the batch size was set to 16, and the 
learning rate was 0.00005, which was optimised using the Adam opti-
miser. For the AlexNet and SIAlex models, the number of epochs was set 
as 200. A cross-entropy loss function was adopted the cross entropy loss 
function. 

Table 2 
The Birdsdata dataset collected 20 types of bird sounds both indoors and out-
doors in the form of actual collection and network collection.  

Types of birds 

Number of 
samples 

Types of birds 

Number of 
samples

In view of this discovery, we re-examined the application strategies 
of deep learning, no longer pursuing model complexity but exploring the 
potential  of  shallow  networks.  Through  meticulous  algorithm  optimi-
sation  and  structural  design,  we  strive  to  maintain  the  model  perfor-
mance while reducing the computational complexity  and deployment 
costs,  achieving  a  balance  between  speed  and  accuracy.  The  specific 
contributions of this study are as follows:  

• We propose a lightweight SIAlex model that utilises AlexNet as the 
backbone,  fully  exploiting  the  performance  of  minimalist  models. 
Ensuring a good balance between speed and accuracy. 

• The  method  of  cascading  multiple  activation  functions  fully  in-
troduces  nonlinear  factors  such  that  the  model  approximates  the 
nonlinear  expression  function  of  the  learning  features  while  also 
improving the gradient propagation.

4.2.4. Comparison with existing model methods 

Classic deep learning models, such as ResNet and DenseNet (Zhu and 
Newsam, 2017), and lightweight deep learning models, such as Mobi-
leNet (Sandler et al., 2018), ShuffleNet (Zhang et al., 2018), and Effi-
cientNet  (Tan  and  Le,  2019),  were  selected  to  demonstrate  the 
effectiveness of the proposed model. We train the bird sound recognition 
dataset using the above model, record the testing accuracy, and testing 
time of different models, and compare them with the model proposed in 
this study. The experimental data are shown in Fig. 8. 

Compared to other lightweight models, SIAlex improved the accu-
racy  to  93.66%  with  a  testing  time  of  only  2.466  ms.  SIAlex's  multi- 
indicator evaluation for each category is shown in Fig. 10, and Fig. 9 

Table 3 
Comparison of AlexNet under different activation function and classifier.   

ReLU 

✓  

Leaky 
ReLU 

✓ 
✓  

AlexNet 
AlexNet  
AlexNet  

Basic classifier

can  further  improve  the  efficiency  and  performance  of  deep  learning 
models and promote the development of related fields.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The evaluation indicators of the deep learning model include several metrics to measure its performance. These metrics are primarily focused on assessing the quality of predictions made by the model. Some of these key metrics are:

1. Testing Accuracy: This metric measures how often the model makes correct predictions when presented with new, unseen data. It is calculated as the ratio of the total number of accurate predictions to the total number of input samples.

2. Precision: Also known as Positive Predictive Value, it calculates the proportion of true positive predictions among all predicted positives. High precision indicates fewer false positives.

3. Recall or Sensitivity: This metric evaluates the ability of the model to identify true positives correctly. It is computed as the fraction of relevant instances that were retrieved over the total amount of relevant instances.

4. Specificity: This metric quantifies the capability of the model to accurately predict negative outcomes. It is determined as the ratio of true negatives to the sum of true negatives and false positives.

5. F1 Score: This is the harmonic mean of precision and recall, providing a single value that balances both metrics. It ranges from 0 to 1, where higher values indicate better performance.

6. Inference Time: This refers to the duration required for the model to process a given input and generate an output. Lower inference times suggest faster processing capabilities.

These metrics help researchers understand the strengths and weaknesses of their deep learning models, enabling them to fine-tune and optimize their designs accordingly.
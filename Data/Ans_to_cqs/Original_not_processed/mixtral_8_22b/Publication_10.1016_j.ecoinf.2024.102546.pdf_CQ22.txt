Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3. Materials and methods 

3.1. Dataset curation

Future research into the quantitative assessment of domain gaps or 
shifts between training and testing data can be beneficial for developing 
effective methods for improved cross-domain performance and deciding 
on whether domain adaptation/generalization is needed on unseen data 
(Doan  et  al.,  2023).  The  current  evaluation  metrics  (Kynk¨a¨anniemi 
et al., 2019) focus on image fidelity and reality, which contribute to the 
assessment  of  image  generation  by  rapidly  growing  diffusion  models 
instead of evaluating the domain gaps. On the other hand, the human 
evaluation  protocol,  which  is  widely  adopted  to  distinguish  synthetic 
images, like side-by-side image comparison (Wang et al., 2023), can be a 
useful strategy to measure the domain gaps before more solid domain- 
gap metrics are discovered. 

6. Conclusion

Fig. 6. Example predictions by YOLOv8 trained on the Year 2021 data and tested the images from the same season (in-season) and trained on the Year 2022 data and 
tested on the Year 2022 data (cross-season). The percentage and label attached to each bounding box indicate the detection confidence and the predicted weed class, 
respectively. 

Fig. 7. Weed detection accuracies of YOLOX and YOLOv8 with and without I3Net-based domain adaptation (DA). In-season and cross-season metrics refer to the 
accuracies of models trained and tested on the data from the same and different seasons, respectively, and the error bar indicates the standard error over three 
modeling replications.

6  https://github.com/ultralytics/ultralytics/issues/189 

EcologicalInformatics81(2024)1025466B. Deng et al.                                                                                                                                                                                                                                    

Fig. 3. Flowchart of our cross-season weed detection.  

Table 2 
Primary hyperparameter settings of the three detection models.  

Hyperparameters 

Detection models 

Initial learning 

rate 

Learning rate 
schedule 
NMS threshold 
Confidence 
threshold 
Weight decay 
Image size 
Batch size 

Optimizer 

YOLOX 

YOLOv8 

0.01 

0.01 

Warm up +
cosine decay 
0.45 

Warm up + cosine 
decay 
0.7 

0.25 

0.0005 
800 
8 

SGD 

0.25 

0.0005 
800 
8 

SGD 

Classification loss 

BCE 

Location loss 

IoU 

Data augmentation 

Color jitter 
+ mosaic 

VFL (Zhang et al., 
2021) 
DFL (Li et al., 2020) 
+ CIoU (Zheng et al., 
2021)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper primarily focuses on using weed detection accuracies to evaluate the performance of the deep learning models YOLOX and YOLOv8. These accuracies are measured both when the models are trained and tested on data from the same season (in-season) and when they are trained and tested on data from different seasons (cross-season). The authors also use the standard error over three modeling replications to provide an indication of variability in their results. However, it should be noted that the paper does not explicitly mention the use of other common classification metrics such as precision, recall, or F1 score.
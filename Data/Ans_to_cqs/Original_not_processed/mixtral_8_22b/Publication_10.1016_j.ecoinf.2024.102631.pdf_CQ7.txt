Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The network takes images with dimensions of 256 × 256 pixels and 
three  color  channels  as  input.  It  follows  a  fully  convolutional  design, 
with  each  layer  applying  2D  convolutions  using  4  × 4  filters.  Batch 
Normalization and Leaky-ReLU activation functions are used after each 
convolution  layer  to  facilitate  network  training  and  stability.  The 
encoder consists of 5 blocks, which progressively reduce the spatial di-
mensions and learn features. It starts with a convolutional layer with 32 
output channels and strides of 2, followed by a residual block. The re-
sidual block comprises a Conv-ReLU-Conv structure, where a convolu-
tional layer is followed by a Rectified Linear Unit (ReLU) activation and 
another convolutional layer. This configuration is designed to effectively 
capture  and  enhance  image  features,  serving  as  a  critical  component 
within the network's architecture. The residual block output is then fed

Table 4 
Quantitative  comparison  on  UCCS  dataset  using  UIQM,  UCIQE,  and  NIQE 
metrics.   

UIQM↑ 

UCIQE↑ 

NIQE↓ 

Raw_Images 
Fusion 
IBLA 
U-Transformer 
Funie-GAN 
Water-Net 
UWCNN 
Ours 

2.29 
2.99 
2.36 
3.02 
3.05 
3.13 
2.78 
3.17 

0.410 
0.476 
0.480 
0.539 
0.558 
0.550 
0.455 
0.568 

4.57 
4.38 
4.29 
3.96 
4.37 
4.07 
4.12 
4.03  

A quantitative analysis of this dataset is given in Table 5, confirming 
our  model's  effectiveness.  Our  method  has  achieved  a  UIQM  score  of 
3.26 and an NIQE score of 5.39, followed by the Funie-GAN in terms of 
UIQM with a score of 3.21 and Fusion in terms of NIQE with a score of 
5.40. While measuring the UCIQE score, IBLA achieved a score of 0.626, 
although their resultant images are just color deviations from the orig-
inal images. However, since UCIQE measures a linear combination of 
chroma, saturation, and contrast, a higher value can be expected just by 
having a change in colors (even if they are not desirable).

3.3. Loss Functions 

In this work, we used a linear combination of the adversarial loss 

function Ladv, L1  loss function, and the perceptual loss function Lper. 

Ltotal = Ladv + λ1Lper + λ2L1

(4)  

(2) 

(3) 

where λ1  and λ2  are scaling factors set to 7 and 3, respectively. 

Adversarial Loss: We used adversarial loss to facilitate the training 
of the generator network and the discriminator network in a competitive 
manner. This loss function encourages the generator to minimize the log 
probability  that  the  discriminator  assigns  to  the  generated  samples, 

Fig. 4. PatchGAN Discriminator: It processes the image patch-wise, and each block in the output label map matrix shows whether the corresponding image patch is 
real or fake.

24.36 
0.885 

15.25 
0.656 

25.59 
0.893  

EcologicalInformatics81(2024)1026319A.B. Bakht et al.                                                                                                                                                                                                                                

Table 2 
Quantitative  comparison  on  C-60  dataset  using  UIQM,  UCIQE,  and  NIQE 
metrics.   

UIQM↑ 

UCIQE↑ 

NIQE↓ 

Raw_Images 
Fusion 
IBLA 
U-Transformer 
Funie-GAN 
Water-Net 
UWCNN 
Ours 

1.99 
2.78 
1.81 
2.65 
3.10 
2.57 
2.25 
3.12 

0.478 
0.512 
0.574 
0.534 
0.572 
0.578 
0.466 
0.591 

5.18 
4.74 
5.02 
4.94 
4.73 
4.88 
4.89 
4.67  

Table 3 
Quantitative comparison on U45 dataset using UIQM, UCIQE, and NIQE metrics.   

UIQM↑ 

UCIQE↑ 

Raw_Images 
Fusion 
IBLA 
U-Transformer 
Funie-GAN 
Water-Net 
UWCNN 
Ours 

2.44 
3.14 
1.60 
3.10 
3.21 
3.18 
2.82 
3.23 

0.481 
0.532 
0.579 
0.553 
0.602 
0.587 
0.471 
0.612 

NIQE↓

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The criteria used to split the data for deep learning model training typically include the size of the dataset, the complexity of the model, and the desired level of generalization. Commonly, the dataset is divided into three subsets: training, validation, and testing sets.

The training set is used to fit the parameters of the model, while the validation set is employed during the training process to tune hyperparameters and prevent overfitting. Lastly, the testing set is utilized to evaluate the final performance of the trained model on unseen data.

Regarding the proportions of these splits, there isn't a one-size-fits-all solution. A common practice is to use around 60-80% of the data for training, 10-20% for validation, and the remaining 10-20% for testing. However, these percentages may vary depending on the specific requirements and constraints of the problem at hand. For instance, when dealing with small datasets, it might be more appropriate to adopt techniques such as cross-validation or bootstrapping to maximize the utilization of available data.
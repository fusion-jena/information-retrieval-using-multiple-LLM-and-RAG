Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

)

(cid:0)

Ω

fj

= τT +

1
2

λ||w||2

(9)  

2.2.6. Heterogenous ensemble techniques 

Heterogeneous ensemble learning is the practice of combining pre-
dictions of multiple models that are different in terms of their underlying 
algorithms  or  architectures  to  improve  overall  performance.  Utilizing 
heterogeneous ensemble techniques can address biases and improve the 
implemented  two 
effectiveness  of  models. 

In  our  study,  we 

EcologicalInformatics81(2024)10258312G. Agboola et al.                                                                                                                                                                                                                                

Fig. 11. The proportion of area covered by each susceptibility category in each scenario.  

heterogeneous  techniques  to  optimize  the  performance  of  five  base 
learners and mitigate potential overfitting:

FR = F/E 

Soil 

Distance to 
stream 

Land Cover 

Lithology 

Distance to road 

Inceptisols 
Ultisols 
Bodies of Water 
Alfisols 
Entisols 

0–50 
50–100 
100–150 
150–250 
250–360 
Forest 
Grassland 
Wetland 
Cropland 
Barrenland 
Urban 
Water body 
Mica-schist 
Henderson-Gneiss 
Garnet-mica-schist 
Biotite-gneiss and schist 
Migmatitic-granitic-gneiss 
Amphibolite and biotite- 
gneiss 
Granite-gneiss 
Porphyroblastic-gneiss 
Caesars-Head Granitic-Gniess 
<200 
200–400 
400–600 
600–800 
>800 

124,464 
438,856 
6717 
343 
11,028 

305,134 
190,058 
71,058 
14,915 
243 
439,207 
10,914 
815 
50,952 
447 
73,274 
5799 
9949 
104,512 
44,510 
178,276 
70,962 

11,509 
39,400 
116,161 
6129 
1,60,092 
104,592 
72,603 
51,114 
193,007 
581,408    

357 
809 
0 
0 
49 

631 
419 
129 
36 
0 
1072 
20 
0 
0 
1 
122 
0 
11 
123 
11 
510 
376 

6 
13 
165 
0 
302 
148 
107 
81 
577 

21% 
75% 
1% 
0.1% 
2%

EcologicalInformatics81(2024)1025832G. Agboola et al.                                                                                                                                                                                                                                

Fig. 2. Research workflow  

Pourghasemi, 2021), and the Support Vector Machine (SVM) models (F. 
Huang et al., 2022; Y. Huang and Zhao, 2018; Shahzad et al., 2022; Xing 
et al., 2021). These models are flexible and adaptable systems that can 
manage diverse types of data and scenarios for LSM. Another type of 
MLM that has been explored for LSM is the Artificial Neural Network 
(ANN), especially in the Radial Basis Function (RBF) (W. Chen et al., 
2018; Ngo et al., 2021) and Multi-Layer Perceptron (MLP) configuration 
(Adnan  et  al.,  2020;  Chang  et  al.,  2022;  Z.  Zhao  et  al.,  2022).  These 
models can learn from data and adjust their parameters to improve their 
performance.

2021; Hu et al., 2020; K. Li et al., 2023), and weighted average (WA) (K. 
Li  et  al.,  2023;  Zeng  et  al.,  2023)  have  been  empirically  proven  to 
mitigate  overfitting  issues,  thereby  enhancing  the  precision  of  LSM 
(Kadavi et al., 2018).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors used heterogeneous ensemble techniques to optimize the performance of five base learners and mitigate potential overfitting. They combined predictions from multiple models with different underlying algorithms or architectures. This approach helps to address biases and improve the effectiveness of models. However, the specific methods used to ensure the generalizability of the deep learning model, such as using a diverse dataset, cross-validation, or stratified splitting, are not explicitly mentioned in the given context. Therefore, it cannot be determined which specific measures were taken to ensure the generalizability of the deep learning model based on the provided information.
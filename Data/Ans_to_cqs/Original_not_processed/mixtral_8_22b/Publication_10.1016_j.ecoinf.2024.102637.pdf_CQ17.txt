Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In view of this discovery, we re-examined the application strategies 
of deep learning, no longer pursuing model complexity but exploring the 
potential  of  shallow  networks.  Through  meticulous  algorithm  optimi-
sation  and  structural  design,  we  strive  to  maintain  the  model  perfor-
mance while reducing the computational complexity  and deployment 
costs,  achieving  a  balance  between  speed  and  accuracy.  The  specific 
contributions of this study are as follows:  

• We propose a lightweight SIAlex model that utilises AlexNet as the 
backbone,  fully  exploiting  the  performance  of  minimalist  models. 
Ensuring a good balance between speed and accuracy. 

• The  method  of  cascading  multiple  activation  functions  fully  in-
troduces  nonlinear  factors  such  that  the  model  approximates  the 
nonlinear  expression  function  of  the  learning  features  while  also 
improving the gradient propagation.

Yconv+conv = W1

*(W2

*x) = W1⋅W2⋅im2col(x) = (W1⋅W2)*x

(11)  

3.2.2. Cascading activation function 

In deep learning, model optimisation often focuses on enhancing the 
performance of complex deep  networks by  flexibly selecting different 
activation  functions.  This  study  reduces  the  complexity  of  the  model 
through  the  fusion  of  convolutional  layers,  thereby  reducing  the 
computational complexity and improving the computational efficiency. 
However,  it  also  brings  about  a  crucial  problem:  a  simple  network 
structure leads to weak nonlinearity, which limits performance. To solve 
the  vanishing  gradient  problem  in  the  ReLU  activation  function,  this 
study replaced the ReLU activation function with a Leaky ReLU (Dubey 
and Jain, 2019; Xu et al., 2020) activation function. The Leaky ReLU 
function provides a non-zero slope to the negative values in the input 
data, as defined below: 

Leaky ReLU(x) =

}

{

x, x ≥ 0
ax, x < 0

(12)

generalised  model  by  adding  convolution  modules  or  introducing  re-
sidual  connection  techniques.  However,  more  complex  and  deeper 
models  often  incur  higher  computational  costs  and  require  longer 
training  time.  To  solve  these  problems,  the  AlexNet  network  was 
designed to be lightweight and optimised to achieve deep training and 
short-term testing (Zhang et al., 2022). Specifically, the two convolution 
layers are merged in the model deployment mode while ensuring that 
the nonlinear factors caused by the activation function are not reduced. 
By reducing the number of layers in the model, the computational effi-
ciency is improved (Ding et al., 2021; Zhao et al., 2017) and the infer-
ence time of the model is significantly shortened.

First, a single convolution layer is merged with a single Batch Nor-
malisation  (BN)  layer.  When  training  the  model,  the  BN  layer  can 
accelerate  the  network  convergence  and  prevent  overfitting.  By  nor-
malising the BN layer, the problems of gradient disappearance and ex-
plosion can be effectively solved. The method has been applied to many 
advanced  network  models,  such  as  ResNet,  MobileNet,  Xception,  and 
ShuffleNet, which use BN technology to optimise model performance. 
However,  although  the  BN  layer  plays  an  active  role  in  the  training 
process, additional operations are added to the network forward infer-
ence,  which  affects the  performance of  the  model and  occupies  more 
memory or memory space. To solve this problem, the BN layer param-
eters can be merged into a convolution layer to improve the forward 
inference speed of the model. Through the above optimisation steps, we

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information regarding the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, there is mention of using Batch Normalization (BN), which can help accelerate network convergence and prevent overfitting during training. Additionally, merging the BN parameters into a convolution layer improves the forward inference speed of the model. While these techniques contribute to better model performance, they do not directly address the issue of preventing overfitting through regularization methods like dropout or L2 regularization.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

http://orcid.org/0000-0002-2176-7935 

R E F E R E N C E S

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., … Kudlur, M. 
(2016). TensorFlow: A system for large- scale machine learning. OSDI, 
16, 265–283.

Anderson, T. M., White, S., Davis, B., Erhardt, R., Palmer, M., Swanson, A., 
… Packer, C. (2016). The spatial distribution of African savannah her-
bivores: Species associations and habitat occupancy in a landscape 
context.  Philosophical  Transactions  of  the  Royal  Society  B:  Biological 
Sciences, 371, 20150314. https://doi.org/10.1098/rstb.2015.0314
Babaee,  M.,  Dinh,  D.  T.,  &  Rigoll,  G.  (2017).  A  deep  convolutional  neu-
ral  network  for  background  subtraction.  Pattern  Recognition,  76, 
635–649.

|  1437

F I G U R E   2  The front screen of the DeepMeerkat GUI. A user 
can select a file or directory of videos to screen using a pre- trained 
model. The path to the model is set under “Advanced settings”

creasing false positives.

majority of hummingbird visitation events (Weinstein, 2015). For the 

For training the fine- tuned neural network, I collected images for 

purposes  of  this  article,  I  assumed  that  all  events  are  captured  by 

each class and trained with a batch size of 100 for 20,000 steps. To 

motion  detection  and  were  passed  to  the  neural  network  for  clas-

reduce training time, the feature vectors for the frozen layers were 

sification.  While  quantitative  benchmarks  are  needed  to  validate 

extracted in parallel using Google Cloud DataFlow. These features 

model performance, they provide a coarse description of the errors 

were then the basis for retraining the new fine- tuned layers. To fit

download  for  Mac  and  Windows  with  the  pre- trained  humming-

bird model. In addition, I provide reproducible scripts for local and 

3 |  R E S U LT S

Google  cloud  environments  to  allow  users  to  train  new  models, 

which can then be used in the local software.

2.2 | Test dataset

Feature  extraction  of  the  fixed  inception  layers  completed  in  1 hr 

and  26 min  on  15  CPUs.  Training  of  the  new  layers  completed  in 

27 min on a single CPU. Model evaluation on the 70 test videos com-

pleted in 4 hr and 38 min on 30 CPUs with an average frame rate of 

My  collaborators  and  I  have  been  studying  hummingbird  ecol-

17 frames/s. On average, a video contained  545.84 candidate  mo-

ogy  using  time- lapse  cameras  in  the  Ecuadorian  Andes  since  2013 

tion frames (2.5% of total frames) that were sent for classification by 

(Weinstein & Graham, 2017). Cameras turn on at dawn, off at dusk,

classes. Dropout is a form of regularisation that randomly removes 

a  proportion  of  nodes  to  reduce  overfitting  (Srivastava,  Hinton, 

Krizhevsky, Sutskever, & Salakhutdinov, 2014). The fully connected 

layer reduces the vector of image features to the desired dimension-

ality of length two (foreground and background). The softmax layer 

normalises  this  vector  into  probabilities  that  sum  to  one  across  all 

classes.  DeepMeerkat  is  designed  to  be  conservative,  with  a  high 

threshold for retaining frames (acceptance value = 0.1). This means 

that the model must be more than 90% confident that a frame does 

not contain a foreground object to assign a background label. This 

prioritises minimising false negatives at the potential expense of in-

Methods in Ecology and Evolu(cid:13)on

    |  1437

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps may be involved depending on the specific application and data type. However, some common preprocessing techniques include:

1. Normalization: This involves rescaling the input features so that they fall within a certain range, typically between 0 and 1 or -1 and 1. This helps ensure that each feature contributes equally to the final output and prevents any particular feature from dominating the others due to its larger magnitude.

2. Scaling: Similar to normalization, scaling involves adjusting the values of input features to bring them to a similar scale. This can help improve the convergence speed during training and prevent numerical instability issues.

3. Cleaning: This step involves removing noise, outliers, or missing values from the input data. It also includes handling categorical variables by converting them into numerical representations such as one-hot encoding or ordinal encoding.

4. Augmentation: This technique involves creating additional training examples by applying transformations such as rotation, flipping, zooming, etc., to existing samples. This can help increase the diversity of the training data and improve the generalizability of the model.

In the given context, it appears that the authors extracted feature vectors from the fixed inception layers using Google Cloud DataFlow and used these features as the basis for retraining the new fine-tuned layers. Although not explicitly mentioned, it is likely that the authors performed some form of normalization or scaling on their input data prior to feeding it into the deep learning model. Additionally, the use of dropout as a form of regularization suggests that the authors took measures to prevent overfitting during training.
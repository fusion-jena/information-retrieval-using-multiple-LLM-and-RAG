Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The duration of audio ﬁles in CLO-43DS data is different,
which cannot be directly used as the input to the CNN. The
ﬁrst method for dealing with the multi-variate varying length
audio data is that the signal is repeated from the beginning to
force the ﬁxed duration of 2s, which has been used in [30].
The second method is to directly resize the audio image to a
ﬁxed size.

D. DEEP LEARNING ARCHITECTURE
The feature learning part of our proposed model follows a
VGG style network [24], which has been previously used
for classifying acoustic scenes [5]. The overall architec-
ture is illustrated in Table 1. This network is trained using
Adam optimizer with a learning rate of 10−4. The cate-
gorical cross entropy is utilized as the loss function. The
batch size is 64 samples and the network is trained with
200 epochs.

Our class-based fusion scheme considers the class-based
weights W1(x) and current prediction vector V (x) to make a
ﬁnal prediction for a test instance (x). This method calculates
score for each class using following formula.

Scorek = (cid:88)
Vi(x)=Ck

(W1ik ), 1 ≤ k ≤ m, 1 ≤ i ≤ n

(2)

Finally, it selects the class label as ﬁnal prediction, which

has maximum score using the following equation.

Labelﬁnal = Cargmaxm
k=1

Scorek

(3)

IV. EVALUATION RULE
In this experiment, the dataset was ﬁrst randomly divided into
two parts (85%-15%), where 15% was used as the testing
data. Then, we further split the 85% part into 60%-40% for
tuning the parameters of CNNs. This process is repeated
ﬁve times and an averaged classiﬁcation result is reported.
Since the dataset we used is highly imbalanced (see Fig. 2),
the performance of our proposed bird call classiﬁcation sys-
tem is evaluated using balanced accuracy, weighted preci-
sion, weighted sensitivity, weighted speciﬁcity, and weighted

[21] Y.
by
divisions,’’
Available:
classiﬁcation-results-a

‘‘Acoustic
based

Sakashita
ensemble

adaptive
2018.

scene

on

[22] J. Salamon, J. P. Bello, A. Farnsworth, and S. Kelling, ‘‘Fusing shallow
and deep learning for bioacoustic bird species classiﬁcation,’’ in Proc.
IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Mar. 2017,
pp. 141–145.

[23] J. Salamon, J. P. Bello, A. Farnsworth, M. Robbins, S. Keen, H. Klinck,
and S. Kelling, ‘‘Towards the automatic classiﬁcation of avian ﬂight
calls for bioacoustic monitoring,’’ PLoS ONE, vol. 11, no. 11, pp. 1–26,
Nov. 2016.

[24] K. Simonyan and A. Zisserman, ‘‘Very deep convolutional networks for
large-scale image recognition,’’ in Proc. 3rd Int. Conf. Learn. Repre-
sentations, (ICLR), San Diego, CA, USA, 2015. [Online]. Available:
http://arxiv.org/abs/1409.1556

[25] J. Song and S. Li, ‘‘Bird audio detection using convolutional neural
networks and binary neural networks,’’ in Proc. DCASE Challenge,
Sep. 2018. [Online]. Available: http://dcase.community/challenge2018/task-
acoustic-scene-classiﬁcation-results-a

[26] D. Stowell, E. Benetos, and L. F. Gill, ‘‘On-bird sound recordings:
Automatic acoustic recognition of activities and contexts,’’ IEEE/ACM
Trans. Audio, Speech, Language Process., vol. 25, no. 6, pp. 1193–1206,
Jun. 2017.

[27] D. Stowell and M. D. Plumbley, ‘‘Automatic large-scale classiﬁcation of
bird sounds is strongly improved by unsupervised feature learning,’’ PeerJ,
vol. 2, p. e488, Jul. 2014.

[28] Y. Su, K. Zhang, J. Wang, and K. Madani, ‘‘Environment sound classiﬁ-
cation using a two-stream CNN based on decision-level fusion,’’ Sensors,
vol. 19, no. 7, p. 1733, 2019.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The text does not explicitly state the specific criteria used to determine when training is complete. However, some clues can be found in the description of the training process. The network is trained with Adam optimizer, a learning rate of 10^-4, and categorical cross entropy as the loss function. It also mentions that the batch size is 64 samples and the network is trained with 200 epochs. These details suggest that the number of epochs might be one criterion used to decide when training should stop. In machine learning, an epoch refers to one cycle through the full training dataset. Therefore, training the network for 200 epochs means going through the entire dataset 200 times. While this information doesn't provide a definitive answer regarding the exact stopping criteria, it suggests that the authors may have chosen to train their model for a fixed number of epochs rather than waiting for a validation loss plateau or other similar conditions.
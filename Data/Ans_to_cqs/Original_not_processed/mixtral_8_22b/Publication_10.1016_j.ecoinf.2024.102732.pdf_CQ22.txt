Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

parameter combination, enhancing the model’s performance. Important
indices to measure variable importance in XGBoost include “Gain” and
“Frequency” (Gertz et al., 2020; Li et al., 2023; Luo et al., 2024; Meh-
mood et al., 2024a; Ryu et al., 2020). SVR tuning involves selecting the
appropriate kernel function and optimizing the “C” parameter, which
balances margin width and misclassification tolerance. The versatility of
SVR allows it
to handle complex decision boundaries effectively
(Yudong Li et al., 2020; Rodriguez-Galiano et al., 2015).

2.9. Performance evaluation of the models

specifically a max_depth value of 7 and an eta value of 0.3. Out of the
models that were assessed, it was found that the RF model exhibited the
most superior predictive capability. The model demonstrated a strong fit
to the data, as evidenced by R-squared values of 0.66, 0.70, and 0.81.
The RF model showed exceptional error reduction performance, as seen
by an RMAE of 14.33% and RRMSE of 22.23%. The hyperparameters,
namely max_depth = 25, n_estimators = 1000, and mtry = 1,3,7, were
optimized to attain the observed level of performance in each of the
three combinations.

43.24
42.70
32.46
29.77
44.51
31.46
29.20
27.82
44.87
25.24
23.85
22.23
43.89
30.78
26.53
25.61

MAE

29.67
26.38
19.50
17.15
30.26
16.68
16.42
15.48
30.97
13.24
12.73
11.73
30.06
16.40
14.87
14.38

RMSE

35.42
34.97
26.59
24.38
36.45
25.75
23.90
22.79
36.75
20.67
19.54
18.21
35.95
25.21
21.72
20.97

Parameters

C1=1, Sigma2 = 10
max_depth3 = 7, eta4 = 0.3
max_depth = 25, n_estimators5 = 1000

C = 10, Sigma = 10
max_depth = 7, eta = 0.3
max_depth = 25, n_estimators = 1000

C = 1, Sigma = 10
max_depth = 7, eta = 0.3
max_depth = 25, n_estimators = 1000

C = 1, Sigma = 10
max_depth = 7, eta = 0.3
max_depth = 25, n_estimators = 1000

1 “C” is the regularization parameter.
2 Bandwidths.
3 Maximum depth of a tree.
4 Learning rate.
5 Number of Trees.

EcologicalInformatics82(2024)1027329S.A. Anees et al.

be easily replicated in other studies or regions.

3.2. Evaluation of different predictor combinations and machine learning
algorithms

3.2.1. Variable combinations and model comparison

regression. In: 2018 4th International Conference on Computing Communication and
Automation (ICCCA). IEEE, pp. 1–4.

Strandberg, R., Låås, J., 2019. A Comparison between Neural Networks, Lasso

Regularized Logistic Regression, and Gradient Boosted Trees in Modeling Binary
Sales.

Su, H., Shen, W., Wang, J., Ali, A., Li, M., 2020a. Machine learning and geostatistical

approaches for estimating aboveground biomass in Chinese subtropical forests. For.
Ecosyst. 7, 1–20.

principle of space-for-time substitution in predicting Betula spp. biomass change
related to climate shifts. Appl. Ecol. Environ. Res. 20 (4), 3683–3698.
Vaglio Laurin, G., Pirotti, F., Callegari, M., Chen, Q., Cuozzo, G., Lingua, E.,

Notarnicola, C., Papale, D., 2016. Potential of ALOS2 and NDVI to estimate forest
above-ground biomass, and comparison with lidar-derived estimates. Remote Sens.
9, 18.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't any specific information about the use of deep learning models or their evaluation metrics such as accuracy, precision, or recall. However, the text does mention several performance evaluation metrics for other types of machine learning models like Random Forest (RF) and Support Vector Regression (SVR).

For instance, the paper discusses using R-squared values to demonstrate how well the RF model fits the data. Additionally, they also provide Root Mean Absolute Error (RMAE) and Root Relative Mean Square Error (RRMSE) as indicators of the model's error reduction performance. These metrics can be considered analogous to accuracy, precision, and recall in evaluating classification tasks but are more suitable for regression problems.

Moreover, when discussing the optimization of hyperparameters for the RF model, the authors mention parameters like max\_depth, eta, and n\_estimators. Similarly, for SVR tuning, they talk about selecting appropriate kernel functions and optimizing the 'C' parameter. While these aren't directly related to the requested evaluation metrics, they do highlight important aspects of improving model performance.

Therefore, based on the available context, we cannot provide direct information regarding the evaluation metrics used for deep learning models. Instead, we have discussed the relevant performance evaluation metrics mentioned for other machine learning algorithms within this study.
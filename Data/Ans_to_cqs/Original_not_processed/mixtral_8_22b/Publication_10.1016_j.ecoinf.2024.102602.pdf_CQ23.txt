Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.5.1. Model selection and parametrization 

For  the  classification  task,  before  the  implementation  of  deep 
learning models, all images were resized to the same resolution (227×
197  pixels)  by  considering  the  mean  dimensions  of  the  set,  and  then 
normalized to the [0,1] range (Na and Fox, 2020). Then, six open source 
CNNs  were  selected:  VGG16  (Simonyan  and  Zisserman,  2015), 
ResNet50,  ResNet101  (He  et  al.,  2016),  Inception-v3  (Szegedy  et  al., 
2016), DenseNet201 (Huang et al., 2017) and EfficientNetB0 (Tan and 
Le,  2019).  These  algorithms  were  selected  because  of  their  ease  for 
transfer  learning and  high  performance  on  similar classification  tasks 
(Arun  and  Viknesh,  2022;  Vallabhajosyula  et  al.,  2022).  For  model 
optimization, we used the Adam optimizer algorithm (Kingma and Ba, 
2015), a batch size of 10 and 100 epochs. The learning rates were chosen 
(cid:0) 6 showing 
from empirical trials over 100 epochs, with and 10

2.5.2. Model training and validation 

For  the  classification  task,  the  performance  of  the  models  was 

evaluated  using  5-fold  cross  validation  over  the  dataset  described  in 
Sections 2.3 and 2.4, as this approach provides a robust and unbiased 
estimate of a model’s performance while also guiding effective hyper-
parameter  tuning  for  improved  overall  performance  (James  et  al., 
2023). The dataset was divided into 5 subsets, and, at each iteration of 
the 5-fold cross validation, one was used to evaluate the models using 
the  performance  metrics  that  will  be  presented  in  Section  2.5.4.  The 
remaining  4  subsets  were  used  for  training  (90%  of  the  images)  and 
validation (10% of the images).

A.1b),  with  high  performances  (e.g.,  f1-score  values  above  93.09%; 
Table  4).  Unlike  the  training  task,  the  best  results  were  obtained  by 
(cid:0) 4,  in  terms  of  accuracy 
EfficientNetB0  with  a  learning  rate  of  10
(97.50%),  sensitivity  (96.00%)  and  f1-score  (97.46%),  followed  by 
(cid:0) 4),  which  also  showed 
Inception-v3  with  the  same  learning  rate  (10
high accuracy (96.88%), sensitivity (94.00%), specificity (99.75%) and 
f1-score (96.78%). In the case of the EfficientNetB0 model, only 4% of 
the  images  displaying  the  Cortaderia  selloana  were  confused  by  the 
model as showing no C. selloana (false negatives; Fig. A.1c). Likewise, 
from  the  images  showing  no  C.  selloana,  in  1%  of  the  cases  Effi-
cientNetB0  incorrectly  predicted  the  class  “With  Cortaderia  selloana” 
(false  positives;  Fig.  A.1d).  A  similar  pattern  was  verified  for  the 
remaining CNNs (see confusion matrices in Tables A.6 and A.7).

EcologicalInformatics81(2024)1026029Faster R-CNN ResNet101 iNaturalistFaster R-CNN ResNet101 MS COCOFaster R-CNN ResNet50 iNaturalistFaster R-CNN ResNet50 MS COCOFaster R-CNN Inception-v2 MS COCOAverage inference time per image (ms)3951063668958mAP@0.50IOU76.8579.2374.9380.8081.71Total loss2.241.142.241.201.08A.S. Cardoso et al.                                                                                                                                                                                                                              

with other plant species. Moreover, there are no other plant species in 
Portugal (our study area) with the same features as C. selloana. Both of 
these particularities may be the source of the high results observed in 
this study. 

4.4. Monitoring the invasive alien Cortaderia selloana from online digital 
images

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To ensure the generalizability of the deep learning model, several measures were implemented during the development process. Firstly, the dataset used for training and validating the model consisted of diverse images collected from various sources, including Google Images, Flickr, and iNaturalist. This diversity helped capture different aspects of the target species, such as variations in lighting conditions, angles, and background elements. Secondly, the images were preprocessed by resizing them to a common resolution (227x197 pixels) and normalizing them to the [0,1] range. This step ensured consistency across the input data and facilitated efficient processing by the deep learning models. Thirdly, six popular convolutional neural networks (CNNs) were employed for the classification task, namely VGG16, ResNet50, ResNet101, Inception-v3, DenseNet201, and EfficientNetB0. These architectures have demonstrated excellent performance on similar classification tasks and are well-suited for transfer learning. Fourthly, the optimization of these models was carried out using the Adam optimizer algorithm, with a batch size of 10 and 100 epochs. Learning rates were determined through empirical trials over 100 epochs. Lastly, the performance of the models was evaluated using 5-fold cross-validation over the entire dataset. This technique involves dividing the dataset into five equal parts or 'folds,' where four folds are used for training and the remaining fold is utilized for testing. This procedure is repeated five times, ensuring that every fold serves as the test set once. Cross-validation helps prevent overfitting and provides a more accurate estimate of the model's performance on unseen data. Overall, these measures aimed to enhance the robustness and reliability of the deep learning model when applied to new datasets.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

5 Grid search: C = {10−4; 10−3; 10−2; 10−1;1;10;100}; solver = {sag; saga;

lbfgs}

accuracy comprised between 70% and 80% (“Sand”, “Eunicella singu-
laris”, “Filamentous green algae”, “Filamentous red algae”,” Flabellia
petiolata”, “Haliclona sp.”, “Halimeda tuna”, “Palmophyllum crassum”,
“Encrusting Peyssonnelia sp.”, “Pentapora fascialis”, “Phorbas tenacior”,
“Pleraplysilla spinifera”).

6.3. Post-processing

6.3.1. Semi-automated classification

Reliability diagrams –which plot confidence against accuracy – were
plotted for the baseline ResNet152, the best ResNet18, and the en-
semble network with and without LR (see Fig. 7). The red line shows a
theoretical, perfect calibration (y = x).

60.64
63.82
65.77
66.30

60.17
63.57
65.54
65.94

Table 3
Performances of different ResNet architectures on validation and test sets. ResNetX-Y is written so that X indicates the network's depth and Y the input size. In bold
the best value for each metric.

Network -patch Size

Batch size

Validation set

Test set

Macro-F1

Top-1 accuracy

Micro-F1

Macro-F1

Top-1 accuracy

Micro-F1

ResNet152–224
ResNet50–128
ResNet18–128
ResNet18–224
Ensemble

16
128
128
200
128

37.45
52.04
51.40
54.93
60.56

62.38
64.07
63.90
66.70
70.60

60.46
63.85
63.88
66.44
70.35

38.26
52.27
51.62
53.93
60.38

61.71
64.35
63.60
66.30
70.54

60.09
63.89
63.44
65.94
70.37

with batch size 512 (65.54; see Table 2). These results provide a dif-
ferent perspective than the conclusions drawn by previous studies
(Masters and Luschi, 2018; Mishkin et al., 2016) where the use of small
or even mini-batches enhanced performances. This could be explained
by the high imbalance between classes and the fine-grained nature of
the classification task. Larger batches may therefore be more re-
presentative of the intra-class variability which in turn allows the net-
work to focus on inter-class variance. It will be asserted that our best
ResNet18 (ResNet18–224; 65.94 micro-F1, see Table 3) easily out-
performed deeper network architectures, whether trained from scratch
with a smaller batch size (ResNet50–128; micro-F1 63.89), or pre-
trained with fine-tuned weights (ResNet152–224; 60.09 micro-F1) ac-
cording to standard procedures (King et al., 2018). Our results support
the findings of a recent study which advocated the use of carefully

contextual information. If the patch size is too small, it contains in-
sufficient information and most probably fails to capture whole in-
dividuals; if it is too large, the context scrambles the signature of the
central information, confusing the algorithm. The patch size 64 × 64
pixels performed the worst out of all the metrics; the large differences
between validation and test performances was indicative of its poor
capacity for generalization. A patch size of 224 × 224 gave the highest
micro-F1 on the test set (65.94). This patch size included enough con-
textual noise to regularize overfitting, and it enabled better general-
ization. While the single RestNet18 based on the patch size 224 × 224
obtained the best accuracy, our ensemble network, which was based on
the four tested patch sizes and followed the feature extraction scheme
improved classification
of the local-SPP (Mahmood et al., 2016),

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The hyperparameters of the model are optimized using a grid search method. Specifically, the values used for the parameter C are {10^-4, 10^-3, 10^-2, 10^-1, 1, 10, 100}, while the solvers used are sag, saga, and lbfgs.

Reference(s):
[5]
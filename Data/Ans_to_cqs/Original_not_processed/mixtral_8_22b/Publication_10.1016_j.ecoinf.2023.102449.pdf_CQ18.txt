Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

used  and  the  learning  capacity,  complexity  and  architecture  of  the 
model.

and its relevance to ecological soundscape analysis will depend heavily 
on  the  distribution  of  the  training  data,  what  training  paradigm  was

network deepens (Ioffe and Szegedy, 2015; Prince, 2023). To mitigate 
internal covariate shift and stabilise training residual networks require 
some form  of layer  normalisation  such  as batch  normalisation.  While 
increasing  the  number  of  model  parameters,  batch  normalisation  en-
ables the use of higher learning rates, significantly speeding up training 
and  reducing  power  usage  (Ioffe  and  Szegedy,  2015).  Wide  ResNets 
increase network width and reduce depth (Zagoruyko and Komodakis, 
2017) allowing faster training due to shallower architectures. Our ar-
chitecture  draws on  the  layout  of  BirdNet’s  encoder,  preserving  their 
pre-processing block and dropping the classification layers, setting up a 
wide  residual  network  with  a  width  scaling  factor  K = 4  and  depth 
scaling factor N = 3 (Kahl et al., 2021). Hidden layer activations use a 
rectified linear unit (ReLU) activation function. Additional regularisa-

Group 

Encoder    
Pre-processing 

ResStack 1 

ResStack 2 

ResStack 3 

ResStack 3 

Temporal Framing 
Bottleneck     

Reparameterisation 

Decoder    
Temporal Framing 
ResStack 3 

ResStack 3 

ResStack 2 

ResStack 1 

Post-processing 

Operation 

Input Shape 

Output Shape 

5 × 5 Conv + BatchNorm + ReLU 
Max & Avg pooling +1 × 1 Conv 
Downsampling block 
2 × Residual Block 
Downsampling block 
2 × Residual Block 
Downsampling block 
2 × Residual Block 
Downsampling block 
2 × Residual Block 
Reshape 

Flatten 
Linear 
Sample 
Linear 
Unflatten 

Reshape 
Upsampling block 
2 × Residual Block 
Upsampling block 
2 × Residual Block 
Upsampling block 
2 × Residual Block 
Upsampling block 
2 × Residual Block 
1 × 1 Conv + BatchNorm + ReLU 
2 × 2 ConvTranspose + BatchNorm + ReLU 
5 × 5 Conv

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide explicit information about the strategy used to monitor the model performance during training. However, it mentions the use of batch normalization, which can be considered a method for monitoring and adjusting the model's performance during training. Batch normalization helps to stabilize the learning process by normalizing the input features of each mini-batch, effectively reducing internal covariate shift. This allows for the use of higher learning rates, leading to faster training times and reduced power consumption. Additionally, the wide residual network architecture mentioned in the text may also contribute to improved model performance and stability during training.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1 
General transfer training hyper-parameters for each basic model.  

Hyper-parameter / Model 

ResNet-50 

ViT-S/16 

Volo-d1 

ViP-Small/7 

Learning rate 
Minimum learning rate 
Optimizer 
Scheduler 
Batch size 
Weight decay 
Input size 
Epochs 

1e-4 
2e-3 
1e-5 
1e-5 
Adamw(0.9, 0.999) 
Cosine 
128 
5e-4 
224 × 224 
210  

64 
5e-4 

8e-6 
4e-6 

64 
1e-8 

2e-3 
1e-5 

64 
5e-2 

EcologicalInformatics82(2024)1026936M. Chen et al.                                                                                                                                                                                                                                   

Table 2 
Classification performance (%) of basic models and proposed methods on the 
IP102 dataset.  

which they are applicable. 

5.1. Ablation experiments 

Model 

Accuracy 

Precision 

Recall 

F1-score 

Resnet-50 
ViT-S/16 
Volo-d1 
ViP-Small/7 
VecEnsemble (Ours) 
MatEnsemble (Ours)

be  O

to 

)

(cid:0)

(cid:0)

)

Nnm2 + nm3

, respectively. Here, N represents the number of samples 
O
in the validation set, n is the number of classes, and m is the number of 
basic models.

4.1. Experimental setup and training strategy 

All experiments were conducted on a Linux machine equipped with 
Ubuntu  20.04,  utilizing  an  Intel(R)  Xeon(R)  CPU  E5–2650  v4  @ 
2.20GHz processor. All codes were implemented in Python 3.8, and all 
models were constructed using PyTorch. Each basic model was compact 
and  trained  on  a  single  TITAN  Xp  12G  GPU  for  evaluation.  Standard 
evaluation metrics such as accuracy, macro-precision, macro-recall, and 
macro-F1 score were employed to assess the performance of the models. 
Following  the  guidelines  provided  by  Wu  et  al.  (2019),  we  parti-
tioned  the  IP102  dataset  into  training,  validation,  and  test  sets  with 
sample sizes of 45,095, 7508, and 22,619, respectively. It is important to 
note that the parameters of the basic models were exclusively updated 
using  the  training  set,  whereas  the  weights  of  the  ensemble  methods 
were calculated on the validation set.

Yuan et al. (2022) claimed that the superior performance of a ViT can 
primarily be attributed to extensive pre-training data. If a ViT is trained 
from  scratch  on  a  medium-sized  dataset,  its  performance  lags  behind 
that of a CNN model with comparable parameter sizes. To address this 
limitation, they proposed Volo with the novel Outlook Attention as the 
token mixer module, which enabled fine-grained token representation 
and the aggregation of global information. 

2.3. MLP-like model

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The experimental setup and training strategy mentioned in the given context involve conducting experiments on a Linux machine equipped with Ubuntu 20.04, utilizing an Intel(R) Xeon(R) CPU E5–2650 v4 @ 2.20GHz processor. The codes were implemented in Python 3.8, and all models were constructed using PyTorch. Each basic model was compactly trained on a single TITAN Xp 12G GPU for evaluation. Standard evaluation metrics such as accuracy, macro-precision, macro-recall, and macro-F1 score were employed to assess the performance of the models. Following the guidelines provided by Wu et al. (2019), the IP102 dataset was partitioned into training, validation, and test sets with sample sizes of 45,095, 7508, and 22,619, respectively. Importantly, the parameters of the basic models were exclusively updated using the training set, while the weights of the ensemble methods were calculated on the validation set. This indicates that the validation set was used to monitor the model performance during training.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The  model  initially  included  all  variables,  acknowledging  the  sig-
nificance  of  identifying  and  selecting  pertinent  variables  for  model 
development. Variable importance measures were employed to evaluate 
their significance, which helped identify the most influential variables 
for  predicting  AGB.  However,  it  is  recognised  that  assessing  model 
performance is essential, particularly when dealing with less important 
variables. A stratified sampling method was applied to all AGB obser-
vations for this evaluation. This technique randomly allocated 70% of 
the samples to the training dataset and the remaining 30% to the vali-
dation  dataset.  Three  error  statistics,  specifically  R2,  mean  absolute 
error  (MAE),  and  RMSE  (as  indicated  in  Eqs.  8,  9,  and  10),  were 
employed to compare the accuracies of the models. The generated ma-
chine learning models were then extrapolated for the entire region. This

overfitting issue, we used the parameter (c) to optimise the line fit and 
penalise the number of samples inside the margin. The value of c plays a

The RF algorithm, which has several advantages, was used in this 
study.  It  has  been  noted  in  prior  research  that  RF  outperforms  other 
machine-learning methods, particularly when handling extensive data-
sets (Lourenço et al., 2021). RF provides accurate information regarding 
critical  factors  and  is  less  sensitive  to  parameter  adjustments  (Cutler 
et al., 2007; Zhou et al., 2016). This method uses bagging or boosting 
algorithms to randomly select predictors. Despite these advantages, RF 
has certain limitations. Using a large number of predictors can increase 
model  complexity.  It  also  requires  careful  tuning  and  hyper- 
parameterisation  such  as  “mtry”  and  “ntree”,  there  is  a  risk  of  over-
fitting  if  these  parameters  are  not  adjusted  properly.  This  study  sys-
tematically  evaluated  mtry  from  1  to  10,  aiming  to  strike  a  balance 
between  stability  and  minimise  the  Root  Mean  Square Error  (RMSE).

EcologicalInformatics79(2024)1024085R.K. Singh et al.                                                                                                                                                                                                                                 

Fig. 3. a) Contribution of species with respect to AGB, number of species and individuals, b) Field estimated aboveground biomass using 196 sample plots.  

crucial role in mitigating overfitting. Smaller c values lead the optimiser 
to  select  a  larger  margin,  potentially  misinterpreting  numerous  data 
points.  Conversely,  higher  c  values  result in  smaller  margins and  can 
cause overfitting. Thus, we tuned the c value within the range of 0.1 to 
20,  ensuring  a  balanced  trade-off  between  the  margin  size  and  over-
fitting (Table S2). 

3.5.3. Artificial neural network

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, one technique used to address data bias during preprocessing of the deep learning pipeline is stratified sampling. In this case, the authors used a stratified sampling method to allocate 70% of the samples to the training dataset and the remaining 30% to the validation dataset. This approach helps ensure that both datasets have similar distributions of the target variable, reducing potential bias caused by uneven distribution of classes or features. Other common techniques mentioned in the query but not explicitly discussed in the context include oversampling, undersampling, and diverse data collection. Oversampling involves duplicating examples from the minority class, while undersampling removes examples from the majority class to achieve a more balanced dataset. Diverse data collection aims to gather a wide variety of examples representing different scenarios, populations, or conditions to improve the generalizability of the trained model.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(2)  

∑

Λ15 =

Λ14

j × K14

j + b,

j

where b is a learned bias term. 

Fig. 4. Framework of the bird sound noise reduction network based on deep feature loss.  

EcologicalInformatics80(2024)1025174C. Zhang et al.                                                                                                                                                                                                                                   

2.3.2. Training of the bird sound noise reduction network based on deep 
feature loss

set 

is 

to 
1. 

2.3.3. Experimental setting 

In our experiment, a deep learning environment based on Python and 
Tensorflow – GPU as well as CUDA was constructed under the Windows 
operating system. The deep feature loss bird noise reduction network 
model was trained for 50 epochs on an RTX 2080 Ti GPU using the Adam 
(cid:0) 4. In each 
optimizer (Kingma and Ba, 2014) with a learning rate of 10
epoch, the entire training set was presented in a random order, with one 
noisy bird sound signal per iteration. 

2.4. Comparative algorithms 

Using the same test  set, we compared the  denoising results of the 
deep feature loss-based bird sound noise reduction network with several 
other  bioacoustics  noise  reduction  algorithms,  namely  SEGAN, 
WebRTC, wavelet transform (Priyadarshani et al., 2016), Wiener filter 
(Loizou, 2017), and MMSE STSA (Brown et al., 2017) algorithms.

reducing noise, and accordingly, would not be suitable for processing 
large  volumes  of  recorded  data  collected  in  the  field.  Furthermore, 
compared with the bird sound noise reduction network based on deep 
feature loss and the SEGAN algorithm, we found that the noise reduction 
efficiencies of the WebRTC and Wiener Filter algorithms were notably 
higher. This disparity in performance can be attributed to the fact that 
the deep learning model is larger in size and is thus more demanding of 
computational resources. However, it should be noted that the execution 
time of the deep learning models on CPUs is acceptable.

m=1

λm‖Φm(β) (cid:0) Φm(g(x; θ) ) ‖1,

(3)  

where Φm  represents the feature activation of the m-th feature layer in 
the 1st  to M-th shallowest network layers of the audio classification loss 
network, β represents the clean bird sound signal, g(x; θ) represents the 
processed bird sound signal generated by the noise reduction network 
during training, and θ is a parameter of the noise reduction network. The 
initial value of the weight λm  is set to 1, whereas after 10 training iter-
ations, 
relative  value 
it 
‖Φm(β) (cid:0) Φm(g(x; θ) ) ‖

reciprocal  of 

the 

the 

set 

is 

to 
1. 

2.3.3. Experimental setting

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper uses the Adam optimizer with a learning rate of 10^-4 for training the deep feature loss bird noise reduction network model. The model is trained for 50 epochs on an RTX 2080 Ti GPU. During each epoch, the entire training set is presented in a random order, with one noisy bird sound signal per iteration.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

representing single-specimen photographs by local taxonomists, are
used to predict local high-resolution images (LL!LH).

Between-datasets classification with domain adversarial
training

In addition to the standard CNN setups described above, we employed

large reduction of accuracy, hence difficulty in transfer.

the domain adversarial training of neural networks (DANN, Ganin

Divergence between the source and target datasets was mea-

et al., 2016) which incorporates a certain portion of the unknown tar-

sured with a dataset classification error. A linear support vector

gets in the model. The DANN model jointly predicts the class (family

machine (SVM) was trained to classify images to the source or target

label) of the source images and the dataset (domain) of all input images

dataset with the features of 200 randomly selected images from both

(as in the previous section) by adding layers for the dataset classifica-

and TensorFlow 2.5.0 (https://www.tensorflow.org) libraries, and all

statistical analyses were conducted with R 4.1.0 (R Core Team, 2021).

Between-datasets classification

For the between-dataset prediction, the CNN model was trained with

RE SU LT S

a source dataset to predict images from a different target dataset. The

NN was trained with N images randomly selected from the source

Performance of within-dataset classification

dataset, which was then used to predict all images of the target data-
set and AccT and ΔAcc were measured. We ran the above procedures
for three source–target pairs (training dataset!predicted dataset),
GH!LH, GH!LL and LL!LH. These settings simulate two alternative
scenarios: (i) a global image database is used to predict local samples
(GH!LH and GH!LL) and (ii) conventional

images, as

those

Effects of datasets and the number of images

The accuracy of within-dataset classification and the effect of the

LeCun, Y., Bengio, Y. & Hinton, G. (2015) Deep learning. Nature, 521(7553),
436–444. Available from: https://doi.org/10.1038/nature14539
Mukhoti, J., Kulharia, V., Sanyal, A., Golodetz, S., Torr, P.H.S. & Dokania, P.
K. (2020) Calibrating deep neural networks using focal loss. Advances
in Neural Information Processing Systems, 33, 15288–15299. Available
from:
https://proceedings.neurips.cc/paper/2020/hash/aeb7b30ef
1d024a76f21a1d40e30c302-Abstract.html

Noguerales, V., Meramveliotakis, E., Castro-Insua, A., Andújar, C.,
Arribas, P., Creedy, T.J. et al.
(2021) Community metabarcoding
reveals the relative role of environmental filtering and spatial pro-
cesses in metacommunity dynamics of soil microarthropods across a
mosaic of montane forests. Molecular Ecology in press. Available
from: https://doi.org/10.1111/mec.16275

F I G U R E 3

The effect of increasing numbers of training images on

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there is no explicit mention of specific data augmentation techniques such as flipping, rotation, scaling, etc., being applied in the given context. Therefore, it cannot be determined what data augmentation techniques have been implemented in this particular case. However, these techniques are commonly used in deep learning pipelines to improve model performance and prevent overfitting.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

accuracy  limits  that  we  encountered  training  DNN  models  with  the 
detailed-labelled NIPS4Bplus dataset (Bravo Sanchez et al., 2021). We 
drilled down on the results using another high-quality model (BirdNET) 
and  compared  it  to  our  SincNet  trained  models.  For  that,  we  experi-
mented  with  dimensionality  reduction  of  embeddings  and  an  initial 
dimensionality reduction technique (t-SNE). We generated embeddings 
using  the  BirdNET  model  on  a  randomly  selected  40%  sample  of  the

2.3. Models 

To  generate  embeddings  we  used  two  publicly  available  models: 
VGGish (Hershey et al., 2017) and BirdNET (Kahl et al., 2021); as well as 
models that we trained using the datasets described above. 

VGGish is a 128-dimensional embeddings model originally trained 
on  AudioSet,  an  event  dataset  with  >2  million  10  s  video  clips  from 
YouTube  (Gemmeke  et  al.,  2017).  The  dataset  has  527  classes  domi-
nated by music and human speech, and although it includes some bio-
acoustic events such as “bird”, “whale song”, “fowl” or “wild animals”, it 
is not directly comparable to the bioacoustic tasks addressed here that 
try to differentiate species. We used the TensorFlow Hub implementa-
tion (TensorFlow Hub, 2023) that relies on an enhanced dataset called 
YouTube-8  M  (Abu-El-Haija  et  al.,  2016)  with  3826  classes,  178  of 
which are within the top level class named “Pets & Animals”.

Neural information processing scaled for bioacoustics-from neurons to Big Data. In: 
Proceedings of Neural Information Processing Scaled for Bioacoustics: From Neurons 
to Big Data, 2013. http://sabiod.univ-tln.fr/NIPS4B2013_book.pdf. 

Gupta, Gaurav, Kshirsagar, Meghana, Zhong, Ming, Gholami, Shahrzad, Ferres, Juan 

Lavista, 2021. Comparing recurrent convolutional neural networks for large scale 
bird species classification. Sci. Rep. 11 (1), 17085. https://doi.org/10.1038/s41598- 
021-96446-w. 

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, Sun, Jian, 2015. ‘Deep residual learning 

for image recognition’. arXiv. http://arxiv.org/abs/1512.03385. 

Hershey, Shawn, Chaudhuri, Sourish, Ellis, Daniel P.W., Gemmeke, Jort F., Jansen, Aren, 
Moore, Channing, Plakal, Manoj, et al., 2017. CNN Architectures for Large-Scale 
Audio Classification. In: International Conference on Acoustics, Speech and Signal 
Processing (ICASSP).. https://arxiv.org/abs/1609.09430.

outputs.  The  embeddings  obtained  using  the  BirdNET-Analyzer  v2.1 
models have 420 dimensions.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide any direct reference to the data repository link of the deep learning pipeline. However, based on the mentioned models and their sources, I can infer that the VGGish model was implemented using TensorFlow Hub, which provides various pretrained models including those for audio analysis. Unfortunately, there isn't enough information to determine if this specific implementation is hosted on a public repository like Zenodo, Figshare, Dryad, or GBIF. Similarly, the BirdNET model is referenced but no explicit link to its source code or data repository is provided. Therefore, without additional context, I cannot provide a definitive answer regarding the data repository links for these deep learning pipelines.
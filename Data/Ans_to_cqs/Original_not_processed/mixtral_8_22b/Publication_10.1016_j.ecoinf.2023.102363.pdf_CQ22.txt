Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 3 
Mean performance metrics for each experimental batch size, with standard error, for the entire test set.  

Batch size 

Sound source 

Precision 

Recall 

F1 Score 

Mean AUC (Roc curve) 

Base Model (BM) 

50 Frames 

100 Frames 

200 Frames 

300 Frames 

500 Frames 

Ambient Only 

Ambient 
Clicks 
Delphinid 
Vessel 
Ambient 
Clicks 
Delphinid 
Vessel 
Ambient 
Clicks 
Delphinid 
Vessel 
Ambient 
Clicks 
Delphinid 
Vessel 
Ambient 
Clicks 
Delphinid 
Vessel 
Ambient 
Clicks 
Delphinid 
Vessel 
Ambient 
Clicks 
Delphinid 
Vessel 

0.49 
0.65 
0.55 
0.73 
0.89 
0.58 
0.59 
0.78 
0.88 
0.48 
0.35 
0.85 
0.89 
0.54 
0.23 
0.83 
0.89 
0.53 
0.34 
0.86 
0.91 
0.66 
0.77 
0.79 
0.90 
0.50 
0.60 
0.90 

– 
– 
– 
– 
±0.00 
±0.01 
±0.02 
±0.05 
±0.01 
±0.04 
±0.06 
±0.02 
±0.00 
±0.02 
±0.07 
±0.03 
±0.00 
±0.04 
±0.10 
±0.00 
±0.01 
±0.05 
±0.03 
±0.03 
– 
– 
– 
–

The model is constructed using the EfficientNet B0 network (Tan and 
Le, 2019) which had been trained for generic image classification. The 
EfficientNet feature extraction layers are frozen (transfer learning) with 
only the weights of the final dense classification layers updated during 
training.  Training  was  conducted  within  the  Google  Collaboratory 
‘Colab’  platform  (Bisong,  2019),  using  the  Tesla  K80  GPU,  accessed 
through  cloud  computing.  An  Adam  optimizer  was  used  to  control 
gradient descent during training (Kingma and Ba, 2014), with parame-
ters set to: learning rate of 0.001, decay factor of 0.75 and a step size of 

EcologicalInformatics78(2023)1023632E.L. White et al.

C:  30th  December  2019  and  Day  D:  4th  January  2020,  Table  2.  The 
minimum time between test datasets is 4 days. Each 24-h period of PAM 
data equates to 28,800 frames, a total of 115,200 frames for the four 
days (Table 2). In this dataset the delphinid tonal class is least repre-
sented, accounting for only 0.6% of the frames. 

2.4.1. Model evaluation 

Each fine-tuned model’s performance was evaluated using Precision 
(P),  Recall  (R)  and  Accuracy  (A),  (Mesaros  et  al.,  2016).  These  are 
calculated based on the number of true and false positive detections, NTP 
and NFP, along with the number of true and false negative detections, 
NTN and NTP: 

P =

NTP
NTP + NFP

R =

NTP
NTP + NFN

,

A =

NTP + NTN
NTP + NFP

.

(1)  

(2)  

(3) 

The F1 score, the harmonic mean of the precision and recall, is also

2.2. Model fine-tuning 

To  fine-tune  the  base  model  with  Gulf  of  Mexico  data  the  feature 
extractor remains frozen. Models are trained with the same parameters 
as the base model but we use a cyclical learning rate of 0.0004 and is set 
to run for 50 epochs, with early stopping set to deploy if the validation 
loss does not improve within 10 epochs. A dropout rate of 0.2 is used 
during  fine-tuning,  and  DropConnect  is  employed.  Drop  out  layers 
randomly  discard  the  output  of  the  hidden  nodes  during  training, 
DropConnect randomly discards the input of the hidden layer (Sun et al., 
2022). 

2.2.1. Data acquisition

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The performance of the deep learning model is evaluated using three main metrics: Precision (P), Recall (R), and Accuracy (A). Precision measures the proportion of correctly identified positive instances among all instances predicted as positive. Recall calculates the proportion of correctly identified positive instances among all actual positive instances. Accuracy represents the ratio of correct predictions over the total number of instances. Additionally, the F1 score, which is the harmonic mean of precision and recall, is also used to assess the model's performance.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1. Number of insect species in the dataset used for training and validation.

No.

Species

Numbers

1
2
3
4
5
6
7
8
9

Agrotis puta
Amphipyra pyramidea
Autographa gamma
Hoplodrina complex
Mythimna pallens
Noctua ﬁmbriata
Noctua pronuba
Xestia c-nigrum
Vespula vulgaris

Total

250
250
250
250
250
250
250
250
250

2250

Author Contributions: Conceptualization, T.T.H.; methodology, K.B.; software, J.B.N. and M.V.S.;
validation, K.B., F.H.-N. and T.T.H.; investigation, K.B., J.B.N. and M.V.S.; resources, K.B., J.B.N.
and M.V.S.; data curation, K.B., J.B.N., M.V.S. and F.H.-N.; writing—original draft preparation, K.B.;
writing—review and editing, K.B. and T.T.H.; visualization, K.B. and J.B.N.; supervision, K.B. and
T.T.H.; project administration, K.B. and T.T.H.; funding acquisition, T.T.H. All authors have read and
agreed to the published version of the manuscript.

Funding: This research was funded by Danish 15. Juni Fonden grant number 2019-N-23.

Data Availability Statement: The train and validation data is available online at https://github.
com/kimbjerge/MCC-trap.

Acknowledgments: The authors like to acknowledge Freia Martensen for language and proof
reading the article and Mads Dyrmann for inputs and review.

Sensors 2021, 21, 343

11 of 18

7,047,754 learnable parameters gave a F1-score of 84.93% which is even lower. CNN
architectures with many parameters (more than 20,000,000) such as ResNetV50 [38] and
InceptionNetV3 [39] gave a high training accuracy, but a lower validation F1-score of 69.1%
and 81.7%, respectively. This result indicates overﬁtting and that more training data are
needed when such large deep learning networks are used. A very high F1-score of 96.6%
was ﬁnally achieved by transfer learning on ResNetV50 using pretrained weights and only
training the output layers. This indicates that the state-of-the-art was able to outperform
our proposed model, but requires pretrained weights with many more parameters.

2.2.4. Summary Statistics

5, 3, 64, 128, 512
5, 3, 32, 128, 512

5, 1, 64, 64, 512
3, 3, 64, 64, 256
3, 3, 64, 64, 512

4,330,122
4,266,122
2,197,578
2,196,042
4,331,658

2,185,674
2,164,810

4,352,522
4,331,658

2,185,674
1,163,978
2,215,370

0.942
0.929
0.928
0.923
0.922

0.871
0.871

0.853
0.853

0.749
0.737
0.682

The confusion matrix (Figure 7) was based upon the validation of the chosen model.
The confusion matrix has a diagonal trend, which indicates that the model matched
the validation set well. The model had a recall of 93%, indicating that only 7% of the
moth species in the validation set were missed. A similar precision of 93% was obtained,
indicating that only 7% were wrongly classiﬁed.

Figure 7. Confusion matrix for the validation of the best model. The numbers corresponds to the
species numbers in Table 1.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, it does not explicitly mention the specific criteria used to determine when training is completed. However, there are some indications that can be inferred from the text. For instance, the paper mentions that the chosen model was validated, suggesting that the training process might have been stopped once the desired level of performance was reached during validation. Additionally, the use of a confusion matrix for validation implies that the model's performance was evaluated against unseen data, which could also serve as a stopping criterion if the model performs satisfactorily. Furthermore, the discussion about overfitting suggests that monitoring the validation loss or other metrics related to generalization might have played a role in deciding when to stop training. Nevertheless, without explicit details regarding the stopping criteria, one cannot definitively state what criteria were used to determine when training was complete.
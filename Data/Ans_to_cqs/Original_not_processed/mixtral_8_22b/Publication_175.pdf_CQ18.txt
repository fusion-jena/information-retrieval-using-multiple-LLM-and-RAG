Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

a new task with a very limited dataset, usually only 1–5 thumbnails per 
class.

There are two ways to tackle this problem of lack of data. The first 
one  consists  of  directly  addressing  the  data  itself,  through  data 
augmentation (Van Dyk and Meng, 2012; Wang and Perez, 2017; Wong 
et al., 2016). The second option is to change the classification algorithm. 
Few-shots learning (FSL) algorithms (Fei-fei et al., 2006; Fink, 2005) are 
designed to compute a classification task (query, noted Q) with only a 
few  thumbnails  to  train  (Support  Sets,  noted  SS),  and  it  has  been 
increasingly studied since 2017 (Finn et al., 2017). Few-shots learning 
methods are divided into three main approaches. Metric-based methods 
are embedding both queries (Q) and support sets (Ss), before assigning to 
the query a class, according to distances computed between Q and Ss 
(Sung et al., 2018; Victor and Bruna, 2018; Yanbin et al., 2019). The 
second approach consists of 1) training a model on a large database, and

1)  We  trained  a  classic  DL  algorithms  architecture  with  our  biggest 

dataset AT0 as a baseline for the DL accuracy;  

2)  We  trained  the  same  DL  architecture  with  the  same  hyper- 
parameters (e.g. model architecture and training process) but on a 
much  more  limited  dataset  (AT1).  Hyper-parameters  are  the  pa-
rameters  defining  the  architecture (number of  layers, number and 
size of convolutions, connections between layers) and the training 
process  of  a  Deep  Model  (learning  rate,  neurone  activation,  back- 
propagation compotation).;  

3)  We trained the same DL architecture with limited datasets obtained 
by  subsampling  T0  to  250  and  500  images  per  class  (here  after 
“species” when we are referring to our experiments), corresponding 
to 2500 and 5000 thumbnails in AT0;

3. Results 

The  deep  ResNet  model  trained  on  the  large  AT0  dataset  (3458 
natural thumbnails in average per species) during the first experiment 
obtained a mean accuracy (i.e. percentage of correct classification) of 
78.00%  (standard  deviation  (SD)  of  15.16%)  on  T2  test-dataset 
(Table  2).  With  this  model,  accuracy  varied  among  species  between 
54.14% (Naso brevirostris) and 99.07% (Abudefduf vagiensis). The same 

EcologicalInformatics63(2021)1013203S. Villon et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there isn't any explicit mention of a specific strategy used to monitor the model performance during training. However, some insights can be derived from the described methodology.

Firstly, the authors mentioned using a classic DL algorithm architecture with their largest dataset, AT0, as a baseline for DL accuracy. This suggests they might have monitored the performance of the model by comparing its results against this established benchmark.

Secondly, the authors also conducted experiments where they trained the same DL architecture with smaller datasets, such as AT1, and subsets of AT0 containing 250 or 500 images per class. By comparing the performance of these models with the baseline, they could assess how well the model was performing under different conditions.

Lastly, the authors reported the mean accuracy and standard deviation of their models on the T2 test-dataset. This indicates that they evaluated the performance of their models post-training, which could serve as a form of monitoring during the development phase.

However, without further details about the actual implementation, it cannot be definitively stated what specific strategy was employed to monitor the model performance during training.
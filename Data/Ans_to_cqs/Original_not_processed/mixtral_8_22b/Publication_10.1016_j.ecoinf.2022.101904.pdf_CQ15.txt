Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The objective function and expected function are denoted by U(D, G) 
and E. D and G presents the discriminator and generator networks. D(x) 
calculates the likelihood that x is the true data (i.e., real data) that is 
based  on  training  reference  samples.  In  the  3D  GAN  network,  we 
employed a conditional map unit to produce synthetic samples from a 
random  noise  vector,  like  the  Generative  Adversarial  Minority  Over-
sampling (GAMO) (Subhra Mullick et al., 2019) and 3D-HyperGAMO (S. 
K.  Roy  et  al.,  2021),  only  for  classes  with  a  low  number  of  training 
samples.  The  benefit  of  such  a  methodology  is  that  it  eliminates  the 
problem of imbalanced data, which is typical in wetland mapping. In 
particular, in the 3D GAN model, the 3D patch generator uses seven (c (cid:0)
1, c presents the number of classes) units, one unit for each of the classes 
g samples (see 
with minor ground-truth data. Thus, the unit Ui generates γi
Eq. 2). 
γg

Contents lists available at ScienceDirect 

Ecological Informatics 

journal homepage: www.elsevier.com/locate/ecolinf 

3DUNetGSFormer: A deep learning pipeline for complex wetland mapping 
using generative adversarial networks and Swin transformer

EcologicalInformatics72(2022)1019047A. Jamali et al.                                                                                                                                                                                                                                  

Table 5 
Results  of  the  proposed  deep  model  (ViT  = Vision  Transformer,  ST  = Swin 
Transformer, KI=Kappa index, AA = Average accuracy, OA = Overall accuracy).  

Class 

ViT 

ST 

CoAtNet 

CNN 
+ ST 
(ours) 

GAN 
+ ST 
(ours) 

3DUNetGSFormer 
(ours) 

Bog 
Fen 
Marsh 
Swamp 
Shallow 
water 

Urban 
Deep 

water 
Upland 
KI (%) 
OA (%) 
AA (%) 
Time (h) 

0.59 
0 
0.46 
0 
0.83 

0.97 
0.93 

0.86 
0.79 
0.88 
0.76 
0.92 

0.99 
0.97 

0.91 
0.86 
0.94 
0.82 
0.98 

0.99 
1 

0.87 
71.31 
75.62 
62.96 
2.2 

0.96 
90.66 
91.99 
88.67 
1.5 

0.97 
94.67 
95.43 
93.21 
5 

0.82 
0.76 
0.95 
0.89 
0.91 

0.98 
1 

0.98 
92.84 
93.87 
90.16 
1.5 

0.89 
0.83 
0.88 
0.9 
0.96 

0.99 
0.99

EcologicalInformatics72(2022)1019043A. Jamali et al.                                                                                                                                                                                                                                  

Fig. 2. The architecture of the proposed U-Net discriminator of the 3D GAN network.  

Fig. 3. The architecture of the feature extractor CNN network.  

Fig. 4. The architecture of the Swin Transformer classifier.  

truth data acquired from the three pilot sites were combined, and then a 
stratified  random  sampling  strategy  was  employed  to  partition  the 
reference data into 10% as training and 90% as testing samples in the 3D 
GAN model, and 70% as training and 30% as testing samples in the Swin 
Transformer classifier. 

To fully utilize the advantages of multi-source remote sensing data,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't any explicit information about the specific optimization techniques used in the deep learning pipeline mentioned in the given context. However, based on common practices in deep learning models, popular optimization algorithms include Stochastic Gradient Descent (SGD), Root Mean Square Propagation (RMSProp), Adaptive Moment Estimation (Adam), or their variants. These methods help minimize the loss function during the training process. Without further details, I cannot provide a definitive answer regarding the optimization technique used in this particular case.
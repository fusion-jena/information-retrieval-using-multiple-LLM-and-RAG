Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The relational schema of CASE’s database is shown in Fig. A.18. CASE is based on a concept of Task, i.e., a questionnaire that must be fulfilled by a 
User (i.e., an on-field operator). Each task is composed by a set of Questions, categorized in different types (e.g., multiple answers, date picker) each 
composed by a set of predefined Answers (one of which could be an open answer, defined by the user when selecting the Given answer). The 
dynamic aspect of CASE’s questionnaires lies in the association of each possible answer with a different one to be shown next. Thus, users fulfilling the 
same task may follow different paths of questions depending on the given answers. This enables a customized and efficient user experience, as detailed 
questions on a given topic may be asked only if such a topic has been mentioned by the user. Instances of tasks (i.e., Task on geo-object) are

Fig. 5. Overview of the data platform.  

EcologicalInformatics82(2024)1027137(a)(b)(c)C. Forresi et al.                                                                                                                                                                                                                                  

Table 1 
Summary of data sources.  

Source 

Monitoring network 
Satellite images 
Weather 
Environment 

Provider 

CASE 
ESA 
ARPAE 
CER 

Frequency 

Weekly 
2–3 days 
Daily 
Yearly 

Granularity 

Monitoring task 
100 km2 granule 
25 km2 cell 
Environmental element 

Yearly size 

5.2 MB 
370 GB 
6.5 GB 
4.3 GB  

The validated data is then loaded to the Access tier through an in-
cremental  ETL (extract, transform, and load) procedure acquiring the 
validated data from the Harbor tier. 

3.2.4. Analytical fruition

The results of these integration and enrichment activities are mate-
rialized in the Harbor tier of the data platform, which provides an in-
tegrated and comprehensive view of the available data at the finest level 
of detail. The relational schema of the integrated data is discussed in 
Appendix A.2. 

3.2.3. Validation and loading of enriched H. halys captures

3.2.1. Data sources 

A  summary  of  the  many  data  sources  is  shown  in  Table  1,  which 
reports the frequency of updates and space occupation for each source. 
The monitoring network (managed by CASE, introduced in Section 
3.1.2) is the main source, providing the number of H. halys captures on a 
weekly basis. For each deployed trap, we collected the coordinates of 
deployment and a list of environmental elements visually identified in 
the surroundings by the technician that deployed the trap (e.g., tree fruit 
crops, herbaceous crops, buildings). Then, for each deployed trap and 
week within the monitoring session, we collected the number of H. halys 
captured (divided into small instars, large instars, and adults), together 
with an indication of the correct functioning of the trap.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the provided context, there isn't any direct information about the specific preprocessing steps used before training a deep learning model. However, common preprocessing steps often involve normalization, scaling, and cleaning of the dataset. Normalization can help bring all features to the same scale, while scaling can ensure that extreme values do not skew the model. Cleaning involves removing or handling missing or inconsistent data points. These steps are crucial to improve the performance and accuracy of machine learning models, including deep learning ones.
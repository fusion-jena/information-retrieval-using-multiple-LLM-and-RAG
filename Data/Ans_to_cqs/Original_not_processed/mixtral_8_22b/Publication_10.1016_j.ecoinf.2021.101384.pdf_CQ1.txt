Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

workflow  can  compensate  for  some  of  these  biases  –  through  data 
classification  and  spatio-temporal  aggregation  –  mainly  when  large 
input datasets are used. Generally, it is worth noting that all big data 
processing methods are approximate, but they can discover general and 
valuable knowledge if the approximation is tolerated within the appli-
cation context (Coro, 2020b).

Our workflow was implemented on DataMiner as two different WPS 
Web  services  (Supplementary  material),  namely  Fishing  Activity  and 
Pressure from VTI data (FAP-VTI) and Fishing Activity and Pressure from 
Global Fishing Watch data (FAP-GFW). The FAP-VTI service accepts user- 
provided vessel tracking data in CSV format, and focuses the analysis on 
the implicit time frame and spatial extent of these data. Instead, the FAP- 
GFW  service  embeds  all  daily  vessel  tracking  data  at  0.1
resolution, 
between 2012 and 2016, from the Global Fishing Watch. In this free-to- 
use  dataset,  fishing  activity  is  pre-classified  using  a  machine-learning 
model,  which  allows  skipping  our  fishing  activity  classifier  (Section 
1.1). The FAP-GFW service asks the user to simply draw a bounding box 
on an ocean area, and to specify the analysis period, but is limited to the 
2012–2016 time frame. The other input parameters and the produced

At the time of writing, global-scale GFW data were downloadable for 
scientific purposes for the 2012–2016 period only. GFW distributes 
◦
vessel  data  aggregated  at  0.01
resolutions  with  fishing 
activity cells already classified through a machine learning model.

2.4. Open Science methodology and tools 

Our  workflow  implements  a  FAIR  approach  that  tests  FAIR  data 
principles’  practicability.  It  is  open-source  (Supplementary  material) 
and was integrated with the DataMiner Cloud computing platform of the 
D4Science e-Infrastructure (Coro et al., 2017), which allows accessing 
the mentioned knowledge sources on-the-fly during processing (Candela 
et al., 2016; Coro et al., 2015a). Data FAIRness is facilitated through the 
indexing of these resources in the D4Science catalogue (Assante et al., 
2019b), which can be accessed by all processes via the Catalogue Services 
for the Web (CSW) standard of the Open Geospatial Consortium (OGC, 
2020). Geospatial data are offered as standarised NetCDF files available 
on  a  distributed  ISO/OGC  compliant  Spatial  Data  Infrastructure

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific methods used for collecting raw data in the deep learning pipeline. However, it mentions that the FAP-VTI service accepts user-provided vessel tracking data in CSV format, suggesting that one possible source of raw data could be user-generated or collected data. Additionally, the FAP-GFW service uses daily vessel tracking data between 2012 and 2016 from the Global Fishing Watch, indicating that another potential source of raw data could be publicly available datasets like those provided by organizations such as Global Fishing Watch. Without further context, it cannot be definitively stated what specific methods are being employed for collecting raw data in the deep learning pipeline.
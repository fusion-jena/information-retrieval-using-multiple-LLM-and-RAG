Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Measurement of Performance
To get a sense of how our approaches will perform on new unseen
data, and also to keep a track of if any of our approaches are
overﬁtting, we run all our experiments across a whole range
of train-test set splits, namely 80–20 (80% of the whole dataset
used for training, and 20% for testing), 60–40 (60% of the whole
dataset used for training, and 40% for testing), 50–50 (50% of

the whole dataset used for training, and 50% for testing), 40–60
(40% of the whole dataset used for training, and 60% for testing)
and ﬁnally 20–80 (20% of the whole dataset used for training,
and 80% for testing). It must be noted that in many cases, the
PlantVillage dataset has multiple images of the same leaf (taken
from diﬀerent orientations), and we have the mappings of such
cases for 41,112 images out of the 54,306 images; and during all

Frontiers in Plant Science | www.frontiersin.org

3

September 2016 | Volume 7 | Article 1419

Mohanty et al.

trained using transfer

60–40, we will use

Each of these 60 experiments runs for a total of 30 epochs,
where one epoch is deﬁned as the number of training iterations
in which the particular neural network has completed a full pass
of the whole training set. The choice of 30 epochs was made based
on the empirical observation that in all of these experiments, the
learning always converged well within 30 epochs (as is evident
from the aggregated plots (Figure 3) across all the experiments).
To enable a fair comparison between the results of all the
experimental conﬁgurations, we also tried to standardize the
hyper-parameters across all the experiments, and we used the
following hyper-parameters in all of the experiments:
• Solver type: Stochastic Gradient Descent,
• Base learning rate: 0.005,

• Learning rate policy: Step (decreases by a factor of 10 every

However, there are a number of limitations at the current stage
that need to be addressed in future work. First, when tested on a
set of images taken under conditions diﬀerent from the images
used for training, the model’s accuracy is reduced substantially,
to just above 31%. It’s important to note that this accuracy is
much higher than the one based on random selection of 38 classes
(2.6%), but nevertheless, a more diverse set of training data is
needed to improve the accuracy. Our current results indicate
that more (and more variable) data alone will be suﬃcient
to substantially increase the accuracy, and corresponding data
collection eﬀorts are underway.

Frontiers in Plant Science | www.frontiersin.org

8

September 2016 | Volume 7 | Article 1419

Mohanty et al.

Deep Learning for Plant Diseases

Finally,

At the same time, by using 38 classes that contain both
crop species and disease status, we have made the challenge
harder than ultimately necessary from a practical perspective,
as growers are expected to know which crops they are growing.
Given the very high accuracy on the PlantVillage dataset, limiting
the classiﬁcation challenge to the disease status won’t have
a measurable eﬀect. However, on the real world datasets, we
can measure noticeable improvements in accuracy. Overall,
the presented approach works reasonably well with many
diﬀerent crop species and diseases, and is expected to improve
considerably with more training data.
it’s worth noting that

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors do not explicitly state the specific criteria used to determine when training is complete. However, it can be inferred from the given information that the training process continues until convergence or completion of a predetermined number of epochs. In this case, each experiment ran for a total of 30 epochs, where one epoch is defined as the number of training iterations required for the neural network to fully pass through the entire training set once. Additionally, the authors mention that the learning always converges well within 30 epochs, suggesting that convergence may serve as a criterion for determining when training should stop.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

during the training phase for each evaluated input size, the best image resolution. Then
the DDCN processes the testing images using batches composed of the images with the
best-evaluated size (Figure 8).

Author Contributions: Methodology, W.N.G., J.M.J., P.T.S.d.O., L.P.O. and J.A.C.M.; software, J.A.d.S.
and K.N.; validation, W.N.G., J.M.J., J.A.d.S., K.N., A.P.M.R. and V.L.; formal analysis, V.L., K.N. and
J.M.J.; investigation, L.P.O. and J.A.C.M.; resources, V.L. and J.M.J.; data curation, W.N.G., J.M.J. and
J.A.C.M.; writing—original draft preparation, J.A.C.M. and L.P.O.; writing—review and editing, V.L.,
A.P.M.R., F.D.G.G., D.A.S., D.E.G.F., K.N. and J.M.J.; visualization, J.A.C.M. and K.N.; supervision,
W.N.G., V.L. and J.M.J.; project administration, W.N.G. and J.M.J.; funding acquisition, J.M.J., V.L.
and W.N.G. All authors have read and agreed to the published version of the manuscript.

3.2. Computational Complexity

This section compares the methods in terms of computational efﬁciency and computa-
tional load for training and inference. Table 2 presents the average training and inference
times measured on the hardware infrastructure described in Section 2.2. Considering that
the methods were trained with the same optimizer and learning rates, these results are
highly correlated with the network depth and the selected batch size. For instance, the
DDCN network is deeper than the others, and the consequence is that it took longer than
the other networks for training and inference.

Finally, we present in Figure 11 the accuracy and loss curves. We can see that the
FCN, UNet, and SegNet performed similarly with stable slight variation and close to
the minimum loss value after 100,000 iterations or half the way of the learning process
established. The DCNN presented some increase in loss after 100,000 iterations. After the
learning rate was reduced in the lasts 50,000 iterations by the SGD scheduler, it reached
its minimum value, and the loss ﬂuctuations were reduced. Moreover, the DeepLabV3+
became stable after 100,000 iterations with the minimum loss observed.

3.2. Computational Complexity

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly state the specific criteria used to determine when training is complete. However, based on the information given about the accuracy and loss curves, it appears that the models were trained until they reached a minimum loss value or became stable. This suggests that one possible criterion used could be reaching a plateau in validation loss. Additionally, the mention of reducing the learning rate in the last 50,000 iterations implies that another potential criterion might be a predetermined number of iterations or epochs. Without further clarification from the original source, this is only speculation.
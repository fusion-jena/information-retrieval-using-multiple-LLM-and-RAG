Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

the  experts.  Then  the  network  is  fed  this  annotated  data  and  the  pa-
rameters of the model are adjusted following an optimization process. 
Once this training process is finished, the model is ready to process new 
unseen  data.  DL  networks  are,  thus,  able  to  capture  statistics  and  re-
lationships directly from the data following human expertise. If the data 
used  for  training  the  model  is  representative  of  the  problem  and  the 
annotations are carefully generated, the resulting network can perform 
at a similar level with any input data set. This capacity to translate the 
training to other independent datasets is called the generalization power 
of the network. In practice, differences in performance between the re-
sults with the training data (or data that is similar to it, i.e., from the 
same  site  and  acquired  at  the  same  time)  and  those  of  independent 
datasets (often in research papers referred to as the testing data set) can

In order to solve this problem, we used a DL approach. DL networks 
can be seen as non-linear functions with a large number of parameters. 
The parameters of each DL are adjusted using annotated data (usually 
referred  to  as  a  “supervised  approach”).  Once  this  process  (called 
training the network) is finished, the evaluation of the network on the real 
problem  is  performed  by  feeding  it  an  independent  data  set  not  seen 
during training (the testing data set). DL techniques are currently a hot 
topic in machine learning with applications in a variety of fields, and are 
considered the current state of the art for CV applications. In fact, in 
some disciplines, this technology is considered to have exceeded human 
capabilities (Buetti-Dinh et al., 2019; He et al., 2015).

3.3. Automatic image processing using DL 

In this section we described the methodology we used to integrate DL 
models into our study. As this is the core technology of our automatic 
framework, we also provide detailed explanations on the metrics used to 
evaluate the results of the whole study. 

3.3.1.

Image classification using deep learning

3.1.4. Fieldwork 

Deep learning models are usually defined as “data hungry” because 
their accuracy is heavily dependent on the amount of high quality an-
notated data used to learn the correspondence between the desired la-
bels  and  image  information.  Thus,  to  develop  a  model  capable  of 
matching human expertise on sub-alpine shrub species identification, a 
large and accurate dataset with images and their corresponding labels is 
crucial. As we will describe in detail in later sections, determining the 
exact species of every shrub from images is a difficult task for human 
experts. For our study, fieldwork expeditions were key to confirm each 
of the selected shrub images and validate the orthomosaic annotations. 
Three  fieldwork  missions  were  conducted  during  2021  (June,  August 
and October), to account for seasonal changes in color and flowering 
patterns that helped identify each individual species. 

3.2. Observer study

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not explicitly mention the specific process followed to deploy the trained deep learning model, such as model serialization or platform selection. However, based on common practices in deep learning projects, after training the model, it would typically be saved or serialized so that it can be loaded and used for predictions. The deployment platform could vary depending on the project requirements, but popular choices include cloud platforms like AWS, Google Cloud, or Microsoft Azure, or local servers. Additionally, the model might need further optimizations for production use, such as quantization or pruning, which are not mentioned in the given context.
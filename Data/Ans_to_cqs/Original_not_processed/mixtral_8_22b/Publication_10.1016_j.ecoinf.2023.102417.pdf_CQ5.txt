Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

function to ignore a certain class. These annotations are made manually, 
using special software, and consume large amounts of time. For model 
development  there  are  many  well  established  collections  of  image 
datasets for the training and evaluation of the models, such as ImageNet

A  measure  to  improve  the  accuracy  of  classification  would  be  to 
create larger training data sets. One approach to obtain such data can be 
to create a platform, where experts can label image data while using the 
trained  models.  A  good  example  for  image  classification  here  is  iNa-
turalist (Van Horn et al., 2018). In our case, the focus would not be on 
image  classification,  but  on  semantic  segmentation  of  biomonitoring 
and remote sensing images. 

In this study, we focused on small-scale remote sensing data obtained 
by  hand-held  camera  devices.  However,  our  domain  adaptation 

Appendix A. Dataset visualization 

approach  and  the  joint  energy-based  semantic  segmentation  model 
(JESS) have a broad applicability and can be very valuable for air- and 
spaceborne remote sensing applications. 

CRediT authorship contribution statement

IoU =

A ∩ B
A ∪ B

(19) 

Fig.  3. Methodological  workflow  of  the  Joint  Energy-Based  Semantic  Segmentation  (JESS).  Green  and  orange  arrows  describe  fieldwork  and  the  annotation  of 
images with Labelbox respectively. Light blue arrows describes the data processing that has been done using Python. The deep learning part, implemented with the 
Python library PyTorch is shown in dark blue. The energy-based optimization step of the JESS model is outlined with a red dashed line. The baseline model is trained 
without this optimization step. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) 

EcologicalInformatics79(2024)1024176S. Herdy et al.

To  investigate  the  applicability  of  Semantic  Segmentation  on  bio-
crust  imagery,  the  images  were  classified  using  the  well-established 
Semantic  Segmentation  Model  Unet  (Ronneberger  et  al.,  2015).The 
encoder weights of the model were pretrained on the ImageNet dataset 
(Deng et al., 2009) before training the model on our labeled datasets. 
The pretraining reached an accuracy of 78,57% on the ImageNet Dataset 
(He  et  al.,  2019).  Pretraining  is  well  recommended  in  Deep  learning 
methods,  especially  when  algorithms  are  trained,  based  on  small 
training datasets, as in our case (He et al., 2019). The pretrained model 
weights are made available by the used framework PyTorch and can be 
directly loaded when the model is initialized (Paszke et al., 2019). The 
training  on  our  data  was  performed  for  1000  epochs  on  randomly 
cropped images of size 512 × 512 with a learning rate of 0.0001 and an

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text mentions two main datasets used in the deep learning pipeline: ImageNet and a custom labeled dataset specific to the research. The ImageNet dataset is used for pretraining the encoder weights of the UNet model, achieving an accuracy of 78.57%. This pretraining is common practice in deep learning methods, particularly when working with smaller training datasets like the one used in this study. The custom labeled dataset consists of biocrust imagery, which is used to train the UNet model after it has been pretrained on ImageNet.
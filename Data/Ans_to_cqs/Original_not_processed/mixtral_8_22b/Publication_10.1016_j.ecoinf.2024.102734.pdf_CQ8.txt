Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

query set are negative. Liu et al. (Liu et al., 2019) propose transductive
propagation that iteratively propagates labels from labelled instances to
unlabelled instances according to the graph structure of the embeddings.
Other semi-supervised learning techniques have been applied such as
active learning. Wang et al. (Wang et al., 2022) combines a few-shot

samples (red line) compared to randomly ordered samples, no active
learning (blue line), and active learning using high-certainty samples
(yellow line). The baseline accuracy is calculated using a 2-shot, 2-way
test accuracy without fine-tuning (81.4%). The AST feature extraction
model is fine-tuned on a subset of 500 training samples. Model perfor-
mance is re-evaluated on the test set at 10 sample increments using each
of the three sample orderings. Low certainty refers to queries with a
similar Euclidean distance to prototypical embeddings and therefore lie
along decision boundaries. For high-certainty samples, the order is
reversed with query embedding in close proximity to prototypical em-
beddings being prioritised for model fine-tuning.

2.1. State of the art

Machine learning tasks designed for low-data contexts are referred to
as few-shot learning tasks. Within the context of bioacoustic applications,

EcologicalInformatics82(2024)1027342B. McEwen et al.

the field of computational bioacoustics (Stowell, 2021) has experienced
significant development over the last few years, in part due to public
challenges such as the Detection and Classification of Acoustic Scenes
and Events (DCASE) (Nolasco et al., 2023).

∑

x∈Sd

fθ(x)

3.4. Active learning

(1)

A recommender system, Fig. 4, ranks segment embeddings in order
of lowest certainty to highest certainty. Low-certainty embeddings
correspond to features on decision boundaries. The Euclidean distance ld
is computed between existing prototypes cd and the query embeddings q
providing both class suggestion and class probability using P(q ∈ d | ld)
= softmax(1 (cid:0) norm(ld)). The recommender ranks queries in terms of

This methodology has been implemented into the Listening Lab
Annotator Tool (McEwen et al., 2023b), shown in Fig. 5. The following
section demonstrates the functionality of the annotator and methodol-
ogy workflow.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The information regarding the code repository of the deep learning pipeline is not explicitly mentioned in the given context. Therefore, I cannot provide a specific answer to where it might be available, e.g., GitHub, GitLab, or BitBucket. It would require additional resources or references to determine its location.

Based on the context provided, we can see that there are several studies and methods discussed related to machine learning tasks, specifically focusing on low-data contexts and few-shot learning tasks within the field of computational bioacoustics. However, none of these sources mention any details about the availability of their respective code repositories.

To find out more about the code repository of the deep learning pipeline, one could look for further publications by the authors mentioned in the context, such as Liu et al. (2019), Wang et al. (2022), Stowell (2021), Nolasco et al. (2023), and McEwen et al. (2023b). Additionally, checking the reference sections of these papers may lead to other relevant works containing links to the desired code repositories.
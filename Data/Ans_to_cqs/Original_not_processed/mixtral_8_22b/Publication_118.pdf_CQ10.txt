Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.3. Network architecture 

The architecture used was a heavily modified version of that used in 
“Unsupervised Deep Learning with Pytorch” by github user “eelxpeng” 
(eelxpeng, 2018). As the size of our audio spectrograms has been con-
strained to 128 × 128 pixel RGB images with values repeated in each 
color channel to produce a greyscale image. For this initial experiment 
testing  the feasibility of using an auto-encoder-generated feature rep-
resentation for ecoacoustic analysis, we chose a basic auto-encoder ar-
chitecture  to  minimise  the  complications  that  may  be  introduced  by 
more advanced architectures. A rectified linear unit (ReLU) based acti-
vation function was be used, to help mitigate the vanishing/exploding 
gradient  problem  (Xu  et  al.,  2015).  Networks  using  implicit  pooling 
(determined using pytorch) and explicit max-pooling were used.

For  deep  learning  tasks  you  often  start  from  a  point  of  labelling 
training your data. However this can be slow and problematic. It can be 
useful to have a general representation for machine learning, that does 
not  rely  on  labeled  data  or  domain  specific  features.  This  paper  will 
focus on the use of auto-encoders for automatic feature extraction. First 
we  will  explore  the  relevant  literature,  then  explain  our  method  for 
automatically learning features, our method for comparing with other 
feature  representations,  results,  and  finally  our  discussion  and 
conclusions. 

The outcome of this research is an alternative method for analysing 
eco-acoustic  data.  It  will  be  of  significance  to  the  fields  of  Machine 
Learning and Soundscape Ecology. This research will be of direct benefit 

* Corresponding author. 

E-mail address: benjamin.rowe@hdr.qut.edu.au (B. Rowe).

Fig. 1. The structure of an auto-encoder.  

EcologicalInformatics62(2021)1012372B. Rowe et al.                                                                                                                                                                                                                                    

proportion of non-silent audio clips. The research design is outlined in 
Fig. 2 

3.4. Training 

3.2. Data pre-processing

Contents lists available at ScienceDirect 

Ecological Informatics 

journal homepage: www.elsevier.com/locate/ecolinf 

Acoustic auto-encoders for biodiversity assessment 

Benjamin Rowe *, Philip Eichinski, Jinglan Zhang, Paul Roe 

Queensland University of Technology, Queensland, Australia   

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Eco-acoustics 
Deep learning 
Auto-encoders 

Continuous audio recordings are playing an ever more important role in conservation and biodiversity moni-
toring,  however,  listening  to  these  recordings  is  often  infeasible,  as  they  can  be  thousands  of  hours  long. 
Automating  analysis  using  machine  learning  algorithms  requires  a  feature  representation.  In  this  paper  we 
propose a technique for learning a general feature representation from unlabelled audio using auto-encoders, 
which can be used for analysing environmental audio on a small timescale.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
I am sorry, but there is no information about the code repository link of the deep learning pipeline in the given context. Therefore, I cannot provide the requested information.
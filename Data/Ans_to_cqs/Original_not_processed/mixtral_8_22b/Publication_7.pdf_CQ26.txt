Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on November 16,2023 at 09:41:16 UTC from IEEE Xplore.  Restrictions apply. 

978-1-5090-1537-5/16/$31.00 ©2016 IEEEA. Classiﬁcation Process

Image representations extracted from deep neural networks,
trained on large datasets such as ImageNet [9] and ﬁne tuned
on domain speciﬁc datasets, have shown state-of-art perfor-
mance in numerous image classiﬁcation problems [14]. The
activation vectors of the ﬁrst fully connected layer of a pre-
trained VGGnet [24] are employed as feature representations
in our work. The weights of this deep network are ﬁne tuned
using the Benthoz15 dataset [23] which consists of expert-
annotated and geo-referenced marine images from Australian
seas.

Convolutional neural networks (CNNs) [9], also known as
deep networks, are an important class of machine learning
algorithms applicable, among others, to numerous computer
vision problems. Deep CNNs, in particular, are composed of
multiple layers of processing involving linear as well as non-
linear operators. To solve a particular task, the parameters of
networks are learned in an end-to-end manner. Image represen-
tations extracted from deep CNNs trained on a large dataset
such as ImageNet [10] have shown to produce a promising
performance for diverse classiﬁcation and recognition tasks
[11], [12], [13], [14] and [15]. Spatial pyramid pooling (SPP)
[16] and Multi-scale Orderless Pooling (MOP) [17] schemes
have made CNNs independent of the input image size and
robust for diverse classiﬁcation and recognition applications.
In this paper, we propose a computer vision and deep
learning based framework for the automatic annotation of

C. Classiﬁcation Experiments and Results

Selecting patch sizes that give the best classiﬁcation ac-
curacy is an important step. We trained our classiﬁer using
multiple patches at different scales and achieved the best
performance when these three patch sizes were used: 28 × 28,
224 × 224, and 448 × 448. These correspond to small, medium
and large scales. Feature extraction at different sizes insures an
efﬁcient encoding of coral species independently of their size.

28x28

224x
224

448x
448

l

a
i
t
a
p
S

i

d
m
a
r
y
P

t
e
n
G
G
V

g
n

i
l

o
o
p
-
x
a
M

P
L
M

r
e
y
a
L
2

Coral

Non-
Coral

Fig. 3: Block diagram of the proposed classiﬁcation method.

[16] Y. Gong, L. Wang, R. Guo, and S. Lazebnik, “Multi-scale orderless
pooling of deep convolutional activation features,” in Computer Vision–
ECCV 2014, pp. 392–407, Springer, 2014.

[17] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep
convolutional networks for visual recognition,” in European Conference
on Computer Vision (ECCV), pp. 346–361, Springer, 2014.

[18] L. Edwards, “Catami classiﬁcation scheme for scoring marine biota and
substrata in underwater imagery,” CATAMI Classiﬁcation Scheme for
Scoring Marine Biota and Substrata in Underwater Imagery, pp. 1–17,
2013.

[19] M. D. Stokes and G. B. Deane, “Automated processing of coral reef
benthic images,” Limnology and Oceanography: Methods, vol. 7, no. 2,
pp. 157–168, 2009.

[20] O. Pizarro, P. Rigby, M. Johnson-Roberson, S. B. Williams, and
J. Colquhoun, “Towards image-based marine habitat classiﬁcation,” in
OCEANS 2008, pp. 1–7, IEEE, 2008.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information about specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, some common techniques used to handle data bias include stratified splitting, oversampling, undersampling, and collecting diverse data.

Stratified splitting involves dividing the dataset into training and testing sets while maintaining the same distribution of classes or labels in both sets. This ensures that the model learns from a representative sample of each class.

Oversampling and undersampling involve adjusting the number of samples in each class to balance the dataset. In oversampling, additional copies of minority class examples are added to the dataset, while in undersampling, majority class examples are removed from the dataset.

Collecting diverse data involves gathering data from various sources and ensuring that it represents all possible scenarios and variations. This helps prevent overfitting and improves the generalization ability of the model.

Again, please note that the given context does not mention any of these techniques specifically.
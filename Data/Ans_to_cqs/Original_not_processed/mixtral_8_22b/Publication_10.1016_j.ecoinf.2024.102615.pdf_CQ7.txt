Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.1.3. Data partition 

The raw dataset was randomly shuffled, after which the shuffled data 
were divided into training, validation, and test sets. The training set was 
used to learn the parameters of the model during the training process, 
and the validation set was used to evaluate the modeling and tune its 
hyperparameters. Finally, the performance and generalizability of the 
optimal model were evaluated using the test set. The data partitioning 
ratio between the training, validation, and test sets was set as 8:1:1 and 
the random partition was repeated ten times to reduce the influence of 
random splitting while ensuring converged performance (Geras et al., 
2017). 

2.2. MLP model and model assessments 

2.2.1.

Introduction of MLP

2.3. Model optimization 

The model performance optimization process is broadly divided into 
two stages, as shown in Fig. 3. The R value of the validation set is used as 
the performance indicator during the optimization process, whereby R is 
maximized  using  the  grid  search  method.  During  the  first  stage,  the 
neural network structure and the number of neurons in the hidden layers 
are  adjusted.  In  total,  60  sets  of  combinations,  including  one  to  five 
hidden layers and neurons with a range of 64–2048 (nth  power of 2), 
were considered. The second stage involves adjusting the percentage of 
the dropout layers, the learning rate, and the batch size, to achieve a 
total of 192 combinations (4*4*3*4).

The performance of a neural network is traditionally improved by 
optimizing  its  hyperparameters,  which  influence  the  overall  model 
performance.  Using  different  hyperparameter  combinations  (e.g., 
combining  different  learning  rates  and  dropout  values)  will  lead  to 
marked  differences  in  the  predictive  performance  of  a  model.  As  a 
traditional  hyperparameter  optimization  method,  the  grid  search 
approach is widely used to optimize a model by comparing the values of 
evaluation  metrics  for  different  parameters  and  selecting  the  optimal 
parameters from a large search space (Bhagat et al., 2020). This method 
can optimize the model by comparing the values of evaluation metrics 
for different parameters and selecting the optimal parameters from the 
search space. In this study, the grid search method was used to optimize 
five of the MLP model’s hyperparameters. The hyperparameter adjust-

the percentage value of the dropout layers, the learning rate, and the 
batch size, the optimal model was obtained with an R value of 0.76.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In the given context, the raw dataset was randomly shuffled before being divided into three distinct subsets: training, validation, and testing sets. Each subset serves a specific purpose in the development and assessment of the deep learning model.

The training set is utilized to teach the model about the underlying patterns within the data. By exposing the model to various input-output pairs, it can gradually learn and update its internal parameters through backpropagation and gradient descent algorithms.

On the other hand, the validation set plays a crucial role in tuning the model's hyperparameters and preventing overfitting. Overfitting occurs when the model becomes too complex or specialized to the training data, resulting in poor generalization on unseen data. To mitigate this issue, the validation set is employed to monitor the model's performance throughout the training process and guide the selection of appropriate hyperparameters.

Lastly, the test set is reserved exclusively for evaluating the final model's performance and generalizability. It remains untouched until the completion of the training phase, allowing for an unbiased estimation of how well the model would perform on new, previously unseen data.

Regarding the data partition ratio, the context specifies that the training, validation, and test sets have been allocated in an 8:1:1 proportion. This means that 80% of the available data is dedicated to training, while the remaining 20% is evenly distributed between validation and testing purposes. Furthermore, the random partitioning process is repeated ten times to minimize the impact of chance splits and ensure consistent results across multiple runs.
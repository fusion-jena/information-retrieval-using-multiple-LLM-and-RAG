Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

adjusted to optimize the model (Jafarzadeh et al., 2021). Whereas node- 
size  determines  the  smallest  number  of  observations  in  a  tree  subset 
terminal node, and default value is always at 1 (Jafarzadeh et al., 2021; 
Pham et al., 2018). Gradient boosting iteration is a repetition process of 
increasing  or  decreasing  weights  in  a  training  datasets  (Huang  et  al., 
2022). All hyperparameters were tuned with a grid search method. 

2.5.1. Artificial neural networks

by Boruta were then used to build separate models and their results were 
compared to an XGBoost and ANN model that included all 78 variables.

The development of predictive models often requires the computa-
tion of the smallest possible number of the most influential variables. 
Through  Boruta  feature  selection  model,  a  significant  reduction  in 
dimensionality  of  the  covariates  was  achieved.  The  results  in  Fig.  3 
shows ideal features selected for optimal prediction of carbon stock in a 
reforested landscape. Based on the Boruta feature optimization method, 
a subset of 23 predictor variables were selected for the final prediction 
model of carbon stock. Specifically, band 6 (red), band4 (green 2), band 
8  (near-infrared),  RENDVI,  RE_SR,  GNDVI,  NDVI,  CIG,  SAVI,  RTVI, 
band7 red edge, EVI1, EVI 2, band 4 (5 × 5 mean), band5 (5 × 5 mean), 
band5 (7 × 7 mean), band6 (7 × 7 mean), band7 (7 × 7 mean), band8 (7 
× 7 mean), band8 (7 × 7 mean), elevation, topographic position index 
and Channel Network base. Thereafter, the selected variables identified

Commonly, multicollinearity and image spectral noise impedes sta-
tistical  analysis  of  remotely  sensed  data,  leading  to  poor  carbon  esti-
mation  (Hall,  1999;  Odebiri  et  al.,  2020a).  In  this  regard,  rigorous 
selection  of  optimal  subset  of  variables  with  smallest  error  rate  is 
necessary  for  improving  regression  model  performance  and  reducing 
spectral  noise.  The  Boruta  feature  optimization  method  is  a  wrapper 
algorithm  designed  around  the  random  forest  algorithm  (Kursa  and 
Rudnicki, 2010). It considers the fluctuations in the average precision 
loss  of  trees  in  the  forest  and  uses  the  average  drop  accuracy  (i.e., 
Z_score) to measure the importance of each input feature. Using a mixed 
shadow feature set prior to selection, the correlation between the feature 
and the predicted value is eliminated, which is more advantageous when 
processing remotely sensed data that exhibits strong feature correlation.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
According to the given context, the hyperparameters of the model are optimized using a grid search method. Grid search is a technique where we define a set of values for each parameter and exhaustively consider every combination of these parameters to find the best one. This method can be computationally expensive but it ensures that the optimal solution will not be missed. However, the context does not provide any information about other methods such as random search or Bayesian optimization being used for hyperparameter tuning.
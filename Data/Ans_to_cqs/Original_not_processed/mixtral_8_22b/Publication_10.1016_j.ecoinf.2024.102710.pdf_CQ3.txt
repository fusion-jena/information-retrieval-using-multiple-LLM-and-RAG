Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4 https://github.com/HKathman/pam_annotation_experiments
5 https://github.com/kahst/BirdNET-Analyzer/tree/main/checkpoints/V2.4
6 https://tfhub.dev/google/vggish/1
7 https://tfhub.dev/google/yamnet/1
8 tensorflow.keras.applications.vgg16.VGG16(weights=’imagenet’).
9 tensorflow.keras.applications.resnet_v2.ResNet152V2

(weights=’imagenet’).

Fig. 4. UMAP plots for different embedding layers of different embedding
models for AnuraSet. For UMAP generation, we randomly select 5000 samples
and discard all samples that are aligned to more than one class. Colors and
shapes indicate the 10 classes with the highest occurrence frequency. Layers are
numbered according to their distance from the classification layer, e.g. ‘Bird-
Net-1’ is the last layer before the classification layer of the BirdNet model.

EcologicalInformatics82(2024)1027105H. Kath et al.

both the target modality and the domain, yields the best performance as
a feature embedding model. Using BirdNet-1 as the embedding model,
active learning sample selection strategies significantly reduce the
required for annotation to achieve model
number of
convergence.

This study explores the combination of transfer learning and active
learning as a means to reduce the amount of time needed to annotate
PAM datasets (Fig. 1). Specifically, we compare 5 standard embedding
models pre-trained on large datasets from domains with varying degrees
of proximity to PAM (namely, images, generic audio, or bioacoustics)
and evaluate them with linear classifiers on three soundscape datasets
covering different taxa. For the unparalleled multi-label PAM dataset
AnuraSet, our simple linear model (applied to features extracted with a
transfer learning model) surpasses the convolutional baseline (Ca˜nas
et al., 2023) by a remarkable margin of 21.7%. After identifying the
feature space (i.e. transfer learning model) that gives the best classifi-
cation accuracy for sound event detection with soundscape data, we
investigate the potential of active learning to accelerate learning in this
space. In contrast to previous literature, we perform a comparative study

evaluation was done on focal and citizen-science datasets. Swaminathan
et al. (2024) extend the observation to attention-based architectures pre-
trained on human speech. Lauha et al. (2022) show that transfer
learning can be helpful also for small networks trained from scratch on
small, targeted datasets gathered from online resources such as Mac-
aulay Library2 (as opposed to foundation models trained on large
datasets). While the works cited above evaluate transfer learning models
based on classification performance, McGinn et al. (2023) take a
different approach and investigate the topology of fine grained, sub-
species sound events in the embedding space afforded by BirdNet;
they find that different call types of a same species (e.g., drumming
versus vocalization) form distinct clusters, and that the vicinity of each

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper does not provide specific information about the data annotation techniques used in the deep learning pipeline. However, it mentions that the authors use transfer learning and active learning to reduce the amount of time required for annotating Passive Acoustic Monitoring (PAM) datasets. Transfer learning involves using pre-trained models to extract features from new datasets, while active learning involves selecting the most informative samples for manual annotation. The authors do not mention any specific annotation techniques like bounding box annotation or instance segmentation. Instead, they focus on evaluating the performance of various embedding models pre-trained on large datasets from different domains, including images, generic audio, and bioacoustics. They then apply these models to three soundscape datasets covering different taxa and compare their performance using linear classifiers. Based on the results, they identify the best feature space for sound event detection with soundscape data and explore the potential of active learning to accelerate learning in this space. Overall, the paper emphasizes the importance of leveraging existing knowledge through transfer learning and reducing the need for extensive manual annotation through active learning.

Answer:::
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 4 
Quantitative  comparison  on  UCCS  dataset  using  UIQM,  UCIQE,  and  NIQE 
metrics.   

UIQM↑ 

UCIQE↑ 

NIQE↓ 

Raw_Images 
Fusion 
IBLA 
U-Transformer 
Funie-GAN 
Water-Net 
UWCNN 
Ours 

2.29 
2.99 
2.36 
3.02 
3.05 
3.13 
2.78 
3.17 

0.410 
0.476 
0.480 
0.539 
0.558 
0.550 
0.455 
0.568 

4.57 
4.38 
4.29 
3.96 
4.37 
4.07 
4.12 
4.03  

A quantitative analysis of this dataset is given in Table 5, confirming 
our  model's  effectiveness.  Our  method  has  achieved  a  UIQM  score  of 
3.26 and an NIQE score of 5.39, followed by the Funie-GAN in terms of 
UIQM with a score of 3.21 and Fusion in terms of NIQE with a score of 
5.40. While measuring the UCIQE score, IBLA achieved a score of 0.626, 
although their resultant images are just color deviations from the orig-
inal images. However, since UCIQE measures a linear combination of 
chroma, saturation, and contrast, a higher value can be expected just by 
having a change in colors (even if they are not desirable).

6.2. Non-Reference Evaluation 

In the evaluation of test datasets that do not contain reference im-
ages, three non-reference metrics, namely UCIQE (Yang and Sowmya, 
2015) and UIQM (Panetta et al., 2016), and NIQE (Mittal et al., 2013) 
are used. A higher score in either UCIQE or UIQM signifies an enhanced 
level of human visual perception in the results. At the same time, a lower 
NIQE score indicates a better perception quality. In evaluating the C-60 
challenge  dataset, our method  demonstrated remarkable performance 
across various quality metrics. As shown in Table 2. Our method ach-
ieved the highest UIQM score of 3.12, surpassing all other approaches. 
Funie-GAN  achieved  the  second-highest  UIQM  score  of  3.10.  When 
considering  a  balance  between  chroma,  contrast,  and  saturation,  our 
method  once  again  excelled  with  the  highest  UCIQE  score  of  0.591, 
followed  closely  by  Water-Net  with  a  score  of  0.578.  In  terms  of

24.36 
0.885 

15.25 
0.656 

25.59 
0.893  

EcologicalInformatics81(2024)1026319A.B. Bakht et al.                                                                                                                                                                                                                                

Table 2 
Quantitative  comparison  on  C-60  dataset  using  UIQM,  UCIQE,  and  NIQE 
metrics.   

UIQM↑ 

UCIQE↑ 

NIQE↓ 

Raw_Images 
Fusion 
IBLA 
U-Transformer 
Funie-GAN 
Water-Net 
UWCNN 
Ours 

1.99 
2.78 
1.81 
2.65 
3.10 
2.57 
2.25 
3.12 

0.478 
0.512 
0.574 
0.534 
0.572 
0.578 
0.466 
0.591 

5.18 
4.74 
5.02 
4.94 
4.73 
4.88 
4.89 
4.67  

Table 3 
Quantitative comparison on U45 dataset using UIQM, UCIQE, and NIQE metrics.   

UIQM↑ 

UCIQE↑ 

Raw_Images 
Fusion 
IBLA 
U-Transformer 
Funie-GAN 
Water-Net 
UWCNN 
Ours 

2.44 
3.14 
1.60 
3.10 
3.21 
3.18 
2.82 
3.23 

0.481 
0.532 
0.579 
0.553 
0.602 
0.587 
0.471 
0.612 

NIQE↓

(10)  

where the coefficients are c1 = 0.4680, c2 = 0.2745, and c3 = 0.2575, 
as defined in (Yang and Sowmya, 2015). A higher UCIQE score indicates 
an improved balance between chroma, contrast, and saturation. Mean-
while, UIQM is formed through a linear combination of UICM, UISM, 
and UIConM, representing colorfulness, sharpness, and contrast metrics, 
respectively: 

UIQM = c1⋅UICM + c2⋅UISM + c3⋅UIConM

(11)  

EcologicalInformatics81(2024)1026316A.B. Bakht et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The completion of training for machine learning models depends on several factors, including the specific task, data size, complexity, and available resources. One common criterion used to determine when training should stop is the validation loss plateau. This occurs when the validation loss stops decreasing significantly after each epoch, indicating that further training may lead to overfitting without improving generalization performance. Other criteria include early stopping, which halts training when the validation error starts increasing, and setting a fixed number of iterations or epochs based on computational budget constraints. Ultimately, the choice of criteria depends on the problem domain and desired tradeoff between accuracy and efficiency.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In  the  development  stage,  the  preprocessed  data  was  divided  into 
training (80%), validation (10%), and test sets (10%) for model training 
and testing purposes (Shah et al., 2018), followed by the construction of 
predictive  models  using  both  deep  learning  algorithms  (e.g.,  Deep 
Neural  Networks)  and  traditional  machine  learning  approaches  (e.g., 
AdaBoost  Regressor,  Support  Vector  Regressor,  etc.),  with  hyper-
parameter tuning performed via GridSearchCV with cross-validation (cv 
= 10).  While some studies  might  use different proportions, such as  a 
70–30 split between training and testing/validation (Khan et al., 2022), 
the  80–10-10  split  employed  in  this  study  ensured  a  more  extensive 
training set, potentially leading to a better-generalized model. The test 
set,  which  the  model  has  not  previously  seen  during  the  training  or 
validation phases indicates the model’s performance in real-world sce-
narios (Shah et al., 2018).

In the context of predictive model performance, it is imperative to 
consider the impact of the limited number of observations for each plot. 
A constrained dataset size can significantly influence model accuracy, as 
it  restricts  the  model’s  ability  to  learn  and  generalize  from  the  data 
effectively. This limitation is particularly pertinent in complex agricul-
tural  systems  like  oil  palm  yield  prediction,  where  the  diversity  and 
interplay  of  numerous  variables  demand  a  robust  dataset  for  optimal 
model  training  and  validation.  Therefore,  the  relatively  lower  perfor-
mance  of  some  models  in  this  study  could  be  partly  attributed  to 
insufficient  observations,  which  hampers  the  models’  capability  to 
capture and predict the intricate patterns and interactions inherent in 
the agricultural data. This factor stresses the need for more extensive 
and comprehensive datasets to enhance the predictive accuracy of ma-

Employing GridSearchCV ensured that each candidate combination 
of  hyperparameters  is  assessed  using  cross-validation  with  a  fixed 
number of folds, in this case, ten, as highlighted by Shahhosseini et al. 
(2021). This tenfold cross-validation provides a reliable estimate of the 
model’s performance, balancing the trade-off between bias and variance 
and  guarding  against  overfitting.  The  hyperparameter  tuning  process 
implemented  in  this  study  relied  on  a  detailed  and  systematic  search 
across a predefined grid of hyperparameter values. A unique parameter 
grid  was  defined  for  each  model  considered,  targeting specific  model 
characteristics such as the number of estimators in ensemble methods or 
the  depth  of  trees  in  decision  tree-based  algorithms.  This  process 
ensured that the eventual choice of hyperparameters was not arbitrary 
but  is  backed  by  empirical  evidence  that  aligns  with  Schratz  et  al.

In  stark  contrast,  the  Decision  Tree  Regressor  recorded  negligible 
efficacy, with an MSE of 1610.90 and an R2 of 0.35. This model’s limited 
depth  of  3  and  a  minimalistic  approach  to  sample  splitting  and  leaf 
parameters contributed to its underwhelming performance. 

3.2.2. Possible reasons for the varying performances of the models

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention the criteria used to determine when training is complete. However, based on common practices in machine learning, one possible criterion could be monitoring the validation loss during training. When the validation loss reaches a plateau or starts increasing, it may indicate that the model is starting to overfit the training data, and further training would not improve the model's performance on unseen data. In this study, the authors used a validation set (10% of the preprocessed data) to evaluate the model's performance during training, so they likely monitored the validation loss to decide when to stop training. Another potential criterion could be setting a maximum number of epochs or iterations for training, after which the training process stops regardless of the model's performance.
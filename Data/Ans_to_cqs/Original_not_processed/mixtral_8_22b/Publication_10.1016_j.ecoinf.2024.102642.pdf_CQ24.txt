Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During  the  campaign,  methods  are  proposed  to  assess  inter- 
annotator  variability  and  aggregate  annotation  sets.  Those  methods 
will help to define the minimal number of annotators needed [Dubus 
et al., 2023; Walter et al., 2022]. Increasing the number of annotators 
per sample could be helpful if the variability is too high. Finally, the two 
grouping  methods:  majority  voting  (Section  2.2.3)  and  soft  labeling 
(Section 2.3.2) can also be used to produce relevant annotation sets for 
deep learning. 

6. Conclusion 

In this study, new approaches were explored to improve the accuracy 
and  efficiency  of  cetacean  vocalization  detection  using  deep  learning 
models. Leveraging multi-annotation campaigns involving both expert 
and novice annotators aimed to address the challenges associated with 
the scarcity of ground truth data in underwater PAM studies.

distinctive  annotation  profiles  influenced  by  the  complexity  of  vocalizations  and  the  annotatorsâ€™  strategies, 
ranging from conservative to permissive. To address the challenges of annotation discrepancies, Convolutional 
Neural Networks (CNNs) are trained on annotations from both novices and the expert. The results show varia-
tions in model performance. Our work highlights the importance of annotation guidelines encouraging a more 
conservative  approach  to  improve  overall  annotation  quality.  In  an  effort  to  optimize  the  potential  of  multi- 
annotation  and  mitigate  the  presence  of  noisy  labels,  two  annotation  aggregation  methods  (majority  voting 
and  soft  labeling)  are  proposed  and  tested.  The  results  demonstrate  that  both  methods,  particularly  when  a 
sufficient number of annotators are involved, significantly improve model performance and reduce variability:

directly  to  datasets  where  the  relevance  of  annotations  has  not  been 
evaluated.

CNN  models  were  trained  using  each  annotation  set  to  study  the 
impact  of  inter-annotator  variability  on  the  performance  of  deep 
learning models. All trained models were evaluated on different data-
sets, in different areas with different recording devices and with multiple 
expert annotations.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not directly discuss strategies to handle randomness in the deep learning pipeline, such as managing random seed values. However, it does mention several strategies related to improving the accuracy and efficiency of cetacean vocalization detection using deep learning models. These include leveraging multi-annotation campaigns involving both expert and novice annotators, addressing annotation discrepancies through training CNNs on annotations from both groups, and employing annotation aggregation methods like majority voting and soft labeling. While these strategies do not specifically target handling randomness in the deep learning pipeline, they contribute to enhancing the robustness and reliability of the models by reducing variability and improving overall annotation quality.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

For the ViT models, we adapted the approach of pretraining deep
bidirectional transformers for language understanding (BERT) by Devlin
et al. (2018) for birdsong classification. Table 3 provides an overview of
the investigated ViT models with their respective parameterizations.
ViT-S/16, for example, denotes a ViT variant of reduced complexity with
an input patch size of 16 × 16. The sequence length of the transformer
models is inversely proportional to the square of the provided patch size.
In the following, ViT-B/16 is further investigated as it strikes a balance
between general model complexity and classification performance.

training or validation progress could be observed. To fine-tune our ViT
models, we deployed ImageNet-based weights for pretraining.

Table 2
Overview of our augmentation strategies, their IDs, and related information. For DenseNet-161 and ViT-B/16, their training times are provided in minutes per epoch
for the different augmentation methods. For IDs 5 and 6, we added noise of the bird audio detection data set from the DCASE challenge (Berger et al., 2018; Himawan
et al., 2018; Liaqat et al., 2018), which is abbreviated as BAD.

Data augmentation

Time domain

Frequency domain

Spectrogram

Training time [min. / epoch]

DenseNet-161

ViT-B/16

Gaussian noise
Pink noise (background)
Primary background noise soundscapes
Secondary background noise soundscapes
Primary background noise (BAD)
Secondary background noise (BAD)
Background noise (e.g., wind, thunder, or aircrafts)
Mixed up random bird species
Horizontal roll
Vertical roll
Pitch shift
Time mask
Frequency mask
Gain
Loudness normalization
Horizontal flip
Vertical flip
Time stretch
tanh-based distortion

✓
✓
✓
✓
✓
✓
✓

✓

✓
✓

✓

2.4. Model architecture

The proposed model architectures are based on DenseNet-161
(Huang et al., 2017) and ResNet-50 (He et al., 2016a, 2016b) as well
as different vision transformers (ViT) (Dosovitskiy et al., 2021; Steiner
et al., 2022) for sound event detection.

The design of our SED models is inspired by the 2019 DCASE
workshop and large-scale PANNs for audio pattern recognition (Kong
et al., 2019). DenseNets are utilized to improve the information flow

7 AudioSet project page, https://research.google.com/audioset/

html

8 librosa project page, https://librosa.org/doc/latest/index.

EcologicalInformatics82(2024)1026996A.S. Kumar et al.

Fig. 3. Comparison of different augmentation methods and their combinations in Table 4 using DenseNet-161 and ViT-B/16. Additionally, plot marks for the
conducted experiments and their results are provided.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't enough information available in the given context to provide a detailed explanation about the deployment process of the trained deep learning model. However, some insights can still be derived from the text.

From the context, we understand that the authors used Vision Transformer (ViT) models for birdsong classification, specifically focusing on ViT-B/16. They also mentioned adapting the BERT methodology for this purpose. Furthermore, they employed various augmentation techniques during the training phase, such as Gaussian noise, pink noise, primary and secondary background noises, horizontal and vertical rolls, pitch shifts, masks, gain adjustments, loudness normalization, flips, stretches, and tanh-based distortions.

Regarding the specific details of the deployment process, like model serialization or platform selection, these aspects aren't covered in the provided context. Therefore, I cannot offer any concrete information regarding those topics. It would be best to refer back to the original source material or consult other resources for more comprehensive answers.
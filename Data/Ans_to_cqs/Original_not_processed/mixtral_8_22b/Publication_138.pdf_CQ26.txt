Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The pooling operator is connected after this layer for extracting the main features of a certain area, 
reducing the number of parameters, and preventing the model from over fitting. The maximum pooling 
with a 2 × 2 window and 2 strides is used in this layer. The pooling result is the data with the size of 56 
× 56 × 64.  

After pooling, the LRN (Local Response Normalization) is used to normalize the local response. 
The LRN enhances the larger response value and reduce the smaller response value. Thus, the LRN 
also can improve the generalization of the model. The output result of the LRN is the data with the size 
of 56 × 56 × 64.

After first inception modules, second and third inception modules are set in turn. The output is the 
data with the size of 28 × 28 × 480. Then, the pooling and LRN are performed, the data with the size of 
14 × 14 × 480 is output. 

Ninth  layer: It  is a convolution layer with  480  3  ×  3  filters  and  their stride  is  1. The ReLU is 
selected as the activation function. The result of this layer is the data with the size of 14 × 14 × 480. 
After convolution, the data is performed by LRN. Then, the maximum pooling with a 3 × 3 window 
and 2 strides is used. The output result is the data with the size of 6 × 6 × 480. 

Tenth layer: This layer is dropout layer which is used to improve the generalization capability. 

The size of output result is 17,280.

(1) LeNet is firstly proposed by LeCun et al. in the 1990s, which includes three convolution layers, 
two  sampling  layers,  two  fully  connected  layers.  At  that  time,  it  is  difficult  to  run  LeNet  due  to  the 
limit of computation and memory capacity [21]. 

(2) AlexNet is proposed by Alex et al. and won the ILSVRC 2012 [22]. AlexNet achieves higher 
identifying accuracy than all the traditional machine learning algorithms. It is significant breakthrough 
for machine learning for classification. 

(3) GoogLeNet is proposed by Christian of Google and is the winner of ILSVRC 2014 [23], in 
which inception layers, including different receptive areas with different kernel sizes capturing sparse 
correlation patterns, are integrated into CNN.

(4) VGG is proposed by Simonyan et al. in 2014, in which the convolution filter is a 3 × 3 filter 
and  the  stride  is  2.  VGG-11,  VGG-16  and  VGG-19  respectively  include  11,  16  and  19  layers.  The 
Softmax layer is the final layer for classification [24]. 

(5) ResNet is proposed by Kaiming et al. in 2015 to address to the vanishing gradient problem and 
shows the excellent ability of classification [25]. The popular ResNet consists of 49 convolution layers. 
All  the  algorithms  in  our  experiments  are  coded  in  Matlab  R2019a.  The  values  of  initial 
parameters  are  randomly  generated  for  networks.  The  parameters  are  set  as  follows:  the  number  of 
epochs, learning rate drop factor, learning rate drop period, batch size are 200, 0.0004, 0.6, 30 and 128, 
respectively. The operating system of PC is Window 7 and the GPU is a NVIDIA with 8 GB memory. 

3.4. 

Results and discussions

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there is no information about specific techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, some common techniques used to address data bias during preprocessing of the deep learning pipeline are:

* Stratified Splitting: In this technique, the dataset is divided into training and testing sets while maintaining the same distribution of classes in both sets. This helps to ensure that the model does not become biased towards any particular class.
* Oversampling: In this technique, the minority class samples are duplicated to balance the dataset. This helps to prevent the model from becoming biased towards the majority class.
* Undersampling: In this technique, the majority class samples are removed to balance the dataset. This helps to prevent the model from becoming biased towards the majority class.
* Diverse Data Collection: Collecting diverse data from various sources can help to reduce data bias. For example, collecting images from different geographical locations, lighting conditions, angles, etc. can help to create a more robust model.

However, without further context, it cannot be determined if these techniques were specifically used in the described deep learning pipeline.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

windows of 100-m, 500-m, or 1-km radii), (ii) the category level (e.g.: 
within thematic covariate categories), and (iii) using all remainders.

algorithms character vector with the name(s) of the 
algorithms(s) used for the embedding procedure; 
ncov value for the target number of covariates to 
include in the final set; maxncov value for the 
maximum possible number of covariates to include in 
the final set; nthreads value for the number of cores to 
be used during parallel operations    

corcut value of the correlation coefficient threshold 
used for identifying collinearity; categories character 
vector with category-level covariate names; variables 
character vector with variable-level names

Table 1 
The three functions available in covsel (ver. 1.0.) with information on input data and arguments. See https://github.com/N-SDM/covsel and the function help files 
for additional details and examples.  

Function 

Description 

Common inputs 

Specific arguments 

covsel. filteralgo() 

Collinearity filtering (Step A) 

covsel. embed() 

Model-specific embedding (Step B) 

pa numeric vector of species presences (1) and 
absences (0); covdata data frame with continuous 
covariate data; weights (optional) numeric vector 
with the weights for each value in pa; force 
(optional) character vector with the name(s) of the 
covariate(s) to be forced in the final set 

covsel. filter() 

Wrapper function applying the collinearity 
filtering algorithm at each target level(s) 
(e.g. i: variable level; ii: category level; iii: 
all remainders) 

corcut value of the correlation coefficient threshold 
used for identifying collinearity

data contexts, which are becoming the norm with the increasing amount 
of species occurrence data from citizen science initiatives (Amano et al., 
2016; Dickinson et al., 2010; Pocock et al., 2017) and the availability of 
environmental layers for modelling them (Kuenzer et al., 2014; Soille 
et al., 2018; Sudmanns et al., 2020). Yet, there is currently no reference 
approach,  or  tool,  to  perform  automated,  data-driven,  parsimonious, 
and fast covariate selection for species distribution modelling.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The context does not provide specific information about the methods used for collecting raw data in a deep learning pipeline. However, it mentions that species occurrence data can come from citizen science initiatives and environmental layers for modeling them. Citizen science initiatives often involve volunteers who collect data through various means such as observations, surveys, or sensor readings. Environmental layers refer to geospatial data sets that describe aspects of the environment like climate, topography, land cover, etc. These data sets can be obtained from public repositories or generated by processing remote sensing imagery. Therefore, we can infer that the methods used for collecting raw data might include surveys, sensors, and public datasets. But without more specific context, this cannot be confirmed.
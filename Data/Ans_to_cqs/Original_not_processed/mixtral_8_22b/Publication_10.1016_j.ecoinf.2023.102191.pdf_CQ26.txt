Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

interviewer who was able to interact with dive staff and divers in person 
and detect some behaviours that may impact data quality. One of these 
behaviours is the tendency to underreport marine species seen as com-
mon and overreport rare and charismatic species. An example could be 
the hesitation of citizen scientists to report a dive where the only taxa 
observed were the algae Asparagopsis sp. and S. zonale, while they were 
eager to report the sighting of C. caretta or E. marginatus. This behaviour 
brings  obvious  bias  to  the  acquired  data  and  has  been  repeatedly 
observed in other research using data from citizen scientists or students 
(Bonney et al., 2009; Galloway et al., 2006; Gardiner et al., 2012). In our 
case study and future program, reporting bias would be mitigated in two 
ways:  i)  by  including  a  checklist  of  conspicuous  species,  and;  ii)  by 
ensuring  the  dive  guide  or  staff  take  on  the  role  of  interviewer  and

curator while using the app as a tool for post-dive debriefing. Combined, 
these two steps ensure that there is a specific checklist to cross out and 
that the interview is embedded in regular operations of the dive centre. 
Another aspect that could influence data quality is the possibility of 
misidentifying  species  that  are  less  known  to  the  citizen  scientists 
(Austen  et  al.,  2016;  Freiwald  et  al.,  2018).  This  is  especially  true 
because the app does not require submission of photographic evidence, 
which  could  be  validated  by  experts  (Kosmala  et  al.,  2016)  or  using 
machine learning (Saoud et  al., 2020). Once again, by relying on the 
dive guide to conduct the interview, there is some level of validation of 
the species sighted and reported, as they are often knowledgeable and 
experienced in local fauna and flora. This approach has also been used in 
previous studies as a safeguard for data quality (Branchini et al., 2015;

sponses  and  open-ended  queries  to  get  insight  into  the  best  way  the 
monitoring program can bring value to the dive community. Responses 
were compiled  and data was used to calculate the average SUS score 
(Kumar, 2020).

observations in both terrestrial and marine settings, including recrea-
tional SCUBA divers. Based on opportunistic sampling, these programs 
are commonly affected by surveyor bias as participants are more likely 
to  report  charismatic  and  interesting  fauna  and  flora  (Roberts  et  al., 
2022).  Some  initiatives  leveraging  opportunistic  citizen  science  have 
attempted to reduce the reporting bias by restricting observations to one 
target taxonomic group or taxon (Gizzi et al., 2020; Michonneau and 
Paulay, 2015) or to one location (Happel et al., 2020), however these 
cases  were  generally  used  for  very  specific  research  goals.  Trying  to 
tackle some of these common issues and limitations of citizen science 
programs, this study explores a novel approach. It provides elements of 
standardised monitoring by engaging SCUBA dive operators to interview 
divers  post-dive  using  a  mobile  app  that  limits  user  reporting  to  a

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide explicit information about the use of specific techniques such as stratified splitting, oversampling, undersampling, or diverse data collection for addressing data bias during preprocessing of the deep learning pipeline. However, it mentions several strategies employed to minimize data bias and improve data quality in a case study involving citizen scientists collecting marine species data. These include:

1. Using a checklist of conspicuous species: By providing a list of species to look for, researchers aim to prevent underreporting of common species and encourage accurate reporting of all observed species.

2. Engaging dive guides as interviewers and curators: To validate species identification and reduce misidentification errors, experienced dive guides are involved in conducting interviews and reviewing submitted data.

3. Post-dive debriefings: Regular post-dive debriefings using a mobile app help embed the interview process into routine dive center operations, ensuring consistent data collection and reducing potential biases.

While these methods do not directly correspond to the mentioned preprocessing techniques for deep learning pipelines, they demonstrate efforts to collect reliable and unbiased data, which is crucial for any subsequent analysis, including machine learning applications.
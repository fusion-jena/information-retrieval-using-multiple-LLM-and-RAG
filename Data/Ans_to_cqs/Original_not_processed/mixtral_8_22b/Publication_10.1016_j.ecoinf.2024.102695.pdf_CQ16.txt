Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

◦

rate of 0.05 is implemented. Throughout the training process, a learning 
rate of 0.0001 is used, with 300 training epochs and 24 data instances 
processed  per  batch.  To  prevent  data  snooping  bias,  the  dataset  is 
divided chronologically: 70% for the training set, 10% for the validation 
set, and 20% for the test set. This allocation strategy better simulates 
real-world scenarios, ensuring that the data in the validation and test 
sets differ from the training set data, thereby enhancing the generaliz-
ability of the model. All the tests are run on a Windows 11 operating 
system  using  Python  3.8,  with  specific  hardware,  including  a  laptop 
possessing a 12th-Gen Intel Core i7-12700H GPU at 2.30 GHz and an 
NVIDIA GeForce RTX 3060 GPU. Due to the early stopping strategy, the 
training process converges before the last epoch, preventing overfitting 
issues. 

4.1. Model forecasting performance metrics 

4.3. Case 1: DO prediction in the Shandong peninsula

4.3.1. Data description

such as data gaps and noise, leading to inaccuracies. The slightly wider 
residual distribution of the Informer, which is efficient in long sequence 
data processing, may indicate significant random errors or insufficient 
pattern recognition for certain data points. Compared with those of the 
Reformer, the proposed model achieves a 24.66% improvement in the 
MAE and a 13.19% improvement in the RMSE. While the Reformer re-
duces the computational load, it may not be robust enough for handling

S = ⌊ 1
Ξ(f)

⌋

(25) 

During  training,  the  input  tensor  comprises  three  dimensions:  the 
batch  size,  the  number  of  model  variables,  and  the  future  prediction 
length. The dimension concerning the number of features is segmented 
by dynamically changing the slice sizes and strides, and the segmented 
tensors are folded, adding a slice dimension. Finally, by merging the first 

In the second stage of the attention calculation, the covariates are 
dynamically segmented, capturing their features at different time scales 
through multiscale attention. Specifically, for the static variables S(t), 
the multiscale attention FMSA
S(t)
)
(cid:0)
S(t) = Softmax
FMSA

is calculated as in Formula 30: 

Wf ,S⋅IS(λk)

(30) 

Similarly, for dynamic variables Z(t), the calculation is as in Formula 

31: 

(cid:0)
Z(t) = Softmax
FMSA

)

Wf ,Z⋅IZ(λk)

3.4.3. Attention fusion 

(31)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The training process uses an early stopping strategy to determine when training is complete. Early stopping is a form of regularization used to avoid overfitting by halting the training process before the model starts to fit the noise in the training data. In this case, the training process converges before the last epoch due to the implementation of early stopping, indicating that further training would not improve the model's performance significantly. Therefore, the criterion used to determine when training is complete is based on the convergence of the training process rather than a specific metric such as validation loss plateau.
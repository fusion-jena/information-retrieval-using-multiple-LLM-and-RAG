Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

As it has been proven, ELMs are an important approach for handling and analysing Big
Data as they require the minimum training time relative to the corresponding engineering
learning algorithms. Moreover ELMs do not require ﬁne manipulations to determine their
operating parameters and ﬁnally they can determine the appropriate output weights
towards the most eﬀective resolution of a problem. What is most important, they have
in contrast to corresponding methods which adjust their
the potential to generalize,

508

K. DEMERTZIS ET AL.

performance based solely on their training data set. It is obvious that the emerging use of
ELM in Big Data analysis as well as DELE creates serious prerequisites for complex systems’
development by low-cost machines.

8. Future research

ELMs use the SLFFNN’s general methodology, with the speciﬁcity that the Hidden Layer
(feature mapping) is not required to work in a coordinated fashion. All hidden-layer par-
ameters are independent from the activation functions and from the training data.

ELMs can randomly create hidden nodes or hidden level parameters, before seeing the
training data, while it is remarkable that they can handle non-diﬀerential activation
equations and they do not address known NN problems such as stopping criterion, learn-
ing rate and learning epochs (Cambria & Guang-Bin, 2013; Huang, 2014, 2015).

A mathematical basis has been provided for the understanding of ELM (Equations

(1)–(9)) (Cambria & Guang-Bin, 2013; Huang, 2014, 2015).

For an ELM using SLFFNN and random representation of hidden neurons, input data is
mapped to a random L-dimensional space with a discrete training set N, where
(xi,ti), i [ 1, N with xi [ Rd and ti [ Rc. The speciﬁcation output of the network is the
following:

View related articles 

View Crossmark data

Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=tjit20

JOURNAL OF INFORMATION AND TELECOMMUNICATION
2018, VOL. 2, NO. 4, 492–510
https://doi.org/10.1080/24751839.2018.1501542

Extreme deep learning in biosecurity: the case of machine
hearing for marine species identiﬁcation
Konstantinos Demertzisa, Lazaros S. Iliadisb and Vardis-Dimitris Anezakis

c

aSchool of Engineering, Department of Civil Engineering, Lab of Mathematics and Informatics, Democritus
University of Thrace, Xanthi, Greece; bSchool of Engineering, Department of Civil Engineering, Lab of
Mathematics and Informatics, Democritus University of Thrace, Xanthi, Greece; cDepartment of Forestry and
Management of the Environment and Natural Resources, Democritus University of Thrace, Orestiada, Greece

ARTICLE HISTORY
Received 30 November 2017
Accepted 14 July 2018

499

This is a multi-layered neural network model that receives successive OL data streams
and uses the unsupervised GRELMA algorithm as a basic building block in which the
outputs of each level are used as inputs to the next one (Sun, Zhang, Zhang, & Hu, 2017).
An autoencoder is an ANN used for unsupervised learning of eﬃcient coding. The aim
of an autoencoder is to learn a representation (encoding) for a set of data, but with the
output layer having the same number of nodes as the input layer, and with the
purpose of reconstructing its own inputs (instead of predicting target value Y given
inputs X). The algorithm is described below (Sun et al., 2017):

Algorithm 1. GRELMA Algorithm for Clustering (Sun et al., 2017)

i=1 the number of hidden neurons nh, the penalty coeﬃcient κ and λ

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The query asks about the criteria used to split the data for deep learning model training into different sets like train, test, and validation. However, the provided context does not explicitly mention any specific criteria for splitting the data. Nevertheless, some common practices for splitting data in deep learning include:

* Randomly splitting the dataset into three subsets: training, testing, and validation. Typically, the ratio used is 60% for training, 20% for testing, and 20% for validation. This ensures that the model is trained on a large enough dataset, tested on a separate dataset to evaluate its performance, and validated on another dataset to fine-tune hyperparameters.
* Stratified sampling: When dealing with imbalanced datasets, stratified sampling can be used to ensure that each subset contains approximately the same proportion of samples from each class. This helps prevent overfitting and improves the model's ability to generalize.
* Time series data: For time series data, a common practice is to use a rolling window approach, where the first n observations are used for training, the next m observations are used for validation, and the remaining observations are used for testing. This allows the model to capture temporal dependencies in the data.
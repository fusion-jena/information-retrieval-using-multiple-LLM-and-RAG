Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 2
Overview of our augmentation strategies, their IDs, and related information. For DenseNet-161 and ViT-B/16, their training times are provided in minutes per epoch
for the different augmentation methods. For IDs 5 and 6, we added noise of the bird audio detection data set from the DCASE challenge (Berger et al., 2018; Himawan
et al., 2018; Liaqat et al., 2018), which is abbreviated as BAD.

Data augmentation

Time domain

Frequency domain

Spectrogram

Training time [min. / epoch]

DenseNet-161

ViT-B/16

Gaussian noise
Pink noise (background)
Primary background noise soundscapes
Secondary background noise soundscapes
Primary background noise (BAD)
Secondary background noise (BAD)
Background noise (e.g., wind, thunder, or aircrafts)
Mixed up random bird species
Horizontal roll
Vertical roll
Pitch shift
Time mask
Frequency mask
Gain
Loudness normalization
Horizontal flip
Vertical flip
Time stretch
tanh-based distortion

✓
✓
✓
✓
✓
✓
✓

✓

✓
✓

✓

While the 2021 BirdCLEF challenge is hosted via Kaggle,9 we also
conducted local experiments. We evaluated our setup using general-
purpose graphics processing units (GPGPU). Our test environment is
composed solely of current consumer grade hardware. This test envi-
ronment encompasses similar to Schlosser et al. (2022) (i) our CPU,
“Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz” with 7200 BogoMips and
a maximum CPU load of 99%, (ii) our GPU, “TITAN RTX” with a
maximum GPU load of 99%, (iii) our working memory with 128 GB of
RAM, as well as (iv) our hard drive (SSD), “Samsung 970 EVO Plus SSD”
with 500 GB.

3. Test results, evaluation, and discussion

For the ViT models, we adapted the approach of pretraining deep
bidirectional transformers for language understanding (BERT) by Devlin
et al. (2018) for birdsong classification. Table 3 provides an overview of
the investigated ViT models with their respective parameterizations.
ViT-S/16, for example, denotes a ViT variant of reduced complexity with
an input patch size of 16 × 16. The sequence length of the transformer
models is inversely proportional to the square of the provided patch size.
In the following, ViT-B/16 is further investigated as it strikes a balance
between general model complexity and classification performance.

training or validation progress could be observed. To fine-tune our ViT
models, we deployed ImageNet-based weights for pretraining.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text mentions that the authors conducted local experiments using general-purpose graphics processing units (GPGPUs). Their test environment consists of current consumer-grade hardware, including an Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz with 7200 BogoMips, a TITAN RTX GPU, 128GB RAM, and a Samsung 970 EVO Plus SSD with 500GB storage capacity. Therefore, they use GPGPUs, specifically a TITAN RTX GPU, along with other hardware components like CPU, RAM, and SSD, for training their deep learning models.
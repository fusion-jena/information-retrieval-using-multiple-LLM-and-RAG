Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

across all specimens regardless of their body size. As much as possible,

is, when the input subclass (species and genera within a family) is not

specimens were positioned for photography in dorsal view.

present in the training data. Various parameters are tested that may

Bulk-sample photographs were taken using a Zeiss AXIO Zoom.V16

affect the prediction accuracy, including: (i) the size of the training set;

Stereo Zoom Microscope equipped with a Zeiss AxioCam HRc (High

(ii) the complexity of the training set, which may be affected by the

Resolution 13 Megapixels Colour Microscope) camera at the Imaging

level of intra-class variability, noise from misidentifications, or the

and Analysis Centre at the Natural History Museum (NHM) in London,

presence of out-of-distribution samples; and (iii) the quality of images,

United Kingdom. This instrument has a motorized focus drive and

for example, the resolution of the image using standard macrophotog-

F I G U R E 3

The effect of increasing numbers of training images on

(2.2% reduction for DANN,
predictions by the plain NN model
p = 0.0002, Figure S4). In the GH!LL prediction, a similar trend was
observed (Figure S5) and the target accuracy was not significantly dif-
ferent from the NN (0.015% reduction for DANN, p = 0.98). Loss and

accuracy development during training of models are reported in

Figures S2, S3.

Classification error

Classification error was visualized as a scaled confusion matrix. Starting

with a trial for a within-dataset analysis with 400 training images in the

LH random sampling showed that the large taxonomic groups were cor-
rectly classified in most cases (Table S4). For example, four families

In order to test the effect of the presence of unknown inputs (out-of-

distribution samples) on the classification, we first used an LH-trained
model to predict the class of 16 LH images belonging to eight families/
subfamilies, Coccinellidae, Elateridae, Endomychidae, Hydrophilidae,

datasets. Conversely to the above analyses, here the model was

tion to the classifier (Figure S1). The training procedure then optimizes

trained to classify datasets instead of taxa. Then, a classification error
of the SVM, Îµsource(cid:4)target, was measured as a proportion of incorrect
predictions of 200 test images sampled from the two datasets. An

the model parameters in the shared part of the network to not only

minimize the loss of the label classifier (taxon prediction) but at the

same time to maximize the loss of the domain classifier (dataset predic-

intuitive interpretation of this measure is that the dataset classifica-

tion). This adversarial training procedure optimizes shared intermediate

tion task is harder when the feature distributions between two data-

features to be invariant between the two domains, and hence the

sets are more similar. Therefore, a large classification error indicates

model can generalize across them, which potentially improves the accu-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training, several postprocessing steps can be performed depending on the specific requirements of the project. Some common postprocessing steps include:

1. Model Evaluation: After training, it is essential to evaluate the performance of the model using various metrics such as accuracy, precision, recall, F1 score, etc. These metrics help determine how well the model performs on unseen data.
2. Confusion Matrix: A confusion matrix is a table that summarizes the predictions made by the model compared to the actual values. It provides insights into the types of errors made by the model and helps identify areas where the model needs improvement.
3. Saliency Maps: Saliency maps highlight the regions of an input image that contribute the most to the output prediction. They provide insight into what features the model focuses on while making its predictions.
4. Visualization Techniques: Various visualization techniques like t-SNE, PCA, etc., can be used to understand the high dimensional representations learned by the model.
5. Hyperparameter Tuning: Postprocessing also involves tuning hyperparameters to improve the model's performance further. Grid search, random search, Bayesian optimization, etc., are some commonly used methods for hyperparameter tuning.

Based on the given context, it appears that the researchers have evaluated the performance of their model using various metrics such as accuracy, loss, and classification error. They have also created a scaled confusion matrix to visualize the classification error. However, there is no mention of saliency maps or any other visualization techniques being used. Additionally, it is unclear whether any hyperparameter tuning was done post-training.
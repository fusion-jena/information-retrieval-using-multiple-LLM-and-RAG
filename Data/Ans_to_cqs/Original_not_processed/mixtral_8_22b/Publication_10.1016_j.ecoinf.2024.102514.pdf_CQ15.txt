Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Fig. 9 presents the RMSE and MAE for the eight ML algorithms. The 
majority of performance indicators were found to be higher during the 
training period; however, during the testing period, model performance 
was shown to have greatly improved, which may be due to the under-
fitting problem (Table S4 - supplementary data) (Wang et al., 2022). In 

addition,  the  lowest  prediction  errors  during  the  training  and  testing 
periods belong to GPR, LR, NN, and SVM (Fig. 9).

erquality-iiwq/wq-challenge. 

Vu, H.L., Ng, K.T.W., Richter, A., An, C., 2022. Analysis of input set characteristics and 
variances on k-fold cross validation for a recurrent neural network model on waste 
disposal rate estimation. J. Environ. Manag. 311 (October 2021), 114869 https:// 
doi.org/10.1016/j.jenvman.2022.114869. 

Wainer, J., Cawley, G., 2021. Nested cross-validation when selecting classifiers is 

overzealous for most practical applications. Expert Syst. Appl. 182 (May), 115222 
https://doi.org/10.1016/j.eswa.2021.115222. 

Wang, Y., Zhao, Y., Xu, S., 2022. Application of VNIR and machine learning technologies 
to predict heavy metals in soil and pollution indices in mining areas. J. Soils 
Sediments 22 (10), 2777â€“2791. https://doi.org/10.1007/s11368-022-03263-3. 

Whitehead, P., Bussi, G., Hossain, M.A., Dolk, M., Das, P., Comber, S., Peters, R., 

Charles, K.J., Hope, R., Hossain, S., 2018. Restoring water quality in the polluted

methodology  of  Uddin et  al.  (2022a),  to evaluate the  performance of 
various  ML  algorithms.  Details  of  the  methodology  can  be  found  in 
referred material. This approach is widely used in modelling tasks like 
water  resource  management,  waste  management,  natural  hazard  pre-
diction, and environmental modelling (Gidey, 2018; Pham et al., 2020; 
Vu et al., 2022; Zhao et al., 2021). It offers high precision by utilizing ten 
subsets of the original dataset for both training and testing (Sajib et al.,

MAE  shed  light  on  prediction  accuracy,  R2  measures  goodness  of  fit, 
while NSE and MEF assess performance relative to benchmarks. Z sta-
tistics determine statistical significance, and Taylor diagrams provide a 
holistic visualization of multiple metrics. These metrics were thought-
fully  chosen  to  offer  a  comprehensive  evaluation  of  the  algorithms, 
enabling a multifaceted assessment of their performance. Furthermore, 
these  performance  metrics  offer  distinct  perspectives,  allowing  a 
tailored evaluation based on specific modelling objectives and dataset 
characteristics (Bui et al., 2020; Ding et al., 2023; Georgescu et al., 2023; 
Uddin  et  al.,  2022a,  2022b,  2023a,  2023b,  2023c,  2023d,  2023e, 
2023f).  Notably,  recent  water  research  studies  have  successfully 
employed these metrics for evaluating prediction performance in order 
to accurate WQ prediction (Asadollah et al., 2021; Das et al., 2022; Ding

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not explicitly mention any optimization techniques applied in the deep learning pipeline such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, it discusses various Machine Learning (ML) algorithms including Recurrent Neural Network (RNN) models, which often use optimization techniques like SGD or Adam for updating weights during backpropagation. Additionally, the context mentions the use of nested cross-validation, which is a technique used for hyperparameter tuning and model selection, but this doesn't directly relate to optimization techniques within the deep learning pipeline itself. Therefore, while we cannot definitively state what optimization techniques are being used from the provided information, common practices suggest that techniques like SGD or Adam could potentially be employed.
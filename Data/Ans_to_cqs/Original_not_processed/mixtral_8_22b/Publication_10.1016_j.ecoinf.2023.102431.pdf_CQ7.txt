Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1 
Sample sizes for training, test, and validation groups for discriminant analysis of 
species and sex.   

Species Discrimination 

Fisher Sex Discrimination  

Marten 
Tracks 

Fisher 
Tracks 

Training 
Validation 
Test 
Total 

82 
43 
42 
167 

184 
92 
91 
367 

Individuals 
Total (Male/ 
Female) 

29 (17/12) 
19 (9/10) 
15 (7/8) 
63 (33/30) 

Tracks 
Total (Male/ 
Female) 

169 (72/97) 
99 (55/44) 
99 (50/49) 
367 (177/190)  

EcologicalInformatics79(2024)1024315J.M. Tucker et al.

(3) 

Values above this line (>129.29) would classify the track as male and 
below the line as female. We found this simple sex discrimination model 
produced comparable classification accuracy to the more complex full 
model with some slight increase in misclassification for the training and 
validation  sets  (a  difference  of  1â€“2  individual  tracks),  but  a  modest 
improvement in the test set. (Table 3). 

Fig. 4. Illustration of the FIT calculated variables important in species and fisher sex discrimination. These are 6 of the 124 calculated variables (lengths, angles, and 
areas) generated by FIT. White circles numbered 1 to 7 represent the landmark points placed in the approximate center of each track pad in the feature extraction 
process. Fig. A: 5 top predictor variables for both species and sex discrimination, Fig. B: Final 2 variables in the simple model for fisher sex discrimination.

We  used  a  stepwise  variable  selection  procedure  with  the  p-value 
serving as the decisive factor (threshold of 0.05) to select the variables 
that had the best discriminating power based on their F-ratios. We used 
linear  discriminant  analysis  to  generate  a  discrimination  line  (2  vari-
ables) or hyperplane (>3 variables) depending on the number of vari-
ables  in  the  resulting  model  to  classify  species  or  sex.  To  quantify 
uncertainty  in  classification,  we  then  generated  bands  of  uncertainty 
around  this  discrimination  hyperplane.  We  assigned  values  that  fall 
within these bands a probability value (p) indicating a level of certainty 
about the classification using a logistic regression model, as in Eq. (1) 

p =

1
1 + e(cid:0) 1

2 f ( x)

(1)

To identify variables with the greatest potential to discriminate be-
tween classes of objects (e.g., species or fisher sex) we used a predictor 
screening analysis that uses bootstrap forest partitioning to evaluate the 
contribution  of  predictors  on  the  response  object.  For  each  response 
object  we  employed  predictor  screening  using  1000  decision  trees  to 
evaluate all 124 potential FIT metrics. We then used the top predictor 
variables to evaluate the dataset for two potential sources of bias, 1) the 
use of mirrored right front tracks instead of only left front tracks (foot 
position  bias)  and  2)  the  potential  for  bias  between  observers  when 
manually  marking  the  landmark  points  in  the  FIT  interface (observer 
bias) which is a somewhat subjective process. Foot position (left-front or 
right-front) of each track was recorded for each track during the feature 
extraction  process.  To  evaluate  observer  bias  we  had  3  different  ob-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The table titled "Table 1 Sample sizes for training, test, and validation groups for discriminant analysis" provides information about how the data was divided into three subsets: training, validation, and testing. In total, there were 167 samples for Marten Tracks and 367 samples for Fisher Tracks. Each subset has its own purpose in building and evaluating the performance of the deep learning models. The training set is used to fit the parameters of the model, while the validation set is used to tune the hyperparameters of the model. Finally, the test set is used to assess the final performance of the fully trained model.
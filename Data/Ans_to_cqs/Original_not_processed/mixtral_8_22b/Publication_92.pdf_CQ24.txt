Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Finally, in order to improve the localization accuracy, we decided to create
another class called part of ﬁsh, to ensure that the network does not focus on
a distinctive part of a ﬁsh as a stripe, a ﬁn, the head, but processes the ﬁsh
as a whole. We also created a class ﬁsh which contains some unknown ﬁshes to
make the method able to recognize any ﬁsh even though it is not in the learning
database. However, this class must contain less samples in order to be sure that
a ﬁsh will most likely be labeled by its speciﬁc class rather than the generic class
ﬁsh. Finally, we added 3 classes to our initial thumbnail database as listed in
Table 2.

Table 2. Classes added to the species database

Class Label

Samples

Random/speciﬁc background 116,820/91,247

Part of Fish

Fish

55,848

970

Coral Reef Fish Detection and Recognition in Underwater Videos

165

3.2 Detection/Recognition Pipeline

19. Atkinson, P.M., Tatnall, A.R.L.: Introduction neural networks in remote sensing.

Int. J. Remote Sens. 18(4), 699–709 (1997)

20. Schmidhuber, J.: Deep learning in neural networks: an overview. Neural Netw. 61,

85–117 (2015)

21. Lecun, Y., Bottou, L., Bengio, Y., et al.: Gradient-based learning applied to doc-

ument recognition. Proc. IEEE 86(11), 2278–2324 (1998)

22. Szegedy, C., Liu, W., Jia, Y.: Going deeper with convolutions. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9 (2015)
23. Joly, A., et al.: LifeCLEF 2015: multimedia life species identiﬁcation challenges.
In: Mothe, J., Savoy, J., Kamps, J., Pinel-Sauvagnat, K., Jones, G.J.F., SanJuan,
E., Cappellato, L., Ferro, N. (eds.) CLEF 2015. LNCS, vol. 9283, pp. 462–483.
Springer, Heidelberg (2015). doi:10.1007/978-3-319-24027-5 46

Deep Learning. The architecture of our network follows the GoogLeNet’s
with 27 layers, 9 inception layers, and a soft-max classiﬁer. Once we have a list
of cropped thumbnails and their labels, we send them to our network. We use
inception layers (Fig. 4) based on GoogLeNet architecture [22]. The inceptions
here allows us to reduce the dimension of the picture to one pixel, and therefore
not to be dependent of the dimensional impact. We adapted some parameters
such as the size of the strides and the ﬁrst convolutions adapted to the size of our
thumbnails, which allowed us to achieve better results than a classic architecture
(e.g. [18]).

3.3 Post-processing and Bounding Box Fusion

For each sliding window, we deﬁne a motion score by computing the average
absolute diﬀerence with the window at the same position in the previous frame.
Based on the hypothesis that most of the ﬁshes are moving, we use this score
for the ﬁnal detection decision.

We will use the back-propagation. Given a feature vector representing an
object from class k ∈ {1..K} as network input, we compare the expected value
(100 % of probability to belong to class k) to the results obtained by the network,
and we compute the error of the output layer. Then, the error is back-propagated
from the layer j to the neurons of the layer j − 1. Finally, the weight of each neu-
ron is updated according to a gradient-based descent in order to get a computed
value closer to the expected value [20].

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, there are general strategies used in deep learning pipelines to manage randomness:

1. Seed Initialization: Random seeds can be initialized before training the model to ensure reproducibility. This means setting a specific seed value for all random operations within the code, including data shuffling, dropout regularization, and weight initialization. By doing so, the model's performance should remain consistent across multiple runs.

2. Data Shuffling: To prevent overfitting and ensure that the model learns from diverse examples, datasets are often randomly shuffled during training. This introduces randomness into the process, but it can be controlled by using a fixed seed value or ensuring that the dataset is consistently shuffled in the same way between runs.

3. Dropout Regularization: Dropout is a technique used to prevent overfitting by randomly dropping out nodes during training. While this adds randomness to the process, it can be managed by using a fixed seed value or applying dropout consistently across different runs.

4. Weight Initialization: During the initialization phase, weights are assigned random values. To maintain consistency across runs, these weights can be initialized using a fixed seed value or a deterministic algorithm like Xavier or He initialization.

While the given text does not explicitly mention these strategies, they are commonly used in deep learning pipelines to manage randomness and ensure reproducible results.
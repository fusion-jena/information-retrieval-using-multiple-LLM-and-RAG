Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

To  determine  the  optimal  data-driven  model,  the  conventional 
model performances were compared with the estimation performances 
of the CNN and the CNN with the autoencoder. The ANN model uses 
hidden nodes in hidden layers with learnable weights to extract input 
data. The principles of feature extraction and model training are similar 
to those of the autoencoder and CNN models. Six hidden layers were 
designed  with  128,  128,  256,  256,  512,  and  512  nodes.  The  output 
layers with three nodes estimated the three indices. The loss function 
and optimizer were set as the mean squared error and Adam optimizer, 
respectively, at a learning rate of 0.001. The SVM regression model was 
designed to estimate TDI, BMI, and FAI. The kernel function, C-param-
eter, and gamma of the SVM were set to radial basis functions of 100 and 
0.1, respectively. The SVM was unable to estimate the indices simulta-
neously. The RF uses an ensemble regressor by applying multiple deci-

2.4.2. Convolutional neural network 

CNN is a type of deep learning algorithm primarily used for image 
recognition and processing tasks (Chauhan et al., 2018). It’s composed 
of multiple layers, including convolutional layers that apply kernels to 
extract  features  from  input  images,  followed  by  pooling  layers  that 
reduce  the  spatial  dimensions  of  the  features.  CNNs  are  designed  to 
automatically learn hierarchical representations of data, making them 
highly effective for tasks like object detection, image classification, and 
even natural language processing (Khan et al., 2020). That is, the CNN 
architecture  was  designed  to  efficiently  process  complex  datasets 
(Alshehhi et al., 2017). Convolutional layers adopt internal kernel op-
erations that calculate nonlinear features using trainable weights and

12.282, and 13.897 for the validation (Fig. 5a-f). Table 3 lists the mean 
and standard deviation values of the NSE and RMSE driven by the 10- 
fold  cross-validation  of  the  deep  learning  models.  The  CNN  model 
with  the  autoencoder  showed  better  estimation  performance  for  TDI, 
BMI, and FAI than the CNN model estimation without the autoencoder

This  study  used  evaluation  metrics  to  estimate  aquatic  ecosystem 
health  indices  (Moriasi  et  al.,  2007).  The  Nash–Sutcliffe  efficiency 
(NSE), Root Mean Squared Error (RMSE), and percentage bias (PBIAS) 
were  used  to  measure  the  accuracy  of  the  performance  of  the  deep 
learning  models, including the  CNN model and  CNN with an  autoen-
coder, and conventional machine learning models with ANN, SVM, RF, 
and  XGBoost  for  estimating  TDI,  BMI,  and  FAI  values.  The  NSE  is  a 
commonly  used  score  in  hydrological  and  environmental  models  and 
measures the relative magnitude of the residual variance, indicating the 
degree  of  1:1  line  fitness  of  the  observed  versus  estimated  data  plot 
(Moriasi  et  al.,  2007;  Nash  and  Sutcliffe,  1970).  NSE  ranges  from 
negative infinity to 1, and the mean observed value is used as a refer-
ence.  In  other  words,  an  NSE  below  zero  implies  that  the  estimation

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In this study, the optimization technique applied in the deep learning pipeline is the Adam optimizer. This optimizer is used with a learning rate of 0.001. The Adam optimizer is a popular choice for deep learning applications due to its ability to adaptively adjust the learning rate based on the first and second moments of the gradients. Additionally, it has been shown to perform well across various types of problems and architectures. Other commonly used optimization techniques include Stochastic Gradient Descent (SGD), AdaGrad, and RMSProp. However, these methods are not mentioned or utilized in this particular study.
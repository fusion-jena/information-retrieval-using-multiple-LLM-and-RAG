Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

D. Model Training

Feature Extraction: The hardware to train the model included
a Lenovo laptop equipped with an 8265U CPU at 1.80 GHz
of Intel Core i5, 8 GB of RAM running on a Windows 10 64-
bit system. The software tools included Annaconda3 Jupyter
Notebook with Python 3.8, where the Tensorﬂow, OpenCV-
python3 [37] and Keras [38] libraries were used. The dataset
was divided in a ratio of 75:25 into a training set of 5789
images and a test set of 1930 images. To obtain the pre-
trained weight parameters, we built the base models from the
MobileNetV2, MobileNetV3-Large and MobileNetV3-Small

Authorized licensed use limited to: Thueringer Universitaets. Downloaded on August 29,2024 at 05:51:17 UTC from IEEE Xplore.  Restrictions apply.

of the model was reduced to 60%, trainable parameters were
declined to 188,600 and training, and validation accuracies
were between 92%-94% and 82%-86%, respectively.

Fig. 2. Training and Validation Accuracy of MNV2/MNV3-based Transfer
Learned Models.

Fig. 3. Training and Validation Accuracy of MNV2/MNV3-based Fine-tuned
Models.

Model Validation: The performance of the models is greatly
inﬂuenced by the number of times it goes under training.
Using sparse categorical cross-entropy loss, the models were
validated with a test dataset. The performances of the models
were evaluated by calculating accuracies and losses of training
samples in each epoch. The model based on MobileNetV2
showed (Fig. 2) the training accuracy up to 97% and validation
accuracy up to 84%. The model based on MobileNetV3-Large
exhibit a training accuracy of 94% and validation accuracy up
to 87%.
Fine-Tuning Results: After ﬁne-tuning, the training accuracy
of the MobileNetV2-based model was improved to 97%,
and validation accuracy remained at 86% at each epoch.
The training accuracy of the MobileNetV3-based model was
improved to up to 96% (Fig. 3), and validation accuracy up
to 89% after 10 epochs. Note the gap between training and
validation curves for the MobileNetV3-based model is less
than the MobileNetV2-based model.

optimizer (SGD) with a learning rate of 0.5 and momentum
of 0.9. All models were trained on a training set within 10
and 100 epochs (iterations).
Fine-Tuning: We un-froze the base models and trained the
weights of the top layers of these models along with the
classiﬁer layer added in the previous step. The models were
trained in 10 iterations to force the weights to be tuned from
generic feature maps to feature associated with the leaf dataset.
For a fair comparison, both V2 and V3 models were trained
with the same dataset and training parameters, i.e. 235,704.
The models were ﬁne-tuned and re-evaluated. The models
were saved in the machine with all the class labels.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The text does not explicitly state the specific criteria used to determine when training is complete. However, it can be inferred that the models were trained for a fixed number of epochs, ranging from 10 to 100. Additionally, the performance of the models was evaluated by calculating accuracies and losses of training samples in each epoch. This suggests that the authors might have monitored the training and validation accuracy and loss during the training process to decide when to stop training. A common practice is to stop training when the validation loss stops decreasing or starts increasing, indicating overfitting. However, without explicit information about the stopping criterion, this remains speculative.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

total time (min) 

15 
15 
25 
25 
50 
50 
100 
100 
150 
150 
200 
200 
100 
100 
16,320  

used for each software in each step of the data processing to allow for 
direct comparisons. Therefore, the same results would be expected for 
all the software programs if they worked to perfection.

All the parameters used for all the recognizers are shown in Table S2. 

2.6. Data analysis: Target set 

The analyses of the target set aimed to detect possible differences 
among the software programs in evaluating the results with regard to 
two different aspects: (1) the possibility that different habitats produce 
more or fewer song detections per day and (2) the possibility that great 
tits in different habitats have different times of vocal activity, regardless 
of the total number of songs. Given that the days and sites used to obtain 
the  data  for  the  training  and  target  sets  were  the  same  for  all  three 
software programs, any differences among the software programs must 
be exclusively due to differences or biases in their detection patterns. On 
the other hand, for the general comparison of the software, we measured 
the  time  each  software  spent  on  analyzing  the  entire  target  dataset 
(32,640 recordings).

each detection and saved the file (the best examples of target vocaliza-
tions were labeled and the general cluster names were left for the vo-
calizations of no interest, while the poor quality or confused examples of 
target  vocalizations  were  not  labeled  because  they  were  going  to  be 
excluded from the training data. All these changes in the “Manual ID” 
column determine the criteria that Kaleidoscope Pro must follow in the 
model.); (6) we re-ran the recording and the edited cluster analysis to 
create an advanced classifier (i.e., the recognizer, a ‘.kcs’ file); (7) we ran 
the recognizer on the test set to evaluate its performance; and (8) we 
applied  the  same  recognizer  to  the  target  set  to  obtain  biologically 
relevant results.

2.3. Data processing 

In  this  study,  we  reproduced  the  usual  steps  of  a  single-species 
bioacoustics  study: (1)  we  optimized  a  recognizer  via  automated  and 
manual processes; (2) we applied it to a set of manually validated re-
cordings  to  evaluate  its  performance;  and  finally  (3)  we  applied  the 
recognizer to a large set of recordings, which usually is the real aim of 
the  study.  For  this,  we  created  eight  different  subsets  of  recordings 
(Table 1): six different training sets that included varying amounts of 
cases, a test set, and a target set of recordings. The same sets were always 

EcologicalInformatics79(2024)1023972M. Goitia-Urdiain et al.                                                                                                                                                                                                                        

Table 1 
Sets of recordings used in the study.  

description

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there isn't enough detail about the specific postprocessing steps taken after the model training. However, some insights can be derived from the process described in the text. After creating an advanced classifier (i.e., the recognizer), the researchers evaluated its performance by running it on the test set. Then, they applied the same recognizer to the target set to obtain biologically relevant results. This suggests that the main postprocessing steps might involve applying the trained model to new datasets and interpreting the results based on biological relevance. Unfortunately, without more detailed information, I cannot provide further insight into specific techniques such as saliency maps, metrics calculations, or confusion matrices.
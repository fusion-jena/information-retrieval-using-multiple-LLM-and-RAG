Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.3. Evaluation 

The  evaluation  of  our  models  was  a  crucial  step  in  analyzing  the 
ability of the state-of-the-art deep learning models to detect and identify 
Collembola on microscope slides. To do so we proceed as follows, first, 
we made predictions on an evaluation dataset that we've extracted from 
the main dataset. This ensures that the model has never encountered this 
particular dataset during its training phase. The predictions-annotations 
matching is done by calculating the IoU metric Eq. 1 for each possible 
pair  and  matching  them  based  on  it.  The  IoU  Eq.  1  is  a  metric  that 
quantifies the overlap between the annotation and the prediction box. 
We  considered  that  an  IoU  of  0.5  between  the  prediction  and  the 
annotation  is  a  match.  If  a  prediction  didn't  match  any  annotation 
human annotation, it was considered to be a background element. 

IoU =

Area of Intersection
Area of Union

(1)

Since  we  had  multiple  species,  we  also  used  the  mAP  Eq.  4  (Mean 
average precision) which gives a score for the whole model whereas the 
AP will give one for each one of them. The objective of these models is to 
be  reliable  enough  to  be  used  by  experts  in  real-life  conditions.  This 
means that we had to be confident enough in the model prediction to use 
it. We allowed ourselves 5% of mistakes in the prediction of each species 
of interest. We created a benchmark to compare the results of state-of- 
the-art models of deep learning on this task. To create a benchmark of 
models, we use the mAP to analyze which model does best on average 
for each species and the AP to analyze how good it is at predicting each 
species. To ensure the model's ability to reach 95% precision, we used

relevant regions within the images. ROI pooling aids in the selection and 
analysis  of  specific  image  regions  for  accurate  object  classification.  2 
different  versions  of  Faster  R-CNN  were  created  for  the  benchmark, 
Faster R-CNN (640) with an input dimension of 640 × 640 and Faster R- 
CNN (1280) with an input dimension of 1280 × 1280. They both used 
Resnet50 as a backbone, have 41 m parameters, and are pre-trained on 
ImageNet.  Both  those  dimensions  were  chosen  to  match  the  Yolov5 
input dimensions. Still, after multiple tests, the 640-version had incon-
clusive results. This can be explained by the important features being too 
small using this resolution for Faster R-CNN. Only the 1280 version was 
used in the benchmark.

of  the  model to be  correct when  predicting a  species. The higher  the 
precision for a species, the more confident the model is to be correct 
when  predicting  it.  From  these  two  metrics,  we  calculated  a  more 
advanced  metric,  the  precision-recall  curve,  which  is  a  graph  repre-
senting the balance between the recall and the precision, depending on

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Two primary metrics are used to evaluate the performance of the deep learning model: Intersection over Union (IoU) and Mean Average Precision (mAP).

Intersection over Union (IoU), defined by Equation 1, quantifies the overlap between the annotation and the prediction box. A threshold of 0.5 IoU is set to consider a prediction-annotation pair as a match. Any unmatched predictions are regarded as background elements.

Mean Average Precision (mAP), given by Equation 4, provides a single score for the entire model rather than individual scores for each class, like Average Precision (AP). The goal is to achieve a minimum of 95% precision for each species of interest.

Additionally, precision and recall are mentioned but not explicitly stated as evaluation metrics. However, they contribute to the calculation of another advanced metric called the precision-recall curve. This curve represents the tradeoff between recall and precision, depending on the confidence threshold applied to the model's output probabilities.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Lafferty JD, Williams CKI, Shawe-Taylor J, Zemel RS, Culotta A, editors. 
Advances in neural information processing systems 23. Curran Associates 
Inc; 2010. p. 1324–32. http:// papers. nips. cc/ paper/ 4043- learn ing- to- 
count- objec ts- in- images. pdf

 23.  Naudé JJ, Joubert D. The aerial elephant dataset Zenodo. 2019. https:// 

doi. org/ 10. 5281/ zenodo. 32347 80.

 24.  Ronneberger O, Fischer P, Brox T. U-net: convolutional networks for 
biomedical image segmentation; 2015. CoRR arXiv: 1505. 04597.

 25.  Naude J, Joubert D. The aerial elephant dataset: A new public benchmark 
for aerial object detection. In: Proceedings of the IEEE/CVF conference on 
computer vision and pattern recognition (CVPR) Workshops; 2019.
 26.  Oñoro D, López-Sastre R. Towards perspective-free object counting with 
deep learning, vol. 9911. Berlin: Springer; 2016. https:// doi. org/ 10. 1007/ 
978-3- 319- 46478-7_ 38.

 27.  Tan M, Le QV. Efficientnet: Rethinking model scaling for convolutional

Page 6 of 10

EfficientNet-B5  feature  extractor  [27].  EfficientNet  is  a 
CNN  developed  by  Google,  characterized  by  high  accu-
racy  and  computational  efficiency.  Model-2  was  initial-
ized  by  pre-trained  weights  based  on  the  Imagenet  data 
set  [28].  All  the  parameters  were  optimized  using  the 
Adam optimizer with a learning rate of 0.001.

Training

An  Nvidia  GeForce  RTX  2060  GPU  was  used  for  train-
ing,  with  a  batch  size  of  8.  Model-1  (without  feature 
extractor)  was  trained  for  7  h  and  Model-2  (with  a  fea-
ture  extractor)  for  17  h.  Based  on  the  model’s  perfor-
mance on the validation set, the early stopping technique 
was  applied  to  avoid  over-fitting.  Model-2,  which  used 

pre-trained  weights  and  thus  some  prior  relevant  infor-
mation, converged faster than Model-1 (Fig. 3).

Model evaluation: testing

2018. CoRR arXiv: 1807. 11809

 16.  Kamilaris A., van den Brink C., Karatsiolis S. (2019) Training Deep Learning 
Models via Synthetic Data: Application in Unmanned Aerial Vehicles. 
In: Vento M. et al. (eds) Computer Analysis of Images and Patterns. CAIP 
2019. Communications in Computer and Information Science, vol 1089. 
Springer, Cham. https:// doi. org/ 10. 1007/ 978-3- 030- 29930-9_8

 17.  Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, 
Courville A, Bengio Y. Generative adversarial nets. In: Proceedings of the 
27th international conference on neural information processing systems. 
NIPS’14. MIT Press, Cambridge, MA, USA; 2014, vol. 2, p. 2672–2680.
 18.  Young-Chul Yoon K-JY. Animal detection in huge air-view images using 

cnn-based sliding window. In: International workshop on frontiers of 
computer vision (IWFCV); 2018.

layers  of  the  DL  models  employed.  Specifically,  the 
feed-forward  regression  networks  in  [11,  26]  compress 
and  encode  images  into  smaller  representation  vectors, 
while the combination of CCNN and ResNeXt models in 
[11]  takes  an  input  image  of  size 72 × 72  and  produces 
an  output  density  map  of  size  18 × 18 .  To  overcome 
this  potential  information  loss,  the  UNet  model  was 
employed  as  the  learning  model  [24].  UNet  is  a  CNN 
architecture  originally  proposed  for  biomedical  image 
segmentation,  based  on  an  encoder–decoder-type  net-
work.  The  name  of  the  architecture  is  derived  from  its 
distinctive ‘U’ shape. The down-sampling/encoder block 
encodes the input images into feature representations at 
multiple  level,  capturing  the  context  information  from 
the  image. The  up-sampling/decoder  block  decodes  the 
feature  maps  learned  from  the  encoder.  The  symmet-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The data repository link of the deep learning pipeline can be found in reference number 25, which states "Naude J, Joubert D. The aerial elephant dataset: A new public benchmark for aerial object detection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR) Workshops; 2019." However, this does not provide a direct link to the dataset.

To find the actual link to the dataset, we need to refer to reference number 22, which provides the DOI for the dataset: "22. Naudé JJ, Joubert D. The aerial elephant dataset Zenodo. 2019. <https://doi.org/10.5281/zenodo.3234780>" This DOI leads us to the dataset hosted on Zenodo, which is a general-purpose open-access repository developed under the European OpenAIRE program and operated by CERN.

Therefore, the data repository link of the deep learning pipeline is <https://doi.org/10.5281/zenodo.3234780>.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Therefore, it is kept constant at 10% for every experiment of IFTL. 
This value of γ  (a hyper-parameter) is experimentally chosen and then 
kept  constant  at  10%  for  all  experiments  of  IFTL.  When  γ = 0,  then 
MFJF = PLjF, which represents the prediction done by TUR. If Eq. (19), 
results are in a negative value, then we discard it, and no update is done. 
NDm
is a set of those instances from the mixture set (Dm), that are the K- 
j 
nearest neighbors  of  the jth  instance  of  the  target domain.  As  nearest 
neighbors are taken from the mixture of all the intuitionistically fuzzi-
fied  instances  of  the  target  and  the  source  domain,  i.e.,  Dm,  δPjF  is 
calculated  by  two  ways:  First,  if  x0  in  set  NDm
belongs  to  the  source 
j 
domain (Ds), whose labels for Fth 
(x0) are available, then δPjF  is 

class, μYF

Further  discussions  analyze  IFTL  and  FTL  for  their  abilities  to 
improve learning by restricting overconfidence (controlling hesitancy) 
during TL by considering GRNN and SVR as the TUR. Overconfidence in 
ELM,  GRNN,  or  SVR  arises  when  they  make  predictions  on  a  dataset 
(target  domain) that has a huge data distribution difference from the 
source domain data on which they are trained. In this scenario, they just 
use their training experience to make predictions during testing without 
considering the distribution divergence in the testing dataset from the 
training  dataset.  We  conclude  this  section  with  the  execution  time 
analysis of the approaches.  

a)  GDP prediction using only CO2 emission data2

b)  IFTL Refinement over GRNN and SVR 

In previous results, ELM is chosen as TUR, and IFTL is implemented 
on the ELM predictions to improve its knowledge transfer. Here, GRNN 
and SVR are chosen one by one as a TUR model in the process described 
in Fig. 2. Then, IFTL with the same parameter settings (as was in the case 
of ELM in the previous subsection, where γ = 0.1) is applied over their 
prediction. The improvement of IFTL over these approaches are depicted 
in Fig. 6, where ‘GRNN_IF’  denotes the error in IFTL prediction when 
IFTL  is  implemented,  choosing  GRNN  as  TUR.  Similarly,  ‘SVR_IF’  de-
notes the IFTL error when IFTL is implemented, choosing SVR as TUR. 

EcologicalInformatics77(2023)1022069S. Kumar et al.

This  tightly  coupled  relationship  between  CO2  emission  and  GDP 
motivated us to predict the GDP of a nation using its carbon emission. 
However,  the  available  dataset  of  some  countries  is  not  sufficient 
enough to train a machine learning (ML) model. To overcome this lim-
itation,  this  paper  proposes  a  novel  transfer  learning  methodology. 
Transfer  learning  (TL),  a  machine  learning  (ML)  technique,  utilizes 
previously learned information to resolve new tasks, more specifically, 
related tasks. In conventional ML techniques, every new task is learned 
from the scratch (Che et al., 2021), and the training and test data are 
drawn from the same distribution. However, if the data distribution is 
changed between the training and test data, the performance and reli-
ability  of  the  predicted  output  may  degrade  significantly  (Lu  et  al., 
2019). This may also lead to high computational costs and inappropriate

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide specific details about the hyperparameters used in a deep learning model such as learning rate or optimizer. However, it mentions that there is a hyperparameter called gamma (γ) which is experimentally chosen and kept constant at 10% for all experiments of IFTL (Intuitionistic Fuzzy Transfer Learning). It seems that this hyperparameter plays a role in controlling hesitancy during transfer learning by considering GRNN (General Regression Neural Network) and SVR (Support Vector Regression) as the TUR (Target Underlying Regressor). Unfortunately, the context does not provide further details regarding other hyperparameters typically used in deep learning models.
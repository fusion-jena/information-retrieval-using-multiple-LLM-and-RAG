Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

From a coding perspective, training an SVM is extremely straight-
forward, with training, optimisation and testing executed within only a 
few simple lines of code. The relative complexity with the CNN þ SVM 
approach,  and  by  extension  the  CNN  approach,  rather  lies  in  data 
management and feature extraction. The complexity of these steps is not 
so  much  related to the  actions required, but navigating  the extensive 
literature and knowing “where to start”. As one of the contributions of 
this paper we hope to better guide the user with clear and detailed de-
scriptions  of  these  steps,  that  are  applicable  across  ML  frameworks 
(Table 3). Aside from these preparations, training the CNN classifier is 
further complicated since an optimal is not automatically found. Instead 
performance  metrics  must  be  monitored  across  epochs  and  decisions 
made on when to stop training - typically the point at which training and

screening tool to improve quality of the ground-truth. Here although the 
task  is  a  simple  one,  its  time-consuming  nature  means  that  for  many 
large datasets it would be impossible to complete this task fully due to 
manual analysis constraints. Thus there is a suitable trade-off with the 
error-rate and efficiency of the approaches used. Regardless, the accu-
racy  and  reliability  of  annotations  should  always  be  interrogated,

2.2.4. Feature extraction & visualisation 

In  each  of  our  ML  pipelines,  every  image  that  passes  through  the 
VGG16 feature extractor (Fig. 2) results in a matrix of 1 × 4096 features. 
These  are  then  passed  as  inputs  to  the  classifier.  This  happens  auto-
matically in the CNN approach, as the classification (FC) layers are still 
present in the architecture. 

These extracted ‘deep’ features are numerous and difficult to inter-
pret. Therefore, before undertaking any classification of the extracted 

EcologicalInformatics81(2024)1026195C.A. Game et al.                                                                                                                                                                                                                                

Table 4 
Model hyperparameter glossary for CNN & SVM training.  

CNN: 
Batch size 

Epochs 
Loss 

function 

Learning 
rate 

Optimizer 

SVM: 
C 

γ

As with SVM training, we conducted the CNN training protocol using 5- 
fold  cross-validation  of  the  training  set;  the  same  k-folds  as  the  SVM 
training.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information about the specific methods used for collecting raw data in the deep learning pipeline. However, common methods for gathering raw data include utilizing various types of sensors such as cameras or microphones, conducting surveys, accessing publicly available datasets, or even scraping data from websites. In the case of ecological studies, researchers might use remote sensing techniques like satellite imagery or drone footage to gather environmental data. Without more specific details regarding the study or project, it is challenging to determine exactly what method was employed for data collection.
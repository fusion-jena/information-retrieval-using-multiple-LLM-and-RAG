Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

9730
8440
9124
10,480

The considered dataset is further divided into three datasets, resulting in 22,666 images
for training the model (training set), 7554 images for validating the trained model (valida-
tion set), and 7554 images for assessing the model performance (testing set).

To obtain this dataset, a semantic segmentation is conducted using our previous
works [51–53]. The four classes—namely, urban, bare soil, vegetation and road—are
extracted from the satellite images, meaning that the resulting images each contain both
the real value of pixels of the extracted class and zero for the values of the other classes.
Then, each image containing a given class is divided into non-overlapping blocks of
256 × 256 pixels and saved into folders, each with the name of the corresponding class. A
sample of this dataset is depicted in Figure 4, wherein the white represents a given land
cover class and the black represents the values of other classes.

5.2. Experimental Set-Up

Table 3. CNN Architecture.

No.

Layers

Output Shape

Parameters

Dropout Rate

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

Input
Convolutional
Activation (ReLu)
MaxPooling
Convolutional
Activation (ReLu)
MaxPooling
Convolutional
Activation (ReLu)
MaxPooling
Dropout
Flatten
Fully Connected
Activation (ReLu)
Fully Connected
Activation (softmax)

128 × 128 × 3
128 × 128 × 32
—
64 × 64 × 32
64 × 64 × 32
—
32 × 32 × 32
32 × 32 × 64
—
16 × 16 × 64
16 × 16 × 64
16,384
64
—
4
—

—
896
—
—
9248
—
—
18,496
—
—
—
—
1,048,640
—
—
260

—
—
—
—
—
—
—
—
—
—
0.4
—
—
—
—
—

4.2.1. Data Augmentation

VGG16: Simonyan and Zisserman (2014) proposed the architecture of the VGG16
model. VGG16 is a CNN model that consists of 16 hidden layers, including a total with
convolutional, max pooling and fully connected layers. VGG16 was trained on the
ImageNet dataset, which consists of 1,000,000 images. VGG16 is constructed of ﬁve
blocks of convolutional layers with a 3 × 3 ﬁlter and stride of 1. After each convolution,
an activation function (ReLU) is executed, followed by a max-pooling process with
a 2 × 2 max ﬁlter and stride of 2. At the end of the ﬁve blocks, three FC layers are
added: the ﬁrst two layers with 4096 neurons and an ReLU activation function each,
and the third layer with 1000 neurons and a SoftMax activation function [21]. The
default input size is 224 × 224 × 3 [22].
ResNet-50: The ResNet model’s architecture was proposed in 2015 by He et al. ResNet-
50 is a 50 convolutional neural network layers pre-trained on the ImageNet dataset [23].

except for the ﬁrst and last modules. The FC layer is replaced with a global average
POOL layer and the default input size is 299 × 299 × 3 [25].
DenseNet121: Huang et al. (2017) proposed the architecture of the DenseNet121 model,
another CNN-based architecture trained on the ImageNet dataset. DenseNet121 is
composed of 5 dense blocks. The ﬁrst block consists of a convolution layer with
a 7 × 7 ﬁlter and stride of 2 and a MaxPooling layer with a 3 × 3 max ﬁlter and
stride of 2. The remaining blocks consist of BatchNormalization, the ReLU activation
function, and two CONV layers with 1 × 1 and 3 × 3 ﬁlters. A transition layer follows
each block except for the last, which instead is followed by a classiﬁcation layer. In
DenseNet121, all previous feature-maps are used as input in each layer. The default
input size is 224 × 224 × 3 [26].

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the information provided, it appears that the deep learning model being discussed is used for semantic segmentation. Semantic segmentation is a type of pixel-level prediction task where the goal is to assign a label or category to every pixel in an image. This technique is often used in computer vision applications such as object recognition, scene understanding, and autonomous driving.

In this case, the authors mention conducting semantic segmentation to extract four classes - urban, bare soil, vegetation, and road - from satellite images. Each resulting image contains both the real value of pixels of the extracted class and zeros for the values of the other classes. These images are then divided into non-overlapping blocks of 256 x 256 pixels and saved into folders named after their respective classes.

Additionally, the paper mentions using Convolutional Neural Networks (CNNs) for the experimental setup. CNNs are commonly used for image analysis tasks like semantic segmentation due to their ability to automatically learn spatial hierarchies of features from data. Therefore, based on these details, we can infer that the primary purpose of the deep learning model is semantic segmentation of satellite imagery.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

One other challenge with deep learning methods is when the dataset is imbalanced. With heavily
imbalanced datasets, the error from the overrepresented classes contributes much more to the loss
value than the error contribution from the underrepresented classes. This makes the deep learning
method’s loss function to be biased toward the overrepresented classes resulting in poor classiﬁcation
performance for the underrepresented classes [50]. One should also pay attention when applying deep
learning methods to new applications because one requirement for deep learning is the availability of
a vast amount of training data. Moreover, the training data needs to have similar characteristics as the
testing data. Otherwise, deep learning methods may not yield good performance. Augmenting the
training dataset using diﬀerent brightness levels, adding vertically and horizontally ﬂipped versions,
shifting, rotating, or adding noisy versions of the training images could be potential strategies to

Table 1. Training parameters used in DeepLabV3+.

Training Parameter

Value

Learning policy
Base learning rate
Learning rate decay factor
Learning rate decay step
Learning power
Training number of steps
Momentum
Train batch size
Weight decay
Train crop size
Last layer gradient multiplier
Upsample logits
Drop path keep prob
tf_initial_checkpoint
initialize_last_layer
last_layers_contain_logits_only
slow_start_step
slow_start_learning_rate
ﬁne_tune_batch_norm
min_scale_factor
max_scale_factor
scale_factor_step_size
atrous_rates
output_stride

Poly
0.0001
0.1
2000
0.9
≥100,000
0.9
2
0.00004
‘513,513’
1
True
1
deeplabv3_pascal_train_aug
False
True
0
1 × 10
False
0.5
2
0.25
[6,12,18]
16

−4

of each layer and selected the one that provided the best results. We did this for all the layers (convolutional and fully connected layers). The choice of “100 hidden units” in the fully connected layer was the outcome of our design studies. Each convolutional layer utilizes the Rectified Linear Unit (ReLu) as an activation function, the last fully connected layer uses the SoftMax function for classification. We added a dropout layer for each convolutional layer with a dropout rate of ‘0.1’ to mitigate overfitting [43] after observing that ‘0.1’ dropout value performed better than two other dropout values, which are ‘0.05’ and ‘0.2’.  Figure 5. Our customized convolutional neural network (CNN) model structure. 2.4. NDVI-ML We developed an object-based vegetation detection method, NDVI-ML, which utilizes NDVI [44], machine learning (ML) techniques for classification and computer vision techniques for segmentation. The block diagram of NDVI-ML can be found in Figure 6. The NDVI-ML

makes the deep learning method’s loss function to be biased toward the overrepresented classes resulting in poor classification performance for the underrepresented classes [50]. One should also pay attention when applying deep learning methods to new applications because one requirement for deep learning is the availability of a vast amount of training data. Moreover, the training data needs to have similar characteristics as the testing data. Otherwise, deep learning methods may not yield good performance. Augmenting the training dataset using different brightness levels, adding vertically and horizontally flipped versions, shifting, rotating, or adding noisy versions of the training images could be potential strategies to mitigate the issues when test data characteristics differ from the training data. 5. Conclusions In this paper, we investigated the performance of three methods for vegetation detection. Two of these methods are based on deep learning and another one is an

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does mention one specific regularization method used to prevent overfitting in the deep learning pipeline - dropout. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overfitting. In the given context, it is mentioned that a dropout layer was added for each convolutional layer with a dropout rate of '0.1'. However, there is no explicit mention of other regularization methods such as L2 regularization.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.2. Split the datasets 

In this research, the datasets were randomly divided into three parts 
for training, validation and testing. Here, 60% of the data was used to 
train the deep learning models, and 20% of them was kept to validate the 
models. The remaining 20% data was used to evaluate the performance 
of the models. Table 1 show the number of data in the dataset and how 
they are separated for training, testing and validation. 

2.3. Deep learning models

diction  of  the  deep  learning  model.  This  dataset  was  taken  to  verify 
whether our proposed approach can handle that issue. The dataset was 
shared  by  Jiang  et  al.  (2020)  through  the  Github  repository: 
https://github.com/zhangchuanyin/weed-datasets.

4.1.1. Traditional vs patch-based pipeline 

In the traditional approach, the results indicate that the deep models 
cannot handle the issues with the dataset. For the DeepWeeds dataset, 
DenseNet169 and DenseNet201 models achieved the highest accuracy, 
but the prediction result was affected by inter-class similarity and intra- 
class  dissimilarity  problems.  Fig.  6  shows  a  confusion  matrix  of  the 
DenseNet201 model on the DeepWeeds dataset. It can be seen that the 
misclassification rate for chinee apple and snake weed are very  high. 
Since, there are lot of similarity in the morphology of those two weed 
species,  it  become  challenging  for  the  model  to  distinguish  them  by 
comparing the features of some part of an image. Besides, many weed 
images were classified as non-weed, and some native plants were rec-
ognised as prickly acacia or snake weeds.

images than others, which can affect the performance of the DL models 
(Attri et al., 2023; Kamilaris and Prenafeta-Boldú, 2018). Many studies 
use  data augmentation  techniques  to  increase  the  number  of  training 
data to address that. This approach generally improves the classification 
accuracy (Chen et al., 2022; Hasan et al., 2023; Le et al., 2020; Olsen 
et  al.,  2019;  Sarvini  et  al.,  2019).  Application  of  several  image  pre- 
processing techniques, such as resizing (Chechli´nski et al., 2019; Far-
ooq et al., 2018; Partel et al., 2020), removing background (Alam et al., 
2020; Bah et al., 2018; Jiang et al., 2019), image enhancement (Nke-
melu et al., 2018; Wang et al., 2020) and denoising (Tang et al., 2017), 
can also improve the performance of the models.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about specific techniques used to address data bias during preprocessing of the deep learning pipeline, such as stratified splitting, oversampling, undersampling, or diverse data collection. However, it mentions that the dataset was randomly divided into three parts for training, validation, and testing, with 60%, 20%, and 20% of the data allocated respectively.

Regarding handling potential biases due to class imbalance or other factors, the text discusses using data augmentation techniques to increase the number of training data, which is known to generally improve classification accuracy. Additionally, applying various image preprocessing techniques like resizing, removing background, image enhancement, and denoising can help improve the performance of the models. These methods may indirectly contribute to addressing data bias but do not directly target specific types of bias mentioned in the query.
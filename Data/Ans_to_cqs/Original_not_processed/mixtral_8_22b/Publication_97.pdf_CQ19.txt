Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.5. Neural Network Implementation and Tuning

The U-Net-like CNN described in Algorithm A1 was implemented in a Python-based (Python
3.7.3 was used) computational environment, which was built on top of the Keras framework [54] using
Tensorﬂow [62] as a backend. All computations were performed on a PC with 1 GPGPU Nvidia Tesla
K80 with 16 GB of RAM and required up to 10 h to train one CNN architecture.

To tune the architecture of the neural network, we tested the following combinations of parameters
(Algorithm A1, line #24), which correspond to diﬀerent U-Net architectures: num_layers = {64, 32, 16},
depth = {2, 3, 4}, layer_rate = {2, 1.5, 1.2}, batch_norm = {True, False}, residual = {True, False}, dropout
= {0, 0.5}. Thus, we performed a grid search over 216 diﬀerent U-Net-like architectures and found
several of the best ones which are suitable for forest damage segmentation.

All of the best results corresponded to the conﬁguration when the number of layers was equal to
64, and dropout was applied. The best one, whereby an additional batch normalization is applied,
corresponds to the following parameters: num_layers = 64, depth = 4, layer_rate = 2, batch_norm =
True, residual = False, dropout = 0.5.

58. Yu, L.C.; Sung, W.K. Understanding geometry of encoder-decoder CNNs. arXiv 2019, arXiv:1901.07647v2.
Ioﬀe, S.; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate
59.
shift. arXiv 2015, arXiv:1502.03167v3.
Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. Dropout: A simple way to prevent
neural networks from overﬁtting. J. Mach. Learn. Res. 2014, 15, 1929–1958.

60.

61. Evaluation of the CNN Design Choices Performance on ImageNet-2012. Available online: https://github.

com/ducha-aiki/caﬀenet-benchmark (accessed on 24 March 2020).

62. Abadi, M.; Agarwal, A.; Barham, P.; Brevdo, E.; Chen, Z.; Citro, C.; Corrado, G.S.; Davis, A.; Dean, J.;
Devin, M.; et al. TensorFlow: Large-scale machine learning on heterogeneous systems. arXiv 2016,
arXiv:1603.04467v2.

Therefore, having a batch size of 20 and performing typically up to 1500 epochs for training the

network, we used almost 30,000 diﬀerent augmented images of size 256 × 256.

2.4. Artiﬁcial Neural Network Architecture

The problem of forest damage identiﬁcation is a semantic segmentation problem. Eﬀorts to solve
such problems have recently made signiﬁcant progress due to artiﬁcial neural networks of complex
architecture, which are closely related to a general term referred to as deep learning [55–57].

Semantic segmentation is a pixel-wise classiﬁcation problem aimed at determining the class of
the particular pixel of the image it belongs to. It is usually handled by means of convolutional neural
networks (CNNs). As was noted in the introduction, one of such CNNs is U-Net, which is of an
encoder-decoder architecture [58].

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The deep learning model is built using the Keras framework with TensorFlow as its backend. This information can be found in the first paragraph of section 2.5, where it states that the implementation is based on the Keras framework and uses TensorFlow as a backend.
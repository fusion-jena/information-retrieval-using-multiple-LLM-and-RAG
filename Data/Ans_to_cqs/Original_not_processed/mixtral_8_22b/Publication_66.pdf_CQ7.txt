Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

studies,	 leave-	one-	out	 cross-	validation	was	 also	 carried	 out.	This	 in-

volved	removing	a	single	bird	from	the	dataset	for	each	species,	train-

ing	the	model	on	the	other	birds,	and	then	validating	the	model	on	the	

single	bird.

The	area	under	the	receiving	operating	characteristic	curve	(AUC)	

for	training	and	validation	data	and	sensitivity	and	specificity	for	each	

model	was	calculated,	as	were	the	positive	predicted	value	(PPV)	and	

negative	predicted	value	(NPV).	Sensitivity	and	specificity	are	the	pro-

portion	 of	 positives	 and	 negatives	 correctly	 identified,	 respectively,	

in	 the	 withheld	 data.	 PPV	 is	 the	 number	 of	 true	 predicted	 positives	

divided	by	all	predicted	positives	and	NPV	is	the	number	of	true	neg-

atives	divided	by	all	predicted	negatives.	A	perfect	model,	therefore,	

would	 have	 high	 sensitivity,	 specificity,	 PPV	 and	 NPV.	 These,	 along	

with	validation	AUC,	were	 used	 to	 determine	 the	 optimal	 model	 for

produced	 poorer	 behavioural	 predictions,	 further	 demonstrating	 the	

structed	the	deep	learning	models	in	r	using	H2O	and	conducted	the	

BROWNING et al. 2041210x, 2018, 3, Downloaded from https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12926 by Thuringer Universitats- Und, Wiley Online Library on [16/11/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons Licensecomparative	modelling.	E.B.	and	R.F	wrote	the	manuscript.	All	authors	

contributed	to	editing	of	the	manuscript.	R.F.	and	M.B.	supervised	this	

work.

DATA ACC ESSI BILITY

GPS	track	data	used	in	the	analysis	are	available	at	(http://seabirdtrack-

ing.org/mapper/contributor.php?contributor_id=950).	Other	data		used	

within	 the	 analysis	 (GPS	 &	 dive	 data,	 and	 associated	 matrices	 used

each	species.	The	models	were	then	used	to	predict	the	diving	loca-

tions	of	birds	monitored	with	only	GPS	devices.

2.4 | Alternative prediction methods

To	 compare	 predictions	 obtained	 from	 deep	 learning	 models	 with	

methods	 used	 in	 previous	 studies	 classifying	 foraging	 behaviour	 in	

seabirds	HMMs,	a	naïve	Bayes	classifier	and	speed	and	tortuosity	pre-

dictions	were	implemented	on	the	data	(see	Appendix	S8,	supporting	

information	for	details	of	the	latter).	The	Naïve	Bayes	classification,	a	

supervised	 learning	 method,	 was	 implemented	 in	 r	 using	 the	 e1071	

package	 (Meyer,	 Dimitriadou,	 Hornik,	 Weingessel,	 &	 Leisch,	 2015).	

The	inputs	were	the	same	as	used	in	the	H2O	model,	and	the	depend-

ent	 variable	 (variable	 to	 be	 predicted)	 was	 the	 binary	 dive/not	 dive	

column.

HMMs	 are	 an	 unsupervised	 learning	 method	 that	 identifies	 dis-

crete	 states	within	 time-	series	 data	 and	 have	 been	 used	 extensively

individuals	by	training	deep	learning	models	to	predict	diving	in	European	shags,	

common	guillemots	and	razorbills.	We	validate	our	predictions	using	withheld	data,	

producing	 quantitative	 assessment	 of	 predictive	 accuracy.	 The	 variables	 used	 to	

Handling	Editor:	Jana	McPherson

train	these	models	are	those	recorded	solely	by	the	GPS	device:	variation	in	longi-

tude	and	latitude,	altitude	and	coverage	ratio	(proportion	of	possible	fixes	acquired	

within	a	set	window	of	time).

3.	 Different	combinations	of	these	variables	were	used	to	explore	the	qualities	of	differ-

ent	models,	with	the	optimum	models	for	all	species	predicting	non-diving	and	diving	

behaviour	correctly	over	94%	and	80%	of	the	time,	respectively.	We	also	demonstrate	

the	superior	predictive	ability	of	these	supervised	deep	learning	models	over	other	

commonly	used	behavioural	prediction	methods	such	as	hidden	Markov	models.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In this study, the dataset was divided into a training set and a validation set for each species. Leave-one-out cross-validation was performed, which involved removing a single bird from the dataset for each species, training the model on the other birds, and then validating the model on the single bird. This process was repeated until all birds had been removed once and tested against the remaining birds. The area under the receiving operating characteristic curve (AUC), sensitivity, specificity, positive predicted value (PPV), and negative predicted value (NPV) were calculated for both training and validation data. Sensitivity and specificity measure the proportion of positives and negatives correctly identified, respectively, in the withheld data. PPV is the number of true predicted positives divided by all predicted positives, while NPV is the number of true negatives divided by all predicted negatives. A perfect model would have high sensitivity, specificity, PPV, and NPV. These metrics, along with validation AUC, were used to determine the optimal model for each species.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

32. M. Simon, E. Rodner, and J. Denzler, “Part detector
discovery  in  deep  convolutional  neural  networks,”  in
Asian  Conference  on  Computer  Vision  (ACCV)  (2014),
pp. 162–177.

33. Van G. Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun,
A.  Shepard,  H.  Adam,  P.  Perona,  and  S.  Belongie,
“The iNaturalist Species Classification and Detection
Dataset,” in IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2018), pp. 8769–8778.
34. G. Yang and D. Ramanan, “Upgrading optical flow to
3D scene flow through optical expansion,” in Proceed-
ings  of  the  IEEE/CVF  Conference  on  Computer  Vision
and Pattern Recognition (CVPR) (2020), pp. 1334–1343.

PATTERN RECOGNITION AND IMAGE ANALYSIS 

 Vol. 31 

 No. 3 

 2021

488

RADIG  et  al.

results in an unchanged expected activation in the fol-
lowing  convolutional  layers.  Furthermore,  we  nor-
malize all input channels (red, green, blue, and depth)
by subtracting the respective mean and dividing by the
respective standard deviation over the whole dataset.
We also introduce a feature fusion module to combine
the extracted features from both backbones using one
3 × 3 convolution per FPN scale and reduce the num-
ber of channels from 2 × 256 to 256. This allows us to
use weights pre-trained on the Microsoft COCO data-
set [24] for the region proposal network (RPN) classi-
fier and mask head. Fig. 4 illustrates the general archi-
tecture of the resulting D-Mask R-CNN architecture.
We  restrict  our  evaluation  of  D-Mask  R-CNN  to
instances of deers as it is the most common species in our
RGB-D dataset. We quantify the results of D-Mask R-
CNN using the average precision (AP), AP50, and AP75
metrics as defined by the Microsoft COCO dataset [24].

object  boundaries.  We  therefore  argue  that  weights
initially  optimized  for  color  images  are  also  valuable
for  detecting  such  features  in  depth  images.  As  the
ResNet-50 architecture expects a three-channel RGB
input image as input, its first convolutional layer has to

body  shape,  and  others  can  be  determined  from  it.
This data stream is transmitted for preprocessing to a
powerful  single-board  computer  on  the  base  station.
There, the data stream is broken down into its individ-
ual components and analyzed. Recordings are sorted
out  that  do  not  contain  animal  images  due  to  false
alarms from the motion detector (typically 40–70%).
A further reduction of the data volume is achieved by
masking out image areas where no animals can appear.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The given context does not provide information about the specific hardware resources used for training the deep learning model, such as GPUs or TPUs. Therefore, based on this context alone, it is not possible to determine which hardware resources were utilized for training the deep learning model.
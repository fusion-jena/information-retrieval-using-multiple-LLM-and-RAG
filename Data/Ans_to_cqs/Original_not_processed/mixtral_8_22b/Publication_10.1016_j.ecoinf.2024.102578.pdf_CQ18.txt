Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 4 
Definitions of model performance metrics based on “caret” R package, based on 
true positives (TP), true negatives (TN), false positives (FP), and false negatives 
(FN).  

Metric 

Equation 

Definition 

Accuracy 

Precision 

Recall 

TP + TN
TP + FP + TN + FN 
TP
TP + FP 

TP
TP + FN 

F1 

2*precision*recall
precision + recall  

Proportion of correct predictions in the whole 
data set. 
The proportion of images that a model classified 
as a specific category C that are actually category 
C. 
The proportion of images that are actually a 
specific category C that the a model classified as 
C. 
Weighted average of precision and recall.  

Table 5 
Performance of the image quality model on the test data.  

Location 

Finnmark 

Yamal 

Id 

Bad 
Good 
Bad 
Good 

Precision 

0.920 
0.986 
0.764 
0.979 

Recall 

0.910 
0.988 
0.779 
0.977 

F1 

0.915  
0.987 
0.771  
0.978

EcologicalInformatics81(2024)1025784G. Celis et al.                                                                                                                                                                                                                                    

3.5. Workflow performance

Table 7 
False positive model performance on the test data. To estimate model perfor-
mance metrics, classes that were exclusively in the manual assessment or model 
output  were  not  included.  The  “No  animal”  class  combines  the  Empty,  Bait, 
Bait_yamal, and Rock model classes.    

Finnmark   

Yamal  

Class ID 

Precision 

Recall 

F1 

Precision 

Recall 

F1 

No animal 
Golden eagle 
White-tailed 

eagle 

Raven 
Hooded crow 
Magpie 
Mountain hare 
Reindeer 
Wolverine 
Arctic fox 
Red fox 

0.985 
0675 

0.623 
0.921 
0.941 
– 
– 

0.084 
0.612 
0.310 
0.557 

0.860 
0.899 

0.812 
0.985 
0.888 
– 
– 

0.375 
0.719 
0.781 
0.944 

0.919 
0.771 

0.706 
0.952 
0.914 
– 
– 

0.138 
0.661 
0.444 
0.701 

0.985 
– 

– 

0.04 
– 

0.964 
0.684 
– 

0.013 
0.683 
0.012 

0.834 
– 

0.904 
– 

– 

1.00 
– 

0.908 
0.181 
– 

0.600 
0.693 
0.833 

– 

0.079 
– 

0.936 
0.286 
– 

0.027 
0.688 
0.025

shown to reduce the processing time of image classification load by as 
much as 5× to 13× depending on the tasks (Fennell et al., 2022; Henrich

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there isn't any explicit mention of a strategy implemented to monitor the model performance during training. However, we can infer from Table 4 that the authors used various performance metrics such as Accuracy, Precision, Recall, and F1 score to evaluate their models. These metrics could have been monitored during the training process to ensure optimal performance. Additionally, Table 5 and Table 7 showcase the performance of the image quality model and false positive model respectively on the test data using these same metrics. This suggests that the authors did indeed track these metrics throughout the development and testing phases of their models.
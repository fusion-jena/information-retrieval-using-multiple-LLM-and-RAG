Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

mean of the predicted test set size diverges from the actual test set size. 
Optimal classification accuracy was obtained when the test set size 
was smallest relative to the training set. However, even when the test to 
training set  ratio  was  32:12, the  predicted value  was  close  to the  ex-
pected value, demonstrating the robustness of the model.

Further algorithm validation was undertaken using holdback trials, 
to establish the minimum number of trails required for testing. Although 
the predicted mean for each test:training set size was close to the actual 
test size up to the 32:12 combination, after a ratio of 20:24 the amount 
of  variation  in  the  sequential  trials  increased,  potentially  resulting  in 
reduced algorithm accuracy. 

Fig. 4. Holdback partitioning trials using test and training sets. The test set size 
was plotted against itself (black circle), against the predicted value for the test 
size  (black  X)  and  against  the  mean  predicted  value  for  each  test  size 
(blue square).

Data validation was performed on the captive Amur tiger dataset to 
determine the optimal number of footprint trails required for the algo-
rithm training set and test sets that could provide an accurate population 
estimate. Fig. 4 shows the result of a holdback trial partitioning test on 
training sets for 44 captive Amur tigers. Using the algorithm generated 
in FIT, the analysis was iterated 10 times for each combination of test/ 
training  size,  with  randomly  selected  trails,  to  examine  how  the  pre-
dicted outcome compared with the known test set size. The figure shows, 
for example, that then the test set size (y axis) comprises trails from 4 
tigers, and the test/training set size comprises trails from 04/40 tigers, 
the predicted test set sizes are very similar across a range of partitioning 
trials. However, when the test set size is 36 tigers, and the test/training 
ratio 36/08 tigers, there is a wide range of predicted test set sizes and the

Trails  were  collected  from  free-ranging  Amur  tiger,  of  unknown 
identity, at the study sites. Table 2 reports the trail ID collected for each 
trail. As described above (Section 3.1) sub-trails were formed where >10 
footprints existed in any one trail. For example, the table indicates that 
using the 25 left hind footprints from trail MW09, we were able to derive 
four sub-trails. Section 3.5 below explains how the sex classification for 
images and the resulting designations for each trail were made. 

3.3. Data validation to determine the optimal size for algorithm training 
and test sets

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly state the specific criteria used to determine when training is complete. However, it mentions data validation being performed on the captive Amur tiger dataset to determine the optimal number of footprint trails required for the algorithm training set and test sets that could provide an accurate population estimate. This suggests that the completeness of training might be determined by achieving an acceptable level of accuracy in predicting the population estimate based on the test set. Additionally, the use of holdback trials and iterative analysis for each combination of test/training size implies that stability or consistency in prediction outcomes may also play a role in determining when training is considered complete.
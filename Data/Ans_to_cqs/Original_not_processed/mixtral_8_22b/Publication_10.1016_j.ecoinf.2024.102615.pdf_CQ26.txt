Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

imbalanced small datasets and the application of machine learning algorithms to 
predict total phosphorus concentration in rivers. Eco. Inform. 76, 102138 https:// 
doi.org/10.1016/j.ecoinf.2023.102138. 

Atkinson, P.M., Tatnall, A.R., 1997. Introduction neural networks in remote sensing. Int. 

J. Remote Sens. 18, 699–709. https://doi.org/10.1080/014311697218700. 
Attri, I., Awasthi, L.K., Sharma, T.P., Rathee, P., 2023. A review of deep learning 

techniques used in agriculture. Eco. Inform. 77, 102217 https://doi.org/10.1016/j. 
ecoinf.2023.102217. 

Awadallah, M.A., Abu-Doush, I., Al-Betar, M.A., Braik, M.S., 2023. Metaheuristics for 
optimizing weights in neural networks. In: Mirjalili, S., Gandomi, A.H. (Eds.), 
Comprehensive Metaheuristics, 1st ed. Elsevier, Palestine, pp. 359–377. 
Bashir, O., Bangroo, S.A., Shafai, S.S., Senesi, N., Kader, S., Alamri, S., 2024.

2.1.3. Data partition 

The raw dataset was randomly shuffled, after which the shuffled data 
were divided into training, validation, and test sets. The training set was 
used to learn the parameters of the model during the training process, 
and the validation set was used to evaluate the modeling and tune its 
hyperparameters. Finally, the performance and generalizability of the 
optimal model were evaluated using the test set. The data partitioning 
ratio between the training, validation, and test sets was set as 8:1:1 and 
the random partition was repeated ten times to reduce the influence of 
random splitting while ensuring converged performance (Geras et al., 
2017). 

2.2. MLP model and model assessments 

2.2.1.

Introduction of MLP

an input layer with the 4200 dimensions of the spectral reflectance data; 
ii) the first hidden layer containing 2048 neurons, which uses the ReLu 
activation function to learn the nonlinear expression of the data; iii) a 
batch normalization layer followed by a dropout layer (random dropout

Fine, J.A., Rajasekar, A.A., Jethava, K.P., Chopra, G., 2020. Spectral deep learning for 

Peng, Q., Huo, D., Hall, C.M., 2022. Neural network-based air handling control for 

prediction and prospective validation of functional groups. Chem. Sci. 11, 
4618–4630. https://doi.org/10.1039/c9sc06240h. 

Garrity, D., Bindraban, P., 2004. A Globally Distributed Soil Spectral Library Visible near 
Infrared Diffuse Reflectance Spectra. Nairobi, Kenya, ICRAF /ISRIC Spectral Library.  
Ge, Y., Morgan, C.L., Grunwald, S., Brown, D.J., Sarkhot, D.V., 2011. Comparison of soil 

reflectance spectra and calibration models obtained using multiple spectrometers. 
Geoderma 161, 202–211. https://doi.org/10.1016/j.geoderma.2010.12.020. 
Geras, K.J., Wolfson, S., Shen, Y., Wu, N., Kim, S., Kim, E., Heacock, L., Parikh, U., 
Moy, L., Cho, K., 2017. High-resolution breast cancer screening with multi-view 
deep convolutional neural networks. arXiv:.07047. https://doi.org/10.48550/ 
arXiv.1703.07047.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there is no direct mention of specific techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, it does discuss some aspects related to data partitioning and model evaluation.

For instance, in section 2.1.3, it mentions that the raw dataset was randomly shuffled before being divided into training, validation, and test sets. This could potentially help mitigate any inherent biases present in the original order of the data. Furthermore, the data partitioning ratio was set as 8:1:1, and this random partition was repeated ten times to reduce the influence of random splitting and ensure converged performance.

While these methods do not directly address data imbalance or bias, they contribute towards creating a more robust and reliable model by reducing the impact of chance factors in the data split. For addressing issues like class imbalance or bias specifically, one might consider techniques such as oversampling the minority class, undersampling the majority class, or generating synthetic samples through methods like SMOTE (Synthetic Minority Over-sampling Technique), among others. However, these techniques are not mentioned in the provided context.
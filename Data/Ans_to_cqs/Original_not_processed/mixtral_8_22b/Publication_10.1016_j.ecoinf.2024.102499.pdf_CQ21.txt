Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

85 

80 
10,474 
2570 
900 
783 
454 

346 

267 
223 

132 

107 

36 

32 
14 

10 

4214 

822 

162 

229 
190 
112 

37 

80 
4621 
1640 
615 
430 
322 

103 

168 
77 

80 

78 

30 

18 
1 

8 

65 

40 

32 

7 
4 
4 

3 

4 
90 
62 
16 
35 
28 

10 

13 
11 

35 

9 

5 

4 
1 

1  

EcologicalInformatics80(2024)1024992S. Villon et al.                                                                                                                                                                                                                                   

For each fold, the frames from Trainingclips  were used to train the 
models. The Testingclips were used to assess the robustness of our method 
on short sequences centered around the presence of fish presence. The 
Testingstations, common to all k-fold were used to assess the robustness of 
the method on 1-h videos, corresponding to the real use-case scenarios 
of ecological studies (Supp. Fig. 1). 

2. Methods

clips (refer to Methods for details). Our training data included 8 shark 
species and 19 non-shark species commonly associated with sharks on 
baited underwater videos to add diversity in the training. After trans-
forming video clips into still images at a rate of 1 frame per second, we 
annotated  sharks  and  the  19  non-shark  species  present  in  the  frames 
with bounding boxes. This resulted in 26,947 fish annotations (Table 1). 
Given  that  the  Faster  RCNN  architecture  automatically  generates 
negative samples (Ren et al., 2015), there was no need to annotate any. 
It  is  known  that  classic  deep  architectures  need  numerous  image 
sample  per  class  to  work  correctly  (Lecun  et  al.,  2015;  Villon  et  al., 
2022). Hence, shark species that were infrequently captured in our clips 
(Carcharhinus  albimarginatus,  Galeocerdo  cuvier,  Nebrius  ferrugineus, 
Negaprion  acutidens)  were  excluded  from  the  testing  phase.  However,

Table 1 
The annotated dataset with the number of annotations per species, along with 
details on the number of frames and video clips in which each species appeared. 
Shark  species  are  indicated  with  an  asterisk  (*).  Non-shark  species  show  no 
asterisk. The studied species of sharks are shown in bold.  

Species 

Number of 
annotations 

Number of 
frames 

Number of clips 
with the species 

Carcharhinus 

amblyrhynchos* 
Triaenodon obesus* 
Carcharhinus 

melanopterus* 
Galeocerdo cuvier* 
Nebrius ferrugineus* 
Negaprions acutidens* 
Carcharhinus 

albimarginatus* 
Stegostoma fasciatum* 
Lutjanus bohar 
Plectropomus laevis 
Epinephelus maculatus 
Lethrinus olivaceus 
Aprion virescens 
Carangoides 

fulvoguttatus 
Carangoides ferdau 
Caranx ignobilis 
Symphorus 

nematophorus 

Carangoides 

orthogrammus 
Scomberomorus 
commerson 
Chanos Chanos 
Lutjanus rivulatus 
Grammatorcynus 

bilineatus 

8006 

1432 

434 

243 
207 
112 

85 

80 
10,474 
2570 
900 
783 
454

1. Introduction

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training process, several postprocessing steps can be performed depending on the specific requirements of the project or study. Some common postprocessing steps include generating saliency maps, calculating metrics, and creating confusion matrices.

Saliency maps are visualizations that highlight the regions of an input image that contribute most significantly to the output prediction made by the trained model. These maps help researchers understand how the model makes its decisions and identify potential biases or errors in the model's reasoning.

Calculating metrics involves evaluating the performance of the trained model using various quantitative measures such as accuracy, precision, recall, and F1 score. These metrics provide insights into the strengths and weaknesses of the model and allow researchers to compare different models objectively.

A confusion matrix is a table that summarizes the predictions made by the trained model against the actual labels of the test set. It provides a detailed breakdown of true positives, false positives, true negatives, and false negatives, allowing researchers to analyze the model's performance across different classes accurately.

However, it should be noted that these postprocessing steps may not always be necessary or relevant, depending on the nature of the problem being addressed and the goals of the research. For example, if the primary objective is to develop a tool for automated detection of certain objects in images, then generating saliency maps might not be required. Similarly, if the focus is on optimizing the model's overall performance rather than analyzing its behavior in detail, then calculating metrics and constructing confusion matrices might suffice.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2. Model training: We process data with a deep neural
network and adjust the training epoch, batch size, and
volume size to output a classiﬁer for prediction.
3. Result output: The point cloud test set is segmented,
and we can obtain the ﬁnal segmentation results. The
output ﬁles include point cloud geometric partition
graphs, SPGs, and segmentation results.

Results and Discussion

It can be shown from the recent improvements in deep
learning that enough datasets for training classiﬁers are
highly required when processing the data in complex real-
world scenes. Until now, datasets with rich objects and a
large number of marked labels for point cloud segmenta-
tion in a complex scene have been absent. For example, the
famous Oakland dataset contains fewer than 2 million
markers, while the popular benchmark created from New
York University only provides point clouds in indoor
scenes (Munoz et al., 2009). Both the Sydney city object
dataset and the IQmulus & TerraMobilita competition
dataset were scanned by a 3D Velodyne LiDAR that was
mounted on a car, which measures much fewer points than
a static scanner, as well as the benchmarks scanned by
airborne sensors (Vallet et al., 2015). The Semantic3D
benchmark provides more than 4 billion points in total,
which include 8 categories: artiﬁcial terrain, natural terrain,
high vegetation,

Guan, H., Yu, Y., Zheng, J., Li, J., & Zhang, Q. (2015). Deep
learning-based tree classiﬁcation using mobile LiDAR data.
Remote Sensing Letters, 6(11), 864–873.

Guerrero, P., Kleiman, Y., Ovsjanikov, M., & Mitra, N. J. (2017).
PCPNET: Learning local shape properties from raw point
clouds. Computer Graphics Forum, 37(2), 75–85.

Guinard, S., & Landrieu, L. (2017). Weakly supervised segmentation-
aided classiﬁcation of urban scenes from 3D lidar point clouds.
International Archives of the Photogrammetry, Remote Sensing
and Spatial Information Sciences, XLII-1/W1, 151–157. https://
doi.org/10.5194/isprs-archives-XLII-1-W1-151-2017

Hackel, T., Savinov, N., Ladicky, L., Wegner, J. D., Schindler, K., &
Pollefeys, M. (2017). Semantic3d.net: A new large-scale point
the Pho-
cloud classiﬁcation benchmark . ISPRS Annals of
togrammetry, Remote Sensing and Spatial Information Sciences,
IV-1/W1, 91–98. https://doi.org/10.5194/isprs-annals-IV-1-W1-
91-2017

2165

Fig. 1 The pipeline of the SPG framework. a Represents the original
input data (taking a tree and a pole as examples). b Is a hand-crafted
point cloud partition, c each partition is converted into a superpoint of

the SPG, and d is the network for superpoint embedding and
segmentation based on PointNet and GRU. The black lines represent
the superedges

with K ¼ P3
adjacency:

i¼1 ki,

E ¼ (cid:3)

3
X

i¼1

ki
K

log

(cid:2) (cid:3)
ki
K

in accordance with the optimal

ð1Þ

The linearity, planarity, and scattering are derived by the

following terms:

linearity ¼

planrity ¼

scattering ¼

k1 (cid:3) k2
k1
k2 (cid:3) k3
k1

k3
k1

ð2Þ

The linearity describes how elongated the adjacency is,
the planarity assesses how plane the adjacency is, the high-
scattering values are used to represent an isotropic and
spherical adjacency. We describe these three point features
as geometric features.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given information, it appears that several datasets are mentioned in relation to deep learning and point cloud segmentation. However, none of them seem to correspond directly to the commonly known datasets such as MNIST, CIFAR, or ImageNet, which are typically associated with image classification tasks. Instead, the discussed datasets appear to focus specifically on point cloud data, often collected through various types of LiDAR scanners.

Some of the mentioned datasets include the Oakland dataset, containing fewer than 2 million markers; the NYU benchmark, providing point clouds in indoor scenes; the Sydney city object dataset and the IQmulus & TerraMobilita competition dataset, both scanned by a 3D Velodyne LiDAR mounted on a car; and the Semantic3D benchmark, offering over 4 billion points across eight different categories. These datasets are likely utilized in the deep learning pipeline described in the text, where data is processed with a deep neural network to create a classifier for prediction.

In summary, although specific details regarding the exact datasets used in the deep learning pipeline are not explicitly stated, the context suggests that datasets like those mentioned above, focusing on point cloud data, might be employed instead of traditional image classification datasets like MNIST, CIFAR, or ImageNet.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

950 (4%) 
1110 (4%) 

Total 

24,675 
32,986  

passes over the entire training set) and a batch size of 64, with an ADAM 
(cid:0) 5. Image augmentation was applied 
optimiser and a learning rate of 10
in the form of horizontal flip, 0.2 degree counter clock wise shear and a 
random zoom between 0 and 0.2 - all leading to 224 Ã— 244 pixel RGB 
input tensors. Data was normalised to ImageNet mean values, and the 
pixels values were rescaled in the range of [0, 1]. Model training took 
roughly 4 days. The best model was selected based on minimal valida-
tion loss that occurred at epoch 448. This model showed a training loss 
of 0.256, a training accuracy of 0.899, a validation loss of 0.298 and 
validation accuracy of 0.891. We evaluated the red kite model perfor-
mance based on an independent test set of 2060 images (as described in 
3.3). 950 of these images were true positive red kites images and the

5. Discussion 

In this study we developed a workflow which leveraged citizen sci-
ence data to extract further relevant records from social media posts in 
the  same  region.  The  workflow  functions  as  a  data  filter  enabling 
downsampling of an initially very large dataset into a human analysable 
subset - in our case containing 0.5% of the original posts. By massively 
reducing  data  volumes,  it  becomes  realistic  to  analyse  the  remaining 
data by hand to select true positives, with around one hour required for 
the  4000  or  so  candidate  posts  we  identified.  Our  workflow  thus  ad-
dresses  the  research  gap  identified  by  Burke  et  al.,  2022,  using  gen-
eralisable  methods  to  extract  target  data  from  various,  unverified 
sources to enrich data.

quality  of  extracted  information  in  isolation  through  metrics  such  as 
precision and recall, but do not explore the added value of integrating 
data. These gaps lead to three research questions, which we address in 
what follows. The first two questions relate to the development of our 
workflow:  

1.  How can a generic workflow be developed which leverages expert 
contributions from citizen science data to extract content from social 
media posts related to a given species, which can easily be used in 
practical applications?  

2.  How can multiple modalities (e.g. text and images) be used to extract 
relevant information and does such a combination result in higher 
recall and/or precision? 

The third research question relates to the added value of our work-

flow in integrating data from different sources. 

3. What added value with respect to coverage, type, overlap and vol-
ume can we identify using a combination of citizen science and social 
media data?

Since our workflow is designed to be generic, take advantage of the 
text and image data and combine records from citizen science reports 
with social media data, it uses a combination of a simple rule based- 
approach, existing pre-trained models and a model trained specifically 
for our target species (Fig. 2). Our approach is designed to take advan-
tage  of  what  we  assume  to  be  high  quality  data  collected  by  citizen 
scientists with an interest in ornithology, use off-the-shelf models where 
possible, and reduce the initial number of social media posts in a given 
region to a manageable size for manual verification. Our workflow thus: 

EcologicalInformatics71(2022)1017823M.C. Hartmann et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the provided context, there isn't explicit mention of specific techniques like diverse datasets, cross-validation, or stratified splitting being employed to ensure the generalizability of the deep learning model. However, some aspects mentioned could contribute towards achieving this goal indirectly. For instance, the model was trained on a substantial dataset consisting of 24,675 images and tested on an independent test set of 2060 images. Additionally, image augmentation techniques such as horizontal flips, shearing, and random zooms were applied during training, potentially increasing the diversity of the input data. Furthermore, the model was optimized using the ADAM optimizer with a learning rate of 10, which might have helped improve its performance across varying inputs. Nevertheless, without direct evidence of employing specific techniques aimed at enhancing generalizability, it cannot be definitively stated how the researchers ensured their deep learning model would perform well on new, unseen data.
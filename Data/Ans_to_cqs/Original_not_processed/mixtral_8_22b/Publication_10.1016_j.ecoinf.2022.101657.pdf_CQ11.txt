Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

van Rijsbergen, C.J., 1979. Information Retrieval, 2nd ed. Butterworth-Heinemann, USA.  
Wang, C.Y., Bochkovskiy, A., Liao, H.Y.M., 2021. Scaled-YOLOv4: Scaling cross stage 
partial network. In: Conference on Computer Vision and Pattern Recognition 
(CVPR), pp. 13029–13038. 

Wei, W., Luo, G., Ran, J., Li, J., 2020. Zilong: a tool to identify empty images in camera- 
trap data. Ecol. Inform. 55, 101021 https://doi.org/10.1016/j.ecoinf.2019.101021. 
Weingarth, K., Zimmermann, F., Knauer, F., Heurich, M., 2013. Evaluation of six digital 

camera models for the use in capture-recapture sampling of Eurasian Lynx. 
Wald¨okologie Online 13, 87–92. 

Xi, T., Wang, J., Qiao, H., Lin, C., Ji, L., 2021. Image filtering and labelling assistant 

(ifla): expediting the analysis of data obtained from camera traps. Ecol. Inform. 64, 
101355 https://doi.org/10.1016/j.ecoinf.2021.101355.

The official, MS-COCO pre-trained, version of this architecture has 
then  been  further  trained,  a  technique  known  as  transfer-learning, 
using the WCS dataset and its recommended data split, giving a cred-
ible mean average precision mAP50 score of 0.75. The mAP metric, used 
to  evaluate  the  PASCAL  VOC  challenge  dataset  (Everingham  et  al., 
2010),  takes  the  mean  over  all  considered  species  of  their  respective, 
species-specific  average  precision  as  defined  below.  The  subscript  50 
refers  to  the  requirement  that  the  area  of  the  intersection  of  ground 
truth  bounding  box  and  predicted  bounding  box  needs  to  be  at  least 
50%  of  the  area  of  the  union  of  both  for  an  otherwise  true  species 
identification to be considered correct. The network does not output a 
binary  value  for  species  identification  but  instead  a  belief  b  ∈ [0, 1] 
that  a  particular  species  has  been  identified.  Thresholding  these

6.2.1. Data 

The GBIF database10 contains over 2300 videos of animals, although 
many of these are not suitable for our purpose. We have curated a subset 
of  330  videos  that  are  shot  with  static  cameras.  These  videos  are 
therefore good representations of the style of input this pipeline is ex-
pected  to  receive.  Every  frame  from  each  of  these  videos  has  been 
manually annotated, indicating whether or not it contains an animal. We 
have provided the training (30) and validation (300) split as used to tune 
parameters and evaluate the system, respectively. It is important to note 
that  camera-related  parameters  have  not  been  annotated  or  used  in 
testing  the  DynAIkonTrap. For these  tests we have  assumed the  focal 
length for all cameras to be the same as that of the RPi camera module –

7.1.3. Bespoke neural networks for species sets 

While we selected a suitable neural network for species identifica-
tion, we spent little research so far to train this network to sets of user- 
selected  or  prevalent  species  in  the  region  where  the  camera  trap  is 
deployed.  Training  for  a  bespoke  set  of  species  is  both  possible  and 
desirable. Further research is needed to identify and automate a process 
that,  given  a  set  of  species  of  interest,  outputs  a  robust  and  reliable 
neural network model with suitable weights. 

7.1.4. Low power neural networks in hardware

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Unfortunately, based on the provided context, there is no explicit mention of a specific data repository link for the deep learning pipeline. However, the context mentions the use of the GBIF database, which stands for Global Biodiversity Information Facility. This could potentially be the source of the data used in the pipeline. You can access the GBIF website at <https://www.gbif.org/>. Additionally, the context also mentions that a subset of 330 videos was manually annotated and provided for training and validation purposes. But again, there is no direct link provided for this dataset within the given context.
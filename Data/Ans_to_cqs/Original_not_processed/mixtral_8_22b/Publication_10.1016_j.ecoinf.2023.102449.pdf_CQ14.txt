Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

used  and  the  learning  capacity,  complexity  and  architecture  of  the 
model.

2.5. Pre-training 

The VAE was trained under variational inference to reconstruct mini- 
batches of 6 spectrograms over a maximum of 100,000 batches with a 
hold out split for both validation and test of 20%. Stochastic gradient 
descent is performed using the Adam optimiser (Diederik and Ba, 2017) 
using weight decay. To maximise reconstruction quality without over- 
fitting the learning rate was tweaked to an optimal configuration of η =
0.0005, undergoing annealing by a factor of 100 using a cosine function 
over  the  duration  of  the  training  (He  et  al.,  2018).  The  means  μ  and 
standard  deviations  σ  of  the  latent  distribution  for  all  T  frames  were 
encoded  as  a  set  of  feature  embeddings  and  used  in  subsequent 
experiments. 

2.6. Experiments 

2.6.1. Accommodating sensitivity to differences in device

Group 

Encoder    
Pre-processing 

ResStack 1 

ResStack 2 

ResStack 3 

ResStack 3 

Temporal Framing 
Bottleneck     

Reparameterisation 

Decoder    
Temporal Framing 
ResStack 3 

ResStack 3 

ResStack 2 

ResStack 1 

Post-processing 

Operation 

Input Shape 

Output Shape 

5 × 5 Conv + BatchNorm + ReLU 
Max & Avg pooling +1 × 1 Conv 
Downsampling block 
2 × Residual Block 
Downsampling block 
2 × Residual Block 
Downsampling block 
2 × Residual Block 
Downsampling block 
2 × Residual Block 
Reshape 

Flatten 
Linear 
Sample 
Linear 
Unflatten 

Reshape 
Upsampling block 
2 × Residual Block 
Upsampling block 
2 × Residual Block 
Upsampling block 
2 × Residual Block 
Upsampling block 
2 × Residual Block 
1 × 1 Conv + BatchNorm + ReLU 
2 × 2 ConvTranspose + BatchNorm + ReLU 
5 × 5 Conv

averaged feature representations for temporal integration (see 3.3) and 
compares its performance for site prediction (see 3.3.1).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, it appears that the hyperparameters of the Variational Autoencoder (VAE) model were optimized using stochastic gradient descent with the Adam optimizer. This optimization method uses weight decay. Specifically, the learning rate was adjusted to find an optimal configuration of η = 0.0005. Additionally, annealing was applied to this learning rate by a factor of 100 using a cosine function over the duration of the training. However, there is no explicit mention of specific techniques such as grid search or random search being employed for hyperparameter tuning in the provided context.
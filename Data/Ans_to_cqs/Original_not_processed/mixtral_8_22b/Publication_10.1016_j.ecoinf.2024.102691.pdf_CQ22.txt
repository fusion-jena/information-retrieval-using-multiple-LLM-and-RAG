Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

was analysed using various evaluation matrices. During model training, 
the input image size was set to 512 × 512 and the maximum batch size 
was limited to 4. To prevent underfitting, all models were ensured to 
follow back-propagation after accumulating the batch size 32 times. The 
models were trained using a stochastic gradient descent (SGD) optimizer 
with an initial learning rate of 0.01 and a cycle learning rate of 0.2, with 

EcologicalInformatics82(2024)1026917S. Kaukab et al.

3.2. Model evaluation 

Matrices  such  as  precision,  recall,  and  average  precision  (AP)  at 
various Intersections Over Union (IoU) thresholds were used to evaluate 
the performance of the models using Eqs. (4–6). The AP0.5, indicating 
average precision at 50% IoU, provided a more balanced reflection of 
the global detection performance of the model. The evaluation metrics 
for each model on the validation set were calculated after 300 epochs. 

Precision(P) =

True Positive
True Positive + False Positive

Recall(R) =

True Positive
True Positive + False Negative

Average Precision(AP) =

∫

x=1

x=0

Precision(Recall)d(Recall)

3.3. Performance on the test data 

(4)  

(5)  

(6)

Model 

AP0.5 

Precision 

Recall 

F1 Score 

Baseline detector-1 
Baseline detector-3 
NBR-DF-YOLOv5 
NT-FFN 

0.636 
0.583 
0.833 
0.603 

0.821 
0.80 
0.875 
0.838 

0.50 
0.417 
0.667 
0.417 

0.510 
0.363 
0.851 
0.674  

3.5. Comparison with state-of-the-art models 

To  thoroughly  evaluate  the  proposed  NBR-DF-YOLOv5  model,  we 
compared  it  with  several  state-of-the-art  object  detection  models, 
including Shufflenet V2-YOLOX, Improved YOLOv5s, CA-YOLOv5, Des- 
YOLOv4,  M2Det,  Swin  Transformer,  YOLOv7,  and  EfficientDet.  The 
statistical performance evaluation results of all these approaches were 
compared and presented in Table 6. These models were chosen for their 
relevance  and  high  performance  in  the  domain  of  real-time  object 
detection. To ensure a fair comparison, these models were tested on the 
same apple dataset.

Attention = f (w(n) , n)

(1)  

where,  n  represents  the  input  tensor  with  shape  [batch_size,  height, 
width, channels].  It contains the input  data or feature map  used in a 
neural network, w(n) is a weight tensor derived from n, typically ob-
tained through some form of learned parameters or coefficients, and f (w 
(n), n) represents the process of treating the input feature n. 

The network architecture of the depth-attention YOLOv5 is shown in 
Fig. 10. Unlike other attention mechanisms that are usually introduced 
earlier in the network and trained along with the other layers, the depth- 
attention mechanism was placed after the final processing layers of the 
detection model. It remains unchanged and does not adapt during the 
training  process.  This means that the depth-attention  weights are not 
updated  during  training  and  remain  fixed  throughout  the  learning

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The performance of the deep learning models was evaluated using various evaluation matrices such as precision, recall, and average precision (AP) at different Intersection Over Union (IoU) thresholds. The Average Precision (AP) at 50% IoU, denoted by AP0.5, provides a more balanced reflection of the global detection performance of the model. Other metrics used include Precision, Recall, and F1 score. The evaluation metrics for each model on the validation set were calculated after 300 epochs.
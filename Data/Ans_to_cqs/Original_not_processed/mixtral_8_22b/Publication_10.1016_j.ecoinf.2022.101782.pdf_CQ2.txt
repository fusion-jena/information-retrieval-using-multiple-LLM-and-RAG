Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

950 (4%) 
1110 (4%) 

Total 

24,675 
32,986  

passes over the entire training set) and a batch size of 64, with an ADAM 
(cid:0) 5. Image augmentation was applied 
optimiser and a learning rate of 10
in the form of horizontal flip, 0.2 degree counter clock wise shear and a 
random zoom between 0 and 0.2 - all leading to 224 Ã— 244 pixel RGB 
input tensors. Data was normalised to ImageNet mean values, and the 
pixels values were rescaled in the range of [0, 1]. Model training took 
roughly 4 days. The best model was selected based on minimal valida-
tion loss that occurred at epoch 448. This model showed a training loss 
of 0.256, a training accuracy of 0.899, a validation loss of 0.298 and 
validation accuracy of 0.891. We evaluated the red kite model perfor-
mance based on an independent test set of 2060 images (as described in 
3.3). 950 of these images were true positive red kites images and the

identified by both textual and visual analysis, we could achieve almost 
perfect  precision  (0.992),  at  the  cost  of  lower  recall.  Thirdly,  by 
combining the two approaches, we increased the extracted data volume 
by almost 14%, while still downsamping the original dataset by around 
99.5% and with a precision 0.636. Recall for the workflow overall is not 
known, since this would require manual analysis of over 600,000 posts. 
Our results thus go further than previous work by demonstrating how 
images  retrieved  using  text  and  image  metadata  can  be  combined  to 
achieve  very  high  precision,  or  merged  to  increase  recall  while  still 
filtering initial datasets very effectively. To better understand our results 
and their transferability to other species, we looked more closely at data 
quality.  Flickr  users  often  used  relevant  textual  descriptions  to  label 
their  red  kite  sightings  with  their  captured  images.  This  high  textual

particular species is of interest, typical approaches analyse data using 
either simple keyword extraction (e.g. by compiling lists of potentially 
relevant terms) applied to metadata, followed by some form of disam-
biguation  or  classification  to  remove  records  using  matched  terms  in 
another sense. Progress in off-the-shelf image classification approaches

has  led  to  analysis  based  solely  on  keywords  assigned  to  images  by 
machine  learning  classifiers  (Burke et  al.,  2022)  especially where  the 
identified  classes  are  relatively  broad  (e.g.  presence  of  water,  green 
space  or  trees).  Where  classifications  are  more  fine-grained,  common 
approaches  still  involve  human  annotation  of  metadata  and  image 
content, for example relating activity types to cultural ecosystem ser-
vices  (Pickering  et  al.,  2020).  Many  current  approaches  to  analysing 
image based social media focus on analysing either textual metadata or 
image content, with surprisingly few attempting to combine information 
derived from both modalities simultaneously, even though existing work 
suggest potential improvements in performance (You et al., 2016).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning pipeline primarily uses image data format. The input tensors are 224 x 244 pixel RGB images, which are normalized to ImageNet mean values and have pixel values rescaled in the range of [0, 1]. The model training takes roughly four days, and the best model is selected based on minimum validation loss. The model shows a training loss of 0.256, a training accuracy of 0.899, a validation loss of 0.298, and a validation accuracy of 0.891. The performance of the red kite model is evaluated based on an independent test set of 2060 images. Out of these images, 950 are true positive red kites images.
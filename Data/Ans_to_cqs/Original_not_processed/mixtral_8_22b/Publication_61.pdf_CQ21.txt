Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

5.2. Accuracy analysis

As we have shown in the previous section, the best accuracy on the
validation set was obtained using data augmentation, 0.98 and 0.95 for
train and validation sets respectively. It is important to realize that the
classes are imbalanced, cactus class has almost twice number of ex-
amples. Observing the non-normalized confusion matrix (Fig. 11), we
note that the false positives for the cactus class are almost twice the
false positive for the non-cactus class, this eﬀect is due to the im-
balanced classes. However, taking into account the normalized confu-
sion matrix (Fig. 12) where false positives for both classes have the
same value, we conclude that the errors are equally distributed among
the classes.

5.3. Learned features

E. López-Jiménez, et al.

Ecological Informatics 52 (2019) 131–138

batch size, and data augmentation) aﬀect the ﬁne-tuning of pre-trained
models. DeepPlant is another example of a system for plant identiﬁ-
cation based on a CNN that uses a pre-training approach (Lee et al.,
2015). Background textures it is always an element that complicates the
classiﬁcation task. Xiao et al. proposed a segmentation and crop
method, based on Fourier transformations and a K-means algorithm, to
generate a region of interest in the images; the cropped images are then
used to train a CNN for diﬀerent plant species classiﬁcation (Xiao et al.,
2018). CNN have not been used only for taxonomic classiﬁcation of
plants, but also for the detection of diseases that can aﬀect them
(Sladojevic et al., 2016).

5.3. Learned features

During training, CNN adjust their weights in order to decrease the
loss. Weights from ﬁnal layers are usually diﬃcult to make sense for
humans but weights in ﬁrst layers are more easy to comprehend. In
Fig. 14, we present the ﬁrst layer's ﬁlters that were learned by the
network along with the feature maps for the example cactus presented
in Fig. 13. As can be seen, the ﬁrst and second kernels of the CNN
highlight some features such as the vertical edges, the third and fourth
kernels perform a noise removal, the ﬁfth kernel sharp some features
and ﬁnally the sixth kernel erase the background.

5.4. Dataset extension

Training a neural network requires to set several hyper-parameters
such as the learning rate, number of epochs and batch size. The learning
rate is a special one since it deﬁnes how much the weights are ‘moved’
to decrease the loss. A bigger one could cause the network not to learn,
on the other hand, a smaller one could require much more steps for
learning. To decrease this issue, we train the network with Adam op-
timizer (Kingma and Ba, 2015), a variant of the stochastic gradient
optimization where the learning rate is adjusted automatically. The rest
of the parameters are set empirically, more details are presented in the
experiments section.

5. Experiments

In this section, we present the experimental characterization of the
proposed approach. The implemented network was trained with Adam
optimizer (Kingma and Ba, 2015) on a Inter Core i7 machine with
NVIDIA GeForce 1080 GPU. The hyperparameters were set as follows:
learning rate 0.01, number of epochs 150, batch size 2500.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: After the model training, there are several postprocessing steps that can be performed depending on the specific requirements of the project or study. Based on the given context, two main postprocessing steps are mentioned: metrics calculation and confusion matrix generation.

Metrics calculation involves evaluating the performance of the trained model using various evaluation metrics like accuracy, precision, recall, F1 score, etc. These metrics help assess the effectiveness of the model in making accurate predictions. In the given context, the authors mention that they achieved an accuracy of 0.98 and 0.95 on the train and validation sets, respectively, using data augmentation. They also discuss the impact of imbalanced classes on the results and provide a normalized confusion matrix to better understand the distribution of errors across different classes.

Confusion matrix generation is another essential step in postprocessing. A confusion matrix is a table that summarizes the performance of a classification model by comparing actual values against predicted ones. This helps visualize the types of errors made by the model during prediction. In the given context, the authors generate a non-normalized and a normalized confusion matrix to analyze the distribution of errors between the cactus and non-cactus classes.

Although not explicitly mentioned in the provided context, saliency maps can also be generated as part of postprocessing. Saliency maps highlight the regions of input images that contribute most significantly to the final output of the model. This technique can help identify which parts of the image are crucial for correct classification and improve interpretability.

In summary, the postprocessing steps discussed in the given context include metrics calculation and generating confusion matrices. Additionally, creating saliency maps may also be considered as a valuable postprocessing step to enhance understanding and interpretation of the model's behavior.
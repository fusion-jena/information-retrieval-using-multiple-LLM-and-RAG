Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In this study we have used the ‘Full-archive search endpoint’ to ac-
cess  the  entire  dataset  published  on  Twitter  from  its  beginning  in 
2006–2007 until the end of 2022. To handle the data in JSON format 
returned  by  the  Twitter  API,  we  used  the  twarc2  library  (https:// 
twarc-project.readthedocs.io/).  This  library  simplifies  the  handling  of 
the quota limits imposed by the Twitter API, as well as the pagination 
resulting from a single query. 

The API works by searching for words, hashtags or combinations of 
both  in  the  text  of  tweets,  much  like  the  Twitter  Advanced  Search 
interface  (https://twitter.com/search-advanced?).  To  find  out  how  to 
run the search, in an initial phase of the study, some public tweets with 
observations  were  provided  by  the  social  media  manager  of  the

successfully  located  after  contacting  the  user.  A  training  dataset  has 
been  made  available  at  https://github.com/PabloOtero/diversi 
mar_paper,  which  could  be  useful 
learning 
experiments.

(INDITIC Consortium, 2023). This kind of initiatives would benefit of 
the implementation of the current methodology to reduce human effort. 
Although the usefulness of this methodology has been proven, there 
are still certain limitations that cannot be ignored. The main one cor-
responds to the nature of the tweets themselves, with a low number of 
words  accompanying  the  messages,  not  always  well  written,  which 
makes  it  difficult  to  obtain  useful  information.  This  leads  to  a  high 
number of tweets being lost in the filtering process. Furthermore, using 
exclusively textual information from tweets to obtain a training dataset 
would  only  lead  to  the  creation  of  a  deficient  model.  In  addition, 
biodiversity observations only make sense when accompanied by their 
location, allowing, among other applications, to alert about the presence 
of  invasive  species  or  reuse  the  information  in  species  distribution

Data availability 

Supervised 

data 

available 

at: 

https://github. 

com/PabloOtero/diversimar_paper 

Acknowledgments 

The authors would like to thank the Twitter users who have autho-
rized  the  use  and  publication  of  their  posts  for  scientific  purposes. 
Likewise, the main author of this article would like to expressly thank 
the company Twitter for granting him an ‘Academic Research access’ 
profile for access to the data until April 2023. Unfortunately, the permits 
have currently been withdrawn by the current company, X, making it 
difficult for researchers to continue conducting studies like the current 
one without external funding. We would also like to acknowledge the 
comments made by three anonymous reviewers who have contributed to 
improving the quality of this manuscript. 

References 

Andersen, J.H., Dahl, K., G¨oke, C., Hartvig, M., Murray, C., Rindorf, A., Skov, H.,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: The data repository of the deep learning pipeline is not explicitly mentioned in the given context. However, the context does provide a link to a GitHub repository where a training dataset has been made available. The URL provided is <https://github.com/PabloOtero/diversimar_paper>. It can be assumed that this GitHub repository might contain the necessary data for the deep learning pipeline. Please note that this is not a traditional data repository such as Zenodo, Figshare, Dryad, or GBIF, but rather a code hosting platform. Therefore, if you are looking specifically for a traditional data repository, I regret to inform you that the context does not provide this information.
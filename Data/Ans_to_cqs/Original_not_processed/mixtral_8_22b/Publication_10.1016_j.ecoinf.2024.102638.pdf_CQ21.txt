Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

hyperparameter optimisation process, the performance metrics detailed 
in Section 2.5 are meticulously evaluated using the designated test set. 
Subsequently, based on the performance metrics obtained, we discern 
and  select  the  most  effective  models  for  each  target  variable.  For  an 
illustrative overview of this model selection process, kindly refer to the 
visual representation provided in Fig. 4.

EN 

KNN 
GP 

TREE 

RF 

XGB 

SVR 

MLP 

alpha 
l1_ratio 
neighbours 
weights 
p 
alpha 
max_depth 
min_samples_split 
min_samples_leaf 
max_features 
n_estimators 
max_depth 
min_samples_split 
min_samples_leaf 
max_features 
n_estimators 
learning_rate 
max_depth 
subsample 
colsample_bytree 
reg_alpha 
gamma 
min_child_weight 
C 
gamma 
epsilon 
hidden_layer_sizes 
activation 
alpha 
learning_rate 
max_iter 

* BP denotes Best Parameter. 

(cid:0) 1] 

(cid:0) 5, 10

(cid:0) 2, 10

The dataset is then divided into training sets, which comprise 80% of 
the  data,  and  a  test  set  representing  the  remaining  20%,  in  order  to 
maximise model training while maintaining a sufficient amount of data 
for robust model validation (Adjuik and Davis, 2022; Fern´andez-L´opez 
et al., 2020). This division is done by random sampling to ensure that 
both subsets are representative of the overall dataset and minimise the 
risk  of  bias.  Randomisation  helps  to  preserve  the  distribution  of  key 
features  and  target  variables  across  both  training  and  test  sets, 
enhancing the generalisability of the model. We leverage the scikit-learn 
function GridSearchCV to meticulously fine-tune the hyperparameters of 
each model within the confines of the training set, aiming to enhance 
their predictive performance. A five-fold cross-validation is used and the 
negative mean square error is set as the target measure. Following this

Variable  Model  MAE 

MSE 

RMSE  MAPE 

MASE 

Table 4 includes details such as the name of the prediction model, its 
hyperparameters,  the  parameters  selected  during  the  5-fold  cross- 
validation,  and  the  hyperparameters  adjusted  according  to  the  selec-
tion  criteria  to  determine  the  most  appropriate  configuration  for  the 
prediction model. 

3.2. Performance evaluation of H, LE and CO2  prediction models

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training, several postprocessing steps can be taken depending on the specific requirements of the project or study. Some common postprocessing steps include:

1. Evaluation Metrics Calculation: After training the models, their performance needs to be assessed using various evaluation metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE), and Mean Absolute Scaled Error (MASE). These metrics help quantify the accuracy and reliability of the predictions made by the models.

2. Hyperparameter Optimization: The optimal values of hyperparameters for each model are determined through a rigorous optimization process. In this case, GridSearchCV from scikit-learn library was employed with a five-fold cross-validation and negative mean squared error as the target measure.

3. Model Selection: Based on the performance metrics obtained, the most effective models for each target variable are chosen. Figure 4 provides a visual representation of this model selection process.

4. Data Division: To maximize model training and maintain enough data for robust validation, the dataset is split into training and testing sets. Here, 80% of the data forms the training set, while the remaining 20% constitutes the test set. This division is carried out randomly to ensure representativeness and minimize bias risk.

5. Fine-Tuning Hyperparameters: Using GridSearchCV, the hyperparameters of each model are carefully tuned within the constraints of the training set to improve their predictive capabilities.

6. Table Generation: A table summarizing the results of the prediction models, including their names, hyperparameters, parameters selected during the 5-fold cross-validation, and adjusted hyperparameters, is created.

7. Performance Evaluation: Finally, the performance of the prediction models for H, LE, and CO2 is evaluated. However, no explicit information about saliency maps or confusion matrices has been mentioned in the given context.
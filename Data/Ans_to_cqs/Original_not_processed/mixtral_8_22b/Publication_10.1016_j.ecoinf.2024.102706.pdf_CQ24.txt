Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

and time overhead (Ke et al., 2017). LightGBM sets itself apart from
other tree-based methods through its leaf-wise splitting approach, which
generates more intricate trees. These trees are adept at minimising loss,
leading to enhanced accuracy. The splitting process is guided by a
unique sampling method called Gradient-Based One Side Sampling
(GBOSS) (Ke et al., 2017). This method excludes data with small gra-
dients while utilising the remainder for estimating information gain and
facilitating tree growth. A collection of hyperparameters, such as lear-
ning_rate, max_depth, bagging_fraction, bagging_freq, and n_estimators,
controls the algorithm. These parameters are crucial in shaping how the
leaves are structured within the trees. The model's complexity gradually
rises as the tree grows, guided by these parameters. Simultaneously, loss
decreases, and the algorithm becomes more adept at learning from the
data, thereby improving its efficiency. Moreover, this model is suscep-

verages bagging over boosting. This implies that RF constructs trees
independently and amalgamates their forecasts, a strategy that may
prove less efficacious in capturing intricate patterns when compared
with boosting techniques like LightGBM and XGboost. Both LightGBM
and XGboost stand out as potent gradient boosting models, each char-
acterised by unique variances in their tree growth strategies - leaf-wise
and tree level-wise, respectively. LightGBM specifically emphasises
expeditious training performance through the selective sampling of
high-gradient instances within extensive datasets, high-dimensional
data sets, and categorical attributes, owing to its employment of
histogram-based segmentation and adept management of categorical
variables (Al Daoud, 2019).

linear to machine learning methods, were fitted in training data to
construct regression equations. We used multiple linear regression, Enet,
SVM, RF, XGboost and LightGBM to predict canopy cover from planet
data. In general, all the mentioned models are used in machine learning
as they use algorithms to automatically learn patterns and relationships
from data to make predictions or decisions. However, we categorised

EcologicalInformatics82(2024)1027065A. Gyawali et al.

Table 4
The optimised hyperparameter values with grid search range in machine
learning models.

Model

Hyperparameters

Grid search

Optimised value

Enet

SVM

RF

Xgboost

LightGBM

Î±
l1_ratio

Cost (C)
gamma
Kernal
epsilon

n_estimators
max_features
max_depth
min_samples_split
min_samples_leaf

n_estimators
max_depth
colsample_bytree
min_child_weight
subsample
learning_rate

n_estimators
max_depth
learning_rate
num_leaves
feature_fraction
bagging_fraction
bagging_freq

approach (Li et al., 2022) have also been efficiently used in satellite
image-based vegetation indices for predicting forest canopy cover. The
application of deep convolutional neural networks (DCNN) for semantic
segmentation has also been investigated across various types of young
vegetation cover (Cameron et al., 2022). Regarding advanced machine
learning approaches, the RF and SVM were the foremost priorities for
the researchers for canopy cover modelling from O'brien, 2007 to 2017,
as reported by Thanh Noi and Kappas (2017). Other advanced ML ap-
proaches have recently been added to similar studies (Li et al., 2022;
Zhou et al., 2020). The boosting methodology represents a synthesis of
robust prediction techniques, frequently surpassing the performance of
individual models such as single decision trees. The emergence of
refined ensemble machine learning techniques like light gradient
boosting machines
represents notable advancements

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The context does not provide specific details on handling randomness in the deep learning pipeline, including managing random seed values. However, some common strategies include setting fixed seeds for reproducibility, using cross-validation techniques, and employing ensemble methods to reduce variance caused by random initialization.

Answer:::

The given context primarily discusses different machine learning models, particularly focusing on LightGBM, Random Forest (RF), Support Vector Machines (SVM), and other related topics. It provides detailed explanations about the working principles, advantages, and applications of these models but does not explicitly mention any strategies for handling randomness in the deep learning pipeline, such as dealing with random seed values.

However, it is essential to note that managing randomness in deep learning pipelines is critical for ensuring consistent results and reproducibility. Some commonly employed strategies include:

1. Setting fixed seeds: By initializing a fixed seed value before running experiments, one can ensure that the same sequence of pseudo-random numbers will be generated every time the code runs. This helps maintain consistency and makes results reproducible.

2. Cross-validation techniques: Using k-fold cross-validation or other validation techniques can help mitigate the impact of randomness due to dataset splits. By averaging the performance metrics obtained from multiple folds, one can get a better estimate of the true model performance.

3. Ensemble methods: Combining multiple models trained with different random initializations can help reduce the overall variance caused by randomness. Techniques like bagging, boosting, and stacking can improve the robustness and stability of the final predictions.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 4 
Definitions of model performance metrics based on “caret” R package, based on 
true positives (TP), true negatives (TN), false positives (FP), and false negatives 
(FN).  

Metric 

Equation 

Definition 

Accuracy 

Precision 

Recall 

TP + TN
TP + FP + TN + FN 
TP
TP + FP 

TP
TP + FN 

F1 

2*precision*recall
precision + recall  

Proportion of correct predictions in the whole 
data set. 
The proportion of images that a model classified 
as a specific category C that are actually category 
C. 
The proportion of images that are actually a 
specific category C that the a model classified as 
C. 
Weighted average of precision and recall.  

Table 5 
Performance of the image quality model on the test data.  

Location 

Finnmark 

Yamal 

Id 

Bad 
Good 
Bad 
Good 

Precision 

0.920 
0.986 
0.764 
0.979 

Recall 

0.910 
0.988 
0.779 
0.977 

F1 

0.915  
0.987 
0.771  
0.978

shown to reduce the processing time of image classification load by as 
much as 5× to 13× depending on the tasks (Fennell et al., 2022; Henrich

Quality  control  is  an  important  part  of  every  automated  image 
classification workflow (B¨ohner et al., 2023), and applying an automatic 
classification workflow to a new dataset requires particular care. Opti-
mally, in the case of a multi-annual monitoring program, a workflow 
should be validated by applying it to a year or season of data that has not 
been used for its development. As species identification depends in large 
part on the number of images available for training and the complexity 
of the community in a specific study, it may be necessary to manually 
check all images with animals. 

3. Materials and methods 

3.1. Camera trap setup and data collection 

Images  were  obtained  from  monitoring  programs  of  the  tundra 
carnivore  scavenger  guild  in  two  low  Arctic  regions:  “Yamal”  (the 

1  https://github.com/agentmorris/MegaDetector

3.2. Image quality classification: Training dataset and model training 

Using the manual classifications, we randomly selected images from 
each site, camera, and year, to obtain ~15,000 images of Bad quality 
and  ~  57,000  images  of  Good  quality  for  each  location  (Finnmark 
2016–2018 and 2020–2021 and Yamal 2017–2021). These images were 
then  reexamined  by  GC  and  DE,  and  any  misclassified  images  were 
removed or reclassified. We also excluded marginal images (e.g., partly 
blurred images, images where an animal is only visible with a tail in a 

Table 2 
Total number of images per classification group as assessed manually in Finn-
mark  and  Yamal  (N),  together  with  median  and  mean  (standard  deviation) 
percentage of images for each individual camera trap per year. The total dataset 
(workflow development and independent validation) comprised 36 cameras at 5 
sites for 6 years in Finnmark and 9 or 10 cameras for 6 years in Yamal.  

Location 

Class ID

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there isn't enough detail about the specific machine learning process used to train the models to provide a definitive answer regarding the criteria used to determine when training is complete. However, common criteria used to determine when training is complete include reaching a certain number of epochs, achieving a desired level of accuracy or other performance metric, or observing a plateau in the validation loss. In this case, since the table shows performance metrics such as Precision, Recall, and F1 score, it can be assumed that these metrics might have been used to evaluate the model during training and potentially determine when training should stop. But without more detailed information about the training process, it cannot be confirmed which specific criterion was used to determine when training was complete.
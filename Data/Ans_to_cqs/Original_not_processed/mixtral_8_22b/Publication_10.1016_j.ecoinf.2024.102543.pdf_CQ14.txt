Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

mAP =

1
k

∑k

i=1

APi

(3) 

We  also  consider  Params  and  FLOPs  to  evaluate  the  model.  The 
“Params”  metric  in  a  neural  network  model  refers  to  the  number  of 
learned variables used for making predictions. It is an essential metric 
for  evaluating  the  model's  complexity  and  computational  efficiency. 
Models  with  more  parameters  generally  require  more  resources  for

introduced  variations 

in  color 

The  training  set,  comprising  the  majority  of  the  data  (82%),  con-
taining 4000 images, is used to train the model and adjust its parame-
ters, allowing it to learn from a diverse range of examples and patterns in 
the data. The validation set (13%), consisting of 607 images, is utilized 
during  training  to  fine-tune  hyperparameters  and  assess  the  model's 
performance on unseen data, helping to prevent overfitting and ensuring 
generalization. Lastly, the testing set (5%), which included 260 images, 
serves  as  an  independent  evaluation  of  the  model's  performance  on 
completely unseen data, providing a reliable measure of its real-world 
effectiveness and ability to generalize. 

2.3. Performance evaluation 

To evaluate the ablated model results, we use five metrics, namely 
Precision (Eq. (1)), Recall (Eq. (2)), mAP0.5 and mAP0.5:0.95, related to 
Eq. (3), Params(M), Flops(G), Inference(ms) and Time(h). 

Precision =

Table 2 
Comparing algorithm performance in terms of precision, mean average precision 
at IoU 0.5, mean average precision at IoU 0.95, model parameters, floating-point 
operations, inference time, and total processing time.  

Model 

Prec.  mAP0.5  mAP0.95 

84.4 
62.5 

61.5 
56.5 

34.4 
35 

Par. 
(M) 

3.01 
1.81 

Flops 
(G) 

8.1 
5.1 

Inf. 
(ms) 

0.4 
0.3 

Time 
(h) 

0.33 
0.32 

77.1 

65 

39.2 

2.92 

12.2 

0.7 

0.33 

78.3 

71.1 

38.8 

1.69 

8.6 

0.6 

0.33  

YOLOv8n 
YOLOv8n +
C2f2 
YOLOv8n +
P2 
(Proposed 
model)

Furthermore,  adding  P2  and  C2f2  did  not  considerably  increase 
model complexity, as seen in the lower number of parameters in Fig. 10 
and FLOPs compared to YOLOv8n + C2f2. We also observed that the 
addition of P2 and C2f2 to YOLOv8n did not result in a noticeable in-
crease in inference time, as seen in the similar values of inference time 
between YOLOv8n and the proposed model (YOLOv8n + P2 + C2f2). 
This result suggests that adding P2 and C2f2 simultaneously to YOLOv8n 
can noticeably improve the algorithm's performance without compro-
mising  its  speed  or  model  complexity,  indicating  that  the  algorithm 
obtained can perform satisfactorily in real-time tasks, which is crucial 
for many applications, such as real-time object detection in videos. 

Inference  on  the  testing  set  of  the  INSECT  dataset  is  covered  in

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there isn't any explicit mention of how the hyperparameters of the model were optimized, whether through methods like grid search, random search, Bayesian optimization, or others. However, it does state that the validation set was used during training to fine-tune hyperparameters. This implies that some form of hyperparameter tuning was performed, but the specific method remains unknown based solely on this text.
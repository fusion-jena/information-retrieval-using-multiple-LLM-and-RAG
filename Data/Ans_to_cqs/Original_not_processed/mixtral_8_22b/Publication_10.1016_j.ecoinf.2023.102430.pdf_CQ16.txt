Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.3. Algorithm training and testing 

We trained all algorithms on a random subset of 5256 pictures (80% 

of  the  6570-picture  dataset).  To  gauge  performance  consistency,  the 
training-testing  process  was  repeated  10  times  for  each  algorithm;  in 
each of these (pseudo-)replicate runs, all five algorithms used the same 
training and testing picture subsets.

AN  was  implemented  in  MATLAB  (www.mathworks.com).  For 
training,  we  used  the  stochastic  gradient  descent  with  momentum 
optimizer with default settings except for the initial learning rate (which 
we set to 0.001) and the maximum number of epochs (which we set to 15 
after  preliminary  tests).  The  ‘classical’  machine-learning  algorithms 
(AB, GB, HB, and LD) were run in Python (www.python.org) using the 
scikit-learn toolbox (Pedregosa et al., 2011). We left hyperparameters at 
scikit-learn  default  values,  except  for  the  learning  rate  of  GB  and  HB 
(which we set to 0.15) and the tolerance threshold value for LD (set to 1 
× 10

Abbreviations: CD, Chagas disease; AN, AlexNet; AB, AdaBoost Multi-Class Adaptive Boosting classifier; GB, Gradient Boosting classifier; HB, Histogram-based 
Gradient Boosting classifier; LD, linear discriminant model; DALY, Disability-Adjusted Life Year; GLMM, generalized linear mixed model; AICc, Akaike’s information 
criterion corrected for finite samples. 

* Corresponding author. 

E-mail address: gurgelrg@hotmail.com (R. Gurgel-Gonçalves).   

1  Equal contribution 

https://doi.org/10.1016/j.ecoinf.2023.102430 
Received 27 July 2023; Received in revised form 11 December 2023; Accepted 11 December 2023

We analyzed the outcome of our 50 testing runs (10 per algorithm) 
using  R  4.2.1  (R  Core  Team,  2022)  and  the  RStudio  2023.03.1.446 
interface  (Posit  Software,  2022).  We  first  summarized  our  data  in 
descriptive tables, including cross-classification or ‘confusion’ matrices, 
calculated Cohen’s κ  agreement scores (Cohen, 1960), and ran simple 
exploratory analyses by calculating and graphing frequencies and pro-
portions. Proportions and their Wilson ‘score’ 95% confidence intervals 
(CIs)  (Newcombe,  1998)  were  computed  using  the  Hmisc  R  package 
(Harrel and Dupont, 2023).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly state the specific criteria used to determine when training is complete. However, it mentions that the maximum number of epochs for the AlexNet (AN) algorithm was set to 15 after preliminary tests. This suggests that one possible criterion could be reaching a certain number of epochs during training. Additionally, the authors mention adjusting the learning rates for some algorithms, which implies they may have been monitoring the progress of the training process and making adjustments accordingly. Without further details, it cannot be definitively stated what exact criteria were used to determine when training was complete.
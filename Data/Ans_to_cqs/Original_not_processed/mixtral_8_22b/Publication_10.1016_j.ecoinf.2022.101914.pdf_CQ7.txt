Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

points. Then, to obtain presences and background points to use during 
external evaluation, we set aside one fold. We then used the remaining 
four folds for internal cross-validation during model training.

4.2. Ensemble models

to model performance by a single component model built with subop-
timal training data. However, RF outperformed all the ensemble models, 
suggesting  that  if  researchers  use  a  single  modeling  approach,  they 
should consider using RF. It should be noted that we found RF to be more 
sensitive to the number of background points used to train the model 
than  other  modeling  methods.  However,  researchers  or  practitioners 
might choose to use a single modeling approach that is found to be less 
sensitive to the number of background points, like a GLM or MaxEnt. In 
this situation, since the gain in performance is minimal, the decision on 
how many background points to use might be instead made to minimize 
computation time and answer time-sensitive questions related to con-
servation  actions.  Nevertheless,  using  an  ensemble  of  models  helps 
overcome any issues that any single model might have, as demonstrated

2.8. Training individual models 

To train the component models for the ultimate ensemble models, we 
used  six  modeling  approaches,  categorized  into  regression-based  and 
machine  learning models. Regression-based  models included General-
ized Linear Models (GLMs), Generalized Additive Models (GAMs), and 
Multivariate  Adaptive  Regression  Splines  (MARS).  Machine  learning 
models included Random Forest (RF), Artificial Neural Networks (ANN), 
and Maximum Entropy software (MaxEnt). We built all models using the 
biomod2 package in R (Thuiller et al., 2021).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide explicit information about the specific criteria used to split the data for deep learning model training, such as train, test, and validation sets. However, it mentions that the dataset was divided into five equal parts or folds for cross-validation purposes. One fold was set aside for obtaining presences and background points for external evaluation, while the remaining four folds were used for internal cross-validation during model training. This suggests that the authors may have employed a form of k-fold cross-validation, where k equals five in this case. K-fold cross-validation involves splitting the dataset into k equally sized subsamples or folds. Of these k folds, one fold is retained as the validation data for testing the model, while the remaining k-1 folds are used as training data. This process is repeated k times, with each unique fold serving as the validation data once. The results from each run can then be averaged to produce a single estimation. Nonetheless, without further details regarding the exact methodology used, it cannot be definitively stated what specific criteria were applied to split the data for deep learning model training.
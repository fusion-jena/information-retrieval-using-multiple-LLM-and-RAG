Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The data used for model training and selection were randomly par-
titioned into: data for training candidate models (25% of the data; At); 

data for validating candidate models (50%; Av); data for training the 
selected  candidate  model  (75%;  Bt  = At  + Av);  validation  data  to 
determine the optimal number of epochs to train the selected candidate 
model (25%; Bv). Data partition was performed in R with package dismo 
(Hijmans et al., 2017; R Core Team, 2022). 

2.5. Model selection procedure

2.5. Model selection procedure 

The model selection procedure was performed as follows (Capinha 
et  al.,  2021;  Van  Kuppevelt  et  al.,  2020):  we  randomly  generated  5 
models for each of the four available deep-ANN architecture types (20 
models in total) and trained each one with a small subset of the training 
data  (data  partition  At)  for  4  epochs  (an  “epoch”  corresponds  to  the 
complete training dataset being passed forward and backward across the 
network  one  time;  Capinha  et  al.,  2021).  The  accuracy  of  candidate 
models,  as  provided  by  mcfly  (i.e.,  the  “proportion  of  cases  correctly 
classified”), was then compared using a left-out validation data set (data 
partition Av) and the model with the highest performance was selected 
for training on the full training data (data partition Bt; Bt = At + Av) for 
up to 30 epochs.

After  full  training  and  identification  of  the  optimal  number  of 
training  epochs,  this  model  delivered  an  excellent  average  predictive 
performance for all years (mean AUC = 0.92 ± 0.05 sd) used as testing 
sets. Only  in  a  few cases the  performance went  below this  threshold, 
namely for class “increase >50%” where performance was fair (i.e., AUC 
from 0.7 to 0.8) for years 2013 and 2016, and good (AUC 0.8 to 0.9) for 
the  remaining  years,  and  for  class  “increase  25–50%”  for  year  2017, 
where the model performance was good (AUC = 0.88) (Table 2). 

Concerning the importance of variables, AUC decreased most noto-
riously when the predictor ‘number of eggs’  was randomized, a result 
observed for all test years (Fig. 3). Slight decreases in performance are 
also  apparent  when  temperature  was  randomized  (either  mean, 
maximum or minimum) for most test years. 

3.3. Classical machine learning model accuracy and predictive 
performance

2.4. Data partition 

The ‘autoML’  workflow in  mcfly requires using multiple partitions 
from the full data set of response and predictor variables. Here we fol-
lowed the partition scheme of Capinha et al. (2021), consisting of four 
partitions  that  are  used  at  the  various  stages  of  model  selection  and 
training, and a fifth partition that is used to assess the predictive per-
formance of the fully trained (‘final’) model. To ensure independence 
between the data used for model training and the data used for model 
testing, we used the data of each year (i.e., 2013 to 2019) separately for 
model testing (partition ‘T'), and the data for the remaining years for 
model training. The aim of this procedure is to mimic an operational 
setting where data available for previous years is used to train a model 
that  will  be  employed  in  the  real-time  forecasting  of  changes  in  the 
number of eggs for the coming week.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: The strategy implemented to monitor the model performance during training involves several steps. Initially, 20 candidate models were created, each belonging to one of the four available deep-ANN architectures. These models were trained with a small subset of the training data (At) for four epochs. The accuracy of these candidate models was evaluated using a separate validation dataset (Av). The model with the highest performance was chosen for further training on the complete training dataset (Bt = At + Av) for up to 30 epochs. After full training, the optimal number of training epochs was determined based on the best performance achieved. This final model demonstrated excellent overall predictive performance, with a mean area under the curve (AUC) of 0.92 ± 0.05 standard deviation. However, there were some exceptions where the performance dropped slightly, such as for the 'increase >50%' class in 2013 and 2016, and the 'increase 25–50%' class in 2017. Additionally, variable importance analysis revealed that the 'number of eggs' predictor had the most significant impact on model performance, while slight decreases in performance were observed when temperature variables were randomized.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

similarities. It is widely used to visualize high-dimensional data (van der 
Maaten  and Hinton, 2008). Here the  method has been applied to the 
fc1000 activation vectors returned by ResNet-50. 

4.2. Classifier model 

The labeled dataset was split 70/30 into training and testing data-
sets.  For  model  selection  and  parameter  optimization  50%  of  the 
training data were held-out for validation (see Supporting Information). 
The final classification model, a SVM with a cubic polynomial kernel, 
was trained using the ResNet-50 fc1000 activations of the full training 
dataset. A 10-fold cross-validation was then used to define the score-to- 
posterior-probability transformation function (Platt, 2000), which pro-
vides a basis for estimating the posterior probability for new observa-
tions. Fig. 9 shows a confusion matrix for the test data not seen during 
training.

extracted  automatically  using  the  activations  from  ResNet-50's  deep, 
fully connected layer fc1000, located just before its classification layer. 
Based on these activations, a one vs. all support vector machine (SVM) 
classifier  is  then  used  to  identify  the  optimal  decision  boundary  to 
differentiate ‘boatwhistles' from ‘other’ signals within the feature space 
(Fig.  6)  (e.g.,  Kecman,  2005).  Using  a  pre-trained  deep  CNN  with  an 
SVM is a form of transfer learning (e.g., Bousetouane and Morris, 2015; 
Halberstadt, 2020), and the workflow mirrors that used by Bohnenstiehl 
(2023a) for cataloging American silver perch calls. 

3.5. Data labeling

These  spectrogram images  are  input  into  the  ResNet-50  deep  (50- 
layer) convolutional neural network (CNN), which was developed and 
pre-trained using millions of label images in over 1000 categories (He 
et  al.,  2015).  Although  ResNet-50  was  not  explicitly  trained  on  spec-
trograms, many studies have shown that CNNs pre-trained for one task 
can  extract  meaningful  features  distant  from  the  original  training 
dataset  (e.g.,  Almabdy  and  Elrefaei,  2019;  Bousetouane  and  Morris, 
2015; Copiaco et al., 2019; Razavian et al., 2014). A suite of features is

Acknowledgments 

Hydrophone deployment and recovery were facilitated by O. Caretti, 
with  assistance  from  R.  Patrick  Lyon,  E.  Voigt,  and  K.  Simmons.  The 
members of the Jones Bay Club generously provided use of their boat 
ramp  for  instrument  deployment  and  recovery.  This  work  benefited 
from discussions with S. Ricci and D. Eggleston, as well as from early 
testing and critical conversations with O. Caretti and R. Van Hoeck. 

References 

Almabdy, S., Elrefaei, L., 2019. Deep convolutional neural network-based approaches for 
face recognition. Appl. Sci. 9 (20), 4397. https://doi.org/10.3390/app9204397. 

Amorim, M.C.P., 2006. Diversity of sound production. In: Ladich, F., Collin, S.P., 
Moller, P., Kapoor, B.G. (Eds.), Communication in Fishes. Science Publishers, 
Enfield, NH, pp. 69–103.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

From the given context, it appears that the authors did not specifically mention any optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) in their deep learning pipeline. However, they do discuss some aspects of their approach that could be related to optimization.

In section 4.2, they mention splitting the labeled dataset into 70/30 for training and testing datasets. They also hold out 50% of the training data for validation, indicating that they likely used this subset for tuning hyperparameters or optimizing their model.

Furthermore, they state that they used a Support Vector Machine (SVM) with a cubic polynomial kernel as their final classification model. To train this model, they utilized the ResNet-50 fc1000 activations of the full training dataset. Afterward, they employed a 10-fold cross-validation to define the score-to-posterior-probability transformation function. These steps suggest that they may have used some form of optimization technique, although the specific algorithm remains unclear without further information.

Additionally, in section 3.5, they describe using a pre-trained deep Convolutional Neural Network (CNN) called ResNet-50. While they do not provide details about the optimization techniques used during the initial training of ResNet-50, it is common practice to employ optimization algorithms like SGD or Adam when training deep learning models. Therefore, it can be assumed that similar techniques might have been used during the development of ResNet-50.

In conclusion, while the text does not directly address the use of specific optimization techniques like SGD or Adam, there are indications that some form of optimization took place during the development and application of their deep learning pipeline.
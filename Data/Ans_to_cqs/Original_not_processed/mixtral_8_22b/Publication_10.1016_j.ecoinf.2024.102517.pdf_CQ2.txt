Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Abbreviations: Signal-to-noise ratio, SNR; perceptual evaluation of speech quality, PESQ; Web real-time communications, WebRTC; voice over Internet protocol, 
VoIP; minimum mean-square error short-time spectral amplitude, MMSE STSA; speech enhancement generative adversarial network, SEGAN; generative adversarial 
network, GAN; fully convolutional neural network, FCN; deep autoencoder, DAE; leaky rectified linear unit, LReLU; detection and classification of acoustic scenes 
and events, DCASE; peak signal-to-noise ratio, PSNR. 

* Corresponding author Xinghui Gao at: School of Electronics and Communication Engineering, Guangzhou University, China. 
** Corresponding author Yingying Guo at: School of Mechanical and Electrical Engineering, Guangzhou University, China. 

E-mail addresses: gaoxh@gzhu.edu.cn (X. Gao), guoyingying@gzhu.edu.cn (Y. Guo).

The  audio  classification  loss  network  consists  of  convolutional 
layers, average pooling layers, and a logic classifier layer. Layers 1 to 13 
also include convolutional operations, batch normalization, and LReLU 
operations,  with  down-sampling  operations  performed  between  each 
layer  with  a  factor  of  2.  Layer  14  includes  convolutional  operations, 
normalization, and nonlinear operations, omitting the down-sampling 
operation.  Layers  15  and  16  are  the  average  pooling  and  the  logic 
classifier layers, respectively. 

The loss is calculated as the weighted L1 distance between the first 
and  second  groups  of  activation  features,  which  can  be  expressed  as 
follows (Germain et al., 2018): 

Lβ,x(θ) =

∑M

m=1

λm‖Φm(β) (cid:0) Φm(g(x; θ) ) ‖1,

(3)

Inspiration  for  the  design  of  the  audio  classification  loss  network 
used in the bird noise reduction network training was derived from the 
VGG  architecture  in  computer  vision.  The  audio  classification  loss 
network was alternately pre-trained using different background noises 
and classes of associated sounds combined with a preset acoustic scene 
classification  task  and  an  audio  labeling  task.  The  relevant  training 
dataset is derived from a study by Mesaros et al. (2018). When training, 
the training data is iterated for the two tasks, alternating between the 
audio files for each task, of which acoustic scene classification includes 
classifying  differing  background  noises.  This  ensures  that  the  audio 
classification loss network captures scene information in the presence of 
different background noises. The audio labeling task involves classifying 
different human and animal sounds, thereby enabling the audio classi-

reducing noise, and accordingly, would not be suitable for processing 
large  volumes  of  recorded  data  collected  in  the  field.  Furthermore, 
compared with the bird sound noise reduction network based on deep 
feature loss and the SEGAN algorithm, we found that the noise reduction 
efficiencies of the WebRTC and Wiener Filter algorithms were notably 
higher. This disparity in performance can be attributed to the fact that 
the deep learning model is larger in size and is thus more demanding of 
computational resources. However, it should be noted that the execution 
time of the deep learning models on CPUs is acceptable.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, the deep learning pipeline primarily uses audio data. Specifically, the audio classification loss network is trained using different background noises and classes of associated sounds combined with a preset acoustic scene classification task and an audio labeling task. The relevant training dataset is derived from a study by Mesaros et al. (2018). Therefore, it can be concluded that the primary data format used in this deep learning pipeline is audio data.
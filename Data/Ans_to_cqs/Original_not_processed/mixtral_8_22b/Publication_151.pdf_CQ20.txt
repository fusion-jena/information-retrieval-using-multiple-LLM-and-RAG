Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

by a running average of its recent magnitude to speed
up training (Tieleman and Hinton 2012). The batch size
is ﬁxed to 32. We check the training loss every 50 epochs
and stop the training when the value of the loss function
fails to decrease in two consecutive checks. The model
with the optimal objective function value is chosen.

{

=

=

n
i

n
i

Di}
{

DR
i }

1 and

Evaluation on Simulated Data Sets
DEPP training and parameter sensitivity.—We start by
evaluating DEPP on simulated data sets, testing the
ability to train the CNN model in reasonable times. As
the training epochs advance, the loss function (1) drops
rapidly and stabilizes after around 500 epochs in a typical
case (Fig. S1 of the Supplementary material available on
Dryad). Here, training, which is a one-time process for
each reference tree, ﬁnished in around 20 min for the 200-
taxon data set and 260 minutes for 10,000-taxon data set,
on a machine with one 2080Ti NVIDIA GPU and 8 CPU
cores. Placement of 1000 queries took 4 seconds for the
200-taxon and 30 s for the 10,000-taxon data sets using a
single CPU core. On the small 200-taxon data set, EPA-ng
has an advantage in terms of running time. However, in
the larger HGT data set (10,000-taxon), DEPP placements
are faster than the alternatives with half the running time
of EPA-ng. In terms of the memory usage, APPLES+JC

DEPP implementation details.—We implemented DEPP
using PyTorch and treeswift python packages (Moshiri
2020) and trained the models on 2080Ti NVIDIA GPUs.
The embedding size k is set to 128 for data sets with 200
taxon and 512 for larger data sets (including the real web-
of-life [WoL] data set). Other hyperparameters are ﬁxed
to their defaults (Table S2 of the Supplementary material
available on Dryad) unless otherwise speciﬁed. DEPP
is trained on the reference tree and is used to compute
distances that are then fed to APPLES-II (Balaban
et al. 2022), used identically to APPLES-II+JC (see
below). Branch lengths of the backbone tree provided
to DEPP are re-estimated using RAxML-8 (Stamatakis
2014) under the GTR+CAT model. Given more than one
gene, DEPP has two options: concatenating genes or
computing a summary of distances. For each query, we
can compute the distance between a query and backbone
species j according to each of the N genes, obtaining

Embedding size. The Layer and Rhodes (2017) (LR)
formulation requires n
1 dimensions, which introduces
some challenges. According to the theory, the number of
dimensions needs to increase by one after inserting the
query. Our supervised learning formulation does not
allow that (the embedding size is ﬁxed after training).
Thus, there is no guarantee that the embeddings remain
correct after addition, even if they are before addition.
However, we note that, in LR embeddings, adding a
leaf would require simply dividing one of the n
1
dimensions into two dimensions, leaving the rest of the
embeddings intact. Thus, one can hope that having one
less dimension has a minimal practical impact. More
broadly, for large n, training models with n-dimensional
embedding is impractical. Thus, we often set k < n
1,
and the gap can be more than an order of magnitude for
some of our tests described below. In practice, we use a
rule of thumb to select the default k (which the user can

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Answer: The deep learning model was trained using 2080Ti NVIDIA GPUs. No mention of other specific hardware resources such as TPUs was made in the given context.
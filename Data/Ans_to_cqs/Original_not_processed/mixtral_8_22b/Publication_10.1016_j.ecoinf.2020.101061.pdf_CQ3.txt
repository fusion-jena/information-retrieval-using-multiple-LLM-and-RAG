Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Collecting a suﬃcient number of training samples will often be a
bottleneck in developing supervised methods in airborne imagery. It is
therefore useful to test the number of local training samples needed to
achieve maximum performance. We performed a sensitivity study by
training models using diﬀerent proportions of training data. We se-
lected 5%, 25%, 50% and 75% of the total hand-annotations to com-
pare to the full dataset for the within-site results for each site. We reran
this experiment ﬁve times to account for the random subsampling of
annotations. In addition, we ran the evaluation plots for the pretraining
model only (i.e. 0% hand-annotated data) to assess whether the addi-
tion of hand-annotated data improved the within-site pretraining.

3. Results

For each site, we manually annotated training tiles using the pro-
gram RectLabel (Table 1). Training tiles were selected at random from
the NEON data portal. At higher tree density sites, we cropped the 1km2
tiles to create more tractable sizes for hand-annotation. To enforce a
minimum size threshold for tree annotations, we compared the hand-
annotations to a LiDAR canopy height model and removed any trees
less than 3 m in height. The resulting annotations were compared to the
LiDAR point cloud for further assessment. No attempt was made to
delineate understory trees that were not visible in the RGB imagery.
Since these annotations were made using only remotely sensed ima-
gery, there is some uncertainty in the bounding box extents. While
methods exist for combining imagery and ﬁeld-collected labels (e.g.
Aubry-Kientz et al., 2019; Graves et al., 2018), these are diﬃcult to
implement at large scales. Associated uncertainty should be considered

archived on Zenodo, and all annotations are available as part of the
forthcoming NEON Tree Benchmark (https://github.com/weecology/
NeonTreeEvaluation).

Ecological Informatics 56 (2020) 101061

Fig. 7. Ablation curves of the proportion of hand-annotation training data for each site. Values indicate the number of trees in the training dataset for each cutoﬀ.
Shaded area is the range of results from rerunning the analysis ﬁve times for each site. Note that due to the random sampling among runs, the exact number of trees
will vary slightly. For simplicity, we show the mean number of training trees for each threshold.

Data availability

All data and code are made available through a github repo
(https://github.com/weecology/DeepLidar/) and archived in Zenodo
(doi:https://doi.org/10.5281/zenodo.3347164). We are also actively
available
as part
building
a publicly
NeonTreeEvaluation
(https://github.com/weecology/
NeonTreeEvaluation).

larger dataset

benchmark

of

a

Acknowledgements

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors used manual annotation techniques to label their training data. Specifically, they used the program RectLabel to draw bounding boxes around individual trees in their training images. They enforced a minimum size threshold for tree annotations by comparing the hand-annotations to a LiDAR canopy height model and removing any trees less than 3 meters in height. Additionally, they did not attempt to delineate understory trees that were not visible in the RGB imagery. This suggests that the authors primarily used bounding box annotation techniques to label their training data. However, it is worth noting that the authors acknowledge some uncertainty in the bounding box extents due to the use of remotely sensed imagery alone.
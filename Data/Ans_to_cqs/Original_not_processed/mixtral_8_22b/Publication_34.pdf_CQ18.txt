Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

10.3.5 Evaluation Metrics

Predictions are made for every species of E50 and several model performance
metrics are calculated for each species and for two disjoints and randomly sampled
subsets of sites: A train set (4781 sites) which is used for ﬁtting all models and a
test set (400 sites) which aims at testing models generalization capacities. Then,
train and test metrics are averaged over the 50 species. The performance metrics are
described in the following.

10.3.5.1 Mean Loss

Mean loss, just named loss in the following, is an important metric to consider
because it is relevant regarding our ecological model and it is the objective function
that is minimized during model training. The Mean loss of model m on species i
and on sites 1, . . . , K is:

Loss(m, i,

1, . . . , K

{

)
}

=

1
K

K

k

1

=

λm,θi (xk)

−

yi
k log(λm,θi (xk))

.,.,j is (d.reslat,j

46.

64

×

=

×

×

10.3.4 Detailed Models Architectures and Learning Protocol

MAXENT is learned independently on every species of E50. Similarly, we ﬁt a
classic loglinear model to give a naive reference. Then, two architectures of NN are
tested, one with a single hidden layer (SNN), one with six hidden layers (DNN).
Those models take a vector of environmental variables xk as input. As introduced
previously, we want to evaluate if training a multi-response NN model, i.e. a NN
predicting several species from a single aNh(m)
(x, θ ), can prevent overﬁtting. One
m
architecture of CNN is tested, which takes as input an array Xk. Hereafter, we
described more precisely the architecture of those models.

10.3.4.1 Baseline Models

• LGL Considering a site k, and its environmental variables vector xk, the output
p is simply the

function λLGL of the loglinear model parametrized by β
exponential of a scalar product between xk and β :

R

∈

λLGL(xk, β)

exp

βT xk

=

Whatever is the number of responses for SNN, the model is under-ﬁtting and its
performance are stable, without any big change between SNN50, 200, and 1 K. This
model doesn’t get improvement from the use of training data on a larger number
of species. Furthermore, its performance is always lower than DNN’s, which shows
that stacking hidden layers improves the model capacity to extract relevant features
from the environmental data, keeping all others factors constant.

m (x, θ )

j )T al

:=

1,.

Learning of model parameters is done through optimization (minimization
by convention) of an objective function that depends on the prediction goal.
Optimization method for NN parameters θ is based on stochastic gradient descent
algorithms, however, the loss function gradient is approximated by the back-
propagation algorithm [18].

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The strategy implemented to monitor the model performance during training involves calculating several model performance metrics for each species and for two disjoint and randomly sampled subsets of sites - a train set and a test set. The train set, consisting of 4781 sites, is used for fitting all models, while the test set, comprising 400 sites, aims to test the models' generalization capabilities. These train and test metrics are then averaged over the 50 species. An essential metric considered is the mean loss, also known as loss, which is relevant to their ecological model and serves as the objective function minimized during model training.
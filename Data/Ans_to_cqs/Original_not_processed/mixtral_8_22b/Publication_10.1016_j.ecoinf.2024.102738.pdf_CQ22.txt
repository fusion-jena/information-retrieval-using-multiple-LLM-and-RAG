Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Name

Inputs

Name

Inputs

Dataset 1
Dataset 2
Dataset 3
Dataset 4
Dataset 5
Dataset 6
Dataset 7
Dataset 8
Dataset 9
Dataset 10
Dataset 11
Dataset 12
Dataset 13
Dataset 14
Dataset 15
Dataset 16

Spectral bands (B)
Vegetation Indices (VI)
Soil Indices (SI)
Water Indices (WI)
Cluster (C)
B + VI
B + SI
B + WI
B + C
VI + SI
VI + WI
VI + C
SI + WI
SI + C
WI + C
B + VI + SI

Dataset 17
Dataset 18
Dataset 19
Dataset 20
Dataset 21
Dataset 22
Dataset 23
Dataset 24
Dataset 25
Dataset 26
Dataset 27
Dataset 28
Dataset 29
Dataset 30
Dataset 31

B + VI + WI
B + VI + C
B + SI + WI
B + SI + C
B + WI + C
VI + SI + WI
VI + SI + C
VI + WI + C
SI + WI + C
B + VI + SI + WI
B + VI + SI + C
B + VI + WI + C
B + SI + WI + C
VI + SI + WI + C
B + VI + SI + WI + C

Table 3
LULC classes based on CLC methodology.

Class

Description

Code

RGB color

Artificial surfaces

Agricultural areas

Forest

Scrub and/or
herbaceous
vegetation

Open spaces with
little or no
vegetation
Water bodies

2.3.5. Measuring the accuracy of LULC maps

The assessment of accuracy can be approached from various per-
spectives, utilizing expert judgment (visual inspection) or external sta-
tistical sources, and this process must be carried out to ensure the
highest reliability of the results with the ground truth (Blissag et al.,
2024). In this research, the measurement of accuracy was based on the
validation points. Using this approach, the confusion matrix was
calculated, allowing us to obtain the Overall Accuracy (OA) and Kappa
Index (KI), coefficients necessary for the evaluation of classification
accuracy (Thomlinson et al., 1999). For this purpose, Eqs. (1) and (2)
were followed.

OA = Total number of pixel classified correctly

total number of pixels

*100

(1)

N

∑n

i=1

KI =

ai,i (cid:0)

(Ti + Fi)

N2 (cid:0)

(Ti + Fi)

∑n

i=1

∑n

i=1

2.3.4. Sampling size and validation areas

On this occasion, to define the training and testing samples, we relied
on Chuvieco (2020), who argues that for categorical variables, at least
196 test points should be considered. Furthermore, Chuvieco (2020),
also mentions that, at times, this number can be very small when
compared to the total number of pixels in the image. Therefore, we
explored new sampling strategies that combine the previous scientific
foundation with the current capabilities of GEE.

Additionally, Fig. 7 shows the distribution of the overall accuracies
of the 9 decision trees, indicating that the highest density of cases is
concentrated in the accuracy range between 0.6 and 1. This suggests a
common trend towards high levels of overall accuracy in the classifi-
cations performed by all types of trees.

3.2. LULC maps

As Dataset 19 obtained higher accuracies, its inputs were used to
obtain the LULC map of 1989 and 2005, and 100 decision trees for each
year. Fig. 8 displays the RGB and LULC maps for 1989, 2005, and 2023.

3.2.1. LULC changes

Fig. 9 shows the square kilometers covered by the different LULC
categories and Table 8 shows gains and losses for the three years of
analysis.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, two main metrics are mentioned for evaluating the performance of the land use and land cover (LULC) classification using deep learning models: Overall Accuracy (OA) and Kappa Index (KI). These metrics are calculated using a confusion matrix derived from validation points. The OA is determined by dividing the total number of correctly classified pixels by the total number of pixels and multiplying by 100. On the other hand, the KI measures the agreement between predicted and observed categories while accounting for chance agreements. It is computed using the formula presented in the context, which involves summing the diagonal elements of the confusion matrix and normalizing it by the total number of pixels squared.
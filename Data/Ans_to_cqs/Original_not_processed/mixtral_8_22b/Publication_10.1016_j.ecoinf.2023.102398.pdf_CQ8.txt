Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The training is done from scratch using GPU (Tesla T4) and CUDA 
Version-12.0 in Google Colab with the parameters (i) Input image size =
416, (ii) Batch size  = 16,  (iii) Epochs = 100, 150, 200  and 250, (iv) 
Weights  = Default  to yolov5.pt., (v) Data = custom data set  and (vi) 
Cache = True. 

EcologicalInformatics79(2024)10239811B. Bhagabati et al.                                                                                                                                                                                                                              

Fig. 11. Training Plots for the proposed model.

In  order  to  determine  more  accurate  training  results  and  also  to 
explore the effect of epoch upon training result, apart from 150 epochs, 
the model with attention layer is trained with epoch values 100, 200, 
and  250  under  a  uniform  training  environment  and  with  the  same 
dataset.  The  training  summary  for  each  of  these  epochs  is  shown  in 
Tables 5, 6 and 7 for epochs 100, 200, and 250, respectively. The trends 
of mAP values with increasing epochs are shown in Figs. 14 and Fig. 15. 
The size of the dataset used for custom training is sufficiently large. 
Further, the DL model used in this work is tuned with optimized hyper 
parameters  for  which  overfitting  and  under  fitting  situations  are  pre-
vented from occurring. As mentioned above, the effort to over-train the 
model has been avoided and graded MSE convergence (Figs. 7 to 11) has 
been  adopted  to  fine-tune  the  model.  Results  obtained  for  different

In recent years, it has been observed that deep learning methods can 
be trained with close to zero training error efficiently. The number of 
convolutional and dense layers directly affects the runtime of the model 
(Bienstock et al., 2023). The running time of a DL model increases in 
polynomial-terms with the increased number of associated layers. But 
with the effort of achieving zero error, there are possibilities of over-
training and biased training. To avoid these situations, a restrictive and 
gradually increasing training cycle and accuracy calculation approach 
has  been  adopted.  Based  on  mean  square  error  (MSE),  the  optimal 
training state for the model has been obtained. 

4.1. Performance metrics used to evaluate the model

next  section,  there  are  proven  approaches  based  on  CNN  models  for 
controlling  human-elephant  conflicts  and  saving  other  wild  animalsâ€™ 
lives  from  fatal  road  accidents.  To  train  a  CNN-based  model,  large 
datasets are required. Various groups of people are maintaining large 
datasets  of  wild  animals  that  can  be  used  for  training  a  specially 
configured CNN model to obtain better performance (Zhou et al., 2022). 
However, computation time and local resources required to train such 
approaches  are  high,  which  appears  to  be  a  major  limitation  of  such 
approaches. Hence, ample opportunities exist to design and configure 
fast cloud resident-modified CNN-based DL models with multiple cam-
era inputs for deployment around the critical infrastructure, including 
highways passing through important biological hotspots (like the NH-37 
passing through the KNP).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not provide information about where the code repository of the deep learning pipeline is available. It only mentions details about the training process, such as the use of Tesla T4 GPU, CUDA version 12.0, input image size, batch size, epochs, weights, data, and cache settings. However, it does not specify any links or references to the actual code repository. Therefore, I cannot provide the requested information based on the given context.
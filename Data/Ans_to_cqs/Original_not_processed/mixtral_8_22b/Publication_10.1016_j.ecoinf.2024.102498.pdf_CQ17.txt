Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Due to processing limitations in GEE, it is impossible to utilize many 
samples and automate hyperparameter tuning. To fill this gap and avoid 
overfitting,  the  validation  data  were  employed  for  tuning  the  key 
hyperparameters for each model. Table 3 presents the hyperparameters 
set for each model during training and validation. 

5.3.2. Pixel-based approach 

The  pixel-based  classification  approach  relies  on  the  spectral 

Table 3 
Hyperparameters were set for each machine learning technique within the GEE 
platform.  

Model 

CART   

RF   

GTB   

SVM   

Hyperparameters 

• Max nodes: No limit  
• Min leaf population: 1 

• Number of trees: 264  
• Bag fraction: 0.5  
• Variables per split: Root of the number of variables 

• Number of trees: 20  
• Shrinkage: 0.005  
• Sampling rate: 0.7  
• Loss: Least absolute deviation 

• Kernel type: RBF  
• SVM type: C_SVC  
• Decision procedure: Voting

GTB: The next algorithm used in this study is GTB, which sequen-
tially improves its classification performance. This model is an ensemble 
method  that  builds  trees  sequentially,  with  each  tree  attempting  to 
correct the weakness of the previous tree (Georganos et al., 2018). The 
execution of this model demands high computational power. This lim-
itation allowed for a maximum of 20 trees in the model for this research. 
Like RF, GTB can handle datasets with various features, and combining 
differently weighted trees prevents overfitting (Ouma et al., 2022).

An alternative approach to enhancing the accuracy of LULC maps is 
to leverage deep learning techniques. These techniques operate patch- 
wise,  where  each  patch  incorporates  sophisticated  features  extracted 
from satellite imagery. This enables them to recognize intricate patterns 
effectively.  Particularly,  Convolutional  Neural  Networks  (CNNs)  are 
highly  effective  for  LULC  classification.  These  models  automatically 

EcologicalInformatics80(2024)10249824M. Ganjirad and H. Bagheri

Despite  collecting  a  complete  training  dataset  with  sufficient  di-
versity (covering all land types) as well as encompassing various pixel- 
level  and  object-level  features  for  training  classifiers,  since  the 
training dataset has been collected for a specific geographic location, the 
model's  performance  can  be  influenced  by  changing  the  study  area. 
However, the framework can be successfully implemented in other study 
areas in case of providing training samples from the new target regions. 
Working  with  satellite  images  presents  significant  challenges, 
including the presence of incomplete information and potential biases. 
For instance, cloud coverage can obstruct valuable data for LULC map-
ping in the target study area. Moreover, complex classes within the LULC 
scheme may pose challenges when relying solely on one remote sensing 
source. In this research, even with the best classification strategies, class

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Based on the given context, there is no explicit mention of any regularization methods used to prevent overfitting in the deep learning pipeline such as dropout or L2 regularization. However, some techniques mentioned in the text could potentially help mitigate overfitting. For example, using validation data for tuning key hyperparameters for each model helps ensure that the models do not become too specialized to the training data. Additionally, the use of ensemble methods like Random Forest (RF) and Gradient Tree Boosting (GTB), which combine differently weighted trees, also helps prevent overfitting. While these methods are not explicitly referred to as regularization techniques, they contribute to reducing overfitting in the models.
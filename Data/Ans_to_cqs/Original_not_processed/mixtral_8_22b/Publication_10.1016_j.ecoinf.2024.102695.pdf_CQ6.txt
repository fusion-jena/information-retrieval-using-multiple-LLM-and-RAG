Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

◦

rate of 0.05 is implemented. Throughout the training process, a learning 
rate of 0.0001 is used, with 300 training epochs and 24 data instances 
processed  per  batch.  To  prevent  data  snooping  bias,  the  dataset  is 
divided chronologically: 70% for the training set, 10% for the validation 
set, and 20% for the test set. This allocation strategy better simulates 
real-world scenarios, ensuring that the data in the validation and test 
sets differ from the training set data, thereby enhancing the generaliz-
ability of the model. All the tests are run on a Windows 11 operating 
system  using  Python  3.8,  with  specific  hardware,  including  a  laptop 
possessing a 12th-Gen Intel Core i7-12700H GPU at 2.30 GHz and an 
NVIDIA GeForce RTX 3060 GPU. Due to the early stopping strategy, the 
training process converges before the last epoch, preventing overfitting 
issues. 

4.1. Model forecasting performance metrics 

4.3. Case 1: DO prediction in the Shandong peninsula

S = ⌊ 1
Ξ(f)

⌋

(25) 

During  training,  the  input  tensor  comprises  three  dimensions:  the 
batch  size,  the  number  of  model  variables,  and  the  future  prediction 
length. The dimension concerning the number of features is segmented 
by dynamically changing the slice sizes and strides, and the segmented 
tensors are folded, adding a slice dimension. Finally, by merging the first 

In the second stage of the attention calculation, the covariates are 
dynamically segmented, capturing their features at different time scales 
through multiscale attention. Specifically, for the static variables S(t), 
the multiscale attention FMSA
S(t)
)
(cid:0)
S(t) = Softmax
FMSA

is calculated as in Formula 30: 

Wf ,S⋅IS(λk)

(30) 

Similarly, for dynamic variables Z(t), the calculation is as in Formula 

31: 

(cid:0)
Z(t) = Softmax
FMSA

)

Wf ,Z⋅IZ(λk)

3.4.3. Attention fusion 

(31)

4.2. Experimental setup 

During the training phase, to accommodate multivariate input and 
univariate output, the input and output dimensions of the encoder and 
decoder  are set  to the  number of variables  in the  dataset. The model 
dimensionality  is  set  to  128  to  achieve  a  balance  between  model 
complexity  and  computational  efficiency.  Additionally,  the  model  in-
tegrates 8 attention heads to enhance its ability to process parallel in-
formation flows. The model architecture includes two layers of encoders 
and one layer of decoders, which help to comprehensively capture fea-
tures from time series data. To reduce the risk of overfitting, a dropout 

4.3.2. Results for the Shandong peninsula

2.2. Transformer 

Since its introduction by Vaswani and others in 2017 Vaswani et al. 
(2017), the Transformer model has led to a significant revolution in the 
field of natural language processing (NLP). One of the key innovations of 
the  Transformer  is  the  self-attention  mechanism,  which  allows  the 
model  to  weigh  the  importance  of  different  parts  of  the  input  data 
differently.  The  core  self-attention  mechanism  is  represented  by  For-
mula 5: 

Attention(Q, K, V) = softmax

(

QKT
√
̅̅̅̅̅
dk

)

V

(5) 

Here,  Q,  K,  and  V  represent  the  query,  key,  and  value  matrices, 
respectively.  The  query  matrix  represents  the  current  word  or  token 
being processed, while the key and value matrices represent the entire 
̅̅̅̅̅
input sequence. The term 
dk
is a scaling factor used to normalize the 
dot products, where dk  is the dimensionality of the keys. 

√

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given text does not provide explicit information about any preprocessing steps taken before training the deep learning model. However, it can be inferred that some form of data division was performed based on the mention of dividing the dataset into training, validation, and testing sets. This division is often done after preprocessing steps such as normalization, scaling, and cleaning have been applied to ensure that the model learns effectively from the data. Unfortunately, without more detailed information, we cannot definitively state what specific preprocessing steps were carried out in this case.
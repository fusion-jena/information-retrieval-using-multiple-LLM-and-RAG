Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

by class sample size. We used the SGD (stochastic gradient descent) optimizer with an initial learning rate of 
0.01 for all models except MansNet, which was initially set at 0.1. The learning rate was reduced by a factor of 
10 after steps of 30 epochs. We used batch normalization and models were trained for 150 epochs using Nvidia 
Tesla K80 or P100 GPUs.

Scientific Reports |         (2021) 11:7580  | 

https://doi.org/10.1038/s41598-021-87210-1

4

Vol:.(1234567890)www.nature.com/scientificreports/Model

#Params 
(million)

Model speed 
(ms)

Top-1 accuracy Top-2 accuracy Top-3 accuracy Top-4 accuracy Top-5 accuracy Macro recall

Wide-ResNet101 124.9

InceptionV3

ResNet101

MnasNet-A1

24.0

42.6

1.0

5.46

3.34

3.33

3.28

0.9171

0.9162

0.9133

0.8579

0.9627

0.9610

0.9633

0.9335

0.9782

0.9767

0.9787

0.9609

0.9850

0.9834

0.9852

0.9730

0.9897

0.9882

0.9892

0.9814

0.8552

0.8519

0.8499

0.7689

Macro 
precision

0.8831

0.8881

0.8740

0.8250

Macro 
precision

0.8831

0.8881

0.8740

0.8250

Table 1.   Comparison of model size, speed, and performance, ordered by top-1 accuracy. Text in bold indicates 
the best value in each category. Wide-ResNet101 has the highest top-1 accuracy and macro precision but 
was substantially slower than the other models. MnasNet-A1 was the fastest model but had relatively poor 
performance. InceptionV3 was relatively fast while maintaining good model performance with the highest 
precision and second highest accuracy and recall.

Table 2.   InceptionV3 confusion matrix showing the number of test images for each prediction.

Maximizing  confidence  in  model  predictions.  Given  the  current  model,  a  number  of  steps  can  be 
taken to increase one’s confidence in the model predictions. For example, users can increase accuracy by input-
ting higher-quality images. Misclassified images are often low-resolution because the bee is small in the original 
image and therefore not represented by a sufficient number of pixels to capture important features. A bee may 
also be in poor focus or partially obscured (Fig. 4 A-C). An automated image quality score, based on the number 
of pixels and focus of the input image, could potentially be provided to users as a screening tool to help them 
assess their confidence in class predictions.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about how the hyperparameters of the model were optimized, such as through methods like grid search or random search. However, it does mention that the Stochastic Gradient Descent (SGD) optimizer was used with an initial learning rate of 0.01 for most models, except for MnasNet-A1, which started with a learning rate of 0.1. Additionally, the learning rate was reduced by a factor of 10 after every 30 epochs. This suggests that some form of manual tuning or trial-and-error approach might have been employed to determine these values. Nevertheless, without more specific details regarding the optimization process, we cannot definitively state whether techniques like grid search or random search were utilized.
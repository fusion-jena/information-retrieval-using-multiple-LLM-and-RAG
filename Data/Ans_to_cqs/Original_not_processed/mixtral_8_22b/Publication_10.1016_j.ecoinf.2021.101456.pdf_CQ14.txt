Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.3. Model parameters & hyperparameters 

The dynamic β-VAE has several tunable model parameters, as seen in 
Eqs.  (3)–(8).  These  model  parameters  were  tuned  on  an  independent 
dataset, collected with the identical instrumentation but at a different 
location. The data had a similar distribution as the data used in this work 
and we obtained: a = 0.2 and b = 0.05, w1 = w2 = 1.2, w3 = 0.9 and 
w4 = 1.1. These parameters were found to be sufficiently robust on the 
dataset used in this work without any fine-tuning.

optima into account. The specific formulation of these control mecha-
nisms in Eqs. (6)–(8) force the model optimization to not deviate from 
the previous optimal solutions. The terms comprising min over (: t (cid:0) 1) 
epochs in Eqs. (6) and (7) provide a form of memory of the previous local 
optima. The trade off between long and short term memory of the losses 
and  the  corresponding  optima  help  the  model  to  steer  towards  more 
global optima. These equations provide a sufficiently general formula-
tion for adjusting β as they are only dependent on the two loss compo-
nents. Further, one can also envision a learnable neural network with 
long short-term memory (LSTM) that can perform this dynamic control 
in a recurrent neural network type formulation of a closed loop control 
system (Hochreiter and Schmidhuber, 1997). 

5.2. Influence of the β and γ  parameters

[
rec (cid:0) w4ℒ(t
ℒ(t)

)
rec

′

]

(8)  

with hyperparameters [a, b,w1,w2,w3,w4] ∈ ℝ+
. The notation (: t (cid:0) 1) is 
′
) is the epoch when β was 
used to indicate all epochs up to (t-1) and (t
last changed. The terms associated with (: t (cid:0) 1) provide a form of long 
term memory of the previous local optima for each of the two loss terms. 
The β dynamics in Eq. (5) can be divided into two regimes aimed at 
optimizing  reconstruction-  and  regularization  terms  corresponding  to 
increase- and decrease of β, respectively. 

3.1.1. Reconstruction regime (β↓) 

The value of β is decreased due to the (cid:0) b term in Eq. (5) when Δrec is 
positive; meaning the reconstruction loss is increasing compared to the 
historical  minimum  reconstruction  loss,  according  to  Eq.  (6).  The  β 
decrease  rule  also  checks  if  the  regularization  loss  is  decreasing 
compared to the historical minimum with the term 1 (cid:0) ψ[Δrec] in Eq. (5).

Fig. 1, and since a higher β term favours a well generalised latent space 
over good reconstructions, a reduction in clustering performance could 
be expected. Additionally, the non-adaptive VAE was more cumbersome 
to train as the model collapsed frequently during training. 

5.4. Selection of number of clusters

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The text does not explicitly state how the hyperparameters of the model were optimized. However, it mentions that the model parameters were tuned on an independent dataset, which suggests some form of parameter tuning or optimization was performed. This could have been done through methods such as grid search or random search, where different combinations of hyperparameter values are tested to find the best performing set. It's important to note that the specific method used for hyperparameter optimization is not mentioned in the given context.
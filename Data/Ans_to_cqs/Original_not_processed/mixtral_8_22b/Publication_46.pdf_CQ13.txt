Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Convolution is used for feature extraction, in which a kernel is applied to an input
tensor. A feature map is thus obtained through the product of kernel elements and tensor
input. The procedure is then repeated on multiple kernels to obtain random feature maps
that represent different feature extractors. The hyperparameters involved in convolution
operations are the size and number of kernels. The size could be anything from 3 × 3 to
5 × 5 to 7 × 7, and the kernel could be chosen randomly.

of weights between the neurons in the adjoining layers and will maximize the network
performance, which would help the neurons focus on speciﬁc patterns in the data. In the
ﬁnal layer, the parameters are passed through the SoftMax function, which transforms
them into probabilities that sum to 1, as shown in Equation (1).

modelling approaches and the increased number of datasets has made this model one of
the most globally accepted SDMs. There is a growing concern for the establishment of the
nonlinear relationship between the bioclimatic parameters through innovative approaches
such as deep learning-based models. It has been observed that the BIOCLIM model is
overestimates the species distribution and the higher probability of species occurrence at
the higher altitude. Moreover, it has been observed that the distribution pattern of the
predicted Rhododendron arboreum distribution using CNN architecture is quite different at
some places than from conventional BIOCLIM models. The current work proposed deep
learning-based CNN architecture for probability distribution modelling and proved to
perform better than the traditional BIOCLIM model. There was an underestimation of
species distribution observed in CNN than BIOCLIM. The distribution probability in CNN

A deep neural network is a multi-layered model that can learn complex nonlinear
relationships between the input parameters. The current study is an attempt that has been
made to use the Convolutional Neural Network (CNN) architecture for SDM. During the
last two decades, there has been a huge increase in deep learning and advanced machine
learning algorithms in a variety of research ﬁelds [60,61]. Deep learning conducts high-level
data abstraction using a hierarchical architecture consisting of multiple interconnected
layers with multiple artiﬁcial neurons. The neurons receive the input values and multiply
them with the speciﬁc weight obtained through optimization. Thereafter, the weighted sum
is transformed through the nonlinear activation function to further pass it to the neurons
of the next layer. The CNN architecture is represented in Figure 4 and the pseudocode is
given in Table S1. Through this procedure, the network will learn through the optimal set

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about all the hyperparameters used in the deep learning model. However, it mentions that the hyperparameters involved in convolution operations are the size and number of kernels. The size could range from 3x3 to 5x5 to 7x7, and the number of kernels could be chosen randomly. Additionally, the text discusses the use of a SoftMax function in the final layer, but it does not mention any specific details regarding other hyperparameters such as learning rate or optimizer. Therefore, based on the given context, we cannot determine the exact hyperparameters used in the deep learning model beyond those related to convolution operations.
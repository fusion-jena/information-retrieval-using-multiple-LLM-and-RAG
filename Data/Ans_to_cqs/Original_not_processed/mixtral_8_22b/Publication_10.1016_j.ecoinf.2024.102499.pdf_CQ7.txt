Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

85 

80 
10,474 
2570 
900 
783 
454 

346 

267 
223 

132 

107 

36 

32 
14 

10 

4214 

822 

162 

229 
190 
112 

37 

80 
4621 
1640 
615 
430 
322 

103 

168 
77 

80 

78 

30 

18 
1 

8 

65 

40 

32 

7 
4 
4 

3 

4 
90 
62 
16 
35 
28 

10 

13 
11 

35 

9 

5 

4 
1 

1  

EcologicalInformatics80(2024)1024992S. Villon et al.                                                                                                                                                                                                                                   

For each fold, the frames from Trainingclips  were used to train the 
models. The Testingclips were used to assess the robustness of our method 
on short sequences centered around the presence of fish presence. The 
Testingstations, common to all k-fold were used to assess the robustness of 
the method on 1-h videos, corresponding to the real use-case scenarios 
of ecological studies (Supp. Fig. 1). 

2. Methods

2. Methods 

2.1. Deep learning model 

To assess the robustness of our method, we performed a 5-fold cross- 
validation.  For  each  fold,  we  randomly  selected  80%  of  our  dataset 
Videoclips  to  train  our  model,  and  20%  to  test  it.  We  could  not  split 
directly images from the frame dataset as images from the same videos 
were very alike which can lead to a model with a low generalization 
capacity.  Using  a  80%  training/20%  testing  random  split  on  videos 
rather  than images  ensured  full independency  between  images of  the 
training set and testing set.

Our  deep-learning  models  used  NASnet  architecture  (Zoph  et  al., 
2017) with a Faster-rcnn backbone (Ren et al., 2015) implemented in 
Tensorflow2. The parameters of the model can be found on TensorFlow 
model zoo1 under the name “faster_rcnn_nas”. All images were resized to 
1333 × 800 pixels to match with the pre-training data (COCO dataset 
(Lin et al., 2014)) used to prepare the first layers of our model and save 
computing time during the training phase). We used a learning rate of 
0.008 with a cosine learning rate decay. For each K-fold, the model was 
trained on its own version of Trainingclips  through 200,000 iterations, 
with a batch size of 16 images per iteration. The training was completed 
in 96 h  per model using a GPU-cluster equipped with 4 RTX8000. In 
order  to  evaluate  the  performance  of  our  deep-learning  models,  we 
computed the recall, precision, and F-measure for each model. Briefly, a

U-net) to discriminate sharks at an individual level on images (Le et al., 
2022). The method was able to predict with 81% accuracy whether in-
dividual sharks present in the test dataset had ever been encountered in 
the training dataset.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors performed a 5-fold cross-validation to assess the robustness of their method. For each fold, they randomly selected 80% of their dataset Videoclips to train their model and 20% to test it. They couldn't directly split images from the frame dataset because images from the same videos were too similar, leading to a model with low generalization capacity. Instead, they used an 80% training/20% testing random split on videos rather than images to ensure full independence between images of the training set and testing set. No mention is made of a separate validation set or any specific criteria used to select the folds.
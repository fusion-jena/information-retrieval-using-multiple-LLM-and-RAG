Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We collected the FishInTurbidWater dataset and quickly and weakly 
labeled  it  to  contribute  to  the  first  weakly-supervised  fish  dataset  in 
turbid  waters.  We,  then  used  this  dataset  to  develop  two  novel  deep 
learning networks, one using semi-supervised contrastive learning for 
significantly  accelerated  model  deployment  time,  and  one  weakly- 
supervised  model  to  shorten  deployment  time,  while  providing  high 
accuracy. 

For  semi-supervised  contrastive  learning,  we  first  trained  a  self- 
supervised  contrastive  learning  model  and  then  fine-tuned  it  on  our

weakly  labeled  dataset.  This  semi-,  weakly-supervised  approach  re-
quires only a small portion of our weakly labeled dataset, which makes 
the  development  cycle  from  data  labeling  to  final  model  inferencing 
very fast (i.e., 4 h) while producing a relatively high accuracy of 89%. 
For  the  ensembling  approach,  we  first  trained  two  state-of-the-art 
DNNs on ImageNet. We then performed transfer learning of these two 
DNNs  on  our  weakly  labeled  dataset.  This  generated  two  weakly- 
supervised  DNNs,  which  we  ensembled using  the  XGBoost  technique. 
This novel ensemble technique significantly improves the overall accu-
racy compared to the two weakly-supervised individual DNNs.

1.  Collecting and presenting a new fish dataset in turbid waters.  
2.  Weakly  labelling  this  dataset  to  provide  the  worldâ€™s  first  weakly 

labeled fish dataset in turbid waters.  

3.  Developing two novel deep learning models to achieve two different 

goals.  
a.  A  semi-supervised  model  for  accelerating  model  development 

speed from data collection to deployment.  

b.  A  weakly-supervised  model  for  accelerating  the  development 
time,  while  also  conserving  the  accuracy,  compared  to  the  fast 
semi-supervised model.

2.3. Weakly-supervised DNN ensemble 

A second approach to counteract the problem of limited labeled data 
availability is to combine weak supervision (Laradji et al., 2021) and 
transfer learning. Transfer learning is a technique that provides an op-
portunity to ensure that we make the best use of available labeled data. 
For instance, in this work, we have used the open-source fully-labeled 
ImageNet dataset to pre-train two well-known DNNs, i.e., EfficientNet 
(Tan and Le, 2019) and ViT (Dosovitskiy et al., 2020), with a great ca-
pacity  for  image  classification,  while  requiring  fewer  computational 
resources compared to other DNNs. ImageNet is a huge visual dataset 
with fourteen million images, designed for object recognition tasks (Lab, 
2023). This free dataset has been hand-annotated with bounding boxes 
to indicate what objects are present and where in the images.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not explicitly mention the specific data repository where the FishInTurbidWater dataset or the deep learning pipelines are available. However, common practices suggest that researchers often share their datasets and code on platforms such as Zenodo, Figshare, Dryad, or GitHub. Given that the authors contributed to the first weakly-supervised fish dataset in turbid waters, they might have made the dataset publicly accessible through one of these platforms. To find out the exact location, I recommend checking the original research paper or contacting the authors directly.
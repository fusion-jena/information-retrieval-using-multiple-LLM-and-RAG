Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.5. Model selection procedure 

The model selection procedure was performed as follows (Capinha 
et  al.,  2021;  Van  Kuppevelt  et  al.,  2020):  we  randomly  generated  5 
models for each of the four available deep-ANN architecture types (20 
models in total) and trained each one with a small subset of the training 
data  (data  partition  At)  for  4  epochs  (an  “epoch”  corresponds  to  the 
complete training dataset being passed forward and backward across the 
network  one  time;  Capinha  et  al.,  2021).  The  accuracy  of  candidate 
models,  as  provided  by  mcfly  (i.e.,  the  “proportion  of  cases  correctly 
classified”), was then compared using a left-out validation data set (data 
partition Av) and the model with the highest performance was selected 
for training on the full training data (data partition Bt; Bt = At + Av) for 
up to 30 epochs.

Table 3 
Parameters tested and values for each classical machine learning model used and each test year. xgbTree = extreme gradient boosting tree, RF = random forest, NNET =
neural network, DNN = deep neural network.  

Model 

xgbTree 

RF 
NNET 

DNN 

Parameter 

2013 

2014 

2015 

2016 

2017 

2018 

2019 

Nrounds 
max_depth 
Eta 
Gamma 
colsample_bytree 
min_child_weight 
Subsample 
Mtry 
Size 
Decay 
layer1 
layer2 
layer3 
hidden_dropout 
visible_dropout 

150 
3 
0.4 
0 
0.8 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.8 
1 
1 
2 
3 
0.1 
1 
0 
0 
0 
0 

150 
3 
0.4 
0 
0.6 
1 
1 
2 
5 
0.1 
1 
0 
0 
0 
0 

150  
3  
0.3  
0  
0.6  
1  
1 
2 
5  
0.1 
1  
0  
0  
0  
0

10.3 
1.6 
0.6 
74.9 
0.8 
0.5 
11.4 

8.4 
1.0 
0.7 
79.4 
0.5 
0.3 
9.6 

10.4 
0.9 
0.8 
75.3 
0.7 
0.5 
11.4  

represented  correspond  to  the  lower  decreases  or  increases  in  the 
number of eggs (i.e., 1 to 25% and > 25 to 50%; Table 1). 

3.2. Deep learning model accuracy, predictive performance, and predictor 
importance 

Concerning the performance of the candidate models, the best vali-
dation accuracies were achieved by model 12 in all the years (accuracy 
= 0.833 ± 0.005; mean of years ± sd) (Fig. 2), a model having a Deep 
Convolutional Long Short-Term Memory (DeepConvLSTM) architecture. 
Model  19,  also  with  a  DeepConvLSTM  architecture  had  high  vali-
dation accuracies too (accuracy = 0.829 ± 0.003; mean of years ± sd), 
followed  by model 14, a Convolutional Neural Network  (CNN) archi-
tecture reaching a validation accuracy of 0.821 ± 0.005 (mean of years 
± sd).

This  kind  of modelling exercise, where  time series  data is  used to 
predict one of two or more classes, falls withing the scope of time series 
classification (Capinha et al., 2021; Keogh and Kasetty, 2003). Several 
deep  learning  architectures  can  be  used  for  time  series  classification, 
differing in the type of layers they have and on how information flows 
between them (Fawaz et al., 2019). Because we have no a priori reason 
to expect that a specific deep learning architecture will perform better 
than  others,  we  evaluated  a  large  number  of  models  having  distinct 
architectures and parametrizations (the so called ‘automated machine 
learning’ or ‘autoML’ approach; Guyon et al., 2019; Van Kuppevelt et al., 
2020).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In this study, several deep learning architectures were evaluated due to the lack of a priori reasons to favor any particular architecture over another. These included DeepConvLSTM and CNN architectures. Among these, the best validation accuracies were achieved by model 12 in all the years, which has a DeepConvLSTM architecture. Additionally, model 19, also with a DeepConvLSTM architecture, showed high validation accuracies. Lastly, model 14, a CNN architecture, reached a validation accuracy of 0.821 ± 0.005 (mean of years ± sd). Therefore, both DeepConvLSTM and CNN architectures are employed in the pipeline.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

For the training process, a batch size of 20 images and an RMSprop optimiser with a learning rate of 0.001 and 
a learning rate decay of 0.0001 was used. The chosen loss function was mean squared error, while the prediction 
accuracy was quantified by the MAE of the respective dataset. The MAE of the validation dataset was computed 
after each epoch. Models were trained until the validation MAE did not further improve compared to the preced-
ing epochs and diverged from the training MAE (’overfit’). The trained model was then applied to the test dataset.
All CNN were implemented using the Keras API version (2.3.0.0)54 and the TensorFlow backend (version 
2.2.0)55 in R (version 3.6.3)23. Model training was undergone on a workstation with two CUDA-compatible 
NVIDIA GPUs (GeForce RTX 2080 Ti, CUDA version 11.0).

extracted a random sample of 10% of the dataset of each trait before model training. This ‘test dataset’ was not 
involved in the training process and exclusively served for the independent evaluation of the trained models. 
The remaining data was split into ’training dataset’ and ’validation dataset’ by a ratio of 4:1 (Table 1). The train-
ing dataset was employed to train the weights of the CNN model, whereas the validation dataset indicated the 
training progress after each full training cycle (’epoch’).

Vol.:(0123456789)www.nature.com/scientificreports/Training  process  and  hyperparameters. 
In  order  to  build  upon  a  pre-existing  knowledge  base,  we 
employed  ’transfer  learning’  by  using  pre-trained  layer  weights  (the  storage  of  the  model’s  knowledge)  from 
a classification task on a dataset on www. image- net. org38 for all CNN models used in this study. The regressor 
following the basic CNN consisted of a global average pooling layer followed by two dense layers with 512 and 1 
output units. The latter forces the CNN to output exactly one prediction (trait) value. In case of the mixed data 
model (setups (3) and (4)), the CNN consisted of parallel branches to incorporate the different input data types. 
The branch processing the bioclimatic data consisted of three dense layers with 64, 32 and 4 output units, and 
the last layer of the CNN regressor contained 4 output units. After concatenating the two branches (image and

instance, contained only one image per species (Table 1). In this case, the Plasticity setup reduced the predictive 
performance compared to the Baseline setup, possibly by increasing the discrepancy to the true trait value. Since 
the traits with more accurate predictions profited most from the Plasticity setup, we assume that it supports the 
model in learning to predict the trait expressions themselves rather than extracting them indirectly through 
taxa-specific morphological features. Given that we restricted the number of images per species to a maximum 
of 8, while successful deep learning-based plant species identification usually requires thousands of  images12,22, 
it seems very unlikely that the models inferred traits from species-specific plant features visible in the imagery. 
The latter was underpinned by our finding that the predictions of most traits are void of phylogenetic autocor-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The performance of the deep learning model is evaluated using Mean Absolute Error (MAE), which measures the average magnitude of errors between predicted and actual values without considering their direction. It is calculated by taking the absolute difference between the predicted and actual values, summing those differences, and dividing by the total number of observations. A lower MAE indicates better model performance. Additionally, the validation dataset's MAE is computed after each epoch during the training process to monitor overfitting. Overfitting occurs when the validation MAE does not further improve compared to previous epochs and diverges from the training MAE.
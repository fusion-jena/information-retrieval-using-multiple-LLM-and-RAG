Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Hyperparameters  play  a  crucial  role  in  ensuring  that  the  model 
learns  effectively  and  makes  accurate  predictions  (Mantovani  et  al., 
2015). HPO refers to selecting the most suitable hyperparameter values 
for optimizing model performance (Li and Talwalkar, 2020). Some of the 
HPO  techniques  are  manual  search  (Chang,  2018),  grid  search  (Frie-
drichs and Igel, 2005), random search (Bergstra and Bengio, 2012), and 
Bayesian search (Eggensperger et al., 2013). These  techniques aim to 
optimize the performance of the models. However, each method carries 
certain  benefits  and  has  some  limitations  (Yang  and  Shami,  2020). 
Manual  search  requires  thorough  prior  knowledge  and  experience.  A 
major  limitation  of  Random  search  is  the  random  evaluation  of  each 
iteration, resulting in wasted time evaluating suboptimal areas within 
the  search  space.  Grid  search  is  expensive  and  impractical  for  large

64, 128, 256, 512 

64, 128, 256, 512 

EcologicalInformatics80(2024)1025107S.V.S. Kumar and H.K. Kondaveeti                                                                                                                                                                                                         

Table 8 
Performance comparison of the selected models evaluated using a 60:40 split of training and testing data.   

Validation accuracy 

Precision 

Recall 

F1-score 

Model name 

Feature extractor 

Fine tuner 

Feature extractor 

Fine tuner 

Feature extractor 

Fine tuner 

Feature extractor 

Fine tuner 

MobileNetV2 
EfficientNetB0 
GoogleNet 
DenseNet201 
InceptionV3 
ResNet18 
InceptionResNetV2 
NASNetMobile 

91.21% 
94.47% 
88.47% 
92.41% 
92.99% 
87.81% 
93.04% 
92.83% 

93.16% 
96.56% 
90.16% 
94.21% 
94.56% 
89.33% 
95.44% 
94.68% 

92.54% 
94.99% 
89.98% 
93.22% 
93.86% 
89.38% 
93.73% 
93.41% 

94.11% 
95.31% 
90.97% 
95.02% 
95.12% 
91.25% 
95.13% 
95.44%

Output  
• Best hyperparameter configuration, Sbest  
1.  Initialize an empty set Smanual  to store hyperparameter configurations.  
2.  For each hyperparameter configuration h in M:  
(a)  Train a model with hyperparameters h on dataset D.  
(b)  Evaluate the model's performance on dataset D to obtain a performance metric 

Φ(M, D).  

(c)  Add h to Smanual  if it yields the highest performance so far.  
3.  Initialize an empty set Srandom  to store hyperparameter configurations.  
4.  For each hyperparameter configuration h in Smanual:  
(a)  Sample a random subset of hyperparameters from h to create a new configuration 

hrandom.  

(b)  Train a model with hyperparameters hrandom  on dataset D.  
(c)  Evaluate the model's performance on dataset D to obtain a performance metric 

Φ(Smanual, D).  

(d)  Add hrandom  to Srandom  if it yields the highest performance so far.  
5.  Initialize Sbest  as the best hyperparameter configuration from Srandom  based on

search  tests  various combinations of  the hyperparameters  identified in  manual  search and  trains the selected 
models to achieve the maximum possible accuracy through multiple iterations. Experimental analysis revealed 
that the Fine-tuned EfficientNetB0 model exhibited superior performance, achieving an accuracy of 99.12%. In 
contrast, the performance of the ResNet18 model was disappointing with an accuracy of 93.24%, while other 
models outperformed it.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The strategy implemented to monitor the model performance during training involves initializing an empty set called Smanual to store hyperparameter configurations. Then, for each hyperparameter configuration in M, a model is trained with those hyperparameters on dataset D. The model's performance is then evaluated on dataset D to obtain a performance metric. If this configuration yields the highest performance so far, it is added to Smanual. Next, another empty set called Srandom is initialized to store hyperparameter configurations. Each hyperparameter configuration in Smanual is used to sample a random subset of hyperparameters and create a new configuration called hrandom. A model is then trained with these new hyperparameters on dataset D, and its performance is evaluated again. If hrandom yields the highest performance so far, it is added to Srandom. Finally, Sbest is initialized as the best hyperparameter configuration from Srandom based on search tests that vary combinations of the hyperparameters identified in manual search and train the selected models to maximize accuracy through multiple iterations.
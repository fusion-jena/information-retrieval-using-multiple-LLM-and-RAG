Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Annosys—implementation of a generic annotation system for
schema-based data using the example of biodiversity collection data.
Database. 2017;2017(1):bax018.

Vanhoucke V, Rabinovich A. Going deeper with convolutions. In: 2015
IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Boston: IEEE Conference; 2015. p. 1–9. doi:10.1109/CVPR.2015.7298594.
Ioffe S, Szegedy C. Batch normalization: Accelerating deep network
training by reducing internal covariate shift. CoRR. 2015. abs/1502.03167.
[Online]. Available http://arxiv.org/abs/1502.03167.

35.

37.

36. He K, Zhang X, Ren S, Sun J. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. CoRR. 2015.
abs/1502.01852. [Online]. Available http://arxiv.org/abs/1502.01852.
Jia Y, Shelhamer E, Donahue J, Karayev S, Long J, Girshick R,
Guadarrama S, Darrell T. Caffe: Convolutional architecture for fast feature
embedding. In: Proceedings of the 22Nd ACM International Conference
on Multimedia. New York: ACM; 2014. p. 675–8.
doi:10.1145/2647868.2654889.

38. Mata-Montero E, Carranza-Rojas J. Automated plant species

very beginning the weights of ImageNet were used.
For example, H1KI.PC.PC means the transfer
learning was progressive, done from ImageNet, to
Herbarium1K, to PlantCLEF, and tested with
PlantCLEF data.

• Initialization: weights used to initialize the model.
• Training: training set used (e.g., Herbarium255

training set, PlantCLEF training set, etc.)

• Testing: test set used (e.g., Herbarium255 test set,

PlantCLEF test set, etc.)

• Top-1/Top-5: accuracy achieved with top-1 and

top-5 best predictions, respectively.

no collector was shared by the training and testing sets
to avoid bias in the data. The following four experiments
were conducted:

• R.H255.H255: The neural network was initialized

randomly, trained on the Herbarium255 training set
(70%), and tested on the Herbarium255 test set (30%).

Experiments and results
All experiments measured the top-1 and top-5 accu-
racy of the trained deep learning model under different
circumstances, i.e., herbarium specimens classification
(“Herbarium specimen classification” section, Table 3),
transfer learning across herbarium data from differ-
ent regions (“Cross-Herbaria transfer learning” section,
Table 4), and transfer learning from herbarium data to
non-dried plant images (“Transfer learning from herbar-
ium to non-dried plant images” section, Table 5).

For each of these experiments, table columns are

defined as follows:

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information about the specific data annotation techniques used in the deep learning pipeline. However, it mentions Annosys, which is a generic annotation system for schema-based data using the example of biodiversity collection data. This suggests that some form of data annotation might have been employed in the process, but the exact technique remains unspecified.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 2. Full comparison of training time, prediction time, optimization time (i.e., time to properly
tune the model), and hardware for each model.

Model

Training Time
(Hours)

Prediction Time (Hours)

Optimization
Time (Hours)

Hardware

CNN

XGB

4

4

2.4. Validation

4

Unknown

Desktop with 64 Gb of
RAM and one Titan X
(Maxwell) GPU.

72—can be reduced by
distributing prediction to more
cores or more machines.

2

Desktop with 64 GB of
ram and 64 logical cores.

20.68

32.20

92.98

75.56

76.86

24.73

76.81

Table A3. Confusion matrix comparing the CNN product to the CCMEO validation data. UA is the
user accuracy and PA is the producer accuracy.

Open water

Open water

Fen

Bog

Marsh

Swamp

Upland

PA

3184

130

1

262

59

585

75.43

Fen

100

13,921

626

278

1381

4892

65.67

Bog

3

2828

2153

2

1025

806

31.58

Marsh

Swamp

Upland

184

315

2

3070

230

1470

58.24

6

1002

127

17

1959

4128

27.06

391

10,618

734

465

5771

132,148

88.02

UA

82.32

48.31

59.10

74.99

18.79

91.75

80.28

Remote Sens. 2020, 12, 2

15 of 20

Table A4. Confusion matrix comparing the XGB product to the CCMEO validation data. UA is the
user accuracy and PA is the producer accuracy.

Open water

Open water

2894

49

14

654

49

561

Fen

24

7331

4280

1876

4176

3511

Bog

0

835

4408

159

1023

392

Marsh

Swamp

Upland

67

193

38

3612

197

1164

68.53

2

3663

654

172

1428

1320

19.73

99

6680

3160

2067

17,334

Table 1. List of input variables in the XGBoost model (XGB) and convolutional neural network
(CNN) models. Each variable lists its respective data source, equation, description, and, if needed,
citation. For information on Sentinel-2 band information not listed in the table, see:
[66] and
https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/resolutions/spatial.

Variable

Data Source

Model

ARI

Sentinel-2

XGB/CNN

Band 2
Band 3
Band 4
DEM

NDVI

dNDVI

NDWI

POLr

PSRI

Sentinel-2
Sentinel-2
Sentinel-2
ALOS

Sentinel-2

Sentinel-2

Sentinel-2

Sentinel-1

Sentinel-2

CNN
CNN
CNN
CNN

CNN

XGB

CNN

XGB

CNN

REIP

Sentinel-2

XGB/CNN

Equation
(cid:17)
(cid:16) B8
(cid:16) B8
B3
B2

–

(cid:17)

-
-
-
-
(B8− B4)
(B8 + B4)

Description

Table 3. Model validation statistics (overall accuracy, Kappa statistic, and mean F1-score) of the CNN
and XGB models for the two independent validation data sets.

Validation Data

Model

Overall Accuracy (%)

Kappa Statistic

Mean F1-Score

ABMI plots
ABMI plots

CCMEO
CCMEO

CNN
XGB

CNN
XGB

81.3
75.6

80.3
72.1

0.57
0.49

0.52
0.41

0.56
0.52

0.59
0.52

The per-class F1-scores for the ABMI and CCMEO data are seen in Figure 3 (blue showing the
F1-score for the CNN model and orange the F1-score for the XGB model). The open water class shows

Remote Sens. 2020, 12, 2

10 of 20

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To evaluate the performance of the deep learning models, several metrics were used. These include overall accuracy, Kappa statistic, and mean F1-score. Additionally, confusion matrices were created to compare the products generated by the Convolutional Neural Network (CNN) and eXtreme Gradient Boosting (XGB) models against the validation datasets from Alberta Biodiversity Monitoring Institute (ABMI) and Canadian Centre for Mapping and Earth Observation (CCMEO). Per-class F1-scores were also calculated for both models using these validation datasets. Furthermore, user accuracy (UA) and producer accuracy (PA) were computed for each wetland type based on the confusion matrices.

In summary, the evaluation metrics used for assessing the performance of the deep learning models are overall accuracy, Kappa statistic, mean F1-score, confusion matrices, per-class F1-scores, user accuracy (UA), and producer accuracy (PA).
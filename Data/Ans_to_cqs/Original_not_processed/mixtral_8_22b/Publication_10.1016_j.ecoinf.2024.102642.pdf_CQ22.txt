Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

State-of-the-art metrics [Hildebrand et al., 2022] have been used to: 
1. Compare a pair of annotators, with each one taking turns as the tester 
and  the  pseudo  ground  truth.  2.  Evaluate  the  performance  of  the 
detection models with the annotations from an expert as pseudo ground 
truth. 

For  every  sample  subjected  to  analysis  by  either  the  CNN  or  an 

annotator, four possible outcomes are defined:  

• True Positive (TP): A call is accurately detected and annotated.  
• False Positive (FP): A call is detected, but there is no annotation.  
• True Negative (TN): No call is detected and no call is annotated 

EcologicalInformatics81(2024)1026424G. Dubus et al.

Red  dotted  lines  represent  the  performances  of  the  model  trained 
with the expert annotation set. A Krustkal-Wallis test is performed, using 
the performances of the largest subgroup size and the performance of the 
expert  on  Fig.  8,  to  evaluate  the  null  hypothesis  formulated  as  “the 
performance of the model trained with an expert annotation does not 
significantly differ from performance obtained with models trained on 
non-experts  annotations”.  P-values  computed  for  each  metrics  (AUC, 
mAP) are (0.11, 0.75) and (0.11, 0.21) for SEIO PW and Dcall respec-
tively.  The  null  hypothesis  cannot  be  rejected.  However,  the  perfor-
mances  of  the  model  trained  with  the  expert  annotation  are  always 
higher than the mean performance obtained with models trained with 
non-expert annotations.

During  the  campaign,  methods  are  proposed  to  assess  inter- 
annotator  variability  and  aggregate  annotation  sets.  Those  methods 
will help to define the minimal number of annotators needed [Dubus 
et al., 2023; Walter et al., 2022]. Increasing the number of annotators 
per sample could be helpful if the variability is too high. Finally, the two 
grouping  methods:  majority  voting  (Section  2.2.3)  and  soft  labeling 
(Section 2.3.2) can also be used to produce relevant annotation sets for 
deep learning. 

6. Conclusion 

In this study, new approaches were explored to improve the accuracy 
and  efficiency  of  cetacean  vocalization  detection  using  deep  learning 
models. Leveraging multi-annotation campaigns involving both expert 
and novice annotators aimed to address the challenges associated with 
the scarcity of ground truth data in underwater PAM studies.

Performances of each model are then observed by considering the 
agreement between novice annotations and expert annotations. Fig. 7 
presents  the  results  of  the  models  trained  on  novice  annotation,  by 
showing  the  F1  score  of  the  annotation  set  computed  from  the  mea-
surements of recall and precision in the previous subsection, using the 
expert annotations as pseudo ground truth.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The state-of-the-art metrics used to evaluate the performance of the deep learning model include Area Under Curve (AUC), Mean Average Precision (mAP), F1 Score, Recall, and Precision. These metrics are used to compare pairs of annotators, evaluate the performance of detection models, and observe the agreement between novice and expert annotations. Specifically, the F1 score is calculated based on measurements of recall and precision, while the AUC and mAP are used to determine whether the performance of the model trained with expert annotations significantly differs from those trained with non-expert annotations.
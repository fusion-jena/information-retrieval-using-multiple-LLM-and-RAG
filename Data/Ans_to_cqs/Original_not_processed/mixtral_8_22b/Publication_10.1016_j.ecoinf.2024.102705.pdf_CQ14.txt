Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The term “deep learning” refers to the utilization of neural networks, 
a statistical model for data representation in solving complex problems 
(LeCun  et  al.,  2015;  Christopher  and  Hugh,  2024).  These  neural  net-
works are constructed through training, involving a large set of inputs 
and  labeled outputs. Comprising layered nonlinear transformations, a 
neural  network  contains  numerous  adjustable  parameters.  Training  a 
neural network requires multiple iterations, during which the network 
may produce incorrect results. The discrepancy between the current and 
expected outputs is calculated as loss values. Optimization algorithms 
such as Stochastic gradient descent (SGD) (Herbert and Sutton, 1951), 
Adaptive Gradient (AdaGrad) (Duchi et al., 2011), Root Mean Square 
Prop (RMSProp) (Tieleman and Hinton, 2012), and Adam (Kingma and 
Ba, 2014) are then employed to assess each parameter’s contribution to

In  the  image  signal  processing  pipeline,  image  enhancement  typi-
cally involves the utilization of several fine-tunable mappings, with their 
hyperparameters  adjusted  by  engineers  based  on  their  experience 
(Mosleh  et  al.,  2020).  However,  this  process  of  parameter  tuning  for 
different  scenarios  is  time-consuming.  To  overcome  this  limitation,  a 
recommendation  is  made  to  employ  a  compact  non-local  network 
instead  of  relying  solely  on  experienced  engineers  to  estimate  hyper-
parameters. This approach offers a faster and more effective solution. 
When  predicting  parameters  for  DIP,  the  NLPP  primarily  focuses  on 
capturing  the  large-scale  environmental  conditions  depicted  in  the 
image  rather  than  emphasizing  high-definition  details.  Therefore, 
downsampling the input images into the size of 256 × 256 using bilinear 
interpolation  is  sufficient  for  completing  the  parameter  estimation

Duchi, J., Hazan, E., Singer, Y., 2011. Adaptive subgradient methods for online learning 
and stochastic optimization. J. Mach. Learn. Res. 12, 2121–2159. https://doi.org/ 
10.5555/1953048.2021068. 

Enlin, L., Liwei, W., Qiuju, X., Rui, G., Zhongbin, S., Yonggang, L., 2023. A novel deep 
learning method for maize disease identification based on small sample-size and 
complex background datasets. Eco. Inform. 41, 102011 https://doi.org/10.1016/j. 
ecoinf.2023.102011. 

Fagner, C., Eulanda, M.S., Juan, G.C., 2023. Bag of tricks for long-tail visual recognition 
of animal species in camera-trap images. Eco. Inform. 76, 102060 https://doi.org/ 
10.1016/j.ecoinf.2023.102060. 

Frank, S., Volker, S., 2021. Identification of animals and recognition of their actions in 
wildlife videos using deep learning techniques. Eco. Inform. 61, 101215 https://doi. 
org/10.1016/j.ecoinf.2021.101215. 

Girshick, R., 2015. Fast r-cnn. In: Proceedings of the IEEE International Conference on

feature representations, which are crucial for predicting the threshold of 
image  processing  algorithms  (Wang  et  al.,  2018).  In  this  study,  we 
propose a compact network incorporating a non-local layer capable of 
extracting  long-distance  features  to  predict  parameters  for  the  image 
processing module. Additionally, we introduce a joint training approach 
for image adaptive enhancement modules and deep learning models to 
ensure  that  the  image  processing  results  contribute  to  improved  per-
formance  of  the  deep  learning  model.  The  proposed  method  demon-
strates superior performance across a diverse range of environments. it 
enables  expedited  transitions  from  field  surveys  to  reporting,  even  in 
challenging circumstances, thereby facilitating efficient data collection 
and analysis on a larger scale.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The text does not explicitly mention how the hyperparameters of the model are optimized. However, it can be inferred that optimization algorithms such as Stochastic gradient descent (SGD), Adaptive Gradient (AdaGrad), Root Mean Square Prop (RMSProp), and Adam are used to assess each parameter’s contribution to the output. These optimization algorithms help in finding the optimal weights or parameters for the model. It should be noted that these optimization algorithms are different from hyperparameter optimization techniques like Grid Search and Random Search. Hyperparameter optimization techniques are used to find the best combination of hyperparameters for the model, while optimization algorithms are used to minimize the cost function during the training phase. Since the text does not provide explicit information about the use of hyperparameter optimization techniques, it cannot be confirmed whether Grid Search or Random Search was used for hyperparameter optimization.
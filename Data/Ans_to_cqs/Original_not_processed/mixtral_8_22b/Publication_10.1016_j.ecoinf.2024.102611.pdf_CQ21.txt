Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.4. Level-wise training, validation, and testing strategies with Random 
Forest classifier 

The  data  collection  is  split  into  eleven  groups  based  on  visual 
morphological  characteristics,  as  discussed  in  Section  2.  Each  group 
consists of several plant species varieties in the range of 7–14 plants per 
group covering  100  medicinal  plant  species with  training,  validation, 
and testing proportion as 70:20:10 in both the levels, apart from GSL100 
datasets,  a  completely  new  datasets  RTL80  and  RTP40  comprising 
random collections captured in real-time is used to assess the efficiency 
of the proposed hierarchical classification model. The main evaluation 
objective using random groups is to analyze the sensitivity of classifier 
samples collected in various real-time scenarios.

Table  3  gives  insights  into  the  distribution  of  trained,  tested,  and 
validated samples across  the groups. The data suggest that consistent 
ratios across groups are maintained in the sampling process to avoid any 
potential bias during classification. Different samples are present across 
groups to establish the reliability and generalizability of the model. For 
evaluation,  both  validation  and  test  sets  are  considered  separately  to 
fine-tune the model’s hyperparameters, leading to generalizability and 
scalability towards unseen datasets. 

4.5. Classifier specifications 

The Random Forest (RF) classifier is used in both levels of the hier-
archical classification model for performing group and plant-level pre-
dictions. Let several estimators N and Fi(x) be the function of predicting 
̂
the ith  decision tree for the input X, then the prediction 
C by RF is given 
by (9). 

̂C =

1
N

∑N

i=1

Fi(X)

(9)

in the datasets. Furthermore, the classifier ability to capture the non- 
linear  relationships  among  the  feature  representation  facilitates  the 
understanding of hierarchical relationships concerning a specific class in 
datasets.  It  combines  various  decision  trees  to  produce  predictions  at 
both levels. Bootstrap sampling creates multiple subsets (with random 
selection and replacement) of training data for individual decision trees 
in  a Random Forest. The training, validation, and  testing proportions 
considered in both levels as presented in Table 3.

0.87 
0.86 
0.84 
0.86 
0.8  

leaves. However, the predicted outcomes can prove the generalizability 
of the proposed model towards new real-time datasets without a con-
strained image acquisition setup. In the future, increasing the number of 
samples  and  including  a  pre-processing  method  to  remove  complex 
backgrounds  will  further  improve  the  performance  of  the  proposed 
model. 

5.8. Practical deployment

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
After the model training, there are several postprocessing steps that can be performed depending on the requirements of the project. Some common postprocessing steps include calculating metrics such as accuracy, precision, recall, F1 score, etc., to evaluate the performance of the model. Another important step is creating confusion matrices which provide a more detailed analysis of the model's performance by showing the true positives, false positives, true negatives, and false negatives. Additionally, saliency maps can also be generated to understand how the model arrived at its decisions by highlighting the most relevant features or pixels in the input images. Other possible postprocessing steps could involve fine-tuning the model's hyperparameters, applying regularization techniques, or even retraining the model with different architectures or parameters. However, it should be noted that not all these steps may be necessary or applicable for every project, and the choice of postprocessing steps would depend on the specific goals and constraints of the project.

Unhelpful Answer:
I do not have enough information to determine what postprocessing steps were taken after the model training.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

bagging_freq 
min_child_samples 

int, 1 to 20 
int, 0 to 5  
int, 10, 20, … 
100  
int, 10, 20, … 
100  
ReLu or 
linear  
Adam or 
RMSprop 

Maximum depth of the tree 
Number of middle layers 
Number of nodes in the 
input layer 
Number of nodes in the 
middle layers 
Activation function in the 
input and middle layers 
Optimizer 

Maximum tree leaves for 
base learners   
L1 regularization   
L2 regularization   
A subset of features on each 
iteration (tree)   
Randomly select a part of 
data without resampling   
Frequency for bagging   
Minimal number of data in 
one leaf   

AA SHAPj =

1
ni

∑
⃒
⃒SHAPi,j

⃒
⃒

i

(1)  

where AA_SHAPj  is the average of the absolute SHAP values for input 
variable j (i.e., the daily average temperature on day j), ni is the number 
of response values (4844 for the first flowering date models and 4814 for 
the full blossom date models), and SHAPi,j is the SHAP value for the jth 
input parameter of the ith response value.

et al., 2016), and LightGBM 3.2.1 (Ke et al., 2017) libraries. In building a 
machine  learning  model,  the  structure  and  hyperparameters  of  the 
model must be determined, such as the number of decision trees and the 
depth of trees for RF models and the number of intermediate layers and 
nodes for ANN models. We optimized the structure and hyperparameters 
of the RF and ANN models using Optuna 2.10.0 (Akiba et al., 2019) with 
the  parameters  and  ranges  shown  in  Table  1  as  candidates.  For  the 
LightGBM models, we used Optuna's LightGBM Tuner (optuna.integra-
tion.lightgbm)  for hyperparameter  fitting.  The  performance of  the  al-
gorithms was evaluated by comparing the loss function (mean squared 
error (MSE)) of the optimized models. In addition, the mean absolute 
error (MAE) and the coefficient of determination (R2) were calculated 
for reference.

n_estimators = 92, 
max_depth = 15 
num_layer = 0, 
units (initial layer) = 90, 
units (middle layers) = 60, 
activation = ReLu, 
optimizer = Adam 
num_leaves = 31, 
lambda_l1 = 0 
lambda_l2 = 0 
feature_fraction = 0.5 
bagging_fraction = 0.573 
bagging_freq = 7 
min_child_samples = 20 

0.0530 

0.175 

0.942 

0.0429 

0.156 

0.958 

0.0381 

0.148 

0.964  

0.0586 

0.175 

0.940 

0.0485 

0.170 

0.951 

0.0424 

0.158 

0.955  

EcologicalInformatics71(2022)1018353Y. Masago and M. Lian                                                                                                                                                                                                                       

0.965,  respectively),  followed  by  the  ANN  and  RF  models,  while  the 
differences  were  not  significant.  The  optimized  ANN  models  did  not 
have any intermediate layers, and the model with the input layer fol-
lowed by the output layer was determined to be the best.

We optimized the three machine learning algorithms (RF, ANN, and 
GBDT) using Optuna. Table 2 shows the optimal model parameters and 
their performance (MSE, MAE, and R2). For both the first flowering date 
and the full blossom date, the GBDT models showed the best accuracy 
(MSE = 0.0381 and 0.0424, MAE = 0.148 and 0.158, R2  = 0.964 and 

Table 2 
The best-fit parameters and the performance of the optimized models.  

Algorithm 

Best parameters 

MSE 

MAE 

R2 

First flowering date 
Random forest 

ANN 

GBDT 

Full blossom date 
Random forest 

ANN 

GBDT 

n_estimators = 71, 
max_depth = 16 
n_layers = 0, 
units (initial layer) = 90, 
units (middle layers) = 70, 
activation = ReLu, 
optimizer = Adam 
num_leaves = 31, 
lambda_l1 = 0.000192 
lambda_l2 = 0.0215 
feature_fraction = 1.0 
bagging_fraction = 0.506 
bagging_freq = 3 
min_child_samples = 20

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps can be applied to improve its performance. These may include:

1. Data Cleaning: This involves removing missing values, outliers, and irrelevant data from the dataset. It also includes handling inconsistent data entries.

2. Normalization/Standardization: This step scales numerical input variables so they have zero mean and unit variance. This is important because gradient descent converges much faster with normalized inputs. Common methods include Min-Max Scaling, Z-Score Normalization, and Batch Normalization.

3. Encoding Categorical Variables: Deep learning models cannot handle categorical variables directly. Therefore, these need to be converted into numerical form. Techniques like one-hot encoding, label encoding, and binary encoding can be used.

4. Feature Selection: Not all features contribute equally to predicting the target variable. Some might even degrade the model’s performance. Hence, it is essential to choose only those features which significantly impact the outcome.

5. Splitting Dataset: Before training the model, the dataset should be split into training set, validation set, and test set. Training set is used to train the model, validation set is used to tune hyperparameters, and test set is used to evaluate the final model.

6. Data Augmentation: To increase the amount of training data, various transformations can be applied to existing images (like rotation, zooming, flipping etc.) to create new ones.

These preprocessing steps help in improving the efficiency and effectiveness of the deep learning model. However, the specific steps required depend largely on the nature of the problem being solved and the type of data available.
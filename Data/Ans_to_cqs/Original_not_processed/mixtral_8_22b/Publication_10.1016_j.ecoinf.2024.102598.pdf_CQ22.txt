Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.2. Model performance MLMs comparisons 

The performance of each MLM was assessed using several metrics, 
such as AUC, TSS, Kappa, and phi. The outcomes of these evaluations are 
presented  in  Table  3.  Additionally,  Fig.  4  visually  illustrates  the  pre-
dictive performance of each model with AUC values for both training 
and test datasets. Upon analysing the four performance metrics, it was 
evident that the selected models exhibited high consistency, except for 
the  CART.  The  performance  of  the  remaining  models  displayed  only 
slight variations within a small range, as shown in Table 3. Additionally, 
there was no discernible variation in the classifiers’ prediction accuracy 
according to the Kruskal-Wallis statistic (p > 0.05).

Validation of MLMs is a critical step in assessing each model’s pre-
dictive capabilities, as Mohammadi et al. (2014) emphasized. To eval-
uate  the  accuracy  and  reliability  of  each  MLM,  we  employed  robust 
statistical  measures  provided  by  the  ‘sdm  1.1–8’  package.  These  are 
kappa (Allouche et al., 2006), true skill statistics (TSS; Allouche et al., 
2006), and area under the receiver operator characteristics (ROC) curve 
(AUC; Hanley and McNeil, 1982). Model complexity was evaluated for 
each case by computing the difference between the AUC (train) and AUC 

(CV), represented as Δ AUC (Jarnevich et al., 2018). The Δ AUC scores 
from each model offer insights into their sensitivity to the training data 
(Jarnevich  et  al.,  2018).  Moreover,  the  Kruskal-Wallis  test  was  con-
ducted to determine whether a significant difference in prediction ac-
curacy exists among the classifiers. 

2.5. Ensemble model

Kappa 

0.513 
0.514 
0.585 
0.562 
0.568 
0.556 
0.494 
0.493 
0.541 
0.544 

phi 

0.519 
0.521 
0.591 
0.579 
0.581 
0.574 
0.502 
0.498 
0.557 
0.559 

Sensitivity 

Specificity 

0.834 
0.842 
0.861 
0.898 
0.890 
0.904 
0.835 
0.820 
0.890 
0.888 

0.679 
0.671 
0.723 
0.662 
0.676 
0.650 
0.658 
0.671 
0.650 
0.654  

stand-alone  MLMs.  This  indicates  that  the  ensemble  model  correctly 
identifies positive forest fire cases and accurately predicts negative cases 
compared to the individual MLMs. 

3.3. Predictor variable importance and response curves

Among the five MLMs evaluated, the AUC analysis revealed that RF 
and SVM performed remarkably well with training AUC values of 0.868 
± 0.002 and 0.857 ± 0.001, respectively. MARS followed this, and BRT 
gained  training  AUC  scores  of  0.829  ± 0.001  and  0.809  ± 0.002, 
respectively,  while  CART  exhibited  poor  performance  with  a  low 
training AUC of 0.781 ± 0.001. Similar trends were observed for TSS, 
where  RF,  SVM,  MARS,  BRT  outperform  CART  (Table  3).  All  models 
exhibited  robust  agreement  with  the  predicted  results  in  terms  of  ac-
curacy.  Additionally,  when  considering  the  kappa  and  phi  for  recog-
nizing  accurate  forest  fire  pixels,  all  models,  RF,  SVM,  MARS,  BRT 
outperformed CART (Table 3). The Δ  AUC was lowest for CART with 
0.002, followed by MARS and BRT with both values of 0.003. The Δ AUC 
was  highest  for  RF  and  SVM  with  the  values  of  0.022  and  0.021,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: The performance of each Machine Learning Model (MLM) was evaluated using several metrics such as Area Under Curve (AUC), True Skill Statistics (TSS), Kappa, and Phi. The outcomes of these evaluations are presented in Table 3. Additionally, Figure 4 visually illustrates the predictive performance of each model with AUC values for both training and test datasets. The performance of the remaining models displayed only slight variations within a small range, as shown in Table 3. There was no discernible variation in the classifiers’ prediction accuracy according to the Kruskal-Wallis statistic (p > 0.05). Model complexity was evaluated for each case by computing the difference between the AUC (train) and AUC (CV), represented as Δ AUC. The Δ AUC scores from each model offer insights into their sensitivity to the training data. Moreover, the Kruskal-Wallis test was conducted to determine whether a significant difference in prediction accuracy exists among the classifiers.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 3. CNN Architecture.

No.

Layers

Output Shape

Parameters

Dropout Rate

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

Input
Convolutional
Activation (ReLu)
MaxPooling
Convolutional
Activation (ReLu)
MaxPooling
Convolutional
Activation (ReLu)
MaxPooling
Dropout
Flatten
Fully Connected
Activation (ReLu)
Fully Connected
Activation (softmax)

128 × 128 × 3
128 × 128 × 32
—
64 × 64 × 32
64 × 64 × 32
—
32 × 32 × 32
32 × 32 × 64
—
16 × 16 × 64
16 × 16 × 64
16,384
64
—
4
—

—
896
—
—
9248
—
—
18,496
—
—
—
—
1,048,640
—
—
260

—
—
—
—
—
—
—
—
—
—
0.4
—
—
—
—
—

4.2.1. Data Augmentation

Remote Sens. 2021, 13, 2221

18 of 26

Figure 6. Sample image data augmentation. (a) Rotation results; (b) Zoom results; (c) Shearing results; (d) Horizontal shift
results; (e) Vertical shift results; (f) brightness results; (g) Horizontal ﬂip results; (h) Vertical ﬂip results.

Remote Sens. 2021, 13, 2221

19 of 26

Figure 7. Model accuracy over plain data. (a) Training accuracy; (b) Validation accuracy.

Figure 8. Model accuracy over encrypted data. (a) Training accuracy; (b) Validation accuracy.

The fundamental concept behind the ResNet model is to use shortcut links to bypass
blocks of convolutional layers (bottleneck). The CONV layers each have a 3 × 3 ﬁlter
and are designed according to two rules: (1) the layers have the same number of ﬁlters
with the same output feature map size and (2) the number of ﬁlters is multiplied if the
feature map size is halved. The convolutional layers conduct the downsampling with
a stride of 2. The network ends with an average POOL layer and 1000 FC layers with
a SoftMax activation function. The default input size is 224 × 224 × 3 [24].
Xception: The Xception model’s architecture was proposed by Chollet (2017). This
model is a CNN-based architecture also trained on the ImageNet dataset. The Xception
architecture comprises 36 CONV layers with a 3 × 3 ﬁlter and stride of 2. These CONV
layers are structured into 14 modules, all of which have the ReLu activation function

4.2.1. Data Augmentation

Data augmentation is a technique for increasing the amount of data available for train-
ing the proposed CNN model without actually acquiring new data [49]. This technique is
used to expand the dataset into a larger one more appropriate for DL model training. There
are various strategies used for data augmentation, including rotation, zoom, horizontal,
and vertical shift. These techniques assist in enhancing the efﬁciency of CNNs [50].

Different data augmentation strategies have been utilized in this research. For instance,
we applied a 90-degree rotation range, a zoom and shear range of 20%, a brightness scale
between 0.2 to 1.0 and a shift range of 20% in height and width. Finally, a horizontal ﬂip
and vertical ﬂip have also been applied. Table 4 shows the detailed parameters of our data
augmentation processes

Table 4. Augmentation Parameters.

Augmentation

Rotation
Zoom
Shear
Horizontal shift
Vertical shift
Brightness
Horizontal ﬂip
Vertical ﬂip

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In the given context, there isn't any direct mention or discussion about addressing data bias during preprocessing of the deep learning pipeline. However, I can provide some general information regarding the mentioned techniques to handle data imbalance or bias.

Stratified splitting is a method used to maintain the original class proportions when dividing the dataset into training and testing sets. It ensures that both subsets contain a representative distribution of classes.

Oversampling and undersampling are techniques used to balance datasets containing unequal numbers of observations per class. Oversampling involves duplicating examples from the minority class, while undersampling removes instances from the majority class. Both methods aim to create a balanced dataset for better performance.

Diverse data collection refers to gathering data from multiple sources or environments to ensure that the dataset represents different scenarios and reduces potential biases. By collecting diverse data, models can learn patterns applicable across various situations rather than being limited to specific conditions.

These techniques help mitigate data bias and improve the overall performance of deep learning models. However, it should be noted that these methods may not always guarantee complete elimination of bias, especially if the underlying cause is complex or unknown.
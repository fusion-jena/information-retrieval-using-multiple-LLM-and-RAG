Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

prediction and training points.

fidence intervals from k-fold cross-validation is a yet unsolved puzzle 
requiring further research.

mapping probability samples that are exclusively used for map evalua-
tion are often not available and therefore alternative methods have been 
proposed.  In  machine  learning,  if  data  are  abundant,  a  common 
approach is to randomly divide the full dataset used for modelling into 
three parts: a training set, a validation set, and a test set (Hastie et al., 
2009,  Chapter  7).  The  training  set  is  used  for  fitting  the  models,  the 
validation set is used to estimate prediction error for model selection and 
hyperparameter tuning, while the test set is used for assessing the ac-
curacy of the final model. This paper addresses this latter testing phase, 
with the specific aim to assess the accuracy of a thematic map produced 
by a calibrated statistical prediction method. Data availability is often 
limited  so that setting  aside a  test set  cannot always be afforded and 
therefore resampling methods are used (Hastie et al., 2009; Steele et al.,

√
√
√
√
√
√
√

̂RMSE =

̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅
∑n
wi∙(z(si) (cid:0) ̂zm(si) )2

i=1

∑n

i=1

wi

̂MEC = 1 (cid:0)

∑n

i=1wi∙(z(si) (cid:0) ̂zm(si) )2
∑n
i=1wi∙(z(si) (cid:0) zs )2

(7)  

(8)  

where wi are the weights applied to individual observations (i.e. inverse 
sampling  intensities  for  cross-validation  method  3,  otherwise  the 
weights are constant and set to 1), zs  is the sample mean of reference 
̂

̂
values of the target variable and 
MEC are the estimated map 
accuracy metrics. Note that the pairs of reference observations and map 
predictions were collected over ten folds. 

RMSE and 

In the two model-based methods (methods 4 and 5), the map accu-
racy metrics are computed from simulated residuals at the N nodes of the 
grid covering the entire study area. For each simulation, estimates of 
RMSE and MEC were computed by Eqs. (9) and (10): 

√
√
√
√
√

̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅
∑N

r(si)2

i=1

N

̂RMSE sim =

̂MEC sim = 1 (cid:0)

∑N

i=1

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information on the criteria used to determine when training is complete. However, it mentions that the training set is used for fitting the models, and the validation set is used to estimate prediction error for model selection and hyperparameter tuning. These statements suggest that the training process might continue until the model achieves satisfactory performance on the validation set or reaches a predefined number of iterations. Commonly used criteria to determine when training is complete include reaching a minimum validation loss, achieving a stable validation accuracy, or observing no significant improvement in validation performance after several epochs.
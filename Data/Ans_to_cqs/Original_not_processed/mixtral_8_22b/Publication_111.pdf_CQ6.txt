Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Practically, over/underﬁtting diﬃculties in neural net-
work models are caused by the neural network model’s
excessive/insuﬃcient training epochs [43]. As a result, one
possible solution to the DL-based model’s over/underﬁtting
concerns is to apply the early stopping strategy [44], which is
used to cease training when generalisation performance
starts to degrade for a number of epochs. To track the
generalisation performance,
in the proposed model, the
training data is separated into training and validation
groups.

+e dropout approach [45] is another way to deal with
the overﬁtting problem. Dropout is a regularisation strategy
that allows you to train neural networks with alternative
topologies in parallel by randomly dropping out a certain
proportion of layer neurons. Dropout is indicated by the
black neurons in the fully connected layers, as seen in
Figure 3.

train than LSTM [16].

5.5. GRU-DNN Hyperparameter Analysis. GRU-DNN
model is trained in a supervised learning fashion using lag
features (i.e., using K previous observations), where K
denotes the number of previous observations used in the
training and forecasting task. Typically, K is considered
as a hyperparameter that needs to be optimized. +ere-
fore, we performed a grid search method to obtain the
optimal K value. Figure 6 depicts the grid search for
diﬀerent values of K hyperparameter over search space
ranges from 1 to 15. Speciﬁcally, Figure 6(a) presents the
model performance for water temperature forecasting
using various K values, where K � 6 achieves the lowest
MAE error. Similarly, K � 4 is the optimal value for
signiﬁcant wave height
shown in
Figure 6(b). It is noteworthy that the experiments pre-
sented in Figure 7 are for the ﬁrst dataset of each fore-
casting problem, assuming that the rest of the datasets
have similar behaviour.

forecasting

as

5. Experimental Results

5.1. Experimental Setup. +e experiments were performed
on a computer running 64-bit Linux OS with two 2.3 GHz
Intel 8-core processors. All of the utilized predictive
models were implemented in the Python programming
language version 3.8.5. Moreover, deep learning model
(i.e., GRU-DNN) implementation is performed using

Hidden layer (s) Outputlayer ...GRUcellGRUcellGRUcellGRUcellGRUcellGRUcell...XXXX......Input layer...FClayerFClayerGRUlayer 8483, 2021, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2021/8551167 by Vamsi Krishna Kommineni - Friedrich-Schiller-Universität , Wiley Online Library on [28/08/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License8

Computational Intelligence and Neuroscience

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The given context does not provide explicit information about the preprocessing steps involved before training a deep learning model such as normalization, scaling, or cleaning. However, it can be inferred that some form of preprocessing might have been done due to the mention of 'lag features' and 'K previous observations'. These terms suggest that time series data was used, and possibly some sort of feature engineering or transformation was applied to prepare the data for training the GRU-DNN model. Nevertheless, without specific details regarding these aspects, it cannot be confirmed what exact preprocessing steps were taken.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics82(2024)1027346or mel-frequency cepstral coefficients (MFCC) can be represented as a
single-channel images. We evaluate a commonly applied pre-trained
ResNet-50 architecture (He et al., 2016). Dufourq et al. (Dufourq
et al., 2022) also demonstrates strong classification performance and
low data requirements of ResNet-50 when comparing alternative pre-
trained CNN-based models. With the rapid development of transformer-
based models we also evaluate two transformer models Audio Spectro-
gram Transformer (AST) (Gong et al., 2021) which, like ResNet, oper-
ates on the spectrogram. We also evaluate HuBERT (Hsu et al., 2021)
which operates directly on the 1-D waveform. We evaluate the models
using the same feature extractor fθ and user model Uϕ pipeline presented
in Fig. 1 using a two-class binary output. Hence Uϕ is a two layer {100,2}
fully connected network.

Both transformer models outperform the ResNet-based architecture
with AST marginally outperforming the HuBERT model in terms of
validation accuracy 98.8%. While results will of course vary between
applications, AST demonstrates a good trade-off between data re-
quirements and classification performance. AST computes each segment
in 7.4 ms on average compared to HuBERT at 17.5 ms and ResNet at 1.9
ms. Computation time was evaluated using an Intel i7–9700 CPU with
an NVIDIA RTX2070 GPU. Table 1 reports validation results. The vali-
dation set is automatically generated by the application (McEwen et al.,
2023b) and maintains an 80/20% training-validation split. Training and
validation data are exported and the model is evaluated separately for
repeatability. Separate testing using a manually prepared test dataset
containing 500 samples achieved a similar test accuracy of 98.4% and
precision-recall results of 0.982 and 0.984 respectively. In addition to

We compare three classification models - ResNet-50 (He et al., 2016),
Audio Spectrogram Transformer (AST) (Gong et al., 2021) and HuBERT
(Hsu et al., 2021). The classification models compared in Section 4.1
were trained on a dataset containing 3000 5 s segments automatically
generated by the annotator. The dataset includes 1130 possum vocal-
isations and 1870 noise sources. The dataset has an 80/20% training-
validation split which is used to evaluate model performance during
training. Model hyperparameters are implemented based on the
recommendation outlined by the model authors. Each segment has been
manually reviewed by a human. 500 segments are separated into a test
set containing 187 possum vocalisations and 313 noise samples. The
distribution of target features to non-target features is representative of
the true distribution post-segmentation. Both validation and test dataset
performance are reported. Using the test dataset we evaluate the model

samples (red line) compared to randomly ordered samples, no active
learning (blue line), and active learning using high-certainty samples
(yellow line). The baseline accuracy is calculated using a 2-shot, 2-way
test accuracy without fine-tuning (81.4%). The AST feature extraction
model is fine-tuned on a subset of 500 training samples. Model perfor-
mance is re-evaluated on the test set at 10 sample increments using each
of the three sample orderings. Low certainty refers to queries with a
similar Euclidean distance to prototypical embeddings and therefore lie
along decision boundaries. For high-certainty samples, the order is
reversed with query embedding in close proximity to prototypical em-
beddings being prioritised for model fine-tuning.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The pipeline uses two types of deep learning models: convolutional neural networks (CNNs) and transformers. Specifically, it evaluates a pre-trained ResNet-50 architecture, which is a type of CNN, and two transformer-based models: Audio Spectrogram Transformer (AST) and HuBERT. These models operate on different input representations; while ResNet-50 works on single-channel images, AST processes spectrograms, and HuBERT operates directly on the 1-D waveform.
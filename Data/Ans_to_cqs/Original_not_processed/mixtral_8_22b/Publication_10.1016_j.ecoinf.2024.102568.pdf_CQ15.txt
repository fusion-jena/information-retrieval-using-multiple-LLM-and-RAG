Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The LSTM model is trained using the mean squared error (MSE) as 
loss function and the ADAM optimization (Kingma and Ba (2015)). To 
avoid overfitting, a dropout layer is added. The dropout layer, described 
¨
by 
Ozgür  and  Nar  (2020),  is  a  regularization  method  that  randomly 
excludes some inputs from activation and weight updates while training 
a network. Inputs not set to zero are scaled up by 1/(1 (cid:0) rate) such that 
the  sum  over  all  inputs  remains  the  same.  The  training  set  is  pre-
processed using a MinMaxScaler that transforms each feature by scaling 
it  to  a  [0–1]  range.  The  dimension  of  the  temporal  window  must  be 
selected  dynamically  considering  the  performance  obtained  with 
different values on the available dataset. 

In  the  specific  case  of  Modena,  an  Italian  city  spanning  183  km2, 

there are two legal AQ stations denoted by red dots in Fig. 6.

parameters optimization. In: 2012 IEEE 14th International Conference on 
Communication Technology, pp. 1022–1026. https://doi.org/10.1109/ 
ICCT.2012.6511415. 

ISO, 2011. ISO 19156:2011 - Geographic Information – Observations and Measurements. 

https://doi.org/10.13140/2.1.1142.3042. 

Kingma, D.P., Ba, J., 2015. Adam: A method for stochastic optimization. In: Bengio, Y., 

LeCun, Y. (Eds.), 3rd International Conference on Learning Representations, ICLR 
2015, San Diego, CA, USA, May 7–9, 2015. Conference Track Proceedings. URL. 
http://arxiv.org/abs/1412.6980. 

Li, J., Heap, A.D., 2014. Spatial interpolation methods applied in the environmental 

sciences: a review. Environ. Model Softw. 53, 173–189. URL. http://www.sciencedi 
rect.com/science/article/pii/S1364815213003113. https://doi.org/10.1016/j. 
envsoft.2013.12.008. 

Maag, B., Saukh, O., Hasenfratz, D., Thiele, L., 2016. Pre-deployment testing,

Zou, H., Hastie, T., 2005. Regularization and variable selection via the elastic net. J. R. 

Stat. Soc. Ser. B 67, 301–320. 

EcologicalInformatics81(2024)10256818

7.1. Performance 

Comparing the results of VR + SVR and LSTM, we can observe that, 
even if in some cases VR + SVR is performing better than LSTM, the 
increase in performance is usually less significant. Instead, in most cases 
where LSTM shows better results it significantly increases accuracy and 
reduces RMSE, MAE and MRE. Nevertheless, in cases where the size of 
the training dataset is restricted, the performance of LSTM models may 
prove to be inadequate. In these cases, the VR + SVR can provide better 
performance than the LSTM. From our experiments, we observed that a 
training period with <4000 observations generates a model unable to 
achieve satisfactory performance levels.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The deep learning pipeline uses the ADAM optimization technique, specifically mentioned as Adam optimization (Kingma and Ba (2015)). This optimization method is used for stochastic optimization during the training process of the LSTM model. Additionally, a dropout layer is employed as a regularization method to avoid overfitting. This layer randomly excludes certain inputs from activation and weight updates while training the network. Inputs not set to zero are scaled up by 1/(1 - rate) to maintain the total input sum. Furthermore, the training set undergoes preprocessing using a MinMaxScaler, which scales each feature into a [0-1] range. However, the dimension of the temporal window needs to be selected dynamically based on the performance achieved with different values on the available dataset.
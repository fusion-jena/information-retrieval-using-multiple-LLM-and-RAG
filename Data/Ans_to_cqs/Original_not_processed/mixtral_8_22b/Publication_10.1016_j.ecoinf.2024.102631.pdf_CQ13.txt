Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.3. Loss Functions 

In this work, we used a linear combination of the adversarial loss 

function Ladv, L1  loss function, and the perceptual loss function Lper. 

Ltotal = Ladv + λ1Lper + λ2L1

(4)  

(2) 

(3) 

where λ1  and λ2  are scaling factors set to 7 and 3, respectively. 

Adversarial Loss: We used adversarial loss to facilitate the training 
of the generator network and the discriminator network in a competitive 
manner. This loss function encourages the generator to minimize the log 
probability  that  the  discriminator  assigns  to  the  generated  samples, 

Fig. 4. PatchGAN Discriminator: It processes the image patch-wise, and each block in the output label map matrix shows whether the corresponding image patch is 
real or fake.

Table 4 
Quantitative  comparison  on  UCCS  dataset  using  UIQM,  UCIQE,  and  NIQE 
metrics.   

UIQM↑ 

UCIQE↑ 

NIQE↓ 

Raw_Images 
Fusion 
IBLA 
U-Transformer 
Funie-GAN 
Water-Net 
UWCNN 
Ours 

2.29 
2.99 
2.36 
3.02 
3.05 
3.13 
2.78 
3.17 

0.410 
0.476 
0.480 
0.539 
0.558 
0.550 
0.455 
0.568 

4.57 
4.38 
4.29 
3.96 
4.37 
4.07 
4.12 
4.03  

A quantitative analysis of this dataset is given in Table 5, confirming 
our  model's  effectiveness.  Our  method  has  achieved  a  UIQM  score  of 
3.26 and an NIQE score of 5.39, followed by the Funie-GAN in terms of 
UIQM with a score of 3.21 and Fusion in terms of NIQE with a score of 
5.40. While measuring the UCIQE score, IBLA achieved a score of 0.626, 
although their resultant images are just color deviations from the orig-
inal images. However, since UCIQE measures a linear combination of 
chroma, saturation, and contrast, a higher value can be expected just by 
having a change in colors (even if they are not desirable).

(cid:0)

(cid:0)

⃦
⃦
X , Z))

G

j

(6) 

Where, Lper  represents the perceptual loss. j refers to the index of the 
chosen  layer  in  the  VGG-19  network.  Cj,  Hj,  and  Wj  are  the  channel 
number, height, and width of the feature map in the jth layer of VGG-19. 
Φj(G(X, Y) ) is the feature representation of the generated underwater 
image G(X, Y) in the jth layer. Φj(Y) is the feature representation of the 
ground truth image Y in the jth layer. In this formulation, the perceptual 
loss measures the dissimilarity between feature maps extracted from the 
VGG-19  network,  helping  to  guide  the  optimization  process  toward 
generating  images  that  align  more  closely  with  the  features  of  the 
ground truth images.

The network takes images with dimensions of 256 × 256 pixels and 
three  color  channels  as  input.  It  follows  a  fully  convolutional  design, 
with  each  layer  applying  2D  convolutions  using  4  × 4  filters.  Batch 
Normalization and Leaky-ReLU activation functions are used after each 
convolution  layer  to  facilitate  network  training  and  stability.  The 
encoder consists of 5 blocks, which progressively reduce the spatial di-
mensions and learn features. It starts with a convolutional layer with 32 
output channels and strides of 2, followed by a residual block. The re-
sidual block comprises a Conv-ReLU-Conv structure, where a convolu-
tional layer is followed by a Rectified Linear Unit (ReLU) activation and 
another convolutional layer. This configuration is designed to effectively 
capture  and  enhance  image  features,  serving  as  a  critical  component 
within the network's architecture. The residual block output is then fed

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Unfortunately, the provided context does not explicitly mention any specific hyperparameters such as learning rates or optimizers used for the deep learning model. However, it does provide information about some aspects of the model's architecture and loss functions.

For instance, the total loss function used in the model is a linear combination of three different losses: adversarial loss (Ladv), perceptual loss (Lper), and L1 loss. These losses are combined with scaling factors λ1 and λ2, which are set to 7 and 3, respectively.

Additionally, the context mentions that the network uses batch normalization and leaky ReLU activation functions after each convolution layer. The encoder part of the network consists of five blocks, starting with a convolutional layer with 32 output channels and strides of 2, followed by a residual block. Each residual block contains a Conv-ReLU-Conv structure.

However, without further details regarding the specific implementation of the deep learning model, it is impossible to determine other important hyperparameters like learning rate, optimizer, number of epochs, etc.
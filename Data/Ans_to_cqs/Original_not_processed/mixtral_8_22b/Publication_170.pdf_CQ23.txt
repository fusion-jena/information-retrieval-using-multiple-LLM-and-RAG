Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

STUDY AREA
Our study area (28°14′‐28°22′N, 99°4′‐99°17′E) was located
at the Baimaxueshan Mountains (BMXSM), which were
part of the Hengduan Mountains in China (Fig. 1). The
Hengduan Mountains region, also known as the Mountains
of Southwest China, was identiﬁed as a global biodiversity
hotspot (Myers 1988, 1990, Myers et al. 2000). The altitude
changes from about 2000 m to higher than 4600 m above
sea level.

METHODS

Adaptive Incremental Training Method
To reduce the demand for computing resources for
model training, we proposed an adaptive incremental
training method (Fig. 2). In our method, we carried the
training process of the DCNN model in stages. The
model training starts with a smaller training set, and
then the trained model can be applied to the practical
ecological
In practical applications,
users can expand the previous training set by adding

survey project.

232

Wildlife Society Bulletin (cid:129) 45(2)

 23285540, 2021, 2, Downloaded from https://wildlife.onlinelibrary.wiley.com/doi/10.1002/wsb.1176 by Vamsi Krishna Kommineni - Friedrich-Schiller-Universität , Wiley Online Library on [29/08/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons LicenseTable 1. Incremental training sets sequence. There were 36,490 images in
BMXSM dataset.

Overall error

= (

FN FP TP

)/(

+

+

TN FN FP

+

+

Training sets(i)a

Number of
images(n)

n/N (%)

Empty images
rateb (%)

1
2
3
4
5
6
7

7,298
10,947
14,596
18,245
21,894
25,543
29,192

20
30
40
50
60
70
80

80.36
80.52
80.50
80.35
80.53
80.48
80.47

a i represented serial number of training set Traini, i = 1, 2, …, 7. Here
Train1 was initial training set and Traini (i = 2, 3, …, 7) was in-
cremental training set.

Model Evaluation
Model performance evaluation included both errors and
time costs evaluation. The iteration termination condition
of the model was the same in each round training (i.e., the
loss rate of the model was less than a preset threshold ɛ
−4]). We used the same test set (Test) to
[here ɛ = 1.0 × 10
evaluate the performance of models in diﬀerent training
phases. The model error evaluation used three metrics that
included overall error, commission error, and omission
error, which were deﬁned by equations (2), (3) and (4).

Table 2. Main conﬁguration parameters of the two platforms used.

)

(2)

(3)

(4)

Commission error

=

/(
FP TP

+

FP

)

Omission error

=

/(
FN TP

+

FN

)

Yang et al. (cid:129) An Adaptive Approach for Empty Images Filtering

233

 23285540, 2021, 2, Downloaded from https://wildlife.onlinelibrary.wiley.com/doi/10.1002/wsb.1176 by Vamsi Krishna Kommineni - Friedrich-Schiller-Universität , Wiley Online Library on [29/08/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License3.6 GHz central processing unit (CPU) and 32 GB of
memory, we used the incremental training mode to train
and test the DCNN model. On another Dell PowerEdge
C4130 rack server with two Tesla K80 GPUs and 256 GB
of memory, we used the zero‐start and the incremental
training modes to train and test models, respectively.

RESULTS
The incremental training mode performed consistently with
the zero‐start training mode when the training sample size

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, it appears that the authors employed an adaptive incremental training method to improve the generalizability of their deep convolutional neural network (DCNN) model. This approach involved carrying out the training process in stages, starting with a smaller training set and gradually expanding it by incorporating more data. By doing so, they aimed to minimize the computational demands associated with model training while still ensuring its robustness and applicability across various scenarios.

However, there is no explicit mention of specific techniques such as diverse datasets, cross-validation, or stratified splitting being utilized to enhance the model's generalizability. Nevertheless, the fact that the authors evaluated the performance of their model using different training phases suggests that some form of validation may have been implemented during the development process. Additionally, the inclusion of multiple metrics like overall error, commission error, and omission error further indicates a comprehensive assessment strategy designed to gauge the model's accuracy and reliability under varying conditions.
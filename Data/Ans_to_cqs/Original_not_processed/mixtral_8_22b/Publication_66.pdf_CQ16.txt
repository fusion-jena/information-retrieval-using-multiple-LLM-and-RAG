Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(b)

All GPS points

(c)

TDR recorded dives

(d)

TDR recorded non-dives  

(e)

Correctly predicted dives

(f)

Correctly predicted non-dives

(g)

Incorrectly predicted dives

(h)

Incorrectly predicted non-dives

BROWNING et al. 2041210x, 2018, 3, Downloaded from https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12926 by Thuringer Universitats- Und, Wiley Online Library on [16/11/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License688  |    Methods in Ecology and Evolu(cid:13)on

F I G U R E   4  Box	plots	of 	results	of	the	10-	fold	cross-	validation	
optimal	models	for	(a)	guillemots,	trained	using	coverage,	Xbar and 
Ybar,	and	(b)	shags	and	(c)	razorbills,	trained	using	altitude,	coverage,	
Xbar and Ybar.	T_AUC	is	the	training	AUC,	V_AUC	is	the	validation	
AUC,	PPV	is	the	positive	predicted	value	and	NPV	the	negative	
predicted	value.	The	solid	line	in	the	middle	of	the	boxes	represents	
the	mean	for	that	value

events.	The	impact	of	window	size	was	explored;	increasing	it	consis-

tently	increases	model	performance	(see	Appendix	S4).

The	data	for	each	model	were	randomly	split	into	10	equal	parts	

for	k-	fold	cross-	validation.	Each	model	was	then	trained	on	90%	of	the	

data	and	validated	on	the	remaining	10%;	this	was	performed	for	each	

tenth	of	the	data.	Additionally,	to	determine	 how	well	these	models	

might	 perform	 on	 data	 collected	 on	 birds	 from	 different	 colonies	 or

3 |  RESU LTS

3.1 | Model results

and	the	correct	predictions	produced	from	the	model	trained	with	all	

variables.	The	locations	with	the	highest	number	of	correctly	predicted	

dives	match	the	locations	with	the	greatest	number	of	true	dives.	The	

false	positives	and	negatives	are	also	shown,	demonstrating	the	accu-

racy	of	the	model.

3.1.1 | 10- fold cross- validation

3.1.2 | Leave- one- out cross- validation

Nearly	all	models	had	high	mean	specificity	values	above	90%,	mean-

The	 results	 of	 models	 trained	 with	 leave-	one-	out	 cross-	validation	

ing	 they	 predicted	 non-	diving	 events	 with	 high	 accuracy,	 and	 most	

showed	increased	variation	and	slightly	reduced	accuracy	compared	

had	mean	sensitivity	values	above	70%,	indicating	they	also	predicted	

to	10-	fold	cross-	validated	models	(Table	3;	see	Appendix	S6,	support-

diving	 events	 well	 (Table	2;	 see	 Appendix	 S5,	 supporting	 informa-

|  689

F I G U R E   6  Box	plots	of 	the	optimum	models	for	(a)	guillemots	
trained	with	coverage,	Xbar and Ybar	(b)	shags	trained	with	altitude	
and	coverage	and	(c)	razorbills	trained	with	coverage,	using	leave-	
one-	out	cross-	validation.	T_AUC	is	the	training	AUC,	V_AUC	is	the	
validation	AUC,	PPV	is	the	positive	predicted	value	and	NPV	the	
negative	predicted	value.	The	solid	line	in	the	middle	of	the	boxes	
represents	the	mean

was	 variation	 in	 the	 optimum	 models	 between	 species;	 however,	

altitude	 and	 coverage	 together	 were	 also	 good	 predictors	 of	 diving	

behaviour	in	all	species	(Table	3).	The	weakest	models	remained	the	

same	as	those	using	10-	fold	cross-	validation	(Appendix	S6).

3.2 | Alternative prediction methods

For	several	individuals	of	each	species,	the	HMMs	collapsed	for	both	

three-		and	two-	state	models.	This	meant	that	predictions	were	made	

for	 31	 and	 26	 guillemots,	 11	 and	 9	 shags	 and	 41	 and	 37	 razorbills,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide explicit information about the specific criteria used to determine when training is completed. However, it mentions that each model was trained using a k-fold cross-validation method, where the data was randomly split into 10 equal parts. Each model was then trained on 90% of the data and validated on the remaining 10%. This process was repeated for each tenth of the data. Additionally, a leave-one-out cross-validation method was also employed to evaluate the performance of the models on data collected from different bird colonies or individuals. These methods suggest that the completion of training might be determined based on achieving satisfactory performance metrics such as AUC, PPV, NPV, sensitivity, and specificity during the cross-validation processes.
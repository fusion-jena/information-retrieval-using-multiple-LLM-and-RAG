Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

different formats to achieve the best performance when leveraging MS data.

and  Mean  IoU  (MIoU).  Notably,  in  addition  to  the  comprehensive 
evaluation  provided  by  the  Overall  Accuracy  and  IoU,  their  mean 
counterparts  (MA  and  MIoU)  consider  the  individual  values  for  each 
class within the dataset and compute their average. This approach en-
hances the reliability of the classification results for imbalanced data. 
Furthermore, the highest MIoU value was employed to determine the 
point at which the DNN attained optimal performance. 

Finally, the selected SoftGroup architecture was trained over 1000 
epochs for each type of input data, and metrics such as MIoU or MA were 
computed after each epoch to analyse the training performance. Once 
trained, the configuration weights that achieved the best classification 
performance were selected to establish the reference trained model and, 
finally, it was used to perform predictions on the real MS point clouds 
collected in Section 2.3, i.e. the ground truth data.

3.2. Fuel model retrieval 

Once the DL model is trained, a few samples of real MS point clouds 
are segmented using the methodology exposed in Section 2.3 to create a 
ground truth. This ground truth was required to validate the classifica-
tions using a DL model trained with synthetic data. As mentioned in the 
previous section, the best classification performance of the SoftGroup 
semantic branch was achieved by the MS input data case; therefore, this 
pretrained  model was  used,  and  its  efficiency was  tested  by  inferring 
these real MS point clouds (Fig. 9). In addition, these inferences on real 
MS point clouds are taken as inputs for the decision tree scheme pre-
sented in Section 2.5, where the final fuel models are retrieved. The final 
fuel model classifications are summarised in Table 3. 

4. Discussion

Contents lists available at ScienceDirect 

Ecological Informatics 

journal homepage: www.elsevier.com/locate/ecolinf 

Wildfire response of forest species from multispectral LiDAR data. A deep 
learning approach with synthetic data 

Lino Comesa˜na-Cebral *, Joaquín Martínez-S´anchez , Gabriel Su´arez-Fern´andez , Pedro Arias 

CINTECX, Universidade de Vigo, Applied Geotechnology Group, 36310 Vigo, Spain   

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Multispectral LiDAR 
Deep learning 
Fire response 
Synthetic data 
Wildfire

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, there are several common methods used in machine learning and deep learning pipelines to handle data imbalance and bias, including stratified sampling, oversampling, undersampling, and diverse data collection.

Stratified sampling involves dividing the dataset into subsets based on certain criteria, such as class labels, and then randomly selecting instances from each subset to maintain the original distribution. This technique can help ensure that minority classes are adequately represented in both the training and validation sets.

Oversampling refers to increasing the number of instances belonging to underrepresented classes by duplicating existing examples or generating new ones through interpolation or extrapolation techniques. Synthetic Minority Over-sampling Technique (SMOTE) is one popular example of an oversampling algorithm.

Undersampling, conversely, reduces the size of the majority class(es) by removing some instances to balance the dataset. Random Under-Sampling (RUS), NearMiss, and Condensed Nearest Neighbor Rule (CNNR) are well-known undersampling algorithms.

Lastly, collecting diverse data can also mitigate biases introduced by limited representation in datasets. By gathering more varied examples, researchers can improve the generalizability and robustness of their models across different scenarios and populations.

In summary, while the given context does not explicitly mention any specific techniques used to address data bias during preprocessing of the deep learning pipeline, commonly used approaches include stratified sampling, oversampling, undersampling, and diverse data collection.
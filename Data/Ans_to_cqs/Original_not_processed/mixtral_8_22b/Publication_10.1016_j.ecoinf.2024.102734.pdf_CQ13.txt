Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We compare three classification models - ResNet-50 (He et al., 2016),
Audio Spectrogram Transformer (AST) (Gong et al., 2021) and HuBERT
(Hsu et al., 2021). The classification models compared in Section 4.1
were trained on a dataset containing 3000 5 s segments automatically
generated by the annotator. The dataset includes 1130 possum vocal-
isations and 1870 noise sources. The dataset has an 80/20% training-
validation split which is used to evaluate model performance during
training. Model hyperparameters are implemented based on the
recommendation outlined by the model authors. Each segment has been
manually reviewed by a human. 500 segments are separated into a test
set containing 187 possum vocalisations and 313 noise samples. The
distribution of target features to non-target features is representative of
the true distribution post-segmentation. Both validation and test dataset
performance are reported. Using the test dataset we evaluate the model

samples (red line) compared to randomly ordered samples, no active
learning (blue line), and active learning using high-certainty samples
(yellow line). The baseline accuracy is calculated using a 2-shot, 2-way
test accuracy without fine-tuning (81.4%). The AST feature extraction
model is fine-tuned on a subset of 500 training samples. Model perfor-
mance is re-evaluated on the test set at 10 sample increments using each
of the three sample orderings. Low certainty refers to queries with a
similar Euclidean distance to prototypical embeddings and therefore lie
along decision boundaries. For high-certainty samples, the order is
reversed with query embedding in close proximity to prototypical em-
beddings being prioritised for model fine-tuning.

∑

x∈Sd

fθ(x)

3.4. Active learning

(1)

A recommender system, Fig. 4, ranks segment embeddings in order
of lowest certainty to highest certainty. Low-certainty embeddings
correspond to features on decision boundaries. The Euclidean distance ld
is computed between existing prototypes cd and the query embeddings q
providing both class suggestion and class probability using P(q ∈ d | ld)
= softmax(1 (cid:0) norm(ld)). The recommender ranks queries in terms of

This methodology has been implemented into the Listening Lab
Annotator Tool (McEwen et al., 2023b), shown in Fig. 5. The following
section demonstrates the functionality of the annotator and methodol-
ogy workflow.

Grid view mode presents all five-second segments to the user as a grid
shown in Fig. 7. Samples are again ordered by the recommender. This is
an efficient method of quickly annotating recordings. The user can hover
over the spectrogram to play the sample and define on-click keys to
quickly apply labels.

By default, this method assigns samples to training or validation sets,
automatically maintaining an 80/20% training-validation split for each
class. Model prototypes are updated automatically and the user can
trigger model training and view training metrics through the training
tab. Training is mostly automated to make the methodology accessible
to all and does not require deep learning knowledge. We define seg-
mentation and classification endpoints so that users wanting to adapt
the pipeline can define their own segmentation and classification
models.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide specific details about the hyperparameters used in the deep learning models such as learning rate, optimizer, etc. It only mentions that the models were trained on a dataset containing 3000 5 s segments generated by the annotator and that the dataset was divided into an 80/20% training-validation split. Additionally, it states that the model hyperparameters were implemented based on the recommendation outlined by the model authors. However, it does not specify what those recommendations are or what the exact values of the hyperparameters used in the models are.
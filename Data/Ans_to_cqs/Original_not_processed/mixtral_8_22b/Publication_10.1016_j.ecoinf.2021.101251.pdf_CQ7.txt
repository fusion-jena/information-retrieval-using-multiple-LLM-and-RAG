Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

enables advanced search of data across the data packages in ESS-DIVE 
and  retrieval  of  the  specific  data  found  through  the  user’s  search 
query. The projects listed below are funded under the ESS program, and 
are required to submit data to ESS-DIVE for long-term stewardship. 

2.2. AmeriFlux

the best way for data producers to receive credit but are time consuming 
and  result  in  a  static  view  of  the  data.  Challenging  decisions  such  as 
defining  authorship  and  following  usage  policy  requirements  such  as 
attribution are not directly solved. In cases where usage rules for the 
integrated data are not the same, a method for unifying the data usage 
policies is needed. In our experience, treating all the data as having the 
most restrictive policy can work if the policies are compatible. That is an 
approach  we used  with FLUXNET  which  often  has  two different  data 
usage policies.

As  data  becomes  more  accessible,  data  consumers  are  able  to 
incorporate data from many data packages into an analysis. However, 
when a large number of data packages have been used, properly citing 
the data in the reference section without going over page limits can be 
difficult. For example, a paper based on the data from all the AmeriFlux 
sites, all of the NGEE-Tropics data packages, or all of the WFSFA data 
packages would need to include 350, 70, and 96 citations, respectively, 
in the references section. This large number of required citations often 
leads to the citations for the data being provided in the supplementary 
materials, within data/metadata files, or as a table of identifiers. In all of 
these cases, it is difficult to programmatically find the citation of the 
data and thus is unlikely to be counted in the data’s citation metrics. A 
solution to this challenge would be to provide a single citation that can

5.1. Data collections

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The provided context does not contain information about the criteria used to split the data for deep learning model training (e.g., train, test, validation). Therefore, I cannot provide an accurate response based solely on the given context. Typically, the dataset is divided into three parts: training set, validation set, and testing set. The division ratio depends on various factors like the size of the dataset, its complexity, and the problem statement. Commonly, the dataset is divided into 80% for training, 10% for validation, and 10% for testing. However, these ratios may vary depending on the specific use case.
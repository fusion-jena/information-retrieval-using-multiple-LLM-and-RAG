Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

mapping probability samples that are exclusively used for map evalua-
tion are often not available and therefore alternative methods have been 
proposed.  In  machine  learning,  if  data  are  abundant,  a  common 
approach is to randomly divide the full dataset used for modelling into 
three parts: a training set, a validation set, and a test set (Hastie et al., 
2009,  Chapter  7).  The  training  set  is  used  for  fitting  the  models,  the 
validation set is used to estimate prediction error for model selection and 
hyperparameter tuning, while the test set is used for assessing the ac-
curacy of the final model. This paper addresses this latter testing phase, 
with the specific aim to assess the accuracy of a thematic map produced 
by a calibrated statistical prediction method. Data availability is often 
limited  so that setting  aside a  test set  cannot always be afforded and 
therefore resampling methods are used (Hastie et al., 2009; Steele et al.,

If the full sample dataset is acquired by simple random sampling and 
if k equals the sample size (i.e. leave-one-out cross-validation, hereafter 
LOOCV), estimation from cross-validation is known to be nearly unbi-
ased  (Bengio  and  Grandvalet,  2004;  Krzanowski,  2001;  Steele  et  al., 
2003).  Since  the  computational  burden  of  LOOCV  is  heavy,  k  is 
conventionally set to five or ten, in which case bias is no longer negli-
gible but can be parametrically corrected (Fushiki, 2011). However, the 
sample datasets used for mapping environmental variables generally are 
not  acquired  by  simple  random  sampling.  Rather,  they  are  amalgam-
ations  of  several  datasets,  each  with  its  own  formal  or  opportunistic 
sampling  design,  which  strictly  impedes  design-based  inference.  The 
latter also applies to probability samples whose inclusion probabilities 
are not published, such as the LUCAS dataset (dâ€™Andrimont et al., 2020).

3.  Moderately clustered sample. This sample was produced by stratified 
sampling. First, the study area was divided into 100 compact geo- 
strata  using  the  spcosa  package  (Walvoort  et  al.,  2010).  In  each 
sample, 20 of the 100 geo-strata were randomly selected to form a 
stratum  and  50%  of  the  total  sample  size  was  randomly  chosen 
within  this  stratum.  The  locations  of  the  remaining  50%  of  the 
sample were randomly chosen from the stratum formed by the other 
80 geo-strata.

Pebesma (2021). In contrast, the current work aims to tackle bias in map 
accuracy  estimates  from  cross-validation  by  balancing  the  impact  of 
residuals in the regions with different sampling intensities. Otherwise, 
residuals in densely sampled regions dominate the accuracy estimates 
and  this  effect  arises  even  if  the  clustered  sample  comprises  the  full 
feature space and, hence, no extrapolation occurs.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not directly discuss deep learning pipelines, but it does mention some techniques used in machine learning that could potentially help address data bias during preprocessing. These include dividing the full dataset into three parts: a training set, a validation set, and a test set. This approach is commonly used when data are abundant.

However, when data availability is limited, resampling methods are used instead. One such method mentioned is Leave-One-Out Cross-Validation (LOOCV), where the sample size is equal to k. This method is known to provide nearly unbiased estimations, although it has a heavy computational burden. Therefore, k is conventionally set to five or ten, in which case bias is no longer negligible but can be parametrically corrected.

Stratified sampling is another technique discussed in the text. It involves dividing the study area into strata and then selecting samples within these strata. This method was used to produce a moderately clustered sample.

While the text does not explicitly mention oversampling, undersampling, or diverse data collection, these are common techniques used in machine learning to address class imbalance and improve model performance. Oversampling involves duplicating examples from the minority class, while undersampling involves removing examples from the majority class. Diverse data collection involves collecting more varied data to better represent the population.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In  the  subsequent  sections,  we  assess  model  performance  in  the 
context of model complexity and explainability, crop specific training 
against  generalized  training,  and  contrast  remote  sensing  and  meteo-
rological measurements to provide predictive performance for several 
potential use cases. 

3.1. Model complexity 

The management of model complexity is an important consideration 
in  data-driven  model  development (Jolliffe,  1986; Wang  et  al.,  2017; 
Yates  et  al.,  2021).  In  this  study  we  define  model  complexity  as  the 
number of variables included in the predictor set. Proper selection of 
variables, i.e. feature selection, can minimize training costs, measure-
ment  requirements,  and  decrease  the  likelihood  of  overfitting  (Banks 
and Fienberg, 2003; Cawley and Talbot, 2010).

the  primary  hyperparameters  required  for  application  of  the  random 
forest method. Here, hyperparameter optimization was performed using 
Bayesian  Optimization  to  minimize  a  5  k-fold  cross-validation  loss 
during model development (Section 2.7).

3.4. Model Explainability 

Data-driven  modelling  exercises  are  often  motivated  to  achieve 
maximum predictive performance using all available predictor variables 
rather than focusing on understanding the value of each predictor var-
iable toward developing more parsimonious models. Feature selection is 
often performed prior to model development to reduce training costs,

2.7. Machine learning training methodology 

To ensure accuracy and reliability of results, a repeated nested k-fold 
cross-validation approach was employed for all ML models developed 
here. An outer 10 k-fold cross-validation provided an initial division of 
the data into holdout data and data for model development (Molinaro 
et al., 2005). An inner 5 k-fold cross-validation was then used to mini-
mize the bias caused by tuning of hyperparameters on training results, as 
demonstrated in previous studies (Cawley and Talbot, 2010; Yates et al., 
2022).  The  training  performance  was  calculated  based  on  the  inner 
cross-validated  ensemble,  while  the  testing  performance  was  deter-
mined by selecting the optimal set of hyperparameters from the entire 
training set to predict on the holdout data. This process was repeated 15 
times for each unique model to reduce the potential bias and variance 
related to random sampling (Kim, 2009).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The strategy implemented to monitor the model performance during training involves a repeated nested k-fold cross-validation approach. This approach ensures accuracy and reliability of results for all machine learning models developed.

Initially, a 10 k-fold cross-validation is applied to divide the data into holdout data and data for model development. Following this, a 5 k-fold cross-validation is utilized within the first fold to minimize the bias caused by tuning of hyperparameters on training results.

The training performance is determined based on the inner cross-validated ensemble, while the testing performance is established by choosing the best set of hyperparameters from the complete training set to forecast on the holdout data. To further diminish any potential bias or variance related to random sampling, this process is executed 15 times for every distinct model.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.5.3. Artificial neural network 

ANNs are used for modelling complex phenomena and developing 
nonlinear regression (Haykin and Lippmann, 1994). It consists of three 
different  layers:  input,  hidden,  and  output  (Ingram  et  al., 2005).  The 
ANN uses calibration parameters, such as the learning rate and weight, 
to improve the results. The weights were randomly initialized using the 
Nguyen–Widrow method (Zhou et al., 2016). An ANN fitted with CARET 
is a single hidden-layer neural network with two hyperparameters: size 
and decay. The number of units in the hidden layer (size) and regular-
isation parameter to avoid overfitting (decay) were passed as arrays of 
1–20  and  0.025–1,  respectively.  The  Transfer/Activation  function  for 
the  model was  selected  as  the  logistic  function.  The  optimum  combi-
nation of parameters for the best model fit was based on accuracy. 

3.6. Model fitting and evaluation

The  model  initially  included  all  variables,  acknowledging  the  sig-
nificance  of  identifying  and  selecting  pertinent  variables  for  model 
development. Variable importance measures were employed to evaluate 
their significance, which helped identify the most influential variables 
for  predicting  AGB.  However,  it  is  recognised  that  assessing  model 
performance is essential, particularly when dealing with less important 
variables. A stratified sampling method was applied to all AGB obser-
vations for this evaluation. This technique randomly allocated 70% of 
the samples to the training dataset and the remaining 30% to the vali-
dation  dataset.  Three  error  statistics,  specifically  R2,  mean  absolute 
error  (MAE),  and  RMSE  (as  indicated  in  Eqs.  8,  9,  and  10),  were 
employed to compare the accuracies of the models. The generated ma-
chine learning models were then extrapolated for the entire region. This

overfitting issue, we used the parameter (c) to optimise the line fit and 
penalise the number of samples inside the margin. The value of c plays a

Fig. 5. Model predicted vs observed AGB (a) Random Forest, (b) Artificial Neural Network, (c) Support vector machine of Sentinel-1 and Sentinel-2.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps can be taken to ensure optimal performance. These may include data cleaning, where missing or irrelevant values are removed from the dataset. Normalization or standardization can also be performed to rescale features so they have similar magnitudes, improving convergence during optimization. Scaling techniques like min-max scaling or z-score normalization can help achieve this. Additionally, feature engineering might involve creating new features from existing ones to better capture patterns in the data. Lastly, splitting the dataset into training, validation, and testing sets allows for proper model evaluation and tuning. In the given context, no specific preprocessing steps are mentioned for the artificial neural network model. However, these general practices could apply depending on the nature of the input data.
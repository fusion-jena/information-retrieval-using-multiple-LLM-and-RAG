Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

A pooling layer offers downsampling functionality that decreases the dimensionality
of the feature maps to achieve translation invariance to the alterations and the biases
incorporated and thus helps in reducing the number of learnable parameters. There are
two types of pooling operations, namely Max Pooling and Global Average Pooling [64].
The ﬁrst one extracts speckles from the input feature maps and offers maximum values
in each of the speckles and leaves the remaining values unattended. The second one
downsamples a feature map with a size equaling product of height and width into an array
of a one cross one by averaging the elements of each feature map by retaining the depth of
the feature map. The advantage of Global Average Pooling lies in reducing the number of
learnable parameters along with offering the CNN with variable sized input.

Convolution is used for feature extraction, in which a kernel is applied to an input
tensor. A feature map is thus obtained through the product of kernel elements and tensor
input. The procedure is then repeated on multiple kernels to obtain random feature maps
that represent different feature extractors. The hyperparameters involved in convolution
operations are the size and number of kernels. The size could be anything from 3 × 3 to
5 × 5 to 7 × 7, and the kernel could be chosen randomly.

Remote Sens. 2021, 13, 3284

15 of 17

Supplementary Materials: The following are available online at https://www.mdpi.com/article/10
.3390/rs13163284/s1. Table S1: Methodology of the convolution and full connected layer.

Author Contributions: Conceptualization, data curation, formal analysis: A.A., M.K.P., and P.K.S.;
funding acquisition: P.K.S.; supervision: P.K.S. and M.L.K.; validation, visualization, writing—
original draft: A.A., M.K.P., and P.K.S.; writing—review and editing: A.G., P.K.S., and M.L.K. All
authors have read and agreed to the published version of the manuscript.

Funding: National Mission on Himalayan Studies, G.B. Pant National Institute of Himalayan Envi-
ronment (NIHE), Almora, Uttarakhand, India.

Institutional Review Board Statement: Not applicable.

Informed Consent Statement: Not applicable.

applications and relevance to most current MAXENT studies. Divers. Distrib. 2014, 20, 1–9. [CrossRef]
Schmidhuber, J. Deep learning in neural networks: An overview. Neural Netw. 2015, 61, 85–117. [CrossRef]

60.
61. Guo, Y.; Liu, Y.; Oerlemans, A.; Lao, S.; Wu, S.; Lew, M.S. Deep learning for visual understanding: A review. Neurocomputing 2016,

62.

187, 27–48. [CrossRef]
Fukuda, S.; De Baets, B.; Waegeman, W.; Verwaeren, J.; Mouton, A.M. Habitat prediction and knowledge extraction for spawning
European grayling (Thymallus thymallus L.) using a broad range of species distribution models. Environ. Model. Softw. 2013, 47,
1–6. [CrossRef]

63. Harris, D.J. Generating realistic assemblages with a joint species distribution model. Methods Ecol. Evol. 2015, 6, 465–473.

[CrossRef]

64. Lin, M.; Chen, Q.; Yan, S. Network in network. arXiv Prepr. 2013, arXiv:1312.4400.
65. Agarap, A.F. Deep learning using rectiﬁed linear units (relu). arXiv Prepr. 2018, arXiv:1803.08375.
66.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The provided context does not explicitly mention any specific techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, it can be inferred that some form of data preprocessing was likely performed given the nature of the research involving remote sensing and deep learning methodologies.

Stratified splitting involves dividing the dataset into training and testing sets while maintaining the same proportions of classes within both sets. This technique ensures that the classifier has been trained on a representative sample of all possible classes.

Oversampling and undersampling involve adjusting the balance between majority and minority classes in order to improve classification performance. In oversampling, additional instances of the minority class are added to the dataset, whereas in undersampling, instances of the majority class are removed.

Diverse data collection refers to collecting a wide variety of samples representing various conditions and scenarios in order to ensure that the resulting model generalizes well across different situations.

While these techniques were not specifically mentioned in the provided context, they may still have been employed during the preprocessing stage of the deep learning pipeline. It is important to note that addressing data bias is crucial in ensuring accurate and reliable results when applying deep learning algorithms to real-world problems.
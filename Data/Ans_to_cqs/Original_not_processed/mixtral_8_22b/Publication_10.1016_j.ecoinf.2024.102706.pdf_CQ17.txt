Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

tible to overfitting; therefore, it is crucial to address this issue by tuning
the hyperparameters.

The Enet algorithm is a regularisation process that combines lasso
and ridge regression (Zou and Hastie, 2005). The shrinkage process from
the lasso and ridge adds penalty terms L1 and L2, respectively. This
means that by adding penalties, they (lasso and ridge) remove or shrink
predictors, which do not affect reducing the sum of square error (SSE).
Enet aggregates L1 and L2 penalties from lasso and ridge and forms a
linear regularisation method (Eqs. (3)–(5)).

∑

SSELasso =

(x (cid:0) ̂x)2 + λ

∑

SSERidge =

(x (cid:0) ̂x)2 + λ

∑

∣β∣

β2

∑

SSEEnet = 1 (cid:0)

∑

(
(x (cid:0) ̂x)2 + λ

(1 (cid:0) α)

∑

∑

)

|β|

β2 + α

(1 (cid:0) α)

∑

∑

)

|β|

β2 + α

Where x is the true value, ̂x is the predicted value, alpha (α) and beta
(β) are the penalty terms, and λ is the coefficient of the penalties. α is a
regularisation strength, where higher values specify more robust regu-
larisation. β represents the mixing parameter and determines the bal-
ance between the L1 and L2, denoted as the L1 ratio (l1_ratio). In the
equation of SSEEnet, if l1_ratio = 1, it will turn out to be SSELasso, and if
l1_ratio = 0, it will be SSERidge. Hence, mixing parameters between L1
and L2 for Enet regression, the l1_ratio should lie between 0 and 1. In
elastic net regression, the regularisation parameter lambda (λ) is not
explicitly used but is indirectly controlled through the α parameter.
Hence, the Enet regression needs α and l1_ratio parameters to be opti-
mised for a best-performing model.

100
5
0.05
20
0.9
0.8
5

linear as a simple statistical model and the rest of the advanced machine
learning models as machine learning. The regression analyses were
conducted using Python 3.12.2, distributed by Anaconda Inc. The Scikit-
learn library (Pedregosa et al., 2011) was employed for all models except
those utilising XGboost and LightGBM. For XGboost and LightGBM
models, the respective Python libraries “xgboost” (Chen and Guestrin,
2016) and “lightgbm” (Ke et al., 2017) were utilised. Additionally, a 5-
fold cross-validation grid search strategy was implemented to tune
hyperparameters within the training dataset for all machine learning
models, leveraging functionalities inherent in their respective Python
packages. The information on hyperparameters and their optimised
value for all machine learning models are presented in Table 4. More-
over, we converted the canopy cover output percentage to a fraction
(0–1) for convenience and smoothness in the further calculation.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, the Elastic Net (Enet) algorithm is discussed as a regularization method used to prevent overfitting. It combines two popular regularization techniques - Lasso (L1) and Ridge (L2) regressions. Both these techniques add penalty terms to the loss function during optimization, causing some coefficients to become zero or close to zero, effectively removing them from the model.

Lasso Regression uses L1 regularization, which adds absolute size of coefficient as penalty term to the loss function. This leads to sparse solutions, i.e., many variables have zero weights.

Ridge Regression uses L2 regularization, which adds squared size of coefficient as penalty term to the loss function. This results in smaller coefficients, making the model simpler and less likely to overfit.

Elastic Net combines both L1 and L2 regularizations, allowing for a balance between the two. This is done by introducing a new parameter called 'alpha', which controls the overall strength of regularization, and another parameter called 'l1_ratio'. The latter determines the proportion of L1 vs L2 regularization. When l1_ratio=1, Elastic Net becomes equivalent to Lasso Regression, while when l1_ratio=0, it becomes equivalent to Ridge Regression.

Therefore, in the context of the deep learning pipeline mentioned, Elastic Net could be used as a regularization method to prevent overfitting, combining the benefits of both Lasso and Ridge Regressions. Other common regularization methods include Dropout, Early Stopping, and Batch Normalization. However, these are not explicitly mentioned in the provided context.
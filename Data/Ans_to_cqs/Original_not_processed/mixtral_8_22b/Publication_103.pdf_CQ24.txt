Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

R.PC.PC

I.PC.PC

Random

ImageNet

PlantCLEF using transfer learning from herbarium data

H1K.PC.PC

H1KI.PC.PC

Herbarium1K

ImageNet,Herbarium1K

CRLeaves

CRLeaves

CRLeaves

CRLeaves

PlantCLEF

PlantCLEF

PlantCLEF

PlantCLEF

CRLeaves

CRLeaves

CRLeaves

CRLeaves

PlantCLEF

PlantCLEF

PlantCLEF

PlantCLEF

0.37

0.51

0.416

0.491

0.334

0.523

0.273

0.425

0.50

0.61

0.542

0.590

0.566

0.726

0.498

0.661

Jose et al. BMC Evolutionary Biology  (2017) 17:181 

Page 11 of 14

non-dried leaf scans. Therefore, we conducted the follow-
ing experiments:

• R.CR.CR: The neural network was initialized

randomly, trained on the Costa-Rica leaf scans
training set (70%) and tested on the Costa-Rica leaf
scans test set (30%).

• I.CR.CR: The neural network was pre-trained on the
generalist dataset ImageNet to initialize the weights,
fine-tuned on the Costa-Rica leaf scans training set
(70%) and tested on the Costa-Rica leaf scans test set
(30%).

Vanhoucke V, Rabinovich A. Going deeper with convolutions. In: 2015
IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Boston: IEEE Conference; 2015. p. 1–9. doi:10.1109/CVPR.2015.7298594.
Ioffe S, Szegedy C. Batch normalization: Accelerating deep network
training by reducing internal covariate shift. CoRR. 2015. abs/1502.03167.
[Online]. Available http://arxiv.org/abs/1502.03167.

35.

37.

36. He K, Zhang X, Ren S, Sun J. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. CoRR. 2015.
abs/1502.01852. [Online]. Available http://arxiv.org/abs/1502.01852.
Jia Y, Shelhamer E, Donahue J, Karayev S, Long J, Girshick R,
Guadarrama S, Darrell T. Caffe: Convolutional architecture for fast feature
embedding. In: Proceedings of the 22Nd ACM International Conference
on Multimedia. New York: ACM; 2014. p. 675–8.
doi:10.1145/2647868.2654889.

38. Mata-Montero E, Carranza-Rojas J. Automated plant species

Discussion and conclusions
This study is, to our knowledge, the first one that ana-
lyzes a big dataset with thousands of specimen images
from herbaria and uses deep learning. Results show the
potential of deep learning on herbarium species identifi-
cation, particularly by training and testing across different
herbarium datasets. This could potentially lead to the cre-
ation of a semi, or even fully, automatic system to help

Jose et al. BMC Evolutionary Biology  (2017) 17:181 

Page 12 of 14

Fig. 7 Comparison of losses of R.PC.PC, I.PC.PC and H1K.PC.PC experiments

taxonomists and experts do their annotation, classifica-
tion, and revision work at herbarium.

Experiments and results
All experiments measured the top-1 and top-5 accu-
racy of the trained deep learning model under different
circumstances, i.e., herbarium specimens classification
(“Herbarium specimen classification” section, Table 3),
transfer learning across herbarium data from differ-
ent regions (“Cross-Herbaria transfer learning” section,
Table 4), and transfer learning from herbarium data to
non-dried plant images (“Transfer learning from herbar-
ium to non-dried plant images” section, Table 5).

For each of these experiments, table columns are

defined as follows:

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper does not explicitly discuss strategies to handle randomness in the deep learning pipeline, such as managing random seed values. However, it mentions two main approaches used in its experiments: randomly initializing the neural network (denoted as 'R') and initializing the neural network with pre-trained weights from another dataset (denoted as 'I'). These initialization methods can impact the overall performance of the deep learning models. Random initialization may introduce variability due to the stochastic nature of the process, while pre-training on another dataset might provide more consistent starting points for fine-tuning on specific tasks. Nonetheless, the paper does not delve into specific techniques to manage randomness within the deep learning pipeline.
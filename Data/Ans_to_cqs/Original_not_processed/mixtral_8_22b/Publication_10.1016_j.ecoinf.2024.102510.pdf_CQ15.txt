Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Yang, L., Shami, A., 2020. On hyperparameter optimization of machine learning 

algorithms: theory and practice. Neurocomputing 415, 295–316. 

Shorten, C., Khoshgoftaar, T.M., 2019. A survey on image data augmentation for deep 

Yang, C.-L., Harjoseputro, Y., Hu, Y.-C., Chen, Y.-Y., 2022a. An improved transfer- 

learning. J. Big Data 6 (1), 1–48. 

Sitepu, A.C., Liu, C.-M., Sigiro, M., Panjaitan, J., Copa, V., 2022. A convolutional neural 
network bird’s classification using north american bird images. J. Health Sci. 6 (S2), 
15067–15080. 

Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., Patwary, M., 

Prabhat, M., Adams, R., 2015. Scalable bayesian optimization using deep neural 
networks. In: International Conference On Machine Learning. PMLR, pp. 2171–2180. 

learning for image-based species classification of protected indonesians birds. CMC 
Comp. Mater. Continua 73 (3), 4577–4593.

search  tests  various combinations of  the hyperparameters  identified in  manual  search and  trains the selected 
models to achieve the maximum possible accuracy through multiple iterations. Experimental analysis revealed 
that the Fine-tuned EfficientNetB0 model exhibited superior performance, achieving an accuracy of 99.12%. In 
contrast, the performance of the ResNet18 model was disappointing with an accuracy of 93.24%, while other 
models outperformed it.

3.3. Result analysis 

In  this  study,  the  performance  of  eight  pre-trained  CNN  models, 
namely MobileNetV2, EfficientNetB0, GoogleNet, DenseNet201, Incep-
tionV3,  ResNet18,  InceptionResNetV2,  and  NASNetMobile  was  evalu-
ated. We assessed their effectiveness as both feature extractors and fine- 
tuners.  The  experiments  utilized  hyperparameters  optimized  through 
the  HHOS  approach,  which  resulted  in  the  best  training  outcomes. 
Strategy  1  utilizes  pre-trained  models  as  feature  extractors  and  then 
trains  the  classifier  using  the  extracted  features.  During  testing,  we 
observed that the validation accuracy was lower than retraining a few 
top  layers.  On  the  other  hand,  strategy  2,  which  is  the  fine-tuning 
strategy, retains the weights of the earlier layers while retraining only

Remarks 

Initial Learning Rate 

Learn Rate Drop 
Factor 
Learn Rate Drop 
Period 
Learn Rate 
Scheduling Method 

The initial learning rate 
used at the beginning 
Factor by which the 
learning rate is dropped 
Number of epochs between 
each learning rate drop 
Method for scheduling the 
learning rate 

Shuffle 

Optimizer 

Activation function 

Shuffle the training data 
before each epoch 
The optimization 
algorithm used for training 
To introduce non-linearity 
into the output of neurons 

1 

2 

3 

4 

5 

6 

7 

8 

Real numbers >0 

Real numbers 

0.0001, 0.001, 
0.01, 0.1 
0.1, 0.5, 0.9 

Higher values can result in faster convergence, but too high can 
lead to instability 
Higher values lead to larger drops in the learning rate 

Positive integers 

5, 10, 20 

Smaller values lead to more frequent drops in the learning rate 

Fixed, Step, 
Exponential, 
Polynomial 
Boolean (True, False) 

fixed 

True, False

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The paper mentions the use of an Optimizer in the deep learning pipeline, specifically in the context of training the classifier using the extracted features from pre-trained models. However, the specific optimization technique or algorithm used is not explicitly stated in the given excerpts. Commonly used optimization algorithms in deep learning include Stochastic Gradient Descent (SGD), Adam, RMSprop, and others. These algorithms help minimize the loss function during the training process. Without further information, it cannot be definitively determined which specific optimization technique was employed in this case.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

• Data-augmentation, also called data transformation or distortion, is used to artiﬁcially increase
the number of samples in the training set by applying speciﬁc deformations on the input images,
e.g., rotation, ﬂipping, translation, cropping, or changing the brightness of the pixels. In this way,
from a small number of initial samples, one can build a much larger dataset of transformed
images that still are meaningful for the case study. The set of valid transformations that
improves the performance of the CNN-model depends on the particularities of the problem.
Several previous studies have demonstrated that increasing the size of the training dataset using
different data-augmentation techniques increases performance and makes the learning of CNNs
models robust to changes in scales, brightness and geometrical distortions [44,47].

3.2. Detection Phase

To obtain an accurate detection in a new image, different from the images used for training the

improvement techniques such as data-augmentation and transfer-learning, which allow reutilization
of the knowledge acquired from a set of images into other new images. Currently, the most commonly
used software implementing CNNs is the open source library of Tensorﬂow by GoogleTM ([18]),
which requires programming skills since it does not have a graphical user-interface.

wt+1 = µwt − α∆J(wt)

(2)

where µ is the momentum weight for the current weights wt and α is the learning rate.

The network weights, wt, can be randomly initialized if the network is trained from scratch.
However, this is suitable only when a large labeled training-set is available, which is expensive in
practice. Several previous studies have shown that data-augmentation ([44]) and transfer learning ([45])
help overcoming this limitation.

•

Deep CNNs, such as ResNet and GoogLeNet, are generally trained based on the prediction loss
minimization. Let x and y be the input images and corresponding output class labels, the objective of
the training is to iteratively minimize the average loss deﬁned as

J(w) =

1
N

N
∑
i=1

L( f (w; xi), yi) + λR(w)

(1)

This loss function measures how different is the output of the ﬁnal layer from the ground truth.
N is the number of data instances (mini-batch) in every iteration, L is the loss function, f is the
predicted output of the network depending on the current weights w, and R is the weight decay with
the Lagrange multiplier λ. It is worth mentioning that in the case of GoogLeNet, the losses of the

Remote Sens. 2017, 9, 1220

6 of 22

two auxiliary classiﬁers are weighted by 0.3 and added to the total loss of each training iteration.
The Stochastic Gradient Descent (SGD) is commonly used to update the weights.

wt+1 = µwt − α∆J(wt)

(2)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Data augmentation techniques are used to artificially increase the number of samples in the training set by applying specific deformations on the input images. This allows building a larger dataset of transformed images that are still meaningful for the case study. Some examples of these transformations include rotation, flipping, translation, cropping, or changing the brightness of the pixels. These techniques help improve the performance of Convolutional Neural Network (CNN) models and make them more robust to changes in scales, brightness, and geometric distortions. The effectiveness of these techniques depends on the specific problem being addressed.
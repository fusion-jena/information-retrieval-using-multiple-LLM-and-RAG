Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In  addition  to  calculating  MAE  values,  we  also  compared  the 
inference-time to determine the most computationally efficient model. 
We set the batch to 400 to achieve 100% GPU utilization of each model 
and to measure the maximum queries per second (QPS) achieved under 
real-world conditions (Ding et al., 2021). All models were tested under 
the same experiment setting and the same GPU to make comparisons 
between them fair. To do so, we first expand the dataset by a factor of 
five (from 8002 to 40,010 images). Then, we fed batches of 50 images 
into the model to warm up the hardware, before recording the inference- 
time of batches of 100 to quantify QPS. 

pi,j =

(cid:0)
exp

)

zi,j
(cid:0)

exp

zi,m

∑M

m=1

)

(1)  

3. Results

L = (cid:0)

1
N

∑N

i=1

yi,jlog(pi)

(4)  

where yi, j is a soft label of the i-th sample (the ground truth age is j), and 
pi is vector of probability predicted from i-th sample based on Eq. (3). 

2.5. Training

Table 4 
Performance of different models on the PAD Lite dataset under a five-fold SE 
Protocol with an OR method.  

Model 

Method 

MAE 

CS (3)(%) 

ResNet-18 
ResNet-50 
VGG-11-bn 
VGG-16-bn 
MobileNet-V3 
RepVGG-A0 
RepVGG-A1 

OR 
OR 
OR 
OR 
OR 
OR 
OR 

2.80 ± 0.30 
2.89 ± 0.29 
2.90 ± 0.31 
2.92 ± 0.28 
2.93 ± 0.44 
2.41 ± 0.46 
2.45 ± 0.44 

64.10 
63.12 
61.42 
61.63 
64.94 
70.62 
70.10  

Fig. 4. The effect of different thresholds for CS. We tested the performance of 
models under different thresholds with the same experimental settings. And it’s 
just  show  the  better  model  within  four  types  of  networks  (VGG,  ResNet, 
RepVGG and MobileNet-V3). 

struggle with adult pandas, often categorizing pandas aged 17 to 25 with 
pandas aged 7 to 17 years.

Table 2 
Performance of different models under five-fold SE Protocol with MC and OR 
methods.  

Model 

Method 

MAE 

ResNet-18 
ResNet-50 
VGG-11-bn 
VGG-16-bn 
MobileNet-V3 
RepVGG-A0 
RepVGG-A1 

MC/OR 
MC/OR 
MC/OR 
MC/OR 
MC/OR 
MC/OR 
MC/OR 

3.37 ± 0.36/2.93 ± 0.33 
3.37 ± 0.38/2.91 ± 0.31 
3.66 ± 0.48/3.42 ± 0.39 
3.68 ± 0.45/3.40 ± 0.44 
3.50 ± 0.67/3.36 ± 0.69 
2.63 ± 0.64/2.51 ± 0.47 
2.65 ± 0.64/2.57 ± 0.50 

CS (3)(%) 

61.48/63.27 
59.01/64.12 
58.58/58.81 
58.08/60.25 
61.30/63.83 
68.56/69.35 
68.41/68.37  

Table 3 
Parameters and inference time of different models.  

Model 

ResNet-18 
ResNet-50 
VGG-11-bn 
VGG-16-bn 
MobileNet-V3 
RepVGG-A0 
RepVGG-A1 

Params 

11.7 M 
25.5 M 
9.5 M 
15 M 
5.4 M 
8.3 M 
12.8 M 

Speed 

3384 
1114 
875 
693 
3645 
4270 
3277

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific strategy used to monitor the model performance during training. However, it mentions that the Mean Absolute Error (MAE) was calculated to evaluate the performance of various models. Additionally, the Cross-entropy loss function was employed for optimization purposes. These metrics could be monitored throughout the training process to assess the model's progress and adjust hyperparameters if necessary.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Additionally,  we  created  a  metadata  .csv  file  that  listed  all  of  the 
necessary classification information for training the CNN and k-NN . The 
.csv file had five columns. The first, dialect_folder, listed the file path to 
the folder containing the bird song. Each folder was labeled with the 
dialect type and the year it was collected, for example, “ABLA_2022”. 
Note, each folder name contained the label of the songs inside, as this 

EcologicalInformatics82(2024)1026572B. Story et al.

grams, and created the metadata .csv file necessary for training the CNN 
and k-NN. Identical in structure to the metadata file outlined above, this 
file contained the metadata for the training set as opposed to the met-
adata for the entire dataset. 

No additional preprocessing was applied to the spectrograms as the 
goal was to do the least amount of preprocessing possible in order to 
demonstrate  the  power  of  human-machine  teaming.  Further,  when 
assessing the accuracy of the CNN on noise-filtered data versus raw data, 
the accuracy levels were similar: after 10 trials, the CNN’s accuracy on 
the  filtered  data  averaged  to  99.22%  and  the  accuracy  on  the  non- 
filtered data averaged to 99.25%.

Fig. 1. Architecture for the DialectDecoder pipeline. The user inputs the ∼2 s song clips and the location data from a metadata file into the system (Input). Then, 
DialectDecoder prepares the labeled data for processing (Preprocess). Next, the program trains the CNN and k-NN (Train) to classify unknown songs. Then, Dia-
lectDecoder classifies or prompts the user to classify new songs the program has not seen before (Classify). Finally, after human classification has been completed, the 
program adds in the user classified songs to the dataset for retraining (Retrain) and retrains the networks with the new data. 

Fig. 2. Example spectrograms of the 9 different dialects.

Declaration of competing interest 

None. 

Data availability 

Data will be made available on request. 

References 

Alzubaidi, Laith, Zhang, Jun, Humaidi, Abdul Jabbar, et al., 2021. Review of deep 

learning: concepts, cnn architectures, challenges, applications, future directions. 
J. Big Data 8 (1), 53. 

Budalakoti, S., Srivastava, A.N., Otey, M.E., 2009. Anomaly detection and diagnosis 

algorithms for discrete symbol sequences with applications to airline safety. IEEE 
Trans. Syst. Man Cybern. Part C Appl. Rev. 39 (1), 101–113. 

Cohen, Yarden, Nicholson, David Aaron, Sanchioni, Alexa, Mallaber, Emily K., 

Skidanova, Viktoriya, Gardner, Timothy J., jan 2022. Automated annotation of 
birdsong with a neural network that segments spectrograms. eLife 11, e63853. 
Derryberry, Elizabeth P., Danner, Raymond M., Danner, Julie E., Derryberry, Graham E.,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors have stated that the data used in their study will be made available upon request. However, they did not specify any particular data repositories such as Zenodo, Figshare, Dryad, or GBIF where the data can be accessed directly. Therefore, if one wants to access the data, they should contact the corresponding author or relevant research group associated with the paper.
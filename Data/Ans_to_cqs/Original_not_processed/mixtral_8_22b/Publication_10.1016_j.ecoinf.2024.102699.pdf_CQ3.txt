Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

For the ViT models, we adapted the approach of pretraining deep
bidirectional transformers for language understanding (BERT) by Devlin
et al. (2018) for birdsong classification. Table 3 provides an overview of
the investigated ViT models with their respective parameterizations.
ViT-S/16, for example, denotes a ViT variant of reduced complexity with
an input patch size of 16 × 16. The sequence length of the transformer
models is inversely proportional to the square of the provided patch size.
In the following, ViT-B/16 is further investigated as it strikes a balance
between general model complexity and classification performance.

2.3. Data preprocessing

In this contribution, we utilize the 2021 BirdCLEF data set with its
recordings. Since these recordings are weakly labeled, we used
randomly selected chunks with a length of 5 s for the training process.
Some of them also contain non-bird audio events such as wind, thunder,
or aircrafts. To diversify our non-bird audio events, we additionally
deployed related noise from AudioSet, “a large-scale dataset of manually
annotated audio events”7 (Fonseca et al., 2018). However, to also
mitigate the possible side effects of these audio events, we decided to use
the first as well as the last 5 s of each clip after examining them regarding
their audio quality. For this purpose, we extracted clips without bird-
song from the validation soundscape recordings based on the available
metadata. These clips were added as background noise to the training

1.3. Section overview

The remainder of this work is structured as follows. Firstly, we pre-
sent an overview of the fundamental concepts as well as the imple-
mentation of our birdsong augmentation and classification framework
(section 2). This includes a detailed description of the employed
augmentation methods, our data preprocessing steps, the architectural
design of our classification models, and our training configuration.

Fig. 1. Exemplary overview of commonly utilized augmentation methods for data augmentation within learning-based birdsong classification.

EcologicalInformatics82(2024)1026992A.S. Kumar et al.

Table 1
Overview of different audio augmentation techniques. Listed are audio augmentation approaches using, among others, pitch shifting, time stretching, mix up, and
Gaussian noise.

Literature

Pitch
shifting

Time
stretching

Mix
up

Gaussian
noise

Vertical
roll

Horizontal
roll

Blur

Brightness

Masking

Warping

BirdCLEF 2021 (Kahl et al., 2021a)

Table 2
Overview of our augmentation strategies, their IDs, and related information. For DenseNet-161 and ViT-B/16, their training times are provided in minutes per epoch
for the different augmentation methods. For IDs 5 and 6, we added noise of the bird audio detection data set from the DCASE challenge (Berger et al., 2018; Himawan
et al., 2018; Liaqat et al., 2018), which is abbreviated as BAD.

Data augmentation

Time domain

Frequency domain

Spectrogram

Training time [min. / epoch]

DenseNet-161

ViT-B/16

Gaussian noise
Pink noise (background)
Primary background noise soundscapes
Secondary background noise soundscapes
Primary background noise (BAD)
Secondary background noise (BAD)
Background noise (e.g., wind, thunder, or aircrafts)
Mixed up random bird species
Horizontal roll
Vertical roll
Pitch shift
Time mask
Frequency mask
Gain
Loudness normalization
Horizontal flip
Vertical flip
Time stretch
tanh-based distortion

✓
✓
✓
✓
✓
✓
✓

✓

✓
✓

✓

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there is no explicit mention of specific data annotation techniques used in the deep learning pipeline, such as bounding box annotation or instance segmentation. Instead, the focus appears to be on birdsong classification using the 2021 BirdCLEF dataset, where the recordings are weakly labeled. The authors have used randomly selected chunks of 5 seconds duration for the training process, some of which may include non-bird audio events like wind, thunder, or aircraft sounds. They have also incorporated additional noise from the AudioSet dataset to diversify their non-bird audio events. Furthermore, they have examined the audio quality of the clips and extracted those without birdsong from the validation soundscape recordings based on metadata. These clips were then added as background noise to the training data. Therefore, while the context does not provide details about traditional data annotation techniques, it discusses the handling of weak labels and incorporating relevant background noise into the training data for improved birdsong classification.
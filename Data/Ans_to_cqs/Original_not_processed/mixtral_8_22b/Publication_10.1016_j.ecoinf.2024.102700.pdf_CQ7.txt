Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

for model validation, either in addition to internal validation strategies
(n = 22), or as the sole validation method (n = 17). Notably, a consid-
erable number of studies did not use an explicit validation method to
measure model performance or did not clearly state this in the text (n =
94, 32%) (category “not provided” in Fig. 10).

Various strategies were employed for the internal/external valida-
tion of the model with a predominant use of internal strategies (n = 174,
59%). The most common internal strategy was “cross-validation” (CV),
utilised in 84% of cases (n = 147). Among the CV methods, “k-fold” was
most frequently used (n = 82, 56%), followed by “bootstrap” (n = 34,
23%), “leave-one-out CV” (n = 19, 15%), “repeated split sampling” (n =
5, 3%) or “not stated/other CV” (including stratified monte carlo CV and
spatial block CV, n = 13, 9%). Internal validation was also performed
using “simple sample splitting” (n = 16, 5%) or “generalised CV” (n =
12, 4%), which does not require data partitioning into training and test
datasets. Instead, it utilises a mathematical formula that considers the
trajectory of fitting errors while varying the model's hyperparameters
(Hastie et al., 2009). Regarding external validation strategies, only a
minority of studies (n = 39, 13%) employed a fully independent dataset

2.3. Screening process

Screening was performed at three stages for efficiency: title, abstract
and full text. The inclusion and exclusion criteria were aligned with
those listed and descripted in Table 1, ensuring consistency throughout
the screening process. To include abstracts and full text, all the criteria
were applied and the reason for exclusion was reported. To check for
inter-rater reliability in the inclusion criteria, two reviewers indepen-
dently screened a subset (10%) of titles and abstracts for relevance
against predetermined criteria before the screening process began. The

Fig. 1. Flow Diagram resuming the searching, screening and synthesis process performed systematically according to criteria listed in Table 1.

EcologicalInformatics82(2024)1027003E. Pasanisi et al.

Cohen's Kappa test measured a perfect agreement (k = 1, n = 100%) for
both title and abstract screening. The Fig. 1 illustrates the flow diagram
of the screening process.

Fig. 9. Modelling approaches used in the examined articles (count) over the years (A): RB = Regression-based, ML = Machine-learning, DIST-ENV = distance based/
envelope, PPM = Point Process Models, ENS = Ensemble approach, OTH=Other approaches. Specific models implemented in reviewed literature (B): GAM =
Generalised Additive models; GLM = Generalised linear models; MARS = Multivariate Adaptive Regression Splines; MAXENT = Maximum Entropy model; RF =
Random Forest; BRT = Boosted Regression Trees; GBM = Generalised Boosted Models; ANN = Artificial Neural Networks; CART = Classification and Regression
Trees; FDA = Flexible Discriminant Analysis; SVM = Support Vector Machine; LDA = Linear Discriminant Analysis; BNM = Bipartite Network modelling; QDA =
Quadratic Discriminant Analysis; ENFA = Ecological Niche Factor Analysis; BIOCLIM = BIOCLIM; LGCP-INLA = Log-gaussian Cox Process via Integrated Nested

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The context provided does not explicitly mention the specific criteria used to split the data for deep learning model training. However, it discusses various strategies employed for internal and external validation of models. For internal validation, cross-validation is the most commonly used strategy, specifically k-fold cross-validation. This technique involves dividing the dataset into 'k' subsets or folds, where each fold serves as the testing set once, while the remaining 'k-1' folds form the training set. Other internal validation techniques mentioned include bootstrap, leave-one-out cross-validation, repeated split sampling, simple sample splitting, and generalized cross-validation. External validation strategies involve using a fully independent dataset, but this was only done in a minority of studies. Therefore, while the exact criteria for splitting the data for deep learning model training are not specified, these validation strategies suggest that datasets are typically divided into training and testing sets, often using cross-validation techniques like k-fold cross-validation.
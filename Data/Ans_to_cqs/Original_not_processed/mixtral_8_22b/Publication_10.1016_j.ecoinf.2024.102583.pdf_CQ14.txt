Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

loss function, effectively preventing overfitting and promoting simpler 
models,  utilization  of  a  more  advanced  optimization  approach  that 
combines  first-order  gradients  (loss  function  gradient)  with  second- 
order  gradients  (loss  function  curvature)  which  makes  it  faster  than 
some other models, ability to handle missing data during tree building 
by  employing  weighted  quantile  sketch,  built-in  cross-validation  sup-
port  that  aids  the  model  evaluation  and  hyperparameter  tuning,  and 
imbalanced  data  handling.  Thanks  to  its  exceptional  efficiency  and 
performance and its availability in various programming languages like 
Python, R, and Java, XGBoost has found widespread adoption in land-
slide risk assessment (Akinci et al., 2021; Badola et al., 2023; Can et al., 
2021; Hussain et al., 2022b). 

{(cid:0)

Given  a 
)}(cid:0)
xi, yi

set  with  n 
|D|= n, xi ∈ Rm, yi ∈ R

Recent  research  has  demonstrated  the  effectiveness  of  leveraging 
single-algorithm ensemble methods to improve the predictability of ML- 
based  LSM.  Notable  algorithms  in  this  category  include  the  Random 
Forest  (RF)  model  as  demonstrated  by  studies  such  as  (Achour  and 
Pourghasemi,  2020;  Adnan  et  al.,  2020;  Akinci  et  al.,  2021;  Kalantar 
et al., 2020; Youssef and Pourghasemi, 2021). Additionally, the XGBoost 
model  as  indicated  by  (Badola  et  al.,  2023)  and  (Can  et  al.,  2021), 
LightGBM (Ye et al., 2022), and AdaBoost (Gupta and Shukla, 2023; Nhu 
et al., 2020) have been recognized for their ability to improve the ac-
curacy  of  ML-based  LSM  (Adnan  et  al.,  2020).  In  addition  to  single- 
algorithm  ensembles,  mixed-based  algorithm  ensembles,  which 
combine different models, have also shown promise in improving LSM 
performance. These ensemble techniques, such as stacking (Fang et al.,

Sørensen, R., Zinko, U., Seibert, J., 2006. On the calculation of the topographic wetness 
index: evaluation of different methods based on field observations. Hydrol. Earth 
Syst. Sci. 10 (1), 101–112. https://doi.org/10.5194/hess-10-101-2006. 
Sun, D., Wen, H., Wang, D., Xu, J., 2020. A random forest model of landslide 
susceptibility mapping based on hyperparameter optimization using Bayes 
algorithm. Geomorphology 362. https://doi.org/10.1016/j.geomorph.2020.107201. 
Süzen, M.L., Doyuran, V., 2004. A comparison of the GIS based landslide susceptibility 
assessment methods: multivariate versus bivariate. Environ. Geol. 45 (5), 665–679. 
https://doi.org/10.1007/s00254-003-0917-8. 

Taalab, K., Cheng, T., Zhang, Y., 2018. Mapping landslide susceptibility and types using 

random Forest. Big Earth Data 2 (2), 159–178. https://doi.org/10.1080/ 
20964471.2018.1472392.

)

(cid:0)

Ω

fj

= τT +

1
2

λ||w||2

(9)  

2.2.6. Heterogenous ensemble techniques 

Heterogeneous ensemble learning is the practice of combining pre-
dictions of multiple models that are different in terms of their underlying 
algorithms  or  architectures  to  improve  overall  performance.  Utilizing 
heterogeneous ensemble techniques can address biases and improve the 
implemented  two 
effectiveness  of  models. 

In  our  study,  we 

EcologicalInformatics81(2024)10258312G. Agboola et al.                                                                                                                                                                                                                                

Fig. 11. The proportion of area covered by each susceptibility category in each scenario.  

heterogeneous  techniques  to  optimize  the  performance  of  five  base 
learners and mitigate potential overfitting:

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The context does not provide specific information about how the hyperparameters of the model are optimized. However, one of the cited papers, Sun et al. (2020), mentions using a Bayesian algorithm for hyperparameter optimization in a random forest model. This suggests that Bayesian optimization could be used for hyperparameter tuning in machine learning models. Other common methods for hyperparameter optimization include grid search and random search. Grid search involves testing all possible combinations of hyperparameters within a specified range, while random search randomly selects combinations from the same range. Both methods aim to find the optimal combination of hyperparameters that minimizes the loss function and improves the accuracy of the model. Without further context, it cannot be determined which method was used specifically for the XGBoost model mentioned in the given text.
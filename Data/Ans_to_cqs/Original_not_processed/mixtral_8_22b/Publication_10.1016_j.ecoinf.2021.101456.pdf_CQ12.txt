Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

dataset  recorded  at  a  different  location  and  was  gradually  expanded 
until  reconstructions  were  sufficiently  good.  In  order  to  visualize  the 
latent  representation,  the  size  of  the  bottleneck  of  the  model  (latent 
dimension) was limited to two. Details of the network architecture are 
reported in Table 1.

Given unlabelled input data consisting of N samples with F features, 
x âˆˆ â„NÃ—F,  the  probabilistic  encoder  of  a  VAE  maps  the  input  to  the 
posterior density p(z|x) over the latent variable, z âˆˆ â„NÃ—L. In practice, 
L << N and the encoder neural network approximates the true posterior 
density, p(z|x), with a multivariate Gaussian, qÎ¸(z|x) âˆ¼ ğ’© (Î¼Î¸, Ïƒ2
Î¸ ). The 
decoder of a VAE reconstructs the input data from the latent variable 
and is given by the density function pÏ†(x|z). The encoder and decoder 
neural networks are parameterised by Î¸  and Ï†, respectively. The opti-
mization objective of a VAE consists of two competing terms and it can 
be shown to be (Kingma and Welling, 2014)  

â„’VAE = (cid:0) EqÎ¸ [logpÏ†(x|z)] + KL[qÎ¸(z|x)||p(z)]
â„’VAEâ‰œâ„’rec + â„’reg

(1) 

(2)

All models were implemented in PyTorch (Paszke et al., 2019) and 
trained for 5000 epochs using the Adam optimizer (Kingma et al., 2015) 
(cid:0) 3. The models were trained on Nvidia GTX 
with a learning rate of 10
1050 graphics processing unit with 4 GB memory with a batch size of 
256. A decision to adapt Î² was taken every fifth epoch to avoid random 
fluctuations. The scaling of the clustering loss, Î³, in the semi-supervised 
mode was cycled between 0.01 and 0.2 every 100 epochs. 

4.4. Results 

The clustering performance on the labelled test set for the unsuper-
vised and semi-supervised instances of the dynamic Î²-VAE is presented 
in Table 2. 

The dynamic Î²-VAE performs better than the baselines in the ARI- 
and AMI-scores which quantifies the intra-class cohesion and inter-class 
separability. While HCA have been successfully used to identify groups

The  dynamic  Î²-VAE  was  evaluated  in  unsupervised  and  semi- 
supervised  modes  to  obtain  latent  representations,  which  were  clus-
tered using K-means (Lloyd, 1982). Their clustering performance was 
compared with the baseline methods: PCA, Kernel-PCA, HCA using the 
standard  implementations  in  sklearn  (Pedregosa  et  al.,  2011)  and  a 
conventional VAE on the same data. The encoder neural network qÎ¸(z|x) 
consists  of  9  fully  connected  layers,  with  rectified  linear  unit  (ReLU) 
activation (except for the last layer). The encoder predicts the mean and 
the  variance  of  the  approximate  posterior  distribution.  The  decoder 
neural network pÏ†(x|z) is implemented with 10 fully connected layers 
and ReLU activation (except the last layer, which has sigmoid activa-
tion). The VAE uses a bottleneck L = 2 to create the latent representa-
tion.  The  model  layout  was  developed  on  a  independent  unlabelled

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not explicitly mention the specific type of deep learning model used in the pipeline, such as Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Transformers. However, it does provide information about the use of Variational Autoencoders (VAEs) in the process.

Variational Autoencoder (VAE) is a generative model that combines deep learning techniques with Bayesian inference. It consists of an encoder and a decoder, both typically composed of multiple layers of artificial neurons. The encoder transforms the input data into a lower-dimensional latent space, while the decoder attempts to reconstruct the original input from this latent representation.

In this case, the VAE is employed to map the input data to a posterior density over the latent variables. The encoder neural network approximates the true posterior density with a multivariate Gaussian, while the decoder reconstructs the input data from the latent variable. The optimization objective of the VAE includes two competing terms: the reconstruction loss and the regularization term.

Although the text does not specify whether the encoder and decoder components of the VAE utilize convolutional, recurrent, or other types of layers, it does mention that the encoder consists of nine fully connected layers, and the decoder comprises ten fully connected layers. This suggests that the VAE might employ feedforward neural networks rather than convolutional or recurrent architectures. Nonetheless, without explicit confirmation, we cannot definitively state the exact type of deep learning model used in the pipeline.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

6.2. Benchmarking setup 

There are two ways in which the metric can be applied to test the 
system. Firstly, as a comparison with existing camera traps. Secondly, to 
evaluate the pipeline with respect to the underlying neural network. As 
there is no dataset available that contains full videos with corresponding 
PIR sensor information, such data would need to be manually produced. 
This was done at a small scale to inform development, but it was not 
feasible  to  scale  this  up  for  a  robust  test  set.  We  instead  focus  on 
benchmarking different system configurations compared to existing ML 
approaches. 

6.2.1. Data

The official, MS-COCO pre-trained, version of this architecture has 
then  been  further  trained,  a  technique  known  as  transfer-learning, 
using the WCS dataset and its recommended data split, giving a cred-
ible mean average precision mAP50 score of 0.75. The mAP metric, used 
to  evaluate  the  PASCAL  VOC  challenge  dataset  (Everingham  et  al., 
2010),  takes  the  mean  over  all  considered  species  of  their  respective, 
species-specific  average  precision  as  defined  below.  The  subscript  50 
refers  to  the  requirement  that  the  area  of  the  intersection  of  ground 
truth  bounding  box  and  predicted  bounding  box  needs  to  be  at  least 
50%  of  the  area  of  the  union  of  both  for  an  otherwise  true  species 
identification to be considered correct. The network does not output a 
binary  value  for  species  identification  but  instead  a  belief  b  ∈ [0, 1] 
that  a  particular  species  has  been  identified.  Thresholding  these

(

α
S

+

1 (cid:0) α
R

Eα =

∈ (0, 1]

(3)  

as the weighted harmonic mean between specificity S and recall R with 
α ∈ [0, 1] and S, R ∈ (0, 1]. Eα  reduces to R for α = 0 and to S for α = 1 
while E0.5 considers specificity and recall with equal weight. In the case 
when S or R are zero, the definition of Eα  should be extended to yield 
zero.  As  with most  performance metrics,  higher  means  better.  To the 
best  of  our  knowledge  this  measure  is  new  and  representative  of  the 
ecology use cases in our application. 

Another  measure  that  is  sometimes  used  for  classification  is  the 
Matthews Correlation Coefficient (MCC), which is the geometric mean 
between informedness (defined as S + R (cid:0) 1) and markedness (Powers 
and Ailab, 2011). Although the informedness aspect captures some of 
our objectives, MCC is a single number that does not allow us to mediate 
between our two use cases. 

6.2. Benchmarking setup

7.1.4. Low power neural networks in hardware 

It is desirable to reduce the power consumption of the DynAIkonTrap 
to make deployment in remote locations easier and less expensive. An 
implementation by Si et al. (2020) of a Multilayer Perceptron model on 
an Field Programmable Gate Array (FPGA) was shown to run in a similar 
time frame to a software-based approach, but at a clock speed lower by a 
factor of 144. The authors portray this as the ability to achieve a lower 
power consumption or improve performance by a factor of 144. Results 
published by Intel (2016) indicate a Convolutional Neural Network can 
be  run  on  an  FPGA  with  twice  the  efficiency  of  a  software/CPU 
implementation.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The text mentions several metrics used to evaluate the performance of the deep learning model. One of them is the Mean Average Precision (mAP) metric, which is used to evaluate the PASCAL VOC challenge dataset. This metric calculates the mean over all considered species of their respective, species-specific average precision. The subscript 50 refers to the requirement that the area of the intersection of the ground truth bounding box and the predicted bounding box needs to be at least 50% of the area of the union of both for an otherwise true species identification to be considered correct.

Additionally, the text introduces another measure called Eα, which is described as the weighted harmonic mean between specificity and recall with α ∈ [0, 1] and S, R ∈ (0, 1]. Eα reduces to R for α = 0 and to S for α = 1 while E0.5 considers specificity and recall with equal weight. It is mentioned that this measure is new and representative of the ecology use cases in the application.

Furthermore, the Matthews Correlation Coefficient (MCC) is also discussed as a measure for classification. However, it is noted that although the informedness aspect captures some of the objectives, MCC is a single number that does not allow mediating between the two use cases.
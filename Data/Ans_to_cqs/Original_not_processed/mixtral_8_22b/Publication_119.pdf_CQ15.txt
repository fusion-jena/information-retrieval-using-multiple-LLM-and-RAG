Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

a batch normalization layer after hidden layer 2 increased the testing accuracy of the model. These additional layers—dropout and batch normalization—reduce the over-fitting of the model on the training data and increase generalization on testing data. The hyperparameters in this network—the learning rate, number of epochs, and batch size—were further tuned such that the testing accuracy and the kappa were the best among all models. The optimized hyperparameter values for learning rate and batch size were 0.007 and 48, respectively.  Figure 4. Network architecture implemented for the Deep Neural Network (DNN) model along with the number of neurons that were optimized for each hidden layer. The output layer contains 11 neurons, corresponding to the number of classes to be classified. 2.7. Accuracy Assessment To compare the accuracy of various models independent testing data, i.e., same for all models, is used for model evaluation. The confusion/error matrix and subsequent met-rics are

2.6. Deep Neural Network

A Deep Neural Network (DNN) consists of multiple hidden layers, made up of ‘n’
number of neurons. These neurons are interconnected with neurons of the preceding and
next layer, through some weight (m) and bias (c), such that,

y = mx + c

(1)

The network attempts to learn the values for the various weights and biases, the
parameters of the model, by minimizing the cost function. The choice of the cost function
is important, as this function guides the model in the direction of the correct weights and
biases for accurate predictions. The cross-entropy loss function is used for the DNN model.
The number of neurons and the number of hidden layers all affect the predicting
power of the network. Thus, these parameters were adjusted, resulting in the architecture
seen in Figure 4. The presence of a dropout layer, with a value of 0.23, after hidden

Remote Sens. 2021, 13, 3495

8 of 29

Remote Sens. 2021, 13, x FOR PEER REVIEW 8 of 30   2.6. Deep Neural Network A Deep Neural Network (DNN) consists of multiple hidden layers, made up of ‘n’ number of neurons. These neurons are interconnected with neurons of the preceding and next layer, through some weight (m) and bias (c), such that,  y = mx + c (1) The network attempts to learn the values for the various weights and biases, the pa-rameters of the model, by minimizing the cost function. The choice of the cost function is important, as this function guides the model in the direction of the correct weights and biases for accurate predictions. The cross-entropy loss function is used for the DNN model. The number of neurons and the number of hidden layers all affect the predicting power of the network. Thus, these parameters were adjusted, resulting in the architecture seen in Figure 4. The presence of a dropout layer, with a value of 0.23, after hidden layer 5 and a batch normalization layer after hidden layer 2

the U-Net model is the labeling of each pixel in the training data. Abundant data are needed
for training deep neural networks of at least 1000 image samples. In such cases, a pretrained
network can reduce the amount of training data needed [47]. The U-Net implemented
within arcgis.learn had a ResNet-50 backbone that was pretrained on ImageNet.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, it appears that the authors have utilized the cross-entropy loss function for their Deep Neural Network (DNN) model. However, there is no explicit mention of specific optimization techniques like Stochastic Gradient Descent (SGD) or Adam being employed in the deep learning pipeline. Nevertheless, it is common practice to use optimization algorithms alongside the chosen loss function to minimize the error during the training process. Therefore, while not explicitly stated, it is likely that one of these optimization techniques has been used.

In addition to the optimization techniques, the authors also discuss other methods they have used to improve the performance of their model. For instance, they added a dropout layer with a value of 0.23 after hidden layer 5 and a batch normalization layer after hidden layer 2. Both of these techniques help prevent overfitting and improve the overall performance of the model. Furthermore, the authors fine-tuned several hyperparameters, including the learning rate and batch size, to achieve optimal results. They found that the optimized hyperparameter values for learning rate and batch size were 0.007 and 48, respectively.

Overall, while the exact optimization technique used in the deep learning pipeline remains unspecified, the authors have clearly taken steps to enhance the performance of their DNN model using various optimization strategies and techniques.
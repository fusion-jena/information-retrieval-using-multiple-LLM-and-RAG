Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Short-billed (common/mew) Gull Larus canus 
Geographic information system (GIS) 
Open access data 
Big data 
Socio-economics 
Multi-year field work 
Machine learning ensemble predictions 
(RandomForest Treenet CART MARS)

MARHH_NO_C 

MHH_CHILD 
FHH_CHILD 

FAMILIES 
AVE_FAM_SZ 
HSE_UNITS 
URBAN 
RURAL 
VACANT 
OWNER_OCC 
RENTER_OCC 
POP_MILE 
HH_INCOME_ 
HH_10K 
HH10TO15K 
HH15TO20K 
HH20TO25K 
HH25TO30K 
HH30TO35K 
HH35TO40K 
HH40TO45K 
HH45TO50K 
HH50TO60K 
HH60TO75K 
HH75TO100K 
HH100_125K 
HH125_150K 
HH150_200K 
HH_200K 
HHMEDIAN 
PER_CAPITA 
Polygon area  

x-coordinate  
y-coordinate  

Total number of people in the census block in year 
2000 
Total number of people who are white alone 

Total number of people who are black alone 

Total number of people who are Native American 
alone 
Total number of people who are Asian alone 

Total number of people who are Hawaian Pacific 
Island alone 
Total number of people who are ‘other’ alone 

Number of Multirace Individuals in census unit 

Total number of people who are Hispanic alone

2.3.4. RandomForest (bagging) 

We performed a RandomForest analysis using 200 trees and balanced 
weights. Similar to CART Ensembles and Bagger, RandomForest belongs 
to the tree algorithm family, but then is enhanced via ‘bagging’ (Brei-
man, 2001). Prior studies have shown RandomForest to be a superior 
choice  for  predictive  accuracy  in  classification  problems  (Mi  et  al., 
2017) and inference (Breiman, 2001). 

2.3.5. MARS regression splines

2.3. Machine learning ensembles 

We  used  the  combined  2014–2015  Short-billed  Gull  presence/ 
absence data cube to train an ensemble of machine learning (ML) models 
in Salford Predictive Modeler (SPM) v8.3 (Formula 1). We used several 
established and leading ML algorithms (Hegel et al., 2010; Fern´andez- 
Delgado et al., 2014 for overview) and combined them in an averaged 
ensemble to obtain the best-possible prediction and overall synergy from 
each algorithm best as possible (sensu J. Friedman ‘many weak learners 
make for a strong learner’; Friedman, 2002, Hastie et al., 2009. See also 
Hardy et al., 2011, Fox et al., 2017 and Boulanger-Lapointe et al., 2022 
for applications). That approach will overcome stand-alone approaches 
such  as  linear  regression,  which  do  not  offer  unique  solutions,  e.g. 
Ascombe’s Quartet, and have difficulty to meet all required parametric 
assumptions (McArdle, 1988). The specific algorithms and their settings 
are described below.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about the use of regularization methods such as dropout or L2 regularization in the deep learning pipeline. However, it mentions the use of machine learning ensembles and various algorithms like RandomForest, CART, MARS, etc. These techniques help improve model performance and reduce overfitting by combining multiple models or decision trees. While these methods may not directly correspond to traditional regularization techniques, they still contribute to preventing overfitting in the modeling process.
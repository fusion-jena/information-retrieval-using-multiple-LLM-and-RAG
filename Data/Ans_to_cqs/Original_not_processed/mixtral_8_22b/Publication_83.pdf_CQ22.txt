Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

TABLE 3. Classification performance of different methods using single
CNN-based model. Here, Mel-CNN, Harm-CNN, and Perc-CNN denote that
the input to those CNNs are Mel-spectrogram, harmonic-component
based spectrogram, and percussive-component based spectrogram.
Subnet-CNN denotes that a SubSpectralNet architecture is used with the
Mel-spectrogram as the input.

trogram. In addition to different TFRs, a different deep learning architecture, SubSpectralNet, is employed
to classify bird sounds. Experimental results on classifying 43 bird species show that fusing selected deep
learning models can effectively increase the classiﬁcation performance. Our best fused model can achieve a
balanced accuracy of 86.31% and a weighted F1-score of 93.31%.

FIGURE 6. Confusion matrix (%) of the best result using the fusion of selected CNN-based models. Here, the x and y axes denote the code of each bird
species to be classified.

(95.5%) can achieve a signiﬁcantly higher classiﬁcation
accuracy than 40 mel-bin(91.2%), but with more computa-
tional load. In addition, since multiple CNN-based models
need to be trained in this study, such a high computational
load will signiﬁcantly increase the training time. Further-
more, this study aims to investigate the effectiveness of fusion
of different CNN-based models rather than obtaining a single
CNN-based model with the best performance, Therefore,
we select 40 mel-bin for the subsequent analysis.

To further improve the classiﬁcation performance, we not
only fuse CNN-based models with different input features,
but also fuse another CNN-based model with a different
architecture (see Table 2). The best classiﬁcation F1-score
can be up to 93.31%. Different from the VGG style network,
the SubSpectralNet can be regarded as a ﬁlter with different

VOLUME 7, 2019

175359

J. Xie et al.: Investigation of Different CNN-Based Models for Improved Bird Sound Classification

[5] M. Dorfer, B. Lehner, H. Eghbal-Zadeh, H. Christop, P. Fabian, and
W. Gerhard, ‘‘Acoustic scene classiﬁcation with fully convolutional
in Proc. DCASE Challenge,
neural
Sep. 2018. [Online]. Available: http://dcase.community/challenge2018/task-
acoustic-scene-classiﬁcation-results-a

I-vectors,’’

networks

and

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper uses balanced accuracy and weighted F1-score as evaluation metrics for their deep learning models. Balanced accuracy is calculated by taking the average of true positive rate and true negative rate, which helps to account for imbalance in data classes. Weighted F1-score is the harmonic mean of precision and recall, where weights are assigned according to the number of instances in each class. This metric takes into consideration both false positives and false negatives, providing a comprehensive measure of classification performance. Additionally, the confusion matrix is presented to visualize the performance of the best fused model.
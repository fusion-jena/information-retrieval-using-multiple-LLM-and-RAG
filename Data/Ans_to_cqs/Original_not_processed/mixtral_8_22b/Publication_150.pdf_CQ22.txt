Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Macro 
precision

0.8831

0.8881

0.8740

0.8250

Table 1.   Comparison of model size, speed, and performance, ordered by top-1 accuracy. Text in bold indicates 
the best value in each category. Wide-ResNet101 has the highest top-1 accuracy and macro precision but 
was substantially slower than the other models. MnasNet-A1 was the fastest model but had relatively poor 
performance. InceptionV3 was relatively fast while maintaining good model performance with the highest 
precision and second highest accuracy and recall.

Table 2.   InceptionV3 confusion matrix showing the number of test images for each prediction.

Scientific Reports |         (2021) 11:7580  | 

https://doi.org/10.1038/s41598-021-87210-1

4

Vol:.(1234567890)www.nature.com/scientificreports/Model

#Params 
(million)

Model speed 
(ms)

Top-1 accuracy Top-2 accuracy Top-3 accuracy Top-4 accuracy Top-5 accuracy Macro recall

Wide-ResNet101 124.9

InceptionV3

ResNet101

MnasNet-A1

24.0

42.6

1.0

5.46

3.34

3.33

3.28

0.9171

0.9162

0.9133

0.8579

0.9627

0.9610

0.9633

0.9335

0.9782

0.9767

0.9787

0.9609

0.9850

0.9834

0.9852

0.9730

0.9897

0.9882

0.9892

0.9814

0.8552

0.8519

0.8499

0.7689

Macro 
precision

0.8831

0.8881

0.8740

0.8250

Of the four models tested, InceptionV3 presents a good balance between performance and speed. InceptionV3 
had the highest precision and nearly matched Wide-ResNet’s accuracy and recall while being 39% (2.1 ms) faster 
(Table 1). Likewise, InceptionV3 was 5.8 percentage points more accurate than MnasNet while only 0.06 ms 
slower. The speed and accuracy of InceptionV3 makes it useful for web-based and mobile applications that rely 
on both speed and reliable predictions. We therefore focus on our InceptionV3 results for the remainder of this 
paper.

The InceptionV3 model nicely separated species into distinct groupings based on traits extracted from the 
model (Fig. 2). There was little overlap in two-dimensional trait space, which corresponds with the high degree 
of accuracy in the classification results.

by class sample size. We used the SGD (stochastic gradient descent) optimizer with an initial learning rate of 
0.01 for all models except MansNet, which was initially set at 0.1. The learning rate was reduced by a factor of 
10 after steps of 30 epochs. We used batch normalization and models were trained for 150 epochs using Nvidia 
Tesla K80 or P100 GPUs.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The performance of the deep learning models is evaluated using several metrics such as accuracy, precision, recall, and macro precision. Accuracy measures the proportion of correct predictions out of total predictions made. Precision refers to the ratio of correctly predicted positive observations to the total predicted positives. Recall, also known as sensitivity, measures the proportion of actual positives that were identified correctly. Macro precision is the unweighted mean of precisions across all classes.

In the given context, four different models - Wide-ResNet101, InceptionV3, ResNet101, and MnasNet-A1 - have been compared based on their performance and speed. The comparison includes various metrics like model size, speed, top-1 accuracy, top-2 accuracy, top-3 accuracy, top-4 accuracy, top-5 accuracy, macro recall, and macro precision. Among these models, InceptionV3 offers a good balance between performance and speed, having the highest precision and closely matching Wide-ResNet's accuracy and recall while being significantly faster.
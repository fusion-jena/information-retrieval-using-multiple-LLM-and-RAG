Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.5.3. Artificial neural network 

ANNs are used for modelling complex phenomena and developing 
nonlinear regression (Haykin and Lippmann, 1994). It consists of three 
different  layers:  input,  hidden,  and  output  (Ingram  et  al., 2005).  The 
ANN uses calibration parameters, such as the learning rate and weight, 
to improve the results. The weights were randomly initialized using the 
Nguyen–Widrow method (Zhou et al., 2016). An ANN fitted with CARET 
is a single hidden-layer neural network with two hyperparameters: size 
and decay. The number of units in the hidden layer (size) and regular-
isation parameter to avoid overfitting (decay) were passed as arrays of 
1–20  and  0.025–1,  respectively.  The  Transfer/Activation  function  for 
the  model was  selected  as  the  logistic  function.  The  optimum  combi-
nation of parameters for the best model fit was based on accuracy. 

3.6. Model fitting and evaluation

Fig. 5. Model predicted vs observed AGB (a) Random Forest, (b) Artificial Neural Network, (c) Support vector machine of Sentinel-1 and Sentinel-2.

The  model  initially  included  all  variables,  acknowledging  the  sig-
nificance  of  identifying  and  selecting  pertinent  variables  for  model 
development. Variable importance measures were employed to evaluate 
their significance, which helped identify the most influential variables 
for  predicting  AGB.  However,  it  is  recognised  that  assessing  model 
performance is essential, particularly when dealing with less important 
variables. A stratified sampling method was applied to all AGB obser-
vations for this evaluation. This technique randomly allocated 70% of 
the samples to the training dataset and the remaining 30% to the vali-
dation  dataset.  Three  error  statistics,  specifically  R2,  mean  absolute 
error  (MAE),  and  RMSE  (as  indicated  in  Eqs.  8,  9,  and  10),  were 
employed to compare the accuracies of the models. The generated ma-
chine learning models were then extrapolated for the entire region. This

overfitting issue, we used the parameter (c) to optimise the line fit and 
penalise the number of samples inside the margin. The value of c plays a

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there isn't any explicit mention of specific optimization techniques like Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being used in the deep learning pipeline. However, the text does indicate that the learning rate and weight are used as calibration parameters to improve the results in the artificial neural network (ANN) model. Additionally, the Nguyen-Widrow method is mentioned for initializing the weights randomly. Furthermore, the regularization parameter 'decay' is used to prevent overfitting. In the case of support vector machines (SVM), the parameter 'c' is utilized to optimize the line fit and penalize the number of samples within the margin. Therefore, while the exact optimization techniques aren't specified, the use of learning rates, weight initialization methods, and regularization parameters suggest that some form of optimization is indeed taking place in the deep learning pipeline.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

58. Yu, L.C.; Sung, W.K. Understanding geometry of encoder-decoder CNNs. arXiv 2019, arXiv:1901.07647v2.
Ioﬀe, S.; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate
59.
shift. arXiv 2015, arXiv:1502.03167v3.
Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. Dropout: A simple way to prevent
neural networks from overﬁtting. J. Mach. Learn. Res. 2014, 15, 1929–1958.

60.

61. Evaluation of the CNN Design Choices Performance on ImageNet-2012. Available online: https://github.

com/ducha-aiki/caﬀenet-benchmark (accessed on 24 March 2020).

62. Abadi, M.; Agarwal, A.; Barham, P.; Brevdo, E.; Chen, Z.; Citro, C.; Corrado, G.S.; Davis, A.; Dean, J.;
Devin, M.; et al. TensorFlow: Large-scale machine learning on heterogeneous systems. arXiv 2016,
arXiv:1603.04467v2.

Remote Sens. 2020, 12, 1145

6 of 17

Training data were generated as batches of size (m, 256, 256, 3), where m is the batch size (m =
20 in our experiments). The batches consisted of sub-images of size 256 × 256 that were randomly
cropped out from the original satellite images presented in Table 1. We had a stream (internally, a
Python generator) of almost never repeated sub-images; these sub-images were combined into batches
and used for the neural network training. Satellite images for sites #1, #3, #5, #7–10 were used for
training and #2, #4, #6 for validation. Corresponding batches of mask data had shape (20, 256, 256, 1).
The network training assessment was performed on sub-images generated from image #2 (Table 1).
Images #4 and #6 were used for visualization and demonstration of the algorithm eﬃcacy.

arXiv:1704.02386.

34. LeCun, Y.; Bengio, Y.; Hinton, G. Deep learning. Nature 2015, 521, 436–444. [CrossRef]
35. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classiﬁcation with deep convolutional neural networks.

36.

Commun. ACM 2017, 60, 84–90. [CrossRef]
Shrestha, A.; Mahmood, A. Review of DL algorithms and architectures. IEEE Access 2019, 7, 53040–53065.
[CrossRef]

37. Brodrick, P.G.; Davies, A.B.; Asner, G.P. Uncovering ecological patterns with convolutional neural networks.

Trends Ecol. Evol. 2019, 34, 734–745. [CrossRef]

38. Kattenborn, T.; Eichel, J.; Fassnacht, F.E. Convolutional neural networks enable eﬃcient, accurate and
ﬁne-grained segmentation of plant species and communities from high-resolution UAV imagery. Sci. Rep.
2019, 9, 17656. [CrossRef]

39. Hamdi, Z.M.; Brandmeier, M.; Straub, C. Forest damage assessment using deep learning on high resolution

remote sensing data. Remote Sens. 2019, 11, 1976. [CrossRef]

study states that good results for image classiﬁcation problems using CNN take place when the
batch normalization is applied after a 2D-convolutional layer. Since there are no reasons to put batch
normalizations right after the dropout layer (it doesn’t transform inputs and introduce a bias), we
placed it after each convolutional layer. There are still various possible extensions to the convolutional
block. The provided architecture (Algorithm A1, CONV_BLOCK) is the closest to the original U-Net
solution that incorporates both the batch normalization and residual connection. Another important
parameter in our CNN is layer rate. This parameter deﬁnes how the number of layers will change
depending on the depth of the neural network. Its default value is 2. This means that the number of
layers is multiplied by two each time we dive one level deeper through the U-Net architecture (for
classic U-Net, we have 64, 128, 256, etc., as the number of layers). Therefore, we can not only tune the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, the data format used in the deep learning pipeline is images. Specifically, the training data were generated as batches of size (m, 256, 256, 3), which indicates that the input data consists of three channels, likely representing red, green, and blue color channels. Additionally, the context mentions that the batches consisted of sub-images of size 256 x 256 that were randomly cropped out from the original satellite images. Furthermore, the corresponding batches of mask data had shape (20, 256, 256, 1), indicating that the output data also consists of single-channel images. Therefore, the deep learning pipeline uses image data as its input and output format.
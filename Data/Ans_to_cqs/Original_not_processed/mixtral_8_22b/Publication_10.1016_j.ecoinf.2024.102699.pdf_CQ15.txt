Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

ViT models are often computationally costly compared to conven-
tional SED models. However, our 5 s long log-mel spectrograms are more
feasible to process. The influences of class imbalances were alleviated by
utilizing the binary cross entropy based focal loss as well as the so-called
scaled focal loss (Arunodhayan Sampathkumar, 2021). Our training
setup includes the Adam optimizer (Kingma and Ba, 2015) with a
concatenating cosine-annealing linear scheduler with an initial learning
rate of 0.0001, decaying by a factor of 0.001⋅learning rate, and a batch
size of 32. For validation, we used a 5-fold cross validation. Given the
class imbalance, we additionally deployed a data set sampler by
upsampling and duplicating randomly selected samples and augmenting
them. Our models were trained for 50 epochs without mixing up aug-
mentations as well as for 100 epochs with mixing up augmentations.
Early stopping was introduced to prevent overfitting when no further

The evaluated augmentation techniques consistently exhibited the
same qualitative effects across all three of our deep learning models.
Specifically, if the performance of an augmentation set improved for one
model, it also improved for the other two, and vice versa. However,
further research is needed to determine if this observation can be
generalized. For optimizing augmentation strategies, a more effective
approach would be to use only the least computationally intensive
models, e.g., DenseNet-161 or ResNet-50. The best combined augmen-
tations would then be applied to the best-performing model (ViT-B/16).
In our future work, we plan on focusing on the creation of so-called
no-call classifiers with training samples that do not contain bird events
in general. For this purpose, as well as to explore different transformer-
based models with improved prediction scores, encompassing, i.a., data-
efficient image transformers (DeiT) as well as hybrid transformers (Han

Table 2
Overview of our augmentation strategies, their IDs, and related information. For DenseNet-161 and ViT-B/16, their training times are provided in minutes per epoch
for the different augmentation methods. For IDs 5 and 6, we added noise of the bird audio detection data set from the DCASE challenge (Berger et al., 2018; Himawan
et al., 2018; Liaqat et al., 2018), which is abbreviated as BAD.

Data augmentation

Time domain

Frequency domain

Spectrogram

Training time [min. / epoch]

DenseNet-161

ViT-B/16

Gaussian noise
Pink noise (background)
Primary background noise soundscapes
Secondary background noise soundscapes
Primary background noise (BAD)
Secondary background noise (BAD)
Background noise (e.g., wind, thunder, or aircrafts)
Mixed up random bird species
Horizontal roll
Vertical roll
Pitch shift
Time mask
Frequency mask
Gain
Loudness normalization
Horizontal flip
Vertical flip
Time stretch
tanh-based distortion

✓
✓
✓
✓
✓
✓
✓

✓

✓
✓

✓

In ecology, changes in environmental conditions are often closely linked to shifts in species diversity. This
relationship can be investigated by analyzing avian vocalizations, which are robust indicators of trends in
biodiversity. Within this contribution, we explored various data augmentation techniques and deep learning
strategies for the classification of birdsong within natural soundscapes. For this purpose, we employed three
fundamental deep neural network architectures, such as vision transformers, to classify 397 different bird spe-
cies. To improve both the accuracy and generalizability of our models, we incorporated up to 19 well-established
data augmentation techniques commonly used in audio classification. This included an iterative selection process
where only augmentations that enhanced classification performance were selected. The primary augmentation
technique involved the integration of various noise samples and non-bird audio elements, which significantly

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Answer: In the given context, the authors utilized the Adam optimizer (Kingma and Ba, 2015) in their deep learning pipeline. They mention using the Adam optimizer along with a concatenating cosine-annealing linear scheduler with an initial learning rate of 0.0001, decaying by a factor of 0.001*learning rate, and a batch size of 32. Additionally, early stopping was implemented to prevent overfitting when no further improvement was observed during training.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(Fig.  1),  considering  its  performance  in  the  new  environment  with 
limited re-training. Starting with a pre-trained model (EfficientNet B0, 
Tan and Le, 2019), we freeze the networks original feature extractor. We 
then train fully connected layers, which operate on the output of the 
EfficientNet  B0  network,  to  classify  the  PAM  data  frames  into  the 
specified sound source classes. Audio data is input to the network in a 
novel  spectrogram representation (White  et  al.,  2022). Networks  pre- 
trained on image data have been proven to perform well on classifica-
tion  tasks  using  spectrograms  as  image  input,  transformed  from  raw 
audio data (Allen et al., 2021; Shiu et al., 2020; Stowell, 2022). In this 
work  we fine-tune the  parameters  of the fully connected layers  itera-
tively  on  small  batches  of  training  data  acquired  from  a  new  region. 
Through  an  experimental  approach  to  retraining  we  demonstrate  the

The model is constructed using the EfficientNet B0 network (Tan and 
Le, 2019) which had been trained for generic image classification. The 
EfficientNet feature extraction layers are frozen (transfer learning) with 
only the weights of the final dense classification layers updated during 
training.  Training  was  conducted  within  the  Google  Collaboratory 
‘Colab’  platform  (Bisong,  2019),  using  the  Tesla  K80  GPU,  accessed 
through  cloud  computing.  An  Adam  optimizer  was  used  to  control 
gradient descent during training (Kingma and Ba, 2014), with parame-
ters set to: learning rate of 0.001, decay factor of 0.75 and a step size of 

EcologicalInformatics78(2023)1023632E.L. White et al.

2.3. Training data 

A training set was developed from the Gulf of Mexico data consisting 
of data from a single day: December 3rd 2019 (selected arbitrarily). The 
24  h  of  PAM  data  was  divided  into  frames  of  3  s,  reviewed  visually 
(spectrograms)  and  aurally  using  Audacity  software  (version  3.0.02, 
2021)  and  classified  into  one  of  the  four  sound  source  categories. 
Following  the  principles  detailed  in  White  et  al.,  2022,  frames  were 
assigned a single label based on a hierarchy of rules: (i) If a whistle is 
present in the 3 s frame the label is ‘Delphinid Tonal’, regardless of the 
presence of another sound source; (ii) biological clicks are only labelled 
as such in the absence of whistles; (iii) a sound source is labelled if any 
detection is made by an analyst regardless of signal strength in the frame 
in respect to the ambient noise. After annotation a training set of 28,800 
frames is available across the four classes.

sparsely occurring signal types. Our analysis shows that models trained 
on one geographic region can be applied to another specific region with 
a small amount of additional labelled data, and that an initial globally 
trained model is not required. We hope to encourage the adaptation of 
existing  bioacoustic  CNN  architectures  to  new  marine  regions  and 
soundscapes in order to exploit the wealth of information which is held 
in PAM datasets globally.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Audio data is input to the network in a novel spectrogram representation. Networks pre-trained on image data have been proven to perform well on classification tasks using spectrograms as image input, transformed from raw audio data. In this work, the authors use a spectrogram representation of the audio data as input to their deep learning pipeline. Therefore, both audio and image data formats are used in the deep learning pipeline.

Reference(s):
[White et al., 2022]
[Allen et al., 2021]
[Shiu et al., 2020]
[Stowell, 2022]
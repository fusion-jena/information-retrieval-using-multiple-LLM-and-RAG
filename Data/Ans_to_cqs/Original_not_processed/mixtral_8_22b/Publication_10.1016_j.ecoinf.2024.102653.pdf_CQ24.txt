Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

This study used the train_test_split function from the scikit-learn library 
(version 1.2.2), a function that facilitates randomized shuffling and di-
vision of datasets into distinct training and testing subsets. Specifically, 
70%  of  the  data  was  designated for  training  the  classification  model, 
while the remaining 30% was set aside for testing (Dobbin and Simon, 
2011). Individuals from the control and pesticide-exposed groups were 
evenly  distributed  between  these  subsets,  guaranteeing  a  balanced 
representation of each class. The recurrent neural network (RNN) model 
was designed to incorporate three primary input parameters: the bee’s 
age, the daily number of flights, and the total time spent outside daily. 
Following  data  preparation,  labels  for  both  training  and  testing 
datasets  underwent  a  transformation  known  as  one-hot  encoding, 
facilitated  by  the  OneHotEncoder  function  from  the  scikit-learn  library

2.6. Performance metrics 

Conventional  metrics  were  used  to  evaluate  the  model’s  perfor-
mance, such as confusion matrices, accuracy, recall, precision, F1-score, 
Cohen  Kappa,  and  the  area under  the  ROC curve,  some of  which  are 
defined below (Johnson and Khoshgoftaar, 2019).  

Table 1 
List of hyperparameters utilized for training the Recurrent Neural Network 
model.  

Hyperparameter 

Value 

Optimizer 
Learning rate 
Batch size 
Epochs 
Dropout rate 
Loss function 

ADAM (Kingma and Ba, 2017) 
0.003 
32 
100 
0.5 
Binary Cross Entropy  

●  Confusion Matrix

2. Methods 

Fig. 2 summarizes a series of processes in designing a deep-learning 
architecture that acts as a toxicovigilance tool. Subsequent sections will 
provide a comprehensive description of each section. 

2.1. Data collection

2.4. Model training and hyperparameters 

The  model’s  weights  were  randomly  initialized  using  a  specific 
integer seed to guarantee consistent reproducibility of results. Further-
more, meticulous manual tuning of hyperparameters was conducted to 
achieve  optimal  model  performance.  The  selected  hyperparameters, 
which yielded the best performance metrics, are presented in Table 1. 

2.5. Model validation 

Two distinct validation methodologies were implemented. The first 
method  involved  partitioning  the dataset into  training and  validation 
sets, with 70% of the data designated for training and 30% for valida-
tion.  This  division  was  performed  through  random  selection  (Dobbin 
and Simon, 2011).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To handle randomness in the deep learning pipeline, this study employs several strategies. Firstly, it uses the train_test_split function from the scikit-learn library to facilitate randomized shuffling and division of datasets into distinct training and testing subsets. This ensures that the data is not biased towards any particular group or pattern. Secondly, the model's weights are randomly initialized using a specific integer seed. This guarantees consistent reproducibility of results, meaning that if the same code is run again, it should produce the same results. Lastly, when partitioning the dataset into training and validation sets, the division is performed through random selection. This helps prevent overfitting and ensures that the model can generalize well to unseen data.
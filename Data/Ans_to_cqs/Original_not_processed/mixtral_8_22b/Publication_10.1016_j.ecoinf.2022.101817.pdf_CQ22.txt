Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The feasibility of the entire system is evaluated based on the cost and 
energy consumption. The unit cost must be considered, especially for 
large-scale deployments. Considering the HDP hardware, only the CPU 

Table 4 
Performance metrics of the five CNN models over the validation dataset.  

Precision(%) =

True Positives
True Positives + False Positives

*

100

Recall calculates the amount of correctly predicted out of all possible 
positives. The opposite of this measure is called the “recall specificity”, 
which provides the number of false predictions from all possible nega-
tives. The recall is calculated as: 

Recall(%) =

True Positives
True Positives + False Negatives

*

100

(5) 

Our  final  evaluation  parameter  is  the  F1  score,  which  provides  a 
measure between precision and recall, calculated as the harmonic mean 
of the two metrics. The F1 score was evaluated as: 

(4) 

Metric 

Model 1 

Model 2 

Model 3 

Model 4 *  Model 5

Frames processed per second
Watts

(8) 

Power consumption measurements were performed using software- 
based  monitoring,  when  the  system  supported  it.  Only  MediaTek 
Pumpkin i500 power consumption was measured using a high-precision 
external power meter. 

Fig. 7. Confusion matrices generated during the cross-validation phase of the designed CNN models. (a) The average of confusion matrices of the five CNN models. 
(b) Confusion matrix of the best CNN model that was subsequently used in the next phase of the proposed pipeline.

classification  pipeline.  The  validation  performance  of  CNN  Model  4 
with respect to each of the six environmental conditions is shown in the 
confusion matrix of Fig. 7 (b).

Model 2 

Model 3 

Model 4 *  Model 5 

Accuracy 
Precision 
Recall 

Sensitivity 

99.1% 
97.3% 
97.3% 

99.1% 
97.4% 
97.4% 

98.8% 
96.5% 
96.5% 

99.2% 
97.5% 
97.5% 

98.9% 
96.8% 
96.8% 

Recall 

99.5% 

99.5% 

99.3% 

99.5% 

99.4% 

Specificity 

F1 Score 
Avg. True 
Positives 
Avg. False 
Positives 
Avg. True 

Negatives 

Avg. False 

Negatives 

97.3% 
243.2 
(16.2%) 
6.8 
(0.5%) 
1243.2 
(82.8%) 
6.8 
(0.5%) 

97.4% 
243.5 
(16.2%) 
6.5 
(0.4%) 
1243.5 
(83.0%) 
6.5 
(0.4%) 

96.5% 
241.3 
(16.0%) 
8.7 
(0.6%) 
1241.3 
(82.8%) 
8.7 
(0.6%) 

97.5% 
243.7 
(16.2%) 
6.3 
(0.4%) 
1243.7 
(83.0%) 
6.3 
(0.4%) 

96.8% 
242 
(16.2%) 
8 (0.5%) 

1242 
(82.8%) 
8 (0.5%)  

* Best CNN model based on Accuracy.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Three main metrics are used to evaluate the performance of the deep learning models in this study: accuracy, precision, and recall.

Accuracy measures the proportion of correct predictions made by the model out of the total number of instances. It can be expressed as a percentage or a fraction. In Table 4, we see that the accuracy of Model 4 is 99.2%, while Model 5 has an accuracy of 98.9%.

Precision measures the proportion of true positive predictions out of all positive predictions made by the model. A higher precision indicates fewer false positives. Precision is also known as the positive predictive value. In Table 4, we see that Model 4 has a precision of 97.5%, while Model 5 has a precision of 96.8%.

Recall measures the proportion of true positive predictions out of all actual positive instances. A higher recall indicates fewer false negatives. Recall is also known as sensitivity or true positive rate. In Table 4, we see that Model 4 has a recall of 97.5%, while Model 5 has a recall of 96.8%.

In addition to these three metrics, another metric called the F1 score is used to provide a balance between precision and recall. The F1 score is the harmonic mean of precision and recall. In Figure 7, we see that the F1 scores of Models 4 and 5 are 97.5% and 96.8%, respectively.
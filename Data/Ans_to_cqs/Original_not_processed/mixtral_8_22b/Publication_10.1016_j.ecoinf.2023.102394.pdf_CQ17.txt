Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Acknowledgments 

This study was supported by the National Natural Science Founda-

tion of China (42171054, 42377302). 

References 

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., et al., 2016. Tensorflow: A 
system for large-scale machine learning. In: Paper Presented at 12th USENIX 
Symposium on Operating Systems Design and Implementation, Savannah, United 
States. 

Abatzoglou, J.T., Dobrowski, S.Z., Parks, S.A., Hegewisch, K.C., 2018. TerraClimate, a 
high-resolution global dataset of monthly climate and climatic water balance from 
1958-2015. Sci. Data 5, 170191. https://doi.org/10.1038/sdata.2017.191. 
Adhikari, K., Owens, P.R., Libohova, Z., Miller, D.M., Wills, S.A., Nemecek, J., 2019. 

Assessing soil organic carbon stock of Wisconsin, USA and its fate under future land 
use and climate change. Sci. Total Environ. 667, 833–845. https://doi.org/10.1016/ 
j.scitotenv.2019.02.420.

The  structure  of  the  MLP  model  consists  of  3  components:  (1)  an 
input  layer  with  the  input  predictors  matching  the  number  of  input 
environmental covariates; (2) two hidden layers consisting of 60 neu-
rons with the rectified linear unit (ReLU) activation function (Table 2); 
and (3) an output layer consisting of a single element (the SOCS vari-
able) with a linear activation function for regression purposes. To reduce 
overfitting, we created dropout layers with dropout rates of 0.2 and 0.3 
in this study. After model structure definition, we compiled the model 
with the adaptive moment estimation (Adam) optimization algorithm, 
the loss function of the mean squared error, and the model metric of the 
mean  absolute  error.  The  next  step  involved  fitting  the  model  to  the 
training dataset. In this step, weights were estimated by the model. Once 
the model was trained, we predicted the SOCS given the testing dataset.

Allaire, J.J., Chollet, F., 2018. Keras: R Interface to “Keras”. Available at. https://CRAN. 

R-project.org/package=keras. 

Attri, I., Awasthi, L.K., Sharma, T.P., Rathee, P., 2023. A review of deep learning 

techniques used in agriculture. Ecol. Inform. 77, 102217 https://doi.org/10.1016/j. 
ecoinf.2023.102217. 

Austin, P.C., 2011. An introduction to propensity score methods for reducing the effects 
of confounding in observational studies. Multivar. Behav. Res. 46, 399–424. https:// 
doi.org/10.1080/00273171.2011.568786. 

Austin, P.C., Stuart, E.A., 2015. Moving towards best practice when using inverse 

probability of treatment weighting (IPTW) using the propensity score to estimate 
causal treatment effects in observational studies. Stat. Med. 34, 3661–3679. https:// 
doi.org/10.1002/sim.6607. 

Barbosa, A.M., Brown, J., Jim´enez-Valverde, A., Real, R., 2016. modEvA: Model 

Evaluation and Analysis. Available at. https://cran.r-project.org/web/packages/ 
modEvA.

The  successful  prediction  is  also  attributed  to  the  robust  deep 
learning models. Lamichhane et al. (2019) reviewed soil carbon map-
ping methods and reported that deep learning is a promising technique 
for improving SOC prediction. The emphasis on deep learning has also 
been noted by Wadoux et al. (2019), who used this technique to predict 
soil  C.  They  found  that  the  deep  learning  technique  significantly 
increased  prediction  accuracy.  In  addition  to  SOC  prediction,  some 
recent studies have used deep learning in the field of ecological infor-
matics and  demonstrated the  ability of this technique  to improve the 
prediction  of  ecological  components  such  as  forest  vegetation  cover 
(Kalinaki  et  al.,  2023)  and  agriculture-related  elements  (Attri  et  al., 
2023).  According  to  them,  deep  learning  has  the  potential  to  be  a 
valuable technique for ecological analysis.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the provided context, there is mention of one specific method used to prevent overfitting in the deep learning pipeline - dropout layers. Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps to prevent complex co-adaptations on training data. This technique is mentioned in the context as being used with dropout rates of 0.2 and 0.3. However, other common regularization techniques like L2 regularization or early stopping are not explicitly mentioned in the provided context. Therefore, while dropout is confirmed as a method used in this case, it cannot be definitively stated whether these other methods were also employed without further information.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.5. Automated machine learning (AutoML) 

2.6. Generative adversarial networks (GANs) 

AutoML offers a substantial level of automation, encompassing tasks 
such  as  including  data  pre-processing,  algorithm  selection,  and  opti-
mizing hyperparameters. These individual steps can pose challenges for 
non-expert data scientists, creating substantial barriers to designing and 
implementing algorithms effectively. Consequently, AutoML was intro-
duced to streamline these intricate processes for non-experts, making it 
more accessible to use these techniques correctly and efficiently (Rah-
man et al., 2023). In this study, two AutoML models, AutoGluon and 
TabPFN, were employed. 

2.5.1. AutoGluon

Δi(s, t) = i(t) (cid:0) NtL
Nt

i(tL) (cid:0) NtR
Nt

i(tR)

(2)  

where Δi (s,t) = the reduction in MSE, Nt, NtL, NtR is the sample size of 
the parent node and the left (L) and rig (R) child nodes respectively, and 
i(t) is the MSE. 

2.3.1. Random Forest (RF) 

RF  is  a  bagging  ensemble  data-driven  model  that  utilises  decision 
tree models as its basis. RF generates multiple individual decision tree 
models by utilizing different subsets of the training dataset through a 
technique  called  bootstrap  aggregating  or  bagging  (Chiaverini  et  al., 
2023). RF employs the bootstrapped samples, which consist of randomly 
selected  subsets  from  the  training  data,  to  make  predictions  on  the 
output. When faced with unseen scenarios, RF combines the predictions 
from the ensemble of trees it has developed to arrive at a final prediction 
(Leuenberger et al., 2018): 

ypred = 1

nbtree

∑nbtree

i=1

yi

(3)

3.2. Hyperparameter tuning of random Forest and XGBoost 

We determine the optimal hyperparameters to achieve peak perfor-
mance for the RF and XGBoost models through a 5-fold cross-validation 
process (Table 2). The findings reveal that RF models necessitate greater 
depth compared to XGBoost, while the number of estimators is roughly 

Table 2 
Hyperparameter tuning of RF and XGBoost models.  

Models 

Random 
Forest 

XGBoost 

Classification 
task 

Maximum 
depth 

n_estimators 

Learning 
rate 

Binary 

Multi-class 
Binary 
Multi-class 

190 

150 
5 
5 

5 

6 
6 
4 

–  

– 

0.05  
0.05  

EcologicalInformatics82(2024)1027116S. Khanmohammadi et al.

Ashrapov, I., 2020. Tabular GANs for uneven distribution arXiv preprint arXiv: 

2010.00638.  

Azedou, A., Amine, A., Kisekka, I., Lahssini, S., Bouziani, Y., Moukrim, S., 2023. 

Enhancing land cover/land use (LCLU) classification through a comparative analysis 
of hyperparameters optimization approaches for deep neural network (DNN). Eco. 
Inform. 78, 102333. 

Babu, K.N., Gour, R., Ayushi, K., Ayyappan, N., Parthasarathy, N., 2023. Environmental 
drivers and spatial prediction of forest fires in the Western Ghats biodiversity 
hotspot, India: an ensemble machine learning approach. For. Ecol. Manag. 540, 
121057. 

Barros-Rosa, L., de Arruda, P.H.Z., Machado, N.G., Pires-Oliveira, J.C., Eisenlohr, P.V., 
2022. Fire probability mapping and prediction from environmental data: what a 
comprehensive savanna-forest transition can tell us. For. Ecol. Manag. 520, 120354.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The strategies employed to handle randomness in the deep learning pipeline include setting a fixed random seed value, using deterministic algorithms, and employing techniques such as k-fold cross-validation and ensemble methods. Setting a fixed random seed value ensures reproducibility of results across different runs of the same experiment. Deterministic algorithms eliminate the need for random initialization, thus reducing variability in outcomes. K-fold cross-validation involves splitting the dataset into k equal parts and iteratively training the model on k-1 folds while testing on the remaining fold, thereby reducing bias due to random sampling. Ensemble methods combine multiple models trained on different subsets of the data, averaging their outputs to reduce variance and improve generalization performance.

Based on the given context, there is no direct mention of specific strategies used to handle randomness in the deep learning pipeline. However, the context does discuss the use of AutoML models like AutoGluon and TabPFN, which may incorporate some of these strategies implicitly. Additionally, the context mentions the use of 5-fold cross-validation for hyperparameter tuning of RF and XGBoost models, indicating one possible strategy for handling randomness. It should be noted that the effectiveness of each strategy depends on the specific problem domain and dataset characteristics.
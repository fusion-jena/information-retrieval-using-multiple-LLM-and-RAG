Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

curator while using the app as a tool for post-dive debriefing. Combined, 
these two steps ensure that there is a specific checklist to cross out and 
that the interview is embedded in regular operations of the dive centre. 
Another aspect that could influence data quality is the possibility of 
misidentifying  species  that  are  less  known  to  the  citizen  scientists 
(Austen  et  al.,  2016;  Freiwald  et  al.,  2018).  This  is  especially  true 
because the app does not require submission of photographic evidence, 
which  could  be  validated  by  experts  (Kosmala  et  al.,  2016)  or  using 
machine learning (Saoud et  al., 2020). Once again, by relying on the 
dive guide to conduct the interview, there is some level of validation of 
the species sighted and reported, as they are often knowledgeable and 
experienced in local fauna and flora. This approach has also been used in 
previous studies as a safeguard for data quality (Branchini et al., 2015;

of standardisation, by focusing on the frequency and abundance of specific taxa, while retaining a recreational 
dive  plan.  Additionally,  the  app  also  collects  metadata  on  location,  number  of  dives  and  number  of  divers 
enabling normalisation based on “sampling effort”. In this pilot study, the use of the app was tested to compile 
information on the frequency and abundance of 18 marine taxa selected by local experts based on their con-
servation  status,  commercial  interest,  ecological  function  and/or  their  non-indigenous  origin.  Additionally,  a 
question-based survey was conducted to assess the opinion of users on the app’s usability and the potential value 
for the diving community/industry, showcasing a high usability score and interest among users. Basic statistical 
analysis of the data crowdsourced over the 1-month trial illustrates the potential and value of regional moni-

sponses  and  open-ended  queries  to  get  insight  into  the  best  way  the 
monitoring program can bring value to the dive community. Responses 
were compiled  and data was used to calculate the average SUS score 
(Kumar, 2020).

To  better  accommodate  the  needs  of  dive  operators,  while  main-
taining  usability  for  research  and  management,  the  trial  included  a 
query for dive staff to detail how to enhance a future long-term moni-
toring program. Most of the received suggestions were already planned 
to be included in a full-scale monitoring program, although these were 
not yet part of the trial. Frequency maps, such as the one showcased in 
Fig. 1, would allow the dive centre to identify dive sites with the highest 
probability  of  encountering  specific  taxa  or  keep  statistics  about  the 
number of times these taxa have been observed throughout a year. This 
information would benefit the dive centres for marketing purposes, as 
recreational  SCUBA  divers  usually  prefer  areas  where  they  can  see 
charismatic species (Grafeld et al., 2016). The query also confirmed that 
a poster presence for each dive centre with the target taxa that are part

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps may be necessary depending on the nature of the dataset. These steps help improve the performance and accuracy of the model. Some common preprocessing techniques include:

1. Data Cleaning: This involves removing any irrelevant, incorrect, or duplicate data from the dataset. It might also involve handling missing values if present.

2. Normalization: This process scales numeric columns to a similar range, typically between 0 and 1. Techniques like Min-Max Scaling or Z-score Standardization can be employed for this purpose. Normalization helps prevent features with larger magnitudes from dominating the learning process.

3. Feature Scaling: Similar to normalization, feature scaling rescales the values of different features to a standard scale without distorting differences in ranges or units. Common methods include Min-Max Scaling and Standardization.

4. Encoding Categorical Variables: Deep learning models cannot directly handle categorical variables. Therefore, it is essential to convert them into numerical form using encoding techniques like One-Hot Encoding or Label Encoding.

5. Dimensionality Reduction: High dimensional datasets can lead to increased computational complexity and risk of overfitting. Techniques like Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), or t-Distributed Stochastic Neighbor Embedding (t-SNE) can reduce the dimensions of the input space.

6. Data Augmentation: To increase the size of the dataset artificially, various transformations like rotation, zooming, shifting, etc., can be applied to existing images. This technique is particularly useful when dealing with image data.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(8)  

where  dim is  the dimension  of the  feature vectors,  Ft(τ) is the τth  the 
element of the feature vector of the query image t and FDBs (τ) is the τth 
element of the feature vector of the sth  image in the database DB. 

ComputersandElectronicsinAgriculture187(2021)1062696A. Loddo et al.                                                                                                                                                                                                                                  

Metrics. Four performance measures, namely average retrieval pre-
cision, ARP, average retrieval rate, ARR, Fscore  and mean average preci-
sion, mAP, have been used to compare the retrieval performances. The 
mathematical  formulation  for  each  of  these  performance  measures  is 
given as follows: 

ARP =

number of relevant images retrieved
total number of images retrieved (η)

=

100
ω

∑ω

i=1

r(DBi)
η

(9)  

ARR =

22 
30 
77 
118 
32 
24 
15 
55 
103 
160 
4  

ComputersandElectronicsinAgriculture187(2021)1062697A. Loddo et al.                                                                                                                                                                                                                                  

Table 5 
Summary of the experimentation results on the local dataset (StD is the deviation standard of Acc value and Time is the training time in minutes).  

Network 

AlexNet 
ResNet18 
ResNet50 
ResNet101 
GoogLeNet 
ShuffleNet 
SqueezeNet 
MobileNetV2 
InceptionV3 
VGG16 
SeedNet 

Acc 

93.43 
97.47 
96.46 
96.97 
95.45 
96.46 
95.96 
93.94 
96.46 
95.96 
97.47 

Spec 

91.08 
96.63 
94.44 
96.61 
95.06 
95.90 
95.75 
93.16 
95.99 
94.82 
99.88 

Sen 

91.36 
95.50 
94.98 
94.66 
93.47 
94.37 
94.71 
91.51 
94.81 
94.86 
96.81 

MAvG 

90.15 
96.11 
96.15 
96.34 
94.67 
95.57 
95.13 
92.67 
95.74 
94.22 
96.60 

MFM

3.2.2. Retrieval results 

The retrieval experiments ensued the following strategy: every image 
is used as a query image and is matched with every remaining image in 
the database. Tables 6 and 7 present the retrieval performances of all the 
networks in terms of the four considered measures by choosing the top 
100 matches, i.e. η = 100, and the two chosen similarity distances. 

3.3. Deep learning vs traditional machine learning comparison 

A comparison of deep learning approaches with traditional machine

retrieval measurements. 

To  sum  up,  SeedNet  seems  robust  for  both  tasks,  reaching 
outstanding performance results with both datasets and having a low 
training time if compared to the  other examined networks. However, 
other CNNs obtained satisfactory results, like the Residual Networks (e. 
g. ResNet18) in both tasks, or SqueezeNet in the retrieval one. The CNNs 
are also generally preferable in performances with respect to the tradi-
tional  methods,  even  though  the  last  ones  reached  interesting  results 
with low training times. 

4. Conclusions

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, there is no information about the specific strategy implemented to monitor the model performance during training. Therefore, I cannot provide a helpful answer to this query.
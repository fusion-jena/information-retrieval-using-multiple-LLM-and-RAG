Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Model Training
Deep learning models
iteratively: generating
learn data
predictions of the training set samples, evaluating them by

3https://github.com/ultralytics/yolov5/tree/v4.0
4https://cocodataset.org/#home

the loss function, and modifying their parameters according
to the loss function’s gradient. Thus,
the loss declines in
each training round (aka epoch), and the model’s annotation
accuracy increases (Figure 2 and Supplementary Figure 1).
As the number of parameters of a typical deep learning
model
is large, overﬁtting is a major concern. A common
(partial) remedy is the use of validation data, which are set-
aside while training, yet their annotations by the models are
the training
monitored. The validation data help to limit
duration, thereby reducing the risk of overﬁtting. In this study
we used 30 epochs per training task, based on our validation
runs (Figure 2).
The data of

1

May 2021 | Volume 9 | Article 600931

fevo-09-600931

May 17, 2021

Time: 21:42

# 2

Gerovichev et al.

Deep Learning for Insect Ecoinformatics

INTRODUCTION

Machine learning methods are increasingly applied to
biological problems that involve classiﬁcation of images and
extracting information from them. The current leading approach
to such tasks is supervised learning using deep neural networks

1https://www.gbif.org

(DNNs), and particularly convolutional neural networks (CNNs),
which are able to extract abstract high level features from images.
Identifying objects within the image and classifying them may
be treated as separate tasks. Yet, more recent methods, such as
“Faster R-CNN” (Ren et al., 2015), and YOLO (Redmon et al.,
2016) consider both problems at the same time.

The Deep Learning Network
For machine learning we used an open source implementation3
of YOLO version 5 (Ren et al., 2015; Redmon and Farhadi,
2017, Redmon and Farhadi, 2018). This algorithm simultaneously
predicts bounding boxes around objects (insects, in this case)
within the image, their class labels (species) and conﬁdence
scores. To this end, the algorithm generates uniformly spaced
tentative bounding boxes, adjusts them (by changing their center
and dimensions) to nearby objects, and assigns them a class label
and conﬁdence score. This process is governed by a loss function
that penalizes high-conﬁdence wrong predictions (wrong class
or small IoU) and low-conﬁdence correct ones. Bounding boxes
of low conﬁdence, and ones that share an object with a higher
conﬁdence box, are discarded. Typically, some classes, here
“Glycaspis,” are more frequent than the others. This so-called
class-imbalance may lead to overestimation of these classes. To

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, there isn't any explicit mention of specific optimization techniques like Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being used in the deep learning pipeline. However, it does discuss the general concept of iteratively improving the model through backpropagation and updating its parameters based on the loss function's gradients. This implies that some form of optimization technique is indeed employed, but without further details, one cannot definitively state whether it is SGD, Adam, or another method.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Figure 10. Comparison of accuracies of different classiﬁers for all datasets.

Big Data Cogn. Comput. 2021, 5, 53

11 of 15

Table 2. Performance of hand-crafted descriptors and D-CNN models for ﬁrst dataset.

Technique’s
Name

SVM
Kernel

LBP [10]
HOG [11]
LETRIST [12]
GLCM [13]
GLCM [13]
CJLBP [14]
LTrP [15]
AlexNet [17]
ResNet-50 [18]
VGG-19 [19]
GoogleNet [20]
Inceptionv3 [21]
CoralNet
BoF

Polynomial
Linear
Linear
RBF
Polynomial
Linear
Linear
Linear
Linear
Linear
Linear
Linear
–
Linear

Sensitivity

Speciﬁcity Accuracy

F1-Score

Cohen’s
Kappa (κ)

70.1%
66.3%
56.2%
66.2%
73.1%
71.2%
48.4%
94.1%
92.2%
92.1%
85.1%
77.1%
92.1%
99.1%

75.9%
69.3%
59.7%
75.1%
80.4%
77.3%
50.2%
96.3%
96.4%
92.1%
93.1%
92.3%
97.3%
99.0%

71.8%
67.1%
56.6%
69.3%
76.7%
72.7%
49.1%
95.2%
94.5%
92.2%
88.2%
83.3%
95.0%
99.08%

0.729
0.678
0.579
0.704
0.766
0.741
0.493
0.952
0.942
0.921
0.889
0.840
0.950
0.995

0.731
0.663
0.594
0.732
0.751
0.743
0.524
0.966
0.952
0.851
0.873
0.862
0.962
0.982

.

(3)

where (cid:107) ξi − ξ j (cid:107) provides the Euclidean distance between two samples. Width of Gaussian
kernel can be set by variance (cid:36) that controls the classiﬁer performance.

3.5. Confusion Matrix

It is the parameter to validate the performance of a machine learning model. It also
tells us the accomplishment of the classiﬁcation problem. Following are some of the
essential parameters of the confusion matrix.

1.
2.
3.
4.
5.

6.

7.

8.

9.

True Positive (TP): It is the accurate prediction of the bleached corals.
True Negative (TN): It is the accurate prediction of the unbleached corals.
False Positive (FP): It is the false prediction of the bleached corals.
False Negative (FN): It is the false prediction of the unbleached corals.
Sensitivity (TPR): It is the ratio of accurate prediction of the corals and can be given
by Equation (4).

Sensitivity

(TPR) =

TP
(TP + FN)

(4)

Table 4. Performance of hand-crafted descriptors and D-CNN models for third dataset (Bleached,
healthy, and dead (BHD) Dataset).

Technique’s
Name

LBP [10]
HOG [11]
LETRIST [12]
GLCM [13]
CJLBP [14]
LTrP [15]
AlexNet [17]
ResNet-50 [18]
VGG-19 [19]
GoogleNet [20]
Inceptionv3 [21]
CoralNet
BoF

Classiﬁer Sensitivity

Speciﬁcity

Accuracy

F1-Score

Cohen’s
Kappa (κ)

SVM
SVM
SVM
SVM
SVM
SVM
SVM
SVM
SVM
SVM
SVM
–
SVM

69.3%
74.42%
55.3%
65.2%
70.2%
47.4%
86.37%
85.43%
82.1%
80.55%
81.10%
91.1%
98.1%

71.4%
60.05%
58.5%
74.1%
76.3%
49.2%
83.73%
85.80%
82.1%
80.51%
76.44%
96.3%
98.0%

69.8%
75.2%
55.4%
68.3%
71.7%
48.1%
92.20%
92.60%
82.2%
88.60%
86.30%
94.0%
98.11%

0.689
0.665
0.569
0.694
0.731
0.483
0.850
0.856
0.821
0.805
0.787
0.940
0.985

0.691
0.621
0.584
0.722
0.733
0.514
0.826
0.852
0.781
0.803
0.761
0.952
0.972

Big Data Cogn. Comput. 2021, 5, 53

12 of 15

Figure 11. Confusion matrices of binary class and multi-class datasets.

Sensitivity

(TPR) =

TP
(TP + FN)

(4)

Speciﬁcity (Sy): It is the ratio of the prediction of unbleached corals and can be given
by Equation (5).

Speci f icity =

TN
(TN + FP)

(5)

Accuracy: The ratio of correct prediction to the total number of instances can be given
by Equation (6).

Accuracy =

(TN + TP)
(FP + TP + FN + TN)

(6)

F1-score: It is the weighted mean of sensitivity and speciﬁcity and can be given by
Equation (7).

F1 − score =

∗ 2

(7)

(cid:21)

(cid:20) (TPR ∗ Sy)
(TPR + Sy)

Cohen’s Kappa (κ): κ gives us the amount of data by the execution of classiﬁer for the
examination of the its performance in case of some coincidence. It can be calculated
by Equation (8) [34].

(cid:20)

κ = 2 ∗

(TP ∗ TN − FP ∗ FN)
(TP + FP) ∗ (FP + TN) + (TP + FN) ∗ (FN + TN)

(cid:21)

(8)

3.6. Dataset

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The performance of the deep learning model is evaluated using several metrics such as sensitivity, specificity, accuracy, F1-score, and Cohen's kappa (κ). Sensitivity measures the proportion of actual positives that are correctly identified as positive, while specificity measures the proportion of actual negatives that are correctly identified as negative. Accuracy is the ratio of correct predictions to the total number of instances. F1-score is the harmonic mean of precision and recall, where precision is the ratio of true positives to the sum of true positives and false positives, and recall is the same as sensitivity. Finally, Cohen's kappa (κ) is a statistical measure that compares observed accuracy with expected accuracy due to chance alone. These metrics provide insights into the strengths and weaknesses of the deep learning model and help researchers improve their models.
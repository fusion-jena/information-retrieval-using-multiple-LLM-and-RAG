Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

practitioner can manually annotate a few examples and then start using 
a  pre-trained  ResNet152V2  model  to  find  new  examples.  Once  addi-
tional examples have been found via the pre-trained model, these new 
examples can be incorporated into the training set. This iterative process 
can be repeated until a large training set is obtained, after which, the 
pre-trained CNN can be fine-tuned to create a more robust classifier. We 
thus argue that practitioners can begin using CNNs relatively early on 
within  a project to  speed up  the rate at  which calls  are found.  These 
findings  oppose  existing  knowledge  that  deep  learning  requires  large 
training  datasets.  One  possible  explanation  for  the  good  performance 
achieved in this study is due to the high signal-to-noise ratio. It was also 
hypothesised  that  good  performance  was  obtained  due  to  the  lack  of 
variation within the calls (e.g. gibbons and whydah datasets), however

The software code was written in Python 3 for audio pre-processing 
and  the  general  methodology,  and  the  CNNs  were  implemented  in 
Tensorflow 2 (Abadi et al., 2015). Each CNN was trained for 50 epochs 
(number of iterations of the CNN learning algorithm) using the Adam 
optimiser  (Kingma  and  Ba,  2014)  and  a  batch  size  of  32.  The  hyper- 
parameters  were  obtained  by  conducting  a  random  search  using 
similar values to that in the study of Dufourq et al. (2021). Spectrograms 
were generated using the Librosa library (McFee et al., 2020). 

4. Results

Using  a  pre-trained  CNN  feature  extractor  and  adding  a  softmax 
output layer is less complex than having to optimise a CNN from scratch 
and requires less network design decisions and also less time on hyper- 
parameter tuning. We thus argue that this approach renders the use of 
deep learning much more accessible to practitioners. Extensive hyper- 
parameter tuning also requires expensive GPU hardware which might 
not  be  accessible  to  practitioners.  Our  findings  revealed  that  results 
could be obtained on limited hardware within 9 h (10 epochs of fine- 
tuning  the  feature  extractor)  which  would  cost  2USD,  at  the  time  of 
writing, if that was executed on a Microsoft Azure virtual machine – thus 
rendering this approach affordable and accessible. 

It is well accepted that no single machine learning algorithm – or in

There were not a large number of studies that compared various pre- 
trained neural network architectures. Xie et al. (2018) compared various 
implementations of VGG16 pre-trained on the ImageNet dataset. Their 
results  show  that  VGG16  with  transfer  learning  (whereby  the  feature 
extractor was frozen) did not outperform VGG16 with random initiali-
sation.  To  overcome  this,  a  multi-channel  model  was  created  which 
inputs  Short-time  Fourier  transform,  mel-spectrogram  and  chirplet 
spectrograms  into  three  separate  pre-trained  VGG16  models  respec-
tively. This approach led to the best results and had considerably less 
neural  network  trainable  parameters  compared  to  the  single  VGG16 
with  random  initialisation.  Henri  and  Mungloo-Dilmohamud  (2021) 
collected and created a birdsong dataset from Xeno-Canto to create a 
CNN  classifier.  The  authors  compared  MobileNetV2  (Sandler  et  al.,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps may be required depending on the type of data being used. In the given context, the focus is on audio data, specifically bird songs. The software code mentioned in the text is written in Python 3 for audio preprocessing. However, the specific preprocessing techniques applied to the audio data are not explicitly stated in the provided context. Generally, common preprocessing steps for audio data include noise reduction, normalization, and segmentation. Noise reduction involves removing unwanted sounds or background noise from the audio recordings. Normalization scales the amplitude of the audio signals to a standard range, typically between -1 and 1. Segmentation divides the continuous audio stream into smaller segments or frames, allowing the model to learn local patterns within the audio data. Additionally, spectrogram generation is mentioned in the context, which converts the raw audio waveform into a visual representation of frequency over time. This step is crucial for feeding the audio data into a convolutional neural network (CNN) since it allows the model to analyze both temporal and spectral features simultaneously.
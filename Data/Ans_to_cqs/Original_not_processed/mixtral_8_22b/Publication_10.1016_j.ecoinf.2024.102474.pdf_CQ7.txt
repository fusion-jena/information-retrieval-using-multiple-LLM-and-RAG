Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.2. Data 

The data used here was provided as part of the EarthNet2021 Chal-
lenge and consists of 23,904 training datacubes located across Europe

4.3. The prediction task

3. Results 

3.1. Model training efficiency 

Model  training  on  our  hardware  (NVIDIA  GTX  1080  GPU)  took 
~460  h.  The  optimization  was  stopped  once  the  validation  ENS 
repeatedly  failed  to  improve  compared  to  the  score  evaluated  from 
previous  epochs.  The  model  at  the  epoch  with  the  highest  attained 
validation  score  was  selected.  We  noted  a  significant  acceleration  of 
convergence  when  employing  the  baseline  framework,  as  shown  in

topography information are used as model inputs for past and present 
time  steps  and  guide  predictions  for  future  time  steps.  Datacubes  are 
divided between a set used for model training and four distinct sets for 
testing, as defined by EarthNet2021 (see Section 2.5).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
According to the given context, the data is divided into a set used for model training and four distinct sets for testing. This division is defined by EarthNet2021, but the specific criteria used to split the data are not explicitly mentioned in the provided text. However, it can be inferred that the splitting process aims to create separate datasets for different stages of the machine learning pipeline, such as training, validation, and testing. These splits help ensure that the model generalizes well to unseen data and prevent overfitting during training.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

hyperparameters  (i.e.,  ‘AutoML’;  He  et  al.,  2021).  This  represents  an 
important  advantage  for  non-experts  in  deep  learning,  as  it  does  not 
require  the  manual  assembly  of  the  models  and  definition  of  their 
hyperparameters. The AutoML procedure starts by generating a set of 
candidate models with architectures and hyperparameters (e.g. number 
of layers; learning rate) selected at random from a prespecified range of 
values (see Fig. 2). Each candidate model is trained using a small subset 
of the data (data partition At; Fig. 2) during a small number of epochs. 
After  training,  the  performance  of  the  candidate  models  is  compared 
using a left-out validation data set (Av; Fig. 2). The selected candidate 
model (usually the best performing among candidates) is then trained on 
the full training data (Bt; Fig. 2). In this step it is required to identify an 
optimal number of training epochs, to avoid under- or overfitting of the

0.91 

Architecture 

Case study 2 

ResNet 
InceptionTime 
LSTM 
CNN 
ResNet 
LSTM 
InceptionTime 
CNN 
CNN 
ResNet 
InceptionTime 
LSTM 
ResNet 
CNN 
InceptionTime 
LSTM 
CNN 
InceptionTime 
LSTM 
ResNet 

Accuracy of 
candidate 
models (% 
correct) 

AUC of 
selected 
model 

Architecture 

Accuracy of 
candidate 
models (% 
correct) 

0.67  
0.63  
0.52  
0.82 
0.61  
0.52  
0.67  
0.52  
0.48  
0.67  
0.66  
0.52  
0.66  
0.77  
0.67  
0.52  
0.52  
0.75  
0.52  
0.7  

0.95 

Case study 3 

ResNet 
InceptionTime 
LSTM 
CNN 
ResNet 
LSTM 
InceptionTime 
CNN 
CNN 
ResNet 
InceptionTime 
LSTM 
ResNet 
CNN 
InceptionTime 
LSTM 
CNN 
InceptionTime 
LSTM 
ResNet 

0.5  
0.81 
0.8  
0.69  
0.5  
0.81  
0.76  
0.51  
0.5  
0.49  
0.5  
0.75  
0.5  
0.66  
0.5  
0.61  
0.5  
0.51  
0.77  
0.5

Table 1 
Type of architecture and accuracy of candidate models and predictive performance of selected models (bold). The accuracy of candidate models was measured using 
the proportion of correctly classified cases. The accuracy of selected models was measured using the area under the receiver operating characteristic curve (AUC).  

Candidate 
model 

Architecture 

Accuracy of 
candidate 
models (% 
correct) 

AUC of 
selected 
model 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 

Case study 1 

ResNet 
InceptionTime 
LSTM 
CNN 
ResNet 
LSTM 
InceptionTime 
CNN 
CNN 
ResNet 
InceptionTime 
LSTM 
ResNet 
CNN 
InceptionTime 
LSTM 
CNN 
InceptionTime 
LSTM 
ResNet 

0.32  
0.66  
0.31  
0.7  
0.46  
0.32  
0.57  
0.68  
0.39  
0.32  
0.32  
0.39  
0.32  
0.49  
0.85 
0.39  
0.6  
0.79  
0.36  
0.42  

0.96 

AUC of 
selected 
model  

0.91 

Architecture 

Case study 2

future direction for understanding deep learning models in the context of sequential 
data. arXiv, 2004.12524.  

Siddiqui, S.A., Mercier, D., Munir, M., Dengel, A., Ahmed, S., 2019. Tsviz: 

demystification of deep learning models for time-series analysis. IEEE Access 7, 
67027–67040. 

Menardi, G., Torelli, N., 2014. Training and assessing classification rules with 

Thompson, N., Greenewald, K., Lee, K., Manso, G., 2020. The computational limits of 

imbalanced data. Data Min. Knowl. Discov. 28, 92–122. https://doi.org/10.1007/ 
s10618-012-0295-5. 

Olden, J.D., Lawler, J.J., Poff, N.L., 2008. Machine learning methods without tears: a 
primer for ecologists. Q. Rev. Biol. 83, 171–193. https://doi.org/10.1086/587826. 
Palomo, L.J., Gisbert, J., Blanco, J.C., 2007. Atlas y libro rojo de los mamíferos terrestres 

de Espa˜na. Organismo Aut´onomo de Parques Nacionales Madrid. 

Peters, D.P.C., Havstad, K.M., Cushing, J., Tweedie, C., Fuentes, O., Villanueva-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: Before training a deep learning model, several preprocessing steps can be applied to improve its performance. These may include:

1. Normalization: This involves rescaling the input features so they have a mean of zero and standard deviation of one. It helps to speed up the learning process and prevent any single feature from dominating the others due to its scale.

2. Scaling: Similar to normalization, scaling adjusts the values of numeric columns to a common scale, but instead of centering around zero, it scales them between a specific range (e.g., 0-1 or -1 to 1). This is particularly useful when dealing with different units of measurement.

3. Cleaning: This refers to handling missing values, outliers, and noisy data. Missing values can be replaced with statistical measures like mean, median, mode, or imputation techniques. Outliers can be detected and removed if necessary, while noise reduction can be achieved through various filtering techniques.

4. Encoding: For categorical variables, encoding converts textual categories into numerical form. Common encoding techniques include label encoding, one-hot encoding, and ordinal encoding.

5. Feature Selection: Selecting relevant features can help reduce dimensionality, decrease training times, and improve overall model performance. Techniques such as correlation matrix, mutual information, chi-square test, or recursive feature elimination can be used for feature selection.

6. Augmentation: Data augmentation generates new samples by applying transformations to existing ones. This technique can increase the size of the dataset and introduce more variability, helping the model generalize better.
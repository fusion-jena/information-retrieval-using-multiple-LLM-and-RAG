Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics72(2022)10181710BiofilmBubblesClearLow LightOverexposureTurbidityBiofilm2355.23.2132.8Bubbles1.2242101.84Clear2.60.4244.210.81Low Light20.20.6246.201Overexposure2.60.41.80243.81.4Turbidity0.42.80.40.40.8245.2Predicted LabelsTrue LabelsBiofilmBubblesClearLow LightOverexposureTurbidityBiofilm23735032Bubbles22400026Clear30244111Low Light30124600Overexposure00002500Turbidity02102245Predicted LabelsTrue Labels(a)(b)J. Soom et al.                                                                                                                                                                                                                                    

Table 5 
Results of individual conditions tested with frame difference and scanlines.  

Metric 

Condition: Clear condition 

Condition: Low Light condition 

Condition: Air bubbles 

Frame Difference 

Scanlines 

Frame Difference 

Scanlines 

Frame Difference 

Scanlines

VGG16 (also referred to as the OxfordNet) (Simonyan and Zisserman, 
2014)  CNN architecture and  was selected primarily due to its  perfor-
mance on the ImageNet (Deng et al., 2009) dataset.

Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., 2009. Imagenet: A large-scale 
hierarchical image database. In: 2009 IEEE Conference on Computer Vision and 
Pattern Recognition, pp. 248–255. https://doi.org/10.1109/CVPR.2009.5206848. 
Dwivedi, A.K., 2021. Role of digital technology in freshwater biodiversity monitoring 

through citizen science during covid-19 pandemic. River Res. Appl. 37, 1025–1031. 
https://doi.org/10.1002/rra.3820. 

Ellenfeld, M., Moosbauer, S., Cardenes, R., Klauck, U., Teutsch, M., 2021. Deep fusion of 
appearance and frame differencing for motion segmentation. In: 2021 IEEE/CVF 
Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 
pp. 4334–4344. https://doi.org/10.1109/CVPRW53098.2021.00490.

organisms. Mar. Technol. Soc. J. 29, 10–22. 

He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition. In: 

2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 
pp. 770–778. https://doi.org/10.1109/CVPR.2016.90. 

Hern´andez-Ontiveros, J., Inzunza-Gonz´alez, E., García-Guerrero, E., L´opez-Bonilla, O., 
Infante-Prieto, S., C´ardenas-Valdez, J., Tlelo-Cuautle, E., 2018. Development and 
implementation of a fish counter by using an embedded system. Comput. Electron. 
Agric. 145, 53–62. https://doi.org/10.1016/j.compag.2017.12.023. 

Hu, J., Zhao, D., Zhang, Y., Zhou, C., Chen, W., 2021. Real-time nondestructive fish 

behavior detecting in mixed polyculture system using deep-learning and low-cost 
devices. Expert Syst. Appl. 178, 115051 https://doi.org/10.1016/j. 
eswa.2021.115051.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The information regarding the data repository link of the deep learning pipeline is not explicitly mentioned in the given context. Therefore, it is not possible to provide a helpful answer based solely on this information. It would be necessary to have access to additional resources or details about the study to determine where the deep learning pipeline has been stored and made available.
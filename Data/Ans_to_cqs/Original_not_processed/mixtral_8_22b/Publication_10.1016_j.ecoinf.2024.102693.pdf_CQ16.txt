Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1 
General transfer training hyper-parameters for each basic model.  

Hyper-parameter / Model 

ResNet-50 

ViT-S/16 

Volo-d1 

ViP-Small/7 

Learning rate 
Minimum learning rate 
Optimizer 
Scheduler 
Batch size 
Weight decay 
Input size 
Epochs 

1e-4 
2e-3 
1e-5 
1e-5 
Adamw(0.9, 0.999) 
Cosine 
128 
5e-4 
224 Ã— 224 
210  

64 
5e-4 

8e-6 
4e-6 

64 
1e-8 

2e-3 
1e-5 

64 
5e-2 

EcologicalInformatics82(2024)1026936M. Chen et al.                                                                                                                                                                                                                                   

Table 2 
Classification performance (%) of basic models and proposed methods on the 
IP102 dataset.  

which they are applicable. 

5.1. Ablation experiments 

Model 

Accuracy 

Precision 

Recall 

F1-score 

Resnet-50 
ViT-S/16 
Volo-d1 
ViP-Small/7 
VecEnsemble (Ours) 
MatEnsemble (Ours)

be  O

to 

)

(cid:0)

(cid:0)

)

Nnm2 + nm3

, respectively. Here, N represents the number of samples 
O
in the validation set, n is the number of classes, and m is the number of 
basic models.

Val. 

76.21 
77.73 
74.88 
77.45 
75.60 
76.93 
77.81 
75.93 
77.22 
77.21 
77.36 

Test 

76.10 
77.17 
74.92 
77.01 
75.48 
76.60 
77.24 
76.03 
76.89 
76.89 
77.10 

Val. 

76.16 
77.38 
75.31 
77.58 
75.99 
77.40 
77.92 
76.29 
77.53 
77.85 
77.98 

Test 

76.00 
77.05 
75.30 
77.13 
75.64 
76.94 
77.32 
76.15 
77.23 
77.24 
77.39 

Val. 

76.57 
77.78 
75.43 
78.02 
76.19 
77.81 
78.53 
76.66 
78.09 
78.25 
78.68 

Test 

76.03 
77.08 
75.07 
77.12 
75.43 
76.99 
77.35 
76.03 
77.13 
77.07 
77.25  

Fig. 4. Results of ensemble methods with different numbers of basic models.

74.57 
75.13 
76.20 
73.41 
77.39 
77.25 

70.67 
70.11 
71.30 
69.53 
73.20 
72.74 

66.48 
67.32 
67.46 
65.07 
69.05 
68.71 

68.09 
68.41 
68.68 
66.87 
70.61 
70.22  

Table 3 
Classification performance (%) of the proposed methods and other works on the 
IP102 dataset.  

Name 

Method 

Accuracy 

Precision 

Recall 

F1- 
score 

40.1 

/ 

/ 

/ 

Wu et al. 
(2019) 
Yang et al. 
(2021) 

Liu et al. 
(2022) 
Peng and 
Wang 
(2022) 
An et al. 
(2023) 

Zheng 
et al. 
(2023) 

Wang 

et al. 
(2023) 
Lin et al. 
(2023) 
Ayan et al. 
(2020) 
Ung et al. 
(2021) 

Su et al. 
(2023) 

Ours 

ResNet-50 

CNN + Spatial and 
channel attention 
mechanism 
ViT-B/16 + FRCF +
LSMAE 
CNN and Transformer 
hybrid architecture 

49.4 

73.29 

74.69 

74.58 

/ 

/ 

/ 

/ 

/ 

/ 

/ 

/ 

Feature fusion network 

65.60 

60.9 

59.7 

60.3 

PCNet 

AA-Trans (Backbone: 
ViT-B/16)

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The table titled "Table 1 General transfer training hyper-parameters for each basic model" provides information about various hyperparameters used during the training process for several models such as ResNet-50, ViT-S/16, Volo-d1, and ViP-Small/7. However, it does not explicitly mention any specific criteria or metrics used to determine when the training should be stopped.

Typically, there are multiple ways to decide when a machine learning model has been trained sufficiently. Some common criteria include observing a plateau in the validation loss, reaching a predefined maximum number of epochs, or achieving a target accuracy level on the validation data. In this case, since we do not have explicit information regarding these aspects from Table 1, we cannot definitively state what criteria were used to determine when training was complete.

However, based on general practices in deep learning, one possible criterion could be the use of a cosine scheduler mentioned under the 'Scheduler' column for all models. This type of scheduler gradually reduces the learning rate over time, which can help prevent overfitting and improve convergence. Additionally, the 'Epochs' column indicates that all models were trained for 210 epochs, suggesting that a fixed number of iterations might also have been employed as a stopping criterion.
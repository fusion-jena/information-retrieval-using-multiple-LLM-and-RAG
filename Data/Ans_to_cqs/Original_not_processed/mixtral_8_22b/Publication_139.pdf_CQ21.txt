Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.4.1. Training database assembly 

To assemble a training database, three randomly selected days from 
each month of the calendar year (see Table 1) were selected for anno-
tation. Select spectrogram images were manually labeled via visual in-
spection by two trained graduate students into a set of sound categories 
that evolved as the year progressed. On average, around 60 images could 
be  annotated  per  minute,  which  corresponds  to  8  min  of  audio  data. 
Therefore,  annotating  every  single  spectrogram  from  a  single  day 
(10,800 images) would take 3 h. Assembling the training database took 
less time in practice because only 10–15% of the spectrograms from each 
of the selected days were actually annotated (many spectrograms which 
contained only background noise were simply not needed). 

In total, this database consists of over 40 k images of bio-, anthro- 
and  geophonies.  The  full  contents  of  the  database  are  presented  in 
Table 2.

2.6. Generating prediction record 

After  neural  network  training  and  validation,  predictions  for  the 
entire  unvalidated  recording  period  were  generated.  Specifically,  for 
each of the five training folds, a single prediction for every 8 s-interval 
was made for the duration of the year. The five prediction records that 
resulted were then averaged together to create a single prediction re-
cord.  To  summarize  this  data  more  succinctly,  these  8  s-interval  pre-
dictions were then aggregated into hourly prediction records, containing 
the number of instances that each particular sound stimuli was predicted 
per  hour.  This  record  was  normalized  by  the  number  of  8-s  intervals 
actually recorded during that hour, to account for the possibility that 
missing audio data would render a given prediction record artificially 
low. 

2.7. Collecting weather data

3. Results 

3.1. Training database performance 

The average unweighted and category-weighted training classifica-
tion  accuracies  are  98.9%  and  97.4%,  respectively.  For  the  sake  of 
brevity,  the  training  confusion  matrix  is  not  produced,  but  the  test 
confusion matrix for all 19 sound classes is shown in Table 4. For the test 
data, the average unweighted and weighted accuracies are 94.7% and 
88.8%. The F1-scores for the training and testing datasets, respectively, 
are.980 and.907 (Chinchor, 1992). 

Slightly  lower  testing  accuracy  scores  are  seen  in  categories  like 
background noise, which potentially contain a very faint sound occur-
ring in one of the other categories that is not readily detected by the 
human eye, or other such class ambiguities. Additionally, lower scores 
are seen for smaller classes with a fewer number of training examples, 
like the squirrel “kuk” and the white-breasted nuthatch call.

(cid:0) 4 and a decay of 1×10

The  optimizer  Adam  was  selected  with  an  initial  learning  rate  of 
(cid:0) 7 and early stopping was used to prevent 
1×10
overfitting  (Kingma  and  Ba,  2014).  Typically,  the  model  trained  be-
tween 10 and 15 epochs before training ceased, where each epoch took 
roughly 1 min on an NVIDIA GeForce RTX 2080 Ti GPU. In addition to 
the conventional accuracy metric, the area under the curve, as computed 
by  a  Riemann  sum,  was  also  used  for  training.  From  the  assembled 
database of annotated clips, in each of the five cross-validation folds, 
10% of the data was reserved for comparing the training and validation 
loss/accuracy  after  each  epoch,  and  a  further  10%  was  reserved  for 
performance testing after all training had ceased. All results reported are 
the average after five-fold cross-validation.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: After the neural network training and validation process, several postprocessing steps were taken to generate predictions and analyze the model's performance. These steps include generating a prediction record, collecting weather data, and evaluating the model using various metrics.

First, predictions for the entire unvalidated recording period were generated. For each of the five training folds, a single prediction was made for every 8-second interval throughout the year. The resulting five prediction records were then averaged together to create a single prediction record. To provide a more concise summary of the data, these 8-second interval predictions were aggregated into hourly prediction records, indicating the number of instances each specific sound stimulus was predicted per hour. This record was normalized by the number of 8-second intervals actually recorded during that hour to account for missing audio data.

Next, weather data was collected to complement the acoustic data analysis. Although no details about the collection method or type of weather data gathered are mentioned, it can be assumed that this information was integrated into the study to examine any potential correlations between environmental conditions and sound events.

Finally, the model's performance was evaluated using various metrics. The average unweighted and category-weighted training classification accuracies were found to be 98.9% and 97.4%, respectively. However, slightly lower testing accuracy scores were observed in certain categories due to factors such as faint sounds, class ambiguity, and small sample sizes. The F1-scores for the training and testing datasets were.980 and.907, respectively. Besides the conventional accuracy metric, the area under the curve was also calculated using a Riemann sum. It should be noted that the training and validation loss/accuracy were compared after each epoch, and a separate dataset was reserved for performance testing after all training had ceased. All reported results represent the average after five-fold cross-validation.
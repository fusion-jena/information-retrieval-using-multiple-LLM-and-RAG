Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We expected higher performance of the embeddings, but a possible 
limitation to our study may be the use of pre-trained CNNs. This may 
have limited the potential performance of using embedding from neural 
networks.  Interestingly,  higher  classification  accuracy was  seen  when 
using  BirdNET,  a  CNN  trained  on  audio  data  of  bird  vocalizations  in 
comparison  with  VGGish  and  Wav2Vec  2.0,  two  models  trained 
respectively on a vast Youtube audio dataset and a human speech audio 
dataset. This suggests that training a model with data input of gibbon 
vocalizations might result in an even better performance of neural net-
works for individual gibbon discrimination. However, such data training 
requires a large dataset and is very computationally costly, hence why 
we chose to test pre-trained CNNs.

contribute  to  larger  data  input  for  future  training  of  deep  learning 
models (Best  et  al., 2023).  Accordingly, the  rapid growth of  artificial 
intelligence  and  deep  learning  methods  could  bring  about  important 
innovations related to feature extraction and classification with the aim 
of overcoming the current challenges halting the use of PAM for indi-
vidual acoustic discrimination and succeeding in a sustained and non- 
invasive monitoring of animal populations.

The second approach we used to extract embeddings was from the 
pre-trained  BirdNET  model.  The  original  version  of  the  model  was

We used three different deep-learning architectures to extract em-
beddings from the female calls. The first that we used, VGGish, is a CNN 
based upon Configuration A of the VGG image classification model with 
11 weight layers (Simonyan, 2015). VGGish was trained by Google to 
perform  general-purpose  audio  classification  using  a  preliminary 
version of the YouTube-8 M dataset (Abu-El-Haija et al., 2016). Once 
trained,  the  final  classification  layer  was  removed  from  the  network, 
leaving  a  128-dimensional  acoustic  feature  embedding  as  the  CNN 
output. To input the sound files into the VGGish model, we converted 
the  waveforms  to  Mel-spectrograms  (Rabiner  and  Schafer,  2010),  a 
signal representation derived from the spectrogram that substitutes the 
normal frequency scale with the nonlinear Mel scale, taken from 20th- 
century  psychoacoustics  studies  that  explored  the  relationship  be-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In this case, the authors used three different deep-learning architectures to extract embeddings from the female calls. The first one mentioned is VGGish, which was trained by Google to perform general-purpose audio classification using a preliminary version of the YouTube-8M dataset. Another model used is Wav2Vec 2.0, although it's not explicitly stated what dataset it was trained on; however, it can be assumed that it was trained on a human speech audio dataset since it's designed for automatic speech recognition tasks. Lastly, they also used the BirdNET model, which was specifically trained on audio data of bird vocalizations. Therefore, the datasets used in their deep learning pipeline include a preliminary version of the YouTube-8M dataset, a human speech audio dataset, and a dataset containing bird vocalization sounds.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

to get blurred (as information over an increasingly larger receptive
field is mixed through repeated convolutions). We visualize our model
architecture in Fig. 5 and in the following, the data flow in the model
is described in more detail.

4.5. Implementation details

We have implemented our models in PyTorch (Paszke et al., 2017).
We trained ğ‘€ = 5 models with batch size ğµ = 64 and a base learning
rate ğ›¼ = 10âˆ’4. The learning rate is automatically reduced by a factor
of 0.1 when the validation loss has not improved for 15 consecutive
epochs. We apply weight decay to control the strength of the unit
Gaussian prior, with an empirically chosen magnitude of 10âˆ’3 that
is inversely proportional to the hyperparameter ğœ† from Eq. (1). We
chose ğ›½1 = 0.9, ğ›½2 = 0.999 and ğœ– = 10âˆ’8 as hyper-parameters for the
Adam optimizer. Each neural network was trained on a single Nvidia
RTX2080Ti GPU for âˆ¼14 days.

5. Experimental results and discussion

Fig. 5. Proposed model architecture. Convolutional layers are shown in cyan, with the kernel size and number of output channels in brackets. Batch normalization layers
are in lime, ReLU activations in yellow and grouped convolutions in purple, with an additional argument for the number of groups. âŠ• and âŠ™ denote element-wise addition,
respectively concatenation along channel dimension, branching arrows indicate sharing (â€˜â€˜copyingâ€™â€™) of the respective tensor. In our experiments we set ğ‘µ blocks = [2, 3, 5, 3] and
ğ‘µ channels = [256, 512, 1024, 2048].

(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

variables, and of their associated uncertainties. A CNN gradually trans-
forms its inputs into outputs with a series of simple transformations
(layers). One can think of the early layers as a pre-processing of the
two input modalities, the middle layers as a feature extractor and the
last layers as a regression, but there is no clear distinction between
those parts. At the heart of all of them is the (discrete) convolution
operator, i.e., a linear filtering of a ğ‘-channel input image with a ğ‘˜Ã—ğ‘˜Ã—ğ‘
kernel of weights. In each convolutional layer, multiple convolutions
with different filter weights are applied, so the output is again an
â€˜â€˜imageâ€™â€™ with as many channels as there were filters. The filter weights
are the trainable parameters of the model. Convolutional layers are
interleaved with some element-wise, non-linear activation functions.
The series of transformations gradually abstracts the bare image pixels
into a sequence of feature maps until, at the output level, they have

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
A Convolutional Neural Network (CNN) is used in the proposed model architecture. This is evident from the description of the model implementation details and the visual representation of the model architecture in Figure 5. The use of convolutional layers, denoted in cyan, indicates the application of CNN principles. Furthermore, the mention of kernel sizes and output channels supports this conclusion. Additionally, the explanation of how a CNN gradually transforms its inputs into outputs using a series of simple transformations aligns with the characteristics of a CNN.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

libraries	 such	 as	 TensorFlow	 (Abadi	 et	al.,	 2016).	 Currently,	 the	

Raw count data are available from Enlighten: Research Data (https://

doi.org//10.5525/gla.researchdata.732).  Source  code  is  available 

from http://dx.doi.orgI10.5281/zenodo.2562058.

greatest challenge for implementing these algorithms for bespoke 

O R C I D 

applications is obtaining sufficiently large training datasets. In this 

regard,  citizen  scientists  have  a  clear  role  to  play.  While  we  have 

shown that the trained algorithm achieves high accuracy levels, it 

Colin J. Torney 

 https://orcid.org/0000-0003-1673-7835  

should  be  noted  that  the  algorithm  employed  the  crowd- sourced 

data  to  create  the  training  sets.  Hence,  both  methods  should  be 

R E F E R E N C E S

viewed  as  complementary  approaches  with  citizen  science  data 

forming the foundation for automated algorithms (Rey et al., 2017).

science counts of the survey and comparison to expert counts.

tee  that  the  approach  is  transferable  and  how  to  appropriately 

4 |  D I S CU S S I O N

filter the data may be affected by the wording of the guidelines, 

the image resolution and sizes used, or the set of volunteers that 

participate  in  the  project.  Other  more  sophisticated  approaches 

to processing citizen science data have been proposed (Swanson 

From our results, we see that both citizen science and deep learn-

et al., 2016); however, given the range of counts provided by the 

ing methods are capable of producing highly accurate image counts. 

volunteers  and  the  large  errors  we  observe  in  the  baseline  met-

Counting the wildebeest within the survey images is a difficult and 

rics  (c.  11%  and c.  9%  undercount  for  the  mean  and  median,  re-

time- consuming task. When collecting the census images, there are

parallel  computing  power  provided  by  graphical  processing  units 

tion professionals to focus on other tasks. Two novel methods can 

(GPUs)  and  accessibility  of  image  training  datasets,  such  as  the 

potentially replace the use of manual counts by experts, the deploy-

COCO  dataset  (Lin  et al.,  2014),  have  led  to  rapid  improvements 

ment of citizen scientists and the use of automated object detection 

in  the  performance  of  multilayer  deep  convolutional  neural  net-

algorithms.  In  this  work,  we  deploy  both  approaches  and  evaluate 

works  (DCNNs).  These  multilayer  neural  networks  are  a  form  of 

the performance and merits of each.

deep learning and are distinct to traditional machine learning ap-

1.1 | Citizen science and the wisdom of crowds

proaches to computer vision in that no hand- crafted features are 

required;  instead,  the  convolutional  layers  extract  relevant  fea-

Our  results  show  that  deep  learning  algorithms  are  now  at  a 

state where they can legitimately replace manual counters and re-

move  a  large  burden  from  conservation  organisations.  The  further 

great advantage of automated image processing is that it will allow 

us to leverage emerging image collection technologies, such as un-

manned  aerial  vehicles,  satellite  platforms  or  fixed  camera  traps; 

coupling  these  advances  in  image  collection  tools  with  automated 

processing  will  greatly  increase  the  accuracy  of  population  esti-

mates.	As	we	move	towards	fully	automated	wildlife	counts,	it	only	

remains  to  ensure  the  availability  of  sufficient  training  data  that  is 

representative of all potential survey images. This can be achieved 

by combining state- of- the- art deep learning methods with validated 

crowd- sourced training data.

An	automated	wildebeest	image	count	will	not	only	be	a	signifi-

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific process followed to deploy the trained deep learning model, including details like model serialization or platform selection. However, based on common practices in deploying deep learning models, one could infer that after training the model using libraries such as TensorFlow, the model might have been serialized into a format suitable for deployment, such as a.h5 file for Keras models or a.pb file for TensorFlow models. As for platform selection, depending on the application requirements, the model could have been deployed on various platforms, ranging from cloud services like AWS, Google Cloud, or Azure to edge devices like mobile phones or embedded systems. Nevertheless, without concrete evidence from the provided context, these assumptions remain speculative.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

dataset  increased  by  50%  after  this  data  augmentation,  which  is 
important for small training datasets such as ours.

ence data available for training were limited, so we sought to explore 
different network sizes in conjunction with pre-training with additional 
acoustic reference data from outside of our audio domain, followed by 
fine-tuning with ROI data from our study site.

Pouyanfar, S., Sadiq, S., Yan, Y., Tian, H., Tao, Y., Reyes, M.P., Shyu, M.L., Chen, S.C., 
Iyengar, S.S., 2018 Sep 18. A survey on deep learning: algorithms, techniques, and 
applications. ACM Comput. Surv. (CSUR). 51 (5), 1–36. https://doi.org/10.1145/ 
3234150. 

Priyadarshani, N., Marsland, S., Castro, I., 2018 May. Automated birdsong recognition in 

complex acoustic environments: a review. J. Avian Biol. 49 (5), jav–01447. 
Pyle, P., DeSante, D.F., 2003. Four-letter and six-letter alpha codes for birds recorded 
from the American Ornithologist’s union check-list area. North American Bird 
Bander. 28 (2), 64–79. 

Quinn, C.A., Burns, P., Gill, G., Baligar, S., Snyder, R.L., Salas, L., Goetz, S.J., Clark, M.L., 
2022 May 1. Soundscape classification with convolutional neural networks reveals 
temporal and geographic patterns in ecoacoustic data. Ecol. Indic. 138, 108831. 

Roca, I.T., Desrochers, L., Giacomazzo, M., Bertolo, A., Bolduc, P., Deschesnes, R.,

The  recordings  for  each  ROI  were  cut  using  start/stop  times  and 
sequentially appended to create a single merged .wav file for each spe-
cies and set (i.e., train, validation) combination. Each merged .wav file 
was  then  sliced  into  2-s  increments  and  mel-spectrograms  computed. 
Note that since there were some ROIs from the pattern matching that 
were >2 s, some 2-s recording clips included parts from different ROIs. 
Performing data augmentation on training datasets is a widely used 
technique in deep learning, including sound classification applications, 
to  regularize  models  and  provide  more  robust  features  (Salamon  and 
Bello, 2017). We performed data augmentation by splitting the merged . 
wav  training  file  for  each  species  in  half  and  then  adding  these  two 
halves sample-wise. This augmented .wav file was similarly split into 2-s 
increments as above and mel-spectrograms were computed. The training

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In addressing data bias during preprocessing of the deep learning pipeline, several techniques can be employed. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into subsets or strata based on certain criteria, ensuring that each subset maintains the same distribution of classes as the original dataset. This technique helps prevent class imbalance issues when partitioning the dataset into training and testing sets.

Oversampling is another method used to handle class imbalances in datasets. It works by duplicating examples from the minority class until it reaches a balance with the majority class. Synthetic Minority Over-sampling Technique (SMOTE) is one popular approach for generating synthetic samples instead of simply replicating existing ones.

Undersampling, conversely, reduces the number of instances in the majority class to match the size of the minority class. Random under-sampling is a simple way to achieve this, but it may lead to loss of valuable information. More sophisticated methods like NearMiss and Condensed Nearest Neighbor (CNN) rule help maintain important characteristics while reducing the majority class size.

Diverse data collection aims to gather a wide range of representative samples to ensure that the model generalizes well across various scenarios. In the case of limited availability of ence data for training, researchers might seek out additional acoustic reference data from other domains and fine-tune their models with specific region-of-interest (ROI) data.

Data augmentation is also commonly applied to increase the amount of training data artificially. By applying transformations such as rotation, scaling, flipping, or cropping, new variations of existing samples are generated. For sound classification tasks, data augmentation could involve splitting merged wave files and adding them together sample-wise, creating new combinations of sounds.

These techniques collectively contribute to mitigating data biases and improving the performance of deep learning models. However, selecting the appropriate strategy depends on factors such as the nature of the dataset, the problem being addressed, and the computational resources available.
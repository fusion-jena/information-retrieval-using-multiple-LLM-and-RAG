Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

directly  to  datasets  where  the  relevance  of  annotations  has  not  been 
evaluated.

Acknowledgments 

The authors acknowledge all the annotators that gave their time in 
this manual annotation of our datasets. We particularly mention Julie 
Saidlitz  from  Astrolabe  Expeditions1  for  driving  with  us  the  practical 
aspects of the annotation campaigns. The authors acknowledge the Ple 
de Calcul et de Donn´ees Marines5 (PCDM) for providing DATARMOR 
(storage,  data  access,  computational  resources,  visualization,  web- 
services, consultation, support services).  

Appendix A. Appendix 

A.1. Number of annotations per annotator and per label

Fig. 10. Number of positive annotations per annotator and per label.  

EcologicalInformatics81(2024)10264210G. Dubus et al.

During  the  campaign,  methods  are  proposed  to  assess  inter- 
annotator  variability  and  aggregate  annotation  sets.  Those  methods 
will help to define the minimal number of annotators needed [Dubus 
et al., 2023; Walter et al., 2022]. Increasing the number of annotators 
per sample could be helpful if the variability is too high. Finally, the two 
grouping  methods:  majority  voting  (Section  2.2.3)  and  soft  labeling 
(Section 2.3.2) can also be used to produce relevant annotation sets for 
deep learning. 

6. Conclusion 

In this study, new approaches were explored to improve the accuracy 
and  efficiency  of  cetacean  vocalization  detection  using  deep  learning 
models. Leveraging multi-annotation campaigns involving both expert 
and novice annotators aimed to address the challenges associated with 
the scarcity of ground truth data in underwater PAM studies.

In this kind of project, volunteer citizen scientists give free time to 
explore  and  annotate  large  datasets.  Contact  with  the  annotator  is 
extremely important: (i) proposing a clear explanation about the aim of 
the  annotation  campaign,  (ii)  answering  questions,  (iii)  organizing 
meetings throughout the campaign and share a report after. Those steps 
motivate the annotators and increase the educational potential of the 
campaign.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't enough information available to provide a direct link to the data repository of the deep learning pipeline mentioned in the given context. It would require more specific details or references to find the exact location of the dataset. However, it is stated that the authors acknowledge the Ple de Calcul et de Données Marines (PCDM) for providing DATARMOR, which includes storage, data access, computational resources, visualization, web-services, consultation, and support services. This suggests that the data might be accessible through DATARMOR, but further research or contact with the authors would be necessary to confirm this.
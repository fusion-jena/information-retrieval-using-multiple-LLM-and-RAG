Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

structural  elements  i.e.,  number  of  artificial 
neurons  which  is  far  better  compared  to 
traditional  feedforward  neural  networks. 
Figure 3 shows the structure of CNN that can 
be  used  in  many  applications.  For  image 
recognition  applications,  several  baseline 
architectures of CNNs have been developed, 
which  have  been  successfully  applied  to 
complicated  tasks  of  visual  imagery.  The 
common CNN architectures that are largely 
used in image data recognition includes: (i) 
AlexNet  (Krizhevsky  et  al.  2012),  (ii) 
PredNet (Banzi et al. 2019), (iii) GoogLeNet 
(Szegedy  et  al.  2015), 
(iv)  Overfeat 
(Sermanet  et  al.  2014),  and  (v)  VGG 
(Simonyan  et  al.  2014).  These  models  and 
their training and testing processes, are freely 
available for transfer learning purpose.  

Figure 3: The Structure Convolutional Neural Network. 

96

In  this  work,  a  combined  architecture  is 
proposed  where  by  the  CNN  layer  is 
embedded with the long short-term (LSTM) 
memory  layer  to  pass  those  significant 
features extracted in the initial layers to the 
deeper layers of the network. This supports 
effective  aggregation  of  feature  maps  for 
precise classification and ultimately accurate 
recognition.

Generally, the proposed network system has 
20  layers:  12  convolutional  layers,  five 
pooling  layers,  one  Fully  Connected  layer, 
one LSTM layer, and one output layer with 
the  softmax  function  for  classification.  The 
convolutional  layer  with  a  size  of  3  ×  3 
kernels is used for extracting feature and it is 
activated  by  the  ReLU  function.  The  max-
pooling layer with a size of 2 × 2 kernels is 
applied to reduce the dimensions of an input 
image. In the last part of the architecture, the 
function map is shifted to the LSTM layer to 
for 
information 
extract 
classification  purpose.  Using  the  reshape 
method,  the  input  size  of  the  LSTM  layer 
becomes (196,512). After analyzing the time 
features,  the  model  sorts  the  plant  species 
images  through  a  fully  connected  layer  to 

required 

time 

predict whether they belong under any of the 
100 categories of the plant species. 

Experimental set up

appear  in  controlled  backlit  and  front-lit 
versions,  with  several  samples  per  species. 
They vary considerably in sharpness, noise, 
illumination patterns, and shadows. Since the 
dataset is not large enough for deep learning 
to overcome the problem of overfitting, data 
augmentation was used to enlarge the dataset 
training  and  overcoming 
for  network 
overfitting. The dataset used in this work can 
be  downloaded  at  UCI  Machine  Learning 
Repository:  One-hundred  plant  species 
leaves  data  set  Data  Set  database.  Figure  5 
shows sample of image data of 24 species in 
grey  scale  from  UCI  Machine  learning 
repository. 

Figure 4: Feature extraction from a plant leaf. 

Figure 5: A sample plant species image from UCI repository. 

97 

 
Tanzania Journal of Forestry and Nature Conservation, Vol 90, No. 3 (2021) Special Issue: 
Embracing Science and Technology in Nature Conservation. pp 93-103 

Identification of leaf pattern 

indicate

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? "

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper does not explicitly mention the use of standard datasets such as MNIST, CIFAR, or ImageNet. However, it mentions using a dataset containing images of 100 different plant species leaves. This dataset was obtained from the UCI Machine Learning Repository and was used for training and validating the proposed deep learning model. To address the issue of limited dataset size and prevent overfitting, data augmentation techniques were employed to increase the amount of available training data.

Unhelpful Answer:
I do not know what datasets were used in the deep learning pipeline.
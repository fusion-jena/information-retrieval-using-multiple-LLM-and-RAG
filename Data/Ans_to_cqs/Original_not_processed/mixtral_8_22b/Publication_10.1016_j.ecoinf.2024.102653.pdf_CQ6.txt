Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2. Methods 

Fig. 2 summarizes a series of processes in designing a deep-learning 
architecture that acts as a toxicovigilance tool. Subsequent sections will 
provide a comprehensive description of each section. 

2.1. Data collection

2.6. Performance metrics 

Conventional  metrics  were  used  to  evaluate  the  model’s  perfor-
mance, such as confusion matrices, accuracy, recall, precision, F1-score, 
Cohen  Kappa,  and  the  area under  the  ROC curve,  some of  which  are 
defined below (Johnson and Khoshgoftaar, 2019).  

Table 1 
List of hyperparameters utilized for training the Recurrent Neural Network 
model.  

Hyperparameter 

Value 

Optimizer 
Learning rate 
Batch size 
Epochs 
Dropout rate 
Loss function 

ADAM (Kingma and Ba, 2017) 
0.003 
32 
100 
0.5 
Binary Cross Entropy  

●  Confusion Matrix

2.4. Model training and hyperparameters 

The  model’s  weights  were  randomly  initialized  using  a  specific 
integer seed to guarantee consistent reproducibility of results. Further-
more, meticulous manual tuning of hyperparameters was conducted to 
achieve  optimal  model  performance.  The  selected  hyperparameters, 
which yielded the best performance metrics, are presented in Table 1. 

2.5. Model validation 

Two distinct validation methodologies were implemented. The first 
method  involved  partitioning  the dataset into  training and  validation 
sets, with 70% of the data designated for training and 30% for valida-
tion.  This  division  was  performed  through  random  selection  (Dobbin 
and Simon, 2011).

In the proposed architecture, the model employs dual BiLSTM layers 
in its structure. Following each BiLSTM layer, dropout regularization is 
incorporated to mitigate the risk of overfitting. Specifically, this regu-
larization method entails the random deactivation of 50% of the neural 
connections (Srivastava et al., 2014). After these first layers, the archi-
tecture introduces a densely connected layer comprised of 128 neurons. 
This  layer  utilizes  a  Rectified  Linear  Unit  (ReLU)  activation  function 
(Nair and Hinton, 2010). A mechanism was proposed to facilitate the 
classification  of  the  processed  sequences,  as  depicted  in  Fig.  3.  The 
model  harnessed  the  capabilities  of  the  NVIDIA  CUDA  Deep  Neural 
Network  (cuDNN)  library  to  enhance  computational  efficiency  and 
reduce training duration, as presented by Chetlur et al. (2014). 

2.4. Model training and hyperparameters

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there isn't any explicit information about the preprocessing steps involved before training the deep learning model in the given context. However, it does mention that the model uses two distinct validation methodologies, one of which involves partitioning the dataset into training and validation sets. It also mentions that the model's weights were randomly initialized using a specific integer seed to ensure consistent reproducibility of results. Furthermore, it states that meticulous manual tuning of hyperparameters was conducted to achieve optimal model performance. These hyperparameters, which yielded the best performance metrics, are presented in Table 1. Therefore, while we cannot provide specific details on the preprocessing steps, we can infer that some form of data preparation and manipulation likely occurred prior to training the model.
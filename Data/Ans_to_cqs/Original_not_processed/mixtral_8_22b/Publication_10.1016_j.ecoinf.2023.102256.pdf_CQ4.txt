Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Over  the  past  few  years,  deep  learning  has  revolutionized  several 
research  fields  such  as  bioinformatics  (Li  et  al.,  2020)  and  medicine 
(Piccialli et al., 2021) by enabling automated processing of large and 
complex  datasets.  Considered  a  branch  of  machine  learning,  deep 
learning refers to algorithms, commonly called deep neural networks, 
able  to  automatically  detect  very  complex  and  highly  discriminating 
patterns  in  data  (Chollet,  2018).  The  succession  of  processing  layers 
performing  linear  and  non-linear  transformations  allows  the  neural 
networks  to  learn  representations  of  data  with  multiple  levels  of 
abstraction  (LeCun  et  al.,  2015).  This  ability  makes  deep  learning 
particularly  relevant  for  solving  complex  problems  such  as  speech 
recognition, object detection, computer vision, and many other domains 
(Taigman et al., 2014; Hinton et al., 2012). Naturally, these practices

ment while enabling a simple CNN to be improved whilst also keeping 
the number of network parameters low (Case I: 27 654 parameters, Case

The baseline model comprised a simple CNN architecture that had 
two  convolutional layers (8  filters of  size 16 × 16,  with ReLU activa-
tions), followed by max pooling (4 × 4), a flattening operation, and two 
fully-connected layers (32 ReLU and 22 softmax units respectively) (see 
Fig. 2). Given only a spectrogram input, the baseline model produced a 
probability for each of the 22 species. 

3.3.2. Case II: One-hot encoding 

Different methods were explored and tested to enhance the neural 
network with contextual information. One straightforward approach to 
incorporate additional information into a deep learning classifier, while 
maintaining  model  simplicity,  is  to  utilize  a  multi-branch  CNN.  As  a 
result, we investigate this approach as the initial method to integrate

To train and evaluate the efficiency of each method, the bird song 
dataset was split into a training, validation and testing dataset. For each 
species and country, we randomly selected 70% of the downloaded re-
cordings for the training dataset and kept the remaining 30% for testing. 
Data  augmentation  was  only  applied  to  the  training  dataset,  and  the 
segments obtained were randomly distributed between the training and 
validation sets with a ratio 0.8/0.2. In each case, the model was trained 
for 40 epochs with a batch size of 8 segments and a learning rate of 0.001 
using the Adam optimizer. The training process was performed ten times 
to account for the effect of random weight initialization in neural net-
works. In each execution, we applied the model to the testing dataset 
and recorded the confusion matrix, number of True Positives (TP), False 
Positives (FP), True Negatives (TN) and False Negatives (FN). From that

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not provide specific details about the data augmentation techniques used in the deep learning pipeline. However, it mentions that data augmentation was applied to the training dataset. Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models without actually collecting new data. Common data augmentation techniques include fliping, rotating, scaling, cropping, zooming, shearing, and more. These techniques can help improve the performance and generalization capabilities of deep learning models.
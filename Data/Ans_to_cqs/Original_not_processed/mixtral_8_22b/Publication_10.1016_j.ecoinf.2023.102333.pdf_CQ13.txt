Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

adjustable parameters that control network architecture, optimization 
procedure,  and  regularization.  Common  hyperparameters  include 
number of layers and nodes, learning rate, dropout rate, etc. However, 
optimizing  model hyperparameters  remains imperative  for leveraging

A search space of the primary hyperparameters was defined. Opti-
mization  algorithms  were executed to  find  the  highest  accuracy from 
1,788,160  hyperparameter  combinations.  Our  model  was  trained  for 
200 epochs for each trial to find three DNN architectures that were used 
to  build  three  models  and  train  them  for  the  entire  dataset  for  1000 
epochs (Table 2).

3.1. DNN Architecture optimization 

The  optimization  process  considers  three  main  factors.  The  main 
factor is the model’s accuracy when tested using the validation dataset. 
Computation time and costs is the second deciding factor, as both can 
add up over time with the increasing complexity of models and datasets. 
Efficient  methods  for  hyperparameter  optimization  are  necessary  to 
obtain high-performing models while leading to more efficient utiliza-
tion of computational resources. Lastly, commonalities and differences 
between  hyperparameters  can  better  highlight  the  impact  of  each 
hyperparameter on the model’s performance.

This  study  adopted  an  automated  optimization  architecture  by 
comparing three architecture optimization algorithms: Random Search 
(RS), Hyperband, and Bayesian optimization. Hyperparameters of our 
DNN model were automatically optimized according to a search grid of 
several hyperparameters listed in Table 1. A dropout layer was added to 
prevent overfitting and to reduce statistical noise. The dropout regula-
rization  randomly  removed  different  sets  of  neurons  using  a  pre-
determined  dropout  rate,  which  resulted  in  training  different  neural 
networks and reduced overfitting (Srivastava et al., 2014). 

2.6.1. Random search

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In this study, several hyperparameters were considered for the deep learning model. These included the number of layers and nodes, learning rate, and dropout rate. Specifically, the optimization process involved comparing three architecture optimization algorithms: Random Search (RS), Hyperband, and Bayesian optimization. The hyperparameters of the DNN model were automatically optimized based on a search grid of various hyperparameters listed in Table 1. Additionally, a dropout layer was incorporated into the model to prevent overfitting and decrease statistical noise. This layer randomly eliminated different sets of neurons using a preset dropout rate, resulting in the training of distinct neural networks and reducing overfitting.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

filter, WQ inlets, weekly street sweeping, wet pond, and wetland detention. Bars with gray outlines and white filling represent BMPs for which efficiency data was 
not available.

driven  approaches  such  as  machine  learning.  Furthermore,  while  the 
tool provides valuable BMP decision support, its effectiveness relies on 
the accuracy of the PLET model used to estimate current pollutant loads 
in watersheds. As such, accurate data collection and consistent updating 
of  input  parameters  in  the  PLET  model  are  vital  for  maintaining  the 
relevance and usefulness of the tool. Also, atmospheric deposition is not 
currently considered in the PLET framework, despite being a significant 
source of nutrient pollution in urban environments. This absence high-
lights a limitation in our current analysis and underscores the need for 
future research to address and incorporate atmospheric deposition data 
for  a  more  comprehensive  assessment  of  pollutant  sources  in  urban 
watersheds. In addition, post-BMP loads result from the implementation 
of a single type of BMP within each HUC-12 watershed, rather than from

pollutant  loads  intersecting  with  disadvantaged  communities.  Lastly, 
the  tool  offers  post-BMP  analysis  for  nutrient  pollution  levels,

Introduction to a Natural Treasure. https://www.epa.gov/sites/default/files/201 
8-01/documents/58692_an_river_lagoon_an_introduction_to_a_natural_treasure_ 
2007.pdf. 

United States Environmental Protection Agency (USEPA). Overcoming Barriers to Green 

Infrastructure. https://www.epa.gov/green-infrastructure/overcoming-ba 
rriers-green-infrastructure#waterrights. 

United States Geological Survey. Watershed Boundary Dataset. https://www.usgs.gov/ 
national-hydrography/watershed-boundary-dataset#:~:text=The%20Watershed% 
20Boundary%20Dataset%20(WBD,portion%20of%20the%20stream%20network. 
Vaz, J., Wasserman, S., 2015. Examining environmental injustice in Florida. Atlas of 

Maine 2015 (2), 1. 

Montgomery, M.C., Chakraborty, J., 2015. Assessing the environmental justice 

Venkataramanan, V., Packman, A.I., Peters, D.R., Lopez, D., McCuskey, D.J., 

consequences of flood risk: a case study in Miami, Florida. Environ. Res. Lett. 10 (9), 
095010.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
There are several techniques used to address data bias during preprocessing of the deep learning pipeline. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into training and testing sets while preserving the percentage of samples for each class. This ensures that the distribution of classes remains similar between the two datasets, reducing any potential bias introduced by imbalanced classes.

Oversampling is a technique where additional copies of minority class examples are added to the dataset to balance it. This can be done randomly or through synthetic sample generation using methods like SMOTE (Synthetic Minority Over-sampling Technique). Oversampling helps prevent the model from becoming biased towards the majority class.

Undersampling is another approach to addressing data bias, where examples from the majority class are removed until both classes have equal representation. However, this method may lead to loss of important information if too many instances are discarded.

Diverse data collection aims to gather a wide range of representative samples to ensure that the trained model generalizes well across different scenarios. This includes collecting data from various sources, demographics, geographical locations, etc., depending on the specific application.

These techniques help mitigate data bias during preprocessing of the deep learning pipeline, ensuring fairer and more accurate predictions.
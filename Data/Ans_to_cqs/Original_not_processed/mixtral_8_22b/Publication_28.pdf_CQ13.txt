Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

1.2. Paper's contribution 

This  paper  presents  a  new  parameter  tuned  deep  learning  based 
EfficientNet model with Variational Autoencoder (PTDLEN-VAE) model 
for satellite imagery analysis on ecology management. The goal of the 
PTDLEN-VAE model is to examine the satellite images using DL model 
for  ecological  condition  monitoring.  Initially,  the  satellite  images  are 
pre-processed to improve the contrast level of the image. Besides, the 
PTDLEN based feature extractor is utilized using EfficientNet model for

displayed in Fig. 2. The presented PTDLEN-VAE model includes a series 
of operations such as pre-processing, EfficientNet based feature extrac-
tion, IKHO based parameter optimization, and VAE based classification. 
Once  the  contrast  level  of  the  input  satellite  images  is  improved,  the 
features  are  extracted  by  the  EfficientNet  model,  and  the  hyper-
parameter of the EfficientNet model is optimally adjusted by the IKHO 
algorithm.  Finally,  the  VAE  based  classification  model  is  executed  to 
assign the proper class labels to the satellite images. 

3.3. Structure of EfficientNet model

Fig. 5. Structure of VAE.  

Stochastic  gradient  descent  (SGD)  on  BP  is  managing  stochastic 
input,  then  not  stochastic  unit  within  the  networks.  The  solution  is 
named as “reparameterization trick”, which is to transfer the sampling 
to input layer. It is easy from N(μ(x), θ(x)) by sampling ∈ ~ N(0, I), af-
terward calculating pmodelz = μ(x) + θ1/2(x) * e. Where μ(x) and θ(x) are 
the mean and covariance of (z| x). So, Eq. (13) is calculated as: 

L(q) = Ee∼N(0,I)pmodel

(cid:0)

x|z = μ(x) + θ1/2(x) × ∈

)

(cid:0) DKL(q(z|x)‖pmodel(z) )

(14) 

In VAE is comprised of input layer, various AEs, and output layer. 
Then, an unsupervised pre-training step, the supervised fine-tuning step 
is implemented for learning the entire network parameters by employ-
ing the BP technique. This technique is comprised of 1 input layer, 5 
hidden layers, and 1 output layer. 

4. Performance validation

VAE initially draw an instance z from the code distribution pmodel(z). The 
sample is executed via a differentiable generator network g(z). Lastly, x 
undergo sampling from a distribution Pmodel(x; g(z)) = pmodel(x| z). At the 
time of training process, the estimated inference network (or encoder) q 
(z| x)  can  be  employed  for  obtaining  z  and  Pmodel  (x| z)  is  afterward 
considered  as  the  decoder  network  (Dai  et  al.,  2019).  The  main 
perception of VAE is that it can be trained using the maximization of the 
variational lower bound L(q) related to the data point x:

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The paper does not explicitly mention the specific hyperparameters used in the deep learning model. However, it mentions that the hyperparameters of the EfficientNet model are optimized using the IKHO algorithm. Typically, common hyperparameters in deep learning models include learning rate, batch size, number of epochs, optimizer type, etc. In this case, we cannot provide the exact values or types of these hyperparameters since they are not mentioned in the given context.
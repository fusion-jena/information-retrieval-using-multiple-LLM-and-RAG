Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1. Number of samples in the training, validation and test sets for Amazon database.

Set

Tiles

Available Def.
Samples

Available No-def.
Samples

Balanced Samples
(per Class)

Total Samples

Training

Validation

1, 7, 9, 13

5, 12

2706

963

78,431

39,697

Test

2, 3, 4, 6, 8, 10, 11, 14, 15

40,392

1,675,608

8118

2889

-

16,236

5778

1,716,000

The EF network architecture consisted of three convolutional layers (Conv) including the
Rectiﬁed Linear Unit (ReLU), two Max-pooling layers (MaxPool), and two Fully Connected layers
(FC), with a softmax layer at the end with two outputs, corresponding to “deforestation” and
“no-deforestation” classes. The ﬁlter and output size of each layer are summarized in Table 3.

132456789101112131415Remote Sens. 2020, 12, 910

11 of 28

Table 2. Number of samples in the training, validation and test sets for Cerrado database.

Set

Tiles

Available Def.
Samples

Available No-def.
Samples

Balanced Samples
(per Class)

Total Samples

20,306
40,515
59,102
78,431

717
2127
5421
8118

1434 + 5778
4254 + 5778
10,842 + 5778
16,236 + 5778

Table 7. Training tiles used for the Cerrado database.

Training Set

Tiles

Available Def.
Samples

Available No-def.
Samples

Balanced Samples
(per Class)

Total Samples
(tr + val)

1 Tile
2 Tiles
3 Tiles
4 Tiles

5
5, 13
1, 5, 13
1, 5, 12, 13

671
1240
2287
4182

17,370
33,760
50,273
65,717

2013
3720
6861
12,546

4026 + 3,978
7440 + 3,978
13,722 + 3,978
25,092 + 3978

2.7. Accuracy Assessment

The performance of the evaluated methods was expressed in terms of Overall Accuracy (OA),

F1-Score, and Alarm Area (AA).

•

•

•

Overall Accuracy (OA): is a global metric that indicates the percentage of samples correctly
classiﬁed in relation to the total samples. It is deﬁned by:

OA =

tp + tn
P + N

× 100

(2)

3. Results and Discussion

In this section, we present and discuss the results obtained by the methods described in Section 2
for the Amazon and Cerrado databases. Firstly, we report the average of Overall Accuracy (OA) and
F1-score computed over ten runs, each run with a different choice of training samples for the class
“no deforestation”. Next, we present the probability maps generated in each experiment, and ﬁnally,
we analyze how semi-automatic approaches could be designed based on these methods to reduce
human intervention with minimal accuracy loss.

3.1. Amazon Biome

1 Tile2 Tiles3 Tiles4 TilesTiles for trainingF1-Score(%)46.148.352.252.445.156.061.663.249.257.262.263.039.745.050.550.743.345.450.052.343.543.847.250.0SVMEFSNCSVM-L1CSVM-L2CSVM-L31 Tile2 Tiles3 Tiles4 TilesTiles for trainingOverallAccuracy(%)95.295.596.296.294.196.897.397.895.797.097.798.093.694.895.995.994.594.995.896.294.594.595.395.8SVMEFSNCSVM-L1CSVM-L2CSVM-L3Remote Sens. 2020, 12, 910

16 of 28

ﬂagged as potentially deforested and, second, the proportion of total deforestation concentrated in the
areas indicated by the classiﬁer. The ﬁrst metric is the Alarm Area deﬁned in Equation (6), whereas the
second metric is the Recall deﬁned in Equation (4).

Figure 11. Predicted maps for tile 2 computed by SVM, EF, SN, and CSVM using one, two, three,
and four tiles for training.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To handle randomness in the deep learning pipeline, such as variations caused by different random seed values, several strategies can be employed. One common approach is to perform multiple runs or experiments with different initializations and then aggregate the results. This technique, known as ensemble learning, helps mitigate the impact of individual model variance and improves overall performance. Another strategy involves setting specific random seeds during development and testing phases to ensure reproducibility and consistency across experiments. Additionally, techniques like weight initialization schemes, batch normalization, and dropout regularization can help stabilize the learning process and reduce sensitivity to random factors.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

query set are negative. Liu et al. (Liu et al., 2019) propose transductive
propagation that iteratively propagates labels from labelled instances to
unlabelled instances according to the graph structure of the embeddings.
Other semi-supervised learning techniques have been applied such as
active learning. Wang et al. (Wang et al., 2022) combines a few-shot

Fig. 2. Raw field data are segmented into 5-s spectrograms. A pre-trained feature extraction generates 527-dimensional embedding vectors for each segment.
Coloured vectors denote embeddings that were previously labelled.

EcologicalInformatics82(2024)1027344B. McEwen et al.

Fig. 3. Transfer learning using pre-trained feature extraction model and fully connected layers with adjustable output nodes.

Fig. 4. Prototypical learning provides a class probability distribution that a query belongs to each class. The probability distribution is used to rank the priority of
samples. The colours denote the class distributions in a simplified 2-dimensional view.

Both transformer models outperform the ResNet-based architecture
with AST marginally outperforming the HuBERT model in terms of
validation accuracy 98.8%. While results will of course vary between
applications, AST demonstrates a good trade-off between data re-
quirements and classification performance. AST computes each segment
in 7.4 ms on average compared to HuBERT at 17.5 ms and ResNet at 1.9
ms. Computation time was evaluated using an Intel i7–9700 CPU with
an NVIDIA RTX2070 GPU. Table 1 reports validation results. The vali-
dation set is automatically generated by the application (McEwen et al.,
2023b) and maintains an 80/20% training-validation split. Training and
validation data are exported and the model is evaluated separately for
repeatability. Separate testing using a manually prepared test dataset
containing 500 samples achieved a similar test accuracy of 98.4% and
precision-recall results of 0.982 and 0.984 respectively. In addition to

EcologicalInformatics82(2024)1027346or mel-frequency cepstral coefficients (MFCC) can be represented as a
single-channel images. We evaluate a commonly applied pre-trained
ResNet-50 architecture (He et al., 2016). Dufourq et al. (Dufourq
et al., 2022) also demonstrates strong classification performance and
low data requirements of ResNet-50 when comparing alternative pre-
trained CNN-based models. With the rapid development of transformer-
based models we also evaluate two transformer models Audio Spectro-
gram Transformer (AST) (Gong et al., 2021) which, like ResNet, oper-
ates on the spectrogram. We also evaluate HuBERT (Hsu et al., 2021)
which operates directly on the 1-D waveform. We evaluate the models
using the same feature extractor fθ and user model Uϕ pipeline presented
in Fig. 1 using a two-class binary output. Hence Uϕ is a two layer {100,2}
fully connected network.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific hardware resources used for training the deep learning models mentioned. However, it mentions that computation time was evaluated using an Intel i7-9700 CPU with an NVIDIA RTX2070 GPU. This suggests that the authors may have used this setup for their experiments, but it doesn't confirm whether they exclusively used these resources or if other hardware components were involved during the training process. Therefore, based on the given context, I cannot definitively state which hardware resources were used for training the deep learning models.
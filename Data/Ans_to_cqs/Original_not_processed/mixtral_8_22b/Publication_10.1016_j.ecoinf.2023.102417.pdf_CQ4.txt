Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

When  it  comes  to  the  application  of  deep  learning  models,  it  is 
mandatory to train the model on training data that resemble the data, on 
which  the model will be applied  later. If the  model is  applied over  a 
longer period of time (as it can be the case in long term biomonitoring), 
the training dataset and the dataset, on which the model is applied can 
have  different  data  distributions  due  to  some  reasons,  like  different 
recording  conditions  (brightness,  scale)  or  recording  devices/sensors 
(contrast,  resolution).  This  is  a  well-known  phenomenon  called  data 
drift  or  domain  drift  (Farahani  et  al.,  2020).  Thus,  it  is  necessary  to 
create specific datasets tailored to the desired application. The field of 
machine learning that is dealing with this discrepancy across domain 
distributions, is called domain adaption, with the aim of increasing the 
transferability of models to datasets with similar statistical properties.

function to ignore a certain class. These annotations are made manually, 
using special software, and consume large amounts of time. For model 
development  there  are  many  well  established  collections  of  image 
datasets for the training and evaluation of the models, such as ImageNet

IoU =

A ∩ B
A ∪ B

(19) 

Fig.  3. Methodological  workflow  of  the  Joint  Energy-Based  Semantic  Segmentation  (JESS).  Green  and  orange  arrows  describe  fieldwork  and  the  annotation  of 
images with Labelbox respectively. Light blue arrows describes the data processing that has been done using Python. The deep learning part, implemented with the 
Python library PyTorch is shown in dark blue. The energy-based optimization step of the JESS model is outlined with a red dashed line. The baseline model is trained 
without this optimization step. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) 

EcologicalInformatics79(2024)1024176S. Herdy et al.

A  measure  to  improve  the  accuracy  of  classification  would  be  to 
create larger training data sets. One approach to obtain such data can be 
to create a platform, where experts can label image data while using the 
trained  models.  A  good  example  for  image  classification  here  is  iNa-
turalist (Van Horn et al., 2018). In our case, the focus would not be on 
image  classification,  but  on  semantic  segmentation  of  biomonitoring 
and remote sensing images. 

In this study, we focused on small-scale remote sensing data obtained 
by  hand-held  camera  devices.  However,  our  domain  adaptation 

Appendix A. Dataset visualization 

approach  and  the  joint  energy-based  semantic  segmentation  model 
(JESS) have a broad applicability and can be very valuable for air- and 
spaceborne remote sensing applications. 

CRediT authorship contribution statement

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit information about the specific data augmentation techniques used in the deep learning pipeline. Data augmentation techniques mentioned in general include flipping, rotating, scaling, changing brightness, contrast, and resolution. However, these techniques are not explicitly stated as being used in the described methodology. Therefore, based solely on the provided context, I cannot give a definitive list of data augmentation techniques applied in their deep learning pipeline.
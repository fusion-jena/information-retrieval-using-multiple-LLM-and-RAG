Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Nearest Neighbors, Support Vector Machines, Decision Trees, Boosting, 
and Random Trees (Yang et al., 2010). However, a growing subarea in 
machine learning, called deep learning, can leverage large image data-
sets  to train  deep neural networks such as  Convolutional Neural  Net-
works (CNNs) (Jordan and Mitchell, 2015) to make accurate predictions 
on new, unseen images.

2.3. Imagery splitting ratio 

We split the dataset in a ratio of 80:10:10 for training, validation, and 
testing sets, respectively. This ratio was used due to prior evidence of 
exceed performances in detecting small insects and diseases using deep 
learning models (Kiratiratanapruk et al., 2022; Krishnadas and Sampa-
thila, 2021; Mulchandani et al., 2019). The training dataset consisted of 
1543 images, 243 images for the validation set, and 247 images for the 
testing set and all were at input resolution of 640 × 640 pixels. The 1280 
× 1280 pixels input resolution consisted of 1637 images for the training 
set, 255 images for the validation set, and 262 images for the testing set. 
Independently,  we  trained  5  different  versions  of  the  YOLOv5  model 
family  including:  YOLOv5n,  YOLOv5s,  YOLOv5m,  YOLOv5l,  and 
YOLOv5x models to detect alates on sorghum leaves. 

2.4. Characteristics of deep learning models

2.5. Description of hyperparameters and training aspects 

The  five  detection  models  were  trained  using  a  manually  labeled 
image dataset for alate detection. After conducted initial computational 
experiments with different combinations of epochs of 300, 400 and 500 
with batch sizes of 8, 16, and 32, it was determined that the optimal 
hyperparameters for our training were 400 epochs with a batch size of 8 
and were constant when comparing the five detection models using 640 
× 640, and 1280 × 1280 pixels input resolutions on each model type. 
The batch defines the number of samples to be worked through before 
moving on to update the internal model parameters (Radiuk, 2017). The 
epoch  number  used  in  training  determines  the  number  of  times  the 
detection model processes the dataset dedicated to training the model 
(Brownlee,  2018).  The  computer  infrastructure  used  for  training 

2.6. Evaluation metrics of the trained models

To  improve  the  overall  performance  of  the  developed  models,  we 
suggest  incorporating  more  training  images  with  different  alate  den-
sities collected using different devices (e.g., smartphones, drones, etc.), 
that could potentially improve the validation of these models (Grijalva 
et  al.,  2023b).  In  addition,  applying  different  preprocessing  and 
augmentation  techniques  can  diversify  the  training  data  and  increase 
overall  performance.  Our  web  application  comes  with  certain  limita-
tions, including the need for the user to follow the recommended image- 
capturing instructions and a good internet connection for proper oper-
ation. This could restrict its utility in areas where internet is not acces-
sible.  However,  the  framework  of  this  study  and  the  models  tested 
provided a better understanding of the capabilities of computer vision 
models, based on deep learning, to pest detection for pest monitoring in 
sorghum.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

While the given context does not explicitly mention specific preprocessing steps taken before training the deep learning models, it does suggest that various combinations of epochs and batch sizes were experimented with during the training process. Additionally, it mentions the use of different input resolutions for the images. These factors indicate that some form of preprocessing might have been applied to prepare the dataset for training.

Preprocessing in deep learning typically involves several steps aimed at improving the quality and consistency of the input data. Common preprocessing techniques include:

1. Normalization: Scaling pixel values to a standard range, often between 0 and 1 or -1 and 1, which helps stabilize the learning process by ensuring consistent input scales across different features.

2. Scaling: Resizing images to maintain uniform dimensions, making it easier for the model to learn from them. This step may involve either downscaling larger images or upscaling smaller ones.

3. Cleaning: Removing irrelevant information or noise from the dataset, such as removing unwanted objects or artifacts present in the images.

4. Augmentation: Applying transformations like rotation, flipping, zooming, cropping, or color shifting to generate additional training examples and enhance the robustness of the model.

In the given context, although no explicit details about preprocessing are mentioned, it is reasonable to assume that similar steps might have been employed to ensure the successful training of the YOLOv5 models for alate detection on sorghum leaves.
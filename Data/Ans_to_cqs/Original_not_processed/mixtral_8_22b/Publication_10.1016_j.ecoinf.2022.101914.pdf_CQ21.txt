Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Fig. 2. Diagram showing the procedure used to build models. (a) Obtain presences (red circles) and generate background points (black triangles). (b) Separate data 
into folds. (c) Set aside one fold to use as an external evaluation dataset and keep the remaining folds as training data. (d) Perform scale optimization for envi-
ronmental covariates: calculate focal statistics for each covariate using a moving window of radius equal to each scale to be tested, then compare scales using t-tests. 
(e) Subsample background points in the training data to create three datasets: one with the number of background points equal to the number of presences (1×), one 
with a number of background points equal to 10 times the number of presences (10×), and 10,000 background points (10K). Keep the number of presences the same. 
(f) Train component models separately using each background point subsample. (g) Evaluate component models using the evaluation dataset (using AUC and TSS).

2.7. Subsetting background points 

We  made  three  random  selections  of  background  points  from  the 
training dataset for use during model construction. First, we made the 
“1×” selection, which selected the same number of background points as 
presences in the training dataset. Second, we made the “10×” selection, 
which selected a number of background points equal to ten times the 
number of presences in the training dataset. Third, we made the “10 K” 
selection, which selected 10,000 background points from the training 
dataset.  We  used  this  approach  to  maintain  consistency  across  back-
ground  points  used  to  train  each  model  so  there  was  no  variation  in 
model results due to variation in the response variable. 

2.8. Training individual models

points. Then, to obtain presences and background points to use during 
external evaluation, we set aside one fold. We then used the remaining 
four folds for internal cross-validation during model training.

by  its  relatively  constant  performance  regardless  of  the  number  of 
background  points  used  to  train  component  models.  As  with  many 
ecological  problems, the  decisions made during  the modeling process 
demand careful consideration in the full context of the question at hand, 
since different questions often differ in their requirements from a model.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The provided context does not explicitly mention any specific postprocessing steps after the model training. However, it can be inferred that some form of evaluation is performed on the trained models using an external evaluation dataset. This evaluation involves calculating metrics such as Area Under Curve (AUC) and True Skill Statistic (TSS). These metrics help assess the performance of the trained models. Additionally, the context mentions that the models were evaluated using a separate fold set aside for external evaluation, indicating that some form of comparison or validation might have been conducted between the predicted values and actual values. It should also be noted that the context discusses maintaining consistency across background points used to train each model to avoid variations in model results due to differences in the response variable. This suggests that some form of standardization or normalization might have been applied before or after the model training. However, without further information, it cannot be confirmed if other postprocessing steps like saliency maps or confusion matrices were employed.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

S
E
C
N
E
I
C
S
R
E
T
U
P
M
O
C

Y
G
O
L
O
C
E

ABCDownloaded from https://www.pnas.org by THUERINGER UNIVERSITAETS UND LANDESBIBLIOTHEK JENA on August 28, 2024 from IP address 141.35.40.48.Table 1. Performance of different deep learning architectures

Architecture

No. of layers

Short description

AlexNet

NiN

VGG

GoogLeNet

8

16

22

32

ResNet

18, 34, 50, 101, 152

23. Sutskever I, Vinyals O, Le QV (2014) Sequence to sequence learning with neural
networks. 2014 Advances in Neural Information Processing Systems (NIPS) (Neural
Information Processing Systems Foundation, La Jolla, CA).

24. Cho K, et al. (2014) Learning phrase representations using RNN encoder-decoder for

statistical machine translation. arXiv:1406.1078.

25. He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition.
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (IEEE, New
York).

yellow-billed oxpeckers roosting on their large mammalian hosts. Afr J Ecol.

26. Simonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale

13. Goodfellow I, Bengio Y, Courville A (2016) Deep Learning (MIT Press, Cambridge,

image recognition. arXiv:1409.1556.

MA).

27. Mnih V, et al. (2015) Human-level control through deep reinforcement learning.

8

16

22

32

ResNet

18, 34, 50, 101, 152

A landmark architecture for deep learning winning ILSVRC
2012 challenge (31).
Network in Network (NiN) is one of the ﬁrst architectures
harnessing innovative 1 × 1 convolutions (49) to provide
more combinational power to the features of a convolutional
layers (49).
An architecture that is deeper (i.e., has more layers of
neurons) and obtains better performance than AlexNet
by using effective 3 × 3 convolutional ﬁlters (26).
This architecture is designed to be computationally efﬁcient
(using 12 times fewer parameters than AlexNet) while offering
high accuracy (50).
The winning architecture of the 2016 ImageNet competition
(25). The number of layers for the ResNet architecture can be
different. In this work, we try 18, 34, 50, 101, and 152 layers.

25% (757,000) nonempty images and randomly selected 757,000
empty images. This dataset was then split into training and
test sets.

19. Bridle JS (1990) Probabilistic interpretation of feedforward classiﬁcation network out-
puts, with relationships to statistical pattern recognition. Neurocomputing (Springer,
New York), pp 227–236.

20. Hinton G, et al. (2012) Deep neural networks for acoustic modeling in speech
recognition: The shared views of four research groups. IEEE Signal Proc Mag 29:82–97.
21. Deng L, Hinton G, Kingsbury B (2013) New types of deep neural network learning for
speech recognition and related applications: An overview. 2013 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) (IEEE, New York).
22. Bahdanau D, et al. (2016) End-to-end attention-based large vocabulary speech recog-
nition. 2016 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) (IEEE, New York).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Data bias can significantly impact the performance of deep learning models. To mitigate its effects, several techniques can be employed during the preprocessing stage of the deep learning pipeline. Here are some commonly used methods:

1. Stratified Splitting: When dealing with imbalanced datasets, stratification ensures that each subset (training or testing set) maintains the same distribution of classes as the original dataset. This helps prevent overrepresentation or underrepresentation of certain classes in either set.

2. Oversampling: This technique involves increasing the frequency of minority class examples in the dataset. By duplicating existing samples or creating synthetic ones, it aims to balance the class distribution. Common oversampling techniques include Random OverSampler, SMOTE (Synthetic Minority Over-sampling Technique), and ADASYN (Adaptive Synthetic Sampling).

3. Undersampling: Unlike oversampling, undersampling reduces the size of the majority class by removing instances until both classes have equal representation. Some popular undersampling algorithms are NearMiss, Tomek Links, and Edited Nearest Neighbors.

4. Diverse Data Collection: Collecting diverse data from various sources can help reduce data bias. This approach focuses on gathering representative samples from all possible subgroups within the population. For example, when building a facial recognition system, ensuring diversity in terms of age, gender, ethnicity, etc., will lead to a more robust model.

These techniques aim to create balanced datasets that accurately represent the underlying population, ultimately improving the generalization capabilities of deep learning models. However, choosing the appropriate method depends on factors such as dataset size, computational resources, and specific application requirements.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 7 
MLP parameters and performance.  

S/N 

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14 

Variables 

Input layer neurons 
Hidden layer nodes 
Output layer neurons 
Requested samples per class 
Start learning rate 
End learning rate 
Momentum factor 
Sigmoid constant 
Iterations 
Acceptable RMS 
Training RMS 
Testing RMS 
Accuracy Rate 
Skill measure (%) 

Results 

8 
10 
8 
1688 
0.0008 
0.0001 
0.5 
1.0 
10,000 
0.01 
0.1430 
0.1446 
88.79% 
0.8719

EcologicalInformatics79(2024)1024283P. Verma et al.                                                                                                                                                                                                                                  

Fig. 1. Location map.  

differentiation as a feedforward propagation (FP) algorithm to train the 
neural networks by employing supervised learning based on stochastic 
gradient descent (SGD). Backward pass always improves the efficiency 
of MLP neural networks. Generally, MLP consists of three main layers of 
nodes: an input layer, an output layer, and one or more hidden layers in- 
between them (Kumar and Agrawal, 2023). Apart from the input layer,

2.3. Research design and data processing 

The research workflow of the study is organised into three stages, 
viz., data processing and classification, change detection and prediction, 
and  valuation  of  C  sequestration.  At  the  first  stage,  pre-processing  of 

2  https://earthexplorer.usgs.gov/.  
3  www.openstreetmap.org/#map=4/21.84/82.79

each layer is a neuron that operates on non-linear activation function, 
such as  sigmoid. These  neurons are  organised in such a way that the 
networks are directed only from lower to upper layers, and the neurons 
within  the  same  layer  are  independent.  Input  data  is  provided  to  the 
input node, processed through the hidden layers in an iterative manner, 
and  the  output  layer  provides  the  pattern  classification.  The  Markov 

EcologicalInformatics79(2024)1024284P. Verma et al.                                                                                                                                                                                                                                  

Table 1 
Raster and supplementary data information used in the study.  

S. No. 

Data type 

Source 

Date Acquired (yyyy/mm/dd) 

Path and rows 

Resolution 

Columns and rows 

Bands 

USGS Earth Explorer 
USGS Earth Explorer 
USGS Earth Explorer

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it appears that the authors use stochastic gradient descent (SGD) as an optimization technique for training their Multilayer Perceptron (MLP) neural network model. This method is employed during the feedforward propagation (FP) algorithm to improve the efficiency of the MLP neural networks. However, there is no mention of other optimization techniques like Adam being utilized in this particular study.
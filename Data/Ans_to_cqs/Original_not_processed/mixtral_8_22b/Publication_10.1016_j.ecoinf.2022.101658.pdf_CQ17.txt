Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

our study, is due to the deep learning method’s loss function, which is 
biased toward overrepresented classes. This represents a performance 
barrier for any study using semantic segmentation with few images and 
high class-imbalance.

Since the multi-class (i.e. 10 classes) semantic segmentation problem 
is quite complex, the set of 290 training images was found to be insuf-
ficient to obtain satisfactory results, even with the use of the transfer 
learning framework. Therefore, we employed data augmentation stra-
tegies to increase the training set size fourfold: horizontal flip, Gaussian 
noise  addition,  and  contrast  reduction.  The  first  two  augmentation 
strategies are fairly common. The contrast reduction strategy was spe-
cifically motivated by our application, namely given that we expect the 
images  to  contain  shadows  occluding  the  vegetation  types,  contrast 
reduction helps to simulate areas of low lighting which can be expected 
in the dataset.

Fig.  2. Schematic  diagram  of  methodolog-
ical 
steps.  Field  measurements  were 
collected at three field sites: Pelican Moun-
tain  (PM),  Calling  Lake  (CL)  and  Conklin 
(CK). Manual classification of fuel types used 
in  semantic  segmentation  training  and 
testing were limited to data collected at PM 
and  CL.  Field  measurement  data  and  raw 
photos  from  the  CK  field  site  were  used 
solely for independent validation. R refers to 
the  R  programming  language  and  environ-
ment for statistical computing (R Core Team, 
2020).  DCNN  refers  to  deep  convolutional 
neural network.   

m (class 5). Overstory composition for each transect was estimated for 
up to two species. Fuel load for each FWD size class was calculated by 
transect using the following formula: 
)

([

([

]

]

)

Wfwd =

π2 G1sec(h)n QMD2 c
8L

× S1

+

π2 G2sec(h)n QMD2 c
8L

× S2

(1)

when  comparing  forb  and  grass  cover  measurements  from  photos  to 
corresponding  field-based  measurements,  but  this  is  likely  due  to  the 
influence  of  outliers.  It  is  noteworthy  that  the  algorithm  was  trained 
with a small set of images (n = 290), which necessitated the use of data 
augmentation strategies to increase the number of training images, but 
likely at the cost of reduced flexibility and robustness for applying the 
trained network in new situations. We expect that training the network 
with a larger set of images taken at different sites would greatly improve 
the algorithm’s performance in new locations.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information on specific regularization methods such as dropout or L2 regularization used to prevent overfitting in the deep learning pipeline. However, it mentions the use of data augmentation strategies like horizontal flipping, Gaussian noise addition, and contrast reduction to increase the training set size fourfold. These techniques help mitigate overfitting by introducing variations into the existing dataset, thereby improving the model's ability to generalize better to unseen data. While these aren't traditional regularization methods, they serve a similar purpose in preventing overfitting.
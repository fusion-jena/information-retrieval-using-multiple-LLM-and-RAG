Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

‖ps‖2 + ‖qt‖2 + b2

s + b2

t

)

(4) 

Consequently, the unknown parameters bs, bt, ps, qt can be estimated 
optimizing  the  regularized  cost  function  using  stochastic  gradient 
descent: 

ps, qt, bs, bt = argmin
ps,qt ,bs,bt

L .

2.2. Embedding layers 

(5)  

Embedding  layers  are  modules  of  deep  neural  networks  that  are 
generally  implemented  in  natural  language  processing  problems 
(Sharma  et  al.,  2020)  and  collaborative  filtering  (He  et  al.,  2017) 
because they can solve the one-hot encoding problem (Yu et al., 2022), 
where the latent representations in the classic models are composed of 
sparse representations generally using vectors mainly composed of zero 
layers  replace  such  dispersed  vectors  with 
values.  Embedding

Shah, J., Mishra, B., 2020. Iot-enabled low power environment monitoring system for 
prediction of pm2.5. Pervas. Mobile Comput. 67, 101175 [Online]. Available: 
http://www.sciencedirect.com/science/article/pii/S1574119220300560. 
Sharma, A.K., Chaurasia, S., Srivastava, D.K., 2020. Sentimental short sentences 

classification by using cnn deep learning model with fine tuned word2vec. Procedia 
Computer Science 167, 1139–1147. International Conference on Computational 
Intelligence and Data Science. [Online]. Available: http://www.sciencedirect.com/ 
science/article/pii/S1877050920308826. 

Tamhane, A., Arora, S., Warrier, D., 2017. Modeling contextual changes in user 

behaviour in fashion e-commerce. In: Kim, J., Shim, K., Cao, L., Lee, J.-G., Lin, X., 
Moon, Y.-S. (Eds.), Advances in Knowledge Discovery and Data Mining. Springer 
International Publishing, Cham, pp. 539–550. 

van Buuren, S., Groothuis-Oudshoorn, K., 2011. mice: Multivariate imputation by

We  trained  algorithms  with  the  amount  of  data  available  after 
manually  removing  the  testing  set,  as  described  above.  During  the 
training, 10% of data served as the validation set to check for overfitting. 
We used a min-max scaler reverted at the end to compute the perfor-
mance metrics in the test set. In all cases, we used a batch size of 64 
samples and trained the algorithm during 100 epochs using Adam as the 
algorithm  to  optimize  the  network  parameters.  Algorithms  were 
implemented in Pytorch (Paszke et al., n.d.) and models were trained 
and tested in Google Colaboratory.2

EcologicalInformatics71(2022)1017753L.M. Rivera-Mu˜noz et al.                                                                                                                                                                                                                      

1 for the rest, arriving at the expression shown in Eq.(9). 

∂J
∂Enj

=

∂J
∂ej

In other words: 
⎤

⎡

∂J
∂E

=

⎢
⎢
⎢
⎢
⎢
⎣

0

⋮

0

∂J
∂e1
⋮
∂J
∂ek

⎥
⎥
⎥
⎥
⎥
⎦

0

⋮

0

(9) 

(10) 

Thus,  only  the  ith  column  of  the  embedding  layer  should  be  filled 

with the gradient descent of the DMF network. 

2.3. DMF model (DMF1) 

DMF is based on a model of nonlinear latent variables in which there 
are much fewer latent variables than observed variables (Fan and Cheng, 
2018). Thus, DMF can recover non-sampled information in a data matrix 
with nonlinear features in its structures (De Handschutter et al., 2021; 
Xue et al., 2017a).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit details about the deployment process of the trained deep learning model. However, it mentions that the algorithms were implemented in PyTorch and models were trained and tested in Google Colaboratory. This suggests that the model might have been deployed using these tools or similar platforms. The specific steps taken for model serialization or platform selection are not mentioned in the given context.
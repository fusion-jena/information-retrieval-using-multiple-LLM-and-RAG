Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1 
Pre-processing hyper-parameters for each dataset and the number of testing files 
used.   

Lemurs 

Alethe 

Gibbons 

Whydah 

Low pass filter cut off 
Downsampling rate 
Nyquist rate 
Segment duration 
Number of testing files 
Testing time (min) 

4000 
9600 
4800 
4 
46 
1840 

3100 
6400 
3200 
2 
27 
810 

2000 
4800 
2400 
4 
22 
1300 

9000 
18400 
9200 
3 
78 
1560

and  thus  we  could  compute  the  performance  of  the  network  on  each 
testing  file  and  report  on  the  F1-score  as  it  was  commonly  used  in 
literature. Unless stated otherwise, model training and testing was done 
on Microsoft Azure using the Data Science Virtual Machine and a NCv2- 
series virtual machine (NVIDIA Tesla P100 GPU).

Table 4 
Comparison of the average F1 score across the different network architectures and dataset configurations. The exponent approach was used for the spectrogram input. 
The feature extracted was frozen. The results are averaged across 13 unique executions. The results are ordered (highest to lowed) based on the average of each 
network architecture across all configurations. The best three performing network architectures on a particular dataset configuration is highlighted in bold.  

Method 

ResNet101V2 
ResNet152V2 
InceptionResNetV2 
ResNet50V2 
DenseNet169 
DenseNet201 
VGG16 
DenseNet121 
InceptionV3 
ResNet101 
Xception 
MobileNetV2 

G 25 

95.30 
95.18 
94.70 
94.97 
94.92 
94.84 
97.26 
94.58 
92.22 
96.17 
93.88 
94.62 

G 50 

97.40 
96.92 
96.75 
97.04 
96.95 
96.72 
98.09 
96.69 
95.42 
97.49 
95.79 
96.65 

G 100 

96.27 
96.58 
96.57 
95.13 
95.69 
95.86 
94.99 
95.00 
95.40 
94.23 
95.50 
91.65 

L 25

model  development  and  to  accelerate  the  rate  at  which  vocalisation 
events are found. Initially in an acoustic survey, only a relatively small 
number of manually annotated vocalisation examples may be available.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the strategy implemented to monitor the model performance during training?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, there isn't any explicit mention of a specific strategy implemented to monitor the model performance during training. However, one can infer from the text that the performance of the network was computed on each testing file and reported on the F1-score, which is commonly used in literature. This suggests that the F1-score might have been used as a metric to evaluate the model's performance during training. Additionally, the use of Microsoft Azure with the Data Science Virtual Machine and a NCv2-series virtual machine (NVIDIA Tesla P100 GPU) implies that these tools were utilized for both training and testing the models. It's also mentioned that the results are averaged across 13 unique executions, indicating that multiple runs were conducted to ensure consistency and reliability in the evaluation process. Therefore, while not explicitly stated, it can be assumed that monitoring the model performance during training involved tracking metrics such as the F1-score and conducting multiple runs to validate the results.
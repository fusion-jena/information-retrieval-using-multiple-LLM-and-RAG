Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

respectively.  N  represented  the  number  of  images  in  the  test  set  or 
validation set. 

2.2.6. Model training and hyper-parameters 

We  trained  models  on  the  training  sets  and  tracked  the  model 
training process on the validation set. The training process was stopped 
when  the  loss  value  of  the  model  on  the  validation  set  no  longer 
decreased. We then tested the model performance on the test set. Our 
computing platform was a Dell PowerEdge C4130 rack server with two 
Tesla  K80  GPUs  and  256  GB  of  memory.  For  the  deep  learning  envi-
ronment,  we  utilized  the  PyTorch  1.7.1  platform.  We  employed  the 
Stochastic Gradient Descent (SGD) optimizer with a momentum value of 
μ  = 0.9 to train the model. Other hyper-parameter settings for model 
training were shown in Table S.4 of the Supporting Information. 

3. Results 

3.1. Experiment results of transfer strategy optimization

2.2.1. DCNN model architecture 

To carry out our experiments, we used the ResNext-101 model (Xie 
et  al.,  2017),  which  is  a  member  of  the  ResNeXt  family  of  models. 
ResNeXt model combines the advantages of the ResNet (He et al., 2016) 
and InceptionV3 (Szegedy et al., 2016) models. ResNet and InceptionV3 
models  are  widely  used  for  empty  camera  trap  image  recognition 
(Norouzzadeh et al., 2021, 2018; Willi et al., 2019; Yang et al., 2021c). 
The ResNeXt series models exhibit a remarkably similar architecture to 
the  ResNet  series  models  that  are  extensively  used  for  recognizing 
camera trap images (Meng et al., 2023; Norouzzadeh et al., 2021, 2018; 
Willi et al., 2019; Yang et al., 2021b). Therefore, the findings of ResNext- 
101 can be generalized to the ResNet family of models.

ResNext-101 model is a simple, highly modular network architecture 
(Table 1), which includes six stages, denoted as Si (i = 0,1, …,5). Except 
for the first and the last stages (S0 and S5), each stage contains multiple 
stacks of blocks with 32 groups of grouped convolutions. Each group of 
grouped convolutions consists of three repeat layers with eight channels. 
This model has a total of 25.0 × 106 parameters. 

2.2.2. Transfer learning conceptual model

Regarding source model construction, we suggest that practitioners 
with abundant computational resources should build the source model 
with a larger number of readily available labeled samples. Doing so can 
contribute  to  improved  transfer  learning  performance.  However,  for 
those  with  limited  computational  resources,  it  may  be  necessary  to 
compromise between sample size and computational resources based on 
the potential threshold effect. 

CRediT authorship contribution statement

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning environment used to build the model is PyTorch version 1.7.1. This information is found in section 2.2.6 of the given text, where it states, "For the deep learning environment, we utilized the PyTorch 1.7.1 platform."
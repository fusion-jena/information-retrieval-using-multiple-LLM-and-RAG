Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

linear to machine learning methods, were fitted in training data to
construct regression equations. We used multiple linear regression, Enet,
SVM, RF, XGboost and LightGBM to predict canopy cover from planet
data. In general, all the mentioned models are used in machine learning
as they use algorithms to automatically learn patterns and relationships
from data to make predictions or decisions. However, we categorised

EcologicalInformatics82(2024)1027065A. Gyawali et al.

Table 4
The optimised hyperparameter values with grid search range in machine
learning models.

Model

Hyperparameters

Grid search

Optimised value

Enet

SVM

RF

Xgboost

LightGBM

α
l1_ratio

Cost (C)
gamma
Kernal
epsilon

n_estimators
max_features
max_depth
min_samples_split
min_samples_leaf

n_estimators
max_depth
colsample_bytree
min_child_weight
subsample
learning_rate

n_estimators
max_depth
learning_rate
num_leaves
feature_fraction
bagging_fraction
bagging_freq

and time overhead (Ke et al., 2017). LightGBM sets itself apart from
other tree-based methods through its leaf-wise splitting approach, which
generates more intricate trees. These trees are adept at minimising loss,
leading to enhanced accuracy. The splitting process is guided by a
unique sampling method called Gradient-Based One Side Sampling
(GBOSS) (Ke et al., 2017). This method excludes data with small gra-
dients while utilising the remainder for estimating information gain and
facilitating tree growth. A collection of hyperparameters, such as lear-
ning_rate, max_depth, bagging_fraction, bagging_freq, and n_estimators,
controls the algorithm. These parameters are crucial in shaping how the
leaves are structured within the trees. The model's complexity gradually
rises as the tree grows, guided by these parameters. Simultaneously, loss
decreases, and the algorithm becomes more adept at learning from the
data, thereby improving its efficiency. Moreover, this model is suscep-

For machine learning models, the GARI, GI, GNDVI, GREEN, MSR,
NDVI, RDVI, RED, SI2, and VARIg were selected as described earlier in
the methodology sections. The performance of the models is presented
in Table 6. In the testing dataset, it was observed that all models
demonstrated lower coefficients of determination than the training data,
except for the linear and Enet models. Machine learning algorithms
outperformed linear in modelling forest canopy cover. The LightGBM
exhibited the most substantial model training ability (R2 = 0.69), fol-
lowed by SVM, RF and XGboost, each with an R2 value of 0.66. All six
models yielded satisfactory results with an R2 ≥ 0.58 in training data,
meeting our study's canopy cover prediction criteria. Further evaluation
in independent
test samples, considering additional fit statistics,
XGboost and LightGBM demonstrated superior predictive capabilities
with R2 = 0.64, RMSE = 0.16, rRMSE = 22%, and MAE = 0.12, out-

100
5
0.05
20
0.9
0.8
5

linear as a simple statistical model and the rest of the advanced machine
learning models as machine learning. The regression analyses were
conducted using Python 3.12.2, distributed by Anaconda Inc. The Scikit-
learn library (Pedregosa et al., 2011) was employed for all models except
those utilising XGboost and LightGBM. For XGboost and LightGBM
models, the respective Python libraries “xgboost” (Chen and Guestrin,
2016) and “lightgbm” (Ke et al., 2017) were utilised. Additionally, a 5-
fold cross-validation grid search strategy was implemented to tune
hyperparameters within the training dataset for all machine learning
models, leveraging functionalities inherent in their respective Python
packages. The information on hyperparameters and their optimised
value for all machine learning models are presented in Table 4. More-
over, we converted the canopy cover output percentage to a fraction
(0–1) for convenience and smoothness in the further calculation.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The given context does not provide explicit details about the specific criteria used to split the data for deep learning model training into train, test, and validation datasets. However, based on common practices in machine learning and deep learning, some possible criteria could include:

1. Random Splitting: Data is randomly divided into three subsets - training set, validation set, and test set. Typically, the ratio used is 60:20:20 or 70:15:15.

2. Stratified Splitting: When dealing with imbalanced classes, stratified sampling ensures that the distribution of target variables remains consistent across different subsets.

3. Time-series Splitting: For temporal data, splitting should be done sequentially rather than randomly to maintain the chronological order.

4. Cross-Validation: K-Fold cross-validation is often used to evaluate the model's performance and prevent overfitting. It involves dividing the data into k equal folds, where one fold serves as the test set, and the remaining k-1 folds form the training set. This process is repeated k times, ensuring every fold gets a chance to act as the test set.

Without explicit mention in the provided context, it cannot be definitively stated which criteria were used for splitting the data for deep learning model training.
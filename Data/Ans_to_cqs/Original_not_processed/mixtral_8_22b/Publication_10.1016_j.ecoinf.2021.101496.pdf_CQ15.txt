Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

As  the  third  approach  for  SPAD  value  modelling  based  on  the 
Rasp2SPAD indices, an artificial neural network was implemented. The 
same predictors as in the previously described LM and GLM were uti-
lized. All predictors were scaled prior to analysis. The ANN  was con-
structed empirically, with the first layer containing two input nodes (one 
per each of the two parameters that were found to be the most suitable 
by LM and GLM) and the last layer containing only one node (predicted 
SPAD value). As the fit of the ANN does not give the same output for 
each run  (because of a random number generator used for numerical 
optimization of the model), 100 successful runs were performed, and the 
model residuals obtained in each run were analysed for a more reliable 
evaluation of the model performance.

Fig. 5. Technical specifications of the proposed sensor Rasp2SPAD as the low-cost alternative to the standard commercial handheld sensor SPAD-502Plus (Kon-
ica Minolta). 

Table 4 
The best fitting simple linear model (LM) for SPAD value prediction was based 
on parameters obtained by the Rasp2SPAD prototype.  

Response: SPAD value 

Coefficients     

Estimate 

Intercept 
Cb 
Cr 
R-squared (training data): 0.81 
Mean absolute difference (test data): 1.85  

89.53762 
0.47547 
(cid:0) 0.76144 

t value 

7.56 
10.18 
(cid:0) 8.51 

p value 

< 0.001 
< 0.001 
< 0.001 

Furthermore, the GLM approach applied on the dataset also achieved 
significant  results  (Table  5),  although  only  slightly  better  than  LM. 
Artificial R-squared = 0.81 on training data reached the same level as by 
LM; nevertheless, lower mean absolute deviance of testing data = 1.79

value gained by SPAD-502Plus (Fig. 6). Furthermore, modelling of the 
SPAD  value  was  performed  by  a)  a  simple  linear  model  (LM),  b)  a 
generalized linear model with a gamma distribution (GLM), and c) an 
artificial neural network (ANN). The dataset contained 89 valid records 
(outliers were removed according to the empirical rule), while one re-
cord represented the mean value of the total 10 spot measurements on 
one  leaf.  For  the  purpose  of  SPAD  value  modelling,  the  dataset  was 
divided  into  a  “training  dataset”  containing  60  randomly  selected  re-
cords and a “testing dataset” that contained the remaining 29 records. 
All three models were then fitted using the “training dataset” and further 
validated using the “testing dataset”.

In terms of the data quality, Rasp2SPAD demonstrated to be able to 
provide a complex dataset by capturing and processing the simple colour 
image as a result of the evaluation procedure on the winter rapeseed. In 
addition to the extraction of DN for RGB channels, 19 other indices were 
calculated. These parameters were all tested on their relationship to the 
SPAD value gained by the SPAD-502Plus handheld sensor. The corre-
logram (Fig. 6) provided an initial hint of more or less correlated indices 
with the six best performing NRI (r = (cid:0) 0.869), hue (r = 0.865), ExG-ExR 

EcologicalInformatics67(2022)1014965K. Kˇríˇzov´a et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given information, it is not explicitly stated which optimization technique was used for the numerical optimization of the Artificial Neural Network (ANN) model. However, it is mentioned that a random number generator was utilized during this process. This could suggest the use of stochastic gradient descent (SGD) or another similar optimization algorithm that incorporates some form of randomness. Unfortunately, without additional details about the implementation of the ANN, we cannot definitively determine whether SGD, Adam, or any other specific optimization technique was employed.
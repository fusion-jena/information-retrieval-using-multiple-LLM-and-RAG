Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 1 
An example training instance for different weakly-labeled settings. Here yj indicates the targets of an instance. Red indicates an 
error in the training data (deviation from ground truth) – it is not known to the user where errors occur. Note that in the weak 
j=1yj = 1 and yj⩾0). In our setting 
label setting, only 0s may be errors. Shading represents a constraint (in this example, that 
(structurally incomplete), the constraint may not be met, but errors are neither random deviations. Mixtures of these settings 
are possible, e.g., we may have structurally incomplete with missing labels, etc. 

∑
5

EcologicalInformatics72(2022)1018492R. Beigait˙e et al.

Various other terms have been used to describe weakly supervised 
learning tasks in the literature (Table 1). Weak labels (Sun et al., 2010) 
or  partial labels (Xie  and  Huang, 2018)  are  mainly  considered  in  the 
context  of  binary  labels.  They  often  include  different  types  of  noise 
coming from the labeling process and data sources and lacks constraints. 
Distribution learning (Gao et al., 2017) entails the constraint that the 
outputs  must  sum  to  unity,  but  this  constraint  is  already  met  in  the 
training data, unlike in our structural incompleteness setting.

For the incomplete observations, we do not know the true underlying 
distribution of the target values; we only have noisy or incomplete ob-
servations. Thus, it would be misleading to assess the prediction accu-
racy  on  those  observations.  A  further  challenge  is  that  the  complete 
observations  are  not  uniformly  distributed  worldwide.  Therefore,  to 
evaluate predictive performance on incomplete data, we assess how well 
the dominant vegetation cover type is predicted and measure the pre-
diction accuracy similarly to a classification task. More specifically, we 
check whether the largest predicted fraction of each observation equals 

Fig. 8. Number of zero and non-zero values in distribution of each vegetation type.

An extended abstract version of an early version of this study was 
presented  at  the  ICML  Workshop  On  the  Art  of  Learning  with  Missing 
Values  (Artemiss)  (Beigaite  et  al.,  2020).  We  have  also  investigated 
added  value  of  including  extreme  climatic  variables  in  the  modelling 
process  when  the  task  is  reduced  single-output  classification  problem 
(Beigait ˙e et al., 2022). While the task from the computational perspec-
tive  was  different,  some  of  the  data  from  that  study  are  reused  here. 
Apart from the difference in the computational setting and methods, the 
paper primarily focused on biological aspects rather than computational 
aspects of natural vegetation cover prediction.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The provided context does not explicitly mention any specific criteria for determining when training is complete. However, based on common practices in machine learning, one might use various metrics such as validation loss plateau, accuracy, precision, recall, F1 score, ROC curve, etc., depending on the nature of the problem and the chosen model. It is important to note that the choice of metric should align with the goals of the project and the desired outcomes. Without more information about the specific models being trained and their objectives, it is difficult to provide a definitive answer.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Precision 
(0 to 1) 

Recall/Sensitivity/True 

Positive Rate 
0 to 1 
F1-Score 
(0 to 1) 

Overall Accuracy 

(0 to 1) 

AUC-ROC curve 

(0 to 1) 
Kappa Score 
((cid:0) 1 to 1) 

TP
(TP + FP)
TP
(TP + FN)

2 × Precison × Recall
(Precision + Recall)

TP + TN
(TP + TN + FP + FN)
Sensitivity = Recall 
1 (cid:0) Specificity = 1 (cid:0)
P0 (cid:0) Pe
1 (cid:0) Pe 
Where: 
P0 = TP + TN
And 

N 

(TN/((TN + FP)) )

Pe =
(TP + FN).(TP + FP) + (FP + TN).(FN + TN)
N2  

This quantifies the correct % of predicted landslide pixels among all the predicted landslide-prone 
pixels. 
This quantifies the correct % of predicted landslide pixels among all the true landslide-prone pixels.

FR = F/E 

Soil 

Distance to 
stream 

Land Cover 

Lithology 

Distance to road 

Inceptisols 
Ultisols 
Bodies of Water 
Alfisols 
Entisols 

0–50 
50–100 
100–150 
150–250 
250–360 
Forest 
Grassland 
Wetland 
Cropland 
Barrenland 
Urban 
Water body 
Mica-schist 
Henderson-Gneiss 
Garnet-mica-schist 
Biotite-gneiss and schist 
Migmatitic-granitic-gneiss 
Amphibolite and biotite- 
gneiss 
Granite-gneiss 
Porphyroblastic-gneiss 
Caesars-Head Granitic-Gniess 
<200 
200–400 
400–600 
600–800 
>800 

124,464 
438,856 
6717 
343 
11,028 

305,134 
190,058 
71,058 
14,915 
243 
439,207 
10,914 
815 
50,952 
447 
73,274 
5799 
9949 
104,512 
44,510 
178,276 
70,962 

11,509 
39,400 
116,161 
6129 
1,60,092 
104,592 
72,603 
51,114 
193,007 
581,408    

357 
809 
0 
0 
49 

631 
419 
129 
36 
0 
1072 
20 
0 
0 
1 
122 
0 
11 
123 
11 
510 
376 

6 
13 
165 
0 
302 
148 
107 
81 
577 

21% 
75% 
1% 
0.1% 
2%

0.779 
0.776 
0.765 
0.735 
0.711 
0.748 
0.723 

0.890 
0.927 
0.927 
0.959 
0.939 
0.951 
0.955 

0.831 
0.844 
0.838 
0.832 
0.809 
0.837 
0.823  

skill level. In our study, we employed all five previously mentioned 
models in an ensemble learning approach, and the highest accuracy 
was achieved when uniform weights were assigned to each model, 
specifically  a  weight  of  0.2  for  each,  meaning  equal  contribution 
from all models.  

b)  Stacking  Ensemble  Learning:  Stacked  ensemble  learning  is  a 
potent  technique  that  harnesses  multiple  models’  strengths  to 
enhance prediction accuracy. In essence, it entails training a meta- 
model  to  effectively  combine  the  predictions  generated  by  a 
diverse set of base models (Breiman, 1996; Wolpert, 1992).

2.3. Assessing prediction performance 

The  validation  was  carried  out  using  the  test  dataset  which  was 
unseen by the models. The test dataset contains 20% of the landslide 
inventory data, that is 243 landslides and 243 non-landslide records. To 
assess  the  performance  of  our  models,  four  assessment  metrics  were 
employed  on  the  test  dataset,  including  AUC-ROC  (Area  Under  the 
Receiver  Operating  Characteristic  Curve),  overall  accuracy,  Kappa 
Score,  and  F1-Score.  These  metrics  are  briefly  explained  in  Table  2 
below. Here, TP stands for True Positive (correctly identified landslides 
pixel), FP -False Positive (wrongly identified landslide pixels), TN-True 
Negative (correctly identified non-landslide pixels), FN-False Negative 
(wrongly  identified  non-landslide  pixels),  and  N  stands  for  total 
instances. 

2.4. Landslide susceptibility maps consistency analysis

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To assess the performance of their deep learning models, the authors utilized several evaluation metrics. These include the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), Overall Accuracy, Kappa Score, and F1-Score. Each metric provides unique insights into the model's predictive capabilities.

The AUC-ROC measures the ability of the model to distinguish between positive and negative classes. It ranges from 0 to 1, with higher values indicating better performance.

Overall Accuracy represents the proportion of correctly classified instances out of the total number of instances. This can be calculated as (TP+TN)/(TP+TN+FP+FN), where TP is True Positive, TN is True Negative, FP is False Positive, and FN is False Negative.

The Kappa Score is a statistical measure that compares observed accuracy with expected accuracy (random chance). It ranges from -1 to 1, with 1 representing perfect agreement between the model and actual class labels.

Lastly, the F1-Score is the harmonic mean of precision and recall. It considers both false positives and false negatives, providing a more balanced view of the model's performance compared to using only accuracy or precision.
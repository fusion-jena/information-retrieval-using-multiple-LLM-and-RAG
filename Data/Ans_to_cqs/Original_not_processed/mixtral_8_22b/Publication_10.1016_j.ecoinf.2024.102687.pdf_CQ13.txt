Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2.2. Fine-tuning of the pretrained CNN backbone 

Fine-tuning of the pretrained CNN backbone was performed using 
classical meta-metric learning architectures adapted from the EasyFSL

The model was fine-tuned using the Adam optimizer initialized with 
a default learning rate of 0.0001 and a weight decay of 0. The experi-
ments were conducted in Pytorch on 8 x NVIDIA GeForce RTX 2080 Ti. 
Fine-tuning was performed on the entire model for 100 epochs, with one 
epoch  corresponding  to  500  episodic  tasks.  Estimation  of  the  energy 
consumption related to the training of the models was calculated using 
the Python package CodeCarbon (https://codecarbon.io/) (v2.3.2 with 
Python v3.8.0). This corresponds to the sum of CPU energy, GPU energy 
and  RAM  energy  in  kilowatt-hour  (kWh).  Two  episodic  tasks  were 
constructed from the training set with 5 way-1 shot and 5 way-5 shot 
tasks.  Latent  space  representations  were  finally  extracted  from  the 
backbone of the model to evaluate their capacity to improve the quality 
of clustering. 

2.3. Parameter estimation of the latent space dimensionality

11 / 30 
14 / 30 
12 / 30 
15 / 30  

EcologicalInformatics82(2024)1026879J. Poutaraud et al.                                                                                                                                                                                                                              

Fig. 8. Estimation of the energy consumption related to the training of the models. This corresponds to the sum of CPU energy, GPU energy and RAMenergy in 
kilowatt-hour (kWh). Fine-tuning of four CNN backbones (i.e. ResNet18, VGG16, DenseNet121 and AlexNet) were accomplished on 5 way-1 shot (Left) and 5 way-5 
shot (Right) tasks using Matching Networks (MN), Prototypical Networks (PN) and Relation Networks (RN).

In a second step, the pretrained DenseNet was selected for evaluating 
the capacity of the MEC method to improve the quality of clustering on 
different numbers of classes. Table 4.4 and Table 4.5 present the results 
obtained when fine-tuning a pretrained DenseNet on 5 way-1 shot and 5 
way-5 shot tasks, on 10, 20 and 30 classes respectively. 

Overall, results show that using PN fine-tuned on 5 way-5 shot tasks 
returned the number of clusters closer to the ground truth on different 

Fig. 5. Parameter estimation of latent space dimensionality to improve the quality of clustering. The dimension of the space to embed into (i.e. n_components) was 
determined according to the highest DBCV score. Results are given for the baseline versus fine-tuned models on 5 way-1 shot tasks (Top), and 5 way-5 shot tasks 
(Bottom) for Matching Networks (MN), Prototypical Networks (PN), and Relation Networks (RN).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning model was fine-tuned using the Adam optimizer initialized with a default learning rate of 0.0001 and a weight decay of 0. The experiments were conducted in PyTorch on 8 x NVIDIA GeForce RTX 2080 Ti. Fine-tuning was performed on the entire model for 100 epochs, with one epoch corresponding to 500 episodic tasks.

Answer:::

Unhelpful Answer:
I don't have enough information to provide an accurate answer. Please provide more details about the specific deep learning model being referred to.

Answer:::

Answer:
The deep learning model was fine-tuned using the Adam optimizer initialized with a default learning rate of 0.0001 and a weight decay of 0. The experiments were conducted in PyTorch on 8 x NVIDIA GeForce RTX 2080 Ti. Fine-tuning was performed on the entire model for 100 epochs, with one epoch corresponding to 500 episodic tasks.
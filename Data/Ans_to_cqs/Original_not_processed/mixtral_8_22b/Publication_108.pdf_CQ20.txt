Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

eight high-end GPGPUs (general purpose graphic processing unit) from NVIDIA (Tesla P100) with
16 GB of RAM each. The training was based on backpropagation to update the neural network’s
weights using the Adam Optimizer [31] on the computed loss function, which is the cross-entropy
between the predicted pixel value of the label (probability of the class damaged) and the true pixel
value of the label (0 for non-damaged and 1 for damaged). The training stopped when overﬁtting
started to occur (when the neural network performed worse on the validation dataset and better on
the training dataset from one epoch to the next; this indicates that the neural network is starting to
fail to generalize). Fine-tuning of hyperparameters is very important for an optimal performance of
the network on a speciﬁc problem. We tested different settings to optimize our architecture (Table 1).
Several experiments were performed with different learning rates, numbers of ﬁlters, and numbers of

Table 1. Hyperparameter ﬁne-tuning results.

Scenario Number of Blocks Number of Filters

Learning Rate Mean IoU Accuracy

1
2
3
4
5
6
7
8
9
10

3
4
5
6
4
4
4
4
4
4

64 , 64, 64
64, 64, 64, 64
64, 64, 64, 64, 64
64, 64, 64, 64, 64, 64
16, 32, 64, 128
32, 64, 128, 256
64, 128, 256, 512
16, 32, 64, 128
16, 32, 64, 128
16, 32, 64, 128

0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.01
0.0005
0.00001

0.30
0.38
0.36
0.32
0.42
0.38
0.31
0.008
0.42
0.39

82%
89%
86%
86%
94%
88%
84%
71%
94%
90%

Scenario 5 was selected as the optimal solution as it had the best values for IoU and accuracy
while training on fewer epochs. The resulting architecture is shown in Figure 4 and described in
the following.

2.5.1. Encoding Path

In general, the limitations of deep learning in comparison to other machine learning methods
are the requirement of large and high-quality training data, as well as hardware limitations related to
GPU computing power. The most notable advantage of deep learning is the grade of automatization
and a high potential to generalize when using large amounts of representative training data, which
might, however, not always be available; especially with respect to ground-truth labels that might be
scarce or not exist at all.. Furthermore, the black-box nature makes these algorithms a good choice for
classiﬁcation as suggested in this study, but a challenge for modeling physical properties, as is often
the case in forestry. This case study relied on high-resolution orthophotos with 20-cm spatial resolution
obtained from an aerial survey. The great advantage of these data is that many details can be detected.

As stated in the Introduction, there exist many different CNNs for different applications, and it is
important to ﬁnd the appropriate solution for a speciﬁc task. Retraining existing powerful architectures
such as VGG19 [27], ResNet, or Inception Net to solve other problems by transfer learning is one
approach that is often used. However, this was not an option in our four-band setting and very
particular segmentation task with features that might differ signiﬁcantly from features learned by
networks trained on common image datasets such as ImageNet. Thus, we opted to implement a U-Net
architecture, which is a particular implementation of a CNN ﬁrst developed by Ronneberger [19] for
biomedical image segmentation. One major advantage of CNNs is the ability to extract spatial features
and to detect patterns independently of their position on the input image. The U-Net architecture is

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning model was trained using eight high-end General Purpose Graphic Processing Units (GPGPUs) from NVIDIA, specifically Tesla P100 models. Each GPU has 16GB of RAM. These GPUs were utilized during the training process, which employed backpropagation to update the weights of the neural network using the Adam Optimizer. The training continued until overfitting began to occur, indicating that the neural network started failing to generalize.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Minar, M.R., Naher, J., 2018. Recent Advances in Deep Learning: An Overview URL:htt 
p://arxiv.org/abs/1807.08169https://doi.org/10.13140/RG.2.2.24831.10403, doi: 
10.13140/RG.2.2.24831.10403,arXiv:1807.08169v1. 

M¨olle, J.P., Kleiven, E.F., Ims, R.A., Soininen, E.M., 2022. Using subnivean camera traps 
to study Arctic small mammal community dynamics during winter. Arct. Sci. 8, 
183–199. https://doi.org/10.1139/AS-2021-0006/ASSET/IMAGES/AS-2021- 
0006TAB2.GIF. 

Mos, J., Hofmeester, T.R., 2020. The Mostela: an adjusted camera trapping device as a 
promising non-invasive tool to study and monitor small mustelids. Mamm. Res. 65, 
843–853. https://doi.org/10.1007/S13364-020-00513-Y. 

Norouzzadeh, M.S., Morris, D., Beery, S., Joshi, N., Jojic, N., Clune, J., 2021. A deep

We trained a deep neural network in R (R Core Team, 2022) using the 
R-package  keras  (Chollet  et  al.,  2017),  an  interface  to  TensorFlow 
(Abadi et al., 2015) for R. We used the ResNet50 architecture (for details 
about the model architecture see He et al., 2016) with an adam opti-
mizer (Kingma and Ba, 2014). We trained the model from scratch for 55 

EcologicalInformatics76(2023)1021504H. B¨ohner et al.                                                                                                                                                                                                                                 

Fig. 3. The camera trap developed by Soininen et al. (2015) consists of a Reconyx camera that is mounted on the ceiling of a metal box that can be entered by small 
mammals. The camera traps are protected with stones when placed in the field.

Fig. 4. Example images of a vole (A), a lemming (B), a stoat (C), a least weasel (D), a shrew (E), and a bird (F).  

epochs with a one-cycle learning rate policy with a minimum learning 
rate of 0.000001 and a maximum learning rate of 0.001 (Smith, 2018). 
We explored different hyper-parameters and choose the ones that gave 

the best results for training the final model. All images were resized to 
224 x 224 pixels previous to training and image augmentation (shifts, 
horizontal flips, rotations, zooms and shears) was applied to expand the 

EcologicalInformatics76(2023)1021505H. B¨ohner et al.

To overcome the challenge of manually processing huge amounts of 
data, artificial intelligence is increasingly used in ecology, for example 
to  automate  the  classification  of  camera  trap  images  (Christin  et  al., 
2019). Deep neural networks are a type of machine learning models that 
have proven to be especially useful for image recognition (Krizhevsky 
et al., 2012) and have been applied for animal detection and classifi-
cation with great success (e.g. Norouzzadeh et al., 2018; Tabak et al., 
2019; Willi et al., 2019; Zualkernan et al., 2022). These models can for 
example separate empty images from images containing an animal (e.g. 
Beery et al., 2019), identify species, count individuals, and categorize 
animal behaviour with an accuracy that matches or even outperforms 
humans (Norouzzadeh et al., 2018). However, developing and training 
neural  networks require  programming  skills  and  have  therefore  often

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text mentions that an adam optimizer was used in the deep learning pipeline. Adam is a popular optimization algorithm used in deep learning that combines the advantages of two other extensions of stochastic gradient descent, namely adaptive gradient algorithm (AdaGrad) and root mean square propagation (RMSProp). It uses estimates of first and second moments of gradients to adaptively control the learning rates for each weight in the neural network. This allows it to converge faster and perform better than traditional stochastic gradient descent methods.
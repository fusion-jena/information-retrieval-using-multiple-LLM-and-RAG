Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

et al., 2021; Stowell et al., 2019). Some of the state-of-the-art techniques 
for  handling  big  datasets,  such  as  deep  learning  and  convolutional 
neural networks (Stowell, 2022; Stowell et al., 2019), can be difficult to 
run for ornithologists, managers, and researchers without bioacoustics 
or  engineering  backgrounds.  However,  user-friendly  and  ready-to-use 
machine  learning  approaches  have  recently  been  developed  and  are 
increasingly accessible to respond to real-life monitoring challenges and 
the general public (Cole et al., 2022). Among these approaches is Bird-
NET, a research project between The Cornell Lab of Ornithology and the 
Chemnitz University of Technology. BirdNET facilitates the automated 
detection and classification of bird vocalizations, through a developed 
deep neural network, from sound recordings (Kahl et al., 2021). Bird-
NET is able to identify over 3000 bird species (Wood et al., 2021) and is

1174  
915 
891 

0.78 
0.76  

EcologicalInformatics72(2022)1019104R. Manzano-Rubio et al.                                                                                                                                                                                                                      

2.9. Traditional bird data censuses

EcologicalInformatics72(2022)1019107R. Manzano-Rubio et al.                                                                                                                                                                                                                      

Tolkova, I., Chu, B., Hedman, M., Kahl, S., Klinck, H., 2021. Parsing birdsong with deep 

audio embeddings. arXiv preprint (arXiv:2108.09203).  

Towsey, M., Znidersic, E., Broken-Brow, J., Indraswari, K., Watson, D.M., Phillips, Y., 
Street, G., 2018. Long-duration, false-colour spectrograms for detecting species in 
large audio data-sets. J. Ecoacoust. 2 (1). 

Turgeon, P., Van Wilgenburg, S., Drake, K., 2017. Microphone variability and 

degradation: implications for monitoring programs employing autonomous 
recording units. Avian Conserv. Ecol. 12 (1). 

Van Rossum, G., Drake Jr., F.L., 1995. Python tutorial, (Vol. 620).. Centrum voor 

Wiskunde en Informatica, Amsterdam, The Netherlands.

technique. For example, Cole et al. (2022) recently evaluated BirdNET 
for detecting 13 bird species in North America and found that the recall 
rate  ranged  from  9%  to  68%.  Similarly,  the  recall  rate  obtained  by 
Tolkova et al. (2021) ranged between 11% and 71% for three common 
bird species. Previous research has claimed that the accuracy of BirdNET 
increases when analysing bird songs of species that BirdNET was more 
familiar with (Arif et al., 2020). The Eurasian bittern is a widespread 
species, with over 900 recordings uploaded to Xeno-canto database and 
the Macaulay library of sounds, which may have contributed to the high 
recall  obtained  when  using  BirdNET  with  that  target  species.  We  are 
aware that we did not verify BirdNET detection, and therefore cannot 
rule  out  some  false  positives  (mislabelled  vocalizations)  within  the 
BirdNET  output. However,  no BirdNET  detection occurred  within  the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide information on the specific data repository link of the deep learning pipeline used in this study. Therefore, it is not possible to determine the exact link to resources like Zenodo, Figshare, Dryad, or GBIF based solely on the available text. Further details would need to be sought from the authors or other sources related to their work.
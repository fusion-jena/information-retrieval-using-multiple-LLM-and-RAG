Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

4.5. Implementation details

We have implemented our models in PyTorch (Paszke et al., 2017).
We trained ğ‘€ = 5 models with batch size ğµ = 64 and a base learning
rate ğ›¼ = 10âˆ’4. The learning rate is automatically reduced by a factor
of 0.1 when the validation loss has not improved for 15 consecutive
epochs. We apply weight decay to control the strength of the unit
Gaussian prior, with an empirically chosen magnitude of 10âˆ’3 that
is inversely proportional to the hyperparameter ğœ† from Eq. (1). We
chose ğ›½1 = 0.9, ğ›½2 = 0.999 and ğœ– = 10âˆ’8 as hyper-parameters for the
Adam optimizer. Each neural network was trained on a single Nvidia
RTX2080Ti GPU for âˆ¼14 days.

5. Experimental results and discussion

FU Berlin, 2019. Remote Sensing Data Analysis online course. [Online]. Available from:

https://blogs.fu-berlin.de/reseda/, Accessed: 2021-08-31.

Gal, Y., Ghahramani, Z., 2016. Dropout as a Bayesian approximation: Representing
model uncertainty in deep learning. In: Proceedings of the 33rd International
Conference on Machine Learning, Vol. 48. pp. 1050â€“1059.

Garnot, V.S.F., Landrieu, L., Giordano, S., Chehata, N., 2019. Time-space tradeoff in
deep learning models for crop classification on satellite multi-spectral image time
series. In: IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing
Symposium. IEEE, pp. 6247â€“6250.

4.3. Loss function

The loss function, which is optimized during training, measures
the quality of a set of network parameters ğœ½ w.r.t. the training data
îˆ° = {(ğ’™ğ‘–, ğ’šğ‘–)}ğ‘
ğ‘–=1, under some regularizing prior assumptions. We use a
standard loss function îˆ¸(îˆ°; ğœ½) whose minimization corresponds to max-
imizing the posterior probability of the parameters given the training
data. As it is commonly done in machine learning (see e.g. Goodfellow
et al., 2016), we assume a zero-mean isotropic Gaussian prior over
the network parameters (corresponding to îˆ¸
2 regularization) and a
Gaussian likelihood function with mean Ì‚ğğ‘– âˆ¶= Ì‚ğ(ğ’™ğ‘–; ğœ½) âˆˆ R5 and
diagonal covariance matrix with logarithmic elements Ì‚ğ’”ğ‘– âˆ¶= Ì‚ğ’”(ğ’™ğ‘–; ğœ½) âˆˆ
R5:

îˆ¸(îˆ°; ğœ½) = ğœ† â€–ğœ½â€–

2
2 +

âˆ‘

[ Ì‚ğ‘ ğ‘–ğ‘— + exp(âˆ’ Ì‚ğ‘ ğ‘–ğ‘— )( Ì‚ğœ‡ğ‘–ğ‘— âˆ’ ğ‘¦ğ‘–ğ‘— )2]

ğ‘–,ğ‘—

(1)

the Adam (Kingma and Ba, 2015) variant of stochastic gradient descent
(SGD), which adaptively scales the magnitude of the parameter updates
based on the statistics of previous updates to speed up convergence.
During training, we periodically evaluate the prediction error of the
model (i.e., the current set of parameters) on a held-out validation set
and keep the configuration ğœ½âˆ— with the lowest error as the final model.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In this study, two main regularization techniques were employed to prevent overfitting in their deep learning pipeline. Firstly, they utilized dropout as a Bayesian approximation method, which represents model uncertainty in deep learning. This technique involves randomly dropping out neurons during training, effectively preventing complex co-adaptations between them. Secondly, they applied L2 regularization, also known as weight decay, to control the strength of the unit Gaussian prior. By adding a penalty term to the loss function, L2 regularization discourages large weights, thus reducing overfitting. These regularization methods contribute to improving the generalization performance of the models.
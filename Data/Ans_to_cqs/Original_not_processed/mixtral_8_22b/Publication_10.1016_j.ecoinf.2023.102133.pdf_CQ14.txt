Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.3.4. Model hyperparameters selection 

For  both  MLP  and  RF  predictors,  the  grid  search  technique  was 
applied to compute the optimum values of hyperparameters. Regarding 
the MLP, the network was implemented with an input layer, 3 hidden 
layers, and  an output layer. In the model selection  phase, the perfor-
mance  obtained  using  different  hyperparameters  was  compared.  Spe-
cifically, the following hyperparameters were tested:  

1.  Solvers:  Limited-Broyden–Fletcher–Goldfarb–Shanno 

(L-BFGS), 
Adam,  Stochastic  Gradient  Descent  (SGD)  with  constant  learning 
rate, SGD with adaptive learning rate;  

2.  Number of neurons in the hidden layers: from 1 to 100;  
(cid:0) 2, 10
(cid:0) 1, 10
3.  Regularization factor “alpha” (L2 penalty): 10
4.  Activation function: identity, logistic, tanh, ReLU;  
5.  Learning Rate: constant, invscaling, adaptive. 

(cid:0) 3, 10

(cid:0) 4;

model (Sowmya et al., 2020). The best model is selected comparing the 
performance scores of all possible combinations of hyperparameters.

(cid:0) 3, 10

(cid:0) 4;  

The following configuration optimized the performance of the model 
and was accordingly adopted for analysis: Adam solver (Kingma and Ba, 
(cid:0) 4,  constant  learning 
2014),  10 neurons  per  hidden  layer, alpha  = 10
rate, and rectifier activation function, also called Rectified Linear Unit 
(ReLU) activation function, defined as: 

(

)

{

ReLU

k

=

k, if k > 0;
0, if k⩽0.

(14) 

In the case of the RF, the hyperparameters include the number of 
decision trees in the forest, the maximum depth of the decision tree, the 
number of features considered by each tree when splitting a node, etc. 
This  set  of  hyperparameters  was  tested  using  the  grid  configuration 
shown below:  

1.  Number of decision trees: from 100 to 1000 (in steps of 100);  
2.  Number of features to consider at every split (max features): auto,

sqrt, log2, None;  

3.  Maximum number of levels in decision tree: None, or from 10 to 100 

(in steps of 10);

4.2. Performance evaluation and results

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

Answer: The hyperparameters of the model are optimized using the grid search technique. This technique involves testing various combinations of hyperparameters and selecting the one that yields the best performance score. For both Multilayer Perceptron (MLP) and Random Forest (RF) predictors, this method was applied to determine the optimal values of their respective hyperparameters.

In the case of MLP, the hyperparameters tested included solvers such as Limited-Broyden–Fletcher–Goldfarb–Shanno (L-BFGS), Adam, Stochastic Gradient Descent (SGD) with constant learning rate, and SGD with adaptive learning rate. Other parameters like the number of neurons in the hidden layers (ranging from 1 to 100), regularization factor ("alpha" or L2 penalty), activation functions (identity, logistic, tanh, ReLU), and learning rates (constant, invscaling, adaptive) were also evaluated.

Similarly, for the RF, the hyperparameters tested using the grid search technique included the number of decision trees in the forest (from 100 to 1000 in steps of 100), the maximum depth of the decision tree, the number of features considered by each tree when splitting a node, among others.

After evaluating all these combinations, the configuration that optimized the performance of the model was chosen for further analysis. Specifically, for the MLP, the Adam solver with a constant learning rate, 10 neurons per hidden layer, alpha equal to 10, and the rectifier activation function (also known as the Rectified Linear Unit or ReLU) were used.
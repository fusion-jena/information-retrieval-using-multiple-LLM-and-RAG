Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

For each classification task, after experiments had been conducted, 
the best performing classifier was used to classify the whole dataset. The 

relevant texts from these predictions were then used as data from which 
training, development and testing sets were selected for the next clas-
sification task. 

2.3. Classification models 

2.3.1. Comparator models

Since BERT can only accept a maximum of 512 tokens as input, we 
used smaller subsections of the longer web articles as classifier input, an 
approach previously applied to long document classification (Fiok et al., 
2021; Sun et al., 2019). To identify the sections of text most likely to be 
relevant to the classification decision, documents were split into chunks 
of 300 tokens using the NLTK word-tokenizer. Chunks that contained 
any  of  our  original  query  terms  (i.e.  any  bat-related  or  exploitation- 
related  term)  were  retained.  If  more  than  one  chunk  contained  key-
words,  the  first  such  chunk  of  the  document  was  retained  for  use  in 
model training and evaluation. Before being fed into the BERT model, 
texts are tokenized using the BERT tokenizer. Here, we set the maximum 
number of tokens as 400 for social media posts, as most were very short, 
and 512 for web articles, meaning some texts would have been further 
truncated at this step.

2.2.2. Training and testing data 

For each classification task in the pipeline, training, development, 
and  testing  sets  were  created  through  manual  annotation  of  2000 
randomly selected data points (randomly split into datasets using 60â€“20- 
20 ratio), according to the relevancy criteria outlined in Fig.1. To avoid 
over-fitting, development sets were used to evaluate classification per-
formance during experiments, whilst the testing sets were used used to 
evaluate the final classifier for each classification task. As the best per-
forming models and training parameters were identified during exper-
imentation, the testing sets allowed us to identify whether the classifiers 
can maintain their performance when applied to unseen data.

We  also  investigated  the  extent  to  which  using  active  learning  to 
select training data could reduce the number of texts required to achieve 
peak  performance  for  BERT  models.  Our  experiments  showed  that 
classifiers  trained  using  data  selected  randomly  or  with  the  least 

EcologicalInformatics75(2023)1020766S.B. Hunter et al.                                                                                                                                                                                                                               

Fig. 4. The relationship between the number of texts in training data (increased at increments of 50) and the median F1 achieved by classifiers over five runs. The 
model used, the classification task and the dataset from which training and testing data are derived are compared. N.B. this does not include stratified training data.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The authors do not explicitly state the specific criteria used to determine when training is complete. However, they mention that development sets were used to evaluate classification performance during experiments, while testing sets were used to evaluate the final classifier for each classification task. This suggests that the authors might have monitored the performance on the development set to decide when to stop training. Additionally, they discuss investigating the effect of using active learning to select training data, which implies that they may have considered factors like the size of the training data and its impact on peak performance. Nevertheless, without explicit information about the stopping criterion, it cannot be definitively stated what exact criteria were used to determine when training was complete.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During  the  campaign,  methods  are  proposed  to  assess  inter- 
annotator  variability  and  aggregate  annotation  sets.  Those  methods 
will help to define the minimal number of annotators needed [Dubus 
et al., 2023; Walter et al., 2022]. Increasing the number of annotators 
per sample could be helpful if the variability is too high. Finally, the two 
grouping  methods:  majority  voting  (Section  2.2.3)  and  soft  labeling 
(Section 2.3.2) can also be used to produce relevant annotation sets for 
deep learning. 

6. Conclusion 

In this study, new approaches were explored to improve the accuracy 
and  efficiency  of  cetacean  vocalization  detection  using  deep  learning 
models. Leveraging multi-annotation campaigns involving both expert 
and novice annotators aimed to address the challenges associated with 
the scarcity of ground truth data in underwater PAM studies.

To  evaluate  the  performance  of  the  CNNs  trained  on  different 
annotation sets, datasets containing other SEIO PBW vocalizations and 
blue whale’s Dcall were chosen. Only a small portion of those datasets 
were used to reduce computational time, as hundreds of networks were 
trained and compared. 

For the evaluation of the SEIO PBW vocalizations, 7 h of audio sig-
nals recorded during the OHASISBIO program were used [Royer, 2009; 
Torterotot et al., 2020]. These recordings were collected at the SWAMS 
site, located in the Indian Ocean’s oceanic zone, between Kerguelen and 
Amsterdam Island. This dataset is called SWAMS in this paper. The re-
cordings were made in March 2015 using a hydrophone deployed at a 
depth of 1000 m. The sampling rate was 240 Hz. A total of 102 vocali-
zations were manually annotated by a second expert.

EcologicalInformatics81(2024)1026429G. Dubus et al.                                                                                                                                                                                                                                  

scaled  up  to  larger datasets  and  explanatory  variable  sets.  The initial 
training  of  the  annotators  could  be  a  source  of  bias,  it  has  to  be  as 
representative as possible of the datasets proposed and in line with the 
difficulty of the task [Kosmala et al., 2016].

Acknowledgments 

The authors acknowledge all the annotators that gave their time in 
this manual annotation of our datasets. We particularly mention Julie 
Saidlitz  from  Astrolabe  Expeditions1  for  driving  with  us  the  practical 
aspects of the annotation campaigns. The authors acknowledge the Ple 
de Calcul et de Donn´ees Marines5 (PCDM) for providing DATARMOR 
(storage,  data  access,  computational  resources,  visualization,  web- 
services, consultation, support services).  

Appendix A. Appendix 

A.1. Number of annotations per annotator and per label

Fig. 10. Number of positive annotations per annotator and per label.  

EcologicalInformatics81(2024)10264210G. Dubus et al.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which platform was used to deploy the deep learning model (e.g., AWS, Azure, Google Cloud platform)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given information, there is no explicit mention of which specific platform or service was used to deploy the deep learning models. Therefore, I cannot provide a definitive answer to your query.
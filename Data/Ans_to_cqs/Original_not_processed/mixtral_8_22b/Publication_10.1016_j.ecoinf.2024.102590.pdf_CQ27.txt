Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The supervised learning workflow adheres to the steps outlined in 
Sections 2.3.1–2.3.5. However, as detailed in Section 2.3.2, we deviated 
by not generating 400 random points over the entire boundary. Instead, 
we used the vegetation types described by Gertenbach (1983) to guide 
the placement of our sampling plots, from which we then collected the 
training and validation points (Fig. A.2). The creation of several small 
sampling plots within each vegetation type, instead of using one large 
area, also helped speed up the sampling process. Performing the data 
collection  within  the  sampling  plots  ensured  that  we  had  training/ 
validation  points  that  were  representative  of  the  diverse  vegetation 
within each boundary. To determine the size of the sampling plots, we 
first calculated 5% of the total area covered by the boundary, divided 
that area by the number of vegetation types intersecting the boundary

The results obtained in this study can be used as a baseline for future 
LULC  analysis  performed  with  other  methodologies,  such  as  deep 
learning  CNN  (Jagannathan  and  Divya,  2021).  Going  forward,  the 
analysis  of  aerial  images  of  KNP  taken  about  every  two  years  deep 
learning methods will be most effective and useful to map land cover or 
more specifically woody cover. Integrating large-area historical datasets 
in  land-use  and  land-cover  analysis  can  serve  as  a  resource  to  better 
understanding  long-term  landscape  changes  and  support  ecological 
monitoring programs. The results of studies such as this one can be used 
to  better  protect  and  preserve  our  natural  heritage,  enable  effective 
management  strategies,  and  contribute  to  the  conservation  of  global 
biodiversity. 

CRediT authorship contribution statement

As with any other type of remotely sensed data and image analysis 
approach, there are challenges in this study that need to be taken into 
consideration  with  the  interpretation  of  the  results.  The  biggest chal-
lenge was working with a mosaic of historical B&W aerial images from 
different years together with compiling the training/validation dataset 
for large-area, high spatial resolution aerial images. Therefore, it was 
essential  to  build  an  easy  and  efficient  sampling  strategy  and  export 
methodology. GEE played a significant role in this process, as it allowed 
different  independent  operators  to  collect  training  and  test  samples 
using a cloud-based server to run the analyses which minimized large 
data  management  and  storage.  Furthermore,  as  mentioned  in  Section 
2.4, photojob 155 (1940) was the aerial image with the highest number 
of  visible  artifacts  (i.e.,  dark  round  green  circles)  and  this  can  be

EcologicalInformatics81(2024)1025903T. Riccardi et al.                                                                                                                                                                                                                                

Table 1 
Details of the five historical photojobs used to attain full coverage of KNP for the 
period 1939–1944.  

Photojob 

Scale 

Format 

Focal 
length 

Resolution 

Area 
(ha) 

165A 

(1942) 

165B 

(1942) 
56p (1944) 
155 (1940) 

150 (1939) 

1:30000 

1:30000 

1:20000 
1:35000; 
1:21000 
1:20000 

7”x7” 

7”x7” 

7”x7” 
7”x7” 

7” 

7” 

7” 
4½” 

0.50 m 

238,727 

0.50 m 

633,260 

0.50 m 
0.75 m 

513,781 
277,629 

7”x7” 

7” 

0.50 m 

55,832

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given information, I cannot provide a specific answer regarding the deployment process of the trained deep learning model. There is no mention of the use of a deep learning model in the provided text, nor any details about how such a model might have been deployed if it existed.

However, generally speaking, after training a deep learning model, common practices include saving the model architecture and weights (model serialization), selecting a platform suitable for running the model (platform selection), and finally deploying the serialized model onto the chosen platform. This could involve integrating the model into a web application, mobile app, or edge device, depending on the intended use case.
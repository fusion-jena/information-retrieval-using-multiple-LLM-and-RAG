Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Imbalanced datasets pose challenges as the dominant classes heavily
influence training, while minority classes have minimal
impact.
Addressing the issue of imbalanced class distribution can involve
undersampling the majority classes and/or oversampling the minority
classes. Undersampling entails randomly removing samples from the
majority classes while oversampling involves duplicating samples from
the minority classes (Beyan and Fisher, 2015; Galar et al., 2011; Kubat
and Matwin, 1997). A more sophisticated and widely used approach is
the Synthetic Minority Over-sampling Technique (SMOTE) (Chawla
et al., 2002). SMOTE augments the dataset by generating synthetic
samples based on the feature space and combining features from
neighboring samples. SMOTE has been frequently applied in ecological
informatics studies, where imbalanced data is common, demonstrating
its effectiveness such as in (Bourel et al., 2021; Shin et al., 2021). In this

At the class level, disparities in performance are influenced in part by
the imbalance within the dataset. Indeed, imbalanced datasets present
challenges, as models tend to be biased towards the majority class,
resulting in difficulties in accurately recognizing and classifying in-
stances from the minority classes. As such, classes with consistently poor
performance, such as Goldsinny Chase (GC) and Model Rotation (MR),
have low sample sizes: eight and four, respectively. The limited number
of samples may hinder the network's ability to discern patterns unique to
these classes, assimilating them with more prevalent classes. While
SMOTE was employed to augment these classes, it may still be insuffi-
cient. Conversely, Fanning (FD) consistently achieves higher class ac-
curacy, being the class with the highest number of samples in the whole
dataset. The abundance of Fanning (FD) trajectories gives the model
plenty of chances to learn and discover patterns to recognize the class.

et al., 2023), which has input data sizes similar to ours while handling an
imbalanced dataset.

6.2. Analysis of class performance and dataset imbalance

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To address data bias during preprocessing of the deep learning pipeline, several techniques can be employed. One common technique is stratified splitting, which ensures that each subset of the data maintains the same percentage of samples for each target class as the complete set. This helps prevent any one class from dominating a particular subset of the data. Another technique is oversampling, which involves duplicating samples from the minority classes to balance out the dataset. On the other hand, undersampling entails randomly removing samples from the majority classes to achieve a balanced dataset. Both oversampling and undersampling aim to mitigate the impact of imbalanced datasets on machine learning algorithms. Additionally, diverse data collection can also help reduce data bias by ensuring that the dataset includes examples from all relevant subgroups or categories. Lastly, the Synthetic Minority Over-sampling Technique (SMOTE) is another commonly used method that generates synthetic samples based on the feature space and combines features from neighboring samples to augment the dataset.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We then use data augmentation to estimate σARU by adding a number 
of known ‘non-detections’ with distance of NA to our dataset of detected 
or simulated calls, increasing our total number of observations M to 300 
per ARU. We used a data augmentation variable zi,j for each observation 
to indicate whether it was a true call or not, so that the total realized 
M
number of calls including false positive CNN detections (NFPi =
j=1zi,j) 
can be modelled in relation to our data augmentation parameter ψ, pi,j 
and the binary detection outcomes of each call (yi,j)(eq. 10–12). 

∑

zi,j ∼ Bernoulli(ψ)

mui,j = zi,j*pi,j

(cid:0)
yi,j ∼ Bernoulli

mui,j

)

(10)  

(11)  

(12)

2.2. Empirical data 

2.2.1. Study area

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Bias 
Density 
Di-Lane 
Distance sampling 
Northern bobwhite 
Precision 
Spatial capture-recapture 
Telemetry

Recordings were analyzed using a previously developed CNN model 
(Nolan  et  al., 2022)  trained on covey call  spectrogram data collected 
separately to this study from Di-Lane and other sites around Georgia and 
Alabama,  USA.  Our  covey  call  acoustic  CNN  is  trained  to  detect  and 
localize all covey calls per ARU during the recording period and note 
their associated time and peak power (dB). We chose to use a relatively 
conservative  score  threshold  echoing  that  of  other  similar  studies 
ˇ
Sirovi´c, 2021; Ruff et al., 2020), where only calls that 
(Rasmussen and 
received a score of 0.95 or higher were officially classified as a positive 
bobwhite detection. All CNN detections at a 0.95 score threshold from a 
random sample of five one-hour files from each year were then manually 
reviewed to evaluate model accuracy and to calculate a false positive 
rate “i.e. how many calls were incorrectly labelled as bobwhite”. Across

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, there are some indications of methods employed to improve the quality and robustness of the dataset.

Firstly, the authors mention using data augmentation to estimate σARU by adding a number of known 'non-detections' with distance of NA to their dataset of detected or simulated calls. This technique can help mitigate potential biases caused by overrepresentation of certain types of data points in the training set. By artificially expanding the dataset with non-detection examples, they aim to create a more balanced representation of both positive and negative cases.

Secondly, the authors describe using a conservative score threshold of 0.95 or higher when classifying positive bobwhite detections. This approach helps reduce the likelihood of false positives, which could introduce bias into the dataset if left unchecked. Additionally, manual review of a random sample of CNN detections allows them to evaluate model accuracy and calculate a false positive rate, further ensuring the reliability of their results.

While these practices do not directly correspond to stratified splitting, oversampling, undersampling, or diverse data collection, they still contribute to addressing data bias within the context of this particular study.
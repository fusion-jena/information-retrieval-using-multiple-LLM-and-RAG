Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics69(2022)1016092T.D. Akinosho et al.                                                                                                                                                                                                                            

Fig. 2. Deep learning framework for highway air quality monitoring and prediction.  

∙ Integrate missing or inaccurate data from heterogeneous sources to 
enhance forecasting accuracy of the developed model. 

∙  Perform a system scalability test to determine response time and 
throughput as hardware device load increases. 

∙  Develop  and  evaluate  a  baseline  deep  learning  model  to  make 
hourly predictions of PM2.5, PM10, and NO2 concentration levels due 
to the deadly nature of these pollutants.

3. A proposed deep learning framework for highway AQ 
monitoring and prediction 

The proposed framework is a four-layered architecture composed of 
the hardware layer, data storage layer, integration layer and analytics 
layer as depicted in Fig. 2. This section introduces these layers and their 
functionalities. 

3.1. Hardware layer

(cid:0) 4,  and  steep  value  of  1.3e

4). 

(cid:0)

4.3.3. Model evaluation 

In this section, the results of the deep learning model developed are 
presented. The model was trained to make day-ahead predictions of the 
three pollutants, but first, an appropriate evaluation metric had to be 
selected.  The  top  metrics  for  regression  problems  are  mean  squared 
error/root  mean  squared  error  (MSE/RMSE),  mean  absolute  error 

Fig. 12. A plot showing the model's training and validation losses against the 
number of epochs. It is worth noting that there was a gradual decrease in both 
losses  as  the  training  epochs  increased  which  indicates  that  the  model  was 
learning. Further training beyond 20,000 epochs would have either resulted in 
overfitting or no further drop in both losses.

The analytics layer handles exploratory and inferential analysis of 
historic highway pollution data to estimate future air quality. The layer 
extracts data from the data storage layer for model training and vali-
dation.  Essential  data  pre-processing  steps  such  as  data  consistency 
verification,  target  attribute  transformation,  feature  extraction,  data 
encoding and data imputation are carried out in this layer as part of the 
first stages of training. A machine learning approach suitable for tabular 
or  time-series  data  such  as  the  historic  pollution  data  is  required  for 
estimation. Deep learning is one of many machine learning approaches 
that has stood the test of time (Akinosho et al., 2020). Frameworks and 
libraries  such  as  fastai,  scikit-learn, PyTorch  and  TensorFlow  make  it 
relatively easy to train a baseline model. Additional functionalities that 
are beginning to gain traction and could be included in implementing

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it can be inferred that the data format used in the deep learning pipeline is not explicitly mentioned. However, considering that the study focuses on predicting hourly concentrations of pollutants like PM2.5, PM10, and NO2 using historical highway pollution data, we can assume that the data format used is likely to be structured data, such as CSV files containing numerical values representing the pollutant concentrations over time. Additionally, since the paper mentions carrying out essential data pre-processing steps like data consistency verification, target attribute transformation, feature extraction, data encoding, and data imputation, it further supports the assumption that the data format used is most likely structured data rather than unstructured data like images, audio, or video.
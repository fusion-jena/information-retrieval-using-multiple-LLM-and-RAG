Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

(b) Computing infrastructure used to train both neural networks 

Infrastructure 
Search strategy 
Training duration 
(best assignment) 

Nvidia Tesla V100 (16GB) 
Manual tuning  
Segmentation 
Classification 

4 h 6 min 24 s 
9 h 22 min 34 s

2.3. Fish classification by species 

A classifier based on deep neural networks was also used for this task. 
In this case, the chosen architecture is a residual neural network, whose 
main characteristic is that it can skip some of its layers (He et al., 2016). 
In particular, the implementation of ResNet50V2 model in TensorFlow 
and  pre-trained  with  the  ImageNet  dataset  (training  parameters  in 
Table  2).  A  problem  encountered  in  initial  tests  related  to  species 

EcologicalInformatics67(2022)1014954X. Lekunberri et al.

Trained model 

-  Manually: 275 

segments  

-  Automatically: 
6921 segments  

-  Data 

augmentation: 
x12 

-  14,896 images  
-  Two trained 

models 

22 of the 30 
fishing sets 
(unannotated) 

Data of the 22 
fishing sets   

-  Annotations  
-  Species 

prediction  

-  Size 

distribution 
Comparison  

Ground truth 

validation (sec. 
2.5) 

Data from 4 fishing 
sets  

-  Official sources  
-  Estimations

mousa et al., 2020) or even the combination of several methods (Agarap, 
2019).  If  it  is  possible  to  increase  the  dataset  as  described  above,  it 
would  also  be  interesting  to  test  other  neural  network  architectures 
(Wang  and  Yang,  2019).  Other  type  of  validations  such  as  an  out-of- 
distribution approach (Hendrycks and Gimpel, 2018) may also be use-
ful.  Stricter  thresholds  for  our  predictions  were  tested,  but  too  many 
segments were discarded. If enough segments are provided, predictions 
should improve. (3) Testing with more ground truth data. Another way 
to detect possible biases that have been ignored so far is to use more 
ground truth data in the comparison. By combining this image analysis 
methodology with logbooks or number of brails recorded at every fish 
event, it would be possible to have tons of catches with its composition 
for each fishing event and the accumulated total catches. For this new

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, common techniques include dropout, which randomly drops out nodes during training to reduce co-dependencies between neurons, and L2 regularization, which adds a penalty term to the loss function to discourage large weights. These methods help prevent overfitting by reducing complexity and improving generalization.

Based on the given context, we cannot determine if these specific methods were employed in their deep learning pipeline. It is important to note that there are various ways to address overfitting, including early stopping, data augmentation, and using simpler models. Without further details, we cannot confirm whether any of these strategies were utilized.
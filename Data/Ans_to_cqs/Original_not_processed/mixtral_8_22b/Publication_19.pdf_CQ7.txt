Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

TABLE II.  

HYPERPARAMETERS DETERMINED USING GRID SEARCH 

Model Name 

Inception-V3 
MobileNet-V2 
ResNet-18 
DenseNet-121 

Batch 
Size 
48 
32 
32 
16 

Learning 
Rate 
0.05 
0.01 
0.005 
0.001 

Parameters 
# of 
Epochs 
75 
100 
150 
100 

Input Image Size 

299 (cid:3400) 299 (cid:3400) 3 
224 (cid:3400) 224 (cid:3400) 3 
224 (cid:3400) 224 (cid:3400) 3 
224 (cid:3400) 224 (cid:3400) 3 

C.  Results 

  As  Table  III  show  all  models  performed  reasonably  well 
with macro-F1 averages above 91%. Because the models are to 
be deployed on IoT edge devices, the size of each model is an 
important  consideration.  As  Table  III  shows 
the  best 
performing model was Inception-V3 with a macro Average F1 
score of 0.93, and the smallest size of 175 MB.   

TABLE III.  

BEST RESULTS FOR EACH NN ARCHITECTURE 

Model 

InceptionV3 
DenseNet-121 
ResNet-18 
MobileNetV2 

Model Size 
(MB) 
175 
446 
480 
507 

Accuracy 

94% 
93% 
92% 
93%

B.  The Training Regime 
  A 60-20-20 train/validate/test regime was used to randomly 
split the data into training, validation and testing sets.  Table I 
shows the class-wise breakdown. As the Table shows, the data 
was  unbalanced  and  the  ghost  images  accounted  for  40.15% 
percentage  of  the  images.    Similarly,  the  “other”  class 
accounted for only 5.2% of the total number of images.  In this 
first stage, SMOTE [23], oversampling or class weights were 
not used. These techniques will be pursued in a second phase of 
this research.  

TABLE I.  

THE TRAINING BREAKDOWN OF DATA 

Class 
Name 

Ghost 
Goat 
Fox 
Donkey 
Other 
Sheep 
Total 

Training
8,188 
5,767 
3,482 
1,178 
1,062 
723 
20,400 

Number of Images 

Validation 
2,705 
1,923 
1,161 
393 
354 
232 
6,768 

Testing  Total
2,754 
1,923 
1,161 
393 
354 
231 
6,816 

13,647 
9,613 
5,804 
1,964 
1,770 
1,186 
33,984 

%
40.15 
28.28 
17.07 
5.77 
5.20 
3.48 
100

Keywords—deep  learning,  transfer  learning,  convolutional 
neural  networks,  animal  classification,  camera  trap,  wildlife 
monitoring, edge computing, TensorFlow lite, raspberry pi, IoT 

I.  INTRODUCTION

%
40.15 
28.28 
17.07 
5.77 
5.20 
3.48 
100 

Inception-V3,  MobileNet-V2,  ResNet-18,  and  DenseNet-
121, were all trained on this data. All architectures were pre-
initialized  to  ImageNet  weights.  Initially,  the  models  were 
trained by freezing the feature extraction layers and training the 
classification layers only. However, better results where the F1-
score  improved  by  about  20-30%,  were  obtained  when  both 
feature extraction and classifier layers of each architecture were 
configured to be trainable. Stochastic Gradient Descent (SGD) 
was  used  as  the  optimizer  for  all  four  models.  Other  hyper-
parameters like batch size, learning rate, and number of epochs 
were  obtained  using  the  grid  search.    The  models  were 
implemented  using  Keras  (https://keras.io/).  The  resulting 
hyperparameters for each model are shown in Table II.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
A 60-20-20 train/validate/test regime was used to randomly split the data into training, validation, and testing sets. This means that 60% of the data was used for training, while 20% was used for validation and another 20% for testing. The specific numbers of images for each set can be found in TABLE I under the headings 'Training', 'Validation', and 'Testing'.
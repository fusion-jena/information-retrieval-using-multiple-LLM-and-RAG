Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

H. ovalis) with a max stride of 32. It is worth noting that data augmen-
tation is a well-proven and common technique to avoid overfitting and 
to render a model generalised. We applied random cropping, horizontal/ 
vertical flipping, image rotation, image translation, zooming, brightness 
adjustment,  contrast  adjustment,  hue  and  saturation  adjustment, 
gaussian  noise,  random  erasing  both  of  our  models  training,  which 
helped increase the size and variability of our dataset.

4.3. Qualitative evaluation

2160 images, whereas the test set contains 539 images (approximately 
20% of the dataset). 

sample images (a) and (b) are from the ECUHO-1 dataset, and sample 
images (c) and (d) are from the ECUHO-2 dataset. 

The training set contains 1736 lab-grown and 424 in situ images, and 
the test set has 469 lab-grown and 70 in situ images. The training set 
contains  95,959  ROIs  (labelled  bounding  boxes  containing  H.  ovalis 
leaves), whereas the test set contains 20,215 ROIs. The ECUHO-2 dataset 
was only used for testing and it contained 160 images (with 3593 ROIs) 
of lab-grown seagrasses. We also extracted the 70 in situ H. ovalis images 
from the test set of ECUHO-1 and named it ‘ECUHO-3’.

[

0, 1
n

∑N

n=1

AP (n)

N

mAP =

(7) 

The mAP is equal to the average of AP values for all classes. To find 
out the average of AP values, first, we need to find out the AP for each 
class and add them all. Then, we need to divide the sum by the total 
number of classes. The AP metric is also dependent on the Intersection 
Over Union (IoU) threshold. IoU is a number that quantifies the degree 
of overlap between ground truth and prediction boxes. To calculate the 
mAP  score, the average performance H. ovalis and/or the overall IoU 
threshold is taken into account. The mAP metric is used by computer 
vision researchers to measure the robustness of object detection models. 
It maximizes the effect of precision and recall by balancing the trade-off 
between them. 

4. Results and discussion

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What criteria are used to determine when training is complete (e.g., validation loss plateau)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, based on the given context, there isn't explicit information about the specific criteria used to determine when training is complete. However, typically, some commonly used criteria include reaching a certain number of epochs or iterations, achieving a desired level of accuracy or loss on the validation set, or observing a plateau in the validation loss. In this case, since the context mentions techniques like data augmentation to prevent overfitting and improve model generalization, it can be inferred that the training process might have been monitored using metrics such as validation loss or accuracy to ensure optimal performance and prevent overfitting. Nonetheless, without more specific details regarding the training procedure, it is not possible to provide a definitive answer.
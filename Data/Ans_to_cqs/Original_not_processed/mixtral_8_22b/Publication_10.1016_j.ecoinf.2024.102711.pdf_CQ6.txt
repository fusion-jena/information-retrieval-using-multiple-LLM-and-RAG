Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Data-driven models depend on a number of parameters, known as 
hyperparameters,  which  are  employed  to  enhance  and  regulate  the 
learning  procedure.  Optimal  hyperparameter  selection 
leads  to 
improved model accuracy and enhanced prediction performance (Aze-
dou et al., 2023). To fine-tune hyperparameters in data-driven models, 
the development dataset (comprising all data records except the evalu-
ation data records) is divided into two categories: the training dataset 
and the tuning dataset. Given the size constrains of the dataset used in 
the  analysis  and  the  need  to  utilize  all  development  data  records  in 
training  to  enhance  the  model’s  generalization  ability,  k-fold  cross- 
validation  was  chosen  as  the  preferred  approach  for  hyperparameter 
tuning (Saha et al., 2022). In this technique, the development dataset is 
evenly divided into k groups. During each iteration, one group called the

2.5. Automated machine learning (AutoML) 

2.6. Generative adversarial networks (GANs) 

AutoML offers a substantial level of automation, encompassing tasks 
such  as  including  data  pre-processing,  algorithm  selection,  and  opti-
mizing hyperparameters. These individual steps can pose challenges for 
non-expert data scientists, creating substantial barriers to designing and 
implementing algorithms effectively. Consequently, AutoML was intro-
duced to streamline these intricate processes for non-experts, making it 
more accessible to use these techniques correctly and efficiently (Rah-
man et al., 2023). In this study, two AutoML models, AutoGluon and 
TabPFN, were employed. 

2.5.1. AutoGluon

2.5.1. AutoGluon 

AutoGluon is an open-source AutoML library designed to democra-
tize machine learning, making it accessible to users of all skill levels. It 
simplifies  the  process  of  training  high-performance  ML  models  on 
structured  data  with  minimal  code.  The  approach  behind  AutoGluon 
emphasizes  advanced  data  processing,  deep  learning,  and  ensemble 
techniques,  rather  than  just  algorithm  selection  and  hyperparameter 
tuning.  It  automates  feature  engineering,  data  pre-processing,  model 
training,  and  ensembling,  utilizing  methods  like  multi-layer  stacking 
and  neural  network  embeddings.  The  core  training  strategy  of  Auto-
Gluon can be summarized in the following steps:  

(1)  AutoGluon trains a collection of ML models M = (m1, m2, …, mn) 
on the training data, including neural networks, tree ensembles, 
and more.

tuning dataset is excluded from the training process. The model is then 
developed using the remaining dataset (i.e. the training dataset), and its 
performance is evaluated using the tuning dataset. The average perfor-
mance of the k models developed on the validation groups represents the 
performance of the machine learning technique for the selected hyper-
parameters (Velasco Hererra et al., 2022). For this study, a value of 5 is 
assumed  for  k.  Grid  search  is  employed  to  evaluate  the  data-driven 
model’s  performance  using  each  combination  of  predefined  hyper-
parameters and identify the best hyperparameters. The maximum depth 
of the tree, the number of trees in the ensemble model, and the learning 
rate (which shows how fast the model learns) are tuned in this study as 
the influential hyperparameters (Cakiroglu et al., 2022).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps are typically involved to ensure the quality and suitability of the input data. While the specific steps may vary depending on the nature of the data and the requirements of the model, some common preprocessing techniques include:

1. Data Cleaning: This involves removing or correcting any inconsistencies, errors, or missing values in the data. This step ensures that the model is trained on accurate and reliable information.

2. Normalization/Standardization: Deep learning models often perform better when the input features have similar scales. Techniques such as Min-Max Scaling, Z-Score Normalization, or Log Transformation can be applied to rescale the data within a certain range or distribution.

3. Feature Selection/Extraction: Depending on the complexity of the problem, not all input features might contribute equally to the output. Selecting relevant features or extracting new ones through dimensionality reduction techniques (like PCA) can help improve the model's efficiency and performance.

4. Encoding Categorical Variables: Since most deep learning models work with numerical inputs, categorical variables should be converted into numerical representations. Common encoding techniques include One-Hot Encoding, Label Encoding, or Target Encoding.

5. Splitting Dataset: Before training the model, the dataset is usually split into three subsets: training set, validation set, and test set. The training set is used to train the model, while the validation set helps tune hyperparameters and prevent overfitting. Finally, the test set provides an unbiased evaluation of the model's performance.
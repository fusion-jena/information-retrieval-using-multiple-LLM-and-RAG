Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

EcologicalInformatics81(2024)1026153M. Zhou et al.                                                                                                                                                                                                                                   

Fig. 3. The performance optimization of the MLP model.  

Table 1 
Hyperparameters tuning ranges.  

Hyperparameters 

Type 

Tuning range 

The number of neural network layers 
The number of neurons per layer 
Dropout 
Learning rate 
Batch size 

Integer 
Integer 
Float 
Float 
Integer 

1–5 
64, 128, 256, 512, 1024, 2048 
0.1, 0.25, 0.35, 0.5 
0.001, 0.0001, 0.00001 
32, 64, 128, 256  

Table 2 
the optimal MLP architecture and parameters.  

Layer type 

Input 
Dense 
Batch normalization 
Dropout 
Dense 
Batch normalization 
Dropout 
Dense(output) 
Total parameters 

Layer size 

4200 
2048 
2048 
2048 
1024 
1024 
1024 
1 
10,715,137  

Parameters 

0 
8,603,648 
8192 
0 
2,098,176 
4096 
0 
1025

imbalanced small datasets and the application of machine learning algorithms to 
predict total phosphorus concentration in rivers. Eco. Inform. 76, 102138 https:// 
doi.org/10.1016/j.ecoinf.2023.102138. 

Atkinson, P.M., Tatnall, A.R., 1997. Introduction neural networks in remote sensing. Int. 

J. Remote Sens. 18, 699–709. https://doi.org/10.1080/014311697218700. 
Attri, I., Awasthi, L.K., Sharma, T.P., Rathee, P., 2023. A review of deep learning 

techniques used in agriculture. Eco. Inform. 77, 102217 https://doi.org/10.1016/j. 
ecoinf.2023.102217. 

Awadallah, M.A., Abu-Doush, I., Al-Betar, M.A., Braik, M.S., 2023. Metaheuristics for 
optimizing weights in neural networks. In: Mirjalili, S., Gandomi, A.H. (Eds.), 
Comprehensive Metaheuristics, 1st ed. Elsevier, Palestine, pp. 359–377. 
Bashir, O., Bangroo, S.A., Shafai, S.S., Senesi, N., Kader, S., Alamri, S., 2024.

After reviewing the literature, the default model was determined as 
the starting point for exploring MLP modeling. The specific structural 
parameters  of  the  default  MLP  model  can  be  referred  to  (Fine  et  al., 
2020). After optimization, the optimal MLP model was used to predict 
soil  Mn  content.  Table  2  briefly  summarizes  the  optimal  network 
structure  and  parameters.  Briefly,  the  batch  normalization  technique 
was used to accelerate the learning process (Ioffe and Szegedy, 2015), 
after which the dropout was employed to mitigate model overfitting. To 
train the MLP model, we applied the Adam optimizer with a learning 
rate of 0.0001, and used a batch size of 32 (Khan et al., 2020). The ReLu 
activation function was used in the network architecture and the early 
stopping mechanism was used to minimize overfitting (Prechelt, 1998). 
Fig. 4 shows the overall structure of the network, which comprises i)

Zhang, Z., Dalca, A.V., Sabuncu, M.R., 2019. Confidence calibration for convolutional 
neural networks using structured dropout. arXiv:1906.09551. https://doi.org/ 
10.48550/arXiv.1906.09551. 

Zhao, Q., Liu, K., Xiong, C., Yang, F., 2023. Establishment of soil quantitative detection 
model based on sparrow search algorithm. In: Li, A., Shi, Y., Xi, L. (Eds.), 6GN for 
Future Wireless Networks, 1st ed. Springer Nature Switzerland, Cham, pp. 32–39. 

EcologicalInformatics81(2024)10261511

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there is no information available regarding the code repository link of the deep learning pipeline in the given context. Therefore, it cannot be provided.
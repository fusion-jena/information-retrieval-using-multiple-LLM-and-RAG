Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The confusion matrix (Fig. 6), F1 curve (Fig. 7), P curve (Fig. 8), PR 
curve (Fig. 9), R curve (Fig. 10) and final training plots (Fig. 11) are 
shown for the training of the model with attention layer with epoch-150. 
Some training batch outputs are shown in Fig. 12, and some validation 
outputs are shown in Fig. 13. 

EcologicalInformatics79(2024)10239812B. Bhagabati et al.                                                                                                                                                                                                                              

Fig. 12. Training Output (Batch-1) showing the labels of the detected object.  

Fig. 13. Validation Output (with images from the dataset). The bounding box and class name are generated and shown for the validated object by the model.

5. Results and discussion 

In this section, the results derived from the training of the proposed 
model  and  the  results  obtained  by  deploying  the  trained  model  are 
discussed. 

5.1. Experimental result

In  order  to  determine  more  accurate  training  results  and  also  to 
explore the effect of epoch upon training result, apart from 150 epochs, 
the model with attention layer is trained with epoch values 100, 200, 
and  250  under  a  uniform  training  environment  and  with  the  same 
dataset.  The  training  summary  for  each  of  these  epochs  is  shown  in 
Tables 5, 6 and 7 for epochs 100, 200, and 250, respectively. The trends 
of mAP values with increasing epochs are shown in Figs. 14 and Fig. 15. 
The size of the dataset used for custom training is sufficiently large. 
Further, the DL model used in this work is tuned with optimized hyper 
parameters  for  which  overfitting  and  under  fitting  situations  are  pre-
vented from occurring. As mentioned above, the effort to over-train the 
model has been avoided and graded MSE convergence (Figs. 7 to 11) has 
been  adopted  to  fine-tune  the  model.  Results  obtained  for  different

Fig.  4. SENet  attention  mechanism.  Different  blocks  of  the  attention  mecha-
nism with sizes are shown. 

Table 2 
Hyper  parameters and their values used in  training the  proposed model. The 
hyper parameters are based on the optimized values for YOLOv5 COCO training 
from scratch.  

Parameter Name 

lr0 
Lrf 
momentum 
weight_decay 
warmup_epochs 
warmup_momentum 
warmup_bias_lr 
box 
cls 
cls_pw 
obj 
obj_pw 
iou_t 
anchor_t 

Value 

0.01 
0.01 
0.937 
0.0005 
3.0 
0.8 
0.1 
0.05 
0.5 
1.0 
1.0 
1.0 
0.2 
4.0 

Parameter Name 

fl_gamma 
hsv_h 
hsv_s 
hsv_v 
degrees 
translate 
scale 
shear 
Perspective 
flipud 
fliplr 
mosaic 
mixup 
copy_paste 

Value 

0.0 
0.015 
0.7 
0.4 
0.0 
0.1 
0.5 
0.0 
0.0 
0.0 
0.5 
1.0 
0.0 
0.0

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: After the model training process, several postprocessing steps can be performed depending on the specific requirements of the project or task. Some common postprocessing steps include:

1. Model Evaluation: This involves evaluating the performance of the trained model using various metrics such as accuracy, precision, recall, F1 score, etc. These metrics help in understanding how well the model has learned from the data and its ability to generalize to unseen data.
2. Confusion Matrix: A confusion matrix is a table that summarizes the predictions made by the model compared to the actual outcomes. It provides insights into the types of errors made by the model and helps identify areas where it needs improvement.
3. Saliency Maps: Saliency maps highlight the regions of input data that contribute most significantly to the output prediction. They provide visual explanations of the decision-making process of the model and help in identifying important features.
4. Hyperparameter Tuning: Postprocessing may involve further optimization of the model's hyperparameters to improve its performance. Techniques like grid search, random search, or Bayesian optimization can be employed for this purpose.
5. Ensemble Methods: In some cases, multiple models may be combined through ensemble methods to improve overall performance. Common techniques include bagging, boosting, stacking, etc.

Based on the given context, it appears that the authors have evaluated the performance of their proposed model using various metrics such as mAP values, confusion matrices, F1 curves, P curves, PR curves, and R curves. Additionally, they have presented training and validation outputs to demonstrate the effectiveness of their approach. However, there is no explicit mention of other postprocessing steps like saliency maps or hyperparameter tuning.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

•
•
•
•

Random scale: increases the scale of the image by a factor picked randomly in [1 to 10%]
Random crop: crops the image edges by a margin in [0 to 10%]
Flip horizontally: randomly mirrors the image from left to right.
Random brightness: multiplies the brightness of the image by a factor picked randomly in [0, 10].

To show the impact of data-augmentation on the performance of the detection, we analyzed
the results of the GoogleNet-based classiﬁer combined with the sliding window technique with and
without data-augmentation. The results are summarized in the two ﬁrst rows of Table 2. As we can
observe, using only ﬁne-tuning, the GoogLeNet-based model reached relatively good performance
(77.64% precision, 89.18% recall and 83.01% F1). Adding data-augmentation further increased the
performance (90.28% precision, 87.83% recall and 89.04% F1). This performance comparison was
performed under the sliding-window detection approach (see next section).

4.2.3. Training Dataset for the CNN-Classiﬁer

The design of the training dataset is key to the performance of a good CNN classiﬁcation model.
From the 82 Ziziphus individuals georeferenced by botanic experts in the training-zone, we identiﬁed
100 80 × 80-pixel image patches containing Ziziphus lotus shrubs and 100 images for Bare soil with
sparse vegetation. Examples of the labeled classes can be seen in Figure 3. We distributed the
100 images of each class into 80 images for training and 20 images for validating the obtained CNNs
classiﬁers, as summarized in Table 1.

Table 1. Training and testing datasets for both CNN and OBIA used for mapping Ziziphus lotus shrubs.
Bare soil: Bare soil and sparse vegetation; Img: 80 × 80-pixel image patches; Poly: digitized polygons.

CNN Classiﬁer

OBIA Classiﬁer

Class

Training

Validation

Training

Accuracy

Assessment

Training-Zone

Test-Zone-1

Test-Zone-2 Test-Zone-1

Test-Zone-2

Ziziphus
Bare soil

80 img
80 img

20 img
20 img

wt+1 = µwt − α∆J(wt)

(2)

where µ is the momentum weight for the current weights wt and α is the learning rate.

The network weights, wt, can be randomly initialized if the network is trained from scratch.
However, this is suitable only when a large labeled training-set is available, which is expensive in
practice. Several previous studies have shown that data-augmentation ([44]) and transfer learning ([45])
help overcoming this limitation.

•

Remote Sens. 2017, 9, 1220

13 of 22

Figure 4. Maps showing the probability of Ziziphus lotus presence according to the CNN-classiﬁer
trained with ﬁne-tuning and data-augmentation and applied on different sliding-window sizes from
38 × 38 to 385 × 385 pixels in Test-zone-1. The ﬁrst and third columns show the heatmaps of the
probability of Ziziphus lotus presence, and the second and fourth columns show the corresponding
binary maps after applying a threshold of probability greater than 50%. The white polygons correspond
to the ground-truth perimeter of each individual georeferenced in the ﬁeld with a differential GPS.

5.1.3. Detection Using GoogLeNet and ResNet under a Detection Proposals Approach

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the given context, it appears that the strategies employed to handle randomness in the deep learning pipeline include data augmentation and transfer learning. Data augmentation involves techniques such as random scaling, cropping, flipping, and adjusting brightness to increase the size and diversity of the training set. Transfer learning, on the other hand, leverages pre-trained models to improve the performance of new models, especially when limited labeled training data is available. These strategies help overcome limitations caused by random initialization of network weights and contribute to better generalization and robustness of the models. However, the context does not explicitly mention handling randomness related to specific aspects like random seed values.
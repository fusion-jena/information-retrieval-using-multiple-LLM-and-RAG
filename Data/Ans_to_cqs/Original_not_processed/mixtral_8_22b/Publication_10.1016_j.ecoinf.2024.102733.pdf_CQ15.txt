Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

During the training of the MLP, Adam optimizer (Kingma and Ba,
2015) is used, as well as the scheduler which adjusts the learning rate
when the network reaches a plateau. For the training, various parameter
values and different regularization techniques were explored, with the
option of using either Cross-Entropy or Focal Loss (Lin et al., 2017). In
detail, the learning rate was set to 0.00001 and 0.0001, batch sizes of 8,
16, 32, and 64 were tested, and weight decay was varied between
0.0001 and 0.001 with a dropout of 0.1. Additionally, Focal Loss (Lin
et al., 2017) introduce additional parameters to investigate: α and γ. The
former serves as a balancing factor, either as a fixed value for all classes
or as the inverse of each class frequency. The latter regulates the impact
of the scaling factor; specifically, when set to zero, Focal Loss is equiv-
alent to Cross-Entropy Loss. We experimented with α set as the inverse of

the decoder is composed of layers sized 3 × 128 and 6 × 64. During the
training of this model, the network weights are iteratively updated using
the Adam optimizer (Kingma and Ba, 2015). Additionally, a scheduler
adjusts the learning rate: when the loss plateaus, meaning it does not
decrease over a certain number of epochs, the learning rate is reduced.

Once the latent representations were extracted from the Autoencoder
model, an MLP was trained for the classification of fish events.
Throughout the training of MLP, Adam optimizer (Kingma and Ba,
2015) with the learning rate modified by a scheduler when the network
reaches a plateau was used. The MLP was trained for several epochs with
various parameter configurations and regularization techniques,
including both Cross-Entropy and Focal Loss, in line with the proposed
method. As regularization techniques, weight decay, batch normaliza-
tion (Ioffe and Szegedy, 2015), and dropout (Srivastava et al., 2014)
were explored. In particular, batch normalization (Ioffe and Szegedy,
2015) was tested on each layer of the network separately and on all
layers simultaneously, with the best results achieved when applied to
the third layer. Dropout has been applied to the input layer, the hidden
layers, and all layers, yielding various results in combination with other

parameters. The parameter values explored alongside the aforemen-
tioned autoencoder include the following: learning rates of 0.00005,
0.0005, 0.0001, 0.005, 0.001, and 0.01; batch sizes of 8, 16, 32, 64, and
128; weight decay values of 0, 0.00001, 0.0001, and 0.001, with
dropout set to 0.05, 0.1, 0.2, and 0.25. For the focal loss parameters, α
was defined as the inverse of class frequency, 0.25, or 0.5, while γ was
set to 0.25, 0.5, 0.75, 1, 1.5, and 2.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The deep learning pipeline employs the Adam optimizer (Kingma and Ba, 2015) throughout its training process. This optimizer is utilized in both the training of the Multi-Layer Perceptron (MLP) and the Autoencoder model. The Adam optimizer dynamically adapts the learning rate for each parameter during the training phase, making it suitable for problems involving large amounts of data or parameters. Furthermore, a scheduler is implemented to adjust the learning rate when the network reaches a plateau, i.e., when the loss function stops decreasing after a certain number of epochs. This strategy helps prevent overfitting and improves the overall performance of the models.
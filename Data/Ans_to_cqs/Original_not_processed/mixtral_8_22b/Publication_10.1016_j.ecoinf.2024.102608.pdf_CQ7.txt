Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

we selected the set of hyperparameter values that resulted in the mini-
mum  MAE  and  RMSE,  as  well  as  the  highest  R2,  for  each  machine 
learning model. Table 1 presents the resulting hyperparameter values 
for the four machine learning models.

exploring the parameter space using randomized grid search methods 
(Bergstra and Bengio, 2012). This approach helps strike a balance be-
tween model complexity and the dataset, reducing the risk of overfitting. 
This study’s main research objectives are as follows: to demonstrate

where x denotes the input features, n is the number of input features, m 
(2)
kj  are the 
weights connecting the input layer to the hidden layer, respectively, and 
(2)
k and b

(2)
j  are the hidden layer and output layer, respectively. 
Owing to their ability to understand complicated patterns in data, 
ANNs have been employed in a broad variety of applications, including 
speech recognition, image recognition, natural language processing, and 
freshwater ecology. To train properly, however, ANNs require copious 
amounts  of  data  and  can  be  computationally  intensive  (Lecun  et  al., 
2015; Rumelhart et al., 1986). 

2.5. Model performance assessment techniques 

The algorithms’ performance and parameter settings were assessed 
using a training and test set approach. Evaluation of the models involved 
analyzing key metrics such as the coefficient of determination (R2), root- 
mean-square error (RMSE), and mean absolute error (MAE), and both 
the training and test datasets.

randomly selecting a subset of the parameter grid to explore (Bergstra 
and  Bengio,  2012).  Instead  of  exhaustively  searching  through  all 
possible combinations of hyperparameters, which can be computation-
ally  expensive  and  time-consuming,  randomized  grid  search  offers 
several  advantages, like,  reduces computational cost, especially when 
dealing with a large number of hyperparameters, and it has been shown 
to find equally or more optimal solutions than an exhaustive grid search, 
particularly  when  some  hyperparameters  are  more  influential  than 
others (Bergstra and Bengio, 2012; Lee et al., 2022). We then generated 
various  cases  by  combining  hyperparameter  values  extracted  from  a 
uniform distribution and evaluated their performance. Table S4 in the 
supplementary file summarizes the hyperparameters, their ranges, and 
the intervals used in the grid search. After the randomized grid search,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: The text does not provide explicit information on how the data was split for deep learning model training. However, based on common practices in machine learning, we can infer that the data might have been divided into two sets: a training set and a test set. The training set would be used to fit the model, while the test set would be used to evaluate its performance. Additionally, there could also be a third set called the validation set, which is often used during the process of tuning hyperparameters. In this case, the authors mention using a randomized grid search method to select the best hyperparameters, but they do not specify whether a separate validation set was used for this purpose. Therefore, without further details, we cannot definitively state the exact criteria used to split the data for deep learning model training in this particular study.
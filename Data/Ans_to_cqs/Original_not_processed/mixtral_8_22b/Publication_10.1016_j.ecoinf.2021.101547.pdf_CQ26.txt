Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The  mini-batch  size  is  user-adjustable  (typically  in  binary-number 
increments,  e.g.  2,  4,  8,  16,  32,  etc.)  depending  on  the  data  set.  Too 
small  a  mini-batch  size  may  provide  insufficient  image  data  for  the 
network  to  meaningfully  learn from  some or  all  of  the  classes  within 
each iteration, while too large a mini-batch size may cause the network 
to  overfit  to  the  training  images.  Because  of  the  high  computational 
complexity  of  the  deep  learning  process,  the  mini-batch  size  is  ulti-
mately limited by the amount of memory available to the GPU (in our 
case 8 GB), factoring in the size of the images (in our case 224 × 224 
pixels × 3 bands) and the number of internal parameters in the network 
(25.6 million in the case of ResNet-50). Thus, we were limited to mini-

practices to address a substantially imbalanced dataset with deep learning. Remote 
Sens. Environ. 216, 139–153. 

Laliberte, A.S., Ripple, W.J., 2003. Automated wildlife counts from remotely sensed 

imagery. Wildl. Soc. Bull. 31 (2), 362–371. 

LaRue, M.A., Stapleton, S., Anderson, M., 2017. Feasibility of using high-resolution 
satellite imagery to assess vertebrate wildlife populations. Conserv. Biol. 31 (1), 
213–220. 

Leblanc, G., Francis, C.M., Soffer, R., Kalacska, M., de Gea, J., 2016. Spectral reflectance 
of polar bear and other large arctic mammal pelts; potential applications to remote 
sensing surveys. Remote Sens. 8 (4), 273. 

LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature 521, 436–444. 
Longmore, S.N., Collins, R.P., Pfeifer, S., Fox, S.E., Mulero-P´azm´any, M., Bezombes, F., 
Goodwin, A., De Juan Ovelar, M., Knapen, J.H., Wich, S.A., 2017. Adapting

The  diversity  of  considerations  and  possibilities  for  improvement 
highlights another general challenge of using deep learning at present: 
as a still-burgeoning technology, optimized or standardized approaches 
for different analysis tasks are not yet well-established, so the range of 
possibilities  can  seem  overwhelming,  especially  to  non-expert  users. 
Even the basic training options, some of which are listed in Table 1, are 
numerous and highly technical. A sensible approach is to initially set 
options based on a review of published studies involving similar analysis 
tasks,  then  attempt  to  refine  certain  settings  through  trial  and  error. 
However,  it  should  also  be  noted  that  the  computationally  laborious 
process of training a CNN (several hours to several days of background 
processing) imposes a practical limit on the extent of trial and error that 
can  be  undertaken.  Nevertheless,  deep  learning  is  now  clearly  over-

limitation  of deep neural networks, and one of their few aspects that 
may be viewed as inferior to classical machine-learning algorithms and 
rule-based  image  analysis: whereas  the  user can exert  a  high level  of 
control over the predefined variables at the basis of the latter (e.g. the 
yellowish tint could be designated as a requisite criterion), there is no 
way to control precisely what CNNs learn ‘under the hood’ (Ball et al., 
2017). At most, the user can carefully select and manipulate the training 
images to attempt to influence the network to learn particular charac-
teristics and/or assign more importance to certain characteristics than 
others.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Techniques such as stratified splitting, oversampling, undersampling, and diverse data collection are commonly employed to mitigate data bias during the preprocessing stage of the deep learning pipeline. Stratified splitting involves dividing the dataset into subsets, ensuring each subset maintains the same class distribution as the original dataset. This technique helps prevent any single class from being underrepresented or overrepresented in the training, validation, or test sets.

Oversampling and undersampling are methods used to balance datasets containing unequal numbers of samples across various classes. In oversampling, additional copies of minority class examples are created to match the quantity of majority class instances. Conversely, undersampling reduces the number of majority class examples to equalize the sample sizes between classes. Both strategies aim to create a balanced dataset, thereby improving model performance and reducing bias towards the dominant class.

Diverse data collection focuses on gathering a wide variety of data points representing multiple scenarios, conditions, and variations. By incorporating diverse data sources, models become better equipped to handle real-world complexities and minimize biases stemming from narrowly focused datasets.

In summary, addressing data bias during preprocessing involves employing techniques like stratified splitting, oversampling, undersampling, and diverse data collection. These methods help ensure fair representation of all classes, promote balanced datasets, and improve overall model performance.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The training dataset, including 268 sample plots with 39 features in a 
tabulated format, was used as input during the training process (opti-
mizing  hyper-parameters  and  fitting  models  with  these  data).  The 
training data set was divided into two parts, in which 80% was used for 
training/validation, and the remaining 20% was kept as unseen data for 
testing. To eliminate bias, 10-fold cross-validation was used during the 
training  process,  and  mean  RMSE  was  used  as  the  objective  function 
(lost function). Table 4 shows RMSE, MAE, and R2  using different op-
timizers. In addition, we tested the differences between the RMSEs in 
Table 4 using the Wilcoxon Signed-Rank test with paired samples. The 
differences between (XGBoost-BO vs. XGBoost-TDO, XGBoost-BOHB vs. 
XGBoost-TDO) or (LightGBM-BO vs. LightGBM-TDO, LightGBM-BOHB 
vs.  LightGBM-TDO)  are  significant.  However,  the  performance  of 
XGBoost-TDO  versus  LightGBM-TDO  appears  to  be  the  same (Fig.  3),

2. Data and methods 

The overall workflow of the proposed method is shown in (Fig. 1). 
For  simplicity,  the  workflow  starts  with  collecting  sample  plots  and 
image processing to generate a tabulated dataset, including on-field AGB 
and  independent  features.  This  data  set  is  fed  into  hybrid  models,  in 

which  Bayesian  optimizers  and  TDO  tune  the  hyperparameters  of 
XGBoost and LightGBM using RMSE as the objective function. The best 
model  (with  the  smallest  RMSE)  is  selected  to  estimate  AGB  for  the 
entire study area. This model was also interpreted to understand better 
what operates inside the ‘black box.’ More detailed descriptions of the 
study area,  data specifications and preparation, and  algorithm justifi-
cations are presented in the following sections. 

2.1. Field data

2.4. Hyperparameter optimisation 

Hyperparameter  optimization  searches  for  the  best  set  of  hyper-
parameters  for  a  machine  learning  model  that  can  perform  best  on  a 
given  task.  In  many  packages,  random  or  grid  search  is  the  default 
method  for  selecting  optimal  hyperparameters  (Snoek  et  al.,  2012). 
There are many studies on the selections of optimization algorithms for 
theoretical  problems  or  engineering  applications,  in  which  various 
versions  of  Bayesian  optimizers  and  Swarm-based  algorithms  were 
compared (Stenger et al., 2019; Stenger and Dirk, 2022).

partly delineate the cause-effect relationships in machine learning and 
confirm the generalization of the proposed method.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
To ensure the generalizability of their deep learning model, several measures were implemented. Firstly, the authors utilized a diverse dataset consisting of 268 sample plots with 39 features in a tabulated format. This diversity in the dataset helped capture a wide range of variations within the data, thereby improving the model's ability to generalize well to new, unseen data.

Secondly, the authors employed a technique called stratified splitting to divide the training dataset into two parts. Eighty percent of the data was allocated for training and validation purposes, while the remaining twenty percent was reserved exclusively for testing. By employing this strategy, the authors aimed to maintain a balance between the classes present in both the training and testing datasets, ensuring that the model could learn from a representative subset of the entire dataset.

Lastly, the authors incorporated a technique known as 10-fold cross-validation during the training process. Cross-validation is a powerful tool that helps prevent overfitting by evaluating the model's performance on multiple subsets of the training data. In this case, the authors used 10-fold cross-validation, where the original dataset was randomly partitioned into ten equal-sized subsamples. Of these ten subsamples, nine were used for training the model, while the remaining one was retained for validating its performance. This process was repeated ten times, each time with a different subset held out for validation. The average performance across all ten trials was then calculated to provide a more robust estimation of the model's true error rate.

In summary, the authors took several steps to ensure the generalizability of their deep learning model. These included utilizing a diverse dataset, implementing stratified splitting for dividing the training dataset, and incorporating 10-fold cross-validation during the training process.
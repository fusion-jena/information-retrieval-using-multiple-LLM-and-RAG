Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

The results of these integration and enrichment activities are mate-
rialized in the Harbor tier of the data platform, which provides an in-
tegrated and comprehensive view of the available data at the finest level 
of detail. The relational schema of the integrated data is discussed in 
Appendix A.2. 

3.2.3. Validation and loading of enriched H. halys captures

Fig. 5. Overview of the data platform.  

EcologicalInformatics82(2024)1027137(a)(b)(c)C. Forresi et al.                                                                                                                                                                                                                                  

Table 1 
Summary of data sources.  

Source 

Monitoring network 
Satellite images 
Weather 
Environment 

Provider 

CASE 
ESA 
ARPAE 
CER 

Frequency 

Weekly 
2–3 days 
Daily 
Yearly 

Granularity 

Monitoring task 
100 km2 granule 
25 km2 cell 
Environmental element 

Yearly size 

5.2 MB 
370 GB 
6.5 GB 
4.3 GB  

The validated data is then loaded to the Access tier through an in-
cremental  ETL (extract, transform, and load) procedure acquiring the 
validated data from the Harbor tier. 

3.2.4. Analytical fruition

The relational schema of CASE’s database is shown in Fig. A.18. CASE is based on a concept of Task, i.e., a questionnaire that must be fulfilled by a 
User (i.e., an on-field operator). Each task is composed by a set of Questions, categorized in different types (e.g., multiple answers, date picker) each 
composed by a set of predefined Answers (one of which could be an open answer, defined by the user when selecting the Given answer). The 
dynamic aspect of CASE’s questionnaires lies in the association of each possible answer with a different one to be shown next. Thus, users fulfilling the 
same task may follow different paths of questions depending on the given answers. This enables a customized and efficient user experience, as detailed 
questions on a given topic may be asked only if such a topic has been mentioned by the user. Instances of tasks (i.e., Task on geo-object) are

Data  are  organized  into  three  tiers,  in  accordance  with  multi-tier 
architectures (Ravat and Zhao, 2019; Zburivsky and Partner, 2021) to 
logically  separate  the  subsequent  processing  activities.  The  Raw  tier 
hosts the data lake (Stein and Morrison, 2014), i.e., a storage repository 
that holds the data in its raw, native format. The Harbor tier provides an 
integrated and comprehensive view of the available data at the finest 
level of detail. The Access tier provides a higher-level view of data that is 
ready to be consumed for analytical purposes. Details about the tech-
nological stack and the conceptual/logical representations of the data 
stored in the different tiers are provided in Appendix A.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Based on the provided context, there is no explicit mention of specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, it can be inferred that some form of validation process is applied to the data before being loaded onto the Access tier. Additionally, the use of incremental ETL procedures suggests that data transformation processes are employed, but the exact nature of these transformations or any potential bias mitigation strategies are not specified. Therefore, without further information, it is not possible to provide a definitive answer regarding the techniques used to address data bias during preprocessing of the deep learning pipeline.
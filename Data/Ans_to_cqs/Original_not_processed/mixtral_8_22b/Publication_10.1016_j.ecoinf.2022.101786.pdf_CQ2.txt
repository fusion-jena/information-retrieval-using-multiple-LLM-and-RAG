Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. ‘Imagenet classification with deep 
convolutional neural networks’, in. Adv. Neural Inf. Proces. Syst. 1097–1105. 
Langenk¨amper, D., et al., 2017. BIIGLE 2.0 - browsing and annotating large marine 

image collections. Front. Mar. Sci. 4 (83) https://doi.org/10.3389/ 
fmars.2017.00083. 

LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. nature 521 (7553), 436. 
Levin, L.A., 1991. Interaction between metazoans and large, agglutinating protozoans: 
implications for the community structure of deep-sea benthos. Am. Zool. 31, 
886–900. 

Levin, L.A., Gooday, A.J., 1992. Possible roles for Xenophyophores in deep-sea carbon 
cycling. In: Rowe, G.T., Pariente, V. (Eds.), Deep-Sea Food Chains and the Global 
Carbon Cycle. Kluwer, Netherlands, pp. 93–104. 

Levin, L.A., et al., 2019. Global observing needs in the Deep Ocean. Front. Mar. Sci. 6 

(241) https://doi.org/10.3389/fmars.2019.00241.

2.4. Model evaluation 

The testing dataset was composed of 500 manually annotated images 
from  the  same  transect  and  was  only  used  to  calculate  performances 
outside  the  Darknet  framework  by  comparing  CNN  predictions  with 
manual annotations. 

Predictions  come  as  a  .JSON  file  containing  the  coordinate  of  the 
bounding box of each tentative annotation as well as a confidence score 
ranging from 0.05 at minimum (lower confidence scores introduced too 
much  noise  and  therefore  were  not  recorded)  to  1  at  maximum.  Pre-
dictions made on the testing set were compared to the manual annota-
tion of the same images.

With growing anthropogenic pressure on deep-sea ecosystems, large quantities of data are needed to understand 
their ecology, monitor changes over time and inform conservation managers. Current methods of image analysis 
are too slow to meet these requirements. Recently, computer vision has become more accessible to biologists, and 
could help address this challenge. In this study we demonstrate a method by which non-specialists can train a 
YOLOV4 Convolutional Neural Network (CNN) able to count and measure a single class of objects. We apply CV 
to  the  extraction  of  quantitative  data  on  the  density  and  population  size  structure  of  the  xenophyophore 
Syringammina  fragilissima,  from  more  than  58,000  images  taken  by  an  AUV  1200  m  deep  in  the  North-East 
Atlantic.  The  workflow  developed  used  open-source  tools,  cloud-base  hardware,  and  only  required  a  level  of

challenge of collecting representative datasets in the deep sea.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
In the given context, the primary data format used in the deep learning pipeline is images. This is evident from the mention of using a YOLOV4 Convolutional Neural Network (CNN) to analyze more than 58,000 images taken by an Autonomous Underwater Vehicle (AUV). Additionally, the testing dataset consists of 500 manually annotated images, further emphasizing the use of images in the deep learning process. Furthermore, the model outputs its predictions in a JSON file containing bounding boxes and confidence scores for each detected object within the analyzed images. However, it should be noted that while the context primarily focuses on image data, other types of data such as CSV or audio files could also potentially be integrated into the deep learning pipeline depending on the specific application and objectives of the project.
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

2165

Fig. 1 The pipeline of the SPG framework. a Represents the original
input data (taking a tree and a pole as examples). b Is a hand-crafted
point cloud partition, c each partition is converted into a superpoint of

the SPG, and d is the network for superpoint embedding and
segmentation based on PointNet and GRU. The black lines represent
the superedges

with K ¼ P3
adjacency:

i¼1 ki,

E ¼ (cid:3)

3
X

i¼1

ki
K

log

(cid:2) (cid:3)
ki
K

in accordance with the optimal

ð1Þ

The linearity, planarity, and scattering are derived by the

following terms:

linearity ¼

planrity ¼

scattering ¼

k1 (cid:3) k2
k1
k2 (cid:3) k3
k1

k3
k1

ð2Þ

The linearity describes how elongated the adjacency is,
the planarity assesses how plane the adjacency is, the high-
scattering values are used to represent an isotropic and
spherical adjacency. We describe these three point features
as geometric features.

2. Model training: We process data with a deep neural
network and adjust the training epoch, batch size, and
volume size to output a classiﬁer for prediction.
3. Result output: The point cloud test set is segmented,
and we can obtain the ﬁnal segmentation results. The
output ﬁles include point cloud geometric partition
graphs, SPGs, and segmentation results.

Results and Discussion

Hu, X., & Yuan, Y. (2016). Deep-learning-based classiﬁcation for
DTM extraction from ALS point cloud. Remote Sensing, 8(9),
730.

Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating
deep network training by reducing internal covariate shift. Paper
presented in International Conference on Machine Learning,
Lille, pp. 448–456.

Landrieu, L., & Boussaha, M. (2019). Point cloud oversegmentation
with graph-structured deep metric learning. Paper presented at

123

2172

Journal of the Indian Society of Remote Sensing (September 2021) 49(9):2163–2172

the 2019 The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Landrieu, L., & Obozinski, G. (2017). Cut pursuit: Fast algorithms to
learn piecewise constant functions. Siam Journal on Imaging
Sciences, 10(4), 1724–1766.

In deep learning, convolutional neural networks (CNNs)
are widely used in 2D image processing, and the applica-
tions of CNNs have demonstrated abundant achievements.
However, in terms of point cloud processing, the process-
ing efﬁciency is still not ideal because of massive com-
putation and data storage. To date, 3D data have been
preprocessed with deep learning methods for most point
cloud processing applications. These methods convert
point clouds into a large number of 2D images and further
process the images by CNNs (Zhang et al., 2018). For
example, the points at a range of a certain point are pro-
jected into a window image and further processed by CNNs
(Hu & Yuan, 2016). Alexandre Boulch et al. proposed
SnapNet, which is also based on CNNs for point cloud
processing by converting data to snapshots (Boulch et al.,
2017). SnapNet handles snapshot data in two types of
images: RGB views and depth composite views with
geometric features. Each group of snapshots is pixelated by

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

The given context does not explicitly mention the specific data format used in the deep learning pipeline. However, it mentions that the input data consists of point clouds, which are a type of 3D data structure consisting of a collection of points in space. In this case, the point cloud data is being used for segmentation tasks using a SuperPoint Graph (SPG) framework.

Furthermore, the context mentions that some deep learning methods convert point clouds into a large number of 2D images and further process them using Convolutional Neural Networks (CNNs). This suggests that the data format used in the deep learning pipeline could be either point cloud or 2D images generated from point clouds.

Additionally, the context references several papers related to deep learning and computer vision, including one paper titled "Deep-learning-based classification for DTM extraction from ALS point cloud" (Hu & Yuan, 2016). This indicates that the authors may have used LiDAR point cloud data as their input data format.

Therefore, while the exact data format used in the deep learning pipeline is not specified, it can be inferred that the input data consists of point cloud data, potentially converted into 2D images for processing using CNNs.
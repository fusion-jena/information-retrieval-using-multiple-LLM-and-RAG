Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Our study was mainly to propose a new model based on CNN, which 
provides an efficient and effective approach for single seed classification 
and retrieval. Our network architecture, SeedNet, contains six learnable 
layers, 5 of which are convolutional, and the last is fully-connected. The 
ReLU (Rectified Linear Unit) activation is applied to the output of every 
convolutional  layer.  Moreover,  we  periodically  insert  a  pooling  layer 
between successive convolutional layers, in order to reduce the amount 
of parameters and computation in the network, and hence to also control 
overfitting. More precisely, we employ a Max pooling strategy with a 
3 × 3 filter size, and 1 × 1 stride. The input image size is 224 × 224 × 3. 
According to the chosen dataset, the output of the last fully-connected 
layer  is  fed  to  a  N-way  softmax,  which  produces  a  distribution  over 
the N class labels to predict, i.e. 6 for the Canadian and 23 for the local

3.2.2. Retrieval results 

The retrieval experiments ensued the following strategy: every image 
is used as a query image and is matched with every remaining image in 
the database. Tables 6 and 7 present the retrieval performances of all the 
networks in terms of the four considered measures by choosing the top 
100 matches, i.e. η = 100, and the two chosen similarity distances. 

3.3. Deep learning vs traditional machine learning comparison 

A comparison of deep learning approaches with traditional machine

ComputersandElectronicsinAgriculture187(2021)1062692A. Loddo et al.                                                                                                                                                                                                                                  

Table 1 
Description of the proposed network, SeedNet.  

Operation layer 

Input Image 

Convolutional Layer 

Convolutional 

Pooling Layer 

Convolutional Layer 

Pooling Layer 

Convolutional Layer 

Pooling Layer 

Convolutional Layer 

Pooling Layer 

Convolutional Layer 

Pooling Layer 

ReLu 

Max Pooling 

Convolutional 

ReLu 

Max Pooling 

Convolutional 

ReLu 

Max Pooling 

Convolutional 

ReLu 

Max Pooling 

Convolutional 

ReLu 

Max Pooling 

Inner Product Layer 

Fully Connected 

Softmax Layer 

Softmax 

Classification Layer 

Classification Output 

Filters # 

Size 

Stride value 

Activations 

Layer # 

- 

64 

- 

1 

256 

- 

1 

256 

- 

1 

256 

- 

1 

256

22 
30 
77 
118 
32 
24 
15 
55 
103 
160 
4  

ComputersandElectronicsinAgriculture187(2021)1062697A. Loddo et al.                                                                                                                                                                                                                                  

Table 5 
Summary of the experimentation results on the local dataset (StD is the deviation standard of Acc value and Time is the training time in minutes).  

Network 

AlexNet 
ResNet18 
ResNet50 
ResNet101 
GoogLeNet 
ShuffleNet 
SqueezeNet 
MobileNetV2 
InceptionV3 
VGG16 
SeedNet 

Acc 

93.43 
97.47 
96.46 
96.97 
95.45 
96.46 
95.96 
93.94 
96.46 
95.96 
97.47 

Spec 

91.08 
96.63 
94.44 
96.61 
95.06 
95.90 
95.75 
93.16 
95.99 
94.82 
99.88 

Sen 

91.36 
95.50 
94.98 
94.66 
93.47 
94.37 
94.71 
91.51 
94.81 
94.86 
96.81 

MAvG 

90.15 
96.11 
96.15 
96.34 
94.67 
95.57 
95.13 
92.67 
95.74 
94.22 
96.60 

MFM

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, there is no information available regarding the specific hardware resources used for training the deep learning models mentioned in this paper. Therefore, it cannot be determined whether GPUs, TPUs, or other types of hardware were utilized during the training process.

Based on the given context, the authors have focused primarily on describing their proposed methodology, experimental setup, and evaluation metrics rather than providing details about the computational infrastructure employed for training their models. Consequently, without additional information from the authors or further analysis of related literature, it remains unclear what type of hardware resources were used for training these deep learning models.
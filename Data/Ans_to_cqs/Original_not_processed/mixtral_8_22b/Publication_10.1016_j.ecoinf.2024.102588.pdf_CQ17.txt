Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Batch normalization and dropout layers were adopted to assist in the 
feature extraction of the convolutional layers. The feature generalization 
of  the  input  mini-batch  was  performed  by  calculating  the  mean  and 
standard deviation of the batch normalization layer to avoid covariate 
shifts  (Cooijmans  et  al.,  2016).  Overfitting  of  the  CNN  model  was 
reduced  using  a  dropout  layer  that  inactivates  random  nodes  that 
contain learnable weights (Ashiquzzaman et al., 2018). Fully connected 
layers  obtain  processed  features  using  convolutional  operations  for 
classification  or  regression  tasks.  CNN  model  training  adjusts  the 
weights  by  conducting  backpropagation,  whereas  the  loss  function 
minimizes  the  error between  the observed and  estimated results.  The 
gradient  descent  algorithm  iteratively  updates  kernel  weights  by 
calculating the partial derivative of the loss function with respect to each

data  can  be  implemented  using  the  encoded  information  of  the 
autoencoder, which concentrates the latent information of the original 
data  (Wang  et  al.,  2014;  Zabalza  et  al.,  2016).  Convolutional  neural 
networks (CNN) are deep learning architectures that have demonstrated 
outstanding performance in processing multidimensional data (Chaddad 
et al., 2021; Gupta and Bag, 2022; Ren et al., 2017). The CNN model has 
been successfully applied to the estimation of variables in aquatic en-
vironments,  such  as  water  quality  (Zhang  et  al.,  2022),  streamflow 
(Hussain  et  al.,  2020),  and  algae  (Cao  et  al.,  2022).  Therefore,  deep 
learning algorithms are suitable applications for sustainably estimating 
aquatic ecosystem health. Only one previous study applied a CNN-based 
approach, not to classify health levels but to augment aquatic ecosystem 
health data (Lee et al., 2021). Consequently, the deep learning fusion

This  study  adopted  an  autoencoder,  which  is  an  unsupervised 
learning algorithm. It consists of an encoder and a decoder network that 
work together to learn an efficient representation of input data (Badri-
narayanan  et  al.,  2017).  During  training,  the  encoder  compresses  the 
input  data  into  a  lower-dimensional  latent  space,  while  the  decoder 
reconstructs the original input data from this compressed representation 
(Ng, 2011). In particular, effective dimensionality reduction was ach-
ieved using an encoder (Wang et al., 2016). The manifold nodes of the 
encoding layer represent the nonlinear features of the input data (Charte 
et al., 2018; Jia et al., 2015b). Specifically, the encoding layer is utilized 
for  further  application  to  the  input  of  sequential  machine  learning 
models (Ahmadlou et al., 2021; Ju et al., 2015; Shahi et al., 2022). Based 
on the latent information in the encoding layer, the decoder regenerates

the vectorized input dimensions (Fig. 2b). By referencing the VGG ar-
chitecture  (Sengupta  et  al.,  2019),  19  convolutional  layers  were 
designed such that the number of kernels gradually increased from 128 
to  512  with  a  kernel  size  of  1  Ã— 3.  After  each  convolutional  feature 
extraction, batch normalization was performed by assigning an activa-
tion function to the Leaky ReLU (Fig. 2b). When the last convolutional 
layer extracted the input features, the dropout layer was placed with a 
random node inactivation of 40% (Fig. 2b). Three fully connected layers 
with 16, 32,  and 64 nodes were used to process the convolved infor-
mation. Finally, the output layer contained three nodes to estimate the 
TDI, BMI, and FAI values simultaneously. For CNN model training, the 
loss function was assigned as the mean squared error, and the stochastic 
gradient  descent  optimizer  was  Adam  (Kingma  and  Ba,  2014).  The

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text mentions two regularization methods used to prevent overfitting in the deep learning pipeline. These methods are batch normalization and dropout layers.

Batch normalization assists in feature extraction of convolutional layers by performing feature generalization of the input mini-batch. This is done by calculating the mean and standard deviation of the batch normalization layer to avoid covariate shifts.

Dropout layers help reduce overfitting of the CNN model by inactivating random nodes containing learnable weights. This method randomly sets a fraction rate of input units to 0 at each update during training time, effectively preventing complex co-adaptations on training data.

These techniques contribute to improving the overall performance and accuracy of the deep learning models by reducing overfitting and enhancing their ability to generalize well to new, unseen data.
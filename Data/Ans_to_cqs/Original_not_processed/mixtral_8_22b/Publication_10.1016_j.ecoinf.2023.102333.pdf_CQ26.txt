Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

adjustable parameters that control network architecture, optimization 
procedure,  and  regularization.  Common  hyperparameters  include 
number of layers and nodes, learning rate, dropout rate, etc. However, 
optimizing  model hyperparameters  remains imperative  for leveraging

and backpropagation. In: Proceedings of the IEEE-INNS-ENNS International Joint 
Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges 
and Perspectives for the New Millennium, 1. IEEE, Chicago, pp. 114–119. 
Li, L., Jamieson, K., De Salvo, G., et al., 2018. Hyperband: A Novel Bandit-Based 

Approach to Hyperparameter Optimization. 

Li, J., Wu, Z., Hu, Z., et al., 2020. Thin cloud removal in optical remote sensing images 

based on generative adversarial networks and physical model of cloud distortion. 
ISPRS J. Photogramm. Remote Sens. 166, 373–389. https://doi.org/10.1016/j. 
isprsjprs.2020.06.021. 

Lin, Z., Zhong, R., Xiong, X., et al., 2022. Large-scale rice mapping using multi-task 

spatiotemporal deep learning and Sentinel-1 SAR time series. Remote Sens. 14, 699. 
https://doi.org/10.3390/rs14030699.

A R T I C L E  I N F O    

A B S T R A C T    

Keywords: 
Deep learning 
Optimization algorithms 
Image processing 
Remote sensing 
Land-use and land-cover classification 
Google Earth Engine

Determining  the  optimal  DNN  architecture  is  an  active  area  of 
research. Recent studies have evaluated different techniques for tuning 
hyperparameters, such as network structure, activation functions, reg-
ularization,  and  optimization  algorithms  (Xu  and  Liang,  2021).  For 
instance,  Lecun  et  al.  (1998)  demonstrated  the  effectiveness  of  sto-
chastic  gradient  descent  (SGD)  for  DNN  training.  Though it  has  been 
shown  to  get  stuck  in  local  minima,  momentum  was  shown  to  help 
overcome  this  by  accelerating  SGD  convergence  (Luo  et  al.,  2021). 
Dropout  regularization  has  also proven  essential  for  controlling  over-
fitting  as  network  depth  and  complexity  increase  (Gupta  and  Raza, 
2020). More recently, exponential linear unit (ELU) activation functions 
have  gained  some  traction  for  addressing  ReLU  limitations  like  dead 
neurons  and  slow  convergence  (Alkhouly  et  al.,  2021;  Clevert  et  al.,

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The context does not provide specific information about techniques used to address data bias during preprocessing of the deep learning pipeline. However, common techniques mentioned in literature include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into subsets or strata based on certain criteria, ensuring each subset maintains a similar distribution of classes or features as the original dataset. This technique helps prevent class imbalance issues during training and evaluation.

Oversampling and undersampling are methods used to balance datasets with unequal class distributions. Oversampling increases the number of instances from the minority class, while undersampling reduces the number of instances from the majority class. These techniques can be applied randomly or through more sophisticated approaches like Synthetic Minority Over-sampling Technique (SMOTE) and NearMiss.

Diverse data collection focuses on gathering a wide range of examples representing various scenarios, conditions, and populations. This approach aims to reduce potential biases introduced by limited or skewed data sources, promoting fairness and generalizability in deep learning models.
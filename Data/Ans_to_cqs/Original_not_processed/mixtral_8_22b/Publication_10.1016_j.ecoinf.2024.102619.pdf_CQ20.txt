Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 5 
Classifier  training  performance:  with  tuned  hyperparameters,  mean  cross- 
validation (CV) and final training accuracy.      

illumination patterns. 

3.2. Classification performance 

Set 

Images 

Classifier 

Hyperparameters 

Mean CV 

Train 

Accuracy 

1 

6682 

2 

992 

3 

459 

SVM:Lineard 
SVM:Linear 
SVM:RBFd 
SVM:RBF 
CNN 
SVM:Lineard 
SVM:Linear 
SVM:RBFd 
SVM:RBF 
CNN 
SVM:Lineard 
SVM:Linear 
SVM:RBFd 
SVM:RBF 
CNN 

C = 23.0 

(cid:0)

– 

(cid:0) 15.0 

C = 23.0, γ = 2
lr = 0.001 
– 

(cid:0) 15.0 

C = 23.0 

– 

C = 23.75, γ = 2
lr = 0.001 
– 

C = 23.0 

– 

C = 25.25, γ = 2
lr = 0.001 

(cid:0) 15.0 

0.93 (± 0.003) 
0.93 (± 0.003) 
0.95 (± 0.003) 
0.96 (± 0.003) 
0.92 (± 0.023) 
0.87 (± 0.010) 
0.87 (± 0.010) 
0.87 (± 0.018) 
0.91 (± 0.016) 
0.90 (± 0.015) 
0.83 (± 0.021) 
0.83 (± 0.021) 
0.78 (± 0.016) 
0.86 (± 0.020) 
0.82 (± 0.020) 

1.00 
1.00 
0.97 
0.98 
0.98 
1.00 
1.00 
0.91 
0.97 
0.97 
1.00 
1.00 
0.84 
0.95 
0.95

(μ = 9.6 ± 12.09) minutes.  However,  CNN  training  required,  on 
average, 387.09 ( ± 472.32) minutes more, ranging between 67.78 and 
1015.3  (μ = 402.46 ± 491.07) minutes;  a  significant  deterioration  in 
training time. In addition, this was approximately 40–50× slower than 
the CNN þ SVM. Notably, training each CNN þ SVM with only a CPU 
was still faster than training each CNN with a GPU. 

EcologicalInformatics81(2024)10261910C.A. Game et al.

Pytorch (Paszke et al., 2019) ML framework for deep learning (CNNs). 
These libraries also include tools for data pre-processing, model selec-
tion and evaluation. To keep our model training and analysis pipelines 
comparable, we use skorch, a scikit-learn compatible neural network li-
brary that wraps PyTorch. This allows the same scikit-learn training and 
evaluation procedure to be used for both models. Skorch is also helpful 
for end-users in CNN training, as it has a clear and simple interface. It 
only requires end-users to add the prepared datasets, model and specify 
the associated hyperparameters (Table 4). Documentation for the entire 
machine learning pipeline can be found at (PyTorch, 2023) for Pytorch 
(Paszke  et  al.,  2019;  scikit-learn,  2023)  for  scikit-learn  and  (skorch, 
2022)  for  skorch.  Commercial  restrictions  apply  to  the  availability  of 
data  used  in  this  work.  However,  links  to  public  code  examples  of

workflow stages (Table 3) can be found in Supplementary Table 1. This 
work was supported by an NVIDIA GeForce RTX 2080 SUPER Graphical 
Processing Unit (GPU) with 8GB VRAM and an Intel Core i7–9700 CPU.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The study uses an NVIDIA GeForce RTX 2080 SUPER Graphical Processing Unit (GPU) with 8GB VRAM and an Intel Core i7–9700 CPU for training the deep learning model. The authors mention that using a GPU for training the Convolutional Neural Network (CNN) significantly increased the training time compared to using a Central Processing Unit (CPU), making it about 40-50 times slower than the combination of CNN and Support Vector Machine (SVM).
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.6. Cost of computation in terms of time 

The hardware configurations for the experiments in this study con-
sisted of Intel Core i7-10750H 2.60 GHz CPU, GeForce RTX 2070-MaxQ 
GPU, and 16GB RAM. As seen in Table 5, the Swin Transformer classifier 
had  the  least  cost  in  terms  of  time,  with  approximately  1.5  h  of  its 
required training time, while the CoAtNet algorithm required about 5 h 
for its training. The proposed 3DUNetGSFormer required around 3.5 h 
for its training time. 

4. Discussion 

Understanding  dynamic land  cover changes  in wetland-dominated 
regions such as Canada requires accurate mapping of diverse and com-
plex  wetland  ecosystems.  Up-to-date  maps  showing  wetlands’  extent 
and  spatial  pattern  are  critical  for  protecting  and  conserving  these 
important  ecosystems.  The  rapid  evolution  of  free  access  and  high

Where γm presents the number of training samples belonging to the 
wetland class with the highest number of training samples and γi shows 
the number of training data of other wetland classes. The 3D patches of 
Sentinel-1/2 imagery with the dimension of γi × S × S × B, as well as the 
output of the conditional map unit, are employed in each of Ui. Next is to 
transform the output of the conditional unit map (intermediate feature 
expressed by If) to a feature with the length of γi. It should be noted that S 
and B present image patch size (i.e., 8) and the number of utilized bands 
(i.e., 18). In the GAN network, a dense layer is employed first, followed 
by  a  softmax  layer.  For  the  generation  of  the  class-specific  random 
feature (Im) with the dimension of n × γ, the feature vector n = S × S × B 
times is repeated. The 3D Sentinel-1/2 image patches with the dimen-
sion  of  γi  × S  × S  × B  are  then  converted  into  a  matrix  (Pm)  with  a

Fig.  8. Wetland  classification  maps  in  pilot  site  of  a)  Avalon,  Grand  Falls-Windsor,  and  Gros  Morne  National  Park  using  b)  Vision  Transformer  (ViT),  c)  Swin 
Transformer, d) CoAtNet, and e) proposed end-to-end algorithm. 

EcologicalInformatics72(2022)1019048A. Jamali et al.                                                                                                                                                                                                                                  

Fig. 9. Examples of extracted features of an Urban (road) object utilizing three convolutional layers of the CNN network.

On the other hand, transformers are the most frequently deployed 
techniques in natural language processing (NLP) (Vaswani et al., 2017). 
Researchers  are now exploring the capability of transformers  in com-
puter  vision  and  remote  sensing,  given  their  huge  success  in  the  lan-
guage domain. They have recently proven to be practical in a variety of 
applications,  including  the  classification  of  remote  sensing  imagery 
(Bazi et al., 2021; D. Hong et al., 2021; J. He et al., 2020). Transformers 
utilize  an  attention-based  approach  rather  than  using  convolutional 
operations utilized by CNNs. As such, transformers, unlike CNNs, can 
acquire global contextual knowledge through self-attention. This means 
that transformers have a better generalization capability as compared to 
CNN algorithms. For example, the state-of-the-art vision transformer of 
Swin Transformer incorporates a hierarchical transformer with shifting

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The hardware resources used for training the deep learning models in this study include an Intel Core i7-10750H 2.60 GHz CPU, GeForce RTX 2070-MaxQ GPU, and 16GB RAM. These resources were used to train various models, including the Swin Transformer classifier, CoAtNet algorithm, and the proposed 3DUNetGSFormer. The Swin Transformer classifier had the least cost in terms of time, requiring approximately 1.5 hours for its required training time, while the CoAtNet algorithm required about 5 hours for its training. The proposed 3DUNetGSFormer required around 3.5 hours for its training time.
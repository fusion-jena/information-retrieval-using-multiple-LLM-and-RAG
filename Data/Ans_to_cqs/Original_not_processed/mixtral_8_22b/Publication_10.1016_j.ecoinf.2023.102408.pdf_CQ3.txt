Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

3.5.3. Artificial neural network 

ANNs are used for modelling complex phenomena and developing 
nonlinear regression (Haykin and Lippmann, 1994). It consists of three 
different  layers:  input,  hidden,  and  output  (Ingram  et  al., 2005).  The 
ANN uses calibration parameters, such as the learning rate and weight, 
to improve the results. The weights were randomly initialized using the 
Nguyen–Widrow method (Zhou et al., 2016). An ANN fitted with CARET 
is a single hidden-layer neural network with two hyperparameters: size 
and decay. The number of units in the hidden layer (size) and regular-
isation parameter to avoid overfitting (decay) were passed as arrays of 
1–20  and  0.025–1,  respectively.  The  Transfer/Activation  function  for 
the  model was  selected  as  the  logistic  function.  The  optimum  combi-
nation of parameters for the best model fit was based on accuracy. 

3.6. Model fitting and evaluation

The RF algorithm, which has several advantages, was used in this 
study.  It  has  been  noted  in  prior  research  that  RF  outperforms  other 
machine-learning methods, particularly when handling extensive data-
sets (Lourenço et al., 2021). RF provides accurate information regarding 
critical  factors  and  is  less  sensitive  to  parameter  adjustments  (Cutler 
et al., 2007; Zhou et al., 2016). This method uses bagging or boosting 
algorithms to randomly select predictors. Despite these advantages, RF 
has certain limitations. Using a large number of predictors can increase 
model  complexity.  It  also  requires  careful  tuning  and  hyper- 
parameterisation  such  as  “mtry”  and  “ntree”,  there  is  a  risk  of  over-
fitting  if  these  parameters  are  not  adjusted  properly.  This  study  sys-
tematically  evaluated  mtry  from  1  to  10,  aiming  to  strike  a  balance 
between  stability  and  minimise  the  Root  Mean  Square Error  (RMSE).

R.K.  Singh:  Conceptualization,  Data  curation,  Formal  analysis, 
Methodology,  Writing  –  original  draft.  C.M.  Biradar:  Conceptualiza-
tion, Funding  acquisition, Investigation, Methodology,  Project admin-
istration, Resources, Supervision, Writing – original draft. M.D. Behera: 
Conceptualization, Investigation, Methodology, Resources, Supervision, 
Writing  –  review  &  editing.  A.J.  Prakash:  Methodology,  Software, 
Writing  –  original  draft.  P.  Das:  Investigation,  Methodology,  Visuali-
zation,  Writing  –  original  draft,  Writing  –  review  &  editing.  M.R. 
Mohanta:  Data  curation,  Formal  analysis,  Software.  G.  Krishna: 
Investigation, Project administration, Visualization, Writing –  original 
draft.  A.  Dogra:  Funding  acquisition,  Project  administration,  Visuali-
zation, Writing – review & editing. S.K. Dhyani: Funding acquisition, 
Visualization, Writing – review & editing. J. Rizvi: Project administra-

overfitting issue, we used the parameter (c) to optimise the line fit and 
penalise the number of samples inside the margin. The value of c plays a

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Unfortunately, the given context does not provide any specific details about the use of data annotation techniques in the deep learning pipeline. However, I can still explain some common data annotation techniques used in deep learning pipelines. These include:

1. Bounding Box Annotation: This technique involves drawing rectangular boxes around objects within images to identify their location. Each object is assigned a unique label, allowing models to learn what each object looks like and where it typically appears.

2. Semantic Segmentation: In semantic segmentation, every pixel in an image is labeled according to the class it belongs to. For example, all pixels belonging to cars would be labeled as 'car', while those belonging to roads would be labeled as 'road'. This allows models to understand the overall layout of scenes and recognize individual objects more accurately.

3. Instance Segmentation: Similar to semantic segmentation, instance segmentation assigns labels to each pixel in an image. However, instead of grouping all instances of a particular class together, instance segmentation differentiates between individual instances of the same class. This enables models to distinguish between multiple objects of the same type within a scene.

4. Polygon Annotation: This technique involves manually tracing polygons around objects within images to create precise masks. While more time-consuming than bounding box annotation, polygon annotation produces highly detailed representations of objects, making it useful for applications requiring high levels of precision.

5. Landmark Annotation: Also known as keypoint annotation, landmark annotation involves marking specific points on objects within images. Commonly used for facial recognition tasks, landmark annotation helps models locate key features of interest, enabling them to better understand the structure of objects.
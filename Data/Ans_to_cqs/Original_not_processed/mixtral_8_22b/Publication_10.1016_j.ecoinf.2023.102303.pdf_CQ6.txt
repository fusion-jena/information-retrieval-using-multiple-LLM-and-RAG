Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

We collected the FishInTurbidWater dataset and quickly and weakly 
labeled  it  to  contribute  to  the  first  weakly-supervised  fish  dataset  in 
turbid  waters.  We,  then  used  this  dataset  to  develop  two  novel  deep 
learning networks, one using semi-supervised contrastive learning for 
significantly  accelerated  model  deployment  time,  and  one  weakly- 
supervised  model  to  shorten  deployment  time,  while  providing  high 
accuracy. 

For  semi-supervised  contrastive  learning,  we  first  trained  a  self- 
supervised  contrastive  learning  model  and  then  fine-tuned  it  on  our

Labeling Time 
Training Time 
Turnaround 

Time 
Accuracy 

Very Short 
1.0 h 

4 h 

89.4% 

Short 
6.9 h 

22 h 

94.0% 

Very Long 
~3.5 h 

94 h 

98%  

4. Discussions 

The  development  of  deep  learning  methodologies  continues  to 
advance at an astonishing rate and be applied to various applications 
ranging from biomedical (Azghadi et al., 2020), hydrological processes 
in river channels (Talukdar et al., 2023) and agricultural (Olsen et al., 
2019) systems, to marine (Laradji et al., 2021; Saleh et al., 2022b), and 
environmental (Jahanbakht et al., 2022a) sciences. The application of 
deep learning technologies has been also used in profiling the ecosystem 
services of estuarine habitats by community members (Yee et al., 2023). 
In this paper, we extend the application of deep learning methodologies 
to advance state-of-the-art underwater fish video processing techniques 
applied to turbid waters.

1.  Collecting and presenting a new fish dataset in turbid waters.  
2.  Weakly  labelling  this  dataset  to  provide  the  world’s  first  weakly 

labeled fish dataset in turbid waters.  

3.  Developing two novel deep learning models to achieve two different 

goals.  
a.  A  semi-supervised  model  for  accelerating  model  development 

speed from data collection to deployment.  

b.  A  weakly-supervised  model  for  accelerating  the  development 
time,  while  also  conserving  the  accuracy,  compared  to  the  fast 
semi-supervised model.

case are weakly labeled. 

Implemented using the Keras APIs under the TensorFlow framework, 
the first self-supervised contrastive learning phase composes of a train-
able encoder, followed by a trainable projection head. The encoder takes 
a  batch  of  input  images  and  augments  them  with  random  horizontal 
flipping, random ±7o rotation, and random ±40% brightness.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
Before training a deep learning model, several preprocessing steps can be involved depending on the specific requirements of the task and the nature of the dataset. However, based on the given context, only one preprocessing step is explicitly mentioned - image augmentation. This involves applying transformations such as random horizontal flipping, random rotation, and random changes in brightness to the input images. These transformations help increase the diversity of the training data and improve the robustness of the model. Other common preprocessing steps not mentioned in the context include normalization, scaling, and cleaning. Normalization scales pixel values to a certain range, typically between 0 and 1 or -1 and 1, which helps stabilize the learning process. Scaling resizes the images to fit the input size required by the model architecture. Cleaning removes any irrelevant or corrupted data points from the dataset.
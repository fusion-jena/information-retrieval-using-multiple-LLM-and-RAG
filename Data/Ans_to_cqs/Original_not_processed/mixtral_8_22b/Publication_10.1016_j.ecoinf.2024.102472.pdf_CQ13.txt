Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

adjusted to optimize the model (Jafarzadeh et al., 2021). Whereas node- 
size  determines  the  smallest  number  of  observations  in  a  tree  subset 
terminal node, and default value is always at 1 (Jafarzadeh et al., 2021; 
Pham et al., 2018). Gradient boosting iteration is a repetition process of 
increasing  or  decreasing  weights  in  a  training  datasets  (Huang  et  al., 
2022). All hyperparameters were tuned with a grid search method. 

2.5.1. Artificial neural networks

De Lucia, G., Lapegna, M., Romano, D., 2022. Towards explainable AI for hyperspectral 
image classification in edge computing environments. Comput. Electr. Eng. 103, 
108381. 

Domingues, G.F., Soares, V.P., Leite, H.G., Ferraz, A.S., Ribeiro, C.A.A.S., Lorenzon, A.S., 
Marcatti, G.E., Teixeira, T.R., De Castro, N.L.M., Mota, P.H.S., De Souza, G.S.A., De 
Menezes, S.J.M.D.C., Dos Santos, A.R., Do Amaral, C.H., 2020. Artificial neural 

networks on integrated multispectral and SAR data for high-performance prediction 
of eucalyptus biomass. Comput. Electron. Agric. 168, 105089. 

Douwes, E., Rouget, M., Diederichs, N., O’donoghue, S., Roy, K., Roberts, D., 2015. 
Buffelsdraai Landfill Site Community Reforestation Project. XIV World Forestry 
Congress. 

Dube, T., Mutanga, O., 2015. Evaluating the utility of the medium-spatial resolution 

Landsat 8 multispectral sensor in quantifying aboveground biomass in uMgeni 
catchment, South Africa. ISPRS J. Photogramm. Remote Sens. 101, 36–46.

During  the  MLPNN  training  phase,  initial  arbitrary  connection 
weights were assigned. Inputs were forward-fed from the input to the 
hidden  layer.  Hidden  neurons  multiplied  inputs  by  weights,  summed 
products,  and  processed  sums  through  a  transfer  function.  Results 
propagated  to  the  output  layer,  with  output  values  compared  to  ex-
pected  values  for  error  computation.  Iterative  error  back-propagation 
adjusted  connection  weights  until  reaching  a  target  minimal  error. 
The network then accurately estimated carbon stocks for both training 
and new input data without training data. This required tests to deter-
mine optimal learning rate (0.01), momentum (0.18), and training it-
erations  (500).  The  trained  network  was  subsequently  used  for  feed- 
forward predictions on continuous spatial data. 

2.5.2. Optimal predictor variable selection

Extreme  Gradient  boosting  (XGBoost)  is  known  for  its  speed  and 
accurate  prediction  process,  especially  when  dealing  with  large  and 
complex  datasets.  Its  power  lies  in  its  ability  to  minimize  bias  error 
within  and  between  spectral  predictors.  XGBoost  builds  sequential 
models to reduce errors of predictors that could potentially lower the 
estimation accuracy (Huang et al., 2022; Tamiminia et al., 2022; Zhang 
et al., 2019b). In each sequent model, the weights of all weak learners 
(predictors)  are  increased  (or  boosted)  and  the  weights  for  strong 
learners in the model are decreased (Huang et al., 2022; Pham et al., 
2021). This process is repeated until the error is reduced, and the data is 
correctly  predicted.  Furthermore,  XGB  permits  model  tuning  through 
various  hyper-parameters,  namely:  ntree,  tree  node  size  and  gradient 
boosting iteration. Ntree builds a multitude of decision trees and select

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:

In the given context, there is no explicit mention of the specific hyperparameters used in the deep learning model. However, based on the information available, we can infer that some hyperparameters have been adjusted and tuned during the model development process.

For instance, in the case of Multilayer Perceptron Neural Network (MLPNN), the optimal learning rate (0.01), momentum (0.18), and training iterations (500) were determined after conducting several tests. These parameters are crucial in adjusting the connection weights during the training phase to reach a target minimum error.

Additionally, the text mentions Extreme Gradient Boosting (XGBoost), which is known for its speed and accurate prediction process. XGBoost allows for model tuning through various hyperparameters such as ntree, tree node size, and gradient boosting iteration. Although the exact values of these hyperparameters are not mentioned, their importance in minimizing bias error within and between spectral predictors is highlighted.

Lastly, the context states that all hyperparameters were tuned using a grid search method. Grid search is a common technique used in machine learning to find the best combination of hyperparameters for a particular algorithm. It involves specifying a set of possible values for each parameter and evaluating the performance of the model for every combination of those values.

Therefore, while the specific hyperparameters used in the deep learning model are not explicitly stated, we can conclude that they include learning rate, momentum, training iterations, ntree, tree node size, and gradient boosting iteration. These hyperparameters were likely fine-tuned using a grid search method to achieve optimal performance.
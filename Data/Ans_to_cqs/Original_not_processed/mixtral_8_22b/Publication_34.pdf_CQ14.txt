Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In the NN models learning, there is still work to be done on quick automated
procedure for tuning optimization hyper-parameters, especially the initial learning
rate, and we are looking for a more suited stopping rule. On the other hand, in the
case of models of species distributions, we can imagine to minimize the number of
not null connections in the network, to make it more interpretable, and introduce an
L1-type penalty on the network parameters. This is a potential important perspective
of future works.

10 A Deep Learning Approach to Species Distribution Modelling

197

m (x, θ )

j )T al

:=

1,.

Learning of model parameters is done through optimization (minimization
by convention) of an objective function that depends on the prediction goal.
Optimization method for NN parameters θ is based on stochastic gradient descent
algorithms, however, the loss function gradient is approximated by the back-
propagation algorithm [18].

terms (which are constant relatively to

N

K

i

1

k

1

=

=

L

(yi

k)i

1,N

,k

|]

∈[|

1,K

∈[|

|]|

(λmi ,θi )i

1,N

∈[|

|]

:=

λmi ,θi (xk)

−

yi
k log(λmi ,θi (xk))

(10.4)
Following the principle of maximum likelihood, for ﬁtting a model architecture,

we minimize the objective function given in Eq. (10.4) relatively to parameters θ .

10.2.2 Links with MAXENT

For our experiment, we want to compare our proposed models to a state of the art
method commonly used in ecology. We explain in the following why and how we
can compare the chosen reference, MAXENT, with our models.

shift and help to regularize our model as well. It consists of a parameterized
centering and reduction of pre-activations. This facilitates optimization and enables
to raise the learning rate leading to a quicker convergence. At the same time, it has
a regularization effect because the centering and reduction of a neuron activation
is linked to the mini-batch statistics. The mini-batch selection being stochastic at
every iteration, a neuron activation is stochastic itself, and the model will not rely
on it when it has no good effect on prediction.

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "How are the hyperparameters of the model optimized (e.g., grid search, random search)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific methods used for hyperparameter optimization such as grid search or random search. However, it mentions that the optimization method for neural network parameters is based on stochastic gradient descent algorithms. This suggests that the hyperparameters might also be optimized using similar techniques, possibly involving some form of iterative refinement or adaptation during training. Nonetheless, without further details, it cannot be definitively stated which exact methods are employed for hyperparameter optimization.
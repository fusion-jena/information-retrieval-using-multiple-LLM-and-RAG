Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

85 

80 
10,474 
2570 
900 
783 
454 

346 

267 
223 

132 

107 

36 

32 
14 

10 

4214 

822 

162 

229 
190 
112 

37 

80 
4621 
1640 
615 
430 
322 

103 

168 
77 

80 

78 

30 

18 
1 

8 

65 

40 

32 

7 
4 
4 

3 

4 
90 
62 
16 
35 
28 

10 

13 
11 

35 

9 

5 

4 
1 

1  

EcologicalInformatics80(2024)1024992S. Villon et al.                                                                                                                                                                                                                                   

For each fold, the frames from Trainingclips  were used to train the 
models. The Testingclips were used to assess the robustness of our method 
on short sequences centered around the presence of fish presence. The 
Testingstations, common to all k-fold were used to assess the robustness of 
the method on 1-h videos, corresponding to the real use-case scenarios 
of ecological studies (Supp. Fig. 1). 

2. Methods

U-net) to discriminate sharks at an individual level on images (Le et al., 
2022). The method was able to predict with 81% accuracy whether in-
dividual sharks present in the test dataset had ever been encountered in 
the training dataset.

2. Methods 

2.1. Deep learning model 

To assess the robustness of our method, we performed a 5-fold cross- 
validation.  For  each  fold,  we  randomly  selected  80%  of  our  dataset 
Videoclips  to  train  our  model,  and  20%  to  test  it.  We  could  not  split 
directly images from the frame dataset as images from the same videos 
were very alike which can lead to a model with a low generalization 
capacity.  Using  a  80%  training/20%  testing  random  split  on  videos 
rather  than images  ensured  full independency  between  images of  the 
training set and testing set.

Fig. 1. Boxplot representations of precision, recall and F-measure metrics obtained for the 5 CNN models build through cross-validation. The results obtained on 
models trained with only the 3 common shark species of interest are in blue, and the results obtained with models trained on 27 species are shown in green. (For 
interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) 

(Testingclips) and 1-h videos (Testingstations) datasets. Pooling all species, 
the correlation coefficients R were 0.88 and 0.85 for the video clips and 
1-h  video  datasets,  respectively.  The  correlation  was  highest  for 
C.  amblyrhynchos  (R  = 0.90),  followed  by  T.  obesus  (R  = 0.72)  and 
C. melanopterus (R = 0.40) using the video clips dataset. Lower R values 
were  observed  with 
for 
C. amblyrhynchos, 0.35 for C. melanopterus, 0.44 for T. obesus). 

full-hour  video  dataset 

(0.88 

the

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer: To ensure the generalizability of their deep learning model, the authors employed several strategies. Firstly, they utilized a diverse dataset consisting of multiple species, including three common shark species of interest. Secondly, they conducted a 5-fold cross-validation process, where for each fold, they randomly selected 80% of their dataset Videoclips to train their model and 20% to test it. This approach helped prevent overfitting and improved the model's ability to perform well on unseen data. Lastly, instead of directly splitting images from the frame dataset, they opted to split videos into training and testing sets. This decision was made because images from the same videos tended to be highly similar, potentially leading to a model with poor generalization capacity. By ensuring full independence between images in the training and testing sets, the authors aimed to improve the overall performance and reliability of their deep learning model.
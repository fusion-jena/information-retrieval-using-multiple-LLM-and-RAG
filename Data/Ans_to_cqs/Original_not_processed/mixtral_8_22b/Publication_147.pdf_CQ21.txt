Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Table 2. Full comparison of training time, prediction time, optimization time (i.e., time to properly
tune the model), and hardware for each model.

Model

Training Time
(Hours)

Prediction Time (Hours)

Optimization
Time (Hours)

Hardware

CNN

XGB

4

4

2.4. Validation

4

Unknown

Desktop with 64 Gb of
RAM and one Titan X
(Maxwell) GPU.

72—can be reduced by
distributing prediction to more
cores or more machines.

2

Desktop with 64 GB of
ram and 64 logical cores.

right by 90 degrees, ﬂipped horizontally or vertically, or left as is. Since marsh and swamp wetland
classes were somewhat rarer than the other classes, during batch creation (using a batch size of 24),
we ensured that there were at least six patches containing each of those labels. Using a geometrically
decaying learning rate, the model was trained for 110 epochs, where each epoch was composed of 4800
training samples. Model training took approximately 3–4 h and prediction over all the study area at
10 m resolution took a similar amount of time. Training and prediction was done on a desktop with
64 Gb of RAM and one Titan X (Maxwell) GPU. A full comparison of computation time between the
models can be seen in Table 2.

Vienna, Austria, 2013.

77. Hacker Earth.

Beginners Tutorial on XGBoost and Parameter Tuning in R. Available
online: https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-
tutorial-on-xgboost-parameter-tuning-r/tutorial/ (accessed on 29 August 2019).

78. Parisien, M.-A.; Parks, S.A.; Miller, C.; Krawchuk, M.A.; Heathcott, M.; Moritz, M.A. Contributions of
ignitions, fuels, and weather to the spatial patterns of burn probability of a boreal landscape. Ecosystems
2011, 14, 1141–1155. [CrossRef]

79. Atienza, R. Advanced Deep Learning with Keras: Apply Deep Learning Techniques, Autoencoders, Gans, Variational
Autoencoders, Deep Reinforcement Learning, Policy Gradients, and More; Packt Publishing Ltd.: Birmingham,
UK, 2018.

Table 1. List of input variables in the XGBoost model (XGB) and convolutional neural network
(CNN) models. Each variable lists its respective data source, equation, description, and, if needed,
citation. For information on Sentinel-2 band information not listed in the table, see:
[66] and
https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/resolutions/spatial.

Variable

Data Source

Model

ARI

Sentinel-2

XGB/CNN

Band 2
Band 3
Band 4
DEM

NDVI

dNDVI

NDWI

POLr

PSRI

Sentinel-2
Sentinel-2
Sentinel-2
ALOS

Sentinel-2

Sentinel-2

Sentinel-2

Sentinel-1

Sentinel-2

CNN
CNN
CNN
CNN

CNN

XGB

CNN

XGB

CNN

REIP

Sentinel-2

XGB/CNN

Equation
(cid:17)
(cid:16) B8
(cid:16) B8
B3
B2

–

(cid:17)

-
-
-
-
(B8− B4)
(B8 + B4)

Description

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The given context does not provide explicit details about the postprocessing steps involved after the model training for either the Convolutional Neural Network (CNN) or Extreme Gradient Boosting (XGB) models. However, based on common practices in machine learning, some possible postprocessing steps could include:

1. Evaluation metrics: Calculating various evaluation metrics such as accuracy, precision, recall, F1 score, ROC curves, etc., to assess the performance of the models. These metrics help understand how well the models have learned from the training data and their ability to generalize to unseen data.

2. Confusion Matrix: Creating a confusion matrix to visualize the performance of the classification models. The confusion matrix displays the number of correct and incorrect predictions made by the classifier compared to the actual values. This helps identify any biases or imbalances in the dataset.

3. Saliency maps: Generating saliency maps for CNN models to highlight important features contributing to the final output. Saliency maps help interpret the decisions made by the model and improve transparency in deep learning applications.

4. Hyperparameter tuning: Optimizing hyperparameters using techniques like grid search, random search, or Bayesian optimization to find the best combination of parameters that maximizes the model's performance.

5. Ensemble methods: Combining multiple models to create an ensemble that improves overall performance. Common ensemble methods include bagging, boosting, stacking, and voting.

Without specific information regarding the postprocessing steps used in this case, these suggestions serve as potential options based on standard practices in machine learning.
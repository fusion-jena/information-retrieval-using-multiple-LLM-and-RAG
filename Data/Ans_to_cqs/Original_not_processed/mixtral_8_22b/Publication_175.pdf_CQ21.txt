Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

a new task with a very limited dataset, usually only 1–5 thumbnails per 
class.

1)  We  trained  a  classic  DL  algorithms  architecture  with  our  biggest 

dataset AT0 as a baseline for the DL accuracy;  

2)  We  trained  the  same  DL  architecture  with  the  same  hyper- 
parameters (e.g. model architecture and training process) but on a 
much  more  limited  dataset  (AT1).  Hyper-parameters  are  the  pa-
rameters  defining  the  architecture (number of  layers, number and 
size of convolutions, connections between layers) and the training 
process  of  a  Deep  Model  (learning  rate,  neurone  activation,  back- 
propagation compotation).;  

3)  We trained the same DL architecture with limited datasets obtained 
by  subsampling  T0  to  250  and  500  images  per  class  (here  after 
“species” when we are referring to our experiments), corresponding 
to 2500 and 5000 thumbnails in AT0;

There are two ways to tackle this problem of lack of data. The first 
one  consists  of  directly  addressing  the  data  itself,  through  data 
augmentation (Van Dyk and Meng, 2012; Wang and Perez, 2017; Wong 
et al., 2016). The second option is to change the classification algorithm. 
Few-shots learning (FSL) algorithms (Fei-fei et al., 2006; Fink, 2005) are 
designed to compute a classification task (query, noted Q) with only a 
few  thumbnails  to  train  (Support  Sets,  noted  SS),  and  it  has  been 
increasingly studied since 2017 (Finn et al., 2017). Few-shots learning 
methods are divided into three main approaches. Metric-based methods 
are embedding both queries (Q) and support sets (Ss), before assigning to 
the query a class, according to distances computed between Q and Ss 
(Sung et al., 2018; Victor and Bruna, 2018; Yanbin et al., 2019). The 
second approach consists of 1) training a model on a large database, and

2.3. Model comparison 

All  the  DL  and  FSL  models  were  tested  on  the  independent  T2 

dataset. 

First, we compared the results of experiments 1 and 2 in order to 
estimate the decrease in performance of a classic ResNet DL architecture 
when trained on a large dataset AT0 (i.e. between 11,340 and 73,450 
images per species after data augmentation, with an average of 3458 
natural thumbnails per species) or trained on a more limited dataset AT1 
(i.e. between 400 and 14,360 images per species after data augmenta-
tion, with an average of 315 natural thumbnails per species).

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The text does not provide explicit information about the specific postprocessing steps involved after the model training. However, based on common practices in machine learning, some possible postprocessing steps could include generating saliency maps to visualize which parts of the input image contribute most to the model's predictions, calculating metrics such as accuracy, precision, recall, and F1 score to evaluate the model's performance, and creating confusion matrices to understand how well the model can distinguish between different classes. These postprocessing steps would typically be applied to the test set (T2) mentioned in the text, which was used to compare the performance of various deep learning and few-shot learning models.
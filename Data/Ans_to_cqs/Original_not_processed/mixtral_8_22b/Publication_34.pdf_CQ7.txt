Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

10.3.5 Evaluation Metrics

Predictions are made for every species of E50 and several model performance
metrics are calculated for each species and for two disjoints and randomly sampled
subsets of sites: A train set (4781 sites) which is used for ﬁtting all models and a
test set (400 sites) which aims at testing models generalization capacities. Then,
train and test metrics are averaged over the 50 species. The performance metrics are
described in the following.

10.3.5.1 Mean Loss

Mean loss, just named loss in the following, is an important metric to consider
because it is relevant regarding our ecological model and it is the objective function
that is minimized during model training. The Mean loss of model m on species i
and on sites 1, . . . , K is:

Loss(m, i,

1, . . . , K

{

)
}

=

1
K

K

k

1

=

λm,θi (xk)

−

yi
k log(λm,θi (xk))

.,.,j is (d.reslat,j

46.

64

×

=

×

×

10.3.4 Detailed Models Architectures and Learning Protocol

MAXENT is learned independently on every species of E50. Similarly, we ﬁt a
classic loglinear model to give a naive reference. Then, two architectures of NN are
tested, one with a single hidden layer (SNN), one with six hidden layers (DNN).
Those models take a vector of environmental variables xk as input. As introduced
previously, we want to evaluate if training a multi-response NN model, i.e. a NN
predicting several species from a single aNh(m)
(x, θ ), can prevent overﬁtting. One
m
architecture of CNN is tested, which takes as input an array Xk. Hereafter, we
described more precisely the architecture of those models.

10.3.4.1 Baseline Models

• LGL Considering a site k, and its environmental variables vector xk, the output
p is simply the

function λLGL of the loglinear model parametrized by β
exponential of a scalar product between xk and β :

R

∈

λLGL(xk, β)

exp

βT xk

=

SNN, DNN and CNN models are ﬁtted with the package mxnet: All model
0.03, 0.03), then we
parameters were initialized with a uniform distribution U (
applied a stochastic gradient descent algorithm with a momentum of 0.9, a
batch-size of 50 (batch samples are randomly chosen at each iteration), and an initial
8. The choice of initial learning rate was critical for a good
learning rate of 10−
optimization behavior. A too big learning rate can lead to training loss divergence,

−

190

C. Botella et al.

whereas when it is too small, learning can be very slow. We stopped when the
average slope of the training mean loss had an absolute difference to 0 on the last 100
3. The learning took approximately 5 min for SNN, 10 min for
epochs inferior to 10−
DNN, and 5 h for CNN (independently of the version).

10.3.5 Evaluation Metrics

10.3.2 Species Selection

For the genericity of our results and to make sure they are not biased by the choice
of a particular category of species, we have chosen to work with a high number of
randomly chosen species. From the 7626 initial species, we selected species with
more than 300 observations. We selected amongst those a random subset of 1000
species to constitute an ensemble E1000. Then, we randomly selected 200 species
amongst E1000 to constitute E200, and ﬁnally randomly selected 50 in E200 which

10 A Deep Learning Approach to Species Distribution Modelling

183

gave E50. E50 being the main dataset used to compare our model to the baselines, we
provide in Fig. 10.1 the list of species composing it. The full dataset with species
of E1000 contains 6,134,016 observations in total (see Table 10.1 for the detailed
informations per species).

10.3.3 Environnemental Data

=

Question: %INSTRUCTIONS:
Use the provided pieces of context to answer the query in less than 400 words. If you don't know the answer, just say that you don't know, don't try to make up an answer.

%Query
Query: "What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?"

Provide your answer as follows:

Answer:::
Answer: (provide your answer to query in less than 400 words)

Answer:::

Helpful Answer:
The data is split into a train set and a test set. The train set consists of 4781 sites and is used for fitting all models. The test set comprises 400 sites and is aimed at testing the models' generalization capabilities. Both sets are randomly sampled subsets of sites. After predictions are made for every species of E50 using these datasets, several model performance metrics are calculated for each species and for both the train and test sets. These metrics are then averaged over the 50 species.
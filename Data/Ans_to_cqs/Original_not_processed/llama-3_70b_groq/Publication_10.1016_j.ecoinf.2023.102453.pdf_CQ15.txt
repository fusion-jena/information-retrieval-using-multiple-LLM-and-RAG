Answer:::

The provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD or Adam. The context discusses neural architecture search (NAS), progressive learning, and hyperparameter optimization, but it does not specify the optimization algorithms used for training the models.

However, it is worth noting that the context mentions hyperparameters such as learning rate, batch size, and dropout, which are commonly used in optimization algorithms like SGD, Adam, and others. But without explicit mention, it is unclear which specific optimization techniques are applied in the deep learning pipeline.

In summary, based on the provided context, I don't know the specific optimization techniques applied in the deep learning pipeline.
Answer::: 

According to the provided context, the optimization technique applied in the deep learning pipeline is stochastic gradient descent (SGD) with a momentum ('sgdm') optimizer. This is mentioned in the text as: "The models were trained for a maximum of 10 epochs using the stochastic gradient descent with a momentum ('sgdm') optimizer." There is no mention of the Adam optimizer in the provided context.
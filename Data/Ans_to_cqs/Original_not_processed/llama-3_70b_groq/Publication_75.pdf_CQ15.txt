Answer::: 

According to the provided context, the optimization technique applied in the deep learning pipeline is Adam. This is mentioned in the text as: "The most efficient solver for classification turned out to be Adam, as Sgdm experienced drops in performance." This indicates that Adam was used as the optimizer for the classification task and performed better than Sgdm (Stochastic Gradient Descent with Momentum). There is no mention of other optimization techniques like SGD being used in the pipeline.
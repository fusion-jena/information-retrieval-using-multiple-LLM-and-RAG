Answer: 

The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as random seed value. However, it does mention the use of dropout regularization to prevent overfitting and reduce statistical noise. Dropout regularization randomly removes different sets of neurons using a predetermined dropout rate, which results in training different neural networks and reduces overfitting.

Additionally, the context discusses the use of Random Search (RS) as an optimization method for hyperparameter tuning. RS involves selecting and evaluating a set of hyperparameters randomly to find the set with the best accuracy. This method is suitable for ill-structured global optimization problems where the objective function may be non-convex and non-differentiable.

While the context does not provide a direct answer to the query, it implies that randomness is handled through the use of regularization techniques like dropout and optimization methods like Random Search. However, it does not provide specific strategies for handling randomness in the deep learning pipeline, such as setting a random seed value.
Answer: 

The provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention that AlexNet's triumph is attributed to practical strategies like ReLU nonlinear layers and dropout regularization techniques. These techniques are used to improve the performance and prevent overfitting of the model, but they are not optimization techniques used to update the model's parameters.

Therefore, based on the provided context, I don't know what optimization techniques are applied in the deep learning pipeline.
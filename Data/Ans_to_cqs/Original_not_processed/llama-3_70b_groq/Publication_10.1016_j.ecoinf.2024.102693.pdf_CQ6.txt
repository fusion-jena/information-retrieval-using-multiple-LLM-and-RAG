Answer: 

Unfortunately, the provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. The context primarily focuses on the hyper-parameters used for training different models, their performance on the IP102 dataset, and a brief overview of transformer-like and MLP-like models. 

However, it can be inferred that the input size for all models is 224 Ã— 224, which might suggest that some form of resizing or cropping is involved in the preprocessing step. Additionally, the context does not provide any information about normalization, scaling, or cleaning, which are common preprocessing steps in deep learning. Therefore, it is not possible to determine the exact preprocessing steps involved without further information.
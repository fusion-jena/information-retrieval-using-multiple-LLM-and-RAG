Answer:::

The provided context does not explicitly mention regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention Batch Normalisation (BN) as a technique used to prevent overfitting. According to the text, the BN layer can "prevent overfitting" and "solve the problems of gradient disappearance and explosion" during the training process. 

It is worth noting that the context focuses more on model optimization, structural design, and algorithm optimization to reduce computational complexity and deployment costs, rather than regularization methods specifically. Therefore, I cannot provide a comprehensive list of regularization methods used to prevent overfitting based on the provided context.
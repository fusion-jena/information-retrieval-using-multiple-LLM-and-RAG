Answer: 

Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on explaining the architecture of a convolutional neural network (CNN) and its components, including the convolutional layer, pooling layer, fully connected layer, and output layer. It also provides mathematical formulations for the convolution operation and the output value of a neuron in the convolutional layer. However, it does not discuss optimization techniques used to train the CNN model. Therefore, I don't know the answer to this specific question based on the provided context.
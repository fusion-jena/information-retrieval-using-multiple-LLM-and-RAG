Answer:::

The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that a patch size of 224 Ã— 224 "included enough contextual noise to regularize overfitting" and enabled better generalization. This suggests that the contextual noise in the patch size may be acting as a form of regularization, but it is not a traditional regularization method like dropout or L2 regularization.

The context also mentions that training a logistic regression (LR) on the penultimate and final layers of the ensemble network improved classification performances, which may be related to regularization, but it is not a direct answer to the query.

In summary, the provided context does not explicitly mention the regularization methods used to prevent overfitting, and the mentioned techniques are not traditional regularization methods.
Answer:::

Unfortunately, the provided context does not explicitly mention regularization methods used to prevent overfitting in deep learning pipelines. The context appears to be a jumbled collection of characters, symbols, and codes, which do not provide any meaningful information related to the query.

However, I can provide a general answer to the query based on my knowledge. Common regularization methods used to prevent overfitting in deep learning pipelines include:

1. Dropout: A technique where randomly selected neurons are ignored during training, which helps prevent complex co-adaptations between neurons.
2. L1 and L2 regularization (Lasso and Ridge regression): These methods add a penalty term to the loss function to discourage large weights, which can lead to overfitting.
3. Early stopping: Stopping the training process when the model's performance on the validation set starts to degrade, which can prevent overfitting.
4. Batch normalization: Normalizing the inputs to each layer, which can help reduce overfitting by reducing the effect of internal covariate shift.
5. Data augmentation: Artificially increasing the size of the training dataset by applying transformations to the existing data, which can help prevent overfitting by reducing the risk of memorization.

Please note that these answers are not derived from the provided context, but rather from general knowledge of deep learning techniques.
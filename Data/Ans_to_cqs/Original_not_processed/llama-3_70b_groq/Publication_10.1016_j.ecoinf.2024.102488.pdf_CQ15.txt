Answer: 

The provided context does not mention specific optimization techniques used in a deep learning pipeline, such as SGD or Adam. The context appears to be related to feature selection and classification models, but it does not delve into the details of deep learning optimization techniques. 

However, it's worth noting that the context does mention LightGBM, which is a gradient boosting decision tree algorithm. Gradient boosting algorithms often use optimization techniques such as gradient descent to optimize the loss function. But the specific optimization technique used in LightGBM is not mentioned in the provided context.

In summary, the provided context does not provide information about optimization techniques used in a deep learning pipeline, such as SGD or Adam.
Answer: 

The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of batch normalization, which is a technique used to mitigate internal covariate shift and stabilize training. Batch normalization is not typically considered a regularization method in the classical sense, but it can have a regularizing effect on the model.

The context also mentions the use of a wide residual network architecture, which can help to prevent overfitting by allowing the model to learn a perturbation of the input signal at each layer rather than a transformation. This can help to reduce the risk of overfitting by making the model less prone to fitting the noise in the training data.

However, the context does not mention other common regularization methods such as dropout, L2 regularization, or early stopping. Therefore, it is not possible to determine from the provided context what regularization methods are used to prevent overfitting in the deep learning pipeline.
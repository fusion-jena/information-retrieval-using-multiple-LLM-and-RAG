Answer: 

The provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context focuses on the architecture of Convolutional Neural Networks (CNNs), the use of TensorFlow, and the application of Transfer Learning. It discusses the advantages of CNNs, the importance of large training datasets, and the use of Inception v3, a pre-trained CNN model. However, it does not provide information about the optimization techniques used during the training process. Therefore, I don't know the answer to this specific question based on the provided context.
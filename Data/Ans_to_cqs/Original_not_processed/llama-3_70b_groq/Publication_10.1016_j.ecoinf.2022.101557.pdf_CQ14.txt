Answer: The provided context does not explicitly mention how the hyperparameters of the model are optimized. It only mentions that ADAM optimization with a learning rate of 0.0001 is used to find optimal weights in the neural network. It does not provide information on how the hyperparameters such as the learning rate, batch size, and dropout rate were chosen. Therefore, it is not possible to determine whether grid search, random search, or some other method was used to optimize the hyperparameters.
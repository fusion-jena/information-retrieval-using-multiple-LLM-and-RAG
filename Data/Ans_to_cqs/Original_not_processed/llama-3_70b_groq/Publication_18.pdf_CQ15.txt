Answer: 

The provided context does not explicitly mention optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the architecture of Convolutional Neural Networks (CNNs), their applications, and techniques to improve their performance, such as data augmentation and transfer learning. 

However, it is worth noting that optimization techniques are a crucial part of the deep learning pipeline, as they are used to update the model's parameters during training. Common optimization techniques used in deep learning include SGD, Adam, RMSProp, and Adagrad, among others. These techniques are used to minimize the loss function and optimize the model's performance.

Unfortunately, the provided context does not provide specific information about optimization techniques used in the deep learning pipeline.
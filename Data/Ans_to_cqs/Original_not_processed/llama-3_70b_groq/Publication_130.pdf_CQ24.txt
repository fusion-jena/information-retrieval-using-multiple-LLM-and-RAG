Answer: 

The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention some techniques that can be related to handling randomness or variability in the data.

One such technique is oversampling, which was used to handle the case where one bird had only 1,197 images. By creating duplicates of 403 randomly sampled images, the model was able to learn from a more balanced dataset.

Another technique mentioned is the use of small batch sizes, which has been shown to improve models' generalization capability. This could help reduce the impact of randomness in the data by averaging out the effects of individual data points.

Additionally, the context mentions the use of a CNN architecture (VGG19) and the initialization of the model with pre-trained weights from the ImageNet dataset. This could help the model learn more generalizable features that are less dependent on random variations in the data.

However, it does not provide explicit information on strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value.
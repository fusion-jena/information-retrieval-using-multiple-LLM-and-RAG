Answer: 

The provided context mentions stochastic gradient descent (SGD) as an optimization technique used in the deep learning pipeline. It is stated that SGD has been shown to be effective for DNN training, but it can get stuck in local minima. To overcome this, momentum is used to accelerate SGD convergence. However, it does not mention Adam as an optimization technique. 

Additionally, the context does not provide an exhaustive list of optimization techniques used in the deep learning pipeline. It only mentions SGD and its variant with momentum. Therefore, it cannot be concluded that Adam or other optimization techniques are applied in the deep learning pipeline based on the provided context.
Answer:::

Unfortunately, the provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. The context primarily focuses on the architecture and hyperparameters of various deep learning models, such as Unet, Pspnet, and Deeplabv3+, used for semantic segmentation tasks. It does not provide information on preprocessing steps like normalization, scaling, or cleaning.

However, it is common practice in deep learning to perform certain preprocessing steps before training a model. These steps may include:

* Normalization: scaling the input data to a common range, usually between 0 and 1, to prevent features with large ranges from dominating the model.
* Scaling: resizing images to a fixed size, which is often necessary for convolutional neural networks (CNNs).
* Cleaning: removing noise, handling missing values, and correcting errors in the data.

Since the context does not provide specific information on these preprocessing steps, it is unclear what exact steps were taken before training the models.
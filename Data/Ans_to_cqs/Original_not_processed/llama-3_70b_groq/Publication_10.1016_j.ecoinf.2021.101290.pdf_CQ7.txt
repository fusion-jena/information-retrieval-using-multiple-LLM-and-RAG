Answer: 

Unfortunately, the provided context does not explicitly mention the criteria used to split data for deep learning model training. The context primarily focuses on various topics such as deep learning frameworks, event-stream processing, optogenetics, and machine learning applications in biology. It does not provide information on data splitting criteria for deep learning model training.

However, in general, the common practice is to split the data into training, testing, and validation sets based on the following criteria:

* Training set: This set is used to train the model, and it typically consists of 60-80% of the total data.
* Testing set: This set is used to evaluate the performance of the model, and it typically consists of 20-40% of the total data.
* Validation set: This set is used to tune the hyperparameters of the model, and it typically consists of 10-20% of the total data.

The exact proportion of data allocated to each set may vary depending on the specific problem, dataset, and model architecture. But without further information, it's difficult to provide more specific criteria.

In summary, the provided context does not provide the criteria used to split data for deep learning model training. However, the general practice is to split the data into training, testing, and validation sets based on the proportion of data allocated to each set.
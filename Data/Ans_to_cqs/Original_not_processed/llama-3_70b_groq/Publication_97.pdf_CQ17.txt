Answer::: 

Based on the provided context, the regularization methods used to prevent overfitting in the deep learning pipeline mentioned are:

1. Dropout: This is explicitly mentioned in the context as one of the parameters tuned in the grid search for the U-Net architecture, with a dropout rate of 0.5 being used in the best-performing model.

2. Batch Normalization: Although not typically considered a regularization method in the classical sense, batch normalization can help reduce overfitting by reducing internal covariate shift. It is mentioned in the context as being applied after each convolutional layer, and its application is cited as contributing to good results in image classification problems using CNNs.

No mention is made of L2 regularization in the provided context. However, data augmentation is also used to prevent overfitting, which involves applying random transformations to the training images, such as random changes to RGB channels and random vertical and horizontal flips.
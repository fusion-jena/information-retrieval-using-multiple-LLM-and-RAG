Answer: 

The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention that techniques like ensemble methods (e.g., bagging, weighted average) have been empirically proven to mitigate overfitting issues in Landslide Susceptibility Mapping (LSM). Additionally, it highlights the advantages of XGBoost, which includes its ability to prevent overfitting and promote simpler models. 

It is worth noting that the context focuses more on the application of machine learning models, including deep learning models like CNN and RCNN, in LSM, rather than discussing specific regularization methods used in deep learning pipelines. Therefore, I don't know the specific answer to the query based on the provided context.
Answer::: 

The hyperparameters used in the deep learning models vary depending on the architecture and task. 

For action recognition, the three ResNet variants use an initial learning rate of 0.001, which is decreased every 10 epochs by multiplying it with γ = 0.1. The optimizer used is Stochastic Gradient Descent (SGD) with a momentum of 0.9.

In another instance of action recognition, the learning rate starts at 0.0005, and the momentum is 0.9, with a weight decay of 0.0005. The learning rate is decreased every 10 epochs by multiplying it with γ = 0.1. A warmup iteration is used in the first epoch with a warmup factor of 1/1000.

For the SlowFast architecture, the initial base learning rate is 0.1, with 5 warmup epochs using a warmup learning rate of 0.001. The momentum is set to 0.9, and the weight decay parameter is 4 × 10^(-5). The learning rate is decreased with a cosine strategy.

In general, the optimization technique used is Stochastic Gradient Descent (SGD), and the loss function is the cross-entropy loss.
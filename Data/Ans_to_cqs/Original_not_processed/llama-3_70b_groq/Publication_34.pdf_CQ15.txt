Answer::: 

According to the provided context, the optimization technique applied in the deep learning pipeline is stochastic gradient descent (SGD) with a momentum of 0.9. This is mentioned in the text as "we applied a stochastic gradient descent algorithm with a momentum of 0.9...". There is no mention of Adam or any other optimization techniques being used.
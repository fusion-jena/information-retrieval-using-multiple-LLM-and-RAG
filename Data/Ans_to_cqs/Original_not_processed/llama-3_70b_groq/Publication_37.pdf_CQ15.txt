Answer::: 

According to the provided context, the optimization technique applied in the deep learning pipeline is mini-batch stochastic gradient descent (SGD) with a constant learning rate of 0.01. There is no mention of Adam or any other optimization techniques being used.
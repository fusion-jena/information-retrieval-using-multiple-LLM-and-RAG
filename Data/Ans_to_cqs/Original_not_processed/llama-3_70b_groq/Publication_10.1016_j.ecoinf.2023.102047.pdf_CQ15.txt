Answer: 

The provided context does not explicitly mention the optimization techniques used in the deep learning pipeline. It mentions the "momentum optimiser value of 0.5" in the model configuration, but it does not specify the optimization algorithm used. 

However, it can be inferred that the momentum optimizer is used, but the specific algorithm (e.g., SGD, Adam) is not mentioned. Momentum optimizer is a technique used to accelerate the convergence of stochastic gradient descent (SGD) by incorporating the momentum of previous gradients into the current update. 

Therefore, it is possible that SGD with momentum is used, but it is not explicitly stated. Other optimization algorithms like Adam, RMSProp, or Adagrad might also be used, but there is no evidence to support this in the provided context.
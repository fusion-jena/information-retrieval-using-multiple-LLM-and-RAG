Answer::: 

According to the provided context, the optimization technique applied in the deep learning pipeline is the Adam optimizer. Specifically, it is mentioned that "Optimization is carried out using the Adam optimizer with a Momentum of 0.9 and a Batch size of 16." This indicates that the Adam optimizer is used to optimize the model's parameters during training, with a momentum value of 0.9 and a batch size of 16. There is no mention of other optimization techniques such as SGD being used in the pipeline.
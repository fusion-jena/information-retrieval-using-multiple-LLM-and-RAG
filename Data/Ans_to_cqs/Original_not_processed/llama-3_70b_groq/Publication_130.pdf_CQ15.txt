Answer: 

The provided context does not explicitly mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention "different hyper-parameters settings (e.g. learning rate)" which suggests that the authors are aware of the importance of hyperparameter tuning in deep learning models. 

Unfortunately, the context does not provide more information about the specific optimization techniques used in the pipeline. Therefore, I don't know the answer to this question based on the provided context.
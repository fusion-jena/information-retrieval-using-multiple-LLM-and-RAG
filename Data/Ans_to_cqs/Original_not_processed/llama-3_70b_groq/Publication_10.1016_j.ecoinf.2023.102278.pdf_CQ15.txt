Answer: 

According to the provided context, the optimization techniques applied in the deep learning pipeline are Adam and Stochastic Gradient Descent (SGD). Specifically, the Adam optimizer with a fixed learning rate of 1.0⋅10⁴ was chosen after comparing with SGD with the same learning rate, weight decay of 1.0⋅10⁻³, and momentum of 1.0⋅10⁻³. The Adam optimizer was selected because it achieved a 1%-2% increase in accuracy compared to using SGD with the recommended parameters by L. Smith (2018).
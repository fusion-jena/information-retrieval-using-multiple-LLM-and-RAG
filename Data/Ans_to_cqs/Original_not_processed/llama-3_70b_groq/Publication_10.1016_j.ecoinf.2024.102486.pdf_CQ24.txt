Answer: 

Based on the provided context, it appears that one strategy employed to handle randomness in the deep learning pipeline is to store variables such as seeds for random number generation locally, using registers or shared memory, rather than in global memory. This approach, as described in Algorithm 2, helps avoid the high read/write latency associated with global memory, allowing for more efficient generation of random numbers.

Additionally, the context mentions that algorithms for generating random numbers are only asymptotically unbiased, which may not be optimal for applications that require resampling from highly skewed probability distributions. This suggests that alternative strategies may be employed to handle randomness in such cases, although the specific strategies are not explicitly mentioned.

It is also worth noting that the context does not provide a comprehensive overview of strategies for handling randomness in the deep learning pipeline. Therefore, it is likely that other strategies are employed in practice, but they are not mentioned in the provided context.

In summary, storing random seeds locally and using asymptotically unbiased algorithms are two strategies employed to handle randomness in the deep learning pipeline, but the context does not provide an exhaustive list of strategies.
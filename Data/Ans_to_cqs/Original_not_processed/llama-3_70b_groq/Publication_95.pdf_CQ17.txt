Answer:::

The provided context does not explicitly mention any regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention data augmentation as a technique used during training. Data augmentation can be considered a form of regularization, as it increases the size of the training set and helps prevent overfitting by exposing the network to more diverse images.

The three-step data augmentation procedure mentioned in the context includes randomly flipping images horizontally, scaling the contrast of each image channel, and randomly cropping the image to a square. This augmentation procedure helps the network learn more robust features and reduces the risk of overfitting.

It's worth noting that the context does not mention other common regularization techniques, such as dropout, L1 or L2 regularization, early stopping, or batch normalization. Therefore, it cannot be concluded that these methods were used in the pipeline.
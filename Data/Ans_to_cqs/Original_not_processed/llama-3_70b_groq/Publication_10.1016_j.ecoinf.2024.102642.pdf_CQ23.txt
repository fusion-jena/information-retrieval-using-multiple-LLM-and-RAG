Answer: 

Unfortunately, the provided context does not explicitly mention measures taken to ensure the generalizability of the deep learning model, such as using a diverse dataset, cross-validation, or stratified splitting. The context primarily focuses on the annotation process, annotation variability, and methods to aggregate annotation sets. It discusses the use of multi-annotation campaigns involving both expert and novice annotators, and the application of majority voting and soft labeling to produce relevant annotation sets for deep learning. 

However, it is mentioned that consistent hyperparameters were maintained across all training instances for each task, and early stopping was used to prevent overfitting. This suggests that some measures were taken to ensure consistency and prevent overfitting, but it does not provide direct information on ensuring generalizability. Therefore, I don't know the specific measures taken to ensure the generalizability of the deep learning model.
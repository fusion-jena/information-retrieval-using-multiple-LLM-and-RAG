Answer: 

The provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the hyperparameter tuning process using randomized grid search methods and the evaluation of machine learning models using metrics like R2, RMSE, and MAE. 

It discusses the advantages of randomized grid search over exhaustive grid search, particularly when dealing with a large number of hyperparameters. However, it does not provide information about the optimization techniques used during the training process.

Therefore, I don't know the answer to this query based on the provided context.
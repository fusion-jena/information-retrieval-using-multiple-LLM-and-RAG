Answer: According to the provided context, the optimization technique applied in the deep learning pipeline is Stochastic Gradient Descent (SGD) with a momentum of 0.9. This is mentioned in the section 2.3. Data splitting and image tiling, where it is stated that "Stochastic Gradient Descent (SGD) was used as the model's optimizer with a momentum of 0.9." There is no mention of Adam or any other optimization techniques being used in the pipeline.
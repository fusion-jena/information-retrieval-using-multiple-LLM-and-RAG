Answer: According to the provided context, the optimization techniques applied in the deep learning pipeline are Adam, SGD (sgdm), and RMSProp. These optimizers are used as solvers for training the network, and their performance is compared in terms of RMSE (Root Mean Squared Error) for different input time series lengths. Specifically, the context suggests that Adam outperforms other optimizers, especially when the input time series length is set to 5 days, and therefore, Adam is adopted as the solver for training the network.
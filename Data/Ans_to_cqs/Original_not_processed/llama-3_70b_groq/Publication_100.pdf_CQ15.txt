Answer:::
Answer: The optimization technique applied in the deep learning pipeline is the Adam variant of stochastic gradient descent (SGD). This is mentioned in the context as "the Adam (Kingma and Ba, 2015) variant of stochastic gradient descent (SGD), which adaptively scales the magnitude of the parameter updates based on the statistics of previous updates to speed up convergence." Additionally, the hyper-parameters for the Adam optimizer are specified as 𝛽1 = 0.9, 𝛽2 = 0.999, and 𝜖 = 10−8.
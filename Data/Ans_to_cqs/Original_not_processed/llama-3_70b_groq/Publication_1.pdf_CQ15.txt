Answer::: 

Based on the provided context, the optimization technique mentioned is stochastic gradient descent (SGD). It is mentioned in the context of training deep learning models in a supervised fashion. However, it does not explicitly mention other optimization techniques like Adam. Therefore, we can conclude that SGD is one of the optimization techniques used in the deep learning pipeline, but we cannot confirm if Adam or other techniques are used as well.
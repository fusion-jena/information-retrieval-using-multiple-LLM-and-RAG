Answer: 

The provided context does not mention specific optimization techniques used in a deep learning pipeline, such as SGD or Adam. The context primarily focuses on machine learning models, particularly LightGBM, and its hyperparameters. It discusses the importance of tuning hyperparameters to address overfitting and improve model accuracy. 

However, it does mention the "learning_rate" as one of the hyperparameters in LightGBM, which is a common parameter in many optimization algorithms, including SGD and Adam. But it does not explicitly state the use of these optimization techniques in the pipeline. Therefore, I cannot provide a definitive answer to this query based on the provided context.
Answer: 

The provided context does not mention specific optimization techniques like SGD or Adam, which are commonly used in deep learning pipelines. The context focuses on hyperparameter optimization for machine learning models, specifically XGBoost and LightGBM, using Bayesian optimizers and TDO (Tree-based Distributed Optimization). It does not delve into the optimization techniques used within the deep learning pipeline. Therefore, I don't know the answer to this specific question based on the provided context.
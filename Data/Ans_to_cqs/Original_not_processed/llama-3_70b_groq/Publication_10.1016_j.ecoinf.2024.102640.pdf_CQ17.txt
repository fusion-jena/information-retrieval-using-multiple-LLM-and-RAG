Answer: 

Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. The context primarily focuses on the architecture and performance of various models, such as SegFormer, FCN, HRNet, PSPNet, and DeeplabV3+, but it does not discuss regularization techniques.

However, it is worth noting that the context does mention the use of the Adam optimizer, which is a popular optimization algorithm used in deep learning. The Adam optimizer has built-in mechanisms to adapt the learning rate and help prevent overfitting, but it is not a regularization method per se.

In the absence of explicit information, I cannot provide a specific answer to the query. If you're looking for information on regularization methods, I can suggest that dropout and L2 regularization are indeed common techniques used to prevent overfitting in deep learning models, but this information is not derived from the provided context.
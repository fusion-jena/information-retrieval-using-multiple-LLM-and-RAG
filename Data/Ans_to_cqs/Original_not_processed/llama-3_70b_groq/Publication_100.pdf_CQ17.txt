Answer::: 

Based on the provided context, the regularization methods used to prevent overfitting in the deep learning pipeline are L2 regularization (also known as weight decay) and an implicit Bayesian approximation through the use of the Adam optimizer. 

The context mentions that "we apply weight decay to control the strength of the unit Gaussian prior, with an empirically chosen magnitude of 10−3 that is inversely proportional to the hyperparameter λ from Eq. (1)". This indicates that L2 regularization is used to prevent overfitting.

Additionally, the context cites the paper "Gal, Y., Ghahramani, Z., 2016. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning." which suggests that dropout is also used as a Bayesian approximation to prevent overfitting. However, it is not explicitly mentioned in the context that dropout is used in the pipeline.

It is worth noting that the context does not explicitly mention other regularization methods such as early stopping, data augmentation, or batch normalization.
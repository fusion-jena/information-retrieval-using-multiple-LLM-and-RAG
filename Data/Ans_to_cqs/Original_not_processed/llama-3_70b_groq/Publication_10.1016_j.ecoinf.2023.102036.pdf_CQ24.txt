Answer:::

Based on the provided context, it appears that the strategy employed to handle randomness in the deep learning pipeline is Bayesian optimization. This is mentioned in the text as "The parameters were tuned with Bayesian optimization (Snoek et al., ...)".

Bayesian optimization is a method for optimizing hyperparameters in machine learning models. It involves defining a probability distribution over the hyperparameters and then using a search algorithm to find the optimal values. This approach can help to handle randomness in the pipeline by systematically exploring the hyperparameter space and identifying the most promising regions.

Additionally, the use of Stochastic Gradient Descent (SGD) as the optimization method is also mentioned. SGD is a stochastic optimization algorithm that adds randomness to the gradient updates, which can help to escape local minima and improve the robustness of the model.

However, there is no explicit mention of a random seed value being used to handle randomness in the pipeline. It is possible that the authors may have used a fixed random seed value to ensure reproducibility of the results, but this is not explicitly stated in the provided context.
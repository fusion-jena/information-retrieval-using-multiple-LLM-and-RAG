Answer::: 

Based on the provided context, the regularization method used to prevent overfitting in the deep learning pipeline is dropout with a dropout rate of 0.2 in the final Fully Connected (FC) layer. Additionally, the Adam optimizer is used with a weight decay of 0.9, which is a form of L2 regularization.
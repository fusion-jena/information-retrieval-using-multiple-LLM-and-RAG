Answer:::

The provided context does not explicitly discuss strategies for handling randomness in the deep learning pipeline, such as random seed values. The context primarily focuses on Extreme Learning Machines (ELMs) and their properties, advantages, and applications.

However, it can be inferred that ELMs inherently handle randomness through their architecture. ELMs use randomly selected input weights and random values of bias in the hidden layer. This randomness is a key aspect of ELMs, as it allows them to learn from data quickly and accurately.

Additionally, ELMs do not require coordinated functioning of the hidden layer, and all hidden-layer parameters are independent from the activation functions and training data. This suggests that ELMs are designed to be robust to randomness and can handle it internally.

However, the context does not provide specific strategies for handling randomness in the deep learning pipeline, such as setting random seed values or using techniques like dropout or batch normalization. Therefore, I cannot provide a more detailed answer to this query based on the provided context.
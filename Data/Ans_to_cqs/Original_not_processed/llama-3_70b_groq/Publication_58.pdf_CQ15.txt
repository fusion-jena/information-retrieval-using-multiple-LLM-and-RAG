Answer::: 

According to the provided context, the optimization technique used in the deep learning pipeline is stochastic gradient descent (SGD) with a momentum of 0.9 and an initial learning rate of 10^(-2). The learning rate is divided by 10 every 20 epochs until it reaches 10^(-6). There is no mention of Adam or any other optimization techniques being used.
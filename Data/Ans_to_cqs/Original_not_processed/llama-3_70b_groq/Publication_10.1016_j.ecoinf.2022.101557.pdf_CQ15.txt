Answer: According to the provided context, the optimization technique applied in the deep learning pipeline is ADAM optimization with a learning rate of 0.0001. This is mentioned in the text as "ADAM optimisation (Kingma and Ba, 2014) with a learning rate of 0.0001 is used to find optimal weights in the neural network in order to minimise the cross entropy loss between the network's predictions and the pre-labelled data." There is no mention of SGD or any other optimization techniques being used.
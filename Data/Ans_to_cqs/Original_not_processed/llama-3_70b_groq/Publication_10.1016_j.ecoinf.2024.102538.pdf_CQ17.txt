Answer: 

The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of weight decay with a value of 1 Ã— 10^(-5) in the Adam optimizer. Weight decay is a form of L2 regularization, which is a common technique used to prevent overfitting by adding a penalty term to the loss function for large model weights. 

No other regularization methods, such as dropout, are mentioned in the provided context. Therefore, it cannot be determined if dropout or other regularization methods were used in the deep learning pipeline.
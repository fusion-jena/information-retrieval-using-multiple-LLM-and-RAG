Answer:::

Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context focuses on the architecture of the network, the use of inception layers, and the post-processing and bounding box fusion steps, but it does not provide information about the optimization techniques used during training.

Therefore, I don't know the answer to this question based on the provided context.
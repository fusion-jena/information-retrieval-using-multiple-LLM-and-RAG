Answer:::

Based on the provided context, it appears that batch normalization is applied to every pre-activation (before applying the ReLU) to every class of NN model, including CNNs. This is mentioned in the first paragraph of the context. Batch normalization is a preprocessing step that involves centering and reduction of pre-activations, which facilitates optimization and enables a quicker convergence. It also has a regularization effect because the centering and reduction of a neuron activation is linked to the mini-batch statistics.

However, it does not explicitly mention other preprocessing steps such as normalization, scaling, or cleaning. The context only mentions that model parameters were initialized with a uniform distribution U(0.03, 0.03), but it does not provide information on other preprocessing steps.

Therefore, based on the provided context, the only preprocessing step that can be confirmed is batch normalization. Other preprocessing steps may be involved, but they are not mentioned in the context.
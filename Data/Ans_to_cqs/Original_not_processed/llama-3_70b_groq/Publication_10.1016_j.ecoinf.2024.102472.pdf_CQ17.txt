Answer: 

The provided context does not explicitly mention regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. The context primarily focuses on the description of Extreme Gradient Boosting (XGBoost) and Artificial Neural Networks (ANNs), specifically Multilayer Perceptron Neural Networks (MLPNNs). 

However, it does mention hyperparameter tuning for both XGBoost and MLPNNs, which can be related to preventing overfitting. For instance, the context mentions tuning the learning rate, momentum, and training iterations for MLPNNs, which can help prevent overfitting. Similarly, XGBoost's hyperparameters, such as ntree, tree node size, and gradient boosting iteration, can be tuned to prevent overfitting. 

Despite this, the context does not explicitly mention regularization methods like dropout or L2 regularization. Therefore, it cannot be determined from the provided context what regularization methods are used to prevent overfitting in the deep learning pipeline.
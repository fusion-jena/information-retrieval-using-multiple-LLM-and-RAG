Answer: 

The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention that early stopping was employed as a regularization technique to prevent overfitting. This is done by stopping the training process when the network begins to overfit the data, i.e., when the error on the validation set starts to increase. 

It's worth noting that the context focuses on the development of an Artificial Neural Network (ANN) and the importance of selecting the appropriate number of layers, neurons in the hidden layer, learning rates, and number of epochs for model training to prevent overfitting. The trial-and-error method is used to determine the optimal number of neurons and learning rate. 

In summary, the only regularization method mentioned in the context is early stopping, but it does not provide information on other regularization methods such as dropout or L2 regularization.
Answer::: 

The optimization technique applied in the deep learning pipeline is the Adam learning rate optimizer. This is stated in the context as "the Adam learning rate optimizer was chosen over the static stochastic gradient descent (SGD)". This implies that Adam was preferred over SGD for this particular study.
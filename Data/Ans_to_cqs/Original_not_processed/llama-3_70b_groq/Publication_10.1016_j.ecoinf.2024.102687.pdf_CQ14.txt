Answer: The provided context does not explicitly mention how the hyperparameters of the model are optimized. It only mentions that the model was fine-tuned using the Adam optimizer with a default learning rate of 0.0001 and a weight decay of 0. However, it does not provide information on how these hyperparameters were chosen or optimized. It is possible that the authors used a default setting for the optimizer without performing a hyperparameter search, but this is not explicitly stated. Therefore, I don't know the answer to this question based on the provided context.
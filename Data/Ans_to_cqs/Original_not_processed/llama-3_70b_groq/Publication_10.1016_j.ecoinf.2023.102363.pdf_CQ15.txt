Answer: According to the provided context, the optimization technique applied in the deep learning pipeline is the Adam optimizer. This is mentioned in the text as "An Adam optimizer was used to control gradient descent during training (Kingma and Ba, 2014)". There is no mention of other optimization techniques such as SGD being used.
The text specifies the following hyperparameters used in the MLP model:

* **Number of neural network layers:**  1 to 5 layers
* **Number of neurons per layer:** 64, 128, 256, 512, 1024, 2048 (powers of 2)
* **Dropout:** 0.1, 0.25, 0.35, 0.5
* **Learning rate:** 0.001, 0.0001, 0.00001
* **Batch size:** 32, 64, 128, 256 


The text does not mention the specific optimizer used in the model.
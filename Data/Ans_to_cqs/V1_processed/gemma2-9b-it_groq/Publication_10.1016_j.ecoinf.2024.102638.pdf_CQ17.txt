The provided text focuses on various machine learning algorithms and their hyperparameters, but it doesn't delve into specific regularization methods used in deep learning to prevent overfitting. 

However,  common regularization techniques in deep learning include:

* **Dropout:** Randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents neurons from relying too heavily on any single neuron and encourages the network to learn more robust features.

* **L2 Regularization (Weight Decay):** Adds a penalty term to the loss function that is proportional to the square of the magnitude of the weights. This discourages the model from learning weights that are too large, which can lead to overfitting.

* **L1 Regularization (Lasso):**  Similar to L2 regularization but uses the absolute value of the weights. This can lead to some weights being driven to exactly zero, effectively performing feature selection.
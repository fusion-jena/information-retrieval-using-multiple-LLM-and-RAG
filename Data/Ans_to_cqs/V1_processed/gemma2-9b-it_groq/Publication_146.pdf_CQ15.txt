The text states that the Adam learning rate optimizer was chosen over the static stochastic gradient descent (SGD).
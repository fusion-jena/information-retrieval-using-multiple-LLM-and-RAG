The text mentions two regularization methods used to prevent overfitting:

* **Dropout:** Applied to the input of the last two layers with a probability of 0.5. This randomly "drops out" (ignores) a percentage of neurons during each training step, forcing the network to learn more robust features.

* **L2-regularization:** Applied to the weights of the last two layers with a penalty factor of 0.001. This adds a penalty to the loss function based on the magnitude of the weights, encouraging the network to learn smaller weights and preventing them from becoming too large.
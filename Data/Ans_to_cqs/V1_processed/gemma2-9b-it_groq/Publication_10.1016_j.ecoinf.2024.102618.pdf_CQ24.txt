The provided text focuses on a study comparing different ensemble deep learning models for plant disease classification. It doesn't delve into the specific strategies used to handle randomness within the deep learning pipeline. 

However,  randomness is inherent in several aspects of deep learning, including:

* **Initialization:**  Weights and biases in neural networks are often initialized randomly. Different initializations can lead to variations in model training.

* **Stochastic Gradient Descent (SGD):** This common optimization algorithm uses random mini-batches of data to update model parameters. The randomness in batch selection introduces variability.
* **Dropout:** This regularization technique randomly "drops out" (ignores) a percentage of neurons during training. This randomness prevents overfitting.

To mitigate the impact of randomness:

* **Random Seed Setting:**  A random seed value is set at the beginning of the training process. This ensures that the random number generator produces the same sequence of "random" numbers each time the code is run, leading to reproducible results.

* **Data Augmentation:**  Artificially increasing the size and diversity of the training dataset by applying random transformations (e.g., rotations, flips, crops) to existing images can help reduce the impact of randomness in data selection.

* **Ensemble Methods:**  Training multiple models with different random initializations and averaging their predictions can help reduce the variance introduced by randomness. 



Let me know if you have any other questions.
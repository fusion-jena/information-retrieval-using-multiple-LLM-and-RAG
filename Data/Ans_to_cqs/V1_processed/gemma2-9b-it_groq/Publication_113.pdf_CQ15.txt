The provided text mentions Stochastic Gradient Descent (SGD) as an optimization technique used in deep learning.  It states: "The SGD algorithm is an iterative algorithm... At every iteration of the SGD algorithm, the parameters take one step toward minimizing the loss value." 


It also mentions  "modern enhancements of it; Kingma & Ba, 2014; Tieleman & Hinton, 2012" which likely refers to variations or improvements upon SGD, such as Adam, but doesn't explicitly name them.
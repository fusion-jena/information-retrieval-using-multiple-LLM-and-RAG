The provided text mentions using weight decay as a method to prevent overfitting. 

It states: "During training, we used weight decay of 1 × 10−6 for the first 5 epochs, and then increased it to 5 × 10−6 for epochs 6 onwards."

Weight decay, also known as L2 regularization, adds a penalty to the model's weights during training. This penalty discourages the model from learning overly complex patterns that might lead to overfitting. 


The text does not explicitly mention other regularization techniques like dropout.
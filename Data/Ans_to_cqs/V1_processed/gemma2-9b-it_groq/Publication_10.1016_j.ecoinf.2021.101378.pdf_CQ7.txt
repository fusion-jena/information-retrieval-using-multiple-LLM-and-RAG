The provided text focuses on using machine learning algorithms like random forest and Maxent for predicting the occurrence of pine wilt disease. It doesn't delve into the specifics of data splitting criteria for deep learning models. 

To answer your query about data splitting for deep learning, here's a general explanation:

Deep learning models, like other machine learning models, require training data to learn patterns and make predictions.  Data is typically split into three sets:

* **Training set:** The largest portion of the data used to train the deep learning model. The model learns the relationships between input features and target variables from this set.
* **Validation set:** A smaller subset of the data used to evaluate the model's performance during training. This helps monitor how well the model is generalizing to unseen data and prevents overfitting (where the model performs well on training data but poorly on new data).
* **Test set:** A separate, held-out portion of the data used only once at the end of the training process to provide an unbiased estimate of the model's final performance.

The exact proportions of these sets can vary depending on the size of the dataset and the specific problem. A common split is 70% for training, 15% for validation, and 15% for testing.
The model's performance is evaluated using Precision, Recall, F1-score, GIoU, and mAP. 

* **Precision** measures the accuracy of positive predictions (objects detected by the model). 
* **Recall** measures the model's ability to detect all the true objects.
* **F1-score** is the harmonic mean of precision and recall, providing a balanced measure of both.
* **GIoU (Intersection over Union)**  measures the overlap between the predicted bounding box and the ground truth bounding box.
* **mAP (mean Average Precision)**  is a metric that averages the precision-recall curves across different thresholds.
The provided text focuses on the development and performance of a deep learning model called Res_AUNet for water sun glint detection in UAV water images. It doesn't delve into the specific strategies used to handle randomness within the deep learning pipeline. 

However, common strategies to address randomness in deep learning include:

* **Setting a fixed random seed:** This ensures that the initialization of weights, shuffling of data, and other random processes are reproducible. This allows for consistent results and easier debugging.

* **Data augmentation:**  This involves applying random transformations (e.g., rotations, flips, crops) to the training data. This introduces variability and helps the model generalize better to unseen data.

* **Dropout:** This technique randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents overfitting by forcing the network to learn more robust features.

* **Batch normalization:** This technique normalizes the activations of each layer, reducing the internal covariate shift and stabilizing training.


Keep in mind that these are general strategies, and the specific techniques used might vary depending on the model architecture and the dataset.
The provided text states that the training setup includes the Adam optimizer (Kingma and Ba, 2015) with a concatenating cosine-annealing linear scheduler with an initial learning rate of 0.0001, decaying by a factor of 0.001â‹…learning rate.  The batch size is 32.
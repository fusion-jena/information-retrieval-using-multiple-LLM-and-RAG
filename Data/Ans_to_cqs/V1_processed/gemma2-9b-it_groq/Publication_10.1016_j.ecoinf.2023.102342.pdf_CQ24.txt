The provided text focuses on data mining techniques and doesn't offer information about strategies for handling randomness in deep learning pipelines. 

However, I can tell you that randomness is inherent in many aspects of deep learning, including:

* **Initialization:** Weights and biases in neural networks are often initialized randomly. This helps break symmetry and allows the network to learn diverse representations.
* **Activation Functions:** Some activation functions, like dropout, introduce randomness during training to prevent overfitting.
* **Data Shuffling:** Training data is typically shuffled randomly before each epoch to ensure that the model doesn't learn patterns specific to a particular data order.

To mitigate the impact of randomness and ensure reproducibility, deep learning practitioners often use techniques like:

* **Setting a Random Seed:**  Specifying a fixed random seed ensures that the same random numbers are generated across different runs, making the results reproducible.
* **Weight Initialization Strategies:**  Using specific weight initialization techniques, like Xavier or He initialization, can help stabilize training and improve performance.
* **Regularization Techniques:** Techniques like dropout and weight decay help prevent overfitting and improve generalization by introducing controlled randomness during training.
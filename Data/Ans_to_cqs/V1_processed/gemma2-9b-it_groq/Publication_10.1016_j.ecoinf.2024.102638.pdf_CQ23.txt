The text mentions that the dataset was randomly divided into 80% for training and 20% for testing to ensure both subsets are representative of the overall dataset and minimize bias. 

Additionally, a five-fold cross-validation was used during the hyperparameter optimization process for each model. This technique involves splitting the training data into five folds, training the model on four folds, and evaluating it on the remaining fold. This process is repeated five times, using each fold as the test set once. 

While the text doesn't explicitly mention stratified splitting, the emphasis on random sampling and ensuring representative subsets suggests that this technique might have been employed to maintain the distribution of key features and target variables across the training and test sets.
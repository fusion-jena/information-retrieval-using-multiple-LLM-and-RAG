The provided text mentions the use of Stochastic Gradient Descent (SGD) for training the Variational Autoencoder (VAE).  It also states that the  "BP technique" is used for learning the entire network parameters.  

While the text doesn't explicitly name other optimization techniques like Adam, it's common practice in deep learning to use various optimization algorithms.
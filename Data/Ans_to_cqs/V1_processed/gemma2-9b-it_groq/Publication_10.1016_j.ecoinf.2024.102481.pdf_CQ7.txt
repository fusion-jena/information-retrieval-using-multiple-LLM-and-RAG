The provided text focuses on applying machine learning, specifically Random Forests, to predict fish maturity. It doesn't delve into the specifics of data splitting for deep learning models. 

However, I can tell you that data splitting is a crucial step in training any machine learning model, including deep learning models. 

The common practice is to split the data into three sets:

* **Training set:** This is the largest portion of the data (typically 70-80%) used to train the model. The model learns patterns and relationships within this data.
* **Validation set:** This set (usually 10-15%) is used to evaluate the model's performance during training. It helps fine-tune the model's hyperparameters and prevent overfitting (where the model performs well on training data but poorly on unseen data).
* **Test set:** This set (typically 10-15%) is held out entirely from the training process. It's used only once at the end to provide an unbiased evaluation of the final model's performance on unseen data. 


Let me know if you have other questions about machine learning or the provided text.
The text states that the Adam optimizer was used for training Model-2.  Stochastic Gradient Descent (SGD) was used for training Model-K.
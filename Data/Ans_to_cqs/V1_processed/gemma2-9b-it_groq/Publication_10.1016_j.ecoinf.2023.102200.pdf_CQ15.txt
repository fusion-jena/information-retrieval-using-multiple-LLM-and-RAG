The provided text mentions the use of Stochastic Gradient Descent (SGD) with momentum for training the Faster R-CNN detector. 

Specifically, it states: "The network was trained for 20,000 iterations using Stochastic Gradient Descent (with momentum), with a learning rate of 0.003, momentum of 0.9, and a batch size of 1." 


It does not mention the use of any other optimization techniques like Adam.
The provided text focuses on the performance of Convolutional Neural Networks (CNNs) in fish species classification and doesn't delve into strategies for handling randomness in the deep learning pipeline. 

However, I can provide some general information on how randomness is handled in deep learning:

* **Random Initialization:**  Weights and biases in neural networks are often initialized randomly. This prevents all neurons from starting with the same values, promoting diverse learning.

* **Dropout:**  A regularization technique where a percentage of neurons are randomly "dropped out" (ignored) during each training step. This prevents overfitting by forcing the network to learn more robust features.

* **Batch Normalization:**  A technique that normalizes the activations of each layer during training, reducing the internal covariate shift and stabilizing learning.

* **Random Data Augmentation:**  Techniques like random cropping, flipping, and rotation are applied to training images to increase the diversity of the training data and improve generalization.

* **Setting Random Seeds:**  Using a fixed random seed ensures that the same random numbers are generated each time a model is trained, allowing for reproducibility.


Let me know if you have any other questions.
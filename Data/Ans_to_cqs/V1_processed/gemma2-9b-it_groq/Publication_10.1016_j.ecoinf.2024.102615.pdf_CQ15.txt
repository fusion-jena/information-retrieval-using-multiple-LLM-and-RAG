The provided text focuses on hyperparameter optimization using the grid search method. It doesn't mention specific optimization techniques used during the training process, such as SGD (Stochastic Gradient Descent) or Adam.
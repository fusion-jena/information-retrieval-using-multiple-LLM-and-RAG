The text mentions the following metrics for evaluating the deep learning model:

* **AUC (Area Under the Curve):** This metric is often used with ROC curves to assess the model's ability to distinguish between positive and negative cases. 
* **mAP (Mean Average Precision):** This metric is commonly used in object detection tasks to evaluate the model's precision and recall at different thresholds.
* **F1-score:** This metric combines precision and recall into a single score, providing a balanced measure of the model's performance.

The text also refers to **precision** and **recall**, which are individual components of the F1-score.
The provided text states that they used an SGD scheduler with an exponential decay parameter of 0.5 to reduce the learning rate after 50,000 iterations.  It does not mention if other optimization techniques like Adam were used.
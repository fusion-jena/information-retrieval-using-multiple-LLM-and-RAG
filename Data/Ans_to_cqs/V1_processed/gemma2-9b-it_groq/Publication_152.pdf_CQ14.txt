The text states that the Stochastic gradient descent (SGD) optimizer and Adam [27] were used with learning rates from 10^-2 until 10^-8.  It doesn't, however, specify if grid search or random search were used to optimize these hyperparameters.
The provided text focuses on the application of deep learning in ecological image classification and doesn't delve into specific strategies for handling randomness within the deep learning pipeline. 

However, it does mention the use of the Adam optimizer (Kingma and Ba, 2014), which is a stochastic optimization algorithm that inherently incorporates randomness in its parameter updates. 

Generally, in deep learning, randomness is managed through techniques like:

* **Setting a random seed:** This initializes random number generators, ensuring reproducibility of results. The text doesn't specify if a random seed is used in the described workflow.
* **Dropout:** A regularization technique that randomly "drops out" (ignores) a percentage of neurons during training, preventing overfitting and introducing stochasticity. The text doesn't mention the use of dropout.
* **Batch normalization:**  A technique that normalizes the activations of neurons within each mini-batch, reducing internal covariate shift and improving training stability. The text doesn't explicitly state if batch normalization is used.



Let me know if you have any other questions.
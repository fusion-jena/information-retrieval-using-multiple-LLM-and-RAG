The provided text mentions dropout as a regularization method used in the classification network. 

Specifically, it states: "The fully connected layers of the networks were replaced with a global max-pooling layer, a dropout layer with a dropout ratio of 0.5 as a network regularizer and single hidden layer with a total of 256 units for a larger image sized model(512 × 512) and 128 units for a smaller image size model (256 × 256)." 


The text does not mention L2 regularization.
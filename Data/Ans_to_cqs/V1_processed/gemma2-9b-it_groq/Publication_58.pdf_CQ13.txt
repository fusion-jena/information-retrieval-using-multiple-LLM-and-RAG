The provided text states that all architectures were trained with stochastic gradient descent (SGD) using a momentum of 0.9.  The initial learning rate was 10^-2, and it was divided by 10 every 20 epochs until reaching 10^-6.
The provided text describes a process for selecting the optimal number of iterations for an XGBoost model, but it doesn't explicitly detail how model performance is monitored *during* training. 

Here's what we can infer:

* **Out-of-sample R2:** The primary performance metric used is out-of-sample R2, calculated by splitting the data into k-folds and evaluating the model on a held-out validation fold. This suggests that performance is assessed periodically, likely at the end of each training epoch or iteration.
* **10-fold cross-validation for pruning:** The text mentions using 10-fold cross-validation to prune the tree during model growth. This implies that performance is monitored during the tree construction process to determine the optimal tree complexity.

However, the text doesn't provide specifics on how performance is tracked *within* each training iteration. It's possible that techniques like early stopping or validation loss monitoring are used, but this information isn't explicitly stated.
The provided text describes the evaluation metrics used for object detection and segmentation tasks, specifically mentioning:

* **Average Precision (AP):**  A measure of the model's ability to correctly identify objects at different confidence levels. It's calculated by averaging the precision-recall curves over various Intersection over Union (IoU) thresholds.

* **Mean Average Precision (mAP):** The average AP across all classes.

* **Average Recall (AR):** The average recall over different IoU thresholds.

* **Mean Average Recall (mAR):** The average AR across all classes. 


Let me know if you'd like more details on any of these metrics.
The provided text focuses on AutoML, Random Forest, and XGBoost algorithms, and doesn't delve into strategies for handling randomness in deep learning pipelines. 

However, I can offer some general information about handling randomness in deep learning:

* **Random Seed:**  Setting a random seed is a fundamental strategy. This ensures that the random number generator starts from the same point each time you run your code, making your results reproducible.

* **Data Shuffling:**  Randomly shuffling your training data before each epoch (iteration over the entire dataset) helps prevent the model from learning patterns that are specific to the order of data points.

* **Dropout:** This regularization technique randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features.

* **Batch Normalization:** This technique normalizes the activations of neurons within each mini-batch (a smaller subset of the training data). This can help stabilize training and reduce the impact of random fluctuations in the data.


Let me know if you have other questions about machine learning concepts!
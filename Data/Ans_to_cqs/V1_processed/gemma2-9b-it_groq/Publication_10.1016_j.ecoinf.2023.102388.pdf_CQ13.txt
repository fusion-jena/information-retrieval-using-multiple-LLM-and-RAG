The provided text describes two different training settings for CNNs: one aimed at preventing overfitting and another intentionally inducing overfitting. 

For the **no-overfitting** setting, the hyperparameters are:

* **Optimizer:** Stochastic Gradient Descent (SGD)
* **Learning rate:** 0.0001
* **Weight decay:** 0.5
* **Batch size:** 32
* **Training epochs:** 200
* **Dropout:** 0.5 (used to reduce overfitting)
* **Batch normalization:** Used to reduce overfitting

The **overfitting** setting uses the same hyperparameters as the no-overfitting setting *except* for the removal of batch normalization, weight decay, and dropout.

The text also mentions a Siamese neural network trained with:

* **Optimizer:** Stochastic Gradient Descent (SGD) with momentum of 0.9
* **Learning rate:** 0.001
* **Batch size:** 32
* **Weight decay:** 0.5 applied every 512 iterations for a total of 2048 iterations.
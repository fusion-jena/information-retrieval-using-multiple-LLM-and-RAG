The provided text states that each model underwent training for 10 epochs, utilizing a batch size of 32.  It does not specify the criteria used to determine when training is complete.
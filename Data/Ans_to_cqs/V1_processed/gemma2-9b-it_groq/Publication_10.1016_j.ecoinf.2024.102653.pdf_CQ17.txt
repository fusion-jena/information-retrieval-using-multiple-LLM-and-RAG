The provided text mentions one regularization method used to prevent overfitting: dropout. 

Dropout randomly deactivates 50% of the neural connections during training. This prevents the model from relying too heavily on any single connection and encourages the learning of more robust features. 


The text does not mention L2 regularization or any other regularization methods.
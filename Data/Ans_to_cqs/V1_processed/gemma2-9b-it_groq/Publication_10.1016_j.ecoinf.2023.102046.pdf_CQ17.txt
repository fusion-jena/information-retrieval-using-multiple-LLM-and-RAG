The provided text mentions L2 regularization as a method used in the training of a Support Vector Classifier (SVC) and Ridge Regression.  

L2 regularization adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This discourages the model from learning weights that are too large, which can lead to overfitting.

The text also mentions dropout as a technique used in the Deep Otolith Inception-V3 model. Dropout randomly "drops out" (sets to zero) a percentage of the neurons in a layer during each training step. This prevents the model from relying too heavily on any single neuron and helps to improve generalization.
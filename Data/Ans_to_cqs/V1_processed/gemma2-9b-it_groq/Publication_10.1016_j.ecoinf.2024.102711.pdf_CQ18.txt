The provided text describes a process of hyperparameter tuning using k-fold cross-validation and mentions the use of an independent dataset for final model evaluation. However, it doesn't explicitly detail the strategy for monitoring model performance *during* the training process. 

We can infer some potential strategies based on common practices:

* **Loss Function Tracking:**  The text mentions the use of a loss function (l) in the context of XGBoost optimization.  It's likely that the model's performance was monitored by tracking the value of this loss function over training iterations. A decreasing loss generally indicates improved performance.
* **Validation Set Performance:** While the primary focus is on tuning hyperparameters, the text states that the development dataset is split into training and tuning sets.  The model's performance could have been evaluated on the tuning set after each training epoch or a set number of iterations to gauge how well it was generalizing.


Without more specific information from the text, we can't definitively say what strategies were used to monitor model performance during training.
The provided text does not specify the exact hyperparameters used for the deep learning models.  It mentions using pre-trained models like AlexNet, VGG-19, ResNet-101, and DenseNet-201, but it doesn't delve into the specific hyperparameters fine-tuned for these models in this particular context. 

The text does refer to Adam as an optimization algorithm (Kingma and Ba, 2014), which is a common choice for training deep learning models. However, it doesn't state whether Adam was used or provide details about its learning rate, weight decay, or other parameters.
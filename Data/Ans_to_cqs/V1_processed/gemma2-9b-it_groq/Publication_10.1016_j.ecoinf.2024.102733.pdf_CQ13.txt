The provided text describes the hyperparameters used in training a diffusion model and a multilayer perceptron (MLP). 

For the diffusion model, the text mentions exploring various combinations of:

* **Pmean:**  ranging from 0 to 4
* **Pstd:** ranging from 0.5 to 1.68
* **Learning rate:** ranging from 0.00001 to 0.001
* **Batch size:** ranging from 64 to 8192
* **Weight decay:** ranging from 0 to 0.59

For the MLP, the text lists the following hyperparameters:

* **Learning rate:** 0.00001 and 0.0001
* **Batch size:** 8, 16, 32, and 64
* **Weight decay:** between 0.0001 and 0.001
* **Dropout:** 0.1
* **Focal Loss parameters:** α (balancing factor, set as the inverse of class frequency, 0.25, or 0.5) and γ (regulates the impact of the scaling factor) with values ranging from 0.25 to 2.

The text also states that the Adam optimizer (Kingma and Ba, 2015) and a learning rate scheduler were used for training the MLP.
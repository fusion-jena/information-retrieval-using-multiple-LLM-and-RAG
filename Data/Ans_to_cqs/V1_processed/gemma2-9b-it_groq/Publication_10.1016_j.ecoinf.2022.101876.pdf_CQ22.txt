The provided text lists several performance metrics used to evaluate the deep learning model. These include:

* **TPR (True Positive Rate)** also known as Recall:  Calculated as TP/(TP + FN), it measures the proportion of actual positive cases that are correctly identified. 
* **FDR (False Discovery Rate):**  Calculated as FP/(FP + TP), it measures the proportion of identified positive cases that are actually false positives.
* **FOR (False Omission Rate):** Calculated as FN/(FN + TN), it measures the proportion of actual positive cases that are missed.
* **ACC (Accuracy):** Calculated as (TP + TN)/(TP + TN + FP + FN), it represents the overall correctness of the model.
* **ERR (Error Rate):** Calculated as (FP + FN)/(TP + TN + FP + FN), it represents the proportion of incorrect predictions.
* **F1 Score:** The harmonic mean of precision and recall, providing a balanced measure of the model's performance.
* **MCC (Matthews Correlation Coefficient):**  A measure of the quality of binary classification that takes into account true and false positives and negatives.
The provided text describes the use of **dropout** as a regularization method to prevent overfitting. 

Dropout randomly "ignores" a percentage of nodes in a layer during each training iteration. This forces the network to learn more robust features that are not reliant on any single node.  

The text states: "The following drop-out layer was assigned a drop-out rate of 0.5, such that each node was ignored with a 50% probability."


The text does not mention other regularization methods like L2 regularization.
The provided text focuses on the training and testing process of various algorithms, including a deep learning algorithm (AN), but it doesn't delve into specific strategies for handling randomness within the deep learning pipeline. 

The text does mention:

* **Random subset selection:** 80% of the dataset (5256 pictures) was randomly selected for training.
* **10 replicate runs:** The training and testing process was repeated 10 times for each algorithm, using the same random subsets each time. This helps assess performance consistency.
* **Random effect:** A "replicate" random effect was included in the models to account for the variability introduced by the repeated runs.

While these strategies address randomness in data splitting and experimental design, they don't directly address potential sources of randomness within the deep learning algorithm itself, such as:

* **Initialization of weights:** Deep learning models often start with randomly initialized weights.
* **Stochastic gradient descent:** This optimization algorithm uses random mini-batches of data, introducing randomness in weight updates.
* **Dropout:** A regularization technique that randomly drops out neurons during training, adding stochasticity.

To fully understand how randomness is handled in the deep learning pipeline described, you would need additional information about the specific implementation of the AN algorithm.
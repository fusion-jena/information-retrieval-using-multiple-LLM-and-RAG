The text describes several measures taken to improve the generalizability of the deep learning model:

* **Data Augmentation:** The authors used data augmentation on the training set to artificially increase the number of training samples by applying slight transformations to the original data. This helps the model learn more robust features and generalize better to unseen data.
* **Splitting into Training, Validation, and Test Sets:** The dataset was split into three sets: 80% for training, 10% for validation, and 10% for testing. This separation ensures that the model is evaluated on data it hasn't seen during training, providing a more reliable estimate of its generalizability.
* **Hyperparameter Tuning using Validation Set:** The validation set was used during training to fine-tune hyperparameters and select the best model setup. This process helps prevent overfitting to the training data and improves the model's ability to perform well on unseen data.
* **Separate Test Set:** The final model's performance was assessed on the separate test set, which was not used during training or hyperparameter tuning. This provides an unbiased evaluation of the model's generalizability to new data.

While the text doesn't explicitly mention techniques like cross-validation or stratified splitting, the described approach of splitting the data and using a validation set for hyperparameter tuning suggests a focus on building a model that generalizes well to unseen data.
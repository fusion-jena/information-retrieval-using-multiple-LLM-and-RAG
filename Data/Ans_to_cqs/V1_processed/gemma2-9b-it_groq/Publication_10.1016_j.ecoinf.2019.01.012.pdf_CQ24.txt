The provided text focuses on the application of a Convolutional Neural Network (CNN) for classifying woody vegetation in repeat photographs. It doesn't delve into specific strategies for handling randomness within the deep learning pipeline. 

However, the text does mention the use of dropout, a technique commonly employed to mitigate overfitting in deep learning models. Dropout randomly "drops out" (ignores) a percentage of neurons during each training step. This introduces randomness and prevents the model from relying too heavily on any single neuron, leading to a more robust and generalizable model.

The text also states that dropout was applied "on the training set only" and that a "sigmoid function" was added to the final layer for binary classification.  

To get a comprehensive answer about strategies for handling randomness in the deep learning pipeline, you'd need to consult resources specifically addressing that topic.
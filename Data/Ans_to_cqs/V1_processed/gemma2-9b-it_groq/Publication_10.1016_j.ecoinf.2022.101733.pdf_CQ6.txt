The provided text focuses on sample collection and selection strategies for a specific application, likely related to remote sensing or image classification. It doesn't delve into the preprocessing steps involved in training deep learning models. 

To answer your query about preprocessing steps for deep learning models, here are some common techniques:

* **Normalization:** Scaling features to a specific range (e.g., 0 to 1 or -1 to 1) can improve model performance.

* **Standardization:**  Transforming data so that it has a mean of 0 and a standard deviation of 1. This is useful for algorithms sensitive to feature scales.
* **Data Cleaning:** Handling missing values, removing duplicates, and correcting inconsistencies in the data.
* **Feature Engineering:** Creating new features from existing ones to potentially improve model accuracy.
* **One-Hot Encoding:** Converting categorical variables into numerical representations that deep learning models can understand.


Remember that the specific preprocessing steps will depend on the nature of your data and the deep learning model you are using.
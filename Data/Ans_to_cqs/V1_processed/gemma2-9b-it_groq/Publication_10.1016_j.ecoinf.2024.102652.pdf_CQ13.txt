The provided text states that the optimizer used is Adam. 

The learning rate is described as varying with group based on the cosine function's law, often in the range between 0.0 and 1.0. 

The text also mentions a batch size of 32 and 100 epochs with an early stopping function.
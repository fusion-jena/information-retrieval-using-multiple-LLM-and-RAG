The provided text focuses on land cover classification using deep learning, particularly in the context of North Korea. It doesn't delve into specific strategies for handling randomness within the deep learning pipeline. 

However, it's common practice in deep learning to address randomness through techniques like:

* **Setting a random seed:** This initializes the random number generator, ensuring reproducibility of results. Different runs with the same seed will produce identical outputs.

* **Data augmentation:**  Artificially increasing the size and diversity of the training dataset by applying random transformations (e.g., rotations, flips, crops) to existing data. This helps the model generalize better and reduces the impact of random fluctuations in the training data.

* **Dropout:** A regularization technique that randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents overfitting and introduces stochasticity into the model.

* **Batch normalization:**  A technique that normalizes the activations of neurons within each mini-batch during training. This helps stabilize the training process and reduce the impact of random initialization.
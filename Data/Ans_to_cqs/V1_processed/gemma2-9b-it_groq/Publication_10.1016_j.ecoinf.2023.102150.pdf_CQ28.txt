The provided text states that the models were trained on a cloud service with 1 GPU, 4 CPUs and 16 GB RAM provided by Sigma2 - the National Infrastructure for High Performance Computing and Data Storage in Norway. 

It does not specify if the trained model was deployed on the same platform or any other platform.
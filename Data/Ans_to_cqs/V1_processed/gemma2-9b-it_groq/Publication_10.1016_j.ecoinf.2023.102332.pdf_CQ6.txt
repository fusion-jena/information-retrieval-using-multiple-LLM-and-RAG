The provided text focuses on using pre-trained word embeddings and doesn't delve into the preprocessing steps for training deep learning models in general. 

However, common preprocessing steps before training a deep learning model typically include:

* **Data Cleaning:** This involves removing irrelevant information like HTML tags, special characters, URLs, and duplicate entries.

* **Text Normalization:**  Converting text to a standard format. This can include:
    * **Lowercasing:** Converting all text to lowercase.
    * **Stemming/Lemmatization:** Reducing words to their root form (e.g., "running" to "run").

* **Tokenization:** Breaking down text into individual words or units (tokens).

* **Encoding:** Representing categorical data (like words) as numerical values that the model can understand. Common methods include one-hot encoding and embedding techniques like Word2Vec.

* **Scaling/Normalization:**  Adjusting the range of numerical features to prevent certain features from dominating the learning process. This can involve techniques like standardization (z-score normalization) or min-max scaling. 


Let me know if you have any other questions.
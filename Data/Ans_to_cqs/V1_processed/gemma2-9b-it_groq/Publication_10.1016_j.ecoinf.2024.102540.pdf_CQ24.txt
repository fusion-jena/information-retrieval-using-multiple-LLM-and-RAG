The provided text focuses on the application of deep learning for detecting pests on sorghum leaves and doesn't delve into specific strategies for handling randomness within the deep learning pipeline. 

However, it does mention using different versions of the YOLOv5 model family and training them with varying input resolutions (640x640 and 1280x1280 pixels). This suggests an attempt to mitigate some randomness by exploring different model architectures and input parameters.

Generally, in deep learning, randomness is addressed through techniques like:

* **Setting a random seed:** This ensures reproducibility by initializing random number generators with a specific value, leading to the same random choices during each run.
* **Data augmentation:**  Randomly applying transformations (e.g., rotations, flips, crops) to training data introduces variations and helps the model generalize better to unseen data.
* **Dropout:** Randomly "dropping out" (ignoring) a percentage of neurons during training prevents overfitting by forcing the network to learn more robust features.

To get a comprehensive answer about randomness handling strategies in deep learning, you'd need to consult resources dedicated to deep learning methodologies and best practices.
The provided text states that the models were trained for 50 epochs using the Adam optimizer (Kingma and Ba, 2014), with a learning rate of 0.001 and a batch size that is not specified.  

The text also mentions dropout (rate of 0.4) after each convolutional layer and the fully connected layer.
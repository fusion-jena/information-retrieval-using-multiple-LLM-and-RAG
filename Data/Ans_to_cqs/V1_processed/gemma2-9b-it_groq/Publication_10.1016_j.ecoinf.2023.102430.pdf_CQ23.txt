The provided text describes the training process for several machine learning models, including a convolutional neural network (AlexNet), but it doesn't explicitly mention specific techniques used to ensure the generalizability of the deep learning model. 

Here's what we can infer:

* **Dataset Split:** The dataset was split into training (80%) and testing (20%) sets. This is a common practice to evaluate how well a model generalizes to unseen data.
* **Repeated Training:** The training-testing process was repeated 10 times for each algorithm, using the same training and testing picture subsets. This type of repeated cross-validation can help assess the model's consistency and robustness.

However, the text doesn't mention:

* **Data Diversity:**  Whether the dataset was diverse enough to represent the range of possible inputs the model might encounter in the real world.
* **Stratified Splitting:** Whether the dataset was split proportionally across different classes (e.g., different bug species) to ensure balanced representation in both training and testing sets.


Therefore, while some steps were taken to evaluate model performance, the text doesn't provide enough information to definitively say what measures were taken to ensure the generalizability of the deep learning model.
The provided text focuses on optimizing deep learning models for efficiency, particularly concerning memory access and computational speed. It doesn't delve into specific regularization methods used to prevent overfitting. 

However,  common regularization techniques to combat overfitting include:

* **Dropout:** Randomly "drops out" (ignores) a percentage of neurons during training. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features.
* **L2 Regularization (Weight Decay):** Adds a penalty to the loss function based on the magnitude of the model's weights. This discourages the model from learning weights that are too large, which can lead to overfitting. 


Let me know if you have other questions about deep learning model optimization.
The provided text describes the AutoML process within the mcfly framework, focusing on data balancing and model selection. It doesn't delve into specific strategies for handling randomness within the deep learning pipeline itself. 

However, it does mention that candidate models are generated with architectures and hyperparameters selected "at random from a prespecified range of values." This implies the use of random number generation for initial model configuration. 

To address randomness in deep learning, common strategies include:

* **Setting a random seed:** This initializes the random number generator, ensuring reproducibility of results. Different runs with the same seed will produce identical model outputs.
* **Random data shuffling:**  Training data is shuffled randomly before each epoch to prevent the model from learning patterns based on the data order.
* **Dropout:** A regularization technique that randomly "drops out" (ignores) a percentage of neurons during training, preventing overfitting and introducing stochasticity.

The text doesn't explicitly state whether these or other randomness-handling techniques are employed by mcfly.
The provided text focuses on the methodology for skyline detection and analysis using deep learning, particularly semantic segmentation and Conditional Random Fields (CRF). It doesn't offer information about the criteria used to split data for training, testing, and validation in deep learning models. 


To answer your query generally, data splitting in deep learning is typically done based on:

* **Percentage Split:** A common approach is to divide the data into three sets:
    * **Training set:**  The largest portion (e.g., 70-80%) used to train the model.
    * **Validation set:** A smaller portion (e.g., 10-15%) used to evaluate the model's performance during training and tune hyperparameters.
    * **Test set:**  A separate, unseen portion (e.g., 10-15%) used only once at the end to provide an unbiased estimate of the model's final performance.

* **Stratification:**  If the dataset has imbalanced classes (some classes have significantly more examples than others), stratification ensures that each split maintains the same class distribution as the original dataset. This helps prevent the model from being biased towards the majority class. 



Let me know if you have other questions about deep learning or semantic segmentation!
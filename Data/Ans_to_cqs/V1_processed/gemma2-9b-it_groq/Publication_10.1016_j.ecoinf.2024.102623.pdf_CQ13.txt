The provided text describes the architecture and training process of a deep learning model but doesn't explicitly list all hyperparameters used. 

However, we can glean some information:

* **Learning Rate:** The text states that both the weight decay and learning rate are set to 0.0001.
* **Optimizer:** The model is trained using the AdamW optimizer.
* **Batch Size:** The batch size is set to 256.
* **Number of Epochs:** The training process runs for 30 epochs.
* **Weight Decay:**  The weight decay is set to 0.0001.
* **Learning Rate Scheduler:** An exponential decay scheduler with a decay rate of 0.95 is employed.
* **Dropout:** Dropout is used during training, although the specific dropout rate is not mentioned.

The text also mentions hyperparameters related to the loss function, specifically  Î»2, but doesn't provide specific values used in the experiments.
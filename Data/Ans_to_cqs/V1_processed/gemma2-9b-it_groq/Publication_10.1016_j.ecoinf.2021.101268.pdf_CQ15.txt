The provided text mentions Adam as an optimization technique used in deep learning. 

Specifically, the paper "Adam: A Method for Stochastic Optimization" by Kingma and Ba (2014) introduces the Adam algorithm. 

The text doesn't explicitly list other optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent).
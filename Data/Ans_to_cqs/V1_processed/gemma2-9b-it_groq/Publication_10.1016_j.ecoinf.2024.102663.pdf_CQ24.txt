The provided text focuses on fish biomass estimation using deep learning and doesn't delve into specific strategies for handling randomness within the deep learning pipeline. 

However, it does mention the use of "bootstrap samples" in the context of Random Forest Regressor. Bootstrap sampling is a technique that involves randomly drawing samples with replacement from the original dataset. This introduces randomness in the training process, which can help improve the model's generalization ability.

Generally, to address randomness in deep learning, practitioners employ techniques like:

* **Setting a random seed:** This ensures that the random number generator starts from the same point each time, making the results reproducible.
* **Data augmentation:** This involves creating variations of existing data points (e.g., rotations, flips, crops) to increase the size and diversity of the training set, mitigating the impact of randomness in the initial data split.
* **Dropout:** This technique randomly "drops out" (ignores) a percentage of neurons during each training step, preventing overfitting and introducing noise that encourages the network to learn more robust features.
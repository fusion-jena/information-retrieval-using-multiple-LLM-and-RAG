The text states that a dropout probability of 0.3 was used after the second to last hidden layer. It also says that the Adam optimizer turned out to be superior to SGD for training the models.  

The specific values for other hyperparameters, such as the learning rate, are not mentioned in the provided text.
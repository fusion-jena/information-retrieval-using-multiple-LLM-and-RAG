The provided text focuses on specific deep learning models (ERNIE, HRNet) and their applications in sentiment analysis and semantic segmentation. It doesn't delve into the general preprocessing steps involved in training deep learning models. 

However, common preprocessing steps for deep learning models generally include:

* **Text Cleaning:** Removing irrelevant characters, punctuation, stop words, and converting text to lowercase.
* **Tokenization:** Breaking down text into individual words or subwords.
* **Normalization:** Converting words to their base form (e.g., "running" to "run").
* **Scaling:**  Adjusting numerical features to a similar scale to prevent features with larger magnitudes from dominating the learning process. This can involve techniques like standardization (subtracting the mean and dividing by the standard deviation) or min-max scaling (scaling values to a specific range).
* **Encoding:** Representing categorical data as numerical vectors using techniques like one-hot encoding or word embeddings.

The specific preprocessing steps will vary depending on the nature of the data and the chosen deep learning model.
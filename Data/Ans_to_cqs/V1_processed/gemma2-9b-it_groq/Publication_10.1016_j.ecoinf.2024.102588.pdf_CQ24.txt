The provided text mentions the use of a random seed (random state at 0) when training a random forest regression model. This suggests that setting a random seed is one strategy employed to handle randomness in the deep learning pipeline. 

By setting a random seed, the researchers ensure that the results are reproducible.  Different random number generators might produce different results even with the same input data, leading to variations in model performance. 

The text doesn't explicitly discuss other strategies for handling randomness in deep learning, such as:

* **Dropout:** A regularization technique that randomly "drops out" (ignores) a percentage of neurons during training. This helps prevent overfitting and introduces some randomness into the network.
* **Batch Normalization:** A technique that normalizes the activations of each layer, which can help stabilize training and reduce the impact of random initialization.
* **Data Augmentation:**  Artificially increasing the size and diversity of the training data by applying random transformations (e.g., rotations, flips, crops) to existing data points.
The text mentions two regularization methods:

* **L2 regularization:** This is implemented through the weight decay term in the loss function. It penalizes large weights, effectively discouraging the model from relying too heavily on any single feature. 

* **Dropout:** While not explicitly described in this excerpt, the text cites Gal and Ghahramani (2016) who discuss dropout as a Bayesian approximation for representing model uncertainty and preventing overfitting.  Dropout randomly "drops out" (ignores) a percentage of neurons during each training step, forcing the network to learn more robust features that are not dependent on any single neuron.
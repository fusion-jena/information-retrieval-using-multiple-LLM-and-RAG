The provided text describes a hyperparameter optimization process but doesn't explicitly detail the strategy used to monitor model performance *during* training. 

It does mention:

* **Performance metrics:**  Validation accuracy, precision, recall, and F1-score are used to evaluate model performance.
* **Training and evaluation:** Models are trained on dataset D and their performance is evaluated on the same dataset.
* **Hyperparameter search:**  Different hyperparameter configurations are tested and the best performing ones are selected.

However, it doesn't explain how these metrics are tracked *while* the model is learning.  Techniques like loss function monitoring, validation set performance checks, and early stopping are common strategies for performance monitoring during training, but the text doesn't confirm their use.
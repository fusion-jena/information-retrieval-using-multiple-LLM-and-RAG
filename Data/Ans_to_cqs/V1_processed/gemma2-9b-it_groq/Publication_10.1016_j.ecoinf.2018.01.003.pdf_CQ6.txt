The provided text focuses on the history and development of neural networks, particularly deep learning. It doesn't delve into the specific preprocessing steps involved in training deep learning models. 

However, common preprocessing steps for deep learning models generally include:

* **Data Cleaning:** Handling missing values, removing duplicates, and correcting inconsistencies.
* **Normalization:** Scaling features to a common range (e.g., 0 to 1 or -1 to 1) to prevent features with larger scales from dominating the learning process.
* **Feature Scaling:**  Similar to normalization, but often involves techniques like standardization (subtracting the mean and dividing by the standard deviation) to ensure features have zero mean and unit variance.
* **Encoding Categorical Data:** Converting categorical variables into numerical representations that the model can understand (e.g., one-hot encoding).
* **Data Augmentation:**  Artificially increasing the size and diversity of the training data by applying transformations like rotations, flips, or crops to existing data points. 


Let me know if you have other questions about neural networks or deep learning!
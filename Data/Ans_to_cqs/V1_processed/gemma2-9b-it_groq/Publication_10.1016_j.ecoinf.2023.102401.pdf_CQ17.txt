The provided text focuses on feature extraction techniques in deep learning models, particularly in the context of object detection. It doesn't discuss regularization methods used to prevent overfitting. 

However, some common regularization methods used in deep learning to combat overfitting include:

* **Dropout:** Randomly "drops out" (ignores) a percentage of neurons during training. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features.

* **L2 Regularization (Weight Decay):** Adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This discourages the weights from becoming too large, which can lead to overfitting.

* **Batch Normalization:** Normalizes the activations of each layer during training, which can help stabilize the training process and reduce the risk of overfitting.
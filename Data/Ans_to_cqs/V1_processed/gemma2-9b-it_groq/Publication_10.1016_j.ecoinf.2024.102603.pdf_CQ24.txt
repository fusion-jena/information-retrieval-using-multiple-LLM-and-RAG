The provided text focuses on comparing XGBoost and deep learning models, highlighting their differences in training time, interpretability, and performance on small datasets. It doesn't delve into specific strategies for handling randomness within the deep learning pipeline. 

To answer your query about strategies for handling randomness in deep learning, here are some common techniques:

* **Setting a Random Seed:**  This is a fundamental practice. By initializing the random number generator with a specific seed value, you ensure that the same "random" numbers are generated every time you run your code. This reproducibility is crucial for experimentation and comparison.

* **Data Shuffling:**  Before training, data is often shuffled randomly. This prevents the model from learning patterns that might arise from the original data order, leading to more robust generalization.

* **Dropout:** This regularization technique randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features.

* **Batch Normalization:** This technique normalizes the activations within each layer of the network, helping to stabilize training and reduce the impact of random initialization.


Let me know if you have any other questions.
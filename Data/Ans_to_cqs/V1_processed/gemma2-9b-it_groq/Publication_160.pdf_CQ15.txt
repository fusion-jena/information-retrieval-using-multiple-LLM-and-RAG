The text mentions the use of two optimization techniques:

* **Adam:** This optimizer is used to train the ResNet18 CNNs from scratch. 
* **Stochastic Gradient Descent (SGD):** This optimizer is used to train the top layer of the ResNet152 initially, and then the whole network.
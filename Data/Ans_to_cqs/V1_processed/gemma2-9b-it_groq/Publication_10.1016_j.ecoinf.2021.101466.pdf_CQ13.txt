The provided text states that the EfficientNet was trained using the Adagrad optimizer.  It also mentions a dropout rate of 0.5 and a batch size of 20.  

The text does not specify the learning rate used for training.
The provided text describes the training process for both gradient boosting and artificial neural networks (ANNs) but doesn't explicitly state the strategy used to monitor model performance during training. 

For the gradient boosting model, it mentions that all hyperparameters were tuned using a grid search method. Grid search involves systematically evaluating a model for all specified combinations of hyperparameter values and selecting the combination that yields the best performance based on a predefined metric (likely accuracy or some other measure of error).

For the ANN, the text describes the backpropagation algorithm used to adjust connection weights iteratively to minimize error. It states that the network's performance was evaluated by comparing its output values to expected values and calculating the error.  The training process continued until a target minimum error was reached.

However, it doesn't specify if any other techniques were used to monitor performance during training, such as:

* **Validation set:**  Splitting the data into training and validation sets allows for evaluating the model's performance on unseen data during training.
* **Early stopping:**  Stopping training when performance on the validation set starts to decrease can prevent overfitting.
* **Loss function monitoring:**  Tracking the loss function (a measure of the difference between predicted and actual values) over training iterations can provide insights into the learning process. 



Therefore, while the text provides information about the training process and performance evaluation, it doesn't explicitly detail the specific strategies used to monitor model performance during training.
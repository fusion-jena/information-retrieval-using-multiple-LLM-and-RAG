The provided text mentions the Adam optimizer being used in the deep learning pipeline.  It states: "We used the Adam optimizer with a learning rate of 0.0005 and trained the geographic prior on 10 epochs with a batch size of 32 â€“ obtained via preliminary experimentation." 


It does not, however, list any other optimization techniques that may be applied.
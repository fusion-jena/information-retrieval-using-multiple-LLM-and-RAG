The provided text focuses on deep learning applications in image analysis, particularly in the fields of karyotyping and neuron tracing. It doesn't delve into the strategies used to handle randomness within the deep learning pipeline. 

To address your query about handling randomness in deep learning, here are some common strategies:

* **Setting a Random Seed:**  A random seed is an initial value used by random number generators. By setting a fixed seed, you ensure that the same "random" numbers are generated each time you run your code, making your results reproducible.

* **Data Augmentation:** This technique introduces controlled variations into your training data (e.g., rotations, flips, crops) to increase its size and diversity. This helps the model generalize better and reduces the impact of randomness in the data itself.

* **Dropout:** During training, dropout randomly "drops out" (ignores) a percentage of neurons in each layer. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features.

* **Batch Normalization:** This technique normalizes the activations of neurons within each mini-batch during training. This helps stabilize the training process and reduce the sensitivity to the initial weights.


Let me know if you have any other questions.
The provided text focuses on data decomposition techniques using Empirical Wavelet Transform (EWT) and doesn't offer information about data splitting criteria for deep learning. 

To answer your query, here are common criteria used to split data for deep learning model training:

* **Training Set:** This is the largest portion of the data (typically 70-80%) used to train the deep learning model. The model learns patterns and relationships within this data.

* **Validation Set:** A smaller portion of the data (around 10-15%) used to evaluate the model's performance during training. It helps monitor how well the model generalizes to unseen data and prevents overfitting (where the model performs well on training data but poorly on new data).

* **Test Set:**  A separate, held-out portion of the data (around 10-15%) used *only* after training is complete to provide an unbiased evaluation of the final model's performance on unseen data.

**Key Considerations:**

* **Data Size:** The split proportions can be adjusted based on the size of the dataset. Smaller datasets might require a smaller validation set.
* **Data Distribution:**  Ensure the splits represent the overall data distribution to avoid biased evaluations.
* **Randomization:**  Split the data randomly to prevent any systematic bias in the splits.



Let me know if you have any other questions.
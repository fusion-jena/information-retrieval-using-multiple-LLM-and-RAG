The provided text mentions **Dropout** as a regularization technique used to prevent overfitting. 

It states: "With limited training samples, learning too many parameters directly will result in overﬁtting even after using over-ﬁtting prevention techniques such as Dropout [7]."


The text does not mention other regularization methods like L2 regularization.
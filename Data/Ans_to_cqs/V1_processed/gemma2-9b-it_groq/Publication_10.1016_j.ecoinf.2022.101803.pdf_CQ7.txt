The provided text focuses on ensemble methods like bagging and boosting, particularly XGBoost, and doesn't delve into data splitting criteria for deep learning models. 

Generally, for deep learning, the data is split into three main sets:

* **Training set:** This is the largest portion of the data used to train the deep learning model. The model learns patterns and relationships within this data.
* **Validation set:** This set is used to evaluate the model's performance during training. It helps monitor how well the model generalizes to unseen data and prevents overfitting (where the model performs well on training data but poorly on new data).
* **Test set:** This set is held out entirely from the training process and is used only once at the end to provide a final, unbiased evaluation of the trained model's performance.

The typical split ratios are:

* 70-80% for training
* 10-15% for validation
* 10-20% for testing

The specific split ratios can be adjusted depending on the size of the dataset and the complexity of the model.
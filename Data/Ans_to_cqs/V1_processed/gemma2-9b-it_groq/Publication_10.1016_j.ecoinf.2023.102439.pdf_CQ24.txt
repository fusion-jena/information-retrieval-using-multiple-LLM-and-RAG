The provided text focuses on the application of artificial neural networks, particularly the NARX model, in ecological informatics. It doesn't delve into the strategies used to handle randomness in the broader deep learning pipeline. 

However, I can offer some general information about how randomness is managed in deep learning:

* **Random Initialization:**  Weights and biases in neural networks are often initialized randomly. This prevents all neurons from starting with the same values, promoting diversity in the network's learning process.

* **Dropout:** This technique randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents overfitting by forcing the network to learn more robust features that are not reliant on any single neuron.

* **Batch Normalization:** This method normalizes the activations of each layer during training, reducing the internal covariate shift and stabilizing learning.

* **Random Data Shuffling:** Training data is often shuffled randomly before each epoch (pass through the entire dataset). This ensures that the network doesn't learn patterns based on the order of data points.

* **Random Seed:** Setting a random seed ensures that the random number generator produces the same sequence of random numbers each time the code is run. This allows for reproducibility of experiments. 



Let me know if you have any other questions.
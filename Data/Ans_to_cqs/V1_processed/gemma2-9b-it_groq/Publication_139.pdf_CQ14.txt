The provided text does not specify the exact method used for hyperparameter optimization (e.g., grid search, random search). It only mentions that the optimizer Adam was selected with an initial learning rate of 1Ã—10<sup>-4</sup> and early stopping was used to prevent overfitting.
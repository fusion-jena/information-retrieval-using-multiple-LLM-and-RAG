The provided text focuses on behavioral studies and learning models in fish, and doesn't contain information about deep learning or strategies for handling randomness in machine learning pipelines. 

To answer your query about randomness in deep learning, here are some common strategies:

* **Setting a random seed:** This initializes the random number generator with a specific value, ensuring that the same "random" numbers are generated every time the code is run. This is crucial for reproducibility.

* **Weight initialization:** Different methods exist for initializing the weights of neural networks, some of which introduce randomness to help break symmetry and improve learning.

* **Dropout:** This technique randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents overfitting and encourages the network to learn more robust features.
* **Batch normalization:** This technique normalizes the activations of neurons within each mini-batch, reducing the internal covariate shift and stabilizing training.

These are just a few examples, and the specific strategies used can vary depending on the task and the architecture of the deep learning model.
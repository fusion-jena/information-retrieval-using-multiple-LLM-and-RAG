The provided text focuses on data preprocessing and model training techniques for a specific classification task related to wildlife exploitation. It doesn't delve into strategies for handling randomness within the broader deep learning pipeline. 

While the text mentions random sampling techniques for data selection (randomly selected sample and balanced stratified sample), it doesn't discuss strategies for managing randomness in other aspects of the pipeline, such as:

* **Random weight initialization:**  Deep learning models often initialize weights randomly. 
* **Dropout:** A regularization technique that randomly drops out neurons during training.
* **Batching:** Training data is often split into batches, and the order of these batches can be randomized.
* **Random seed setting:**  Using a fixed random seed ensures reproducibility by making the random number generator produce the same sequence of numbers each time the code is run. 


To get a comprehensive answer about randomness handling in deep learning, you'd need to consult resources specifically addressing deep learning best practices and common techniques.
The provided text focuses on hyperparameter tuning for machine learning models like decision trees, random forests, and gradient boosting, not deep learning. Therefore, it doesn't offer information on strategies for handling randomness in the deep learning pipeline. 

However, I can tell you that randomness is indeed a common element in deep learning.  Here are some strategies used to manage it:

* **Random Seed:** Setting a random seed is crucial. This ensures that the random numbers generated during processes like weight initialization, shuffling data, and dropout are reproducible. 

* **Weight Initialization:**  Different methods for initializing weights (e.g., Xavier initialization, He initialization) aim to reduce the impact of initial randomness on training.

* **Dropout:** This regularization technique randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents overfitting and introduces stochasticity into the network.

* **Batch Normalization:** This technique normalizes the activations within each layer, helping to stabilize training and reduce the sensitivity to weight initialization.

* **Data Augmentation:**  Artificially increasing the size and diversity of the training data by applying random transformations (e.g., rotations, flips, crops) can help the model generalize better and be less affected by small variations in the data. 



Let me know if you have other questions about machine learning or deep learning!
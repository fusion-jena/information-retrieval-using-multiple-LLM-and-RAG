The provided text states that the models were trained using the Adam optimizer with a learning rate of 10<sup>-5</sup>.  It also mentions that the scaling of the clustering loss, Î³, was cycled between 0.01 and 0.2 every 100 epochs.
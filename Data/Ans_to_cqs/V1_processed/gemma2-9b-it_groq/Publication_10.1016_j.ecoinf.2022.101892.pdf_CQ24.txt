The provided text focuses on age estimation using deep learning and doesn't delve into specific strategies for handling randomness within the deep learning pipeline. 

However, it does mention a few points relevant to randomness:

* **Data Augmentation:** The text states that "aggressive data augmentation" was used, which often involves random transformations (like horizontal flipping) to increase dataset diversity and prevent overfitting. This introduces randomness during the training process.
* **Training Process:** The text describes training on Nvidia V100 GPUs and mentions batch normalization (Ioffe & Szegedy, 2015), a technique that helps stabilize training by normalizing activations. While not directly addressing randomness, these factors influence the training dynamics.

To comprehensively answer your query about strategies for handling randomness in deep learning, one would need to explore concepts like:

* **Random Seed Setting:**  Initializing random number generators with a specific seed ensures reproducibility of results.
* **Dropout:** A regularization technique that randomly "drops out" neurons during training, preventing over-reliance on specific features and introducing noise.
* **Weight Initialization:** Different methods exist for initializing model weights, some of which introduce randomness to aid in learning.



Let me know if you have other questions about the provided text or age estimation using deep learning.
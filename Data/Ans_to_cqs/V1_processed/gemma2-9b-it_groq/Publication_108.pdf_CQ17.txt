The text mentions the use of **dropout** as a regularization method to prevent overfitting.  

It states: "The dropout layers randomly dropped some neurons, causing the feed-forward network to perform worse, while dropout was deactivated and all neurons were kept during the validation." 


The text does not mention any other regularization methods, such as L2 regularization.
The provided text focuses on the application of Convolutional Neural Networks (CNNs) in image analysis tasks related to agriculture and forestry. It doesn't discuss regularization methods used to prevent overfitting in deep learning pipelines.  

To answer your question about regularization methods, here are some common ones:

* **Dropout:** Randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents the network from relying too heavily on any single neuron and encourages the learning of more robust features.

* **L2 Regularization (Weight Decay):** Adds a penalty term to the loss function that is proportional to the square of the magnitude of the weights. This discourages the weights from becoming too large, which can lead to overfitting.

* **L1 Regularization (Lasso):**  Similar to L2 regularization but uses the absolute value of the weights. This can lead to some weights being driven to exactly zero, effectively performing feature selection.

* **Early Stopping:** Monitors the performance on a validation set and stops training when the performance on the validation set starts to decrease.


Let me know if you have any other questions.
The provided text focuses on the training and hyperparameter optimization of Support Vector Machines (SVMs) and Convolutional Neural Networks (CNNs) for image classification. It doesn't specify the criteria used to split the data for training, testing, and validation. 

To answer your query generally, data splitting for deep learning typically follows these common practices:

* **Training Set:** The largest portion of the data (e.g., 70-80%) is used to train the model. The model learns patterns and relationships within this data.
* **Validation Set:** A smaller portion (e.g., 10-15%) is used to evaluate the model's performance during training. This helps in selecting the best model parameters (hyperparameters) and preventing overfitting (where the model performs well on training data but poorly on unseen data).
* **Test Set:**  A separate, held-out portion (e.g., 10-15%) is used only once at the end of training to provide an unbiased estimate of the model's performance on unseen data. 

The specific split ratios can vary depending on the dataset size and the complexity of the problem.
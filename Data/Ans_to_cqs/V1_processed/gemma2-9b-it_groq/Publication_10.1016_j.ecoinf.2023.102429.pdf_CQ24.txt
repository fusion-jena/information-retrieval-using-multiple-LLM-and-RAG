Several strategies are used to handle randomness in the deep learning pipeline. 

* **Random Seed Setting:**  A common practice is to set a fixed random seed value at the beginning of the training process. This ensures that the random number generator used for tasks like weight initialization, data shuffling, and dropout selection produces the same sequence of "random" numbers each time the code is run. This reproducibility is crucial for comparing experiments and tracking model behavior.

* **Data Augmentation:**  Techniques like image rotation, flipping, cropping, and color adjustments introduce controlled variations into the training data. This helps the model learn more robust features and generalize better to unseen data, mitigating the impact of randomness in the training set.

* **Batch Normalization:** This technique normalizes the activations within each layer of the network, reducing internal covariate shift and stabilizing the training process. This can help alleviate the negative effects of randomness in the input data.

* **Dropout:**  During training, dropout randomly "drops out" (sets to zero) a percentage of neurons in each layer. This prevents overfitting by forcing the network to learn more redundant representations and be less reliant on any single neuron.

* **Ensemble Methods:**  Training multiple models with different random initializations and averaging their predictions can reduce the impact of randomness in individual models.

By employing these strategies, deep learning practitioners can minimize the influence of randomness and improve the reliability and reproducibility of their models.
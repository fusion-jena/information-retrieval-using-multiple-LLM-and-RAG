The provided text states that the Adam optimizer algorithm was used for model optimization. 

A batch size of 10 was used, and the models were trained for 100 epochs. 

The learning rates were chosen empirically through trials over 100 epochs.  The specific learning rates are not explicitly mentioned in the text.
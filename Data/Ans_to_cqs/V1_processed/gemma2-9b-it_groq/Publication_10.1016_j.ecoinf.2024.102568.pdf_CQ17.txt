The provided text mentions one regularization method used to prevent overfitting: **dropout**. 

Dropout, as described, randomly excludes some inputs from activation and weight updates during training. This prevents the network from relying too heavily on any single neuron and encourages the learning of more robust features. 

The text does not mention other regularization methods like L2 regularization.
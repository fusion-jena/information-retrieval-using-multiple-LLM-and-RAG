The provided text focuses on the TPEBO algorithm for hyperparameter optimization in LSTM models and doesn't offer details about data splitting criteria for deep learning. 

Generally, in deep learning, data is split into three main sets:

* **Training set:** This is the largest portion of the data used to train the model. The model learns patterns and relationships within this data.

* **Validation set:** This set is used to evaluate the model's performance during training. It helps monitor how well the model is generalizing to unseen data and prevents overfitting (where the model performs well on training data but poorly on new data).

* **Test set:** This set is held out entirely from the training process and is only used for a final evaluation of the trained model's performance on completely unseen data. This provides an unbiased estimate of how well the model will perform in real-world scenarios.

The typical split ratios are 70% for training, 15% for validation, and 15% for testing. However, these ratios can be adjusted depending on the size and nature of the dataset.
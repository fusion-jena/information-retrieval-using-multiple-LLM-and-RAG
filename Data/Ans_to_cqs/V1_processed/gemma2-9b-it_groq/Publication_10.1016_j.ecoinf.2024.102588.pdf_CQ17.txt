The provided text mentions one regularization method used to prevent overfitting: dropout. 

Dropout randomly inactivates a percentage of nodes (in this case, 40%) that contain learnable weights during training. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features. 

The text does not mention L2 regularization.
The provided text focuses on hyperparameter optimization for machine learning models like XGBoost and LightGBM, using techniques like Bayesian optimization and TDO.  

It doesn't mention specific optimization techniques used within the deep learning training process, such as SGD (Stochastic Gradient Descent) or Adam.
The provided text mentions that deep learning models for audio processing often divide long audio files into shorter chunks to fit within the limited memory of GPUs.  

It also states that  "a new run of statistical optimisation must be performed for a new query task," suggesting that GPUs are likely used for both training and inference (query-time adaptation). 


While the text doesn't explicitly state the use of TPUs, it's possible they could also be used given their increasing popularity in deep learning.
The provided text describes the process of splitting the data for training a deep learning model but doesn't explicitly state the criteria used. 

Here's what we can infer:

* **Training set:** The majority of the data is used to train the model. The text mentions removing the testing set before training, implying the remaining data is used for training.
* **Validation set:** 10% of the data is reserved as a validation set. This set is used during training to monitor for overfitting (when the model performs well on training data but poorly on unseen data). 
* **Test set:** The text states that the testing set is removed before training. This set is used *after* training to evaluate the final performance of the model on unseen data.

While the exact criteria for splitting the data (e.g., random sampling, stratified sampling) are not mentioned, the general approach aligns with common practices in deep learning:

1. **Splitting into training, validation, and test sets:** This is typically done to ensure a fair and unbiased evaluation of the model's performance.
2. **Random sampling:** Data is often randomly shuffled and then split into the three sets to avoid introducing bias.
3. **Stratified sampling:** If the data has imbalanced classes, stratified sampling might be used to ensure each set has a representative distribution of classes.
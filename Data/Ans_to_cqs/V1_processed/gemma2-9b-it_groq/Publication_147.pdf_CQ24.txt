The provided text focuses on comparing the performance of deep learning (CNN) and shallow learning (XGBoost) algorithms for wetland mapping. It doesn't delve into the specific strategies used to handle randomness within the deep learning pipeline.  

To address your query about handling randomness in deep learning, here are some common strategies:

* **Setting a Random Seed:**  A random seed is a starting point for the random number generator. By setting a fixed seed, you ensure that the same "random" values are generated each time you run your code, making your results reproducible.

* **Data Augmentation:** This involves introducing controlled variations to your training data (e.g., rotations, flips, brightness adjustments). This helps the model generalize better and reduces the impact of random fluctuations in the data.

* **Dropout:** This technique randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features.

* **Batch Normalization:** This method normalizes the activations of neurons within each mini-batch during training. This can help stabilize the training process and reduce the impact of random initialization. 



Let me know if you have any other questions.
The provided text mentions using "smaller learning rates" during the fine-tuning phase of the training process. It also states that data augmentation and dropout techniques were applied.  

However, the specific optimization algorithm used (e.g., SGD, Adam) is not mentioned in the text.
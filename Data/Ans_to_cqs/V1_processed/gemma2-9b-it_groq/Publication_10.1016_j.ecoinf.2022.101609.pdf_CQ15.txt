The text states that the researchers experimented with several optimization functions, including SGD, RMSProp, LAMB, LARS, and Adam.  They ultimately chose **Adam** as the optimizer for training their model.
The provided text focuses on the mini-batch size and its impact on deep learning training, but it doesn't specify the criteria used to split data into training, testing, and validation sets. 

To answer your query, data splitting for deep learning typically follows these general principles:

* **Training Set:** The largest portion of the data (e.g., 70-80%) is used to train the model. The model learns patterns and relationships within this data.
* **Validation Set:** A smaller portion (e.g., 10-15%) is used to evaluate the model's performance during training. This helps monitor overfitting (when the model performs well on training data but poorly on unseen data) and tune hyperparameters.
* **Test Set:**  A separate, held-out portion (e.g., 10-15%) is used *only* after training is complete to provide an unbiased evaluation of the final model's performance on unseen data. 

The exact split ratios can vary depending on the dataset size and specific project requirements.
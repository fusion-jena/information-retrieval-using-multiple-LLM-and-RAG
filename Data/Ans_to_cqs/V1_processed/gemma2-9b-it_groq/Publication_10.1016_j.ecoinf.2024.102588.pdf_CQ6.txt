The provided text focuses on the architecture and training of CNN and autoencoder models, and doesn't explicitly detail the preprocessing steps involved before training. 

However, it does mention "batch normalization" which is a technique used to normalize the activations of a layer for each mini-batch, helping to stabilize training and improve generalization. This can be considered a form of data preprocessing.

Generally, common preprocessing steps for deep learning models include:

* **Normalization:** Scaling features to a specific range (e.g., 0 to 1) to prevent features with larger scales from dominating the learning process.
* **Scaling:**  Similar to normalization, but often involves subtracting the mean and dividing by the standard deviation of each feature.
* **Cleaning:** Handling missing values, removing outliers, and correcting inconsistencies in the data.
* **Encoding categorical variables:** Converting categorical data into numerical representations that the model can understand. 


It's important to note that the specific preprocessing steps used will depend on the nature of the data and the chosen deep learning model.
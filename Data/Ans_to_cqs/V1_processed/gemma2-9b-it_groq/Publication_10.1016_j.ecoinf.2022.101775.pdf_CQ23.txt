The provided text describes the training process of deep learning models but doesn't specify details about techniques used to ensure generalizability. 

Here's what we know from the text:

* **Data Splitting:** The text mentions that 10% of the data was used as a validation set to check for overfitting. This suggests a basic train-validation split, but it doesn't explicitly state if stratified splitting was used to maintain class proportions.
* **Hyperparameter Tuning:** The text mentions using a batch size of 64 and training for 100 epochs with the Adam optimizer. These are hyperparameters that can influence model performance and generalization.

**Missing Information:**

The text lacks information about:

* **Dataset Diversity:**  There's no mention of the dataset's diversity in terms of features, classes, or other relevant characteristics.
* **Cross-Validation:** The text doesn't describe the use of cross-validation techniques (e.g., k-fold cross-validation) to further evaluate model performance and generalization.


Therefore, based on the provided text, we can't definitively say what measures were taken to ensure the generalizability of the deep learning model.
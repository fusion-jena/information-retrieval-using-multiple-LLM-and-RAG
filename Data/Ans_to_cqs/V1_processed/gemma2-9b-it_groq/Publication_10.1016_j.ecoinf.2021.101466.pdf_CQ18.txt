The provided text doesn't explicitly describe a strategy for monitoring model performance during training. 

It mentions:

* **Training and validation sets:** 80% of the data was used for training, and 20% for validation. This suggests a basic split for evaluating performance.
* **Metrics:** Precision, recall, F-measure are reported in tables, likely calculated on the validation set.
* **Epochs:** Training was conducted for 100 epochs.

However, there's no information on:

* **Loss function:** What function was used to measure the difference between predictions and ground truth during each training step?
* **Learning rate scheduling:** Was the learning rate adjusted over time to optimize training?
* **Visualization tools:** Were any graphs or plots used to track performance metrics during training?



Therefore, based on the given context, we can't definitively state the specific strategy used to monitor model performance during training.
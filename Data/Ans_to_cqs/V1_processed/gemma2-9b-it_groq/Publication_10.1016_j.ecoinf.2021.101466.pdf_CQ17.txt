The provided text mentions dropout as a regularization method used to prevent overfitting. 

Specifically, it states: "The dropout rate for each task was set at 0.5."  

Dropout randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents the model from relying too heavily on any single neuron and encourages the learning of more robust features, thus reducing overfitting. 

The text does not mention other regularization methods like L2 regularization.
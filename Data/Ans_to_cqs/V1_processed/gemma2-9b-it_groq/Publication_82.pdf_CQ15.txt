The provided text doesn't specify the exact optimization techniques used (e.g., SGD, Adam). It only mentions the following hyperparameters:

* Momentum: 0.9
* Weight decay: 0.0005
* Gamma: 0.1
* Batch size: 24 (for GoogLeNet), 100 (for AlexNet) 


These hyperparameters are related to the training process but don't directly reveal the chosen optimization algorithm.
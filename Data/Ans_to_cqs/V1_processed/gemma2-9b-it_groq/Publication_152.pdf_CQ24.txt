The provided text focuses on the application of a U-Net for identifying Hedychium Coronarium, an invasive plant species. It doesn't delve into strategies for handling randomness in the deep learning pipeline. 

To answer your query generally, here are some common strategies used to manage randomness in deep learning:

* **Setting a Random Seed:**  This initializes the random number generator, ensuring that the same "random" values are produced every time you run your code. This is crucial for reproducibility.

* **Data Augmentation:**  Techniques like random cropping, flipping, rotation, and brightness adjustments introduce controlled variations into the training data, helping the model generalize better and be less sensitive to minor variations in input.

* **Dropout:**  During training, dropout randomly "drops out" (ignores) a percentage of neurons in a layer. This prevents overfitting by forcing the network to learn more robust features.

* **Batch Normalization:**  This technique normalizes the activations within each layer, reducing internal covariate shift and stabilizing training.

Let me know if you have other questions about deep learning or the U-Net application described in the text.
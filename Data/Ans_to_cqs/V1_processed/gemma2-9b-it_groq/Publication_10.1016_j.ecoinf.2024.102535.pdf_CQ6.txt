The provided text focuses on the architecture and training process of deep learning models, particularly ResNet, and doesn't delve into specific preprocessing steps. 

However, based on common practices in deep learning, here are some preprocessing steps often involved:

* **Data Cleaning:** This involves handling missing values, removing duplicates, and correcting inconsistencies in the data.

* **Normalization:** Scaling features to a common range (e.g., 0 to 1 or -1 to 1) can improve model training stability and performance.

* **Feature Engineering:**  Creating new features from existing ones can sometimes enhance model accuracy.

* **Data Augmentation:**  As mentioned in the text, techniques like rotation and flipping can be used to artificially increase the size and diversity of the training dataset. 


It's important to note that the specific preprocessing steps depend on the nature of the data and the chosen deep learning model.
The provided text describes the process of hyperparameter optimization and model selection, but it doesn't explicitly state the strategy used to monitor model performance *during* training. 

We know the following:

* **Performance metrics are evaluated on a test set:** This implies that the model's performance is assessed after training is complete.
* **5-fold cross-validation is used for hyperparameter tuning:** This technique involves splitting the training data into five folds, training the model on four folds, and evaluating it on the remaining fold. This process is repeated five times, using each fold as the test set once.

Based on this information, we can infer that the model's performance is likely monitored during each fold of the cross-validation process. However, the specific metrics used and how frequently they are reported are not mentioned in the text.
The provided text mentions the following regularization techniques used to prevent overfitting in the deep learning pipeline:

* **Dropout:** Applied to the input layer, hidden layers, and all layers, with varying results depending on the specific implementation. 
* **Weight Decay:**  Also known as L2 regularization, it penalizes large weights during training, effectively shrinking them and preventing overfitting.
* **Batch Normalization:** Applied to each layer of the network separately and to all layers simultaneously. The best results were achieved when applied to the third layer.
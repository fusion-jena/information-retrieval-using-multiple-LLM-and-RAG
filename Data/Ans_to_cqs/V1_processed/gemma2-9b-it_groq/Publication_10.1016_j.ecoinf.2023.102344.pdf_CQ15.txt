The provided text mentions two optimization techniques used in the deep learning pipeline:

* **Stochastic Gradient Descent (SGDM):** This is explicitly stated as the optimization algorithm used in Approach 2 for training pre-trained models. 

* **Adaptive Moment Estimation (ADAM):**  The text describes ADAM as being preliminarily used but ultimately discarded because it led to a sharp minimum and negligible accuracy.
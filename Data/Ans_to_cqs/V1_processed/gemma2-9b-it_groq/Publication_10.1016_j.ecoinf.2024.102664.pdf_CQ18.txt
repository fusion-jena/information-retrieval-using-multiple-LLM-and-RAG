The provided text describes the model evaluation process using validation and testing data, but it doesn't explicitly state the strategy used to monitor model performance *during* training. 

We know the following:

* **Data Split:** Data is divided into training (2003-2020), validation (2021), and testing (2022) sets.
* **Hyperparameter Tuning:** The model structure and hyperparameters (like kernel size and number of nodes) are optimized using the validation data.

It's likely that the performance on the training data itself was monitored during the training process. This is a common practice to track how well the model is learning. However, the specific metrics used and the frequency of monitoring are not mentioned in the text.
The strategy used to monitor the model's performance during training was **early stopping**.  

This method involves halting the training process when the model's performance on a validation subset starts to degrade.  

In this case, a **patience** of 5 epochs was set. This means that if the validation loss didn't improve for 5 consecutive epochs, the training would be stopped.  

This technique helps prevent **overfitting**, where the model learns the training data too well and performs poorly on unseen data. 


Early stopping was successful in this case, as optimal training was achieved after 9 epochs, with a maximum loss of 3 for both the training and validation subsets.
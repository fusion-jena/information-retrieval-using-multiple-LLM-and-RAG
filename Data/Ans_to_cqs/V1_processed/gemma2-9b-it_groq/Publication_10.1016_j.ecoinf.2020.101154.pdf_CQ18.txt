The text describes the use of **early stopping** as a technique to monitor model performance during training. 

Early stopping is implemented to prevent overfitting, which occurs when a model learns the training data too well and performs poorly on unseen data. 

The process works by halting the training process when the model's performance on a validation set (a subset of the data not used for training) starts to decrease. This helps ensure that the model generalizes well to new data. 

The text specifically mentions that early stopping was used for the RI models, determining the number of iterations required for convergence.
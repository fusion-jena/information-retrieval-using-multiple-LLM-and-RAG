The provided text focuses on forest research and remote sensing techniques, and doesn't delve into the specifics of deep learning pipelines or strategies for handling randomness.  

To answer your query about randomness in deep learning, here are some common strategies:

* **Setting a Random Seed:**  Deep learning models often use random initialization for weights and biases. Setting a fixed random seed ensures that the same "random" values are used each time you run the model, making the results reproducible.

* **Data Augmentation:**  Randomly applying transformations to your training data (e.g., rotations, flips, crops) can introduce variation and help the model generalize better to unseen data.

* **Dropout:** This technique randomly "drops out" (ignores) a percentage of neurons during each training step. This prevents overfitting by forcing the network to learn more robust features.

* **Batch Normalization:** This technique normalizes the activations of each layer, reducing the internal covariate shift and stabilizing training.

* **Stochastic Gradient Descent (SGD):** This optimization algorithm uses a random subset (batch) of the training data to calculate the gradient at each step. This introduces randomness and can help escape local minima.



Let me know if you have other questions about deep learning or the provided research papers!
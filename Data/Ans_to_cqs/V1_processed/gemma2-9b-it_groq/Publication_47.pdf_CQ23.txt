The text describes several measures taken to improve the generalizability of the IUC-NN model:

* **Data Splitting:** The dataset was randomly split into training (90%), validation (20% of training data), and test (10%) sets. 
* **Cross-Validation:**  Five-fold cross-validation was performed on the validation set to assess the model's performance and robustness. This involves iteratively shifting the validation set and training the model on different subsets of the data.
* **Feature Selection:** The authors experimented with different numbers of hidden layers and subsets of features within the neural network to identify the configuration that yielded the lowest cross-entropy loss.

The text does not mention stratified splitting, which is a technique used to ensure that the distribution of classes (e.g., threatened vs. not threatened) is maintained in each data split. 


It's also worth noting that the authors acknowledge the relatively small size of the dataset, which could potentially limit the model's generalizability.
The provided text mentions dropout as a regularization technique used to prevent overfitting in deep neural networks.  

Specifically, it cites the paper by Srivastava et al. (2014) titled "Dropout: a simple way to prevent neural networks from overfitting."  

The text does not, however, list other regularization methods like L2 regularization.
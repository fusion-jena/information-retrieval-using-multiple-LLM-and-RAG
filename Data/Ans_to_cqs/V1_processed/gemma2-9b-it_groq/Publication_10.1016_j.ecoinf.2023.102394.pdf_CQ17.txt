The provided text mentions the use of **dropout** as a regularization method to prevent overfitting in the deep learning model. 

Specifically, it states: "To reduce overfitting, we created dropout layers with dropout rates of 0.2 and 0.3 in this study."

The text does not mention other regularization methods like L2 regularization.
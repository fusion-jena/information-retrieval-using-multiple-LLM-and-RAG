The provided text states that the study compared the SGD and Adam algorithms for optimization. 

It was found that using Adam in combination with ReduceLROnPlateau accelerated the convergence rate of model training and effectively reduced the risk of overfitting.
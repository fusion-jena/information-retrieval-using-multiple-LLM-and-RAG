The text mentions one regularization method used to prevent overfitting: dropout. 

Dropout randomly "drops out" (ignores) a certain proportion of neurons in a layer during training. This forces the network to learn more robust features that are not reliant on any single neuron. 


The text does not mention L2 regularization.
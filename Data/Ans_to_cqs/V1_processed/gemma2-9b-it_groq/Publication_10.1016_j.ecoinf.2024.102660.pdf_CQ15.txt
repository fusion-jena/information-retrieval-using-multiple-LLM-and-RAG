The provided text states that the learning rate was adjusted using the "equal-interval adjustment strategy (StepLR), halving the original rate every 10 epochs of iteration."  

This indicates that a learning rate scheduler, specifically StepLR, was used for optimization.  StepLR is a technique that periodically adjusts the learning rate during training. 

The text does not explicitly mention the base optimizer used (e.g., SGD, Adam).
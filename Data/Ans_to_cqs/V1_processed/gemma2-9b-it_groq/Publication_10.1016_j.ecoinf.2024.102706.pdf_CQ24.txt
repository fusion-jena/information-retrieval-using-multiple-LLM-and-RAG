The provided text focuses on comparing different machine learning algorithms, particularly tree-based methods like LightGBM, RF, and XGBoost, and doesn't delve into the specifics of handling randomness in deep learning pipelines. 

To answer your query about strategies for handling randomness in deep learning, here are some common techniques:

* **Setting a Random Seed:**  This is a fundamental practice. By initializing the random number generator with a specific seed value, you ensure that the same "random" numbers are generated every time you run your code. This makes your results reproducible.

* **Data Augmentation:**  This involves introducing controlled variations into your training data (e.g., image rotations, flips, color adjustments). This helps the model generalize better and reduces the impact of random fluctuations in the data.

* **Dropout:** A regularization technique used in neural networks where a percentage of neurons are randomly "dropped out" (ignored) during each training step. This prevents overfitting and encourages the network to learn more robust features.

* **Batch Normalization:**  A technique that normalizes the activations of neurons within each layer, helping to stabilize training and reduce the sensitivity to initial weight values.



Let me know if you have any other questions about machine learning algorithms or techniques!
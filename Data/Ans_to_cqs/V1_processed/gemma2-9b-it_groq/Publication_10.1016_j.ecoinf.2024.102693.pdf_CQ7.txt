The provided text focuses on the performance of different deep learning models on a specific dataset (IP102) and doesn't describe the data splitting criteria used for training. 

Generally, in deep learning, data is split into three main sets:

* **Training set:** This is the largest portion of the data used to train the model. The model learns patterns and relationships within this data.
* **Validation set:** This set is used to evaluate the model's performance during training. It helps to tune hyperparameters and prevent overfitting (where the model performs well on training data but poorly on unseen data).
* **Test set:** This set is held out entirely from the training process and is only used to evaluate the final model's performance on unseen data. It provides an unbiased estimate of how well the model will generalize to new data.

The specific proportions of data allocated to each set can vary depending on the dataset size and the complexity of the problem. A common split is 70% for training, 15% for validation, and 15% for testing.
The provided text focuses on the comparison of different deep learning models for classifying possum vocalizations and doesn't delve into the specific preprocessing steps applied to the audio data before training. 

However, it does mention that:

* **Data is downsampled to 16 kHz:** This likely involves reducing the sampling rate of the original audio to match the input requirements of the pretrained AST model, which is designed for human-perceivable sounds.
* **Audio spectrograms are used:**  This suggests that the raw audio waveforms are transformed into spectrograms, which are visual representations of the audio signal's frequency content over time. Spectrograms are commonly used as input for deep learning models in audio processing tasks.

Based on common practices in audio deep learning, we can infer that additional preprocessing steps might include:

* **Normalization:** Scaling the amplitude of the audio signal to a specific range (e.g., -1 to 1) to improve model training stability.
* **Windowing:** Applying a window function to the audio signal to reduce spectral leakage and improve frequency resolution.
* **Mel-frequency cepstral coefficients (MFCC) extraction:**  Transforming the spectrograms into MFCCs, which are more robust features for representing human-like sounds.



It's important to note that these are educated guesses based on general knowledge. The exact preprocessing steps used in this specific study are not explicitly described in the provided text.
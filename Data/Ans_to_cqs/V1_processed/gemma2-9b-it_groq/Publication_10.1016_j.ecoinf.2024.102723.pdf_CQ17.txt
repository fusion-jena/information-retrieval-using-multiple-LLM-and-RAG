The provided text focuses on Bayesian Neural Networks and Monte Carlo Dropout (MCD) as a technique for incorporating uncertainty into deep learning models.  

While it mentions dropout as a regularization technique used in MCD, it doesn't delve into other common regularization methods like L2 regularization. 


Therefore, based on the given context, I can only confirm that **Dropout** is a regularization method used in the deep learning pipeline discussed in the text.
The provided text describes a data splitting strategy used in an AutoML process, but it doesn't explicitly state the criteria used for splitting the data into train, test, and validation sets. 

However, we can infer some information:

* **Training data (Bt):** This is the largest portion of the data used to train the deep learning models. 
* **Validation data (Av, Bv):** This data is used to evaluate the performance of candidate models during the training process. The "optimal" number of training epochs is determined by monitoring the model's performance on the validation data.
* **Test data (T):** This data is held out entirely from the training process and is used only to evaluate the final selected model's performance.

The text mentions using a "small subset" of the data for training candidate models (data partition At) and a "final" test data set (T). This suggests that the data is likely split into these three sets:

1. **Training set:** The largest portion used for model training.
2. **Validation set:** A portion used to tune hyperparameters and select the best-performing model.
3. **Test set:** A separate, unseen portion used for final performance evaluation.

The specific proportions for each set are not mentioned in the provided text.
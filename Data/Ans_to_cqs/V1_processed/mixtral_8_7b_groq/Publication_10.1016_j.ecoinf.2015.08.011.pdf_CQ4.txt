The provided context does not include information about data augmentation techniques such as flipping, rotating, or scaling. These techniques are often used in deep learning pipelines to increase the amount and diversity of training data by applying random transformations to the existing data. However, the context focuses on the use of artificial neural networks (ANNs) and the regularization technique of early stopping to prevent overfitting. It mentions the importance of selecting the number of layers, the number of neurons in the hidden layer, the learning rates, and the number of epochs for model training. The data set is split into a training set and a validation set, and the model weights are randomly initialized. The training process is stopped when the network begins to overfit the data. While these techniques are important for developing a robust ANN, they are not data augmentation techniques. Therefore, I don't know the answer to the query about the data augmentation techniques applied in the deep learning pipeline.
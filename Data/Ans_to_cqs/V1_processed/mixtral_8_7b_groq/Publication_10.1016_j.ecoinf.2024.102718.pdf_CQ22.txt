The performance of deep learning models is evaluated using various metrics, including accuracy, precision, recall, F1 score, loss, area under curve (AUC), Cohen’s kappa (CK), model size, inference time, FLOP count, and analysis time.

Accuracy measures the proportion of correctly classified instances out of the total instances in the dataset, providing an overall indication of how well the model is performing. Precision represents the proportion of true positive predictions among all positive predictions made by the model, while recall, also known as sensitivity, measures the proportion of true positive predictions among all actual positive instances in the dataset. The F1 score is the harmonic mean of precision and recall.

Loss measures the error between predicted and actual values in a model, while AUC represents the area under the Receiver Operating Characteristic (ROC) curve. Cohen’s kappa measures the agreement between two raters classifying items into mutually exclusive categories.

Additional metrics include model size, measured in terms of parameters or memory footprint, inference time, which measures the time taken by the model to process a single input and generate an output prediction, FLOP count, representing the number of arithmetic operations performed by the model during inference or training, and analysis time, measuring the time taken by the model to process a given dataset or perform a specific task.

For instance, in the context provided, the deep learning models are evaluated using accuracy, loss, precision, recall, and F1 score, as shown in Table 6. The proposed BEiT model achieves the highest accuracy of 97.33%, precision of 98.2%, recall of 97.7%, and F1 score of 96.5%.
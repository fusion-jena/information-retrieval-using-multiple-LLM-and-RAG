The context provided does not give specific information about the hardware resources used for training the deep learning model. However, it does mention the use of certain techniques and optimizations commonly used in deep learning model training.

During the training of the model, the authors used dropout and ADAM optimization, which are techniques used to prevent overfitting and improve the learning rate of the model, respectively. These techniques can be used on various hardware resources, such as CPUs, GPUs, or TPUs.

Dropout is a regularization technique that prevents overfitting by randomly dropping out a fraction of the neurons during training, which helps the model generalize better. ADAM optimization is an optimization algorithm used to update the weights of the neural network during training, which is known for its efficiency and effectiveness in finding optimal weights.

The authors also mention that the model was trained for 150 epochs with a batch size of 128. The number of epochs and batch size are hyperparameters that can affect the training time and the final performance of the model. These hyperparameters can also be adjusted based on the available hardware resources.

Therefore, while the context does not provide specific information about the hardware resources used for training the deep learning model, it is likely that the authors used a GPU or TPU to accelerate the training process, given the complexity of the model and the amount of data used. However, this is an assumption based on common practices in deep learning model training and not explicitly stated in the provided context.
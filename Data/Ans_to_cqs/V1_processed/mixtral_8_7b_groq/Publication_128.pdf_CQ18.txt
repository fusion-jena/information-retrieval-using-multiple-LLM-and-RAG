The strategy implemented to monitor the model performance during training involves the use of loss metrics, which include softmax cross-entropy and regularization loss. The softmax cross-entropy measures the distance between two distributions, with lower cross-entropy indicating a closer match. Regularization loss is used to reduce overfitting, thereby balancing model performance and complexity. These losses are normalized using softmax, which maps the true loss value to a range between zero and one.

Moreover, the study also employs a boxplot to visualize the 10 times 10-fold cross-validation (CV) prediction accuracy and loss of each deep learning (DL) classification model across five plant image datasets. The boxplot includes actual outcomes represented by black points, providing a comprehensive view of the model's performance during training.

Additionally, the study trains each neural network model with the same parameters and strategies, enabling a fair comparison of their performance. The training epochs vary depending on the model's depth, with models having less than 100 layers undergoing 60 epochs and those with more layers executing 80 epochs. This approach ensures that the model performance is closely monitored during training, and any signs of overfitting or underfitting are promptly addressed.

The computational resources and environment used in this study include CentOS Linux release 7.7.1908 (Core) with 32 CPUs, each having 64 processors. The development environment is built using Python (version 3.6.2) packaged by Conda-forge and compiled with GCC (version 4.8.2,20140120). The tool packages used in the virtual environment are TensorFlow (version 1.12.0) and Numpy (version 1.19.2). These resources provide the necessary computational power to train and monitor the model's performance effectively.
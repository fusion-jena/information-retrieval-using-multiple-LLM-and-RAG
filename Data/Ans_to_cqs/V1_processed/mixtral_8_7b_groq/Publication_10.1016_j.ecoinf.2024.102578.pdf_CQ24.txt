Based on the provided context, the specific strategies employed to handle randomness in the deep learning pipeline, such as the use of a random seed value, are not explicitly mentioned. However, I can provide some general information about how randomness is typically managed in deep learning models.

In deep learning, randomness can be introduced during various stages of the modeling process, such as data preprocessing, weight initialization, and dropout regularization. To ensure reproducibility and comparability of results, it is essential to control this randomness.

1. Data preprocessing: The keras image_data_generator function used in the study mentioned in the context includes several random data augmentation techniques, such as rotation, width and height shift, shear range, zoom range, and horizontal flipping. While the context does not mention a specific seed value, it is common practice to set a fixed seed value for the random number generator to ensure consistent data augmentation across multiple runs.

2. Weight initialization: Random weight initialization is a common practice in deep learning. By setting a fixed seed value, the same set of initial random weights will be generated for each run, allowing for consistent model training.

3. Dropout regularization: Dropout is a technique used to prevent overfitting by randomly dropping out neurons during training. Similar to the other sources of randomness, setting a fixed seed value ensures consistent neuron dropout across multiple runs.

In summary, while the provided context does not explicitly mention the strategies employed to handle randomness in the deep learning pipeline, it is common practice to control randomness during data preprocessing, weight initialization, and dropout regularization by setting a fixed seed value for the random number generator.
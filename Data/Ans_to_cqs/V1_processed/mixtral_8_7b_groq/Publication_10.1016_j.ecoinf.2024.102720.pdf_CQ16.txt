Based on the provided context, the criteria used to determine model fitness in a Structural Equation Modeling (SEM) context have been discussed. However, the context does not provide information on the criteria used to determine when training is complete in the context of machine learning models, such as those used for validation loss plateau.

In machine learning, training a model typically involves adjusting the model's parameters to minimize a loss function that measures the difference between the model's predictions and the actual data. During training, the model's performance on a validation set is monitored to ensure that the model is not overfitting to the training data.

One common criterion used to determine when training is complete is the validation loss plateau. This occurs when the model's performance on the validation set stops improving or starts getting worse, even as the model continues to train on the training data. At this point, further training is unlikely to improve the model's performance on new, unseen data.

To determine when a validation loss plateau has occurred, it is common to monitor the model's performance on the validation set after each epoch or iteration of training. A plot of the validation loss over time can be used to visualize whether the loss is decreasing or increasing. If the validation loss starts increasing or plateaus for a certain number of epochs, training can be stopped.

It is important to note that the specific criterion used to determine when training is complete can depend on the specific problem and dataset being used. Other criteria that may be used include early stopping based on a patience parameter, monitoring other metrics such as accuracy or precision, or using a separate test set to evaluate the model's performance.

Therefore, while the provided context discusses criteria used to determine model fitness in the context of SEM, it does not provide information on criteria used to determine when training is complete in the context of machine learning models. Common criteria used for the latter include monitoring the validation loss over time and stopping training when a validation loss plateau occurs.
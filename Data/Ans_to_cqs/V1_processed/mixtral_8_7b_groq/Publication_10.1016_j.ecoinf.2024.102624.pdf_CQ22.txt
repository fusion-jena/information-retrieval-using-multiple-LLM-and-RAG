Based on the provided context, the specific metrics used to evaluate the performance of the deep learning model were not mentioned. The text focuses on the data splitting process, the application of 10-fold cross-validation with three repetitions during the model building phase, and the calculation of the Run percentage using the Orfeo Toolbox in the QGIS 3.16 environment.

However, the context does mention the use of "accuracy assessments" and "accuracy" in general. In the context of machine learning and remote sensing, common metrics used to evaluate the performance of a model include:

1. Overall Accuracy (OA): the proportion of correctly classified samples out of the total number of samples.
2. User's Accuracy (UA): the proportion of correctly classified samples for a specific class out of the total number of samples belonging to that class.
3. Producer's Accuracy (PA): the proportion of correctly classified samples for a specific class out of the total number of samples predicted to belong to that class.
4. Kappa Statistic: a measure of agreement between the predicted and actual classifications, taking into account the possibility of the agreement occurring by chance.
5. Cohen's Kappa: a statistical measure of inter-rater agreement for categorical data.
6. F1 Score: the harmonic mean of precision and recall, providing a single metric that encapsulates both.

Considering the focus on remote sensing in the provided context, it is possible that additional metrics specific to this field, such as the Kappa statistic or the Run percentage, were used. However, without more specific information, it is not possible to definitively answer which metrics were used to evaluate the performance of the deep learning model.
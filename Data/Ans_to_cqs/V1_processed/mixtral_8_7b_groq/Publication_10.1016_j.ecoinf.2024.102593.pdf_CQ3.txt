The provided context discusses the use of active learning and embeddings in the data generation and model selection process for deep learning, particularly in the field of audio classification. However, it does not explicitly mention the specific data annotation techniques used in the deep learning pipeline.

Common data annotation techniques used in deep learning include:

1. Image classification: This involves labeling an image with a single class or category.
2. Object detection: This involves drawing bounding boxes around objects within an image and labeling each box with the corresponding class.
3. Semantic segmentation: This involves dividing an image into multiple segments or regions and labeling each segment with the corresponding class.
4. Instance segmentation: This is a more advanced form of semantic segmentation, where each instance of an object within an image is segmented and labeled individually.

In the context of audio classification, the annotation techniques may include:

1. Audio event classification: This involves labeling an audio clip with a single class or category.
2. Sound event detection: This involves detecting and labeling individual sound events within an audio clip, similar to object detection in image classification.
3. Acoustic scene classification: This involves labeling an audio clip with the corresponding acoustic scene or environment.

In the provided context, the focus is on using active learning and embeddings to select the most informative samples for manual annotation, rather than the specific annotation techniques used. The authors propose using model embeddings and dimensionality reduction to display model predictions in an embedding visualization (EV) space, which can help in selecting the best sample candidates for manual annotation.

Therefore, while the provided context does not directly answer the query, it can be inferred that the data annotation techniques used in the deep learning pipeline depend on the specific task and dataset, and are not explicitly discussed in the context.
In the context provided, there is no explicit mention of strategies used to handle randomness in the deep learning pipeline, such as setting a random seed value. However, there are some implicit strategies that can be inferred from the described methods.


In the context provided, there are implicit strategies to handle randomness in the deep learning pipeline. One such strategy is the use of different initializations for training similar or identical CNNs. This approach involves altering the training process slightly by shuffling the sequence of data or using different batch sizes. By training an ensemble of architecturally similar or identical CNNs with different initializations, the impact of randomness on the model's performance can be reduced, as the group average prediction from several CNNs is expected to outperform the performance of any single CNN in the same group.

Additionally, the use of weighted random sampling for comparison purposes suggests that the authors are aware of the impact of randomness on model performance. By comparing the model's performance to that of a weighted random sampler, they provide a baseline for evaluating the model's ability to handle randomness. The weighted random sampler randomly guesses classes according to their occurring frequency, which can help to account for class imbalance and reduce the impact of randomness on the evaluation metric.

In summary, while the context does not explicitly mention strategies for handling randomness in the deep learning pipeline, such as setting a random seed value, it does imply the use of different initializations for training similar or identical CNNs and the use of weighted random sampling for comparison purposes. These strategies can help to reduce the impact of randomness on model performance and provide a more reliable evaluation of the model's predictive capabilities.
Deep learning models, like other machine learning algorithms, are susceptible to variations caused by randomness during different stages of the training process. This includes random initialization of weights, sampling of training data, and shuffling of data instances. These sources of randomness can impact the model's performance and convergence to an optimal solution. Several strategies are employed to handle randomness in the deep learning pipeline, and I will discuss a few of them here.

1. **Random Seed Value**: To ensure reproducibility of results, deep learning practitioners often set a fixed random seed value. This seed value is used to initialize the random number generator, which is employed in various processes such as weight initialization, data shuffling, and dropout. By using the same seed value, researchers can reproduce their experiments and obtain consistent results.

2. **Weight Initialization**: Deep learning models typically initialize weights randomly, following certain distributions. Common initialization strategies include zero-mean Gaussian distribution with a small standard deviation or Xavier initialization, which takes into account the number of input and output neurons to calculate the variance of the initialized weights. These initialization methods help in breaking symmetry and ensuring that all neurons in the network can learn different features.

3. **Data Shuffling**: Shuffling training data instances before each epoch helps prevent overfitting and ensures that the model receives diverse data during training. This strategy can also improve convergence by avoiding local minima.

4. **Dropout**: Dropout is a regularization technique used to prevent overfitting by randomly dropping out a certain percentage of neurons during training. This helps in reducing co-adaptation of neurons and encourages the model to learn more robust features.

5. **Batch Normalization**: Batch normalization standardizes and re-scales the activations of the neurons in a layer, which can help reduce the internal covariate shift caused by the changes in the distribution of inputs to each layer. This technique can also act as a form of regularization and improve the model's convergence.

6. **Ensemble Methods**: Ensemble methods, such as bagging or boosting, can be used to reduce the variance caused by randomness in the model. These methods combine multiple models trained on different subsets of the data or with different initialization parameters, which can lead to improved performance and reduced sensitivity to randomness.

In conclusion, various strategies are employed to handle randomness in the deep learning pipeline, such as setting a random seed value, employing different weight initialization techniques, shuffling data, using dropout and batch normalization, and adopting ensemble methods. These strategies help improve model performance, convergence, and reproducibility.
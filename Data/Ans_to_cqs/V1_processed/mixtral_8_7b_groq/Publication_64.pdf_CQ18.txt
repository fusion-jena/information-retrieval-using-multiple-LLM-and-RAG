The text provided does not give a detailed description of the strategy implemented to monitor the model performance during training. However, it does mention that for ResNet50 evaluation, they use random ten crop testing with smaller data augmentation parameters than those used during training. This suggests that they might be using a form of cross-validation, where they evaluate the model's performance on a separate subset of the data that hasn't been seen during training. The term "random ten crop" implies that they are randomly selecting crops (or sections) from the images and using them for testing. This technique is often used to augment the dataset and reduce overfitting.

Additionally, the authors mention that they aim to achieve a performance of around 50% accuracy, which suggests that they might be using a form of thresholding or early stopping to monitor the model's performance during training. Thresholding involves setting a threshold for the model's performance, and stopping the training process if the model's performance doesn't improve beyond that threshold. Early stopping involves stopping the training process if the model's performance on a validation set starts to degrade, even if the training loss is still decreasing.

Overall, while the text does not provide a detailed description of the strategy used to monitor the model performance during training, it does suggest that the authors might be using a combination of cross-validation, thresholding, and early stopping to ensure that the model's performance is satisfactory.
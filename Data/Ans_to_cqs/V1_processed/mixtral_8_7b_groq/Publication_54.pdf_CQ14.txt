The hyperparameters of the model are optimized using a combination of fivefold cross-validation, early stopping, and swapping of misclassified data with the training and validation datasets. The specific hyperparameters mentioned in the context are the exponential decay rate of second-moment estimation (0.999), a positive scalar value for epsilon (1e-08), network layer size, and the optimal number of hidden units. However, the context does not explicitly mention the use of grid search or random search for hyperparameter optimization.

Fivefold cross-validation is used to avoid overfitting and evaluate the model's performance on unbiased data. The training set is divided into five groups, and the model is trained on four of these groups while being tested on the remaining group. This process is repeated five times, with a different group used as the test set each time. The average performance across the five runs is then used as the final performance metric.

Early stopping is employed to prevent the model from overfitting to the training data. The model's performance is monitored during training, and if it stops improving on precision, recall, and F1-score, the training is stopped early.

Additionally, the misclassified data is swapped with the training and validation datasets to improve the stability of the model. The influence of various components in the framework is analyzed by comparing the performance of the model before and after swapping the misclassified data.

While the context does not explicitly mention grid search or random search, it is possible that these techniques were used for hyperparameter tuning in addition to the methods mentioned. However, based on the provided information, it can be concluded that the hyperparameters of the model are optimized using fivefold cross-validation, early stopping, and swapping of misclassified data with the training and validation datasets.
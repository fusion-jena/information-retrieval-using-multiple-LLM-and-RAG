The provided context includes several papers and resources related to deep learning, remote sensing, and data processing. However, not all of them directly address the question of techniques used to address data bias during preprocessing of the deep learning pipeline. Here are some techniques mentioned in the context that can be used to address data bias:

1. Stratified splitting: Stratified sampling is a method of sampling that ensures each subset (or stratum) of the population has an appropriate number of observations. In the context of deep learning, this can be used during the training-testing split to ensure that each subset of the data is represented in both the training and testing sets. This can help prevent bias in the training data from affecting the model's performance on the testing data.
2. Oversampling: Oversampling is a technique used to increase the number of instances of a particular class in the training data. This can be useful when there is an imbalance in the number of instances of different classes. By oversampling the minority class, the model can be trained on a more balanced dataset, reducing the risk of bias.
3. Undersampling: Undersampling is the opposite of oversampling and involves reducing the number of instances of the majority class in the training data. This can also be used to balance the dataset and reduce bias. However, it is important to note that undersampling can result in the loss of valuable information.
4. Diverse data collection: Diverse data collection involves collecting data from a variety of sources to ensure that the dataset is representative of the population. This can help prevent bias in the training data by ensuring that it includes a wide range of examples.

The paper "Cover Type Classification Using Convolutional Neural Networks" (Remote Sens. 2019, 11, 274) mentions using a stratified sampling strategy during the training-testing split to ensure that each subset of the data is represented in both the training and testing sets. The paper "Very High Resolution Tree Cover Mapping for Continental United States using Deep Convolutional Neural Networks" (In Proceedings of the AGU Fall Meeting Abstracts, New Orlean, LA, USA, 11â€“15 December 2017) mentions using a balanced dataset to train the model, but it does not specify whether oversampling, undersampling, or diverse data collection was used.

The paper "Adding gradient noise improves learning for very deep networks" (arXiv 2015, arXiv:1511.06807) does not directly address data bias during preprocessing, but it does mention using gradient noise to improve learning in very deep networks. This technique can help prevent overfitting and improve the model's generalization performance.

In summary, the techniques used to address data bias during preprocessing of the deep learning pipeline include stratified splitting, oversampling, undersampling, and diverse data collection. The provided context mentions using stratified sampling and balanced datasets, but it does not provide detailed information on other techniques.
Based on the provided context, there is no information about deep learning models being used. However, I can provide you with information about preprocessing steps mentioned in the context related to Gradient Boosting Regression Trees (GBRT) model.

The preprocessing steps involved before training a GBRT model include:

1. **Randomly splitting data into train and test datasets:** This is done to ensure that the model is tested using previously unseen combinations of explanatory variables. In the RY data splitting approach, individual observations were grouped according to year of capture, and years were randomly selected for inclusion in the train and test datasets.
2. **Setting initial parameters:** For the RI models, the same initial parameters were set for the analysis of the CS and NW datasets. These included a low learning rate and a high number of iterations. The learning rate determines the contribution of each tree to the model, while the maximum tree depth reflects the degree of interaction in the model. GBRT performs best using fairly shallow trees, so-called tree stumps.
3. **Hyperparameter tuning:** Hyperparameter tuning was done manually for RI models, while for RY models, the best performing models were selected based on MSE. The learning rate and maximum tree depth were optimized using grid search and early stopping techniques to avoid overfitting.

There is no mention of normalization, scaling, or cleaning steps in the provided context. However, these steps are generally recommended for preprocessing data before training machine learning models. Normalization and scaling can help ensure that all features contribute equally to the model, while cleaning can help remove outliers or errors in the data.
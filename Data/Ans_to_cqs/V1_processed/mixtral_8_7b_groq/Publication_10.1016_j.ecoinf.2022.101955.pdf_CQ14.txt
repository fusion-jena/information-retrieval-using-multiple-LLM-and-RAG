Based on the information provided, the context does not give specific details about how the hyperparameters of the models were optimized. However, it is mentioned that a Control() function was used to evaluate tuning parameters for model performance. Typically, hyperparameter tuning involves systematically adjusting the values of hyperparameters to find the set of hyperparameters that result in the best model performance. Common methods for hyperparameter tuning include grid search, random search, and Bayesian optimization.

In grid search, the values of each hyperparameter are selected from a predefined grid of possible values, and the performance of each combination of hyperparameter values is evaluated. In random search, the values of each hyperparameter are randomly sampled from a defined range of possible values, and the performance of each combination of hyperparameter values is evaluated. Bayesian optimization uses a probabilistic model to estimate the performance of different hyperparameter configurations and selects the most promising configurations to evaluate next.

Without more specific information about the Control() function used in this study, it is difficult to provide a more precise answer about how the hyperparameters were optimized. However, it is clear that some form of hyperparameter tuning was performed to evaluate the performance of the models.
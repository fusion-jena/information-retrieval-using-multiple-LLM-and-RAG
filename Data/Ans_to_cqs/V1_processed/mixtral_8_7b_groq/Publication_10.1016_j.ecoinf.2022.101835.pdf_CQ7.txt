The provided context discusses the use of machine learning models for predicting ecological events, specifically the first flowering date and full blossom date, using Random Forest (RF), Artificial Neural Network (ANN), and Gradient Boosting Decision Tree (GBDT) models. The data splitting criteria for deep learning model training, such as train, test, and validation sets, are not explicitly mentioned in the context. However, it is implied that the data splitting process was carried out during the optimization of the RF, ANN, and GBDT models using Optuna.

In general, data splitting is an essential step in machine learning model training to ensure the model's generalization performance. The data is typically divided into three sets: training, validation, and testing. The training set is used to train the model, the validation set is used to tune the hyperparameters and prevent overfitting, and the testing set is used to evaluate the model's performance on unseen data.

In the context, the authors used Optuna to optimize the structure and hyperparameters of the RF and ANN models. Optuna is a hyperparameter optimization framework that automatically searches for the best hyperparameters using a pruning algorithm to reduce the computational cost. The optimization process involves selecting candidates for hyperparameters and their ranges, as shown in Table 1. For the LightGBM models, the authors used Optuna's LightGBM Tuner for hyperparameter fitting.

The performance of the algorithms was evaluated by comparing the loss function (mean squared error (MSE)) of the optimized models. The mean absolute error (MAE) and the coefficient of determination (R2) were also calculated for reference. The results showed that the GBDT models had the best accuracy for both the first flowering date and the full blossom date, followed by the ANN and RF models.

Therefore, while the specific data splitting criteria are not provided in the context, it can be inferred that the data was split during the optimization process using Optuna. The optimization process involved selecting candidates for hyperparameters and their ranges, training the models on the training set, tuning the hyperparameters using the validation set, and evaluating the model's performance on the testing set. The best-fit parameters and their performance are presented in Table 2.
In the deep learning pipeline, a series of optimization techniques were applied to select the most suitable options for the model. These optimization techniques include the selection of appropriate optimizers, loss functions, and activation functions. Experimentation was carried out on popular optimization functions such as SGD, RMSProp, LAMB, LARS, and Adam. Eventually, the Adam optimizer was chosen for model training.

The learning rate, a crucial hyperparameter in the optimization process, was determined using the learning rate finder (lr_find) function of the TabularLearner class. This function resulted in a minimum value of 2.5e-4 for the learning rate. The number of architecture layers, the size of each layer, and dropout rates for the network were optimized using the Bayesian-optimization library.

The final architecture used to train the model consisted of 14 embedding layers, 3 dropout layers, 3 batchnorm1d layers, 3 linear layers, and 2 ReLU activation functions. The embedding layer was adopted for improved performance, inspired by the architecture proposed in Guo and Berkhahn (2016).

In summary, the Adam optimizer, a learning rate of 2.5e-4, and an optimized network architecture were selected through various optimization techniques in the deep learning pipeline.
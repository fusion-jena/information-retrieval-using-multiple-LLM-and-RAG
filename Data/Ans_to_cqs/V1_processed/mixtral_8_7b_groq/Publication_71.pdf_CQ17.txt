In the provided context, the specific regularization methods used to prevent overfitting in the deep learning pipeline are not explicitly mentioned. However, it is mentioned that a dropout layer was added for each convolutional layer with a dropout rate of '0.1' to mitigate overfitting. Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps to prevent overfitting by reducing the co-adaptation of neurons.

The context does not mention the use of L2 regularization, which is another common regularization technique used to prevent overfitting. L2 regularization adds a penalty term to the loss function, which is proportional to the square of the magnitude of the model's weights. This encourages the model to have smaller weights, which can help to prevent overfitting.

It is worth noting that the context provides details about other aspects of the deep learning pipeline, such as the learning policy, learning rate, momentum, weight decay, and data augmentation techniques. These are all important components of a deep learning pipeline, but they are not directly related to the question about regularization methods used to prevent overfitting.
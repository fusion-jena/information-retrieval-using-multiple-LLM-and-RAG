The provided context does not explicitly mention the datasets used in the deep learning pipeline. However, it does mention that data augmentation was applied to enhance sample variation, which suggests that the dataset used may not have been large or diverse enough. The data augmentation techniques included rotation, reflectance bias of ±10%, and random noise of ±10% reflectance for 15% of the input image.

The study used an ensemble of three models with varying pooling techniques (max pooling for two models and average pooling for one) to improve the results. This indicates that the dataset used may have been complex and required multiple models to accurately classify the images.

The training strategy involved using the Adam optimizer with 500 epochs and categorical-cross entropy loss function. The batch size was set to 64. These parameters suggest that the dataset used was not too large, as using a larger batch size would have been more efficient for larger datasets.

Therefore, while the exact datasets used are not specified, the context suggests that the dataset used may have been relatively small and complex, requiring data augmentation and an ensemble of models to accurately classify the images.
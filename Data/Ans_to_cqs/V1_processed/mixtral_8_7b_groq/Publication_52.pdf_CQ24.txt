In the context provided, the Extreme Learning Machine (ELM) approach, which is a type of deep learning algorithm, employs strategies to handle randomness in its learning process. ELM uses a Single-Hidden Layer Feed Forward Neural Network (SLFFNN) with randomly assigned input weights and biases in the hidden layer. These random assignments are made before seeing the training data. This means that the ELM algorithm does not require an initial random seed value to be set, unlike some other deep learning algorithms.

The randomness in the hidden layer parameters of ELM is further exploited by the fact that these parameters are independent from the activation functions and from the training data. This allows ELM to randomly create hidden nodes or hidden level parameters, which contributes to its ability to handle non-differential activation equations and to avoid known neural network problems such as stopping criterion, learning rate, and learning epochs.

Moreover, the ELM algorithm calculates the output weights using a single matrix multiplication, which further speeds up the learning process and reduces the dependency on initial random values. The mathematical basis provided for ELM (Equations (1)-(9)) in the context further supports the robustness of the algorithm against randomness in the learning process.

In summary, the ELM deep learning algorithm handles randomness by randomly assigning input weights and biases in the hidden layer, and by making these parameters independent from the activation functions and training data. The algorithm's ability to create hidden nodes or hidden level parameters randomly, and its use of a single matrix multiplication for output weight calculation, contribute to its robustness against randomness in the learning process.
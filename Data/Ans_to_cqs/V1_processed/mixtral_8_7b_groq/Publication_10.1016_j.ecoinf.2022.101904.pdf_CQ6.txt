The provided context discusses a study that uses deep learning models for complex wetland mapping, but it does not explicitly mention the preprocessing steps involved before training the models. However, it is common practice in deep learning to perform certain preprocessing techniques on the input data before training. These steps can include data normalization, scaling, and cleaning.

Normalization is the process of rescaling numeric columns in the dataset to a common range, often between 0 and 1. This is useful for algorithms that use distance measures or gradients, as it can help to ensure that all features contribute proportionately to the learning process. Scaling can also be used to adjust the magnitude of features, which can help to improve the speed and stability of some optimization algorithms.

Cleaning refers to the process of handling missing or invalid data. This can involve removing rows with missing values, imputing missing values with statistical estimates, or using techniques like dimensionality reduction to handle large amounts of missing data.

In the context of the study, it is likely that the input data underwent some or all of these preprocessing steps before being used to train the deep learning models. For example, remote sensing imagery often requires preprocessing to correct for atmospheric effects, sensor noise, and other sources of variation.

It is also worth noting that the specific preprocessing steps used can depend on the characteristics of the data and the requirements of the model. For example, some models may be more sensitive to the scale of the input features than others, and may require more extensive preprocessing as a result.

In summary, while the provided context does not explicitly mention the preprocessing steps used in the study, it is common practice in deep learning to perform data normalization, scaling, and cleaning before training models. These steps can help to improve the performance and stability of the learning process, and may be particularly important when working with large and complex datasets like remote sensing imagery.
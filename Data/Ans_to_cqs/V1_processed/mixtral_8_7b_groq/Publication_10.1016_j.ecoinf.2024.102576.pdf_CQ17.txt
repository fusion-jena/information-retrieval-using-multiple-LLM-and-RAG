The provided context does not contain specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it is common practice in deep learning to use techniques such as dropout, L2 regularization, and early stopping to prevent overfitting.

Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps to prevent over-dependence on any single neuron and improves the model's ability to generalize. L2 regularization, also known as weight decay, adds a penalty term to the loss function that discourages large weights, which can help to reduce overfitting. Early stopping is a method where training is halted as soon as the validation loss starts to increase, preventing the model from further overfitting to the training data.

These techniques are widely used in deep learning applications, including image classification and land-cover classification. The U-Net model, a type of convolutional neural network (CNN) mentioned in the context, can benefit from these regularization methods.

In summary, while the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, it is common practice to use techniques such as dropout, L2 regularization, and early stopping to prevent overfitting in deep learning models.
Based on the provided context, the text does not mention any details about deep learning models or the criteria used to split the data for training, testing, or validation. The context focuses on an ensemble modeling approach using regression-based and machine-learning models (MaxEnt, RF, and SVM) for predicting species distribution. The data splitting for model evaluation in this case is done using cross-validation, where 70% of the dataset is used for training and the remaining 30% for testing. The model performance is assessed using the Area Under the Receiving Operator Characteristics Curve (AUC), which ranges from 0.5 to 1.0, with higher values indicating more accurate models.

In the context of deep learning models, data splitting is crucial for evaluating model performance and preventing overfitting. Commonly, the data is divided into three subsets: training, validation, and testing.

1. Training set: This subset is used to train the model, allowing it to learn the underlying patterns in the data.
2. Validation set: This subset is used during the training process to tune hyperparameters and prevent overfitting. The model's performance on the validation set is monitored, and adjustments are made accordingly.
3. Testing set: This subset is used to evaluate the final model's performance. It is unseen during the training and validation process, providing an unbiased assessment of the model's ability to generalize to new data.

In summary, the provided context does not contain information on the criteria used to split data for deep learning model training. However, in deep learning models, data splitting is typically done into training, validation, and testing subsets to assess model performance and prevent overfitting.
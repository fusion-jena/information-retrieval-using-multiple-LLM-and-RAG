The provided context discusses the training of Convolutional Neural Networks (CNNs) in the context of overfitting and its implications, particularly in the privacy domain. However, it does not explicitly mention the preprocessing steps involved before training a deep learning model such as normalization, scaling, or cleaning. 

However, we can infer that batch normalization is used in the "no-overfitting" setting as one of the techniques to reduce overfitting. Batch normalization is a common preprocessing step in deep learning that normalizes the input layer by re-centering and re-scaling the activations of the previous layer for each mini-batch. This can help improve the generalization ability of the model and reduce the internal covariate shift during training.

Additionally, we can assume that some form of data cleaning has been performed on the GREMM dataset used in the study, as it is a common practice in machine learning to clean and preprocess the data before training the model. Data cleaning can include steps such as handling missing values, removing outliers, and transforming the data into a suitable format for the model.

In general, preprocessing steps in deep learning can include:

1. Data cleaning: Handling missing values, removing outliers, and transforming the data into a suitable format for the model.
2. Normalization: Scaling the data to a similar range to improve the training process and prevent the model from being dominated by features with larger values.
3. Feature engineering: Extracting new features from the data that can help improve the model's performance.
4. Data splitting: Splitting the data into training, validation, and testing sets to evaluate the model's performance.
5. Augmentation: Increasing the size of the training set by applying transformations to the data, such as rotation, scaling, and flipping.

Therefore, while the provided context does not explicitly mention the preprocessing steps involved, we can infer that some form of normalization (batch normalization) and data cleaning have been performed in the study. Additionally, other preprocessing steps such as feature engineering, data splitting, and augmentation are commonly used in deep learning.
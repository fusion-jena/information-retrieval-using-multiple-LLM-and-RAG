The strategy implemented to monitor the model performance during training involves the use of Grid Search Cross-Validation (GSCV) and k-fold cross-validation. 

Grid Search Cross-Validation (GSCV) is a technique used to systematically train and assess the performance of various models, each characterized by a different set of hyperparameters. In this study, a range of tuned parameters was defined for each algorithm in models 1 to 7 (Random forest, Multivariate adaptive regression splines, Penalized regression, Support vector machine, Gradient boosting, Artificial neural network, and k-Nearest Neighbors). The GSCV process involved generating a series of models with different combinations of these hyperparameters and evaluating their performance through cross-validation. The model with the optimal hyperparameter combination, demonstrating the most superior performance, was selected at the end of this process.

In addition to GSCV, k-fold cross-validation was employed to assess the performance of the models. This technique involves dividing the dataset into k subsets, with some of these subsets used for training the model and the others utilized to evaluate its performance. In this study, a 10-fold cross-validation design was employed, where the dataset was split into two distinct sets: 70% of the dataset was designated for model training, while the remaining 30% was used as a testing dataset. This approach ensured that the model was trained on a sufficient amount of data while also providing an independent dataset to evaluate its performance.

Therefore, the strategy implemented to monitor the model performance during training involves a systematic and rigorous evaluation of the models' performance using Grid Search Cross-Validation and k-fold cross-validation techniques. These methods help in preventing overfitting, estimating the models' performance, and selecting the optimal model with the best hyperparameter combination.
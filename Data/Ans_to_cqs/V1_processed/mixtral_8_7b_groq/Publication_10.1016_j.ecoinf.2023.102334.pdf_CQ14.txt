Based on the provided context, there is no explicit information given about how the hyperparameters of the model are optimized, such as through methods like grid search or random search. However, the authors do mention the use of an improved SoftMax function for training, which suggests that some hyperparameter tuning has been done to select this function.

In addition, the authors mention the use of a specific machine configuration, including a CPU, memory, video card, operating system, CUDA version, and programming environment. It is possible that these specific hardware and software choices were made based on hyperparameter optimization experiments.

Furthermore, the authors discuss the importance of keeping data compact within classes, particularly for complex distributions like the face of a sika deer. This suggests that hyperparameters related to data preprocessing, such as normalization or dimensionality reduction, may have been optimized to achieve this compactness.

Overall, while the specific methods used for hyperparameter optimization are not detailed in the provided context, it is clear that some hyperparameter tuning has been done to improve the model's performance.
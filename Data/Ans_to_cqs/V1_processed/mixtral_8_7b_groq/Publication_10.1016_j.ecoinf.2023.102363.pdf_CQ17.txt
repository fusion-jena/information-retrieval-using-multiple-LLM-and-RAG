The deep learning pipeline described in the provided context uses two regularization methods to prevent overfitting: dropout and DropConnect.

Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps to prevent over-reliance on any single neuron and promotes the development of redundant representations within the network (Srivastava et al., 2014). In the described pipeline, a dropout rate of 0.2 is used during fine-tuning.

DropConnect is a more advanced regularization technique that extends dropout by randomly dropping entire input connections to a neuron instead of individual neurons (Wan et al., 2013). This method helps to prevent overfitting by introducing more variability in the input to each neuron, which encourages the neuron to learn more robust and generalizable features.

In addition to these regularization techniques, the pipeline also employs early stopping, which is a form of regularization that terminates training when the validation loss does not improve within a specified number of epochs. This helps to prevent overfitting by stopping training before the model begins to memorize the training data.

It is worth noting that the pipeline does not mention the use of L2 regularization, which is another common regularization technique that adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. L2 regularization helps to prevent overfitting by encouraging the model to use smaller weights, which reduces the risk of overfitting by limiting the model's capacity to fit the training data.

Overall, the pipeline uses a combination of dropout, DropConnect, and early stopping to prevent overfitting and promote the development of robust and generalizable models.
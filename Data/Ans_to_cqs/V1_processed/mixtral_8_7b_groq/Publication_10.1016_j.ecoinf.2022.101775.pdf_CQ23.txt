The text provided does not give explicit information about the measures taken to ensure the generalizability of the deep learning model. However, it does mention some steps taken during the training and testing of the model that may indirectly contribute to its generalizability.

Firstly, the models were trained on a dataset from which the testing set had been manually removed. This allowed the model to learn from a large portion of the available data. Additionally, 10% of the data was set aside as a validation set during training to check for overfitting. This is a common practice in machine learning to ensure that the model is not just memorizing the training data but is able to generalize to unseen data.

Secondly, a min-max scaler was used to compute the performance metrics in the test set. This suggests that some form of data normalization was performed, which can help the model to handle data with different scales and distributions, thereby improving its ability to generalize.

Thirdly, the authors used a batch size of 64 samples and trained the algorithm for 100 epochs using Adam as the optimization algorithm. The choice of batch size and the number of epochs can affect the model's ability to generalize. A very large batch size may lead to overfitting, while a very small batch size may lead to underfitting. Similarly, training for too many epochs may lead to overfitting, while training for too few epochs may lead to underfitting.

Lastly, the authors used Pytorch to implement the algorithms and trained and tested the models in Google Colaboratory. The choice of deep learning framework and the computational environment can also affect the model's ability to generalize.

However, the text does not mention any explicit steps taken to ensure the generalizability of the model, such as using a diverse dataset, cross-validation, or stratified splitting. Therefore, it is not possible to provide a definitive answer to this query based on the provided context.
Based on the provided context, there is no explicit mention of specific preprocessing steps such as normalization, scaling, or cleaning that were applied to the data before training the deep learning model. However, it can be inferred that the authors used a dataset with a large number of labeled samples (nearly 30,000 per class) and considered this sufficient for training their own network.

The context does mention that the model used a Convolutional Neural Network (CNN) architecture, which often benefits from image data being preprocessed in certain ways. For instance, in computer vision tasks, it is common to normalize pixel values to a range of [0, 1] or to scale them to have zero mean and unit variance. These preprocessing steps can help improve the model's convergence during training and reduce the likelihood of certain issues, such as the vanishing gradient problem.

Additionally, the authors used the Rectified Linear Unit (ReLU) activation function for the convolutional layers and the Exponential Linear Unit (ELU) activation function for the fully connected layer. These activation functions help introduce non-linearity into the model, which is essential for learning complex patterns in the data.

In summary, while the text does not explicitly mention specific preprocessing steps, it can be inferred that some preprocessing was performed on the image data. Common preprocessing techniques for image data include normalization or scaling of pixel values. Additionally, the choice of activation functions (ReLU and ELU) can be seen as a part of the overall preprocessing pipeline.
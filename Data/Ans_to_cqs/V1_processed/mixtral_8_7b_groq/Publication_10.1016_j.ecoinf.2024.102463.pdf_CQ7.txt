The criteria used to split the data for deep learning model training, as described in the provided context, involves a process of standardization and optimization. 

During the training of the model, the data is standardized by calculating the standardization parameters from one of the three datasets, which is defined as a training base. These parameters are then applied to the other two datasets, considered as test bases. 

The best result of the optimization process determines the base that will be used for training. This is determined through a series of steps involving hyperparameter optimization and model training.

First, the HyperBand algorithm is proposed for optimization and is applied to each database to determine the best model, which includes both hyperparameters and parameters. This first treatment allows for the identification of the most suitable database to be the training base.

The HyperBand algorithm is a bandit-based approach for optimizing hyperparameters in a resource-efficient manner. It is an improvement over traditional methods such as grid search and random search, as it is able to allocate resources more efficiently by eliminating underperforming configurations early on in the training process.

In addition, the training process involves partially training models on a small fraction of the data to quickly eliminate underperforming configurations. The remaining configurations are then grouped into sets of different sizes, and the associated models are trained further. The best-performing configurations in each set are promoted to the next stage, while the underperforming configurations are eliminated. This process is repeated until only one configuration remains, which is then considered the best configuration found.

Therefore, the criteria used to split the data for deep learning model training involves a process of standardization, optimization, and hyperparameter tuning using the HyperBand algorithm. The most suitable database is identified as the training base, and the other two datasets are used as test bases. The training process involves partially training models on a small fraction of the data and progressively training the best-performing configurations while eliminating underperforming ones.
Based on the provided context, there are no explicit mentions of specific regularization methods such as dropout or L2 regularization to prevent overfitting in the deep learning pipeline. However, the text does mention some techniques that could indirectly help in reducing overfitting.

Firstly, data augmentation is used, which is a common technique to increase the amount of training data and reduce overfitting. Specifically, the paper mentions that they used random horizontal flips and random crops for data augmentation (Hernández-García, A. & König, P. 2018).

Secondly, the use of batch normalization could also help in reducing overfitting. Batch normalization is a technique that standardizes and normalizes the inputs of each batch, which can result in a regularization effect (Ioffe, S., & Szegedy, C. 2015).

Finally, the use of different models with varying architectures could also help in reducing overfitting. The paper mentions using ResNet-101, Wide-ResNet-101, InceptionV3, and MnasNet-A1. Using different architectures can help in reducing overfitting by providing different ways of learning the same data, which can improve the generalization of the model.

In summary, while the text does not explicitly mention dropout or L2 regularization as methods to prevent overfitting, it does mention the use of data augmentation, batch normalization, and varying model architectures, which could indirectly help in reducing overfitting.
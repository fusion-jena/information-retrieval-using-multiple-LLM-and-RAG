The process followed to deploy the trained deep learning model, as described in the given context, includes model serialization and platform selection. However, specific details about the deployment process are not provided. Therefore, I will answer the query based on common practices in deep learning model deployment.

To deploy a deep learning model, the trained model is first saved or serialized, making it possible to reuse the model for inference without the need for retraining. TensorFlow provides several options for saving models, such as saving the entire model (including the architecture and learned parameters) using the `save` method or saving only the weights using the `save_weights` method. Based on the context, it can be inferred that the entire model was saved since the network architecture was also a consideration during the development phase.

Once the model is serialized, the next step is platform selection. The context states that the CNNs were trained using the TensorFlow backend on a Tesla M100 GPU. However, this GPU might not be available or suitable for the intended deployment platform. TensorFlow models can be deployed on various platforms, including local machines, web applications, mobile devices, and cloud services.

For cloud-based deployment, TensorFlow provides TensorFlow Serving, a flexible, high-performance serving system for machine learning models, allowing for easy deployment of models on production servers. Additionally, TensorFlow offers TensorFlow Lite for deploying models on mobile and edge devices.

In summary, the deep learning model was likely serialized using TensorFlow's model saving methods, and the deployment platform was selected based on the specific requirements of the application. However, the context does not provide enough information to detail the exact steps taken for deployment.
Based on the provided context, there is no explicit mention of the optimization techniques used in the deep learning pipeline. The text discusses the use of machine learning algorithms for detecting ungulate vocalizations, but it does not specify the optimization techniques employed during the training of these models.

However, it does mention the usage of a deep learning method called Fast Fourier Transform (FFT) for analyzing the vocalizations. The authors chose a larger FFT window size to improve the resolution at lower frequencies where cows vocalize. This choice of FFT window size can be considered a hyperparameter tuning decision, which is a type of optimization technique.

In addition, the authors mention the use of cow-specific parameters in the model, which allowed them to detect a larger number of cow vocalizations compared to a previous study. This can also be seen as a form of optimization, where the model is adapted to the specific characteristics of the cow vocalizations.

In summary, while the context does not explicitly mention optimization techniques such as Stochastic Gradient Descent (SGD) or Adam, it does discuss hyperparameter tuning (FFT window size) and the use of cow-specific parameters, which can be considered optimization techniques in a broader sense.


Based on the provided context, the optimization techniques used in the deep learning pipeline include hyperparameter tuning (specifically, the choice of a larger Fast Fourier Transform window size) and the use of cow-specific parameters to adapt the model to the characteristics of cow vocalizations. However, there is no explicit mention of optimization techniques such as Stochastic Gradient Descent or Adam.
Based on the provided context, there is no explicit mention of which optimization techniques are applied in the deep learning pipeline. However, it does mention the potential use of acoustic indices in machine learning and deep learning.

Optimization techniques are crucial in deep learning as they help to minimize the loss function and improve the model's performance. Some commonly used optimization techniques include Stochastic Gradient Descent (SGD), Adagrad, Adadelta, RMSprop, and Adam.

Stochastic Gradient Descent (SGD) is a simple optimization algorithm that updates the model's parameters by taking a single sample from the training dataset at each iteration. Although SGD is computationally efficient, it may not always converge to the optimal solution due to its simplicity.

Adam is a more sophisticated optimization algorithm that combines the advantages of several optimization techniques, including momentum and adaptive learning rates. Adam adjusts the learning rate for each parameter based on the estimated first and second moments of the gradient. This makes it more efficient and robust than SGD in many cases.

The choice of optimization technique depends on several factors, including the size and complexity of the dataset, the model architecture, and the computational resources available. In some cases, a combination of optimization techniques may be used to improve the model's performance further.

Therefore, while the provided context does not explicitly mention which optimization techniques are used in the deep learning pipeline, it is reasonable to assume that one or more of the commonly used optimization techniques, such as SGD or Adam, are used. The choice of optimization technique would depend on various factors, including the specific requirements of the deep learning pipeline.
The generalizability of the deep learning model was ensured through several measures. First, to prevent overfitting and improve the model's ability to generalize, the data set was randomly split into two sets, with 80% used for model training and 20% for model testing. Early stopping, a regularization technique, was employed to halt the training process when the network began to overfit the data, i.e., when the error on the validation set increased.

Second, the selection of the number of layers, the number of neurons in the hidden layer, the learning rates, and the number of epochs for model training was carried out carefully. A trial-and-error method was used to determine the appropriate number of neurons and learning rate, and an early stopping technique was applied to prevent overfitting. Specifically, the number of nodes in the hidden layer was varied between 5 and 23, and the learning rate was varied from 0.01 to 1.0 in increments of 0.05. The mean square error (MSE) between the model output and the measured data was calculated for each configuration, and the number of neurons in the hidden layer and the optimal learning rate were selected based on the maximum model performance indicated by MSE.

Third, the final ANN structure had five input variables with one node accounting for bias, 19 hidden neurons with one node accounting for bias, a 0.7 learning rate, and one output variable of the output layer. This structure was determined based on the optimal number of neurons in the hidden layer and the optimal learning rate having the maximum model performance.

However, there is no mention of other measures such as diverse dataset, cross-validation, or stratified splitting to ensure the generalizability of the deep learning model.
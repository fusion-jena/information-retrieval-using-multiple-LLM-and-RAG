Based on the provided context, the criteria used to split the data for deep learning model training (e.g., train, test, validation) are not explicitly stated. However, it does mention the use of different types of data for various purposes that can be used as a reference for splitting the data.

In the context of deep learning for natural systems, a lack of training data is a significant challenge. One approach to addressing this limitation is "self-supervised learning," which uses unsupervised methods to generate training data from unlabeled data. This generated data can then be used to train supervised models.

Furthermore, the context also mentions the use of labeled data in a semi-supervised framework, which may improve deep learning on limited training data. This implies that a portion of the data is used for training the model in a supervised manner, while the remaining data is used for other purposes.

When comparing the contribution of high-quality annotations to a self-supervised model or a full model combining both self-supervision and hand-annotation data, the model is solely trained using the hand-annotated data. This suggests that the hand-annotated data is used as the training set, and the model's performance is evaluated using a separate test set.

Lastly, the context also mentions the need for better validation data to improve the model. Validation data is typically used to fine-tune the model's hyperparameters and prevent overfitting.

Therefore, while the exact criteria for splitting the data are not provided, we can infer that the data is split into training, validation, and test sets. The training set is used to train the model, the validation set is used to fine-tune the model's hyperparameters, and the test set is used to evaluate the model's performance. Additionally, the data may be split into labeled and unlabeled sets, where unlabeled data is used to generate training data for self-supervised learning.
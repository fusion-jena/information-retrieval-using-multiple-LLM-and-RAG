The text provided does not give a direct answer to the question about strategies employed to handle randomness in the deep learning pipeline. However, it does mention that a sensitivity study was performed, which involved running the same experiment five times to account for the random subsampling of annotations. This suggests that some form of randomness was introduced during the subsampling process, and the researchers took steps to account for it by repeating the experiment multiple times. Additionally, the text mentions the use of a pretraining model, which could potentially involve the use of a fixed random seed value during the pretraining phase to ensure reproducibility. However, these are just indirect inferences, and the text does not provide a clear and direct answer to the question.
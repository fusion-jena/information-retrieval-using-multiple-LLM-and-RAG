The strategy implemented to monitor the model performance during training involves generating 1000 random pseudo-absence points and splitting the collected data into training (80%) and testing (20%) datasets. This process is repeated three times for each model, and cross-validation is performed using bootstrapping. The performance and accuracy of each model are evaluated and compared using AUC (area under the receiver operating curve) and TSS (true skill statistic). The TSS value ranges from -1 to +1, with positive values indicating a perfect fit, while AUC values range from 0.5 to 1, with higher values indicating improved accuracy and predictive power. This strategy allows for a robust evaluation of the models' performance, helping to ensure that the models are accurately predicting the species distribution while avoiding sampling bias.
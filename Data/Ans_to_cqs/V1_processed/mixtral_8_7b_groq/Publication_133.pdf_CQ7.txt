In the context provided, the data for deep learning model training are split into three subsets: "training," "validation," and "test." This splitting strategy is a common practice in machine learning to ensure the model's performance and prevent overfitting.

During the training phase, the algorithm uses the "training" and "validation" sets to create a statistical model. The "training" set is used to train the model by adjusting the parameters based on the loss function's gradient. The "validation" set, on the other hand, is used to monitor the model's performance and limit the training duration. This helps to reduce the risk of overfitting by stopping the training when the validation loss stabilizes.

In the test phase, the model is applied to the "test" set images, ignoring their labels. Each predicted label is associated with a confidence score that considers both the probability that the label is correct and the expected overlap between the ground truth bounding box and the predicted one. This overlap is expressed as the Intersection over union (IoU), indicating the agreement between a predicted bounding box and a ground truth box.

The specific criteria used to split the data into these subsets are not provided in the context. However, it is mentioned that the data used in this study were collected in July 2020 and consist of 46 labeled images divided into the three subsets. The number of images in each subset is not specified.

In summary, the criteria used to split the data for deep learning model training include creating a "training" set for adjusting the model's parameters, a "validation" set for monitoring the model's performance and preventing overfitting, and a "test" set for evaluating the model's performance on new, unseen data. The specific criteria used to split the data in this study are not provided.
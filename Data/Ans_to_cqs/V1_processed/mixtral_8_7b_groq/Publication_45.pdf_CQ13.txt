The hyperparameters used in the deep learning model, as described in the given context, include network capacity, regularization techniques, the loss function, and the optimizer. 

Network capacity refers to the number of layers and neurons in the network. The specific numbers are not mentioned, but it was evaluated iteratively, and the determined architecture was used for Experiment 2.

Regularization techniques are methods used to prevent overfitting in a model by adding a penalty to the loss function. The context does not specify which regularization techniques were used.

The loss function is a measure of the difference between the predicted and actual values. The specific loss function used in this model is not mentioned.

The optimizer is a method used to update the weights and biases of the network based on the gradient of the loss function. The context mentions that stochastic gradient descent was used, but it does not specify if any other optimizers were evaluated.

The learning rate, which is another important hyperparameter that determines the step size at each iteration while moving toward a minimum of a loss function, is not explicitly mentioned in the context. However, it can be inferred that the learning rate was a part of the optimizer's configuration.

In summary, the hyperparameters used in the deep learning model include network capacity, regularization techniques, the loss function, the optimizer, and implicitly the learning rate. The specific values and techniques are not provided in the context.
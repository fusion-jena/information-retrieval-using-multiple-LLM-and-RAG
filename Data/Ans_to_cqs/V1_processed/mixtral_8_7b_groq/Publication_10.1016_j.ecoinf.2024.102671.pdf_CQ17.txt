The provided pieces of context do not directly discuss regularization methods used in deep learning pipelines, such as dropout or L2 regularization. These methods are typically used to prevent overfitting in machine learning models, including deep learning models, by adding a penalty to the loss function or by randomly dropping out neurons during training.

However, some of the sources do discuss methods for analyzing spatial patterns and ecological data, which may involve the use of regularization techniques in their statistical models. For example, Li et al. (2023) propose a new framework for identifying ecological patterns using a spatial weights matrix and a spatial lag model, which could potentially incorporate regularization methods to prevent overfitting. Similarly, Lever et al. (2017) discuss principal component analysis (PCA), a dimensionality reduction technique that can be used to identify patterns in high-dimensional data. While PCA itself does not involve regularization, it can be used as a preprocessing step for other models that do use regularization.

Overall, while the provided context does not directly answer the query about regularization methods in deep learning, it is worth noting that regularization techniques are widely used in statistical modeling and data analysis, and may be applied in various contexts depending on the specific research question and dataset.
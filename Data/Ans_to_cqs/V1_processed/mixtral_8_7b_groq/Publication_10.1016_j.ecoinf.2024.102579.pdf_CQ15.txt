Based on the provided context, the optimization techniques applied in the deep learning pipeline are not explicitly mentioned. The text focuses on the use of NN1, NN2, NN3, and NN4 estimators in the context of inventory estimation and diversity indices in forest management.

However, it is important to note that optimization techniques like Stochastic Gradient Descent (SGD) and Adam are commonly used in deep learning for training models. These optimization algorithms are used to minimize the loss function and find the optimal set of model parameters.

SGD is a simple optimization algorithm that updates the model parameters by taking a step in the direction of the negative gradient of the loss function. Adam, on the other hand, is an optimization algorithm that combines the ideas of momentum and adaptive learning rates. It maintains an exponentially decaying average of past gradients and squared gradients, which helps in adapting the learning rate for each parameter.

In summary, while the provided context does not mention any specific optimization techniques used in the deep learning pipeline, it is common practice to use optimization algorithms like SGD and Adam for training deep learning models.
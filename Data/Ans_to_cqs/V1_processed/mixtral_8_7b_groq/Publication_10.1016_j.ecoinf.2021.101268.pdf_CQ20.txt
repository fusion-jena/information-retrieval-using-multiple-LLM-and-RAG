The provided context does not give specific information about the hardware resources used for training the deep learning models. It mainly focuses on the comparison of different model architectures and their performance in terms of accuracy.

However, it is common practice to use Graphics Processing Units (GPUs) for training deep learning models due to their ability to perform parallel processing, which significantly speeds up the training process compared to using Central Processing Units (CPUs). Some deep learning frameworks like TensorFlow and PyTorch support the use of GPUs for training.

Moreover, there are specialized hardware resources like Tensor Processing Units (TPUs) developed by Google, which are specifically designed to accelerate machine learning tasks, including deep learning. However, the context does not mention anything about using TPUs for training the models.

Therefore, based on the provided context, it can be inferred that GPUs are likely used for training the deep learning models, but there is no explicit information about the hardware resources used.
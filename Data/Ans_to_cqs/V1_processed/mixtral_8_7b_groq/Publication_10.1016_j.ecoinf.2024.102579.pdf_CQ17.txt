The provided context does not contain information about regularization methods used in deep learning pipelines, such as dropout or L2 regularization. The context focuses on the use of sample-based estimators in continuous cover forestry and ecological informatics.

However, I will provide a brief explanation of the mentioned regularization techniques for completeness:

1. Dropout: Dropout is a regularization technique used in deep learning models, particularly in neural networks. During training, dropout randomly sets a fraction of the hidden units to zero, preventing the network from relying too much on specific connections. This helps to reduce overfitting and improves the model's generalization performance.
2. L2 regularization: L2 regularization, also known as weight decay, is a technique that adds a penalty term to the loss function, proportional to the square of the model's weights. By adding this penalty, the optimization process is encouraged to keep the weights small, reducing the risk of overfitting.


The provided context does not contain information about regularization methods used in deep learning pipelines. Regularization techniques like dropout and L2 regularization are used to prevent overfitting in deep learning models. Dropout randomly sets a fraction of hidden units to zero during training, while L2 regularization adds a penalty term to the loss function, encouraging small weights and reducing overfitting risk.
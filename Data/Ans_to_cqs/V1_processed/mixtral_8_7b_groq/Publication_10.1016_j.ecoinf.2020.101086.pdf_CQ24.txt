The provided context does not directly discuss strategies employed to handle randomness in the deep learning pipeline. However, it does discuss strategies used to handle randomness in the context of species distribution modeling, which can be informative by analogy.

In species distribution modeling, randomness can be handled through the use of a species target group, which involves using different sets for presence sites (P) and background sites (S) or using the same set for both if only the target species is used to estimate sampling effort. The relative sampling effort is then computed for each pixel of the study area, which corresponds to the sampling bias within the environmental space. This resulting map is used to bias the random selection of background points, with a higher sampling effort of a cell leading to a greater likelihood of selecting a background site in it.

To further handle randomness, the model input data can be transformed using four approaches that consider either the presence sites or background sites and are implemented in either the geographical or environmental space. When the presence dataset is considered, the distribution is modified to tend toward a uniform distribution, and the background set is obtained via a uniform random selection. When the background set is considered, it is built according to a selection bias that reflects the same sampling bias.

While these strategies are specific to species distribution modeling, they can provide insight into handling randomness in deep learning pipelines. For example, using different sets for training and testing data or using the same set for both can help handle randomness. Additionally, techniques such as data augmentation, dropout, and batch normalization can be used to introduce randomness during the training process and prevent overfitting.

In summary, while the provided context does not directly discuss strategies employed to handle randomness in the deep learning pipeline, it does provide insight into handling randomness in species distribution modeling. Strategies such as using different sets for presence and background sites or modifying the distribution of the presence dataset can be informative by analogy. Additionally, techniques such as data augmentation, dropout, and batch normalization can be used to handle randomness in deep learning pipelines.
Based on the provided context, there are several postprocessing steps that were involved after the model training, although not all of the specific techniques you mentioned (Saliency maps) were explicitly mentioned.

After training the models using 80% of the data, the remaining 20% was used as the evaluation set to assess model performance. Two metrics were used to evaluate the accuracy of the models: the area under the curve (AUC) of the receiver operating characteristics (ROC) and the true skill statistics (TSS). These metrics are commonly used in machine learning to assess the performance of binary classification models.

Additionally, a confusion matrix was likely used to calculate the TSS score. A confusion matrix is a table that is often used to describe the performance of a classification model. It compares the predicted classifications to the actual classifications and can be used to calculate various performance metrics, such as precision, recall, and the F1 score.

To ensure the robustness of the results, committee averaging and weighted mean approaches were applied to create separate ensemble models for each climate scenario and period. Ensemble models are created by combining the predictions of multiple models to improve the overall performance and reduce the risk of overfitting.

When selecting the final ensemble models, special attention was given to those with a TSS score of â‰¥0.7. This indicates that the models with a TSS score of at least 0.7 were considered to be the most accurate and reliable.

In summary, the postprocessing steps involved after the model training included the calculation of the AUC of the ROC and TSS metrics, the creation of ensemble models using committee averaging and weighted mean approaches, and the selection of the final ensemble models based on their TSS score. A confusion matrix was likely used to calculate the TSS score. However, there is no mention of saliency maps in the provided context.
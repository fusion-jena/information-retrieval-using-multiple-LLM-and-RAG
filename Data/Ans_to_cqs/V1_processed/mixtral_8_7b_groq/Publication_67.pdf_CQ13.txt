The hyperparameters used in the deep learning model, as described in the given context, include the learning rate, decay factor, optimization algorithm, and momentum. The model is a Convolutional Neural Network (CNN) based on Inception v.3 architecture.

The learning rate, set to 0.001, controls the step size during gradient descent. A decay factor of 16 every 30 epochs is applied, meaning that the learning rate is reduced by a factor of 16 after every 30 iterations through the entire training dataset. This technique helps fine-tune the model during later stages of training, preventing overshooting of the optimal solution.

The optimization algorithm used is RMSProp, which stands for Root Mean Square Propagation. It is a type of gradient descent algorithm that maintains an adaptive learning rate for each parameter. RMSProp uses a moving average of the squared gradient to scale the learning rate, allowing faster convergence in complex models like deep neural networks.

An additional hyperparameter for RMSProp is the momentum, set to 0.9. Momentum is a technique used to improve convergence in gradient descent by incorporating the past gradients' direction in the current update. It reduces oscillations during training and helps the model navigate through shallow regions of the loss function.

The epsilon value, another RMSProp hyperparameter, is set to 0.1. Epsilon is a small constant added to the denominator in the RMSProp update rule to avoid division by zero and numerical instability.

It is important to note that the authors used pre-trained models and fine-tuned them using their dataset. This approach, known as transfer learning, leverages the knowledge gained from large-scale annotated datasets and adapts it to a specific task, reducing the need for a large dataset and computational resources.

In summary, the hyperparameters used in the deep learning model are:

1. Learning rate: 0.001
2. Decay factor: 16 every 30 epochs
3. Optimization algorithm: RMSProp
4. Momentum: 0.9
5. Epsilon: 0.1

These hyperparameters were used to train the Inception v.3 model on a tree cover classification task.
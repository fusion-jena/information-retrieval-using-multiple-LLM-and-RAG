The criteria used to determine when training is complete in the study is the convergence of the learning curve and the point at which loss values no longer decrease. Additionally, the validation accuracy exceeding the training accuracy is also used as an indicator of a well-trained model. This is a common effect when dropout and data augmentation are applied only to the training data, resulting in a more challenging training set compared to the validation set.

In this study, the learning process was stopped after 90 epochs, where the learning curve had converged and loss values were no longer decreasing. The validation set was used to determine the best model, which reached a maximum accuracy of 96.4% (loss = 0.13). The performance of the final model was then evaluated on a separate test dataset containing 5796 individual tiles, which had not been used during the process of model selection. On these individual tiles, the model reached an accuracy of 96.7% (loss = 0.08).

It is worth noting that to prevent spatial autocorrelation between neighboring tiles, the tiles were collected in one folder, shuffled, and then split into training, validation, and test sets. Data augmentation was performed on the training set to artificially increase the number of training samples by applying slight transformations on the original data.

In summary, the study determined that training was complete based on the convergence of the learning curve, the point at which loss values no longer decreased, and the validation accuracy exceeding the training accuracy. These criteria were used to ensure that the model was well-trained and able to generalize to new, unseen data.
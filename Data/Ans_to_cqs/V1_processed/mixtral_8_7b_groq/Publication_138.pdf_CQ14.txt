The text does not provide explicit information about how the hyperparameters of the model are optimized. However, it does mention some hyperparameters and relevant concepts that can give us an idea of how the optimization might be approached.

Firstly, the text mentions the use of a Min-batch size of 128 for achieving high accuracy in the model. This suggests that the authors have experimented with different Min-batch sizes and found 128 to be the optimal value.

Secondly, the text mentions the use of six different algorithms, namely ICSNet, ICSNet_n, LeNet, AlexNet, VGG-19, GoogLeNet, ResNet-18, ResNet-50, and ResNet152. These algorithms may have different hyperparameters that have been optimized for the given task.

Thirdly, the text mentions the use of a pooling operator and LRN (Local Response Normalization) for extracting the main features of a certain area, reducing the number of parameters, and preventing the model from overfitting. The use of these techniques suggests that the authors have considered the impact of model complexity on the performance of the model.

Lastly, the text mentions the concept of epochs, which is the number of times the learning algorithm will work through the entire training dataset. The number of epochs is also a hyperparameter that can be optimized for the given task.

Therefore, while the text does not explicitly mention the use of grid search or random search for hyperparameter optimization, it does suggest that the authors have experimented with different hyperparameters to achieve the best possible performance of the model.
The provided context does not contain information about deep learning models or their data splitting criteria. However, it does discuss the methodology for an ensemble framework using five statistical models: generalized linear models (GLM), maximum entropy (Max-Ent), random forest (RF), generalized boosting model (GBM), and multivariate adaptive regression lines (MARS). For these models, a 3:1 train-test split was employed for the occurrence points. Additionally, 1000 random pseudo-absence points were generated, which are crucial for model execution and provide a reference for areas unlikely to be inhabited by the target species.

In the context of machine learning, including deep learning, data splitting is essential for evaluating model performance and preventing overfitting. Common splitting strategies include train-test splits, cross-validation, and bootstrapping.

1. Train-test split: Similar to the 3:1 split mentioned in the context, the data is divided into two subsets, where a larger portion is used for training and a smaller portion for testing.

2. Cross-validation: This technique involves dividing the data into 'k' subsets or folds. The model is trained on 'k-1' folds, and the remaining fold is used for testing. This process is repeated 'k' times, with each fold serving as the test set once. The average performance across all iterations is then calculated.

3. Bootstrapping: This method involves randomly sampling the data with replacement to create multiple training sets. Each training set is used to train a model, and the average performance across all models is calculated.

In summary, the provided context discusses data splitting for statistical models but not for deep learning models. Common data splitting strategies for machine learning models include train-test splits, cross-validation, and bootstrapping.
The deep learning pipeline in this study uses video data as its primary input format. The videos are first processed into individual frames at a rate of one frame per second. These frames are then used for training and testing the deep learning model.

To prepare the frames for training, they are annotated with bounding boxes around the sharks and 19 non-shark species present in the frames. This results in a total of 26,947 fish annotations. The deep learning models use NASnet architecture with a Faster-rcnn backbone, implemented in Tensorflow2.

The images are resized to 1333 Ã— 800 pixels to match the pre-training data used to prepare the first layers of the model. The training data includes 8 shark species and 19 non-shark species, but some shark species that were infrequently captured in the clips were excluded from the testing phase.

Therefore, the data formats used in this deep learning pipeline include video data, which is processed into individual frames, and annotated frames in the format of images. The models are implemented and trained using Tensorflow2, which is a machine learning framework for building and training models. No other data formats such as audio or CSV are mentioned in the provided context.
The provided context discusses various studies using deep learning models for plant species and disease classification. However, it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. Nonetheless, I can provide a general overview of some common regularization techniques.

Regularization is a crucial approach to prevent overfitting in deep learning models. It helps to reduce the complexity of models and improve their generalization ability. Here are two common regularization methods:

1. L2 Regularization: This method adds a penalty term to the loss function, which is the sum of squares of all model weights multiplied by a regularization factor (also called lambda or weight decay). This penalty term discourages large weight values, making the model simpler and less prone to overfitting.

2. Dropout: Dropout is a regularization technique that randomly sets a fraction of neurons in a layer to zero during training. This prevents the co-adaptation of neurons and encourages the learning of more robust features. As a result, the model becomes more resistant to overfitting and can generalize better.

These regularization techniques can be applied to deep learning models for plant species and disease classification tasks. By incorporating L2 regularization or dropout, researchers can improve the performance of their models and ensure that they can handle new, unseen data effectively.

In summary, the provided context does not mention specific regularization methods used in the studies. However, L2 regularization and dropout are two common techniques used to prevent overfitting in deep learning pipelines. Researchers working on plant species and disease classification tasks can consider implementing these methods to improve their models' generalization ability.
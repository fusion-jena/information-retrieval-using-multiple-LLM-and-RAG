Based on the provided context, there is no explicit information given about how the hyperparameters of the proposed model were optimized. Hyperparameter optimization is an essential step in building machine learning models, and it can be done through various methods such as grid search, random search, Bayesian optimization, etc. However, the context does not mention any of these techniques being used.

The context does provide information about the architecture of the proposed model (Table 3. CNN Architecture), which includes details about the number of layers, output shapes, parameters, dropout rate, and input shape. It also mentions that data augmentation was used, which is a technique to increase the amount of training data by applying random transformations to the existing data.

Furthermore, the context provides a comparison of the proposed model with other pre-trained models such as VGG16, Xception, ResNet50, and DenseNet121 in terms of training run-time and prediction run-time. However, it does not mention any hyperparameter tuning for these pre-trained models.

Therefore, based on the given context, it is not possible to answer the query regarding how the hyperparameters of the proposed model were optimized.
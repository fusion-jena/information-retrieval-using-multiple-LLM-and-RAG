The provided context does not give specific information about the criteria used to determine when training is complete in the context of machine learning or deep learning. The text focuses on various aspects of model building, such as the use of marginal posterior distributions, prior distributions, and the assessment of convergence using the Gelman-Rubin diagnostic.

However, it is common practice in machine learning and deep learning to use validation loss plateau as a criterion for determining when training is complete. This approach involves monitoring the performance of the model on a validation dataset during the training process. When the validation loss stops decreasing or starts increasing, even if the training loss continues to decrease, it is an indication that the model is starting to overfit to the training data. At this point, the training process can be stopped to prevent overfitting.

Another criterion that can be used to determine when training is complete is early stopping. This approach involves setting a maximum number of training epochs and monitoring the performance of the model on the validation dataset after each epoch. If the validation loss does not improve for a certain number of epochs, the training process can be stopped.

It is important to note that the choice of criterion for determining when training is complete depends on the specific problem and dataset being used. In some cases, other criteria may be more appropriate.

In summary, while the provided context does not give specific information about the criteria used to determine when training is complete, it is common practice in machine learning and deep learning to use validation loss plateau or early stopping as criteria for determining when training is complete. The choice of criterion depends on the specific problem and dataset being used.
The text provided does not go into specifics about the hardware resources used for training deep learning models. However, it is mentioned that deep learning algorithms require large-scale data for training, which implies the need for high-performance computational hardware. In general, deep learning models are known to be computationally intensive and require specialized hardware to reduce training times.

Graphical Processing Units (GPUs) are commonly used for training deep learning models due to their ability to perform parallel computations. GPUs have been shown to significantly reduce deep learning training times compared to Central Processing Units (CPUs). Tensor Processing Units (TPUs) are custom-built Application-Specific Integrated Circuits (ASICs) developed by Google specifically for machine learning tasks. TPUs are designed to accelerate tensor operations, which are fundamental to deep learning computations.

In summary, while the provided text does not explicitly mention the use of GPUs, TPUs, or other hardware resources for training deep learning models, it is implied that high-performance computational hardware is necessary for training deep learning models due to their computational intensity and large-scale data requirements.
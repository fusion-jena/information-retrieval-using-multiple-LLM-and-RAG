The provided context does not contain information about regularization methods used in deep learning to prevent overfitting, such as dropout or L2 regularization. The context focuses on cross-validation methods for assessing the accuracy of thematic maps produced by statistical prediction methods, particularly in situations where data availability is limited.

The text mentions various cross-validation techniques, such as leave-one-out cross-validation (LOOCV) and blocked spatial cross-validation. LOOCV is known to provide nearly unbiased estimation when the full sample dataset is acquired by simple random sampling. However, for mapping environmental variables, sample datasets are usually amalgamations of several datasets with their own sampling designs, making design-based inference challenging.

The paper proposes two alternative cross-validation approaches: an inverse sampling-intensity weighted method and a geostatistical model-based method. These methods aim to address the spatial clustering problem in cross-validation, which tends to produce pessimistically biased map accuracy estimates.

While these cross-validation techniques are essential for evaluating map accuracy, they do not directly address the issue of overfitting in deep learning models. Regularization methods like dropout, L2 regularization, and early stopping are commonly used to prevent overfitting in deep learning pipelines. Dropout randomly sets a fraction of input units to zero during training, helping to prevent over-reliance on specific neurons and reducing overfitting. L2 regularization adds a penalty term to the loss function, which discourages large weights and promotes a more generalized model. Early stopping halts training when the model's performance on a validation set stops improving, preventing the model from learning idiosyncrasies in the training data that do not generalize well.

In summary, the provided context discusses cross-validation methods for assessing the accuracy of thematic maps, mainly in scenarios with limited data availability. However, it does not cover regularization techniques for preventing overfitting in deep learning models, such as dropout, L2 regularization, or early stopping.
The hyperparameters of the models used in the study, namely MLP (Multilayer Perceptron) and RF (Random Forest), are optimized using the grid search technique. This method involves systematically testing different combinations of hyperparameters to determine the set of values that result in the best model performance.

In the case of the MLP model, the following hyperparameters were tested: solvers (L-BFGS, Adam, SGD with constant learning rate, SGD with adaptive learning rate), number of neurons in the hidden layers (from 1 to 100), regularization factor "alpha" (L2 penalty) set to 10, activation functions (identity, logistic, tanh, ReLU), and learning rate (constant, invscaling, adaptive). The best model was selected based on the performance scores of all possible combinations of hyperparameters. The configuration that optimized the performance of the MLP model included the Adam solver, 10 neurons per hidden layer, alpha = 10, and constant learning rate, with the ReLU activation function.

Similarly, for the RF model, the hyperparameters tested included the number of decision trees (from 100 to 1000 in steps of 100), the number of features to consider at every split (max features) set to auto, sqrt, log2, or None, and the maximum number of levels in the decision tree set to None or from 10 to 100 in steps of 10. The best set of hyperparameters was determined by comparing the performance of all possible combinations of hyperparameters.

In summary, the hyperparameters of both the MLP and RF models were optimized using the grid search technique, which involved systematically testing different combinations of hyperparameters to determine the set of values that resulted in the best model performance.
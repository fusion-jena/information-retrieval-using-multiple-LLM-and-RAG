The text provided does not give a detailed account of the measures taken to ensure the generalizability of the deep learning model. However, it does mention some steps that contribute to the model's ability to generalize.

Firstly, the authors used a pre-training technique where the network is first trained on a big available dataset (dataset A) and then fine-tuned on the scarcer domain-specific data (dataset B). This method leverages the knowledge gained from the broader dataset A to improve the learning of the more specific dataset B.

Secondly, the authors used a data augmentation technique during the training of the model. Specifically, they used a simple crop and resize method with the default settings of Caffe. This technique can help increase the diversity of the training data, thereby improving the model's ability to generalize.

Thirdly, the authors conducted experiments on herbarium specimens classification, cross-herbaria transfer learning, and transfer learning from herbarium to non-dried plant images. These experiments can be seen as a form of cross-validation, where the model's performance is tested on different datasets and conditions. However, the text does not explicitly mention whether the datasets were split in a stratified manner to ensure representativeness in each subset.

In summary, while the text does not provide a detailed explanation of the measures taken to ensure the generalizability of the deep learning model, it does mention some techniques that contribute to it, such as pre-training, data augmentation, and cross-validation through experiments on different datasets.
The strategy implemented to monitor the model performance during training involves using two different test datasets and comparing the model evaluation metrics directly. The metrics used for comparison include AUC (Area Under the Curve), Omission, TPR (True Positive Rate), k (Kappa), and TSS (True Skill Statistic).

The first test dataset is created by randomly selecting 25% of the data, and the second test dataset consists of Irish records only. The model performance is evaluated for each of these test datasets separately.

The best model for each metric is highlighted, and the threshold used for comparison is the 10th percentile training presence. This threshold is used because the two test datasets are different, and using a single threshold allows for a direct comparison of the model evaluation metrics.

The model evaluation metrics are calculated for five different models, namely, Random background, Filtered, Targeted, Filtered and Targeted, and Weighted. These models are applied to the Muntjac range in Great Britain and Ireland Isles.

The TSSmax (True Skill Statistic maximum) is used as the single most appropriate threshold for model evaluation. However, in this case, the TSSmax is not used as the threshold for comparison due to the use of two different test datasets.

Therefore, the strategy implemented to monitor the model performance during training involves using two different test datasets and comparing the model evaluation metrics using the 10th percentile training presence threshold. This approach allows for a direct comparison of the model performance and ensures that the threshold used is consistent across both test datasets.
In the context provided, the criteria used to split the data for deep learning model training involves dividing the preprocessed data into three sets: training, validation, and test sets. The training set consists of 80% of the data, the validation set contains 10%, and the test set also contains 10%. This particular split ensures a more extensive training set, potentially leading to a better-generalized model.

The training set is used for learning the model's parameters. The validation set, on the other hand, is employed for tuning the model's hyperparameters. It helps in assessing the model's performance and guiding the selection of the best set of hyperparameters. The test set, which the model has not previously seen during the training or validation phases, indicates the model's performance in real-world scenarios.

Moreover, the use of GridSearchCV with cross-validation provides a reliable estimate of the model's performance. In this case, a tenfold cross-validation is used, which balances the trade-off between bias and variance and guards against overfitting. This process ensures that the eventual choice of hyperparameters is not arbitrary but is backed by empirical evidence.

It's important to note that while the 80-10-10 split is used in the provided context, some studies might use different proportions, such as a 70-30 split between training and testing/validation. The choice of split can depend on the specific requirements of the study and the amount of data available.
In the deep learning pipeline, the optimization techniques that are commonly used include Stochastic Gradient Descent (SGD), as well as adaptive methods such as Adagrad, Adadelta, and Adam.

Stochastic Gradient Descent (SGD) is a popular optimization algorithm used for training deep learning models. It works by iteratively updating the model parameters in the direction of the negative gradient of the loss function. In the context provided, the weights of the model are updated using SGD as follows: wt+1 = µwt − α∆J(wt), where µ is the momentum weight for the current weights wt, α is the learning rate, and ∆J(wt) is the gradient of the loss function with respect to the weights at iteration t.

Adaptive methods, on the other hand, modify the learning rate for each parameter during training. These methods include Adagrad, Adadelta, and Adam, which are built on the foundation of SGD but offer improvements in convergence rate and computational efficiency.

Adagrad adapts the learning rate for each parameter based on the historical gradient information. It maintains a separate learning rate for each parameter and decreases it as the number of updates for that parameter increases. This is particularly useful for sparse data, where some parameters may receive very few updates.

Adadelta is an extension of Adagrad that addresses the diminishing learning rates problem. It uses a running average of squared historical gradients to scale the learning rate, which helps maintain a stable learning rate over time.

Adam, which stands for Adaptive Moment Estimation, is another popular optimization algorithm that combines the ideas from Adagrad and Adadelta. It estimates the first and second moments of the gradient and uses them to adapt the learning rate for each parameter. Adam has been shown to perform well in practice and is widely used in deep learning applications.

In summary, the optimization techniques used in deep learning pipelines include Stochastic Gradient Descent (SGD), as well as adaptive methods such as Adagrad, Adadelta, and Adam. These algorithms help minimize the loss function and optimize the model parameters during training.
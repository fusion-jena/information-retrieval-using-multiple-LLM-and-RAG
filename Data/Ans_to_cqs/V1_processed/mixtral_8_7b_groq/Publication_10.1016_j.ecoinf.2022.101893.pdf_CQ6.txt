Based on the provided context, there is no explicit information about the preprocessing steps taken before training the deep learning model. However, it is mentioned that the network is designed with four convolution kernel pooling layers to extract deep features from multi-view data of WT spectrum, HHT spectrum, and STFT spectrum. It can be inferred that some preprocessing might have been done on the raw data to convert them into these spectra.

In the context, the focus is more on the architecture of the CNN model, feature selection, and the comparison of the performance of different features with classical machine learning models (RF, SVM, MLP) and the proposed models.

In general, for training deep learning models, data preprocessing is an essential step. Common preprocessing steps include:

1. Data cleaning: Removing noise, outliers, or missing values from the dataset.
2. Data normalization/scaling: Transforming the data to a common scale or range to avoid any feature dominating the learning process. This can be achieved through various methods such as min-max scaling, z-score normalization, or decimal scaling.
3. Data augmentation: Creating new samples from the existing dataset to increase the size of the training set, reduce overfitting, and improve model generalization.
4. Data splitting: Dividing the dataset into training, validation, and test sets to evaluate the model's performance.

Without explicit information in the context, it is not possible to determine if these specific preprocessing steps were applied. Nonetheless, it is crucial to perform these steps to ensure the quality of the data and improve the performance of deep learning models.
The provided context does not explicitly mention the techniques used to address data bias during preprocessing of the deep learning pipeline. However, it does mention that the authors varied the choice of dataset type (color, gray scale, leaf segmented) and training-testing set distribution (train: 80%, test: 20%, etc.) to "remove another potential bias."

The authors varied the dataset type to test the model's adaptability in the absence of color information and its ability to learn higher level structural features. This suggests that they were aware of the potential bias that could be introduced by the lighting conditions and the method and apparatus of data collection.

The authors also varied the training-testing set distribution, which could help address bias by ensuring that the model is exposed to a diverse range of data during training. This technique is known as stratified sampling, where the data is divided into subsets or strata based on certain characteristics, and a sample is taken from each stratum.

However, the context does not mention other common techniques used to address data bias, such as oversampling, undersampling, or diverse data collection. Oversampling involves increasing the number of instances of the minority class in the dataset, while undersampling involves reducing the number of instances of the majority class. Diverse data collection involves collecting data from a variety of sources and ensuring that the dataset is representative of the population.

Therefore, while the authors do mention some techniques used to address data bias, they do not explicitly mention the techniques of oversampling, undersampling, or diverse data collection.
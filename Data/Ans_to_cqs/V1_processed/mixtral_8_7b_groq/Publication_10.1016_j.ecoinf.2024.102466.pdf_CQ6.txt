While the provided context does not contain specific details about the preprocessing steps taken before training the deep learning models for bird detection, I can provide some general preprocessing steps that are commonly involved in training deep learning models.

Before training a deep learning model, the input data often undergo several preprocessing steps. These steps can include data cleaning, normalization, scaling, and augmentation. Data cleaning involves removing or correcting any erroneous or missing data points in the dataset. Normalization is the process of scaling numeric data to a common range, often between 0 and 1, to prevent any particular feature from having a disproportionately large influence on the model's training. Scaling is similar to normalization, but it involves resizing data to have a specific range or distribution. Data augmentation is a technique used to increase the size of the training dataset by applying random transformations, such as rotation, flipping, or cropping, to the existing data.

In the context of the bird detection study, the authors might have applied some or all of these preprocessing steps to the raw camera trap images before training their deep learning models. For instance, they might have cleaned the dataset by removing any images that contained no birds or were otherwise unusable. They might have normalized or scaled the pixel values of the images to a common range to improve the model's training. Finally, they might have applied data augmentation techniques to increase the size of the training dataset and improve the model's ability to generalize to new, unseen images.

However, it's important to note that the specific preprocessing steps used in this study may have differed from those listed above. The authors do not provide enough detail in the given context to make a definitive statement about the preprocessing steps they took. Therefore, further investigation would be needed to determine the exact preprocessing methods used in this study.
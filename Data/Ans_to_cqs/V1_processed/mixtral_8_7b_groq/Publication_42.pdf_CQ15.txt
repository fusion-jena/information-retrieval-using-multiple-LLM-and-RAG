The deep learning pipeline in the study described in the provided context utilized two optimization techniques: Stochastic Gradient Descent (SGD) and Adam. These optimizers were used to train 64 different CNN architectures for species classification.

The first 32 architectures were trained using the Adam optimizer, while the remaining 32 were trained using SGD. The authors found that, compared to SGD, Adam was the superior optimizer for training all models. This conclusion was based on the average F1-score for all classes, which was used as a measure for a given architecture's performance.

In addition to comparing the performance of Adam and SGD, the authors also considered the number of learnable parameters in each architecture. They found that the architecture with the highest F1-score (92.75%) had 2,197,578 learnable parameters, which was the lowest among the top five architectures. This choice was made to avoid overfitting the neural network, as an architecture with many parameters and few training data increases the risk of overfitting.

The authors also noted that large deep learning networks, such as ResNetV50 and InceptionNetV3, with many parameters (more than 20,000,000) resulted in overfitting when trained on their dataset. These networks had high training accuracy but lower validation F1-scores.

In summary, the deep learning pipeline in this study utilized two optimization techniques: Stochastic Gradient Descent (SGD) and Adam. The authors found that, compared to SGD, Adam was the superior optimizer for training all models. They also considered the number of learnable parameters in each architecture, choosing the one with the lowest number of learnable parameters (2,197,578) to avoid overfitting. The authors noted that large deep learning networks with many parameters resulted in overfitting, indicating the need for more training data when using such networks.
The provided context discusses the use of deep convolutional neural networks (CNNs) for image classification tasks, specifically for classifying leaf images. However, it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline.

Based on general deep learning practices, common regularization methods used to prevent overfitting include dropout, L2 regularization, data augmentation, and early stopping.

Dropout is a technique where randomly selected neurons are ignored during training, preventing over-reliance on specific neurons and thus improving the model's ability to generalize.

L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages smaller weights, reducing overfitting.

Data augmentation involves artificially increasing the size of the training set by applying random transformations (such as rotation, scaling, or flipping) to the existing training examples. This helps the model learn invariance to these transformations and reduces overfitting.

Early stopping is a method where training is halted as soon as the model's performance on a validation set starts to degrade, preventing the model from further overfitting to the training data.

While the provided context does not mention these specific regularization techniques, it does discuss the use of pre-training and fusion methods during the validation phase. Pre-training involves initializing the model's weights with values learned from a different but related task, which can help improve generalization and reduce overfitting. Fusion methods, such as averaging or max voting, can also help improve generalization by combining the predictions of multiple models.

Therefore, while the provided context does not explicitly mention the regularization methods used to prevent overfitting, it is likely that some combination of dropout, L2 regularization, data augmentation, and early stopping was used during the training of the deep CNN models. Additionally, pre-training and fusion methods were used during the validation phase to further improve the models' ability to generalize.
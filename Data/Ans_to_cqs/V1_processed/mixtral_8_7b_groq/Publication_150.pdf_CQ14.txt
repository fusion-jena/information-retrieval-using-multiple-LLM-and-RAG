Based on the information provided, there is no explicit mention of how the hyperparameters of the models were optimized. The text does not specify whether a method such as grid search, random search, or Bayesian optimization was used to find the best hyperparameters.

However, we do know some details about the fixed hyperparameters used for all models, except for MansNet. These include the use of the SGD optimizer with an initial learning rate of 0.01, which was reduced by a factor of 10 after 30 epochs. The models were also trained for 150 epochs, and batch normalization was applied. Additionally, the text states that the models were trained using Nvidia Tesla K80 or P100 GPUs.

The specific hyperparameters chosen might have been determined through prior knowledge, empirical testing, or literature review. Nonetheless, the context does not provide enough information to accurately answer the query regarding the optimization method used for hyperparameter tuning.
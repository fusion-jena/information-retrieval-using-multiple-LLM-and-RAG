I don't have the information required to provide a direct link to the data repository of the deep learning pipeline used in the study. However, I can describe the data sources and the methods used for data preparation based on the provided context.

The study uses two main data sources: Amazon and Cerrado. The Amazon data consists of Sentinel-2 images from 2016 and 2017, and the Cerrado data includes images from 2017 and 2018. The data is divided into training, validation, and test sets for both databases. The training set for the Amazon database has 78,431 samples, while the validation set contains 39,697 samples. The test set for the Amazon database consists of 1,716,000 samples. For the Cerrado database, the number of samples varies depending on the time frame and the training setup.

The deep learning pipeline involves a neural network architecture with three convolutional layers, two max-pooling layers, two fully connected layers, and a softmax layer with two outputs for deforestation and no-deforestation classes. The filter and output sizes of each layer are specified in Table 3, but the table is not provided in the context.

The data preparation process includes creating balanced samples per class for both Amazon and Cerrado databases. The number of available defined and non-defined samples for each tile is given in the tables.

In summary, while I cannot provide a direct link to the data repository, the context offers detailed information on the data sources, data division, and deep learning pipeline used in the study.
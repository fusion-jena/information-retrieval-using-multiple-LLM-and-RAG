The optimization technique applied in the deep learning pipeline is the Adam optimizer, which was used to minimize the cost function during training. The Adam optimizer is an adaptive learning rate optimization algorithm proposed by Kingma and Ba in 2014. It is a popular optimization algorithm that combines the advantages of two other optimization algorithms: AdaGrad and RMSProp.

Adam uses the past gradients' statistics to adapt the learning rate for each parameter. It calculates the individual adaptive learning rate for each parameter by combining the estimates of the first and second moments of the gradients. The Adam optimizer has been shown to be efficient in many deep learning applications, and it is known for its fast convergence and low memory requirements.

In addition to using the Adam optimizer, a learning rate decay procedure was also implemented in the training pipeline. The learning rate decay procedure was proposed by Ganin et al. (2017) and was adopted to achieve better convergence during training. The learning rate decay procedure is defined by the equation:

μe=μ0(1+αpe)β

where μ0 is the initial learning rate, α and β are hyperparameters set to 10 and 0.75, respectively, and p is the current training epoch. The learning rate decay procedure reduces the learning rate as the number of epochs increases, allowing for a more fine-tuned optimization of the model.

Furthermore, the deep learning pipeline uses ResNet, a deep learning model introduced by He et al. (2016), to improve convergence issues during the training of very deep network architectures. ResNet uses residual learning blocks between layers of the network, allowing for better optimization and higher training accuracy. The residual learning blocks help to alleviate two main problems in training neural networks: the vanishing gradient and the degradation problem.

In summary, the deep learning pipeline uses the Adam optimizer with a learning rate decay procedure and ResNet to optimize the model during training. The Adam optimizer is an adaptive learning rate optimization algorithm that combines the advantages of AdaGrad and RMSProp. The learning rate decay procedure reduces the learning rate as the number of epochs increases, allowing for a more fine-tuned optimization of the model. ResNet uses residual learning blocks between layers of the network, allowing for better optimization and higher training accuracy.
The deep learning pipeline in the ecological informatics study utilized several optimization techniques, including Stochastic Gradient Descent (SGD), Root Mean Square Propagation (RMSProp), and Adam. These optimization algorithms were used in conjunction with varying learning rates and the application or non-application of a dropout layer to tune the model parameters of a convolutional neural network (CNN) for improved performance.

The study systematically varied the optimization algorithm, learning rate, and dropout layer to evaluate the performance of the CNN model. The optimization algorithms tested were SGD, RMSProp, and Adam, while the learning rates used were 0.01 and 0.001. Additionally, the dropout layer was either included or excluded in the model.

The results demonstrated that Model 1, which lacked a dropout layer, used the RMSprop optimizer, and had a learning rate of 0.001, exhibited the most favorable average performance outcome. This model achieved an accuracy rate of 89.43%, a precision of 91.38%, a recall of 95.52%, and an F1-Score of 0.816.

In summary, the deep learning pipeline in the ecological informatics study utilized the optimization algorithms SGD, RMSProp, and Adam, along with varying learning rates and the application or non-application of a dropout layer, to tune the model parameters of a CNN for improved performance. Model 1, which used the RMSprop optimizer, had a learning rate of 0.001, and lacked a dropout layer, exhibited the most favorable performance outcome.
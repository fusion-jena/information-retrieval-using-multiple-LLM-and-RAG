The research paper describes several measures taken to ensure the generalizability of the deep learning model used for classifying sound signals.

Firstly, the dataset was split into training and testing datasets in a 70/30 ratio. This is a common practice to ensure that the model can perform well on unseen data.

Secondly, for model selection and parameter optimization, 50% of the training data were held-out for validation. This is a form of cross-validation, where a portion of the training data is used to validate the model during the training process. This helps to prevent overfitting and ensures that the model can generalize well to new data.

Thirdly, the final classification model was trained using the ResNet-50 fc1000 activations of the full training dataset. After this, a 10-fold cross-validation was used to define the score-to-posterior-probability transformation function. This function provides a basis for estimating the posterior probability for new observations, which is crucial for making accurate predictions on new data.

Fourthly, the research community is continuously exploring approaches for extracting ecologically relevant information from these often-voluminous acoustic datasets. The paper mentions efforts to automate the analysis of these data, including the development of various ecological acoustic metrics, unsupervised machine learning and ordination techniques, and call detection and classification methods. These efforts are aimed at summarizing soundscapes using a small number of parameters and quantifying the rate of occurrence for specific signals. This suggests that the authors are aware of the importance of using diverse and representative datasets for training their model.

Lastly, the authors used a pre-trained deep CNN with an SVM classifier, which is a form of transfer learning. This approach has been shown to improve the generalizability of deep learning models by leveraging the knowledge gained from pre-training on a large dataset.

In summary, the authors have taken several measures to ensure the generalizability of their deep learning model, including dataset splitting, cross-validation, score-to-posterior-probability transformation, using diverse and representative datasets, and transfer learning. These measures provide evidence that the authors have taken care to ensure that their model can perform well on new and unseen data.
Regularization methods are techniques used in deep learning to prevent overfitting, which occurs when a model learns the training data too well, to the point where it performs poorly on unseen data. Two common regularization methods are dropout and L2 regularization.

Dropout is a technique where randomly selected neurons are ignored during training, meaning their weights are not updated. This helps to prevent overfitting by making the model less dependent on any individual neuron, and therefore more robust and capable of generalizing to new data. Dropout is typically applied to the fully connected layers in a deep learning model.

L2 regularization, also known as weight decay, is a technique that adds a penalty term to the loss function, which is the objective function that the model tries to minimize during training. The penalty term is proportional to the square of the magnitude of the weights, which encourages the model to keep the weights small. This helps to prevent overfitting by reducing the complexity of the model, as larger weights allow the model to fit the training data more closely.

Both dropout and L2 regularization are used to prevent overfitting in the deep learning pipeline. They can be used together or separately, depending on the specific problem and model architecture. It is important to tune the regularization parameters, such as the dropout rate and L2 regularization strength, to find the right balance between preventing overfitting and allowing the model to learn the underlying patterns in the data.

In the context of the provided pieces of text, regularization methods are not explicitly mentioned. However, the concept of preventing overfitting is indirectly discussed in the description of pooling layers, which offer downsampling functionality to reduce the number of learnable parameters and achieve translation invariance. This is a form of regularization, as it helps to prevent overfitting by reducing the complexity of the model. Additionally, the use of convolution with multiple kernels can be seen as a form of regularization, as it allows the model to learn multiple feature extractors and therefore be more robust to variations in the data.
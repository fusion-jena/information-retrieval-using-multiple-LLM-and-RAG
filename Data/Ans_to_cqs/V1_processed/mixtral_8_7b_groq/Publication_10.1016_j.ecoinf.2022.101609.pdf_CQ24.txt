Deep learning models often involve randomness during various stages of the training process, such as weight initialization, dropout, data shuffling, and batch sampling. Strategies to handle this randomness include setting a fixed random seed value and using techniques like cross-validation.

Setting a fixed random seed value ensures consistent and reproducible results by controlling the randomness in the model's initialization and data sampling. This is important for debugging, comparing different models, and tracking progress. For example, in TensorFlow, you can set the random seed value using the `tf.random.set_seed()` function.

Cross-validation is another strategy to handle randomness in deep learning. It involves dividing the dataset into multiple folds, training the model on different subsets, and evaluating the model's performance on the remaining portions. This technique helps reduce overfitting, provides a more robust estimate of the model's generalization ability, and allows for better comparison of different models.

In the context of the provided sources, the study by Si et al. (2020) evaluates and calibrates a low-cost particle sensor using machine learning methods, but it does not explicitly mention strategies for handling randomness in the deep learning pipeline. However, it is reasonable to assume that the authors used a fixed random seed value and cross-validation, given the best practices in deep learning research.

The Public Health England report (2019) focuses on improving outdoor air quality and public health, mentioning the use of deep learning for predicting PM10 pollutants and evaluation using deep learning. While it does not discuss strategies for handling randomness, it is likely that the researchers employed the aforementioned techniques to ensure reproducibility and robustness in their models.

In summary, handling randomness in the deep learning pipeline involves setting a fixed random seed value and using techniques like cross-validation. These strategies help ensure reproducibility, reduce overfitting, and provide a more accurate estimate of the model's generalization ability. While the provided sources do not explicitly mention these strategies, it is reasonable to assume that they were employed in the research.
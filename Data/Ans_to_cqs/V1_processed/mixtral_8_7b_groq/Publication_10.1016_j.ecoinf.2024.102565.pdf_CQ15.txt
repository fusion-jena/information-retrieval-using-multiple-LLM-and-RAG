Based on the provided context, there is no explicit mention of any optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD) or Adam. The context focuses more on the architecture and design of Neural Networks (NN) and Recursive Tree (RT) models for predicting Total Suspended Matter (TSM) concentration based on wind speed data.

However, it is important to note that optimization techniques are crucial in training deep learning models. These techniques help minimize the difference between predicted and actual values by adjusting the model's parameters. Common optimization techniques include Stochastic Gradient Descent (SGD), Adam, RMSprop, and Adagrad.

In the context, the number of neurons in the hidden layers is determined using a heuristic rule (H = 2 Ã— N + 1), where N is the number of input features. This rule helps define the structure of the Neural Network model but does not pertain to optimization techniques.

The models are implemented and assessed using MATLAB programming software, but the specific optimization techniques are not mentioned. Therefore, without additional information, it is not possible to provide a specific answer regarding the optimization techniques applied in the deep learning pipeline in this context.
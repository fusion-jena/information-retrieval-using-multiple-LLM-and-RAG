The specific information about the hardware resources used for training the deep learning model in the context provided is not mentioned. However, it is stated that the major enablers for the success of deep learning have been the availability of large amounts of data and the corresponding computational infrastructure required by the learning algorithms. This suggests that some form of high-performance computing hardware was used for training the deep learning model. Given that the paper was published in 2015, the specific hardware used could be GPUs (Graphics Processing Units), which were commonly used for deep learning training at that time. Today, other hardware accelerators such as TPUs (Tensor Processing Units) developed by Google are also used for deep learning training. Nonetheless, the exact hardware resources used for training the deep learning model in this context are not specified.
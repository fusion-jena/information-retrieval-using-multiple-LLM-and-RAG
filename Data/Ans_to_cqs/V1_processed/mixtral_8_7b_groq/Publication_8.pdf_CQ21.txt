After the model training, there are several postprocessing steps involved, such as the calculation of evaluation metrics, the creation of a confusion matrix, and the use of saliency maps in some cases. However, based on the provided context, we only have information about the evaluation metrics calculation and the creation of a confusion matrix.

Firstly, the model's performance is evaluated using an average precision metric. To calculate this metric, around 200 of the un-annotated scans were annotated by hand based on the predicted bounding boxes. These annotations were then compared with the predictions to evaluate the precision of the organ detection model on this dataset.

Secondly, a confusion matrix is created to further evaluate the model's performance. Although the context does not explicitly mention the creation of a confusion matrix, it is implied when discussing the calculation of evaluation metrics. A confusion matrix is a table that is often used to describe the performance of a classification model. It compares the actual values (ground truth) with the predicted values and shows the number of true positives, true negatives, false positives, and false negatives.

The context also provides information about the training process, such as the use of transfer learning and the initialisation of the model weights pre-trained on the ImageNet dataset. However, these steps are part of the training process and not postprocessing steps.

In summary, after the model training, the postprocessing steps involved are the calculation of evaluation metrics, such as average precision, and the creation of a confusion matrix. The context does not mention the use of saliency maps, but it is possible that they are used in other applications or models.
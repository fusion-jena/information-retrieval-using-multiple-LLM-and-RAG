The text provided does not give specific details about the model serialization or platform selection process that was followed to deploy the trained deep learning model. However, it does provide some context about the training process that can be inferred.

The models were implemented in PyTorch, which is a popular open-source machine learning library based on the Torch library. PyTorch provides a range of tools for saving and loading models, which suggests that model serialization was likely done using PyTorch's built-in functions. For example, the `torch.save()` function can be used to save the model's state dictionary to a file, which can then be loaded using the `torch.load()` function.

As for platform selection, the text mentions that the models were trained on an Nvidia GTX graphics processing unit with 4 GB memory. This suggests that the models were trained on a local machine, rather than in a cloud-based environment. However, it is possible that the trained models were deployed to a different platform for inference, such as a web application or mobile device.

Overall, while the text does not provide explicit details about the model serialization and platform selection process, it can be inferred that PyTorch's built-in functions were used for model serialization, and that the models were trained on a local Nvidia GPU.
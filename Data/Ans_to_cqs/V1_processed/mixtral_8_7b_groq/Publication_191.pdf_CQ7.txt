The text provided does not give specific details about the criteria used to split the data for training, testing, and validation of deep learning models. However, it does mention that they train their model using the Caffe framework and employ a step learning policy for parameter setting in training. It also mentions that they set the batch size to 50 and apply L2 weight decay with a penalty multiplier set to 5 × 10 −4.

In general, when training deep learning models, it is common to split the data into three sets: training, validation, and testing. The training set is used to train the model, the validation set is used to tune hyperparameters and select the best model, and the testing set is used to evaluate the final performance of the model.

The splitting of data can be done randomly or by using some specific criteria. A common approach is to split the data into 60% for training, 20% for validation, and 20% for testing. Another approach is to use k-fold cross-validation, where the data is split into k folds, and the model is trained and validated k times, each time with a different fold used as the validation set and the remaining k-1 folds used as the training set.

In the context of plant image classification, the PlantClef2015 dataset mentioned in the text has around 113,205 images, which is still far from matching the scale and variety of existing general major datasets. In such cases, data augmentation techniques can be used to increase the size of the dataset by creating new synthetic samples through various transformations such as rotation, scaling, and flipping.

Overall, while the text does not provide specific details about the criteria used to split the data for deep learning model training, it does give some insight into the parameter setting and training process used for plant image classification.
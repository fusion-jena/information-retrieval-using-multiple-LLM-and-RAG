Based on the provided context, there is no explicit mention of optimization techniques such as Stochastic Gradient Descent (SGD) or Adam applied in the deep learning pipeline. However, it does mention the use of transductive propagation and active learning in a semi-supervised learning setting.

Transductive propagation iteratively propagates labels from labeled instances to unlabelled instances based on the graph structure of the embeddings. This technique can be seen as a form of optimization, where the goal is to effectively use the limited labeled data to improve the predictions for the unlabelled data.

Additionally, active learning is used to select the most informative samples for labeling. In this case, the feature extraction model is fine-tuned on a subset of 500 training samples, and model performance is re-evaluated on the test set at 10 sample increments using three different sample orderings: low certainty, random, and high certainty. Low certainty refers to queries with a similar Euclidean distance to prototypical embeddings and therefore lie along decision boundaries. High-certainty samples have the query embedding in close proximity to prototypical embeddings and are prioritized for model fine-tuning. This approach can be seen as an optimization technique, where the goal is to select the most informative samples to improve the model's performance with limited labeled data.

In summary, while the context does not explicitly mention optimization techniques such as SGD or Adam, it does discuss the use of transductive propagation and active learning as optimization techniques in a semi-supervised learning setting.
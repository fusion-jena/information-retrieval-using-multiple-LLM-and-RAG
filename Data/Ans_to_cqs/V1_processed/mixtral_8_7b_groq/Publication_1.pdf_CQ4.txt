Based on the provided context, the specific data augmentation techniques used in the deep learning pipeline are not mentioned. However, it is mentioned that deep learning (DL) models are used for biodiversity analysis and employ trainable computational models composed of a stack of processing layers. Each layer learns a more abstract representation of the data based on the more elementary representation in the previous layer.

In general, data augmentation techniques are commonly used in deep learning pipelines to increase the amount of training data, reduce overfitting, and improve model performance. Some common data augmentation techniques include:

1. Flipping: This involves flipping the image horizontally or vertically. This can help the model learn invariance to the orientation of the objects in the image.
2. Rotation: This involves rotating the image by a certain angle. This can help the model learn invariance to the orientation of the objects in the image.
3. Scaling: This involves scaling the image by a certain factor. This can help the model learn invariance to the size of the objects in the image.
4. Translation: This involves translating the image by a certain number of pixels. This can help the model learn invariance to the position of the objects in the image.
5. Color jittering: This involves randomly changing the brightness, contrast, saturation, and hue of the image. This can help the model learn invariance to changes in lighting conditions.
6. Cropping: This involves randomly cropping a portion of the image. This can help the model learn to focus on the important parts of the image.
7. Flip, rotate, and scale: This involves applying a combination of flipping, rotating, and scaling to the image. This can help the model learn more complex invariances.

These techniques can be applied on-the-fly during training, or they can be applied to a separate dataset that is used for training. The specific techniques and parameters used can vary depending on the task and the dataset.
The text provided does not give a detailed description of the process followed to deploy the trained deep learning model. However, it does provide some context that can help us make an educated guess.

First, the model is trained using a deep learning framework, although the specific framework is not mentioned. The model is optimized by minimizing the loss function of Eq. (2) and the training procedure is described in supplementary information Box A.

Once the model is trained, it is used to predict a set of labels by thresholding the SDM (Species Distribution Model) output categorical probabilities associated with species. This is done to optimize a model that returns likely species assemblages worldwide while being learned on a set of presence-only observations.

The model is trained on presence-only data, all species combined (multi-species SDM). The input to the model includes a large environmental context around each observation, collected and provided to the model in the form of 64 Ã— 64 2D tensors sampled at the kilometre-scale resolution and centered on the observation. Predictors include WorldClim2 bioclimatic variables, Soilgrids pedological variables, human footprint rasters, terrestrial ecoregions of the world, and the observation location (longitude and latitude).

Therefore, it can be inferred that the model is likely to be serialized and deployed as a predictive model on a platform that can handle large 2D tensors and perform thresholding operations on the output categorical probabilities. The platform could be a cloud-based solution, a high-performance computing cluster, or a local machine with sufficient processing power and memory.

In summary, while the text does not provide a detailed description of the deployment process, it can be inferred that the trained deep learning model is likely to be serialized and deployed on a platform that can handle large 2D tensors and perform thresholding operations on the output categorical probabilities.
Regularization methods are techniques used in deep learning to prevent overfitting, which occurs when a model learns the training data too well, to the point where it performs poorly on unseen data. Two common regularization methods are dropout and L2 regularization.

Dropout is a technique introduced by Geoffrey Hinton and his students in 2012. It involves randomly dropping out, or setting to zero, a proportion of a layer's neurons during training. This helps to prevent overfitting by reducing the co-adaptation of neurons, which is when neurons become too dependent on each other. Dropout can be applied to any layer in a deep learning model, but it is most commonly applied to fully connected layers.

L2 regularization, also known as weight decay, is a technique that adds a penalty term to the loss function, which is the function that the model tries to minimize during training. The penalty term is proportional to the square of the magnitude of the model's weights. This encourages the model to learn smaller weights, which helps to prevent overfitting by reducing the complexity of the model. L2 regularization can be applied to any type of layer, including convolutional layers and fully connected layers.

In the context of the provided information, the specific regularization methods used are not mentioned. However, the paper "FastFCN: The Backbone for Semantic Segmentation" by Wu et al. (2019) [12] uses dilated convolutions, which can help to prevent overfitting by increasing the receptive field of the model without increasing the number of parameters. The paper "Mask R-CNN" by He et al. (2017) [13] uses a multi-task loss function, which can help to prevent overfitting by encouraging the model to learn features that are useful for multiple tasks. The paper "U-Net: Convolutional Networks for Biomedical Image Segmentation" by Ronneberger et al. (2015) [14] uses skip connections, which can help to prevent overfitting by allowing the model to learn both high-level and low-level features.

In addition, data augmentation is another technique that can help to prevent overfitting. Data augmentation involves creating new training examples by applying random transformations to the existing examples. This helps to increase the size and diversity of the training set, which can improve the model's ability to generalize to new examples. The paper "Effect of data-augmentation on fine-tuned CNN model performance" by Poojary et al. (2020) [27] studies the impact of data augmentation on fine-tuned CNN models.

In conclusion, dropout and L2 regularization are two common regularization methods used to prevent overfitting in deep learning. Other techniques such as dilated convolutions, multi-task loss functions, skip connections, and data augmentation can also help to prevent overfitting. The specific regularization methods used in the provided context are not mentioned, but the papers and tools mentioned use different techniques to prevent overfitting.
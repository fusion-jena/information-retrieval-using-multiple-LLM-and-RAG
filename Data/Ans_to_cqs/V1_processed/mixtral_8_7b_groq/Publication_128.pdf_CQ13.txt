The hyperparameters used in the deep learning models mentioned in the context include the learning rate, dropout rate, and batch size. The learning rate and dropout rate are both set to 0.001. This learning rate is used to update the model's weights during training, and the dropout rate is used to prevent overfitting by randomly dropping out a certain percentage of neurons during training. The batch size is set to 30, which means that the model processes 30 images at a time during training.

Additionally, the models use an optimized rectified linear activation function, Leaky ReLU, after the convolution or concatenate layer. This activation function helps to speed up the training process and partially solves the vanishing gradient and exploding problem.

The number of epochs used in training the models depends on the depth of the neural network. For models with less than 100 layers, 60 epochs are used, while models with more than 100 layers are trained for 80 epochs. An epoch is one complete pass through the entire training dataset.

The computational resources and environment used in this study include CentOS Linux release 7.7.1908 (Core) with 32 CPUs, each with 64 processors. The development environment is Python(version 3.6.2) packaged by Conda-forge, which is compiled with GCC (version4.8.2,20140120). The tool packages used in the virtual environment are TensorFlow (version 1.12.0) and Numpy (version 1.19.2).

Regarding the specific models, DenseNet has a scheme to reduce hyperparameters and guarantee gradient propagation, which aims to release gradient vanishing through feature reuse and bypass. However, DenseNet requires a long computational time. AlexNet, on the other hand, has more convolutional layers than LeNet for better high-dimensional feature extraction. It employs rectified linear activation (ReLU) as an activation function to speed up the training process.

Therefore, the hyperparameters used in the deep learning models include learning rate, dropout rate, batch size, number of epochs, and activation functions. The computational resources and environment also play a crucial role in training these models.
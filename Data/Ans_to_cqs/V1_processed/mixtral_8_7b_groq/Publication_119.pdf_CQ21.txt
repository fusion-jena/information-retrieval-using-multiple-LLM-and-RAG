After training the models, several postprocessing steps can be involved, such as generating saliency maps, calculating various metrics, and creating a confusion matrix. However, based on the provided context, it is specified that an error/confusion matrix is used for model evaluation to compare the accuracy of various models. But there is no mention of generating saliency maps or calculating other metrics like precision, recall, or F1-score.

In the context of the Support Vector Machine (SVM) and Deep Neural Network (DNN) models, it is mentioned that the models' hyperparameters were tuned to optimize the testing accuracy and kappa statistic. The DNN model's architecture consisted of a batch normalization layer after hidden layer 2 and dropout layers to reduce overfitting and increase generalization on testing data. The SVM model required tuning of two parameters, C and Î³, using GridSearchCV.

For the Random Forest (RF) model, the context describes the algorithm's construction of multiple decision trees, each predicting a class. Each tree within the RF is created using different training data subsets, which are selected through repetition and replacement of the original training dataset.

In addition, the context describes the use of a pre-trained U-Net model implemented through arcgis.learn, reducing the time and resources spent on ground truth data collection. However, there is no mention of any postprocessing steps for this model.

Therefore, based on the provided context, the postprocessing steps involved after model training include creating a confusion/error matrix for model evaluation. However, there is no explicit mention of generating saliency maps or calculating other metrics.
The strategy implemented to monitor the model performance during training involves using a 60-20-20 train/validate/test regime. This means that the data is split randomly into three sets: training, validation, and testing, with 60% of the data used for training, 20% for validation, and 20% for testing. This approach allows for the monitoring of the model's performance on unseen data during the training process.

Additionally, the training process does not employ techniques such as SMOTE, oversampling, or class weights. These techniques, which are often used to address issues of class imbalance, are reserved for a second phase of the research.

The performance of the model is evaluated using metrics such as accuracy and macro-F1 score. The macro-F1 score is a useful metric for imbalanced datasets, as it calculates the F1 score for each class and then takes the average. This ensures that the performance of the model is not skewed by the majority class.

The validation set is used to provide an unbiased evaluation of the model during the training process. By evaluating the model on the validation set after each epoch, it is possible to monitor the model's performance and identify any signs of overfitting. If the model's performance on the validation set begins to degrade, this can indicate that the model is overfitting to the training data.

Finally, the testing set is used to provide a final, unbiased evaluation of the model. This evaluation is performed after the training process is complete and allows for an assessment of the model's performance on data that it has never seen before.

In summary, the strategy implemented to monitor the model performance during training involves using a 60-20-20 train/validate/test regime, evaluating the model using metrics such as accuracy and macro-F1 score, and monitoring the model's performance on the validation set during the training process. Additionally, a final evaluation is performed on the testing set to provide an unbiased assessment of the model's performance.
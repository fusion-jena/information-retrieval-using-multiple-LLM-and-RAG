The hyperparameters of the models are optimized using a method called Grid Search Cross-Validation (GSCV). This method involves a systematic search for the best combination of hyperparameters within a specified range for each algorithm. 

For instance, in the study, the hyperparameters of seven different algorithms, including Random Forest, Multivariate Adaptive Regression Splines, Penalized Regression, Support Vector Machine, Gradient Boosting, Artificial Neural Network, and k-Nearest Neighbors, were optimized. 

The parameters of these algorithms were tuned through a grid search approach, focusing on minimizing the errors to achieve the best possible performance. The range of each tuned parameter was defined, and a series of models were generated for each algorithm, characterized by a different set of hyperparameters. 

The GSCV method systematically trains and assesses the performance of these various models through cross-validation. The process concludes with the selection of the model demonstrating the most superior performance, determined by its optimal hyperparameter combination. 

For example, for the Random Forest algorithm, the hyperparameters 'ntree' and 'mtry' were tuned, with their ranges set to 100-1000 and 2-35, respectively. The optimal values for these parameters in the three models (MODEL 1, MODEL 2, MODEL 3) are shown in Table 1. Similarly, the other hyperparameters of the remaining algorithms were optimized. 

Therefore, it can be concluded that the hyperparameters of the models were optimized using the Grid Search Cross-Validation method, which involved a systematic search for the best combination of hyperparameters within a specified range for each algorithm.
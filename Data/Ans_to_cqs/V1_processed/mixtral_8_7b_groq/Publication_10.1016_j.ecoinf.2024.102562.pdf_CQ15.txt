The provided context does not explicitly mention the specific optimization techniques used in the deep learning pipeline. However, it does mention that the training of the model was carried out for multiple epochs, which suggests that some form of optimization algorithm was used to update the model's weights during training.

Common optimization algorithms used in deep learning include Stochastic Gradient Descent (SGD), Adam, and RMSprop. SGD is a simple optimization algorithm that updates the model's weights by taking a step in the direction of the negative gradient of the loss function. Adam and RMSprop are more sophisticated optimization algorithms that adapt the learning rate during training based on the historical gradient information.

Without further information, it is difficult to determine which optimization algorithm was used in this particular deep learning pipeline. However, it is mentioned that the training was carried out for 500 epochs, which suggests that some form of optimization algorithm was used to update the model's weights during training.
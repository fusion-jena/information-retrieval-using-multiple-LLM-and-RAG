The criteria used to split the data for deep learning model training typically include a training set, a validation set, and a test set. However, the specific criteria or ratios for splitting the data are not mentioned in the provided context.

In deep learning, the training set is used to train the model by adjusting the model weights to minimize the error between the model output and the expected output. The validation set is used during the training process to evaluate the model's performance and adjust the model's hyperparameters, such as the learning rate or regularization strength. The test set is used after the training process to evaluate the final model's performance on unseen data.

The ratio of splitting the data into these three sets can vary depending on the size and complexity of the dataset. A common practice is to use 60-80% of the data for training, 10-20% for validation, and 10-20% for testing. However, in some cases, researchers may use a stratified sampling method to ensure that each set contains a representative sample of the data, especially when dealing with imbalanced classes.

In the context of fish segmentation models, the training set would consist of manually annotated pixel coordinates around the fish body boundary, with each image assigned a class label name corresponding to the general biological name of the organism. The model would then be trained using transfer learning with COCO pre-trained weights and a ResNet-101 convolutional neural network (CNN) to minimize the error between the model output and the expected output. The validation set would be used to evaluate the model's performance during the training process and adjust the hyperparameters accordingly. Finally, the test set would be used to evaluate the final model's performance on unseen data.

Overall, while the specific criteria for splitting the data for deep learning model training are not mentioned in the provided context, it is a crucial step in the model training procedure to ensure that the model generalizes well to novel input data.
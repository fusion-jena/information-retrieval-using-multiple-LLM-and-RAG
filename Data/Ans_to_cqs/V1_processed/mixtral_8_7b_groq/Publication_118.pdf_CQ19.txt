The deep learning model was built using PyTorch, a popular open-source machine learning library for Python. The architecture of the model is based on a heavily modified version of the one used in the "Unsupervised Deep Learning with Pytorch" project by github user "eelxpeng". The model is an autoencoder, which consists of an encoder and a decoder network. The encoder network takes a 128x128x3 input layer and uses 4 convolutional layers with 3 × 3 pixel kernels, stride of 1 pixels, and zero padding of 1 pixel on each layer except the first. The encoder network also includes 4 max-pooling layers using 4 × 4 pixel kernels, stride of 2 pixels, and zero padding of 1 pixel. The output of the encoder network is then flattened to produce a 384x1x1 feature vector. The decoder network accepts the 384x1x1 feature vector produced by the encoder network as its input and consists of 4 convolutional layers with a 3 × 3 kernel, stride of 1 pixel, and zero padding of 1 pixel on each layer except the last. The decoder network also includes 4 max-unpooling layers using 4 × 4 pixel kernels, stride of 2 pixels, and zero padding of 1 pixel, which produces a 128x128x3 output.

Additionally, the model also uses a version of the autoencoder based on implicit pooling, which consists of a 128x128x3 input layer, and 4 convolutional layers using 4 × 4 pixel kernels, stride of 2 pixels, and zero padding of 1 pixel on each layer except the first. The encoder’s output is a 384x1x1 feature vector. The decoder network accepts the 384x1x1 feature vector produced by the encoder network as its input, and consists of 4 convolutional layers with a 4 × 4 kernel, stride of 2 pixels, and zero padding of 1 pixel on each layer except the last, which produces a 128x128x3 output.

It's important to note that the architecture was modified to suit the specific needs of the project, which is ecoacoustic analysis. The authors chose a basic auto-encoder architecture to minimize complications that may be introduced by more advanced architectures. They used a rectified linear unit (ReLU) based activation function to help mitigate the vanishing/exploding gradient problem. Networks using implicit pooling and explicit max-pooling were used.
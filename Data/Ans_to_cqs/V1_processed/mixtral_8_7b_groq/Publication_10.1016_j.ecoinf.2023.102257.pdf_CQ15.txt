The provided context does not give specific details about the optimization techniques used in the deep learning pipeline for the ML model. The text mentions the use of convolutional neural networks (CNNs) and ensemble learning but does not specify the optimization techniques utilized.

Optimization techniques are crucial in training deep learning models, as they help minimize the error or loss function and improve the model's performance. Common optimization techniques include Stochastic Gradient Descent (SGD), Adagrad, Adadelta, RMSprop, and Adam. Among these, Adam is a popular choice due to its adaptive learning rate and computational efficiency.

Since the context does not explicitly mention any optimization techniques, it is possible that the authors did not include this information in their discussion. Nonetheless, it is essential to consider optimization techniques when implementing deep learning pipelines for tasks like object detection or image classification.
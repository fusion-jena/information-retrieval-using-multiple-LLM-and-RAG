Based on the provided context, the hyperparameters of the model were optimized through a process of manual tuning and annealing, rather than using a formal method such as grid search or random search.

The learning rate was tweaked to an optimal configuration of Î· = 0.0005 undergoing annealing by a factor of 100 using a cosine function over the duration of the training. This process of annealing the learning rate involves gradually reducing the learning rate over time, which can help the model to converge to a more optimal solution and avoid getting stuck in local minima.

In addition to annealing the learning rate, the authors also used stochastic gradient descent with the Adam optimizer and weight decay to optimize the model's parameters. The use of the Adam optimizer, which is an adaptive learning rate algorithm, can help to further improve the model's convergence properties.

Overall, while the context does not explicitly mention the use of grid search or random search for hyperparameter optimization, it does describe a systematic process of manual tuning and annealing that was used to optimize the model's performance.
The context provided discusses various aspects of deep learning models, including hyperparameter optimization, but it does not explicitly mention the criteria used to determine when training is complete. However, it is mentioned that hyperparameter optimization can improve model performance, and one of the hyperparameters that can be optimized is the learning rate.

In the context of training deep learning models, one common criterion used to determine when training is complete is when the validation loss plateaus or stops decreasing significantly. This is because the goal of training a deep learning model is to minimize the difference between the predicted values and the actual values, which is measured by the loss function. As the model trains, the loss function decreases, indicating that the model is learning to make better predictions.

However, at some point, the validation loss may plateau or stop decreasing, indicating that the model has learned as much as it can from the current data and hyperparameters. Continuing to train the model beyond this point may result in overfitting, where the model becomes too specialized to the training data and performs poorly on new, unseen data.

Therefore, monitoring the validation loss during training is an important criterion for determining when to stop training. Additionally, early stopping techniques can be used to automatically stop training when the validation loss plateaus or stops decreasing, preventing overfitting and saving computation time.

While the context provided does not explicitly mention this criterion, it is a standard practice in deep learning model training and is closely related to the hyperparameter optimization discussed in the context.
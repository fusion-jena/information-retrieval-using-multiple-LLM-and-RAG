The deep learning pipeline in the study utilizes several optimization techniques, including Adam, Stochastic Gradient Descent with Momentum (SGDM), and Root Mean Square Propagation (RMSProp). These optimizers are employed to adjust the solver for training the network and manage the input time series length.

Adam, specifically, is the primary optimizer used in this study, as it generally outperforms the other optimizers. When Adam is used as the solver, and the input time series length is set to 5 days, the RMSE of the validation data forecasting results is typically lower. This superior performance of Adam could be attributed to its ability to adaptively adjust the learning rate for each parameter, making it more effective in handling complex and nonlinear problems.

The other optimizers, SGDM and RMSProp, are also used in the deep learning pipeline. SGDM incorporates a momentum term to improve the convergence of the stochastic gradient descent, while RMSProp adjusts the learning rate based on the average of recent gradient magnitudes. However, these optimizers are not as effective as Adam in this particular study, as Adam consistently provides lower RMSE values for the validation data forecasting results.

In summary, the deep learning pipeline in this study primarily utilizes the Adam optimizer, with additional use of SGDM and RMSProp. The Adam optimizer is the preferred choice due to its superior performance in managing the complex and nonlinear interactions among the factors influencing algal blooms in the Zhoushan fishery.
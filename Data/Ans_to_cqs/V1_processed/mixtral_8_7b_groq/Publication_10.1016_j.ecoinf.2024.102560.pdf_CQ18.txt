Based on the provided context, there is no explicit information about the strategy implemented to monitor the model performance during training. However, some of the sources discuss model validation and evaluation techniques that can be used to monitor model performance.

For instance, Peng et al. (2017) used streamflow forecasting as an application and evaluated the performance of their model using the root mean square error (RMSE), the coefficient of determination (R2), and the Nash-Sutcliffe efficiency (NSE) coefficient. These metrics are commonly used to assess the accuracy and efficiency of a model.

Similarly, Kisi et al. (2019) used drought forecasting as an application and evaluated their model using the coefficient of determination (R2), the root mean square error (RMSE), and the mean absolute error (MAE). These metrics are used to assess the goodness of fit and the accuracy of the model.

Moreover, Periasamy and Ravi (2020) used a novel approach to quantify soil salinity and evaluated their model using the coefficient of determination (R2), the root mean square error (RMSE), and the mean absolute error (MAE). These metrics are used to assess the accuracy and reliability of the model.

In summary, while there is no explicit information about the strategy implemented to monitor the model performance during training, it can be inferred that the commonly used approach is to evaluate the model using metrics such as the coefficient of determination (R2), the root mean square error (RMSE), and the mean absolute error (MAE). These metrics are used to assess the accuracy, efficiency, and reliability of the model.
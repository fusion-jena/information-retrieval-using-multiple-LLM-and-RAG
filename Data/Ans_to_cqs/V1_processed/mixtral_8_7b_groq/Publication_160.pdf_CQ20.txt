The context provided does not give specific information about the hardware resources used for training the deep learning models. It mentions various models such as ResNet152, ResNet50, and ResNet18, as well as an ensemble of deep convolutional neural networks, but it does not specify the computational hardware used for training these models.

However, it is common practice to use Graphics Processing Units (GPUs) for training deep learning models due to their ability to perform parallel processing, which is well-suited for the matrix and vector operations that are prevalent in deep learning algorithms. Some models, such as large Transformer models used in natural language processing, may require specialized hardware such as Tensor Processing Units (TPUs) to achieve reasonable training times.

Therefore, while the context does not provide specific information on the hardware resources used for training the models, it is likely that GPUs were used, and possibly specialized hardware such as TPUs for larger models.
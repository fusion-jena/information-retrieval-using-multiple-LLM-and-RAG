The provided context does not give information about how the hyperparameters of the model are optimized. Hyperparameter optimization is an important step in machine learning model development, where the values of parameters that control the learning process are determined. Common methods for hyperparameter optimization include grid search, random search, and Bayesian optimization.

Grid search involves specifying a range of possible values for each hyperparameter and then training the model with all possible combinations of these values. This method is exhaustive and can be computationally expensive, but it guarantees that the best combination of hyperparameters within the specified ranges will be found.

Random search is a more efficient alternative to grid search, where a specified number of random combinations of hyperparameter values are sampled and used to train the model. This method can be less computationally expensive than grid search and can still find good combinations of hyperparameters.

Bayesian optimization is a more sophisticated method that uses Bayesian inference to iteratively select the next set of hyperparameter values to try, based on the performance of previous combinations. This method can be more efficient than both grid search and random search, especially when the number of hyperparameters is large.

Without more information, it is not possible to say which of these methods, if any, were used to optimize the hyperparameters of the model in the given context.
The provided context discusses the use of the Adam optimizer in the training of a custom deep convolutional neural network (D-CNN) model called CoralNet. However, it does not mention any other optimization techniques being used in the deep learning pipeline.

The Adam optimizer is a popular optimization algorithm used in training deep learning models. It is an adaptive learning rate optimization algorithm, which means that it adjusts the learning rate for each parameter individually based on the estimated first and second moments of the gradient. This makes it well-suited for training deep learning models, which often have a large number of parameters and can benefit from adaptive learning rates.

In the context provided, the Adam optimizer is used to train the CoralNet model, which is a D-CNN model used for the extraction of spatial features from underwater images. The training parameters for CoralNet are summarized in Table 1, which includes the optimizer (Adam), the number of epochs (10), and the batch size (64). The loss function used for training CoralNet is cross-entropy.

Overall, the provided context indicates that the Adam optimizer is used in the training of the CoralNet model, but it does not mention any other optimization techniques being used in the deep learning pipeline.
The provided context does not include specific information about the methods used for collecting raw data in the deep learning pipeline. It mainly focuses on the training parameters and architecture of DeepLabV3+, a deep learning model used for semantic segmentation.

However, it can be generally said that deep learning models like DeepLabV3+ typically use datasets that are already available, which can be obtained from various sources such as surveys, sensors, public datasets, and other data repositories. The choice of the dataset depends on the specific application and problem that the deep learning model is intended to solve.

For instance, in the context of remote sensing, raw data can be collected using various sensors such as satellite or aerial imagery, LiDAR, and radar. These sensors capture different types of data that can be used for various applications such as land cover classification, object detection, and change detection.

Once the raw data is collected, it undergoes preprocessing to extract features and transform it into a format that can be used for training deep learning models. This may include steps such as data augmentation, normalization, and transformation.

Therefore, while the provided context does not specifically mention the methods used for collecting raw data, it can be inferred that deep learning models typically use existing datasets that are obtained from various sources, including sensors and public datasets.
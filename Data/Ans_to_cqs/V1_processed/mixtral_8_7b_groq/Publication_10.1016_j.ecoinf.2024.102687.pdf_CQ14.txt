The text does not provide specific information on how the hyperparameters of the model were optimized. There is no mention of methods such as grid search, random search, or any other optimization techniques. The text does provide details on the specific values used for the learning rate and weight decay of the Adam optimizer, as well as the number of epochs and episodic tasks used during fine-tuning. However, it does not mention any process of hyperparameter tuning. Therefore, I cannot provide an answer to this query based on the provided context.
Based on the provided context, the techniques used to address data bias during preprocessing of the deep learning pipeline include batch normalization and category balancing through weight adjustment.

Batch normalization is a technique used to normalize the input data, where the mean of each channel in the current batch is calculated, and then, the variance of each channel in the current batch is calculated. The mean is subtracted from the input, and the result is divided by the standard deviation to obtain the normalized output. This technique is used to address the issue of large differences in the distribution between different batches of training data.

Additionally, to address the imbalance in the categorical data, the context mentions that in the training process, a relatively large weight is generated for the categories that have fewer samples, and a small weight is generated for the categories that have more samples. This solves the problem of the performance of the model being poor for the categories that have small sample sizes. However, the context does not explicitly mention the use of oversampling, undersampling, or stratified splitting techniques.

The context also mentions the use of 10-fold cross-validation, where nine of the parts are used as a training set and one is used for the validation set. This method can help in reducing the bias in the model and improve the generalization ability of the model.

In summary, the techniques used to address data bias during preprocessing of the deep learning pipeline in the provided context include batch normalization and category balancing through weight adjustment. However, the context does not explicitly mention the use of oversampling, undersampling, or stratified splitting techniques.
The specific optimization techniques used in the deep learning pipeline described in the text are not mentioned. However, it is mentioned that the parameters of deep convolutional neural networks (CNNs) are learned in an end-to-end manner, which implies that some form of optimization technique is used to adjust the parameters of the network during training.

Some common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adam, and RMSprop.

* Stochastic Gradient Descent (SGD) is a simple optimization algorithm that updates the parameters of the model by taking a step in the direction of the negative gradient of the loss function.
* Adam is an optimization algorithm that combines the ideas of momentum and adaptive learning rates. It is computationally efficient and has been shown to perform well on a variety of deep learning tasks.
* RMSprop is an optimization algorithm that adjusts the learning rate based on the moving average of the squared gradients.

It is possible that one of these optimization techniques or another optimization algorithm was used in the deep learning pipeline described in the text, but the specific optimization technique is not mentioned.
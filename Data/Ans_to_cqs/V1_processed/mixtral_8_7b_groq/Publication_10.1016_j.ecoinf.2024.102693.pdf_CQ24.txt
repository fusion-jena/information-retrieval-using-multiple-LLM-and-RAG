In deep learning, randomness is managed through various strategies, including the use of random seed values, data augmentation techniques, and dropout regularization.

The random seed value is a crucial parameter that controls the randomness of the training process. It determines the initialization of weights and biases in the neural network, the order of training samples, and the generation of random numbers in various algorithms. By setting a fixed random seed value, researchers can ensure reproducibility of the training process, allowing for consistent results and fair comparisons between different models.

Data augmentation is another strategy used to handle randomness in deep learning. It involves creating new training samples by applying random transformations to the existing data. These transformations can include rotation, scaling, flipping, and cropping, among others. Data augmentation not only increases the size of the training set but also introduces randomness, which can help improve the generalization ability of the model. Random erasing data augmentation, proposed by Zhong et al. (2020), is a specific technique that randomly selects a rectangular region in an image and replaces its pixels with random values, which can enhance the robustness of the model.

Dropout regularization is a technique used to prevent overfitting by introducing randomness during training. It works by randomly dropping out a subset of neurons in each layer of the network, effectively creating an ensemble of thinned networks. This helps to reduce co-adaptation between neurons, leading to improved generalization performance.

In the context of visual transformer-like models, such as ViT (Dosovitskiy et al., 2020), randomness is handled through the token mixer module, which is responsible for fusing spatial information. The token mixer module in a ViT is a self-attention module, which calculates the attention weights based on the input tokens. The attention weights determine the importance of each token, allowing for fine-grained token representation and the aggregation of global information.

In summary, deep learning pipelines employ various strategies to handle randomness, including the use of random seed values, data augmentation techniques, and dropout regularization. These strategies help to improve the reproducibility, robustness, and generalization performance of deep learning models.
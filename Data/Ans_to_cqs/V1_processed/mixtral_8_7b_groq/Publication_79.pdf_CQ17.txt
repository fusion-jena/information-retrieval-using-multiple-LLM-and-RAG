The provided context does not give specific information about the regularization methods used to prevent overfitting in the deep learning pipeline for the study in question. However, it is common practice in deep learning to use techniques such as dropout, L2 regularization, and data augmentation to prevent overfitting.

Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps to prevent over-reliance on any single neuron and promotes the development of redundant representations within the network. L2 regularization, also known as weight decay, adds a penalty term to the loss function that discourages large weights, which can help to prevent overfitting by reducing the complexity of the model.

Data augmentation is another technique commonly used to prevent overfitting, which involves artificially increasing the size of the training set by applying random transformations to the existing data. This can include rotations, translations, flips, and other transformations that preserve the essential features of the data while introducing variability.

While the context does not explicitly mention these techniques, it does note that data augmentation was used in the study, which suggests that some form of regularization was employed. However, without further information, it is not possible to provide a definitive answer to this question.
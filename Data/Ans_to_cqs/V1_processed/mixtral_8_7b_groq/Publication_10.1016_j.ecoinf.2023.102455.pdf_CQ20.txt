The context provided does not give specific information about the hardware resources used for training the deep learning model. However, it is common practice to use Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs) for training deep learning models due to their ability to perform parallel processing, which is beneficial for the computationally intensive nature of deep learning algorithms.

For instance, in one of the configurations mentioned, the optimizer used is Adam, which is a popular choice for deep learning models. Training deep learning models, especially Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), often requires substantial computational resources, and GPUs or TPUs are commonly used to accelerate the training process.

Additionally, the 'epochs' parameter is mentioned in multiple configurations, which refers to the number of times the learning algorithm will work through the entire training dataset. Training deep learning models for a large number of epochs can be computationally expensive, and using GPUs or TPUs can significantly reduce the training time.

Therefore, while the context does not explicitly mention the use of GPUs or TPUs, it is reasonable to assume that they are used for training the deep learning model due to the complexity and computational requirements of the models being trained.
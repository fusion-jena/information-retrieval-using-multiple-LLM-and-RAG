Based on the information provided, while there is no explicit description of the process followed to deploy the trained deep learning model, we can infer some steps based on the context.

 The trained deep learning model was likely deployed through a series of steps including model serialization and platform selection. First, the model would have been saved or serialized, allowing it to be saved as a file and loaded for later use. This is often done using Python's pickle module or framework-specific methods, such as PyTorch's torch.save() function. 

The serialized model could then be deployed on a Windows 10-based mobile laptop, equipped with an Intel(R) i7-11800H CPU and a GeForce RTX3080 GPU with 16GB of video memory. The PyCharm software platform was used for conducting experiments, and Python 3.8 along with the Pytorch 1.9 deep learning framework were utilized for training purposes. However, the context does not provide specific details about how the model was integrated into a larger application or system.

It's important to note that the model's performance was optimized by reducing memory access during the training phase. This was achieved by reconfiguring the computation sequence of 'convolutional layer + batch normalisation + activation layer' on NVI-DIA TESLA V100 GPUs, resulting in memory access reductions of 33%, 22%, and 31% for ResNet-50, Inception V3, and DenseNet models, respectively. These optimizations would have contributed to the computational efficiency of the model during the inference phase when deployed.

In summary, while the context does not provide explicit details about the deployment process, it can be inferred that the trained deep learning model was likely serialized and deployed on a Windows 10-based mobile laptop with an Intel(R) i7-11800H CPU and a GeForce RTX3080 GPU with 16GB of video memory, using the PyCharm software platform, Python 3.8, and the Pytorch 1.9 deep learning framework. Additionally, memory access optimizations were performed during the training phase to enhance the model's computational efficiency during the inference phase.
The hyperparameters of the model were optimized using a grid search method, as stated in the context. Grid search is a systematic approach to tune the hyperparameters of a machine learning model. It involves defining a range of possible values for each hyperparameter and then training the model with all the possible combinations of these values. The combination that results in the best model performance is then selected as the optimal set of hyperparameters.

In the context, it is mentioned that all hyperparameters were tuned with a grid search method. However, the specific hyperparameters that were tuned and the range of values they were tested with are not provided. Nonetheless, the use of grid search for hyperparameter tuning is a common and effective method in machine learning.

It is worth noting that other methods for hyperparameter tuning, such as random search or Bayesian optimization, can also be used. Random search involves randomly selecting values for the hyperparameters within a defined range and training the model with these values. This method can be less computationally expensive than grid search, especially when dealing with a large number of hyperparameters. On the other hand, Bayesian optimization uses a probabilistic approach to select the most promising values for the hyperparameters based on the model's performance. This method can be more efficient than grid search, especially when dealing with a small dataset or a complex model.

Overall, the use of a hyperparameter tuning method, such as grid search, is essential for optimizing the performance of a machine learning model. By finding the optimal set of hyperparameters, the model can be trained to achieve better accuracy and generalization.
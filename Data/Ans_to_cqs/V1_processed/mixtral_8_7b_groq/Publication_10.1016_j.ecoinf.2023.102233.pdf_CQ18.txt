The strategy implemented to monitor the model performance during training involves using a validation metric and a stopping criterion within an active learning approach. Specifically, each successive model iteration was trained on 70% of the training data and validated against 30% of the training data. This train/validation split approach minimizes potential overfitting of the model on small samples sizes. The F1 score, a standard performance metric that includes information on both model precision and recall, was chosen as the validation metric. The iterative model training stopped when the stopping criterion was reached, which was when successive model iterations did not reduce the F1 score. This strategy allowed for the model performance to be continuously monitored during training, and the training process to be automatically stopped when the model performance plateaued. Additionally, a final model was trained using the combined training and validation data, and a separate test dataset was used for final testing/evaluation of the model's performance. However, due to the highly imbalanced and unlabelled test data, the evaluation metrics cannot be generalized to the unlabelled data, and additional research is needed to investigate more appropriate evaluation methods.
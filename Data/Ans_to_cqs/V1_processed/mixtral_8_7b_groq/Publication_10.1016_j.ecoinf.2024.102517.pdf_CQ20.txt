The deep learning model for bird sound noise reduction was trained in an environment based on Python and Tensorflow, using a GPU (Graphics Processing Unit) specifically, an RTX 2080 Ti GPU. The use of a GPU, and particularly a high-end one like the RTX 2080 Ti, is a common practice in deep learning due to its ability to perform parallel processing, which is particularly useful for the matrix and vector operations that are prevalent in deep learning models. The use of GPU technology significantly speeds up the training process compared to using a Central Processing Unit (CPU) alone. Therefore, the hardware resource used for training the deep learning model in this study was a GPU, specifically the NVIDIA RTX 2080 Ti.
While the provided context does not discuss specific strategies used to handle randomness in the deep learning pipeline, I can provide some general information on this topic.

In deep learning, randomness can be introduced at various stages, such as the initialization of weights, dropout regularization, data shuffling, and mini-batch sampling. These techniques help improve model performance, prevent overfitting, and provide a robust learning process.

1. Initialization of weights: Deep learning models typically initialize weights randomly. This helps break symmetry in the network and ensures that different neurons learn different features. Common initialization techniques include Xavier initialization and He initialization.
2. Dropout regularization: During training, dropout randomly sets a fraction of input features to zero, preventing over-reliance on specific features and reducing overfitting. This improves the model's generalization ability.
3. Data shuffling: Shuffling the dataset before each epoch ensures that the model receives data in a different order during each training iteration. This helps the model learn from different parts of the dataset more evenly and prevents overfitting.
4. Mini-batch sampling: In deep learning, data is often divided into smaller subsets called mini-batches. These mini-batches are randomly sampled from the dataset during each training iteration. This introduces randomness into the learning process and helps the model converge faster.

In the context of the provided article, the authors use deep learning techniques to analyze LiDAR data in the context of forest ecology. While they do not explicitly mention strategies for handling randomness, it is likely that they employed some of the above techniques to ensure the robustness and generalization of their models.
The data for deep learning model training was split using a systematic approach. First, sample blocks of 9 by 9 km with 15 km spacing between blocks were created across the prairie region. Then, these blocks were assigned as training, verification, and test sets. Specifically, for the test sample, a column of the sample grid was randomly selected and every fourth column from that was added. Only every second row for the selected columns was included in the test sample. This process resulted in 39,520 (60%) training samples, 23,934 (30%) verification samples, and 6509 (10%) test samples. The majority of the samples were focused on training and verification to maximize model performance.

For training, the Adam optimizer was used with 500 epochs and categorical-cross-entropy loss function. Batch size was set to 64. Data augmentation was applied to enhance sample variation, including rotation, reflectance bias of ±10%, and random noise of ±10% reflectance for 15% of the input image. To determine training convergence, the validation data was monitored across training epochs, and the network weights with the best performance were kept. Early stopping criteria were applied if no improvement was found in 50 epochs.

Furthermore, to account for variability in results due to weight initialization and random selection for batch training, an ensemble of three models was generated. The average of the final output layer was taken for the ensemble. Max pooling was used for two models in the ensemble, and average pooling was used for one. This process aimed to enhance the ensemble results, as both max and average pooling provided improvement in some cases. All CNNs were trained using the TensorFlow backend on a Tesla M100 graphic processing unit.
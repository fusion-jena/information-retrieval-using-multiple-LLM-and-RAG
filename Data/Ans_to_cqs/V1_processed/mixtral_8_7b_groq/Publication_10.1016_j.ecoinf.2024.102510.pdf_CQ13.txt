In the context provided, the hyperparameters used in the deep learning models include the optimizer, activation function, initial learning rate, learning rate drop period, learning rate drop factor, and the number of neurons. The optimizer is a crucial hyperparameter that determines how the model updates its weights during training. The activation function introduces non-linearity into the model, allowing it to learn complex patterns. The initial learning rate determines the step size at each iteration while training, and the learning rate drop period and factor control how the learning rate decreases over time. The number of neurons determines the complexity of the model and its capacity to learn. These hyperparameters were tuned using the Hybrid Hyperparameter Optimization Scheme (HHOS) to find the optimal values that improve the performance of the pre-trained models. Specifically, the Fine-tuned EfficientNetB0 model achieved an accuracy of 99.12%, outperforming other models such as ResNet18.
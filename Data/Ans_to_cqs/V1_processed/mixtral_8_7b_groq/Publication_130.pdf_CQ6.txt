The provided context discusses the use of oversampling as a preprocessing step before training a deep learning model, specifically a Convolutional Neural Network (CNN), to classify individuals in a bird population. However, it does not directly mention other common preprocessing steps such as normalization, scaling, or cleaning.

In general, preprocessing steps for training a deep learning model can include:

1. Data cleaning: This involves removing or correcting erroneous data points that can negatively impact the model's performance.
2. Data normalization: This is the process of scaling numeric data to a common range, such as 0 to 1, to prevent any one feature from having a disproportionately large influence on the model.
3. Data augmentation: This is a technique used to increase the size of the training dataset by applying transformations such as rotation, scaling, or flipping to existing data points. In the provided context, oversampling is used as a form of data augmentation.
4. Data transformation: This involves converting data into a format that can be used by the model. For example, one-hot encoding may be used to represent categorical data as numerical data.

In the context of training a CNN for image classification, additional preprocessing steps may include:

1. Image resizing: This involves resizing images to a consistent size to ensure that the CNN can process them efficiently.
2. Image normalization: This is the process of scaling pixel values to a common range, such as 0 to 1, to prevent any one pixel value from having a disproportionately large influence on the model.
3. Data augmentation: This is a technique used to increase the size of the training dataset by applying transformations such as rotation, scaling, or flipping to existing images.

In the provided context, the researchers applied limited oversampling to their training dataset to address the issue of class imbalance. They created copies of the training data from the less sampled classes, specifically nine sociable weavers and six great tits, until 900 images were available for the training dataset. This helped to prevent the model from over-generalizing for the classes in majority due to its increased prior probability.

Therefore, while the provided context does not explicitly mention other preprocessing steps such as normalization, scaling, or cleaning, it is likely that these steps were also applied before training the CNN.
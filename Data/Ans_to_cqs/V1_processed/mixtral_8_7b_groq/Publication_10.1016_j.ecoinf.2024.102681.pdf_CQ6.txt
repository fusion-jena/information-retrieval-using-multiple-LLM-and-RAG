Based on the provided context, there is no explicit information about the specific preprocessing steps taken before training a deep learning model. However, some general preprocessing steps commonly used in deep learning can be inferred.

Firstly, data cleaning is often a crucial step in any data analysis, including deep learning. This could involve handling missing values, outliers, or incorrect data entries. For instance, Kaur et al. (2020) mention that they compiled a comprehensive dataset for their study, which suggests some form of data cleaning was likely performed.

Secondly, normalization or scaling of the data is another common preprocessing step. This is especially important when dealing with data that have different units or scales. The purpose is to ensure that all features contribute equally to the learning process. In the context of the CNN model used by Kaur et al., normalization could help the model learn spatial hierarchies more effectively.

However, the provided context does not explicitly mention these steps. It primarily focuses on the architecture of the deep learning model (a hybrid CNN-LSTM model) and its application to drought propagation time series.

In conclusion, while the provided context does not give specific details about the preprocessing steps, it is reasonable to infer that data cleaning, normalization, or scaling were likely performed, as these are standard practices in deep learning.
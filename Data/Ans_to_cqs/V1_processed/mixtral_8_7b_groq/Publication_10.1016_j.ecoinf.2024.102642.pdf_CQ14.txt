The hyperparameters of the model were not optimized using methods such as grid search or random search, as stated explicitly in the provided context. Instead, consistent hyperparameters were maintained across all training instances for each task. Specifically, for SEIO PBW vocalization training and blue whale Dcall training, the batch sizes were set to 5 and 4, and the learning rates were set to 1e-4 and 1e-3, respectively. The models were trained for a fixed number of epochs, but an early stopping method was used to keep the model weights before the models overfitted. The patience parameter was set at 10 epochs for all training phases. This decision was based on the fact that more samples were used for SEIO PBW vocalization training. Therefore, the hyperparameters were set based on the specific requirements of the tasks and not through an optimization process.
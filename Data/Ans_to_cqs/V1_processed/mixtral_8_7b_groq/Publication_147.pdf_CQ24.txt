The provided context does not contain information about the specific strategies used to handle randomness in the deep learning pipeline. However, it is mentioned that deep learning techniques were considered for large-scale spatial wetland inventory efforts in Canada, but the study mainly focused on shallow learning models, specifically the XGBoost algorithm.

In general, deep learning models often involve randomness during various stages such as weight initialization, dropout, data shuffling, and batch sampling. To ensure reproducibility of results, deep learning practitioners can employ several strategies to control this randomness.

1. Fixed random seed value: By setting a fixed random seed value, the random number generator used in the model will produce the same sequence of random numbers, ensuring consistent results across multiple runs. This can be done in popular deep learning libraries like TensorFlow and PyTorch.

2. Deterministic dropout: Dropout is a regularization technique used in deep learning to prevent overfitting. In dropout, randomly selected neurons are ignored during training, which helps the model generalize better. By using deterministic dropout, the same neurons are dropped during each forward pass, ensuring consistent results.

3. Disabled data shuffling: Data shuffling is a common practice during training to ensure that the model does not overfit to the training data order. However, if reproducibility is desired, data shuffling can be disabled.

4. Fixed batch order: When using mini-batch training, the order of samples within a batch can be fixed across multiple epochs.

For the specific case of handling randomness in the deep learning pipeline for wetland inventory tasks, one could consider using a fixed random seed value, deterministic dropout, and fixed data shuffling or batch order. However, the given context does not provide enough information to determine if these strategies were employed in the study.
Based on the provided context, there are no specific details mentioned about the preprocessing steps such as normalization, scaling, or cleaning that were applied to the images before training the deep learning model. However, it is mentioned that five training datasets were built with examples of images having different levels of tree cover, continuous and discrete approximations, and inclusion of the auxiliary class "Non-Forest." It can be inferred that some preprocessing was done to create these datasets, such as resizing the images to a consistent size or format, and possibly applying data augmentation techniques during the training phase.

In the training phase, the authors trained five CNN models based on Inception v.3 using two optimization techniques, transfer learning, and data augmentation. Data augmentation is a technique that can be used to increase the size of the training dataset by applying random transformations to the images, such as rotation, scaling, and flipping. This can help improve the performance of the model by providing more varied examples during training.

Additionally, during the training phase, the authors configured the model parameters by training the last two fully connected network layers in their dataset using a learning rate of 0.001 and a decay factor of 16 every 30 epochs. This suggests that some form of preprocessing or normalization may have been applied to the weights of the model during training.

In summary, while the text does not explicitly mention the preprocessing steps taken before training the deep learning model, it can be inferred that some preprocessing was done, such as resizing the images and applying data augmentation techniques. Additionally, some form of preprocessing or normalization may have been applied to the weights of the model during training.
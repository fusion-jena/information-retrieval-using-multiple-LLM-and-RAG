Based on the provided context, the strategy implemented to monitor the model performance during training involves using both a training set and a validation set. The data from the first eight days were used as the training set, and the data from the last two days were used as the validation set. This approach allows for the evaluation of the model's performance not only on the data it has been trained on but also on unseen data.

Additionally, several metrics were used to monitor the model's performance during training. For regression problems like predicting DO concentration, the coefficient of determination (R2), mean squared error (MSE), and mean absolute error (MAE) are commonly used metrics. These metrics provide insights into the model's accuracy, the average magnitude of its errors, and the median absolute difference between its predictions and the actual values, respectively.

In the context of the WTD-GWO-SVR method proposed by Feng et al., the R2 score for both the training and validation sets was reported for each of the three SVR models and the three WTD-GWO-SVR models. This allows for a direct comparison of the models' performance on both the training and validation sets. Furthermore, the MSE and MAE scores for the training and validation sets were reported for the WTD-GWO-SVR models, providing additional perspectives on the models' performance.

It is worth noting that the performance monitoring strategy described here is a common practice in machine learning model development. However, the specific metrics used and the partitioning of the data into training and validation sets can vary depending on the problem and the data at hand.
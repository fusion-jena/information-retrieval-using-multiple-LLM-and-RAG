Based on the provided context, there is no direct information about how randomness is handled in deep learning pipelines. However, the context does mention the use of Random Forest, a machine learning technique, in the development of species distribution models for tropical tuna. Random Forest is an ensemble learning method that uses multiple decision trees and bagging to improve prediction accuracy and handle overfitting.

In the context, Random Forest is used to build prediction models for four species of tropical tuna: skipjack (SKJ), yellowfin (YFT), bigeye (BET), and albacore (FAL). The method achieves the highest accuracy for all analyzed species, and is used to predict the probabilities of finding high and low catches and absences of each species in the study area.

While Random Forest is not a deep learning method, it does use randomness as a key component of its algorithm. Specifically, it uses randomness to select a subset of the training data for each decision tree, and to select a subset of the features to consider at each node of the tree. This randomness helps to reduce overfitting and improve the generalization performance of the model.

In addition, the context mentions the use of cross-validation, a technique that can help to handle randomness in machine learning models. Cross-validation involves dividing the data into multiple folds, training the model on one fold, and testing it on the remaining folds. This process is repeated for each fold, providing a more robust estimate of the model's performance and helping to reduce the impact of randomness.

Overall, while the context does not provide direct information about how randomness is handled in deep learning pipelines, it does suggest that randomness is an important component of machine learning models, and that techniques such as Random Forest and cross-validation can help to handle randomness and improve model performance.
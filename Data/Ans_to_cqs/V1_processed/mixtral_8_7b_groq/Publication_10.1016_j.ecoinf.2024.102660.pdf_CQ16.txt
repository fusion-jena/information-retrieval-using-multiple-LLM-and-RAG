The context provided does not explicitly mention the specific criterion used to determine when training is complete. However, it does mention that the network training persisted for 120 epochs and that the learning rate was adjusted using the equal-interval adjustment strategy, halving the original rate every 10 epochs of iteration. This suggests that the training might be completing after a fixed number of epochs, in this case, 120.

However, it is important to note that in many deep learning applications, training is not typically completed based solely on a fixed number of epochs. Instead, training is often continued until a certain condition is met, such as when the validation loss plateaus or starts to increase, indicating that the model is starting to overfit to the training data. This approach is not mentioned in the provided context, but it is a common practice in machine learning.

Additionally, the context also mentions that 176 image pairs were used for validating the model accuracy during the training phase. This implies that validation was performed during the training process, but it does not specify how the validation results were used to determine when training should be stopped.

In summary, while the provided context does not give a clear answer to the query, it can be inferred that training might be completed after a fixed number of epochs. However, in general, training is often continued until a certain condition is met, such as when the validation loss plateaus.
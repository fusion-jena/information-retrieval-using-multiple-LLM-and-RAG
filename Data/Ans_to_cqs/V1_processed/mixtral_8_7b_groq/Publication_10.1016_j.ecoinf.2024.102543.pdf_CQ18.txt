During the training of the model, a strategy is implemented to monitor its performance using a validation set. The validation set, consisting of 13% (607 images) of the total data, is utilized to fine-tune hyperparameters and assess the model's performance on unseen data. This approach helps prevent overfitting, which occurs when a model learns the training data too well, to the point where it performs poorly on new, unseen data.

In addition to using the validation set, five metrics are employed to evaluate the model's performance: Precision, Recall, mAP0.5, mAP0.5:0.95, and Average Precision (AP). Precision is the ratio of true positive predictions to the total predicted positives, while Recall is the ratio of true positive predictions to the total actual positives in the data. mAP, or mean Average Precision, is the average of AP values for different recall levels, providing an overall measure of the model's accuracy.

Furthermore, considering the model's Params (number of learned variables) and FLOPs (floating point operations) is essential for evaluating its complexity and computational efficiency. These parameters are crucial for assessing the model's performance in real-world applications, where speed and resource usage are critical factors. Inference time, which measures the time taken for the model to process input data and produce output predictions, and training time, which measures the time taken for the model to learn from the training data, are also essential metrics for evaluating the model's efficiency.

When introducing variations in the model, such as adding a P2 layer to the YOLOv8 architecture, it is crucial to balance the potential downsides, such as increased computational complexity and overfitting risks, with other considerations, such as model size and performance requirements. In this case, a modification in the C2f layer, called C2f2, was proposed to counterbalance the increase obtained with the addition of the P2 feature level layer, making the overall architecture lighter.

In summary, the strategy implemented to monitor the model's performance during training involves using a validation set, employing various accuracy metrics, and considering the model's complexity and computational efficiency. Balancing the tradeoffs of introducing variations in the model, such as increased depth, is also essential for achieving optimal performance and preventing overfitting.
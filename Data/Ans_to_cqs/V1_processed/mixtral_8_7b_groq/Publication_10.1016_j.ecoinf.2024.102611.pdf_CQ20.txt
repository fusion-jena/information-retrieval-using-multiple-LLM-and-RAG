Based on the provided context, there is no explicit information given about the specific hardware resources used for training the deep learning models. Therefore, it is not possible to give a definitive answer to this question.

However, it is mentioned that deep learning models are used in the studies, and these models typically require significant computational power for training. Therefore, it is likely that the researchers used hardware accelerators such as GPUs (Graphics Processing Units) or TPUs (Tensor Processing Units) to speed up the training process.

For instance, in the study by Quach et al. (2023), an improved Swin Transformer model is developed for maize seed variety recognition. Swin Transformer is a deep learning model that requires a large amount of computational power for training. Although the study does not mention the specific hardware used, it is likely that the authors used a GPU or TPU for training the model.

Similarly, in the study by Pushpa and Rani (2023), an unbiased lightweight deep convolutional neural network called Ayur-PlantNet is developed for Indian Ayurvedic plant species classification. Deep convolutional neural networks are known to require significant computational resources for training. Therefore, it is possible that the authors used hardware accelerators for training the model.

In summary, while there is no explicit information given about the hardware resources used for training the deep learning models in the provided context, it is likely that the researchers used hardware accelerators such as GPUs or TPUs for training the models due to their computational requirements.
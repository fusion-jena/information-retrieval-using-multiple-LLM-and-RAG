Deep learning methods for underwater fish recognition and quantification often rely on raw data collected through various means, including sensors, surveys, and underwater monitoring systems.

One such method is the use of imaging sonar for quantifying the abundance, species richness, and size of reef fish assemblages (2023). Sonar technology can provide high-resolution images of underwater environments, allowing for accurate fish detection and measurement.

Another method for collecting raw data is through underwater monitoring systems, such as those used for landings data (Huang et al., 2008; Liu et al., 2015). These systems can provide high-definition and high-resolution images of fish, allowing for precise fish length and catch number measurements.

In addition, surveys can be used to collect raw data for deep learning methods. For example, researchers have used underwater video footage to detect, track, and count fish in low-quality, unconstrained settings (Spampinato et al., 2008). This method can provide a large amount of raw data for deep learning algorithms to analyze and learn from.

However, there are challenges in collecting raw data for underwater fish recognition and quantification. One such challenge is fish body occlusion, where fish movement is often irregular, leading to instances where a large number of fish are obstructed by underwater obstacles or overlap with each other's bodies (Liu et al., 2014). This can make it difficult for deep learning algorithms to accurately recognize individual fish.

Another challenge is the scarcity of high-quality, high-definition monitoring of fish length and catch numbers from landings using deep learning (Fish. Res. 246, 106166). Unlike facial recognition datasets, such as the LFW face database (Huang et al., 2008) and the CelebA dataset (Liu et al., 2015), there are only a limited number of high-quality, high-definition monitoring datasets available for fish recognition and quantification.

To address these challenges, researchers have developed methods for modeling pruning techniques for simplifying deep neural networks (Pasandi et al., 2020; Pattanayak et al., 2021). These techniques can help improve the recognition effectiveness for severely occluded fish bodies and reduce the need for large amounts of high-quality, high-definition monitoring data.

In summary, deep learning methods for underwater fish recognition and quantification rely on raw data collected through various means, including sensors, surveys, and underwater monitoring systems. However, there are challenges in collecting high-quality, high-definition monitoring data, and researchers have developed methods for modeling pruning techniques to address these challenges.

References:

Fish. Res. 246, 106166.

Huang, X., Ramesh, V., Berg, A.C., Learned-Miller, E., 2008. Labeled faces in the wild: A database for face recognition in unconstrained environments. Technical Report, University of Massachusetts, Amherst.

Liu, J., Luo, Z., Wang, X., Tang, X., 2014. A survey on occlusion handling methods for object detection. Pattern Recognition 47, 2595–2615.

Liu, M.Y., Luo, P., Wang, X., Tang, X., 2015. CelebA: A large-scale face attributes dataset. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1–9.

Spampinato, C., Chen-Burger, Y.H., Nadarajan, G., Fisher, R.B., 2008. Detecting, tracking and counting fish in low quality unconstrained underwater videos. In: International Conference on Computer Vision Theory and Applications(VISAPP). SciTePress, 514–519.

Pasandi, M.M., Hajabdollahi, M., Karimi, N., Samavi, S., 2020. Modeling of pruning techniques for simplifying deep neural networks. In: 2020 International Conference on Machine Vision and Image Processing (MVIP). IEEE, 1–6.

Pattanayak, S., Nag, S., Mittal, S., 2021. CURATING: a multi-objective based pruning technique for CNNs. Journal of Systems Architecture 116, 102031.

2023. The capacity of imaging sonar for quantifying the abundance, species richness, and size of reef fish assemblages. Marine Ecology Progress Series 717, 157–179.
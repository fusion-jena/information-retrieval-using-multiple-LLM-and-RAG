The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention a strategy to prevent overfitting: using 80% of the samples for training and 20% for testing. This is a common approach to enhance the generalization of a machine learning model.

In addition, the context mentions that data augmentation was used. Data augmentation is a regularization technique used to increase the amount of training data by creating modified versions of the original data, such as adding noise or changing the pitch of audio files. This can help the model generalize better and be less prone to overfitting.

The context also mentions that a CNN was trained for 4 sound classes and then for 5 sound classes. This process of training multiple models and combining their predictions is called ensemble learning, which can also help to reduce overfitting.

As for specific regularization methods such as dropout or L2 regularization, the context does not provide enough information to determine if they were used. Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent overfitting by making the network more robust and less dependent on the training data. L2 regularization, also known as weight decay, is a method that adds a penalty to the loss function based on the size of the weights, which helps to reduce overfitting by discouraging large weight values.

Therefore, while the context provides some information about strategies used to prevent overfitting, it does not explicitly mention the use of dropout or L2 regularization.
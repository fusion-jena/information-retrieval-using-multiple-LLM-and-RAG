Based on the provided context, there is no explicit information about how the hyperparameters of the model are optimized, such as through methods like grid search or random search. However, the text does provide some details about the specific values used for certain hyperparameters in the experiments conducted.

For instance, in Table 1, the authors fixed the voxel size and batch size and adjusted the regularization strength. They found that the results were better when the regularization strength was 0.2. In Table 2, the voxel size was varied while keeping the regularization strength and batch size constant. Similarly, in Table 3, the batch size was changed while the voxel size and regularization strength were kept fixed.

Additionally, the authors mentioned that during model training, they processed data with a deep neural network and adjusted the training epoch, batch size, and volume size to output a classifier for prediction. They also set the learning decay rate to 0.7 so that the learning rate gradually decays as the training progresses.

Therefore, while the specific method of hyperparameter optimization is not provided, it is clear that the authors conducted experiments with different hyperparameter values to find the best combination for their specific task.
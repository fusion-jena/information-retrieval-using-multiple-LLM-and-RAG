Based on the provided context, there is no direct mention of strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, there are some strategies mentioned that could help reduce the impact of randomness on the model's performance.

Firstly, the authors explored different hyperparameters of the CNN architecture to find the optimal network architecture for classifying moth species. This includes adjusting the number of layers, number of neurons, and dropout probability. By tuning these hyperparameters, the model's performance can be improved, and the impact of randomness can be reduced.

Secondly, the authors used a dropout probability of 0.3 after the second to last hidden layer during training. Dropout is a regularization technique that randomly drops out a fraction of the input units of a layer during training, which helps prevent overfitting and reduces the impact of randomness.

Thirdly, the authors used padding before CNN prediction, which involves adding extra pixels around the bounding box found during blob segmentation. The performance of the system changes significantly with variation in padding before CNN prediction. By using padding, the authors ensured that the input images had a consistent size, which can help reduce the impact of randomness.

Lastly, the authors used transfer learning on ResNetV50 using pretrained weights and only trained the output layers. Transfer learning involves using a pre-trained model as a starting point for a new task, which can help reduce the impact of randomness by providing a good initial point for training.

In summary, while the provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value, it does mention some strategies that can help reduce the impact of randomness on the model's performance. These include tuning hyperparameters, using dropout regularization, using padding, and using transfer learning.
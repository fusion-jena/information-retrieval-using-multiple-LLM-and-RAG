The generalizability of the deep learning model was addressed through several measures. Firstly, the dataset was divided into training, validation, and testing subsets, consisting of 60%, 20%, and 20% of the data, respectively. This partitioning strategy helps to reduce overfitting issues and evaluate the model's generalization in a specific domain when cross-validation is not feasible. However, it is important to note that experimenting with different sites or spatially distinct areas in training, validation, and testing does not guarantee generalization to new sites, as unique spatial variability and different domains may be present.

Data augmentation was applied only to the training data to avoid overlapping the same data in different partitions. This technique increases the diversity of the training data, which can help improve the model's generalizability.

The authors used two evaluation metrics to assess the performance of the models: the coefficients of determination (R2) and the root mean squared error (RMSE). R2 measures the percentage of variation in the response variable explained by the model, while RMSE quantifies the difference between predicted and actual values. By using these metrics, the models' performance can be compared, and the best-generalized model can be selected.

The authors conducted a standard machine learning hyperparameter search over two sites due to computational cost. They considered the activation function, the optimizer, and the learning rate during the search. An exhaustive search for the fully connected architecture was also performed for all mono-site and multi-site models. Comparing the training, validation, and testing performance for each model helps avoid overfitting and ensures the best hyperparameters are chosen for the final model.

Despite these measures, the authors acknowledge that when the training data is scarce and not representative, the transfer learning of the models is impossible in areas with conditions very different from the training data. Therefore, it is crucial to ensure that the training data is diverse and representative to enhance the model's generalizability.

In summary, the authors ensured the generalizability of the deep learning model by using dataset partitioning, data augmentation, evaluation metrics, hyperparameter search, and careful consideration of the training data's diversity and representativeness.
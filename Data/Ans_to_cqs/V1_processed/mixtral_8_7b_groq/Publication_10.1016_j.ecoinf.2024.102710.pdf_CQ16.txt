Based on the provided context, there is no explicit information about the specific criteria used to determine when training is complete in the described active learning experiments. The text focuses on the investigation of various active learning strategies and the use of a ResNet-50 variant or BirdNet-1 as transfer learning models, without mentioning the stopping criterion for training.

However, in general, training of machine learning models can be considered complete when a predefined stopping criterion is met. Common stopping criteria include:

1. Validation loss plateau: Training can be stopped when the validation loss does not decrease for a certain number of epochs, indicating that the model has likely converged or started overfitting.

2. Early stopping: A form of regularization where training is halted as soon as the validation loss starts increasing, even if the training loss is still decreasing.

3. Number of epochs: Training can be set to stop after a fixed number of epochs, regardless of the validation loss. This can be useful when there is a time or computational constraint.

4. Patience: A hyperparameter that determines the number of epochs to wait before stopping the training if the validation loss does not improve.

5. Monitoring other metrics: In some cases, training can be stopped based on other metrics, such as accuracy or F1 score, if they reach a desired level or do not improve for a certain number of epochs.

Since the provided context does not mention any specific stopping criterion, it is unclear which of these or other criteria were used in the described experiments.


The provided context does not offer explicit information about the criteria used to determine when training is complete in the described active learning experiments. Common stopping criteria include validation loss plateau, early stopping, number of epochs, patience, and monitoring other metrics. However, the specific criterion used in these experiments remains undisclosed in the given text.
The hyperparameters of the XGBoost model were optimized using grid search functions. This method involves systematically searching through a range of predefined values for each hyperparameter and evaluating the model's performance for each combination of these values. The optimal value for each hyperparameter is then determined based on the test error metric.

In this specific case, the XGBoost model parameters were tuned to find the optimal value for the following hyperparameters: nrounds, max_depth, eta, gamma, min_child_weight, subsample, and colsample_bytree. The optimized XGBoost parameters were: nrounds = 500, max_depth = 4, eta = 0.03, gamma = 1, min_child_weight = 1, subsample = 0.5, colsample_bytree = 0.8.

The selection of these hyperparameters was based on the inputs being trained to the six classes from the ABMI plot training data using the “multi:softmax” objective setting. The authors also wanted to ensure conservative model building since they knew there was little power in the inputs to discriminate between wetland classes.

It is worth noting that the optimization process was performed for an XGBoost model, and the specific method used for optimizing hyperparameters in other models, such as the convolutional neural network (CNN) model mentioned in the context, might differ. However, the text does not provide information on how the hyperparameters of the CNN model were optimized.
The training of the fine-tuned models is set to complete when the validation loss does not improve within 10 epochs. This is a form of early stopping, which is a form of regularization used to prevent overfitting. The models are trained with a cyclical learning rate of 0.0004 for 50 epochs. During the training process, the performance of the models is assessed by calculating Precision (P), Recall (R), and Accuracy (A) based on the number of true and false positive detections, NTP and NFP, along with the number of true and false negative detections, NTN and NFP. The F1 score, the harmonic mean of the precision and recall, is also calculated. The models are validated during training by combining the original base model validation set with 100 frames per class from the labelled December 3rd 2019 training set, randomly pooled per experiment. This way, the modelâ€™s performance is assessed across both environments simultaneously.

Additionally, the training set is amplified using data augmentation to create 600 frames per class. A dropout rate of 0.2 is used during fine-tuning, and DropConnect is employed, which randomly discards the input of the hidden layer. This is done to improve the generalization ability of the model and prevent overfitting.

It is worth noting that the minimum time between test datasets is 4 days, and each 24-hour period of PAM data equates to 28,800 frames, a total of 115,200 frames for the four days. In this dataset, the delphinid tonal class is least represented, accounting for only 0.6% of the frames.

In summary, the training of the fine-tuned models is set to complete when the validation loss does not improve within 10 epochs, and the performance of the models is assessed using Precision, Recall, Accuracy, and F1 score. Additionally, Dropout and DropConnect are used to improve the generalization ability of the model, and data augmentation is used to amplify the training set.
After the model training, there are several postprocessing steps involved. However, the provided context does not explicitly mention all the steps such as saliency maps or specific metrics calculation. Nevertheless, it does mention the creation of a confusion matrix for accuracy assessment.

In the context, the postprocessing step that is explicitly mentioned is the accuracy assessment of the map. This involves creating a confusion matrix, which is a table that is often used to describe the performance of a classification model (in this case, the model used to distinguish iceplant from other vegetation). The confusion matrix compares the actual classes of the data (referred to as the 'reference class') with the classes predicted by the model (referred to as the 'map class').

The confusion matrix for the assessment sample classification, also known as ground validated points, is presented in Table A.8 of the context. It shows the number of instances where the model correctly predicted the class (the diagonal elements of the matrix) and where it made errors (the off-diagonal elements). For example, the model correctly identified 262 instances of iceplant, but it also incorrectly identified 26 instances of other vegetation as iceplant.

The confusion matrix is then used to calculate various accuracy metrics, such as the overall accuracy, producer's accuracy, and user's accuracy. These metrics provide a more comprehensive evaluation of the model's performance than just looking at the confusion matrix alone. However, the context does not provide specific values for these accuracy metrics.

In summary, the postprocessing steps involved after the model training include an accuracy assessment using a confusion matrix. However, the context does not provide information on other postprocessing steps such as saliency maps or specific metrics calculation.
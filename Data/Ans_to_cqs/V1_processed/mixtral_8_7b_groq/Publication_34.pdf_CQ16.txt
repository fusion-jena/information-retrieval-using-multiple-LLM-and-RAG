The context provided does not give specific details about the criteria used to determine when training is complete for the models. However, it does mention that the learning of model parameters is done through optimization of an objective function using stochastic gradient descent algorithms, and the loss function gradient is approximated by the back-propagation algorithm. This suggests that some form of loss convergence is likely used to determine when training is complete.

In the context of machine learning, training is often considered complete when the validation loss plateaus or stops decreasing significantly. This means that the model has learned as much as it can from the training data and further training is unlikely to improve its performance. Additionally, monitoring the validation loss helps to prevent overfitting, which occurs when the model becomes too specialized to the training data and performs poorly on new, unseen data.

In the case of the models described in the context, it is possible that the training is considered complete when the loss on the test set stops decreasing or decreases only marginally. This would ensure that the model has learned to generalize well to new data and is not overfitting to the training set. However, this is just an educated guess based on common practices in machine learning and is not explicitly stated in the provided context.
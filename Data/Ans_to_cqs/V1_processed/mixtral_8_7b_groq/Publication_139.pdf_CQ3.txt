Based on the provided context, the data annotation techniques used in the deep learning pipeline for this specific study mainly involve manual labeling of spectrogram images into sound categories. However, the context does not explicitly mention specific techniques such as bounding box annotation or instance segmentation.

In the training database assembly process, selected spectrogram images were manually labeled via visual inspection by two trained graduate students into a set of sound categories. These categories evolved as the year progressed, suggesting a dynamic annotation process. On average, around 60 images could be annotated per minute, which corresponds to 8 minutes of audio data.

The annotation process focused on selecting and labeling a subset of spectrograms from each day, as many spectrograms containing only background noise were not needed. This strategy allowed for the assembly of a database consisting of over 40 k images of bio-, anthro-, and geophonies.

For new recording sites in nearby geographic regions, the context suggests using existing labeled datasets for initial data labeling and then manually amending false labels. This approach implies a reliance on human annotation for refining the dataset.

In summary, while the context does not explicitly mention techniques like bounding box annotation or instance segmentation, it does describe a manual labeling process for spectrogram images. The focus is on human annotation, with an emphasis on selecting and labeling a subset of images based on sound categories.
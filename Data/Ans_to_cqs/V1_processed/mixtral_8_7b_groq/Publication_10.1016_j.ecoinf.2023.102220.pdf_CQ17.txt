Based on the provided context, no specific regularization methods such as dropout or L2 regularization are mentioned as having been used in the deep learning pipeline. The text describes the use of a deep neural network (DNN) with long short-term memory (LSTM) blocks and the rectified linear unit (ReLU) activation function, as well as the sigmoid activation function for the output layer. It also mentions the use of the machine learning methods Support Vector Machines, Random Forests, and a method called PBP. However, it does not specify any regularization techniques that were used to prevent overfitting.

It is worth noting that regularization is an important technique to consider when building a deep learning model in order to prevent overfitting and improve the model's ability to generalize to new data. Some common regularization methods for deep learning include L1 and L2 regularization, dropout, and early stopping. L1 and L2 regularization add a penalty term to the loss function that encourages the model to have smaller weights, which can help to prevent overfitting. Dropout is a technique in which randomly selected neurons are "dropped out" during training, which helps to prevent the model from becoming too reliant on any one neuron and improves its ability to generalize. Early stopping is a method in which training is halted as soon as the model's performance on a validation set starts to degrade, which can help to prevent overfitting by stopping training before the model begins to memorize the training data.

In summary, while the provided context does not specify which regularization methods were used in the deep learning pipeline, it is important to consider using regularization techniques such as L1 and L2 regularization, dropout, and early stopping in order to prevent overfitting and improve a deep learning model's ability to generalize.
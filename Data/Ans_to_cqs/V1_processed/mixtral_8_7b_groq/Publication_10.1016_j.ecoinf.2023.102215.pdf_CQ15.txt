The deep learning pipeline in the study utilizes the Adam optimization algorithm. The choice of optimization algorithm is crucial as it affects the training process and the final model performance.

Adam, which stands for Adaptive Moment Estimation, is a popular optimization algorithm used in deep learning. It is an extension of the stochastic gradient descent (SGD) algorithm that includes adaptive learning rates for each parameter. Adam combines the advantages of two other optimization algorithms, AdaGrad and RMSProp, by using the moving averages of both the gradient and the squared gradient.

The Adam optimizer has several hyperparameters that need to be set, including the learning rate, beta1, beta2, and epsilon. In this study, the default hyperparameters of Adam are used, which are: (1) betas=(0.9, 0.999), (2) eps=1.0 × 10−8, and (3) weight_decay = 0.

The choice of the Adam optimizer over other optimization algorithms, such as SGD or RMSProp, is based on its ability to converge faster and achieve better performance. Adam is well-suited for problems with large datasets and high-dimensional parameters, making it a suitable choice for this study.

In summary, the deep learning pipeline in this study uses the Adam optimization algorithm with default hyperparameters. The choice of Adam over other optimization algorithms is based on its ability to converge faster and achieve better performance.
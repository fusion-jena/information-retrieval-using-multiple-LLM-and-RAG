The provided context discusses the use of deep learning methodologies in underwater fish video processing and the development of two novel deep learning networks. However, it does not directly mention strategies employed to handle randomness in the deep learning pipeline, such as the use of a random seed value.

In general, deep learning models can contain sources of randomness, such as the initialization of model weights and the order of data samples during training. To address this, deep learning practitioners may employ various strategies to manage randomness and ensure the reproducibility of their results.

One common approach is the use of a fixed random seed value. By setting a specific seed value, the random number generator used in the deep learning pipeline will produce the same sequence of random numbers, ensuring consistent initialization of weights and the order of data samples. This can help in reproducing the same results across multiple runs of the same model.

Another strategy to handle randomness is through techniques like model ensembling. Ensemble methods, such as XGBoost, combine the predictions of multiple models trained with different initializations or subsets of the data. This can help reduce the impact of randomness on the final prediction, as the ensemble averages out the variations introduced by individual models.

In the context of the provided text, the authors discuss the development of deep learning networks for underwater fish video processing in turbid waters. While they do not explicitly mention strategies for handling randomness, it is reasonable to assume that they might have employed techniques like fixed random seed values or model ensembling to ensure the reproducibility and robustness of their results. However, without more information, this is only an assumption.
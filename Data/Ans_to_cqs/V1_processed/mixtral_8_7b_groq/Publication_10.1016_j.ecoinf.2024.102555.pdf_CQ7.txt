The criteria used to split the data for deep learning model training into subsets such as train, test, and validation are not specified in the provided contexts. However, these are common practices in machine learning and deep learning to ensure the model's performance, prevent overfitting, and provide a fair evaluation of the model.

In general, the dataset is divided into three subsets: training, validation, and testing. The training set is used to train the model, the validation set is used to tune the model's hyperparameters and prevent overfitting, and the testing set is used to evaluate the model's performance on unseen data.

The splitting criteria can vary depending on the size and nature of the dataset. A common approach is to use 70% of the data for training, 15% for validation, and 15% for testing. Another approach is to use k-fold cross-validation, where the dataset is divided into k subsets, and the model is trained and validated k times, each time using a different subset for validation and the remaining k-1 subsets for training.

In the context of flood susceptibility mapping using swarm intelligence algorithms and deep learning neural networks (Cai et al., 2021), the authors do not mention the criteria used to split the data. However, they do mention the dataset's size and sources, which include remote sensing data, digital elevation models, and historical flood records.

Similarly, in the context of vegetation change attribution using machine learning techniques (Shi et al., 2020), the authors do not mention the criteria used to split the data. They use remote sensing data, meteorological data, and land use data to train and validate their model.

Therefore, while the specific criteria used to split the data for deep learning model training are not provided in the contexts, it is a common practice to divide the dataset into training, validation, and testing subsets to ensure the model's performance and prevent overfitting.
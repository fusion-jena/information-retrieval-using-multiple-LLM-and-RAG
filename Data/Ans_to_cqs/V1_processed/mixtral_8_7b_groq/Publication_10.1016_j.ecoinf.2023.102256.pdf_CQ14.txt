Based on the provided context, there is no explicit information given about how the hyperparameters of the model are optimized. Hyperparameter optimization is a crucial step in training machine learning models, and methods such as grid search, random search, or Bayesian optimization are commonly used. However, the text does not mention any of these techniques being applied.

In the description of the baseline model, we learn about the architecture, which includes two convolutional layers with 8 filters of size 16 × 16, followed by max pooling (4 × 4), a flattening operation, and two fully-connected layers. The text also mentions that different methods were explored and tested to enhance the neural network with contextual information, one of which is a multi-branch CNN. However, none of these sections mention hyperparameter optimization.

It is possible that the hyperparameters were optimized using a method that is not specified in the text, or perhaps they were set to default values or chosen based on prior knowledge or experience. Without further information, it is not possible to provide a definitive answer.

In summary, the provided context does not give information on how the hyperparameters of the model are optimized.
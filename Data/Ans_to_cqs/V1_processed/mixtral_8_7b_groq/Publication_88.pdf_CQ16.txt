The criteria used to determine when training is complete for the EF and SN models is early stopping, which is triggered when the loss does not improve over the validation set for a certain number of epochs. In this specific case, training stops when there is no improvement in the validation loss for 10 consecutive epochs. This method is used to prevent overfitting and improve the generalization of the model. Additionally, a batch size of 32, 100 number of epochs, a dropout rate of 0.2 in the final FC layer, and the Adam optimizer with weight decay equal to 0.9 and learning rate equal to 10âˆ’3 are used for training the EF and SN models. The binary cross-entropy is used as the loss function. For CSVM and SVM, the validation set is used to add samples to the training set in order to balance the classes, and early stopping is not used.
The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does discuss the use of transfer learning and weak supervision to address the problem of limited labeled data availability.

In the context, the authors use transfer learning by pre-training two deep neural networks (DNNs), EfficientNet and ViT, on the open-source fully-labeled ImageNet dataset. This technique can help prevent overfitting by allowing the DNNs to learn features from a large and diverse dataset before being fine-tuned on the smaller FishInTurbidWater dataset.

Additionally, the authors use a semi-supervised contrastive learning approach, which involves first training a self-supervised contrastive learning model and then fine-tuning it on the weakly labeled FishInTurbidWater dataset. This approach requires only a small portion of the weakly labeled dataset, which can help prevent overfitting by reducing the risk of memorizing the noise in the training data.

While the context does not explicitly mention regularization methods such as dropout or L2 regularization, the techniques used can still help prevent overfitting. Transfer learning and semi-supervised contrastive learning can both help ensure that the DNNs learn generalizable features from the data, reducing the risk of overfitting.

Therefore, while the context does not provide a direct answer to the query, it suggests that the authors use techniques that can help prevent overfitting in the deep learning pipeline.
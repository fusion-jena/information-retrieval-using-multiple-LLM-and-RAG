In the context provided, the training of deep Convolutional Neural Networks (CNNs) is completed when the validation loss plateaus. This is a common practice in machine learning, where the training process is continuously evaluated on a separate validation set to ensure that the model is not overfitting to the training data.

The objective of the training is to minimize the average loss, which measures the difference between the output of the final layer and the ground truth. This is done iteratively using the Stochastic Gradient Descent (SGD) algorithm, which updates the network weights based on the gradient of the loss function.

The training process is typically stopped when the validation loss does not decrease for a certain number of epochs, indicating that the model has reached a state of optimal performance on the validation set. This is known as early stopping, and it helps prevent overfitting by stopping the training process before the model starts to memorize the training data.

In addition to early stopping, other criteria such as patience and minimum delta can also be used to determine when training is complete. Patience determines the number of epochs to wait before stopping the training if the validation loss does not decrease, while minimum delta determines the minimum decrease in validation loss required to consider it as a decrease.

In the case of GoogLeNet, the losses of the two auxiliary classifiers are weighted and added to the total loss of each training iteration. This helps in improving the performance of the model by providing additional supervision to the intermediate layers.

Furthermore, data augmentation and transfer learning can also be used to improve the performance of the model when a large labeled training set is not available. Data augmentation involves creating new training instances by applying transformations such as rotation, scaling, and flipping to the existing data. Transfer learning involves using a pre-trained model as a starting point for training, which helps in reducing the amount of training data required.

In conclusion, the training of deep CNNs is completed when the validation loss plateaus, indicating that the model has reached a state of optimal performance on the validation set. Early stopping, patience, and minimum delta are some of the criteria used to determine when training is complete. Data augmentation and transfer learning can also be used to improve the performance of the model when a large labeled training set is not available.
The provided context discusses the use of machine learning algorithms, including deep learning models, for water quality parameter mapping. However, it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. Nonetheless, I can provide some general information about common regularization techniques used in deep learning.


In deep learning pipelines, overfitting is a common issue where the model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization methods are employed to mitigate overfitting by adding a penalty term to the loss function, which encourages the model to have simpler weights and avoid learning the noise in the training data.

Two popular regularization techniques used in deep learning are L2 regularization and dropout.

1. L2 regularization: This method adds a penalty term proportional to the squared sum of the model's weights. By doing so, it encourages the model to have smaller weights, reducing the risk of overfitting. L2 regularization is also known as weight decay or ridge regression.

2. Dropout: This technique randomly sets a fraction of the neurons in a layer to zero during training. This process prevents the neurons from co-adapting too much, forcing them to learn more robust features. As a result, dropout reduces overfitting and improves the model's generalization performance.

While the provided context does not explicitly mention these regularization techniques, it is possible that they were employed in the deep learning models used in the study.
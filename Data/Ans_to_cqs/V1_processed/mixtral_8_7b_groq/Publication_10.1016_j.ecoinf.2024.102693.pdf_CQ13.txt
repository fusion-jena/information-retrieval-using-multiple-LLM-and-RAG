The hyperparameters used in the deep learning models mentioned in the context are provided in Table 1. This table includes models such as ResNet-50, ViT-S/16, Volo-d1, and ViP-Small/7. 

 For the ResNet-50 model, the learning rate is 1e-4, the minimum learning rate is 2e-3, the optimizer is Adamw(0.9, 0.999), the scheduler is Cosine, the batch size is 128, the weight decay is 5e-4, the input size is 224 × 224, and the number of epochs is 210.

 The ViT-S/16 model has a learning rate of 64, a minimum learning rate of 5e-4, an optimizer of Adamw(0.9, 0.999), a scheduler of Cosine, a batch size of 64, a weight decay of 8e-6, an input size of 224 × 224, and the number of epochs is 210.

 The Volo-d1 model has a learning rate of 1e-5, a minimum learning rate of 5e-4, an optimizer of Adamw(0.9, 0.999), a scheduler of Cosine, a batch size of 64, a weight decay of 4e-6, an input size of 224 × 224, and the number of epochs is 210.

 Lastly, the ViP-Small/7 model has a learning rate of 2e-3, a minimum learning rate of 1e-5, an optimizer of Adamw(0.9, 0.999), a scheduler of Cosine, a batch size of 64, a weight decay of 1e-8, an input size of 224 × 224, and the number of epochs is 210.

 These hyperparameters were used in the deep transfer-learning strategy based on fine-tuning, where the model parameters were initialized using a pre-trained model, and the final classification layer was replaced by a fully connected layer, aligning its output dimension with the number of classes in the target dataset. Subsequently, all parameters across the layers were fine-tuned on the target dataset.
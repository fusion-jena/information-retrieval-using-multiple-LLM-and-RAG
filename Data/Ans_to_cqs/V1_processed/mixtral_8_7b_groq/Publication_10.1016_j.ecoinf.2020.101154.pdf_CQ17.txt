The provided context discusses the methods used to prevent overfitting in a Gradient Boosting Regression Tree (GBRT) model, rather than a deep learning pipeline. However, I will still provide an answer related to the mentioned methods.

In the context, the early stopping technique is used to prevent overfitting in the GBRT model. Early stopping is a form of regularization that involves monitoring the model's performance on a validation set during training. Training is halted when the model's performance on the validation set stops improving or begins to degrade, preventing the model from overfitting to the training data.

In addition, the learning rate and maximum tree depth are set to specific values to control the complexity of the GBRT model. The learning rate, also known as a shrinkage parameter, determines the contribution of each tree to the model. By setting the learning rate to a low value, the contribution of each tree is reduced, which helps to prevent overfitting. The maximum tree depth, which reflects the degree of interaction in the model, is usually kept low. GBRT performs best using fairly shallow trees, so-called tree stumps. By limiting the depth of the trees, the model's complexity is reduced, which also helps to prevent overfitting.

However, the context does not mention any other regularization methods commonly used in deep learning pipelines, such as dropout or L2 regularization. Dropout is a regularization technique that involves randomly dropping out neurons during training to prevent over-reliance on any single neuron. L2 regularization, also known as weight decay, involves adding a penalty term to the loss function that is proportional to the square of the model's weights. This encourages the model to learn smaller weights, which helps to prevent overfitting.

Therefore, while the context provides information on regularization methods used in GBRT models, it does not specifically address the query regarding regularization methods used in deep learning pipelines.
The provided context does not give specific information about the hardware resources used for training the deep learning models. However, it does mention that Extreme Learning Machines (ELMs) are used for analyzing big data, and they have the potential to generalize based on their training data set. ELMs are a type of Single-Hidden Layer Feed Forward Neural Network (SLFFNN) that can approach any continuous function and classify any discontinuous areas. They can accurately learn from a large number of samples, and their learning speed can be thousands of times greater than traditional Back Propagation Feed Forward Neural Networks (BP\_FFNN).

Deep neural networks, including ELMs, can be computationally intensive, and training them can take a significant amount of time and resources. Therefore, specialized hardware, such as Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), are often used to accelerate the training process. GPUs are particularly well-suited for parallel computations, making them ideal for training deep learning models. TPUs, on the other hand, are custom-built by Google for machine learning tasks and can deliver up to 4 trillion operations per second.

While the provided context does not explicitly mention the use of GPUs or TPUs for training ELMs, it is reasonable to assume that they could be used to accelerate the training process, especially when dealing with large datasets. However, without further information, it is not possible to provide a definitive answer to the query.
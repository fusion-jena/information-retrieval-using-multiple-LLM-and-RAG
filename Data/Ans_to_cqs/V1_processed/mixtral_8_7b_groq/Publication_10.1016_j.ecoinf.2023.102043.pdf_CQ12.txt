The type of deep learning model used in the pipeline is YAMNet, which is a pre-trained neural network model developed by Google's TensorFlow team. YAMNet is based on the MobileV1 convolution architecture and is trained on the AudioSet-YouTube corpus, which contains over 521 classes. Specifically, the model is used for extracting embeddings from audio data, which are then fed as input to a sequential and reduce mean layer for mapping to the available labels.

The audio data is pre-processed before being fed into the YAMNet model. First, the audio is converted to a monophonic channel and resampled at 16 kHz. Then, the spectrogram is extracted using short-time Fourier transform (STFT) magnitudes of 25 ms overlapped by 15 ms. After that, the Mel spectrogram is calculated by mapping to 64 bins of the Mel scale within the range of 125â€“7500 Hz.

The YAMNet-based classification algorithm was then put into production to assist in the finer annotation of the detected vocalizations. The algorithm is updated regularly with new data as extracted by the clustering process to further improve its performance.

In summary, the deep learning model used in the pipeline is YAMNet, which is a pre-trained neural network model based on the MobileV1 convolution architecture. The model is used for extracting embeddings from audio data, which are then fed as input to a sequential and reduce mean layer for mapping to the available labels. The audio data is pre-processed before being fed into the YAMNet model, and the algorithm is updated regularly with new data to improve its performance.
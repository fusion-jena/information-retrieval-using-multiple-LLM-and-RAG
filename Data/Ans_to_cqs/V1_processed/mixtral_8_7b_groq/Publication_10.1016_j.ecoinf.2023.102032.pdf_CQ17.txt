Based on the provided context, there is no specific information about the regularization methods used to prevent overfitting in the deep learning pipeline for the study of wetlands in East Africa.

However, it is general practice in deep learning to use regularization techniques to prevent overfitting. Some common regularization methods include L1 and L2 regularization, dropout, and early stopping.

L1 and L2 regularization are techniques that add a penalty term to the loss function, which discourages the model from learning overly complex relationships in the training data. Dropout is a technique that randomly sets a fraction of the model's activations to zero during training, which helps to prevent co-adaptation of features and improves the model's ability to generalize. Early stopping is a technique that stops training when the model's performance on a validation set stops improving, which helps to prevent overfitting to the training data.

Therefore, while the specific regularization methods used in the study are not mentioned, it is likely that one or more of these techniques were used to prevent overfitting in the deep learning pipeline.
Based on the provided context, there is no explicit information about the strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, there are some details about the deep learning models and the detection algorithms used in the study that can be analyzed.

The deep learning models used in this study are Convolutional Neural Networks (CNNs), specifically CNNFULL and CNNFAST. These models are used for a binary classification problem to predict the presence of a search-phase bat call at every instance of time in the spectrogram. The training of these models involves random initialization of weights and biases, which can lead to different results in each training run. However, the context does not provide information on whether a fixed random seed value was used to ensure reproducibility of the training process.

Additionally, the study compares the performance of the deep learning models with other baselines, including a Random Forest model. Random Forest is a machine learning algorithm that uses ensemble learning methods and bagging to improve the accuracy and stability of the model. Bagging involves creating multiple subsets of the training data with replacement and training a decision tree on each subset. The final prediction is made by averaging the predictions from all the decision trees. This process can help reduce the variance in the model and improve the stability, but it also introduces randomness due to the random selection of samples for each subset. However, the context does not provide information on whether a fixed random seed value was used in the Random Forest model.

In summary, based on the provided context, there is no explicit information about the strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, it is common practice in machine learning and deep learning to use fixed random seed values to ensure reproducibility and stability of the models. Future studies should provide more details on the randomness handling strategies used in their pipelines to improve the transparency and reproducibility of their results.
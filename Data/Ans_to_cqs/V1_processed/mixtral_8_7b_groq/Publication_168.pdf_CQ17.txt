Deep learning methods, such as Convolutional Neural Networks (CNNs), have been widely used in Hyperspectral Image (HSI) classification tasks and have achieved remarkable results. One of the challenges in designing these networks is preventing overfitting, which can be addressed using regularization methods.

Regularization methods are techniques used to prevent overfitting by adding a penalty term to the loss function, which encourages the model to have simpler weights and avoid memorizing the training data. Two commonly used regularization methods are dropout and L2 regularization.

Dropout is a regularization technique that randomly sets a fraction of the input units to zero during training, which helps prevent overfitting by reducing the co-adaptation of feature detectors. In other words, dropout encourages the model to learn more robust features that can generalize well to unseen data.

L2 regularization, also known as weight decay, is a technique that adds a penalty term to the loss function that is proportional to the square of the magnitude of the weights. This penalty term encourages the model to have smaller weights, which helps prevent overfitting by reducing the complexity of the model.

In the context of HSI classification using deep learning methods, regularization techniques have been used to improve the generalization performance of the models. For example, Y. Chen et al. [6] designed a new CNN model with regularization based on the 3-D convolutional to solve the HSI classification task. The regularization term used in this model is not specified, but it is mentioned that it helped prevent overfitting and improved the classification performance.

Moreover, other regularization techniques have been proposed in the literature, such as data augmentation, early stopping, and batch normalization. Data augmentation is a technique that generates new training samples by applying random transformations to the existing data, which helps increase the diversity of the training data and prevent overfitting. Early stopping is a technique that stops the training process before the model starts overfitting, which is determined by monitoring the performance of the model on a validation set. Batch normalization is a technique that normalizes the inputs of each layer, which helps reduce the internal covariate shift and improve the training stability.

In summary, regularization methods are essential techniques to prevent overfitting in deep learning pipelines, such as CNNs used in HSI classification tasks. Dropout and L2 regularization are two commonly used regularization methods that help prevent overfitting by reducing the co-adaptation of feature detectors and the complexity of the model, respectively. Other regularization techniques, such as data augmentation, early stopping, and batch normalization, have also been proposed in the literature to improve the generalization performance of deep learning models.
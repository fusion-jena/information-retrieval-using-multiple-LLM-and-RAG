The provided context does not directly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does discuss the use of deep learning techniques in the context of camera trap data and species identification. Regularization methods like dropout and L2 regularization are commonly used in deep learning to prevent overfitting.

Dropout is a regularization technique used during training to prevent over-dependence on any one neuron by randomly dropping out, or "turning off", neurons in a layer during training with a certain probability (often 0.5). This helps to ensure that the model does not become too reliant on any single neuron and improves its ability to generalize to unseen data.

L2 regularization, also known as weight decay, is a method that adds a penalty term to the loss function, proportional to the square of the magnitude of the weights. This encourages the weights to be small, reducing the complexity of the model and helping to prevent overfitting.

The context does mention the use of a Mask R-CNN with Flow-Guided Feature Aggregation by Schindler and Steinhage (2021) for optimizing instance segmentation of animal species in video clips. Mask R-CNN is a deep learning model that incorporates both object detection and semantic segmentation, and it is possible that regularization methods like dropout and L2 regularization were used during training to prevent overfitting.

Additionally, the context discusses the development of low power neural networks for hardware implementation, which can help to reduce overfitting by limiting the complexity of the model. By implementing a neural network on an FPGA, for example, the number of parameters can be reduced, which can help to prevent overfitting.

In summary, while the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, it is likely that techniques like dropout and L2 regularization were used during training. Furthermore, the development of low power neural networks for hardware implementation can help to reduce overfitting by limiting the complexity of the model.
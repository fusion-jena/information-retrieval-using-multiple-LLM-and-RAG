The deep learning models used in the study include ResNet-50, Inception-V3, and ViT. For ResNet-50, the optimizer used is Adam with a learning rate of 0.0001. For Inception-V3, the optimizer used is Adam with a learning rate of 0.0001 as well, since AdamW is not available in Keras. For the ViT model, the optimizer used is AdamW with a learning rate of 1e-4. The batch size used for training is 16 examples for all models. The learning rate for Deep Otolith ResNet is 0.0004, as suggested by Politikos et al. (2021). For every fine-tuning experiment, the model with the smallest validation loss was used for evaluation on the test set.

In addition to the deep learning models, the study also used ridge regression and support vector classification (SVC) models. For ridge regression, the hyperparameter value chosen was α = 6.0, based on a hyperparameter search that showed robust performance for α values in the interval [5.0, 15.0]. For SVC, the hyperparameter value chosen was C = 0.1, based on a hyperparameter search that explored C values in the interval [0.001, 1].

Overall, the study used a variety of hyperparameters for the different models, chosen based on hyperparameter searches and previous work. The specific hyperparameters used for the deep learning models are Adam or AdamW optimizer with a learning rate of 0.0001 or 0.0004, and a batch size of 16 examples.
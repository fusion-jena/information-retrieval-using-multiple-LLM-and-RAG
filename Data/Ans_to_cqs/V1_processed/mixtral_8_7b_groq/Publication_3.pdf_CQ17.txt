The provided context discusses the use of deep learning, specifically TensorFlow and convolutional neural networks (CNNs), for image classification tasks. However, it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline.

However, based on general knowledge of deep learning techniques, there are several regularization methods commonly used to prevent overfitting, such as dropout and L2 regularization.

Dropout is a regularization technique used during training to prevent overfitting by randomly dropping out, or "turning off," a proportion of neurons in a layer of the neural network. This helps to break up co-adaptations between neurons, making the network more robust and less prone to overfitting.

L2 regularization, also known as weight decay, is a technique that adds a penalty term to the loss function, which is proportional to the square of the magnitude of the model's weights. This encourages the model to learn smaller weights, reducing the complexity of the model and helping to prevent overfitting.

Therefore, while the provided context does not specifically mention these regularization techniques, it is likely that they were used in the deep learning pipeline to prevent overfitting.
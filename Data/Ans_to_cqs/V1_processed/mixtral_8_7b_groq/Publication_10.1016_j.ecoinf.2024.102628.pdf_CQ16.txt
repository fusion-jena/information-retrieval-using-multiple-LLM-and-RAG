Based on the provided context, there is no explicit information given about the specific criteria used to determine when training is complete, such as a validation loss plateau. The text focuses on describing the data sampling methods and cross-validation strategies used in the study, as well as providing the results of the model's performance.

However, it can be inferred that the study followed a standard machine learning workflow, which typically includes monitoring the model's performance on a validation set during training. A common approach is to stop training when the validation loss stops decreasing or shows only negligible improvement, indicating that the model has likely found a good fit to the training data and is at risk of overfitting if training continues.

Another possible criterion for determining training completion is based on the number of training epochs or iterations. In the given context, the study involved a cross-validation strategy with 216 unique iterations, each excluding a different combination of one multitemporal transect from each class. If the training process was set to run for a fixed number of epochs, the training would be complete once all of these iterations were finished.

In summary, while the provided context does not give specific details about the criteria used to determine training completion, it can be inferred that the study likely followed standard machine learning practices, such as monitoring validation loss or fixing the number of training epochs.

 The provided context does not offer explicit information on the criteria used to determine when training is complete. However, it can be inferred that the study probably followed standard machine learning practices, such as monitoring validation loss or fixing the number of training epochs.
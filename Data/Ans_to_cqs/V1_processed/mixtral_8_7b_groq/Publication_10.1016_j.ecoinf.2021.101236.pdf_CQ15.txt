The provided context does not explicitly mention the specific optimization techniques used in the deep learning pipeline. However, it is mentioned that the overall process follows current best practices in deep learning as summarized in (Chollet, 2017). Therefore, it is likely that a commonly used optimization technique was employed.

In the context of deep learning, stochastic gradient descent (SGD) and its variants, such as Adam, are widely used optimization techniques. SGD is a simple optimization algorithm used to minimize the loss function by iteratively updating the model parameters in the direction of the negative gradient of the loss with respect to the parameters. Adam, on the other hand, is an adaptive optimization algorithm that combines the advantages of SGD and other methods like Adagrad and RMSProp. It adjusts the learning rate for each parameter individually based on the estimated first and second moments of the gradient.

Considering the popularity and suitability of these optimization techniques in deep learning, it is reasonable to assume that either SGD or Adam (or a similar adaptive method) was used in this pipeline. However, without explicit information in the context, it is not possible to provide a definitive answer.
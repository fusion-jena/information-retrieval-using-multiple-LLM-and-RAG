The data was split for deep learning model training using a randomization method, where 80% of the points were allocated for training and the remaining 20% for validation. This randomization ensured that the data points in each set were independent, and a buffer of 15 meters was applied around all sample points to further guarantee this independence.

Before splitting the data, a search space of primary hyperparameters was defined, and optimization algorithms were executed to find the highest accuracy from 1,788,160 hyperparameter combinations. The model was trained for 200 epochs for each trial to find three DNN architectures, which were then used to build three models and train them for the entire dataset for 1000 epochs.

The hyperparameters that were optimized included adjustable parameters that control network architecture, optimization procedure, and regularization. Common hyperparameters included the number of layers and nodes, learning rate, and dropout rate. An automated optimization architecture was adopted, comparing three architecture optimization algorithms: Random Search, Hyperband, and Bayesian optimization.

To prevent overfitting and reduce statistical noise, a dropout layer was added, which randomly removed different sets of neurons using a pre-determined dropout rate. This resulted in training different neural networks and reduced overfitting.

Therefore, the criteria used to split the data for deep learning model training included a randomization method and a buffer zone, while the optimization of hyperparameters, including the number of layers, nodes, learning rate, and dropout rate, was performed to ensure the highest accuracy of the model.
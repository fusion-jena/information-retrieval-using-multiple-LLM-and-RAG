The context provided does not give specific criteria for determining when training is complete. However, it does mention that the value of the hyperparameter λ changes based on the current number of training rounds (e) and the total number of training rounds (E). Specifically, as e approaches E, λ approaches 1, and A'(x) approaches x. This suggests that the nonlinear factors of the activation function are gradually integrated into the model for classification decisions as training progresses.

In addition, the context mentions the use of structural re-parameterization techniques, which decouple the training structure from the inference structure and enable independent optimization of the training and reasoning stages. This allows the training phase to capture more feature information, while the inference phase simplifies the model structure, reducing computational costs and deployment difficulties.

While these details are informative, they do not provide a clear answer to the query about the criteria used to determine when training is complete. In general, training of machine learning models is considered complete when the validation loss plateaus or decreases only slightly over a number of training iterations. This indicates that the model has learned the underlying patterns in the training data and is unlikely to improve its performance significantly by continuing to train. However, this specific criterion is not mentioned in the provided context.
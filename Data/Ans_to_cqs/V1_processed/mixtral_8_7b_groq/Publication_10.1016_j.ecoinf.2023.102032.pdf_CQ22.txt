The provided context does not contain specific information about the metrics used to evaluate the performance of the deep learning model. However, it is mentioned that the study uses a deep learning model for cloud detection in Sentinel-2 images.

In general, when evaluating the performance of a deep learning model, several metrics can be used, such as accuracy, precision, recall, and F1 score.

Accuracy is the ratio of the number of correct predictions to the total number of predictions. It is a common metric for evaluating the performance of a model, but it may not be the best metric when the classes are imbalanced.

Precision is the ratio of the number of true positives (correctly predicted positive instances) to the total number of predicted positive instances. It measures the proportion of correct positive predictions out of all positive predictions made by the model.

Recall is the ratio of the number of true positives to the total number of actual positive instances. It measures the proportion of correctly predicted positive instances out of all actual positive instances.

The F1 score is the harmonic mean of precision and recall, which gives equal weight to both metrics. It is a more balanced metric than accuracy, especially when the classes are imbalanced.

Therefore, based on common practices in evaluating deep learning models, it can be inferred that the study might have used one or more of these metrics (accuracy, precision, recall, or F1 score) to evaluate the performance of their cloud detection model. However, without explicit information from the context, it is not possible to provide a definitive answer.
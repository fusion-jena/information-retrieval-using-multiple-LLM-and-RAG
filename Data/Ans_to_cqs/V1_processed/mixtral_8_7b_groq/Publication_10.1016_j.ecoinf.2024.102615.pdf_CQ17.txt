The pieces of context provided describe the use of dropout as a regularization method to prevent overfitting in a deep learning pipeline. Specifically, the percentage value of dropout layers was one of the hyperparameters that was optimized in the MLP model's architecture. Dropout is a regularization technique that is commonly used in deep learning to prevent overfitting by randomly dropping out a proportion of the neurons in a layer during training (Srivastava et al., 2014). By doing so, dropout helps to promote the independence of the neurons and reduces the co-adaptation of feature detectors, which can improve the model's generalization performance.

In the study described, the percentage values of the dropout layers were optimized using the grid search method, which involved comparing the values of evaluation metrics for different parameters and selecting the optimal parameters from the search space. The specific tuning ranges of the hyperparameters, including the dropout layers, were determined based on a review of existing literature.

Therefore, based on the provided pieces of context, dropout is a regularization method that was used to prevent overfitting in the deep learning pipeline. However, the context does not mention the use of any other regularization methods, such as L2 regularization. L2 regularization, also known as weight decay, is another regularization technique that is commonly used in deep learning to prevent overfitting by adding a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This penalty term encourages the model to learn smaller weights, which can help to reduce overfitting.

In summary, the pieces of context provided indicate that dropout was used as a regularization method to prevent overfitting in the deep learning pipeline. However, the context does not mention the use of any other regularization methods, such as L2 regularization.
Based on the information provided, while the specific hyperparameters used in the proposed model are not explicitly stated in the text, it does mention the use of learning rate and L2 regularization. Therefore, we can infer that these are among the hyperparameters used in the model.

The learning rate determines the size of the steps that the model takes while going down the “error surface” during training. If the learning rate is too high, the model might overshoot the optimal solution; if it is too low, the training will be slow. Thus, choosing an appropriate learning rate is crucial for the performance of the model.

L2 regularization, also known as weight decay, is a technique to prevent overfitting by adding a penalty term to the loss function, which is proportional to the sum of the squares of all the model’s weights. This encourages the model to keep its weights small, making it less likely to learn noise in the training data.

Unfortunately, the text does not provide enough information to determine the specific values of these hyperparameters. It only mentions that the results were obtained “under the condition of unchanged training methods and parameters.” Therefore, without additional information, it is not possible to give a more precise answer.
The provided context does not give a direct answer to the query regarding the specific criteria used to determine when training is complete. However, it does offer some insights that can help us infer possible criteria.

During the training of the neural network, the validation loss was consistently lower than the training loss before the overfitting point. This suggests that the training process was being monitored using both training and validation loss, and training was potentially stopped when the validation loss started to plateau or increase, indicating that the model was no longer improving its ability to generalize from the training data to new, unseen data.

Additionally, the use of dropout layers, which randomly drop some neurons during training, can explain the consistent lower validation loss. When dropout is deactivated during validation, all neurons are kept, potentially leading to better performance on the validation set. This could also be a criterion for determining when to stop training, as improved performance on the validation set despite the use of dropout could indicate that the model is ready for use.

In summary, while the provided context does not explicitly state the criteria used to determine when training is complete, it suggests that monitoring both training and validation loss, as well as observing the impact of dropout layers on validation performance, could be used to make that determination.
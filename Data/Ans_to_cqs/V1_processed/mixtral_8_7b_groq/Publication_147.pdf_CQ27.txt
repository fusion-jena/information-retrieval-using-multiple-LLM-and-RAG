Based on the information provided, it is not possible to explicitly determine the process that was followed to deploy the trained deep learning model. However, we can infer some steps that were likely taken based on common deep learning practices and some details in the context.

Firstly, after training the deep learning model, which was a Convolutional Neural Network (CNN) in this case, the model would need to be serialized, i.e., saved in a way that it can be loaded and used later without having to retrain it. This is a common practice in deep learning to avoid retraining the model every time it is needed, which can be time-consuming and computationally expensive.

Secondly, the platform for deployment would need to be selected. This could be a local machine, a server, or a cloud platform, depending on the intended use of the model. Given that the training and prediction were done on a desktop with specific hardware (64 GB of RAM and one Titan X (Maxwell) GPU), it is possible that the model was deployed on the same desktop. However, the context also mentions that the prediction time for the CNN model can be reduced by distributing it to more cores or more machines, suggesting that the model could also be deployed on a server or a cloud platform for larger scale predictions.

Lastly, the model would need to be integrated into an application or a workflow for it to be useful. This could involve loading the serialized model, preparing input data in the correct format, running the model on the input data, and handling the output. The context does not provide details on how the model was integrated, but it does mention that the model was used for land cover classification in a remote sensing context.

In summary, while the context does not provide explicit details on the process followed to deploy the trained deep learning model, it is likely that the model was serialized, the platform for deployment was selected, and the model was integrated into a remote sensing workflow.
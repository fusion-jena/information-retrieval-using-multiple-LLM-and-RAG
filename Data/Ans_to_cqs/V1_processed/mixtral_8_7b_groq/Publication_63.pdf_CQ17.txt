The text provided does not explicitly mention the use of any regularization methods such as dropout or L2 regularization to prevent overfitting in the deep learning pipeline. However, it does mention that models were trained until the validation MAE did not further improve compared to the preceding epochs and diverged from the training MAE, which is a form of early stopping, a regularization technique used to prevent overfitting. Additionally, the authors mention that they employed transfer learning by using pre-trained layer weights from a classification task on the ImageNet dataset for all CNN models used in this study. Pre-trained models can act as a form of regularization by providing a strong prior and reducing the capacity of the model, making it less likely to overfit.

Another regularization technique that could have been used but is not mentioned in the text is data augmentation. Data augmentation is a technique that can increase the size of the training set by generating new synthetic samples through various transformations such as rotation, scaling, and flipping of the original images. This can help to reduce overfitting by providing the model with more diverse training data and increasing its generalization ability. The text mentions that a random sample of 10% of the dataset of each trait was extracted as a test dataset, but it does not mention if data augmentation was used on the training dataset.

In summary, while the text does not explicitly mention the use of dropout or L2 regularization, it does mention the use of early stopping and transfer learning as regularization techniques to prevent overfitting. Additionally, data augmentation could have also been used but is not mentioned in the text.
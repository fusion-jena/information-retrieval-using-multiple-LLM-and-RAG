In the study, a strategy was implemented to monitor the model performance during training by using bias-variance decomposition and averaging the predictions of each model trained on different subsets of the original training set. Specifically, eleven machine learning models, including multilayer perceptron, logistic regression, na√Øve Bayes, CatBoost, random forest, XGBoost, LightGBM, weighted soft voting, weighted hard voting, blending, and stacking, were utilized. The hyperparameters of the heterogeneous ensemble models were optimized using the Tree-structured Parzen Estimator (TPE) algorithm, which estimates the probability distribution of the hyperparameters using a probabilistic model and iteratively constructs a tree of Gaussian distributions to approximate the objective function.

To monitor the model performance during training, each ML model was trained on 100 different subsets generated from the original training sets using the bootstrap method, which involves sampling with replacement. The number of samples in each subset equaled the number of samples in the original training data. The predictions of the trained models on the testing dataset were then averaged to obtain the main prediction. The average loss, average bias, and average variance were derived for each model. The noise was assumed as 0, following previous studies.

The use of bias-variance decomposition and averaging the predictions of each model trained on different subsets of the original training set allowed for a more robust estimation of the model performance during training. By using the TPE algorithm to optimize the hyperparameters of the heterogeneous ensemble models, the study ensured that the models were performing at their best, thereby increasing the generalizability of the model to unseen data and its potential applicability as an assistant tool for decision-making.

In summary, the strategy implemented to monitor the model performance during training involved using bias-variance decomposition, averaging the predictions of each model trained on different subsets of the original training set, and optimizing the hyperparameters of the heterogeneous ensemble models using the TPE algorithm.
Based on the information provided, there is no direct mention of strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, there are some measures mentioned that can help reduce overfitting and improve generalization, which are related to handling randomness.

In the model configuration section, a dropout value of 50% is used in the first three layers to improve generalization and reduce overfitting. Dropout is a regularization technique that randomly drops out, or sets to zero, a number of output features of the layer during training. This helps prevent over-reliance on specific neurons and encourages the model to learn more robust features.

Additionally, the authors mention future work to include more species and compare the approach with more advanced network architectures, such as 1D CNNs. When working with larger and more diverse datasets, setting a random seed value can be crucial for reproducibility and ensuring that the results are not influenced by randomness in the data or the model initialization.

In summary, while the text does not explicitly mention the use of a random seed value to handle randomness in the deep learning pipeline, it does discuss measures to improve generalization and reduce overfitting, such as dropout. Furthermore, the importance of reproducibility and handling randomness becomes more apparent when working with larger and more diverse datasets, as mentioned in the future work section.
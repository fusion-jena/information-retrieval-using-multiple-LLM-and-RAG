Based on the provided context, the following measures were taken to ensure the generalizability of the deep learning model:

First, the authors used pre-trained models that were initially trained on a large and diverse dataset, ImageNet, which contains 1.4 million images from 1000 classes. This step helped the model to learn a wide range of features from different categories, enhancing its ability to generalize to new, unseen data.

Second, the authors used transfer learning, a technique that involves reusing previously attained knowledge in similar tasks. Specifically, they used pre-trained models based on MobileNetV2, MobileNetV3-Large, and MobileNetV3-Small as feature extractors. This approach allowed the model to leverage the knowledge gained from the ImageNet dataset and adapt it to the new task, improving its generalizability.

Third, the authors divided the dataset into a training set and a test set in a ratio of 75:25, with 5789 images in the training set and 1930 images in the test set. This stratified splitting helped ensure that the model was trained and tested on a diverse range of data, further enhancing its ability to generalize to new data.

Fourth, the authors added a dropout layer with a dropout rate of 0.2 to reduce overfitting. Overfitting occurs when a machine learning model learns the training data too well, including more terms and using more complicated approaches than necessary. By adding dropout, the authors introduced randomness during training, preventing the model from relying too heavily on specific features and reducing the risk of overfitting.

Finally, the authors used different optimization algorithms for the two models. The first model was compiled and optimized with the Adam optimizer, while the second model was optimized with gradient descent. Using different optimization algorithms can help ensure that the model is not overfitting to a specific optimization method and improve its generalizability.

In summary, the authors took several measures to ensure the generalizability of the deep learning model, including using pre-trained models on a large and diverse dataset, transfer learning, stratified splitting, adding dropout, and using different optimization algorithms. These measures helped ensure that the model was trained and tested on a diverse range of data, prevented overfitting, and improved its ability to generalize to new, unseen data.
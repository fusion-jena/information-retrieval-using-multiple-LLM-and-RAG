The hyperparameters used in the deep learning model include the kernel size for the first layer and the last layer, the convolutional depth for the first layer and the last layer, and the size of the fully connected layer. The model was designed with a customized architecture, inspired by previous work, to classify moth species. The hyperparameters were explored to find the optimal network architecture.

The kernel size refers to the size of the filters used in the convolutional layers. A smaller kernel size will result in a larger receptive field, while a larger kernel size will result in a smaller receptive field. The convolutional depth refers to the number of filters used in the convolutional layers. A larger convolutional depth will result in a greater ability to extract features from the input data. The fully connected layer size refers to the number of neurons in the fully connected layer. A larger fully connected layer size will result in a greater ability to learn complex relationships between the features extracted by the convolutional layers.

Additionally, the model used the Adam optimizer, which turned out to be superior to the SGD optimizer for training the model. The dropout probability was set to 0.3 after the second to last hidden layer to prevent overfitting.

In summary, the hyperparameters used in the deep learning model include the kernel size for the first layer and the last layer, the convolutional depth for the first layer and the last layer, the size of the fully connected layer, the optimizer, and the dropout probability. These hyperparameters were explored to find the optimal network architecture for classifying moth species.
Based on the provided context, there is no explicit mention of any optimization techniques such as Stochastic Gradient Descent (SGD) or Adam used in the deep learning pipeline. The study by Dujardin and Lehning (2022) focuses on comparing interpolation and downscaling techniques for predicting monthly average temperatures, and it does not delve into the specifics of the optimization techniques used in their deep learning model.

However, it is important to note that the selection of optimization techniques depends on the specific deep learning algorithm being used. For instance, in the case of Artificial Neural Networks (ANNs), which are mentioned in the study, optimization techniques such as SGD, Adam, or RMSprop are commonly used.

Therefore, while the provided context does not explicitly answer the query, it is reasonable to assume that optimization techniques such as SGD or Adam were used in the deep learning pipeline, given their widespread use in training ANNs.
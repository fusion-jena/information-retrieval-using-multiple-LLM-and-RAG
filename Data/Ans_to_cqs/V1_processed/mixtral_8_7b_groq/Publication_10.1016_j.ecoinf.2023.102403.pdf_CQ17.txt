The provided context does not contain information about the specific regularization methods used in a deep learning pipeline to prevent overfitting. However, it does discuss data cleaning and filtering for ecological data analysis.

To answer the query, I will provide an overview of common regularization techniques used in deep learning:

1. L1 and L2 regularization: These are methods to prevent overfitting by adding a penalty term to the loss function. L1 regularization, also known as Lasso regularization, adds the absolute value of the magnitude of the coefficients as a penalty. L2 regularization, also known as Ridge regularization, adds the squared magnitude of the coefficients as a penalty. These methods help in reducing the complexity of the model and improving generalization.

2. Dropout: This is a regularization technique specifically designed for neural networks. During training, dropout randomly sets a fraction of input units to 0, which helps in preventing overfitting by making the network more robust and less dependent on specific weights.

3. Early stopping: This is a form of regularization where training is halted as soon as the validation error starts increasing, even if the training error is still decreasing. This prevents the model from learning overly complex patterns in the training data that do not generalize well.

4. Data augmentation: This technique generates new training samples by applying transformations (such as rotation, scaling, or flipping) to the existing data. This increases the size of the training set, reduces overfitting, and improves the model's ability to generalize.

5. Batch normalization: This technique normalizes the inputs of each layer in the network, which helps in reducing internal covariate shift and improving generalization.

In summary, while the provided context does not contain information about regularization methods used in a deep learning pipeline, common techniques include L1 and L2 regularization, dropout, early stopping, data augmentation, and batch normalization. These methods help prevent overfitting and improve the model's ability to generalize.
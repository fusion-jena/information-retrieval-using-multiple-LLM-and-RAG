Based on the provided context, the following measures were taken to ensure the generalizability of the deep learning model:

1. Diverse dataset: The dataset used for training the model includes fish samples with different environment circumstances such as separated fish in seagrass, a shoal (a group of fish), tiny fish, occlusion by corals, and shading and light conditions (Figure 2). This diverse dataset helps the model to learn from various scenarios and improve its ability to generalize to new, unseen data.

2. Data augmentation: To extend the amount of data and further improve the model's generalizability, data augmentation techniques were applied. However, the specific details of these techniques are not provided in the context.

3. Hyperparameter tuning: The number of iterations was set to 4000, and the number of epochs was limited to 200 to prevent overfitting (Table 1). Overfitting occurs when the model learns the training data too well, including its noise and outliers, and performs poorly on new data. By limiting the number of epochs, the model's capacity to memorize the training data is restricted, which helps to improve its generalizability.

4. Batch size and subdivisions: The batch size was set to 16, and subdivisions were set to 64 (Table 1). These settings control the amount of data used in each update of the model's weights, which can affect the model's convergence and generalizability.

5. Random selection of training samples: For each epoch, 64 images were randomly selected and used to train the model (not explicitly mentioned as cross-validation but resembles the concept). This random selection helps to ensure that the model is exposed to a variety of samples during training, which can improve its generalizability.

However, the context does not mention any explicit use of cross-validation or stratified splitting techniques, which are commonly used methods to ensure the model's generalizability. Cross-validation involves dividing the dataset into multiple folds, training the model on different subsets, and evaluating its performance on the remaining portions. Stratified splitting, on the other hand, ensures that the distribution of classes is maintained in the training and validation sets, which can be crucial when dealing with imbalanced datasets.
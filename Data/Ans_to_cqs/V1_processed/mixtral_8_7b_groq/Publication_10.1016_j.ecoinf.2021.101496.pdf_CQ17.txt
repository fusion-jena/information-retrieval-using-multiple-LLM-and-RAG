The provided context discusses the use of an artificial neural network (ANN) to model SPAD values, but it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that adding hidden layers to the ANN resulted in less accuracy due to overfitting.

Regularization methods are commonly used in deep learning to prevent overfitting by adding a penalty term to the loss function, which discourages the model from learning overly complex patterns in the training data that may not generalize well to new data.

Two common regularization methods are dropout and L2 regularization. Dropout is a technique where randomly selected neurons are "dropped out" during training, which helps to prevent over-reliance on any single neuron and encourages the model to learn more robust features. L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to learn smaller weights, which can help to prevent overfitting by reducing the complexity of the model.

Based on the provided context, it is unclear whether these or any other regularization methods were used in the ANN model for SPAD value prediction. However, it is mentioned that the ANN model was constructed empirically, which suggests that various techniques may have been explored and tested during the model-building process. It is possible that regularization methods were used and found to be effective in preventing overfitting, but this information is not provided in the context.

In summary, while the provided context does not explicitly mention the regularization methods used to prevent overfitting in the ANN model for SPAD value prediction, it is mentioned that adding hidden layers resulted in overfitting, which suggests that regularization methods may have been used to address this issue. The two common regularization methods are dropout and L2 regularization, which help to prevent overfitting by adding a penalty term to the loss function and encouraging the model to learn more robust features.
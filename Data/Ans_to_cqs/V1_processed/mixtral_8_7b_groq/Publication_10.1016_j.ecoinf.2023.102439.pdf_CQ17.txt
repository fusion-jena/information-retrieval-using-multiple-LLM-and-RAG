The provided context does not contain specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of artificial neural networks (ANNs) and a hybrid model called W-ANN (wavelet analysis and ANN) for predicting vegetation biomass density.

In general, overfitting is a common issue in machine learning and deep learning models, where the model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization methods are techniques used to prevent overfitting by adding a penalty term to the loss function, which discourages the model from learning overly complex patterns in the training data.

Two common regularization methods used in deep learning are L2 regularization and dropout.

L2 regularization, also known as weight decay, adds a penalty term proportional to the square of the model's weights to the loss function. This term encourages the model to have smaller weights, which in turn makes the model less sensitive to the training data's noise and prevents overfitting.

Dropout is a regularization technique that randomly sets a fraction of the model's hidden units to zero during training. This prevents the model from relying too much on any single hidden unit and encourages the model to learn more robust features. Dropout has been shown to be an effective regularization method for deep neural networks.

Therefore, while the provided context does not mention the specific regularization methods used to prevent overfitting, it is likely that one or both of these methods were used in the ANN and W-ANN models.
The provided context does not give specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that they trained a dataset with a ResNet-101 network for 500 epochs and used image augmentations such as flip, rotations, and affine transformation. These techniques can help to some extent in preventing overfitting by providing more varied data and reducing the risk of the model memorizing the training data.

However, it is common practice in deep learning to use regularization techniques such as L2 regularization, dropout, and data augmentation to prevent overfitting. L2 regularization adds a penalty to the loss function that is proportional to the square of the magnitude of the weights, which encourages the model to keep the weights small and avoid overcomplexity. Dropout is a technique where randomly selected neurons are "dropped out" during training, which helps to prevent the co-adaptation of neurons and improves the model's ability to generalize. Data augmentation, as mentioned in the context, can also help to increase the amount of training data and reduce overfitting by creating modified versions of the existing data.

Therefore, while the provided context does not give specific information about the regularization methods used, it is reasonable to assume that they used common regularization techniques such as L2 regularization and dropout to prevent overfitting.
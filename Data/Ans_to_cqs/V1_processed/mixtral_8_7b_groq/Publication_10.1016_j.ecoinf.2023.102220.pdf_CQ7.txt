The provided context does not give specific details about the criteria used to split the data for training, testing, and validation of the deep learning models, including the Deep Neural Networks (DNNs) and Long Short Term Memory (LSTM) models. However, it does mention that they used different percentages of the initial observations of the time series for training and the complement as test sets. Specifically, they selected 40%, 50%, 60%, and 70% of the initial observations for training and used the complement for testing.

Moreover, the context also states that they compared the performance of the PBP method with Random Forests (RF), Support Vector Machines (SVM), Deep Neural Networks (DNNs), and Long Short Term Memory (LSTM) algorithms. To obtain the algorithm performance using these competing methodologies, they created matrices P with m = 1, 4, and 7. Another matrix that did not contain outbreaks was generated with the same m observations before a threshold lower than the population size x* = 200 that defines an outbreak of aphids in the study area. For RF, they used 2 splitting predictors per tree (for m = 4 and 7) and a total of 1000 trees. For SVM, they used the linear kernel. For DNN, after experimenting with different architectures, they used 17 hidden layers.

However, the context does not provide information on how they split the data for training, validation, and testing of the DNN and LSTM models specifically. Therefore, I cannot provide a specific answer to the query based on the given context.
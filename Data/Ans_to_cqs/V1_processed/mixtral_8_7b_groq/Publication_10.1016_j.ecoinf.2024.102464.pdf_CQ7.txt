The provided context does not directly mention the criteria used to split the data for deep learning model training into sets such as train, test, and validation. However, it does discuss the methodology used in a study involving machine learning models for species distribution modeling.

In this study, seven different machine learning models were employed to assess the impact of urban development on species distribution. The models' performance was evaluated using the Area Under the Curve (AUC) values before and after urban development. The AUC value is a critical measure of model performance, and an average AUC value of 0.84 indicated robust performance across the models.

Before implementing the models, a correlation analysis of the variables was conducted to enhance predictive performance and mitigate the risk of overfitting. The variables used in the species distribution model results included land cover, vegetation (forest type, age class, diameter, and density), terrain (Dem, Aspect, Hill Shade, Slope, and TWI), and distance (distance to water, forests, agricultural land, grassland, roads, and urban areas).

In the context of deep learning model training, data splitting is crucial for evaluating model performance and preventing overfitting. Common practices for data splitting include using a training set for model learning, a validation set for tuning hyperparameters and selecting the best model, and a test set for final model evaluation.

While the context does not explicitly mention these criteria, it is reasonable to assume that a similar approach was followed, given the methodological similarities between machine learning and deep learning models. Therefore, it can be inferred that the data was likely split into training, validation, and test sets for model training, hyperparameter tuning, and final evaluation, respectively. However, this information is not directly provided in the context, so it is not possible to give a definitive answer.
Based on the provided context, there is no explicit information given about how the hyperparameters of the model were optimized. There is no mention of techniques such as grid search, random search, or any other optimization method. The text only provides details about the chosen hyperparameters for the model, which include a filter size of 2, Backpropagation as the learning algorithm, Adam as the optimizer, and a dropout value of 50% in the first three layers. It also mentions that the first three layers are composed of 256 nodes, while the final layer is equal to the number of classes in the dataset. However, the way these specific hyperparameters were determined is not discussed. Therefore, I cannot provide an answer to the query using the given context.
The text does not explicitly mention the use of dropout or L2 regularization to prevent overfitting. However, it does describe other regularization techniques that were employed in the deep learning pipeline.

 To prevent overfitting, the models were trained using a stochastic gradient descent (SGD) optimizer with an initial learning rate of 0.01 and a cycle learning rate of 0.2. The learning rate decay occurred after specific iterations, which can help in preventing overfitting by reducing the learning rate as the model becomes more confident.

 Additionally, data augmentation techniques such as scaling, panning, rotation, color transformation, and mosaic were used to increase the number of training samples and improve model generalization ability. Data augmentation is a powerful regularization technique that can help prevent overfitting by creating new training examples through various transformations of the original data.

 Another regularization technique used in the study was the use of pretrained weights. Pretrained weights were loaded to evaluate their impact on detection results, which can help in preventing overfitting by initializing the model with weights that have already been trained on a large dataset.

 Finally, the models were trained for 300 epochs and evaluated on the validation set after each epoch. Evaluating the model on a validation set after each epoch can help in preventing overfitting by providing an unbiased estimate of the model's performance on new data.

 Therefore, while the text does not explicitly mention the use of dropout or L2 regularization, it does describe other regularization techniques such as learning rate decay, data augmentation, pretrained weights, and validation set evaluation that were used in the deep learning pipeline to prevent overfitting.
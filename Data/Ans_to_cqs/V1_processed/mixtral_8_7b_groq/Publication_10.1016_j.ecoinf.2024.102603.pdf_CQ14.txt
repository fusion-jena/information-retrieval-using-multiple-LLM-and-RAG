Based on the provided context, there is no explicit information about how the hyperparameters of the XGBoost model were optimized in the study conducted by Wu, J., Li, Y., Ma, Y., 2021. However, it is mentioned that the XGBoost model was used and it is a model that does not have an analytical form, but is obtained through numerical optimization of the error function in successive iterations.

Hyperparameter tuning is an essential step in the machine learning process to ensure the best performance of a model. There are several methods for hyperparameter tuning, including grid search, random search, and Bayesian optimization. Grid search involves testing all possible combinations of hyperparameters within a specified range, while random search selects a subset of hyperparameters randomly and tests their performance. Bayesian optimization, on the other hand, uses a probabilistic approach to select the most promising hyperparameters to test.

Given that the study by Wu, J., Li, Y., Ma, Y., 2021 did not provide information on how the hyperparameters were optimized, it is not possible to give a specific answer. However, it is mentioned that the training and validation sets were divided in an 80:20 ratio, which is a common practice in machine learning. This division of data allows for the training of the model on one portion of the data and the validation on another, ensuring that the model's performance is evaluated on unseen data.

In summary, while the provided context does not explicitly state how the hyperparameters of the XGBoost model were optimized, it is mentioned that the model was obtained through numerical optimization of the error function in successive iterations. Hyperparameter tuning is an essential step in the machine learning process, and there are several methods available for this task. However, without further information, it is not possible to provide a specific answer on how the hyperparameters were optimized in the study by Wu, J., Li, Y., Ma, Y., 2021.
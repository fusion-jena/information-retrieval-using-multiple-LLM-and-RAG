The strategy implemented to monitor the model performance during training involves using a technique called randomized grid search for hyperparameter tuning, as described in the study by Bergstra and Bengio (2012). Instead of an exhaustive search through all possible combinations of hyperparameters, which can be computationally expensive and time-consuming, randomized grid search offers several advantages. It reduces computational cost, especially when dealing with a large number of hyperparameters, and has been shown to find equally or more optimal solutions than an exhaustive grid search, particularly when some hyperparameters are more influential than others.

In this study, the researchers randomly selected a subset of the parameter grid to explore. They generated various cases by combining hyperparameter values extracted from a uniform distribution and evaluated their performance. The specific hyperparameters, their ranges, and the intervals used in the grid search are presented in Table S4 in the supplementary file.

After the randomized grid search, the researchers selected the set of hyperparameter values that resulted in the minimum MAE and RMSE, as well as the highest R2, for each machine learning model. Table 1 in the study presents the resulting hyperparameter values for the four machine learning models.

By using this strategy, the researchers were able to identify the best hyperparameter values for each model, which helped strike a balance between model complexity and the dataset, reducing the risk of overfitting. This approach allowed for monitoring the model performance during training by evaluating the model's performance with different hyperparameter configurations, ensuring the selection of the optimal set of hyperparameters for the given task.
Based on the provided context, there is no explicit mention of the specific preprocessing steps taken before training the deep learning model. However, it can be inferred that some data preprocessing was done, as the performance of the models was evaluated using metrics such as the coefficient of determination (R2), root-mean-square error (RMSE), and mean absolute error (MAE). These metrics often require data to be preprocessed, particularly normalized or scaled, to ensure valid and meaningful comparisons.

In the context of machine learning and deep learning, data preprocessing typically includes the following steps:

1. Data cleaning: This step involves handling missing or corrupted data points. Common techniques include removing rows with missing values, imputing missing values using statistical methods, or using algorithms to predict missing values based on other data points.

2. Data normalization or scaling: This step is crucial for many machine learning algorithms, as it ensures that all features have similar ranges and distributions. Common techniques include min-max scaling, which scales features to a range between 0 and 1, and z-score normalization, which scales features to have a mean of 0 and a standard deviation of 1.

3. Feature engineering: This step involves creating new features from existing ones, which can help improve model performance. Techniques include polynomial feature creation, binning, and one-hot encoding for categorical variables.

4. Data splitting: This step involves dividing the dataset into training, validation, and test sets. This allows for the evaluation of model performance on unseen data and helps prevent overfitting.

Given the complexity of artificial neural networks (ANNs) and their need for large amounts of data, it is reasonable to assume that the authors of the provided context performed these preprocessing steps before training their models. However, the specific preprocessing techniques used are not mentioned in the context.
The provided context does not directly discuss the regularization methods used to prevent overfitting in deep learning pipelines. However, it does describe the use of a Res2Net module and group convolution to improve feature extraction in a target tracking algorithm. Regularization methods are crucial for preventing overfitting and improving model generalization, so I will provide a brief overview of common regularization techniques.

Regularization methods used to prevent overfitting in deep learning pipelines include:

1. L1 and L2 regularization: These are the most common types of regularization techniques. L1 regularization adds an absolute value of the magnitude of the coefficients as a penalty term to the loss function. L2 regularization, on the other hand, adds the squared magnitude of the coefficients. L2 regularization is more commonly used and helps in reducing the model complexity by penalizing large weights.

2. Dropout: Dropout is a regularization technique used during training to prevent overfitting. It works by randomly dropping out, or "turning off," neurons in a layer during training with a certain probability. This helps to prevent over-reliance on specific neurons and encourages the model to learn more robust features.

3. Early stopping: Early stopping is a form of regularization where training is halted as soon as the validation loss starts to increase. This prevents the model from continuing to train and overfit to the training data.

4. Data augmentation: Data augmentation is a technique used to increase the amount of training data by applying random transformations to the existing data, such as rotation, scaling, and flipping. This helps the model generalize better to unseen data.

5. Batch normalization: Batch normalization is a technique used to normalize the inputs of each layer in a neural network. This helps to reduce internal covariate shift, which can improve the model's generalization ability.

6. Weight decay: Weight decay is another name for L2 regularization. It adds a penalty term to the loss function that is proportional to the square of the weights. This helps to reduce the magnitude of the weights and prevent overfitting.

In summary, the provided context does not discuss regularization methods explicitly. However, regularization techniques are crucial for preventing overfitting and improving model generalization in deep learning pipelines. Common regularization methods include L1 and L2 regularization, dropout, early stopping, data augmentation, batch normalization, and weight decay.
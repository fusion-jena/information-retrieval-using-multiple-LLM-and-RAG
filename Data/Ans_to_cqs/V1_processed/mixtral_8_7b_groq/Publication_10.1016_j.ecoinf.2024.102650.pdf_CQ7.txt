The provided context does not give specific information about the criteria used to split data for deep learning model training. However, it does discuss the use of a machine learning model called ERNIE, which achieved significant results in previous studies.

In the context, the evaluation metric for ERNIE is perplexity, which is a common metric for evaluating language models. It measures how well a model predicts a sample and is calculated using the formula P(D) = ΣM d=1 ∑Nk d=1 p(θkd), where P(D) represents perplexity, D represents the test set in the corpus, M represents the number of texts, Nk represents the number of words in text d, and p(θkd) represents the probability of topic k appearing in text d.

The context also discusses the use of the pyLDAvis library for clustering and visualizing topics in tweets involving contaminated sites. The pyLDAvis interface displays the ranking of keywords related to each topic, and the relevance of these keywords can be adjusted between 0 and 1 using the λ setting.

However, none of this information pertains to the criteria used to split data for deep learning model training. Common criteria for splitting data include random sampling, stratified sampling, and temporal sampling. The choice of criteria depends on the specific characteristics of the data and the goals of the model training.

Therefore, while the provided context gives insight into the evaluation of machine learning models and the visualization of topics in text data, it does not provide information on the criteria used to split data for deep learning model training.
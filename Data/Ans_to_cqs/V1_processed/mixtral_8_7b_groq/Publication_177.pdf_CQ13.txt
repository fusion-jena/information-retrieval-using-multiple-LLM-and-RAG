The hyperparameters used in the deep learning models include batch size and initial learning rate. These hyperparameters were optimized using Bayesian optimization, which employs a Gaussian process model of the objective function to find the optimal values for each of the six models. The optimization was performed by minimizing the loss on the validation dataset. The models were trained for a maximum of 10 epochs using the stochastic gradient descent with momentum (sgdm) optimizer.

The specific values for the batch size and initial learning rate are not provided in the context. However, it is mentioned that the batch size was optimized between 1 and 32, and the initial learning rate was optimized between 1x10^-4 and 1x10^-2.

It is also worth noting that the weights for the first 10 layers for each pre-trained network were frozen, and the optimization of hyperparameters was performed using the Bayesian optimization. This approach is known as transfer learning, where a pre-trained model is used as a starting point for a new task, and the weights of the early layers are kept frozen while the weights of the later layers are fine-tuned for the new task.

In summary, the hyperparameters used in the deep learning models include batch size and initial learning rate, which were optimized using Bayesian optimization and the sgdm optimizer. The transfer learning approach was used, where the weights for the first 10 layers for each pre-trained network were frozen, and the optimization of hyperparameters was performed for the later layers.
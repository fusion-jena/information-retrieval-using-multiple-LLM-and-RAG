The optimization technique applied in the deep learning pipeline for streamflow forecasting in Bear and Winter Rivers of PEI is the Adam optimizer. This information can be inferred from the provided context, which mentions the use of the Adam optimizer in all of the listed models for both one and three days ahead forecasting. No other optimization techniques, such as SGD, are mentioned.

The Adam optimizer is a popular optimization algorithm used in deep learning models. It is an extension of stochastic gradient descent (SGD) that incorporates adaptive learning rates for each parameter. This means that the algorithm adjusts the learning rate for each parameter based on its past gradients, allowing for more efficient optimization.

In the provided context, the Adam optimizer is used in combination with other hyperparameters, such as the number of CNN layers, BiGRU layers, kernel parameter, activation function, filters, epochs, and learning rate. These hyperparameters are optimized for each model to improve the performance of the deep learning pipeline for streamflow forecasting.

It is worth noting that the learning rate for the Adam optimizer varies across the different models. For example, in the models for one day ahead forecasting, the learning rate for the Adam optimizer ranges from 0.0001 to 0.0015. Meanwhile, in the models for three days ahead forecasting, the learning rate for the Adam optimizer ranges from 0.000455 to 0.0015. This suggests that the learning rate for the Adam optimizer is fine-tuned for each model to improve its performance.

In summary, the Adam optimizer is the optimization technique applied in the deep learning pipeline for streamflow forecasting in Bear and Winter Rivers of PEI. The learning rate for the Adam optimizer varies across the different models, indicating that it is fine-tuned for each model to improve its performance. No other optimization techniques, such as SGD, are mentioned in the provided context.
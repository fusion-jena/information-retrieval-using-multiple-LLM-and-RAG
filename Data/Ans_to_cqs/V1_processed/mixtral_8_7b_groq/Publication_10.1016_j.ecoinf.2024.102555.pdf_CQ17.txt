The provided context does not contain information on the specific regularization methods used to prevent overfitting in deep learning pipelines. However, I can provide a general explanation of some common regularization techniques in deep learning.

Regularization methods are essential in deep learning to prevent overfitting, which occurs when a model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization techniques discourage overly complex models by adding a penalty term to the loss function. Here are two common regularization methods:

1. L2 regularization: This method adds a penalty term proportional to the square of the model's weights to the loss function. This encourages smaller weights, reducing the impact of individual features and helping to prevent overfitting.

2. Dropout: Dropout is a regularization technique specifically designed for neural networks. During training, dropout randomly sets a fraction of the neurons in a layer to zero, effectively preventing them from contributing to the forward pass. This helps to break up co-adaptations between neurons, reducing overfitting and improving the model's generalization performance.

These regularization techniques can be applied to various layers in a deep learning model, such as fully connected layers or convolutional layers in a neural network. The choice of regularization method depends on the specific problem, the model architecture, and the available data.

In summary, the provided context does not mention the specific regularization methods used in the study. However, L2 regularization and dropout are two common regularization techniques in deep learning to prevent overfitting.
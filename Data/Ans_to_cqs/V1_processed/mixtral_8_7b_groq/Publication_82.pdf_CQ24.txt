Based on the provided context, there is no explicit mention of strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, there are some details about the training process that can give us some insight into how they might be handling randomness.

Firstly, they mentioned using the Cafe framework for deep learning, which is a fast, open-source framework for deep learning. The documentation of this framework might include information about handling randomness, but it is not provided in the context.

Secondly, they mentioned using specific training parameters such as the number of epochs, momentum, weight decay, gamma, and batch size. These parameters are used to control the training process and can affect the randomness of the results. For example, using a larger batch size can reduce the randomness of stochastic gradient descent, while using a smaller batch size can increase it. Similarly, using a higher momentum value can reduce the randomness of the updates, while using a lower momentum value can increase it.

Thirdly, they mentioned using different training-testing set distributions, such as Train: 80%, Test: 20%, Train: 60%, Test: 40%, Train: 50%, Test: 50%, Train: 40%, Test: 60%, and Train: 20%, Test: 80%. Using different training-testing set distributions can affect the randomness of the results, as different sets of data will be used for training and testing.

Finally, they mentioned using two different deep learning architectures, AlexNet and GoogLeNet, and three different dataset types, Color, Gray scale, and Leaf Segmented. Using different architectures and dataset types can also affect the randomness of the results, as different models and data will be used for training and testing.

In summary, while the context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value, it does provide some insight into how they might be handling randomness. They mentioned using specific training parameters, different training-testing set distributions, and different deep learning architectures and dataset types, all of which can affect the randomness of the results. However, without further information, it is difficult to say for sure how they are handling randomness in their deep learning pipeline.
The provided context discusses various topics related to data selection, model segmentation, and metric extraction, but it does not explicitly mention the regularization methods used to prevent overfitting in deep learning pipelines. However, I can provide some general information about common regularization techniques used in deep learning.

Regularization methods are essential to prevent overfitting in deep learning models. Overfitting occurs when a model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization techniques discourage the learning of overly complex models by adding a penalty term to the loss function.

Two common regularization methods are L1 and L2 regularization. L1 regularization, also known as Lasso regularization, adds a penalty equal to the absolute value of the model's weights. L2 regularization, or weight decay, adds a penalty equal to the square of the model's weights. These regularization techniques encourage the model to have smaller weights, reducing the complexity of the model and helping to prevent overfitting.

Dropout is another popular regularization technique in deep learning. During training, dropout randomly sets a fraction of the model's outputs to zero, effectively preventing the model from relying too heavily on any single neuron. This helps to improve the model's generalization performance.

Early stopping is another regularization technique that involves monitoring the model's performance on a validation set during training. Training is stopped when the model's performance on the validation set starts to degrade, preventing the model from overfitting to the training data.

In summary, while the provided context does not mention specific regularization techniques used in deep learning pipelines, L1 and L2 regularization, dropout, and early stopping are common regularization methods used to prevent overfitting.
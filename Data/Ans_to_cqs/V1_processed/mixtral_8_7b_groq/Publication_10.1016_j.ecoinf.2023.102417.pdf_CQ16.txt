In the provided context, the training of a model on labeled biocrust images is completed after 100 epochs, and several measures are taken to prevent overfitting during this process. However, the specific criterion used to determine when training is complete is not explicitly stated.

Typically, in machine learning, training is considered complete when the validation loss plateaus or starts to increase, indicating that the model is no longer improving its performance on unseen data. This is because the goal of training is to create a model that generalizes well to new data, rather than one that performs exceptionally well on the training data.

In the provided context, the training data is augmented using various spatial and color transformations, and a cross-validation approach is used with 80% of the training data used for training and 20% used for validation. This approach helps to ensure that the model is not overfitting to the training data, as the validation set provides an independent measure of the model's performance.

Additionally, the training is stopped after 100 epochs, which may suggest that the authors have determined through experimentation that this is a sufficient number of epochs to train the model without overfitting. However, without more information, it is difficult to say for sure.

In summary, while the specific criterion used to determine when training is complete is not provided in the given context, it is common practice to use a validation loss plateau or increase as an indicator that training should be stopped to prevent overfitting. The provided context suggests that measures have been taken to prevent overfitting, such as data augmentation and cross-validation, and that the training has been stopped after a fixed number of epochs. However, without further information, it is difficult to determine the exact criterion used to determine when training is complete.
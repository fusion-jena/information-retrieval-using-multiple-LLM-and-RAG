The provided context does not include information about deep learning pipelines or optimization techniques such as Stochastic Gradient Descent (SGD) or Adam. The context focuses on ensemble methods, specifically bagging and boosting, and their application in XGBoost. It discusses the use of a nested k-fold cross-validation scheme to optimize the iteration number in XGBoost and avoid overfitting, but it does not mention any optimization techniques used in deep learning.

Deep learning optimization techniques are used to minimize the loss function and update the model parameters during training. Some common optimization techniques include:

1. Stochastic Gradient Descent (SGD): An optimization algorithm that uses gradient descent with a randomly selected subset of the training data in each iteration.
2. Momentum: An optimization technique that incorporates the past gradients' direction to accelerate learning.
3. Adagrad: An optimization algorithm that adapts the learning rate for each parameter individually, taking into account the historical gradient information.
4. Adadelta: An optimization algorithm that adapts the learning rate based on the recent past gradients.
5. Adam: An optimization algorithm that combines the ideas from Adagrad and momentum, making it computationally efficient and well-suited for large datasets.

These optimization techniques are widely used in deep learning to train models effectively and efficiently. However, the provided context does not discuss these techniques.
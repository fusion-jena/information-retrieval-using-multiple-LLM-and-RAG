The text provided does not give explicit information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that data augmentation was used to enlarge the dataset to prevent overfitting.

Data augmentation is a technique that can be used to increase the amount of training data by creating modified versions of the images in the dataset. This can help to reduce overfitting by providing the model with a more diverse set of training examples, and can also help to improve the model's ability to generalize to new, unseen data.

The text does not mention the use of other regularization techniques such as dropout or L2 regularization. Dropout is a technique that involves randomly setting a fraction of the activations in a layer to zero during training. This helps to prevent overfitting by reducing the co-adaptation of neurons in a layer, and has been shown to be an effective regularization method for deep neural networks. L2 regularization, also known as weight decay, is a technique that involves adding a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This helps to prevent overfitting by encouraging the model to learn smaller, more conservative weight values.

In summary, while the text does mention the use of data augmentation to prevent overfitting, it does not provide information on the use of other regularization techniques such as dropout or L2 regularization.
The criteria used to determine when training is complete in machine learning and deep learning models often involves monitoring the validation loss and other metrics during the training process. The validation loss is a measure of the error or difference between the model's predictions and the actual values for a validation dataset that is separate from the training dataset.

In the context of the papers provided, the specific criteria used to determine when training is complete are not mentioned. However, it is common practice to use early stopping as a criterion for determining when training should be stopped. Early stopping involves monitoring the validation loss during training and stopping the training process when the validation loss stops decreasing or starts increasing, indicating that the model is starting to overfit to the training data.

For example, in the paper by Marrs and Ni-Meister (2019), they use machine learning techniques for tree species classification using co-registered LiDAR and hyperspectral data. While they do not explicitly mention the criteria used to determine when training is complete, they do use cross-validation to evaluate the performance of their models. Cross-validation involves dividing the dataset into multiple folds, training the model on one fold and evaluating its performance on the remaining folds, and repeating this process for each fold. This approach allows for a more robust evaluation of the model's performance and can help prevent overfitting.

Similarly, in the paper by Liu et al. (2022), they use a neural network guided interpolation for mapping canopy height of China's forests by integrating GEDI and ICESat-2 data. While they do not explicitly mention the criteria used to determine when training is complete, they do use a validation dataset to evaluate the performance of their model. They use the root mean square error (RMSE) and coefficient of determination (R^2) as metrics to evaluate the performance of their model.

In general, the criteria used to determine when training is complete can vary depending on the specific model and dataset being used. However, monitoring the validation loss and using early stopping is a common practice in machine learning and deep learning. Additionally, using cross-validation and evaluating the performance of the model on a validation dataset can help prevent overfitting and ensure that the model generalizes well to new data.
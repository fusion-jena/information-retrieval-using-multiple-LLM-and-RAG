The hyperparameters of the model were optimized using k-fold cross-validation, where the training set was split into k fractions, with each fraction being used as a validation set while the remaining k-1 fractions were used as the training set. This process was repeated k times, with a different validation set each time. The first 4 years (2018-2021) were used as the training set, and the last year (2022) was used as the test set. A value of k equal to 4 was chosen, so that each validation fraction corresponded approximately to one year.

The hyperparameters that define the architecture of the ANN include the number of layers and the number of neurons. The hyperparameters that control the training process include the learning rate, the optimizer, the batch size, and the number of epochs that the training lasts. To avoid an exhaustive search that could consume a lot of time, a small dimensional space was initially defined and explored with the original variables. The initial hyperparameters that composed this space are shown in Table 1. The number of epochs was maintained at 500, as it proved to guarantee the stabilization of the loss curves during the training. The Adam algorithm, which is a stochastic gradient descent method, was used as the optimizer.

It is not specified in the context if a specific method such as grid search or random search was used to explore the hyperparameter space. However, it can be inferred that an iterative approach was used to modify one of the hyperparameters at a time, while keeping the other hyperparameters constant. The model with the highest AUC was chosen in case of small discrepancies among the three metrics used.
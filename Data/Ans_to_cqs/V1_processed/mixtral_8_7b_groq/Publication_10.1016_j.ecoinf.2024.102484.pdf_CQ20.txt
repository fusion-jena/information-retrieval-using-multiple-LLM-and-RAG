The provided context does not give specific information about the hardware resources used for training the deep learning model. It mainly focuses on the factors and data used for monitoring ecosystem service change, land degradation, and quantifying its drivers.

However, it is common practice to use Graphics Processing Units (GPUs) for deep learning model training due to their ability to perform parallel processing, which significantly speeds up the computation time. GPUs are specialized hardware designed to handle matrix and vector operations efficiently, making them ideal for training deep learning models.

Other hardware resources, such as Tensor Processing Units (TPUs), developed by Google, are also used for deep learning model training. TPUs are application-specific integrated circuits (ASICs) designed to accelerate machine learning tasks, particularly for Google's TensorFlow framework.

Considering the complexity of the studies mentioned in the context, it is reasonable to assume that high-performance computing resources, such as GPUs or TPUs, were used for training the deep learning models. However, the specific hardware resources used are not explicitly stated in the provided context.
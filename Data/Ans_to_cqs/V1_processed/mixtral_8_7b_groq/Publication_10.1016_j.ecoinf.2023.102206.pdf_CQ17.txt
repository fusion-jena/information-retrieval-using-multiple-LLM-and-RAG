The provided context discusses the use of IFTL (Instance Forgetting and Transfer Learning) and FTL (Full Transfer Learning) to improve learning by restricting overconfidence during Transfer Learning (TL) in the context of machine learning models such as ELM (Extreme Learning Machine), GRNN (Generalized Regression Neural Network), and SVR (Support Vector Regression). However, it does not directly mention the regularization methods used to prevent overfitting in deep learning pipelines, such as dropout or L2 regularization.

Deep learning pipelines often use techniques such as dropout, L2 regularization, and early stopping to prevent overfitting. Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent over-reliance on any single neuron and thus reduces overfitting. L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the weights. This encourages smaller weights, which can help to reduce overfitting. Early stopping is a technique where training is stopped as soon as the performance on a validation set starts to degrade, which can help to prevent overfitting by stopping training before the model begins to memorize the training data.

In summary, while the provided context discusses techniques for improving learning by restricting overconfidence during TL, it does not directly mention the regularization methods used to prevent overfitting in deep learning pipelines. Common regularization methods used in deep learning pipelines include dropout, L2 regularization, and early stopping.
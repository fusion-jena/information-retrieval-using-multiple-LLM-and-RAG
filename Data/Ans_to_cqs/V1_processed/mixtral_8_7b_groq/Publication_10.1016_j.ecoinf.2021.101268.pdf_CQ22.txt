The performance of the deep learning models in the given context is evaluated using the metric of accuracy. Accuracy is a ratio of correctly predicted observations to the total observations. In other words, it is the percentage of correct predictions out of all predictions made by a model.

In the provided table, the accuracy of four deep learning models, namely AlexNet, VGG-19, ResNet-101, and DenseNet-201, is reported for both raw and pre-processed images. The raw images are the original images without any processing, while the pre-processed images are the images that have undergone some processing like adaptive equalization of the Lightness - IRGB ICanny.

For raw images, the accuracy is reported for train, validation, and test datasets. Similarly, for pre-processed images, the accuracy is reported for train, validation, and test datasets. The train dataset is used to train the model, the validation dataset is used to tune the model's parameters, and the test dataset is used to evaluate the model's performance.

Additionally, the table also reports the accuracy of the models when swapping the testing sets (source â†’ target) between raw and pre-processed images. This metric is used to evaluate the model's robustness and its ability to generalize to different types of inputs.

Furthermore, the paper also includes confusion matrices for the VGG-19 architecture, which provide a more detailed breakdown of the model's performance. Confusion matrices are a table layout that allows visualization of the performance of an algorithm. It is a summary of prediction results on a classification problem.

Therefore, in the given context, the performance of the deep learning models is evaluated using the accuracy metric for raw and pre-processed images, as well as the accuracy metric for swapping testing sets between raw and pre-processed images.
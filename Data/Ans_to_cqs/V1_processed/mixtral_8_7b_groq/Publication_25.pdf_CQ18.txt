Based on the provided context, there is no explicit information about the strategy implemented to monitor the model performance during training. However, we can infer that the model's performance was evaluated using a hand-annotated data set, which was withheld from model training. This allowed for a direct comparison of the contribution of high-quality annotations when compared to the self-supervised model or the full model combining both self-supervision and hand-annotation data. 

Furthermore, the full model's performance was assessed in areas with varying canopy complexity, including well-defined large trees, mixed-species canopies, and tightly packed clusters of trees. The model's tendency to under-segment small tree clusters as canopy complexity increased suggests that the model's performance was being monitored in diverse and challenging conditions. 

Additionally, the discussion of the potential benefits of combining the model with labeled data in a semi-supervised framework and the uncertainty around the use of moderate to low-quality annotations for initial model training also implies that the model's performance was being evaluated in different scenarios. 

In summary, while the context does not provide explicit details about the strategy used to monitor the model's performance during training, it can be inferred that the model's performance was evaluated using a withheld hand-annotated data set, in diverse canopy conditions, and in different training scenarios.
The provided context does not directly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does discuss the use of Feature Pyramid Network (FPN) to improve the detection of small objects in UAV thermal images using residual neural networks.

To answer the query, I will provide some general regularization methods used in deep learning pipelines.

Regularization methods are techniques used to prevent overfitting in deep learning models. Overfitting occurs when a model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization introduces a penalty on the model's complexity to reduce overfitting.

Two common regularization methods are:

1. L2 regularization: This method adds a penalty term to the loss function, proportional to the square of the magnitude of the model's weights. This encourages the model to have smaller weights, reducing its complexity and preventing overfitting.
2. Dropout: During training, dropout randomly sets a fraction of the model's outputs to zero. This prevents the co-adaptation of neurons, making the model more robust and less prone to overfitting.

These methods can be applied to the residual neural networks discussed in the context to prevent overfitting. However, the context does not explicitly mention the use of these regularization techniques.
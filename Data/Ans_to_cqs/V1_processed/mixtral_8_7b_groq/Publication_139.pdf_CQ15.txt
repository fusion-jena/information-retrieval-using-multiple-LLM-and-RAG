The optimization technique applied in the deep learning pipeline is Adam. The Adam optimizer was selected with an initial learning rate of 7 to train the model. The Adam optimizer is an adaptive learning rate optimization algorithm that combines the advantages of AdaGrad and RMSProp. It is a popular choice for deep learning models due to its efficiency and effectiveness. It calculates individual learning rates for different parameters, allowing for better convergence and generalization. The use of the Adam optimizer, along with early stopping to prevent overfitting, contributed to the model's robust performance in classifying sound stimuli in the study.
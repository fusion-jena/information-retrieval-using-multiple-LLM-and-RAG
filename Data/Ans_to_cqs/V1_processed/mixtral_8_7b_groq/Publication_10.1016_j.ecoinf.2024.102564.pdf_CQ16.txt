Based on the provided context, there is no explicit information about the specific criteria used to determine when the training of the machine learning models is complete. However, it does mention that the performance of the models was assessed using the k-fold cross-validation method, with the data divided into 5 folds. This process was repeated 50 times, obtaining the average of the metrics for comparison of all models.

In the context of cross-validation, early stopping is a common technique used to determine when training should be stopped. Early stopping involves monitoring the performance of the model on a validation set during training, and stopping the training process when the performance on the validation set stops improving or starts to degrade. This can help to prevent overfitting, which occurs when a model is trained too well on the training data and performs poorly on new, unseen data.

Additionally, the trained ANN (Artificial Neural Network) in this study is a multilayer perceptron (MLP) with a hidden layer. The number of neurons in the hidden layer was determined using the Fletcher-Gloss method, which suggests a range of possible values for the number of neurons based on the number of inputs and outputs of the network. This method does not provide a specific stopping criterion for training, but rather a way to determine the appropriate architecture of the network.

In summary, while the provided context does not give explicit information about the stopping criterion used for training, it can be inferred that cross-validation with early stopping and the Fletcher-Gloss method for determining the number of neurons in the hidden layer were used to prevent overfitting and determine the appropriate architecture of the network. However, it is important to note that the specific stopping criterion used for training is not specified.
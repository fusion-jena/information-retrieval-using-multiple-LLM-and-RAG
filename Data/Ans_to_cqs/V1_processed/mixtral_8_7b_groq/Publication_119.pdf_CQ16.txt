The text provided does not give specific information about the criteria used to determine when training is complete for the models mentioned. However, it does mention that hyperparameters such as the learning rate, number of epochs, and batch size were tuned for the DNN model to achieve the best testing accuracy and kappa. The choice of these values could be based on various convergence criteria, such as validation loss plateau, though this is not explicitly stated. Similarly, for the SVM model with the RBF kernel, the parameters C and Î³ were optimized using GridSearchCV, which could also involve convergence criteria. In general, training of machine learning models is often stopped when the performance on a validation set plateaus or starts to degrade, indicating that the model is no longer learning effectively from the training data. However, without more specific information from the text, this is just an educated guess.
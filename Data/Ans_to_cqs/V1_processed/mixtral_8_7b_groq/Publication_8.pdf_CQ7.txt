The criteria used to split the data for deep learning model training are not explicitly mentioned in the provided context. However, it does mention that the model was trained twice, once on a training subset of 498 images and then on all 653 annotated images. It is likely that these subsets were created to serve as the training and test or validation sets, but the exact criteria for splitting the data is not specified.

In general, when training deep learning models, it is common to split the data into three subsets: a training set, a validation set, and a test set. The training set is used to train the model, the validation set is used to tune the model's hyperparameters and make decisions during the training process, and the test set is used to evaluate the model's performance.

The criteria for splitting the data can vary depending on the specific use case and the amount of available data. One common approach is to randomly split the data into a training set (e.g., 70% of the data), a validation set (e.g., 15% of the data), and a test set (e.g., 15% of the data). Another approach is to use the first k instances for training, the next k instances for validation, and the remaining instances for testing.

In the context of image processing and deep learning, it is also common to split the data based on the image subjects or classes. For example, if the dataset contains images of different plant species, the data could be split so that the training, validation, and test sets each contain a representative sample of the different species.

Overall, while the specific criteria for splitting the data in the provided context is not mentioned, it is clear that the data was split into at least two subsets for training and testing the deep learning model.
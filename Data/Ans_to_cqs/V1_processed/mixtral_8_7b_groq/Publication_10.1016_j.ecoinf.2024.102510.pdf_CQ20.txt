Based on the provided context, the specific information about the hardware resources used for training the deep learning models is not mentioned. Therefore, it is not possible to directly answer the query. However, it can be inferred that the training process involved computational tasks that required a certain amount of time to complete, as shown in the comparison of existing and proposed methods. This implies that the training was likely performed on a machine with sufficient computational power, which could include GPUs, TPUs, or other high-performance computing resources.

In the context of deep learning model training, GPUs (Graphics Processing Units) are commonly used due to their ability to perform parallel computations efficiently. They are well-suited for handling the large matrix and vector operations that are prevalent in deep learning algorithms. TPUs (Tensor Processing Units) are another type of hardware accelerator specifically designed by Google for machine learning tasks, particularly neural network computations.

Considering the experimental setup and the institutions involved (e.g., universities and research organizations), it is reasonable to assume that the training was conducted on a high-performance computing cluster or workstation equipped with GPUs or TPUs. However, without explicit information, this remains a hypothesis.
The provided context does not contain information about strategies employed to handle randomness in the deep learning pipeline. However, I can provide some general information on this topic.

In deep learning, randomness can be introduced at various stages of the pipeline, such as data preprocessing, weight initialization, and dropout regularization. These sources of randomness can affect the model's performance and reproducibility. To handle randomness, several strategies can be employed:

1. **Fixed random seed value**: To ensure reproducibility, it is essential to set a fixed random seed value at the beginning of the pipeline. This ensures that the same sequence of random numbers is generated during each run, leading to consistent results.
2. **Data shuffling and splitting**: When splitting the dataset into training, validation, and testing sets, it is common to shuffle the data. Shuffling ensures that the distribution of classes is random and that the model does not overfit or underfit specific patterns in the data.
3. **Weight initialization**: Initializing the weights randomly is a common practice in deep learning. However, different initialization methods can affect the model's convergence and performance. For example, Xavier initialization (Glorot & Bengio, 2010) initializes the weights based on the number of input and output neurons, ensuring a balanced distribution of activations and preventing the vanishing or exploding gradient problem.
4. **Dropout regularization**: Dropout is a regularization technique that randomly drops out a fraction of the neurons during training. This helps prevent overfitting and improves the model's generalization performance. The dropout rate can be set as a hyperparameter and tuned during cross-validation.
5. **Batch normalization**: Batch normalization (Ioffe & Szegedy, 2015) normalizes the activations of each layer, reducing the internal covariate shift and improving the model's convergence and stability. Batch normalization also introduces randomness by sampling a mini-batch of data during training.

In summary, handling randomness in the deep learning pipeline involves setting a fixed random seed value, shuffling and splitting the data, initializing the weights appropriately, using regularization techniques such as dropout, and normalizing the activations using batch normalization. These strategies help improve the model's performance and reproducibility.

References:

* Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. Proceedings of the 13th international conference on artificial intelligence and statistics, 249–256.
* Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. International conference on machine learning, 448–456.
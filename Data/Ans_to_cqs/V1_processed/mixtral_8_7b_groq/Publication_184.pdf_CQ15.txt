Based on the provided context, there is no explicit mention of the optimization techniques such as Stochastic Gradient Descent (SGD) or Adam used in the deep learning pipeline. However, it does mention that the learning rate is set to 0.001, which is a hyperparameter that can be used in optimization algorithms like SGD and Adam.

In general, SGD and Adam are commonly used optimization algorithms in deep learning. SGD is a simple optimization algorithm that updates the model parameters by computing the gradient of the loss function with respect to the parameters for a single training example. Adam, on the other hand, is a more advanced optimization algorithm that adaptively adjusts the learning rate based on the historical gradient information.

Given that the model used in the study is YOLOv3, which is a popular object detection algorithm, it is likely that an optimization algorithm such as SGD or Adam was used during training. However, without explicit mention in the provided context, it is not possible to give a definitive answer.
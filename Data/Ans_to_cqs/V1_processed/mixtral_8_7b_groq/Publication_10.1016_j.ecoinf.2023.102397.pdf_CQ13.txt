Unfortunately, the provided context does not include information on the specific hyperparameters used in the deep learning model developed by Benkendorf and Hawkins (2020). The context only mentions that the model's performance improved as the number of hidden layers (which can be interpreted as adding more training recordings) increased. However, it does not specify other hyperparameters such as learning rate, optimizer, batch size, or number of epochs.

Hyperparameters are important parameters that need to be set before training a deep learning model. They can significantly impact the model's performance and need to be carefully chosen. Common hyperparameters in deep learning models include learning rate, optimizer, batch size, and number of epochs.

The learning rate determines the size of the steps that the model takes during training. A learning rate that is too high may cause the model to converge too quickly to a suboptimal solution, while a learning rate that is too low may cause the training process to be very slow.

The optimizer is a function that updates the model's weights based on the gradient of the loss function. Common optimizers include stochastic gradient descent (SGD), Adam, and RMSprop.

Batch size determines the number of samples that are processed before the model's weights are updated. A larger batch size can lead to more stable gradients but may require more memory.

The number of epochs determines the number of times that the entire training dataset is passed through the model during training.

Therefore, without the specific hyperparameters used in the deep learning model developed by Benkendorf and Hawkins (2020), it is not possible to provide a complete answer to the query.
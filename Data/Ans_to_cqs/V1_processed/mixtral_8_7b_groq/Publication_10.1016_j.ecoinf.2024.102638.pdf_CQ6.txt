Based on the provided context, there is no explicit information about the preprocessing steps involved before training a deep learning model. However, it can be inferred that the dataset is divided into training and test sets, and hyperparameters of each model are fine-tuned using the scikit-learn function GridSearchCV. This function might involve preprocessing steps such as normalization, scaling, or cleaning as part of its hyperparameter tuning process.

In general, preprocessing steps for deep learning models can include:

1. Normalization: This involves scaling the input features to a smaller range, usually between 0 and 1. This is done to prevent any single feature from dominating the learning process due to its scale.
2. Scaling: This is similar to normalization but involves scaling the features to have a mean of 0 and a standard deviation of 1. This is useful when the distribution of the features is important for the learning process.
3. Cleaning: This involves removing or correcting any erroneous or missing data in the dataset. This is important to prevent the learning process from being influenced by outliers or incorrect data.

These preprocessing steps are crucial for the performance of deep learning models and are typically applied before training the model. However, the specific preprocessing steps used can depend on the nature of the dataset and the requirements of the deep learning algorithm being used.
Based on the provided context, there is no explicit information given about the strategy implemented to monitor the model performance during training. However, the text does discuss the evaluation of the model performance after training.

The models were evaluated using data from both sites combined and split into two different evaluation sites with different backgrounds of rocky shore and mudflats. The evaluation scores for each model include the number of true positives, false negatives, precision, and recall. The uncertainties of precision and recall for each model were estimated by calculating the Confidence Interval (CI) for each metric.

Moreover, to establish the balance between the precision and recall of each model, the F-score was calculated as the harmonic mean of precision and recall uncertainty of the model. The models that returned the higher number of true positives, the lower number of false negatives, and those that scored best on precision and recall were highlighted in bold.

From this, it can be inferred that after the training process, a thorough evaluation of the model performance was carried out. This evaluation included the calculation of various metrics, estimation of uncertainties, and the computation of the F-score. However, the context does not provide information about the strategy used to monitor the model performance during the training process.
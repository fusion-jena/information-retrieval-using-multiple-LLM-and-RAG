The hyperparameters of the model are optimized using a combination of techniques, including 5-fold cross-validation, early stopping, and empirical trials. However, the context does not explicitly mention the use of grid search or random search for hyperparameter tuning.

During the model training and validation phase, the dataset was divided into five subsets, and at each iteration of the 5-fold cross-validation, one subset was used for evaluating the models, while the remaining four subsets were used for training (90% of the images) and validation (10% of the images). This approach provides a robust and unbiased estimate of a model's performance while also guiding effective hyperparameter tuning for improved overall performance.

For model optimization, the Adam optimizer algorithm was used, along with a batch size of 10 and 100 epochs. The learning rates were chosen from empirical trials over 100 epochs, indicating that the learning rates were optimized through trial and error.

Additionally, an early stop approach was implemented with a patience value of 16 to regularize the model and minimize the loss function. This technique helps prevent overfitting and conserves computational resources by halting the training process prematurely if performance on a validation dataset fails to improve beyond a predefined threshold.

Therefore, while the context does not explicitly mention the use of grid search or random search for hyperparameter tuning, it does describe a systematic approach to hyperparameter optimization, including empirical trials, cross-validation, and early stopping.
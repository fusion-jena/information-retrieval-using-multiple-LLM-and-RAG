The provided context does not give specific information about the learning rate or optimizer used in the deep learning models. However, it does mention that all the models were implemented using TensorFlow, a Python framework commonly used for deep learning. TensorFlow allows for a variety of optimization algorithms and learning rate schedules, so it is possible that different methods used different optimizers or learning rates.

The context does mention that the methods were trained with the same optimizer and learning rates, but it does not specify what those values were. Therefore, while we know that the same optimization algorithm and learning rate were used for all methods, we do not have the specific values for those hyperparameters.

Additionally, the context states that the methods were trained on a specific hardware infrastructure, including a 64-bit Intel i7-8700K@3.70GHz CPU workstation, 64 GB memory, and an NVIDIAÂ® GTX 1080 GPU with 12Gb of memory, under a 10.0 CUDA version. This information may be relevant for understanding the computational complexity of the models, but it does not provide any insight into the specific hyperparameters used for training.

In summary, while the context provides some information about the deep learning models used in the study, it does not give specific details about the learning rate or optimizer used for training.
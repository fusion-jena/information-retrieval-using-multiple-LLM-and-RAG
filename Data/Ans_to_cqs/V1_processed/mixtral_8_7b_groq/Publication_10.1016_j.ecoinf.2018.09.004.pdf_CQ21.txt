Based on the provided context, there are no specific mentions of post-processing steps such as saliency maps or feature importance extraction. However, it is mentioned that several classification models were trained and evaluated using metrics such as accuracy, precision, recall, and the F-measure.

The confusion matrix, which is a table layout that allows visualization of the performance of an algorithm, was likely used to calculate the aforementioned metrics. The confusion matrix contains the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These counts were used to calculate precision, recall, and the F-measure as shown in the formulas (2) and (3) in the context.

Additionally, the k-nearest neighbors (KNN) and decision tree classifiers were described in more detail. KNN is a non-parametric "lazy" learning algorithm that does not make assumptions about the underlying data distribution and does not construct a general internal model. Instead, it stores instances of the training data and classifies new points based on the majority vote of the k-nearest neighbors. The decision tree classifier, on the other hand, applies a series of questions about the attributes of the test record to reach a conclusion about the class label of the record.

In summary, the post-processing steps after model training, as described in the context, include the calculation of various metrics such as accuracy, precision, recall, and the F-measure using a confusion matrix. Furthermore, the context provides a detailed description of the k-nearest neighbors and decision tree classifiers.
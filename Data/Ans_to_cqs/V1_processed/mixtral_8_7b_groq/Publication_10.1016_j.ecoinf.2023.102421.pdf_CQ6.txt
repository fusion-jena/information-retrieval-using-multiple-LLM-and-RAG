Based on the provided context, there is no information about deep learning models being used in the described study. The focus is on optimizing hyperparameters of machine learning models XGBoost and LightGBM using Bayesian optimizers and Threshold Decline Optimization (TDO). Therefore, I cannot directly answer the query regarding preprocessing steps for deep learning models in this specific context.

However, I can provide a general answer about common preprocessing steps for deep learning models. These steps include:

1. Data cleaning: Removing or correcting erroneous, missing, or irrelevant data points.
2. Data normalization/scaling: Transforming data to a common scale or range, such as min-max scaling or z-score normalization, to avoid features with larger values dominating the learning process.
3. Data augmentation: Artificially increasing the size of the dataset by applying transformations like rotation, scaling, or flipping (commonly used in computer vision tasks).
4. Feature extraction/selection: Reducing the dimensionality of the dataset by selecting the most relevant features or extracting new features from the existing ones.

In summary, while the provided context does not discuss deep learning models, common preprocessing steps for deep learning models include data cleaning, normalization/scaling, data augmentation, and feature extraction/selection.
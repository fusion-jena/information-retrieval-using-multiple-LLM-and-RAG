In the context provided, there is no direct mention of the regularization methods used to prevent overfitting in the deep learning pipeline. However, I can provide some information on common regularization techniques used in deep learning based on my general knowledge in this area.

Regularization is a crucial technique in deep learning to prevent overfitting, which occurs when a model learns the training data too well, to the point where it performs poorly on unseen data. Two common regularization methods used in deep learning are L2 regularization and dropout.

L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the model's weights. This penalty term discourages the model from assigning large weights to insignificant features, thereby reducing overfitting.

Dropout is a regularization technique that randomly sets a fraction of the model's hidden units to zero during training. This prevents the model from relying too heavily on any single hidden unit, thereby promoting the learning of more robust features. Dropout has been shown to be an effective regularization method in deep learning, particularly for convolutional neural networks (CNNs) and recurrent neural networks (RNNs).

In summary, while there is no direct mention of regularization methods in the provided context, L2 regularization and dropout are two common techniques used in deep learning to prevent overfitting. L2 regularization adds a penalty term to the loss function that discourages large weights, while dropout randomly sets a fraction of hidden units to zero during training, promoting the learning of more robust features.
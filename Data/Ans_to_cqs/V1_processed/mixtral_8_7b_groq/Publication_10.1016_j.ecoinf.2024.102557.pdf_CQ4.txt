The provided pieces of context do not contain specific information about data augmentation techniques applied in the deep learning pipeline. However, I can provide some general information about data augmentation techniques that are often used in deep learning.

Data augmentation is a technique used to increase the amount of training data by applying various transformations to the existing data. This technique is commonly used in deep learning, particularly in computer vision tasks, to prevent overfitting and improve model generalization.

Some common data augmentation techniques used in deep learning include:

1. Flipping: This technique involves flipping the image horizontally or vertically. Flipping the image horizontally can help the model learn features that are invariant to left-right orientation.
2. Rotation: This technique involves rotating the image by a certain angle. Rotation can help the model learn features that are invariant to orientation.
3. Scaling: This technique involves scaling the image by a certain factor. Scaling can help the model learn features that are invariant to size.
4. Translation: This technique involves translating the image by a certain number of pixels. Translation can help the model learn features that are invariant to position.
5. Color jittering: This technique involves changing the brightness, contrast, saturation, or hue of the image. Color jittering can help the model learn features that are invariant to color.
6. Cropping: This technique involves randomly cropping a portion of the image. Cropping can help the model learn features that are invariant to the location of the object in the image.

These techniques can be applied individually or in combination to create new training examples. The choice of data augmentation techniques depends on the specific task and the dataset.

In summary, while the provided pieces of context do not contain specific information about data augmentation techniques applied in the deep learning pipeline, some common data augmentation techniques used in deep learning include flipping, rotation, scaling, translation, color jittering, and cropping. These techniques can help prevent overfitting and improve model generalization.
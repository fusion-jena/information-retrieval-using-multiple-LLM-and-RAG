The provided context does not specifically mention the regularization methods used to prevent overfitting in the deep learning pipeline for the LSTM model used in the study. However, based on general deep learning practices, common regularization methods include dropout and L2 regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent over-dependence on any single neuron and promotes the development of a more robust model. This method can be applied to various layers of the deep learning model, including the LSTM model.

L2 regularization, also known as weight decay, is a method that adds a penalty term to the loss function, which is proportional to the square of the magnitude of the model's weights. This encourages the model to have smaller weights, reducing the complexity of the model and helping to prevent overfitting.

While the study does not explicitly mention these regularization methods, it is possible that they were used in the deep learning pipeline for the LSTM model. It is also possible that other regularization methods were used, or that the authors relied on other techniques to prevent overfitting, such as early stopping or data augmentation. Without further information, it is not possible to provide a definitive answer.
The provided context does not give specific information about the hardware resources used for training the deep learning models. It mentions the use of different datasets (AT0, AT1, T0, MiniImageNet) and various deep learning techniques such as classic deep learning algorithms, few-shot learning (FSL) architectures, and transfer learning. However, it does not specify the hardware used for training these models.

Deep learning models can be trained on various hardware resources, including CPUs, GPUs, and TPUs. GPUs are commonly used for deep learning due to their ability to perform parallel processing, which is well-suited for the matrix and vector operations used in deep learning algorithms. TPUs (Tensor Processing Units) are custom-built by Google for machine learning tasks and can provide even greater performance than GPUs for certain tasks.

Therefore, while the context does not provide information on the specific hardware used for training the deep learning models, it is likely that GPUs or TPUs were used due to their suitability for deep learning tasks.
In the context provided, there is no explicit mention of regularization methods used to prevent overfitting in deep learning pipelines. However, I can provide a general explanation of such methods using the given information.

Regularization is a technique used to prevent overfitting in machine learning models, including deep learning. Overfitting occurs when a model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization introduces a penalty on the model's complexity to prevent overfitting.

Two common regularization methods are L2 regularization and dropout. L2 regularization, also known as weight decay, adds a penalty term to the loss function, which is the sum of the squares of all the model's weights. This penalty term is then multiplied by a regularization parameter, usually denoted as lambda, and added to the loss function. By doing so, L2 regularization encourages the model to have smaller weights, reducing its complexity and preventing overfitting.

Dropout is a regularization technique specifically designed for neural networks. During training, dropout randomly sets a fraction of the neurons in a layer to zero, effectively preventing them from contributing to the forward pass and the backpropagation of gradients. This process helps to break the co-adaptation of neurons, where some neurons become too dependent on others, and reduces overfitting.

In the context of deep learning for image processing, regularization methods like L2 and dropout can be applied to the convolutional layers, fully connected layers, or both. The choice of regularization method and its hyperparameters depends on the specific problem and dataset.

In summary, L2 regularization and dropout are two common regularization methods used to prevent overfitting in deep learning pipelines. L2 regularization adds a penalty term to the loss function, encouraging smaller weights, while dropout randomly sets a fraction of neurons to zero during training, preventing co-adaptation and reducing overfitting.
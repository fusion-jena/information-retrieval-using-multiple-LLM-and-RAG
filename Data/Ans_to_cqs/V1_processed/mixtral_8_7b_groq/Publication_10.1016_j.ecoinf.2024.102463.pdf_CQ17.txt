While the provided context does not directly mention the regularization methods used to prevent overfitting in deep learning pipelines, it does discuss the use of Long Short-Term Memory (LSTM) networks and Recurrent Highway Networks (RHN) in model architectures. LSTM networks are a type of recurrent neural network that can help prevent overfitting by allowing data to "skip" layers, which can improve the training of long sequences and alleviate the "vanishing gradient" problem. RHN networks are an extension of LSTM networks and can also help prevent overfitting by learning the temporal relations between the results of previous convolution input features.

In addition to these techniques, there are several regularization methods commonly used in deep learning to prevent overfitting. These include:

1. L1 and L2 regularization: L1 regularization adds a penalty equal to the absolute value of the magnitude of the coefficients, while L2 regularization adds a penalty equal to the square of the magnitude of the coefficients. These penalties are added to the loss function, encouraging the model to have smaller coefficients and therefore reducing overfitting.
2. Dropout: Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by reducing the co-adaptation of feature detectors.
3. Early stopping: Early stopping is a technique where training is stopped as soon as the validation error starts to increase, which helps prevent overfitting by not allowing the model to learn too closely to the training data.
4. Data augmentation: Data augmentation is a technique where the training data is artificially expanded by applying transformations such as rotation, scaling, and flipping. This helps prevent overfitting by providing the model with more diverse training data.

It's important to note that the choice of regularization method depends on the specific problem and dataset being used. A combination of regularization methods may also be used to further prevent overfitting.
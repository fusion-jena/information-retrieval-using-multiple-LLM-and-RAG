The text provided discusses a deep learning model's performance evaluation but does not explicitly mention the measures taken to ensure the model's generalizability during its development. However, based on common practices in deep learning model development, we can infer some methods that might have been used.

1. Diverse dataset: The model was evaluated on three different datasets, representing a range of domains, problems, species, and environmental conditions. This suggests that a diverse dataset might have been used during the model's development to ensure that it can generalize well to various types of data.

2. Cross-validation: Although not explicitly mentioned, cross-validation is a standard technique used in machine learning to assess a model's performance and generalizability. It involves dividing the dataset into multiple subsets, training the model on different combinations of these subsets, and testing it on the remaining portions. This process helps to reduce overfitting and provides a more reliable estimate of the model's performance on unseen data.

3. Stratified splitting: In cases where the dataset has a class imbalance, stratified sampling can be used to ensure that each subset contains a representative proportion of samples from each class. This technique can help improve the model's ability to generalize to classes with fewer samples. While not explicitly mentioned in the text, it is a common practice when working with imbalanced datasets.

In summary, although the text does not explicitly mention the measures taken to ensure the model's generalizability, it is likely that a diverse dataset, cross-validation, and possibly stratified sampling were employed during the development of the deep learning model. However, without more information, this remains an educated inference based on common practices in deep learning model development.
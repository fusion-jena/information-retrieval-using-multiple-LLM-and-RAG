The strategy implemented to monitor the model performance during training is not explicitly stated in the provided context. However, it does mention the use of perplexity as a measure of model performance. Perplexity is a common evaluation metric for models like Latent Dirichlet Allocation (LDA), which is used in this study. It measures how well a probability model predicts a sample. A lower perplexity score indicates better generalization performance.

Additionally, the study uses Pythonâ€™s pyLDAvis Library for topic modeling and visualization. While pyLDAvis is primarily a tool for topic exploration and visualization, it could also be used to monitor the model performance indirectly. For instance, by examining the consistency and interpretability of the topics over the course of training, one might gain insights into the model's performance.

However, the provided context does not mention any specific strategies for monitoring the model performance during training, such as using a validation set, early stopping, or learning rate adjustment. Therefore, without additional information, a comprehensive answer cannot be provided.
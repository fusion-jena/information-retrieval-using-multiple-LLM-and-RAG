The provided context does not contain specific information about data augmentation techniques applied in a deep learning pipeline. However, I can provide a general explanation of some common data augmentation techniques used in deep learning.

Data augmentation is a technique used to increase the amount of training data by applying various transformations to the existing data. This helps improve the model's performance and reduces overfitting. Some common data augmentation techniques are:

1. Flipping: Flipping is a technique where the image is horizontally or vertically flipped. This helps the model learn symmetrical features in the data.
2. Rotation: Rotation involves rotating the image by a certain angle. This helps the model learn features that are invariant to rotation.
3. Scaling: Scaling involves changing the size of the image. This helps the model learn features that are invariant to scale.
4. Translation: Translation involves shifting the image horizontally or vertically. This helps the model learn features that are invariant to translation.
5. Color jittering: Color jittering involves changing the brightness, contrast, saturation, and hue of the image. This helps the model learn features that are invariant to color changes.
6. Random cropping: Random cropping involves randomly cropping a portion of the image. This helps the model learn features that are present in different parts of the image.
7. Affine transformations: Affine transformations involve changing the orientation, scaling, and translation of the image. This helps the model learn features that are invariant to these transformations.

These techniques can be applied to the training data to generate new samples, which can then be used to train the deep learning model. The specific data augmentation techniques used depend on the problem at hand and the type of data being used.
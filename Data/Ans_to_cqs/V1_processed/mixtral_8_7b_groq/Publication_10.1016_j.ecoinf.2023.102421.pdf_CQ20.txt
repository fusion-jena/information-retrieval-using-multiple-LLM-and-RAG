Based on the provided context, there is no explicit information about the specific hardware resources used for training the deep learning model. The text focuses mainly on the comparison of different machine learning models, optimization algorithms, and evaluation metrics.

However, it is important to note that deep learning models often require significant computational power for training, and it is common to use specialized hardware such as Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs) to accelerate the training process. GPUs are particularly well-suited for parallel computations, which is beneficial for deep learning models that involve a large number of parameters and operations. TPUs, on the other hand, are custom-built by Google for machine learning tasks and can provide even higher performance than GPUs for certain workloads.

Given that the context discusses the use of machine learning models like XGBoost and LightGBM, which are not deep learning models, it is possible that the training was performed on Central Processing Units (CPUs) or GPUs. However, without more specific information, it is not possible to provide a definitive answer.
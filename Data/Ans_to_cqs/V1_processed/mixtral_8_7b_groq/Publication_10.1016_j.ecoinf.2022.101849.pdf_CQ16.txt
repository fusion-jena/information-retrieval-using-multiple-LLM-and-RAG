The provided context does not give specific information about the criteria used to determine when training is complete in the study. However, it does mention the use of validation loss in the context of weakly supervised learning.

In weakly supervised learning, the training data may be incomplete, noisy, or contain errors, which can make it challenging to determine when training should be stopped. In such cases, it is common to monitor the performance of the model on a validation set during training. The validation set is a separate dataset that is used to evaluate the model's performance and is not used in the training process.

One common criterion for determining when training is complete is to look for a plateau in the validation loss. Validation loss is the loss function calculated on the validation set. When the model is trained, the validation loss is calculated after each epoch (iteration over the entire training dataset). If the validation loss does not decrease or decreases only slightly over a certain number of epochs, it may indicate that the model has learned as much as it can from the training data and that further training is unlikely to improve its performance.

Therefore, while the provided context does not give specific information about the criteria used to determine when training is complete, it is likely that the study used a validation loss plateau or a similar criterion based on the performance of the model on a validation set. However, without more information, this is only an educated guess.
The hyperparameters used in the deep learning models for SEIO PBW vocalization training and blue whale Dcall training include batch sizes and learning rates. Specifically, the batch sizes are set to 5 for SEIO PBW vocalization training and 4 for blue whale Dcall training. The learning rates are set to 1e-4 for SEIO PBW vocalization training and 1e-3 for blue whale Dcall training.

The models are trained for a fixed number of epochs, which are 40 for SEIO PBW vocalization training and 25 for blue whale Dcall training. However, an early stopping method is used to keep the model weights before the models overfit. The patience parameter is set at 10 epochs for all training phases.

It is important to note that the provided context does not explicitly mention the optimizer used in the deep learning models. Therefore, I cannot provide an answer to this part of the query.

Additionally, the context suggests that the annotation sets used to train the models are called pseudo ground truth, and there are variations in model performance due to distinct annotation profiles influenced by the complexity of vocalizations and the annotators' strategies. To address the challenges of annotation discrepancies, two annotation aggregation methods, majority voting and soft labeling, are proposed and tested, which significantly improve model performance and reduce variability.

In conclusion, the hyperparameters used in the deep learning models for SEIO PBW vocalization training and blue whale Dcall training include batch sizes and learning rates. However, the optimizer used in the models is not specified in the provided context. The annotation sets used to train the models are called pseudo ground truth, and there are variations in model performance due to annotation discrepancies. To address this issue, two annotation aggregation methods are proposed and tested, which significantly improve model performance and reduce variability.
The provided context does not give specific criteria used to determine when training of a model is complete. However, it does mention some general considerations for model evaluation.

In the context of Isaac et al. (2020), they discuss the challenges of validating a model in a model-based integration, particularly when considering all available data, appropriate validation metrics, and potential biases in the validation dataset. They suggest that integrating prior knowledge of the biology of a species in a SDM through a prior or offset can help improve predictive performance.

The context also mentions some methods for model evaluation, such as using True Skill Statistics (TSS) with the "modEvA" R-package or thresholding with minimum presence values. These methods can be used to evaluate the performance of a model, but they do not specifically address the criteria used to determine when training is complete.

In general, determining when training is complete can depend on the specific model and training algorithm being used. Some common criteria for determining when training is complete include:

1. Validation loss plateau: Training can be stopped when the validation loss stops decreasing or starts increasing, indicating that the model is overfitting to the training data.
2. Early stopping: Training can be stopped based on a predetermined number of epochs or iterations, or based on a patience parameter that specifies how many epochs to wait before stopping if the validation loss does not improve.
3. Monitoring other metrics: Training can be stopped based on other metrics, such as accuracy or precision, if they reach a desired threshold.

It's important to note that the choice of criteria for determining when training is complete can depend on the specific problem and dataset being used. Experimentation and validation can help determine the best criteria for a given situation.
Based on the provided context, the specific regularization methods used to prevent overfitting in the deep learning pipeline are not mentioned. However, it is mentioned that a random hyperparameter search was conducted to determine the optimum model structure, which implies that some regularization techniques might have been explored.

Common regularization techniques used in deep learning models include L1 and L2 regularization, dropout, and early stopping. L1 regularization adds a penalty equal to the absolute value of the magnitude of the coefficients, while L2 regularization adds a penalty equal to the square of the magnitude of the coefficients. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overfitting. Early stopping is a form of regularization where training is halted as soon as the validation error starts to increase.

In the study described in the context, the researchers used H2O, an open-source platform in R, to construct artificial neural networks and perform predictive modeling. H2O supports various regularization techniques, including L1 and L2 regularization, dropout, and early stopping. Therefore, it is possible that one or more of these techniques were used in the deep learning pipeline, but the specific methods are not mentioned in the provided context.
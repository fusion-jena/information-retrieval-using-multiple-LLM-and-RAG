Based on the provided context, the deep learning pipeline is used for fish detection in highly turbid underwater videos. Therefore, the primary data format used in this pipeline is video.

The context mentions that the videos are processed using deep learning methodologies, and it also refers to input images. This suggests that the video data is likely being processed frame by frame, with each frame treated as an individual image. This is a common approach when working with video data in deep learning.

The input images are augmented with random horizontal flipping, random ±7o rotation, and random ±40% brightness. This is a common data augmentation technique used in deep learning to increase the diversity of the training data and improve the model's ability to generalize.

There is no mention of other data formats such as audio or CSV in the provided context. Therefore, based on the information given, the primary data formats used in this deep learning pipeline are video and images.

It's worth noting that the context discusses the use of weakly labeled data, which suggests that the video or image data may be annotated with labels indicating the presence or absence of fish, but not necessarily with precise bounding boxes or other detailed annotations. This is a common approach when working with large-scale video or image data, where manual annotation can be time-consuming and expensive.

In summary, the primary data formats used in the deep learning pipeline described in the provided context are video and images. The video data is processed frame by frame as individual images, and the pipeline uses weak labels to indicate the presence or absence of fish in the video frames.
Based on the provided context, there is no explicit mention of the regularization methods used to prevent overfitting in the deep learning pipeline. However, the text does mention overfitting as a potential issue when training CNNs on acoustic datasets with few vocalization examples.

In general, there are several regularization techniques that can be used to prevent overfitting in deep learning models. Dropout is a regularization method that randomly sets a fraction rate of input units to 0 at each update during training time, which helps to make the network more robust and less prone to overfitting (Srivastava et al., 2014). L2 regularization, also known as weight decay, is a method that adds a penalty term to the loss function, which shrinks the weights towards zero and helps to prevent overfitting (Goodfellow et al., 2016).

Other regularization techniques include early stopping, data augmentation, and batch normalization. Early stopping is a method that stops the training process before the model starts to overfit, based on a validation set (Prechelt, 1998). Data augmentation is a technique that generates new training examples by applying random transformations to the existing data, which can help to increase the size and diversity of the training set and prevent overfitting (Krizhevsky et al., 2012). Batch normalization is a method that normalizes the inputs of each batch, which can help to reduce internal covariate shift and improve the generalization performance of the model (Ioffe and Szegedy, 2015).

In the context of the study mentioned in the provided text, it is possible that they used one or more of these regularization techniques to prevent overfitting. However, without explicit information, it is not possible to provide a definitive answer.

References:
Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.
Henri, J., & Mungloo-Dilmohamud, S. (2021). Birdsong classification using convolutional neural networks. arXiv preprint arXiv:2102.01513.
Ioffe, S., & Szegedy, C. (2015, December). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456). PMLR.
Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In Proceedings of the 25th international conference on neural information processing systems (pp. 1097-1105).
Prechelt, L. (1998). Early stopping—but when? In Proceedings of the 10th international conference on artificial neural networks (ICANN’98) (Vol. 3, pp. 14-22).
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1), 1929-1958.
Xie, C., Girshick, R., & Farhadi, A. (2018). Pre-training and fine-tuning for object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988).
The regularization methods used to prevent overfitting in the deep learning pipeline, as described in the provided context, include the use of dropout layers and limited oversampling.

Dropout layers are used to limit overfitting by randomly ignoring units (neurons) of the CNN during the training process. Specifically, a dropout layer was added just before the first dense layer in the network. This randomly ignores some of the neurons during training, which helps to prevent overfitting by making the model more robust and less dependent on the specific weights of individual neurons.

Limited oversampling is another regularization technique used in this deep learning pipeline. Oversampling is the process of creating copies of the training data from the less sampled classes. However, in this case, oversampling was restricted to the training dataset only and was not applied to the validation dataset. This was done to avoid overestimating the model's learning progress and to limit overfitting caused by having very similar pictures.

It is important to note that the authors did not mention the use of L2 regularization or any other explicit weight regularization techniques in the provided context. Therefore, based on the given information, it can be concluded that dropout layers and limited oversampling are the primary regularization methods used in this deep learning pipeline to prevent overfitting.
In the context of deep learning, randomness can be introduced at various stages of the pipeline, such as the initialization of weights and biases, the order of training examples, and the selection of mini-batches for stochastic gradient descent. Strategies to handle this randomness include:

1. **Fixed Seed Value**: To ensure reproducibility of results, a fixed seed value can be set for the random number generator used in the deep learning pipeline. This ensures that the same sequence of random numbers is generated each time the pipeline is run, leading to the same initialization of weights and biases, and the same order of training examples.

2. **Data Shuffling**: Even with a fixed seed value, data shuffling can be employed to present training examples in a different order during each epoch. This can help prevent overfitting by ensuring that the model does not learn the training data in a specific order.

3. **Mini-Batch Selection**: In stochastic gradient descent, randomness is introduced in the selection of mini-batches for gradient computation. This can be handled by using a fixed seed value for the random number generator used in the mini-batch selection process.

4. **Dropout**: Dropout is a regularization technique used in deep learning to prevent overfitting. It involves randomly dropping out, or setting to zero, a proportion of the activations in a layer during training. This introduces randomness in the network and helps prevent over-reliance on specific neurons.

5. **Early Stopping**: Early stopping is a technique used to prevent overfitting by stopping the training process before the model starts to overfit the training data. This can be determined by monitoring the performance of the model on a validation set during training, and stopping the training when the performance on the validation set starts to degrade.

6. **Ensemble Methods**: Ensemble methods, such as bagging and boosting, can be used to combine the predictions of multiple models trained on different subsets of the training data. This can help reduce the impact of randomness in the training process by averaging out the predictions of the individual models.

References:
Deb, S., Raha, S.K., Dey, N., Bandyopadhyay, S., 2017. Prediction of groundwater potential zone using artificial neural network and remote sensing data. Ecological Informatics, 45, 78-89.

Tiryaki, G., AydÄ±n, A., 2014. Comparison of artificial neural network and regression tree models for predicting soil properties. Computers and Electronics in Agriculture, 102, 122-129.

Yang, S., Feng, Q., Liang, T., Liu, B., Zang, W., Xie, H., 2018. Modeling grassland above-ground biomass based on artificial neural network and remote sensing in the Three-River Headwaters Region. Remote Sensing of Environment, 204, 448-455.
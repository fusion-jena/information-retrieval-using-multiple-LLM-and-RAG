Based on the provided context, there is no information about the specific regularization methods used in the deep learning pipeline to prevent overfitting. The context discusses the evaluation of iSDMs (Incremental Species Distribution Models) at different spatial extents and subjected to various treatments to test the impact of potential biases and errors. However, it does not mention any regularization techniques used in the deep learning pipeline.

Regularization methods are essential in deep learning to prevent overfitting, which occurs when the model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization techniques introduce a penalty on the model's complexity to prevent overfitting and improve generalization.

Two common regularization methods used in deep learning are dropout and L2 regularization. Dropout is a technique where randomly selected neurons are dropped out during training, preventing the neurons from co-adapting too much and memorizing the training data. L2 regularization, also known as weight decay, adds a penalty term to the loss function proportional to the sum of squares of the model's weights. This penalty term discourages large weights, which can lead to overfitting.

Therefore, without specific information from the context, it is safe to assume that regularization methods such as dropout and L2 regularization could have been used in the deep learning pipeline to prevent overfitting. However, it is essential to consult the original source or the authors for confirmation.
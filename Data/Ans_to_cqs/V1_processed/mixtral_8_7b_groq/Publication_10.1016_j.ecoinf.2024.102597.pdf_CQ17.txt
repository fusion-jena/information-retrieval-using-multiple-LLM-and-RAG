The provided context does not mention any deep learning algorithms being used in the study. In fact, it states that the limited number of samples was not conducive to deep learning. Therefore, the context does not provide information on the regularization methods used to prevent overfitting in the deep learning pipeline.

However, for general knowledge, there are several regularization methods used to prevent overfitting in deep learning algorithms. These include:

1. L1 and L2 regularization: These are methods that add a penalty term to the loss function, which discourages large weights and helps to reduce overfitting. L1 regularization adds an absolute value of the weight as a penalty, while L2 regularization adds the square of the weight.

2. Dropout: This is a technique where randomly selected neurons are ignored during training, which helps to prevent overfitting by reducing the co-adaptation of neurons.

3. Early stopping: This is a method where training is stopped as soon as the validation error starts to increase, which prevents overfitting by not allowing the model to learn too much from the training data.

4. Data augmentation: This is a technique where the training data is artificially increased by applying transformations such as rotation, scaling, and flipping. This helps to prevent overfitting by providing the model with more diverse data to learn from.

5. Batch normalization: This is a technique where the inputs to each layer are normalized, which helps to reduce overfitting by ensuring that the inputs have zero mean and unit variance.


The provided context does not mention any regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization methods used in deep learning include L1 and L2 regularization, dropout, early stopping, data augmentation, and batch normalization.
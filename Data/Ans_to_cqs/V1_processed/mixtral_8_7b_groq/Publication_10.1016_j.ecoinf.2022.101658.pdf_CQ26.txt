The provided context discusses a study that uses deep learning for semantic segmentation of images with high class-imbalance, but it does not explicitly mention the techniques used to address data bias during preprocessing. However, it does mention some data augmentation strategies that were used to increase the size of the training set and improve the model's performance.

The study employed data augmentation strategies such as horizontal flip, Gaussian noise addition, and contrast reduction. These techniques were used to increase the training set size fourfold and improve the model's ability to handle shadows and low lighting conditions in the images.

While these techniques can help to increase the size and diversity of the training set, they do not directly address the issue of class-imbalance. Other techniques such as stratified sampling, oversampling, and undersampling can be used to address class-imbalance during preprocessing.

Stratified sampling involves dividing the data into subsets or "strata" based on the class labels, and then randomly sampling from each stratum to ensure that the training set is representative of the overall class distribution. This technique can help to ensure that the model is trained on a balanced set of examples from each class.

Oversampling involves increasing the number of examples from the underrepresented classes by duplicating or synthesizing new examples. This technique can help to increase the representation of the underrepresented classes in the training set, but it can also lead to overfitting if not done carefully.

Undersampling involves reducing the number of examples from the overrepresented classes to match the number of examples in the underrepresented classes. This technique can help to reduce the impact of the overrepresented classes on the loss function, but it can also lead to loss of information if not done carefully.

Diverse data collection involves collecting data from a variety of sources and conditions to ensure that the training set is representative of the real-world conditions that the model will encounter. This technique can help to increase the diversity of the training set and improve the model's ability to generalize to new conditions.

In summary, while the provided context does not explicitly mention the techniques used to address data bias during preprocessing, it does mention some data augmentation strategies that were used to increase the size and diversity of the training set. Other techniques such as stratified sampling, oversampling, undersampling, and diverse data collection can be used to address class-imbalance and improve the performance of deep learning models on imbalanced datasets.
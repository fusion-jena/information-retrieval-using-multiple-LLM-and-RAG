The provided context does not give specific details about the criteria used to split the data for deep learning model training (e.g., train, test, validation). However, it does mention that a twarc2 library was used to handle the data in JSON format returned by the Twitter API. This library simplifies the handling of the quota limits imposed by the Twitter API and the pagination resulting from a single query.

The API works by searching for words, hashtags, or combinations of both in the text of tweets. In an initial phase of the study, some public tweets with observations were provided by the social media manager of the study. The information retrieved from this social network is defined by a large volume of data and a large heterogeneity that requires cleaning and filtering processes. A simplified flowchart of the data acquisition and processing is provided, which includes a first filter consisting in eliminating those tweets that contain images in GIF format and that in most cases correspond to memes.

Additionally, the study mentions that biodiversity observations only make sense when accompanied by their location, allowing, among other applications, to alert about the presence of invasive species or reuse the information in species distribution models. However, the study did not mention any specific criteria used to split the data for deep learning model training.

It's also worth noting that, the study mentions that the main limitation of the current methodology is the nature of the tweets themselves, with a low number of words accompanying the messages, not always well written, which makes it difficult to obtain useful information. This leads to a high number of tweets being lost in the filtering process. Furthermore, using exclusively textual information from tweets to obtain a training dataset would only lead to the creation of a deficient model.

Therefore, based on the provided context, it is not possible to determine the specific criteria used to split the data for deep learning model training.
Based on the information provided, the specific hyperparameters used in the deep learning model are not explicitly mentioned. However, there are some clues that can help us infer some of the possible hyperparameters used.

Firstly, the deep learning models used in the studies are primarily convolutional neural networks (CNNs) and long short-term memory (LSTM) networks. These models typically have hyperparameters such as learning rate, optimizer, number of layers, number of neurons, batch size, and number of epochs.

In the study with the doi: 10.1038/s41598-020-74215-5, the authors used a deep learning model to classify hyperspectral data. They mentioned that they used a "deep learning framework" but did not specify the type of model or the hyperparameters used. However, they did mention that they used a gradient-based sensitivity analysis (GBSA) and Layer-wise Relevance Propagation (LRP) for interpretability, which suggests that they used some form of gradient descent optimizer.

In the study by Chen et al. (2014), they used a deep learning-based classification of hyperspectral data using a CNN model. They mentioned that they used a "deep learning toolbox" but did not specify the hyperparameters used. However, they did mention that they used a backpropagation algorithm for training, which suggests that they used some form of gradient descent optimizer.

In the study by Li et al. (2017), they used a deep learning-based oil palm tree detection and counting for high-resolution remote sensing images using a CNN model. They mentioned that they used the "Caffe deep learning framework" but did not specify the hyperparameters used. However, they did mention that they used a stochastic gradient descent (SGD) optimizer with a learning rate of 0.0001 and a momentum of 0.9.

In the study by Hu et al. (2015), they used a transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery using a CNN model. They mentioned that they used the "Caffe deep learning framework" but did not specify the hyperparameters used. However, they did mention that they used a SGD optimizer with a learning rate of 0.001 and a momentum of 0.9.

In the study by Liang and Li (2016), they used a hyperspectral imagery classification using sparse representations of convolutional neural network features using a CNN model. They mentioned that they used the "Theano deep learning library" but did not specify the hyperparameters used. However, they did mention that they used a GBSA for interpretability, which suggests that they used some form of gradient descent optimizer.

In the study by Zhang et al. (2016), they used a deep learning for remote sensing data: A technical tutorial on the state of the art using various deep learning models. They mentioned that they used different optimizers such as SGD, Adam, and Adagrad. However, they did not specify which optimizer was used in each study.

Therefore, based on the information provided, it can be inferred that the deep learning models used in these studies primarily used some form of gradient descent optimizer such as SGD, Adagrad, or Adam. However, the specific learning rate and other hyperparameters used are not explicitly mentioned in most of the studies.
Based on the provided context, there is no information given about the specific criteria used to determine when training is complete for the models. However, it is mentioned that for the empirical models, an adaptive phase of 2000 iterations was used and sampling from posterior distributions was done using three Markov chains, each of length 20,000, to ensure convergence. Additionally, a burn-in period of 5000 was applied.

In the context of statistical modeling, convergence refers to the point at which the model's estimates have stabilized and are no longer changing significantly. This is typically assessed by monitoring the change in the model's estimates over time and checking whether they have reached a steady state. In the case of Markov chain Monte Carlo (MCMC) methods, such as those used in this study, convergence can be assessed by checking whether the chains have mixed well and whether the estimates from different chains are similar.

However, the context does not mention any specific criteria related to validation loss plateau or any other metric that is commonly used to determine when training is complete in machine learning models. It is possible that such criteria were used but are not mentioned in the provided context.


There is no information given in the provided context about the specific criteria used to determine when training is complete for the models. Convergence of the models was assessed using an adaptive phase of 2000 iterations and sampling from posterior distributions using three Markov chains, each of length 20,000, with a burn-in period of 5000. However, there is no mention of validation loss plateau or any other metric commonly used to determine when training is complete in machine learning models.
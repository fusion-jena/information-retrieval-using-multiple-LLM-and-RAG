Based on the provided context, there is no explicit information about the specific hardware resources used for training the deep learning model. The text focuses more on the training process, model architecture, and parameter selection of the multi-layer perceptron neural network (MLPNN) model.

However, it is generally known that deep learning models often require substantial computational power for training, which is why specialized hardware such as Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) are commonly used. GPUs have many cores and are designed for parallel processing, making them suitable for handling the large matrix and vector operations involved in deep learning training. TPUs, developed by Google, are application-specific integrated circuits (ASICs) designed to accelerate machine learning tasks, particularly for Google's TensorFlow framework.

Given the complexity of the MLPNN training process described, which includes backpropagation and iterative error adjustments, it is reasonable to infer that the trainers likely used high-performance computational resources, which could include GPUs or TPUs. However, without explicit information in the provided context, this answer is based on general knowledge about deep learning training requirements.
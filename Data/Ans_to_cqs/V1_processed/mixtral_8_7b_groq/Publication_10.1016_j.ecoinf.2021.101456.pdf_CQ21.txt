Based on the provided context, there is no explicit information about specific postprocessing steps such as saliency maps or confusion matrix calculation after the model training. However, it does mention some actions taken to evaluate the performance of the model.

After training the model, the authors visualized the latent representation by limiting the size of the bottleneck of the model (latent dimension) to two. This allowed them to observe the distribution of the data in the latent space. They also generated samples from the latent space of the semi-supervised model, which resulted in a latent space cart-wheel. Traversing different lines in the latent space produced samples that smoothly transitioned between different spectra types.

To assess the clustering performance, they selected a number of clusters and included an additional loss term scaling term (γ(t)ℒcls) in the model. This improvement was implemented by cycling γ between 0.01 and 2 to keep the contribution from ℒcls in a similar range as ℒrec and ℒreg.

Moreover, the authors discussed the impact of a higher β term, which favors a well-generalized latent space over good reconstructions, on clustering performance. They noted a reduction in clustering performance when using a higher β term.

In summary, while the context does not explicitly mention postprocessing steps like saliency maps or confusion matrix calculation, it does describe visualization of the latent space, generation of samples from the latent space, and evaluation of clustering performance through the selection of a number of clusters and the inclusion of an additional loss term.
In the provided context, there is no explicit mention of the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that an early stopping strategy was used during the training process. Early stopping is a form of regularization where training is halted as soon as the validation error starts to increase, preventing the model from overfitting to the training data.

The context also mentions that the dataset was divided chronologically into training, validation, and test sets. This is a good practice as it ensures that the data in the validation and test sets are different from the training set data, thereby reducing the chance of overfitting.

However, the context does not provide information on other regularization techniques such as dropout, L1/L2 regularization, or weight decay. Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent overfitting by making the network more robust and less dependent on the training data. L1/L2 regularization, also known as weight decay, is a method that discourages large weights in the model by adding a penalty term to the loss function.

Therefore, while the context provides some information on strategies used to prevent overfitting, it does not explicitly mention the use of dropout, L1/L2 regularization, or weight decay.
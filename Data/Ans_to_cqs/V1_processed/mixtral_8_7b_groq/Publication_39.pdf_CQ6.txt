The provided context discusses the training of deep learning models for action recognition and object detection, particularly using ResNet variants and Mask R-CNN. However, it does not explicitly mention preprocessing steps such as normalization, scaling, or cleaning. Therefore, I cannot provide a specific answer based on the given context.

However, in general, preprocessing data before training deep learning models is a common and crucial step. Preprocessing can include various techniques, such as:

1. Data normalization: This process involves scaling numeric data to a common range, often between 0 and 1. Normalization helps deep learning models converge faster during training and can improve model performance.

2. Data scaling: Similar to normalization, scaling adjusts the range of features. It can be used when features have different units or scales.

3. Data cleaning: This step involves handling missing or corrupted data points. Common techniques include removing, imputing, or interpolating missing values.

4. Data augmentation: This technique generates modified versions of the original data, such as rotations, translations, or flips, to increase the size and diversity of the training set.

While the context does not mention these preprocessing steps explicitly, it does mention using pretrained models. Pretrained models typically use some form of preprocessing, which is often included in the model's implementation. For instance, the provided context states that they use pretrained ResNets on the Kinetics-400 dataset and a pretrained Mask R-CNN on the COCO 2017train dataset. These pretrained models likely include preprocessing steps within their implementations.

In summary, while the given context does not discuss specific preprocessing steps for the deep learning models, it is common practice to perform data normalization, scaling, cleaning, and augmentation before training deep learning models. Additionally, pretrained models often include preprocessing steps within their implementations.
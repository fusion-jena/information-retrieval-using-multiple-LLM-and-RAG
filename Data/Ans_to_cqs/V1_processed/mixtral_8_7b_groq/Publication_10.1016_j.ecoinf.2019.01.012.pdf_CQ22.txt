The performance of the deep learning model is evaluated using several metrics, including Overall Accuracy (OA), Precision (P), Recall (R), F1 score (F1), and the Matthews correlation coefficient (MCC).

Overall Accuracy (OA) is a commonly used metric that measures the proportion of correctly classified samples out of the total number of samples. It provides an overall measure of the model's performance.

Precision (P) is a metric that measures the proportion of true positive predictions out of all positive predictions made by the model. It provides a measure of the model's ability to correctly identify positive samples.

Recall (R) is a metric that measures the proportion of true positive predictions out of all actual positive samples. It provides a measure of the model's ability to identify all positive samples.

The F1 score (F1) is the harmonic mean of Precision and Recall, which provides a balanced measure of the model's performance in terms of both Precision and Recall.

The Matthews correlation coefficient (MCC) is a metric that measures the correlation between the predicted and actual class labels. It provides a measure of the model's ability to distinguish between classes and is considered a more robust measure than Overall Accuracy for imbalanced datasets.

These metrics are calculated based on the confusion matrix, which consists of pixel numbers for true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). The confusion matrix provides a detailed breakdown of the model's performance in terms of correctly and incorrectly classified samples.

In addition to these metrics, the model's accuracy and loss are also calculated for both the training and validation sets. The loss serves as a measure of how far the model's predictions differ from the actual class, and a lower loss indicates a better fit of the model to the data.

The model's performance is evaluated on two different datasets: individual tiles and whole repeat photographs. The prediction accuracy on individual tiles is calculated using a separate test set of 5796 tiles, while the accuracy on whole repeat photographs is evaluated based on the image pairs of the second set of photographs and compared to the corresponding manual classification.

Overall, these metrics provide a comprehensive evaluation of the model's performance in terms of both its ability to correctly classify samples and distinguish between classes.
The strategy implemented to monitor the model performance during training is not explicitly mentioned in the provided context. However, it can be inferred that the authors used various evaluation metrics to assess the final performance of their models.

For instance, Navares and Aznarte (2020) used a deep learning LSTM model to predict air quality and assessed its performance using statistical indicators such as Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and R-squared. Similarly, Chianese et al. (2019) used a multi-layer perceptron model to predict ambient particulate matter concentration and evaluated its performance based on the coefficient of determination (R2) and Mean Absolute Percentage Error (MAPE).

Moreover, Neckel et al. (2023) compared the performance of their self-made passive samplers with the Sentinel-3b SYN satellite images for monitoring air pollution in Budapest. They used statistical tests such as the Wilcoxon signed-rank test and the Kruskal-Wallis test to compare the differences between the two datasets.

Therefore, while the specific strategy for monitoring the model performance during training is not mentioned, it can be inferred that the authors used various evaluation metrics to assess the final performance of their models. These metrics could also be used to monitor the model performance during training, for instance, by computing them after each training epoch and tracking their evolution. However, this specific aspect is not addressed in the provided context.
Based on the provided context, there is no explicit information about a strategy implemented to monitor the model performance during training. The text focuses on the comparison of different estimators for forest inventory analysis and does not mention any details about model training or monitoring performance during training.

However, it is important to note that monitoring model performance during training is a crucial step in developing a reliable and accurate predictive model. This can be achieved through various strategies such as:

1. **Validation set approach**: Splitting the dataset into a training set and a validation set. The model is trained on the training set, and its performance is evaluated on the validation set. This helps to ensure that the model does not overfit the training data.

2. **Cross-validation**: A technique where the dataset is divided into several folds. The model is trained on all but one fold, and its performance is evaluated on the left-out fold. This process is repeated for each fold, and the average performance is calculated. Cross-validation provides a more robust estimate of the model's performance than the validation set approach.

3. **Early stopping**: A technique used to prevent overfitting by stopping the training process before the model starts to memorize the training data. This requires monitoring the model's performance on a validation set during training and stopping the training when the performance on the validation set starts to degrade.

4. **Learning curves**: A graph showing the model's performance as a function of the training set size. Learning curves help to diagnose whether the model is underfitting or overfitting and can guide the selection of appropriate model complexity and training set size.

Without more information, it is not possible to determine which of these strategies (if any) were used to monitor the model performance during training in the context provided.
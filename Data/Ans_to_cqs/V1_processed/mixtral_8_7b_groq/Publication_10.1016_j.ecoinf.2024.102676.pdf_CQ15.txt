The provided context does not directly mention the optimization techniques used in the deep learning pipeline. However, it does discuss a study where deep learning models were used to classify three sympatric otter species. Although the specific optimization techniques are not specified, it can be inferred that some form of optimization algorithm was employed during the training of these deep learning models.

Common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adagrad, Adadelta, and Adam. SGD is a simple optimization algorithm that updates the model parameters by computing the gradient of the loss function concerning each training example. Adagrad, Adadelta, and Adam are adaptive learning rate methods that modify the learning rate for each parameter during training. These methods often lead to faster convergence and better performance compared to SGD.

In summary, while the provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, it is reasonable to assume that some form of optimization algorithm, such as SGD, Adagrad, Adadelta, or Adam, was employed during the training of the deep learning models.
In the context provided, there are a few strategies mentioned that can be used to handle randomness in the deep learning pipeline. One of the ways to manage randomness is by setting a random seed value. This ensures that the same initial random weights are used each time the model is trained, allowing for reproducibility of results.

In the study by Tamhane et al. (2017), the authors used a fixed random seed value for their experiments to ensure reproducibility. Similarly, in the study by Shah and Mishra (2020), the authors set a random seed value for their word2vec model to ensure consistent results.

Additionally, the use of stochastic gradient descent (SGD) in optimizing the regularized cost function can also introduce randomness in the deep learning pipeline. To handle this, the authors in the study by Xie et al. (2017) used a mini-batch size of 64 samples and trained the algorithm during 100 epochs using Adam as the algorithm to optimize the network parameters. This helps to reduce the randomness introduced by SGD while still allowing for efficient optimization.

Furthermore, the study by Piech et al. (2015) on deep knowledge tracing used a Bayesian approach to handle randomness in the deep learning pipeline. They used a probabilistic matrix factorization model that allowed for uncertainty in the model parameters, which helped to account for the randomness in the data.

In summary, strategies to handle randomness in the deep learning pipeline include setting a random seed value for reproducibility, using mini-batch sizes and optimization algorithms to reduce the randomness introduced by SGD, and using probabilistic models to account for uncertainty in the data.

References:

Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E., 2011b. Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12, 2825–2830.

Piech, C., Bassen, J., Huang, J., Ganguli, S., Sahami, M., Guibas, L.J., Sohl-Dickstein, J., 2015. Deep knowledge tracing. In: Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., Garnett, R. (Eds.), Advances in Neural Information Processing Systems, vol. 28. Curran Associates, Inc, pp. 505–513.

Shah, J., Mishra, B., 2020. Iot-enabled low power environment monitoring system for prediction of pm2.5. Pervas. Mobile Comput. 67, 101175 [Online]. Available: <http://www.sciencedirect.com/science/article/pii/S1574119220300560>.

Sharma, A.K., Chaurasia, S., Srivastava, D.K., 2020. Sentimental short sentences classification by using cnn deep learning model with fine tuned word2vec. Procedia Computer Science 167, 1139–1147. International Conference on Computational Intelligence and Data Science. [Online]. Available: <http://www.sciencedirect.com/science/article/pii/S1877050920308826>.

Tamhane, A., Arora, S., Warrier, D., 2017. Modeling contextual changes in user behaviour in fashion e-commerce. In: Kim, J., Shim, K., Cao, L., Lee, J.-G., Lin, X., Moon, Y.-S. (Eds.), Advances in Knowledge Discovery and Data Mining. Springer International Publishing, Cham, pp. 539–550.

Xie, K., Ning, X., Wang, X., Xie, D., Cao, J., Xie, G., Wen, J., 2017. Recover corrupted data in sensor networks: a matrix completion solution. IEEE Trans. Mob. Comput. 16 (5), 1434–1448.

Xue, H.-J., Dai, X., Zhang, J., Huang, S., Chen, J., 2017a. Deep matrix factorization. In: WHO Regional Office for Europe. Tech. Rep.
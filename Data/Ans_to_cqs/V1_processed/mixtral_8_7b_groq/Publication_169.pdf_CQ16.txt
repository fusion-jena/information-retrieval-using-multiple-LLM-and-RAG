The criteria used to determine when training is complete is not explicitly stated in the provided context. However, it can be inferred that the training process is stopped when a certain level of accuracy is reached. The context mentions that the accuracy obtained reaches 95% on 140 epochs, but it does not specify if this was the stopping criterion.

In general, training of machine learning models is often stopped when the performance on a validation set plateaus or starts to degrade, even if the training loss is still decreasing. This is because a model that performs well on the training set may not generalize well to unseen data if it is overfitting to the training set. By monitoring the performance on a validation set that is separate from the training set, it is possible to detect if the model is starting to overfit and stop the training process before this happens.

In the context of the study, it is possible that the authors stopped the training process based on a combination of factors, such as reaching a desired level of accuracy, observing a plateau in the validation loss, and considering the computational resources available. However, without further information, it is not possible to provide a definitive answer to this query.
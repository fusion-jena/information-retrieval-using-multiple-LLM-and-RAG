The provided context does not give specific information about the optimization techniques used in the deep learning pipeline. However, it is mentioned that the deep learning models were implemented using Mathematica notebooks and CNNs with architectures that include convolutional layers, batch normalization layers, ReLU activation functions, pooling layers, flattening layers, dropout layers, and linear and softmax layers. These architectures are commonly trained using optimization algorithms such as Stochastic Gradient Descent (SGD), Adam, or RMSprop.

To be more precise, the optimization techniques used in the deep learning pipeline can be found in the Mathematica notebooks provided as supplementary materials (Suppl. materials 1, 3, 2, 4). The notebooks contain the code used to define and train the CNNs, and it is likely that the training process includes an optimization algorithm. To determine the specific optimization technique used, one would need to access and examine the code in the notebooks.

In summary, while the provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, it is highly probable that optimization algorithms such as SGD, Adam, or RMSprop were used during the training process. The specific optimization technique can be determined by examining the code in the Mathematica notebooks.
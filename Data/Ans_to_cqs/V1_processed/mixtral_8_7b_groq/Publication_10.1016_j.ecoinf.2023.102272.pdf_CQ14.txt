Based on the provided context, there is no explicit information about how the hyperparameters of the models were optimized, such as through grid search, random search, or any other optimization method. However, it is mentioned that for model selection, 20 models were randomly generated for each of the four deep-ANN architecture types, and these models were trained with a small subset of the training data for 4 epochs. The accuracy of these candidate models was then compared using a left-out validation data set, and the model with the highest performance was selected for training on the full training data for up to 30 epochs.

In addition, the context provides a table (Table 3) that lists the parameters tested and their values for each classical machine learning model used and each test year. These parameters could be considered as hyperparameters, and the table shows that different values were tested for each parameter and year. However, the method used to determine these values is not specified in the context.

Therefore, while the context provides information on the model selection procedure and the parameters tested, it does not explicitly mention the method used to optimize the hyperparameters.
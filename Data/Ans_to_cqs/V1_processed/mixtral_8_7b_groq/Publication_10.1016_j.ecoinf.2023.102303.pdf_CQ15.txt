The provided context discusses two deep learning models, EfficientNet-B7 and Vision Transformer (ViT), used in underwater fish video processing in turbid waters. However, it does not explicitly mention any optimization techniques such as Stochastic Gradient Descent (SGD) or Adam.

EfficientNet-B7 is a Convolutional Neural Network (CNN) that uniformly scales all depth, channel, and resolution dimensions by a compound rate. This scaling is applied in nine stages with different repeat amounts. The DNN designer can adjust the CNNâ€™s scaling by changing this compound rate, resulting in the B0 to B7 variants. Among these variants, EfficientNet-B7 is proven to produce more accurate results.

The Vision Transformer (ViT) model is used in conjunction with the EfficientNet-B7 model. The ViT model is trained on a weakly-supervised fish dataset collected from turbid waters. This model uses semi-supervised contrastive learning for significantly accelerated model deployment time and a weakly-supervised model to shorten deployment time while providing high accuracy.

In general, deep learning optimization techniques such as SGD and Adam are used to minimize the loss function and improve the model's performance. SGD is an optimization algorithm used to train neural networks, where the gradient of the loss function is calculated for each training example and the model parameters are updated accordingly. Adam, on the other hand, is an extension of SGD that adaptively adjusts the learning rate based on the historical gradient information.

Therefore, while the provided context does not explicitly mention any optimization techniques, it is likely that optimization techniques such as SGD or Adam were used during the training of the EfficientNet-B7 and ViT models.
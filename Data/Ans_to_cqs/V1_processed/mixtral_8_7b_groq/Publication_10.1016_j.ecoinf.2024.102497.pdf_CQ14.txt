The context does not provide specific details about how the hyperparameters of the Random Forest Regression (RFR) model were optimized for the nearshore bathymetry study. However, it is mentioned that fine-tuning the model parameters was crucial to achieving optimal prediction results.

There are several methods to optimize hyperparameters in machine learning models, including grid search, random search, and Bayesian optimization. Grid search involves creating a grid of hyperparameter values and training the model for each combination. Random search, on the other hand, randomly selects hyperparameter values from a defined range and trains the model. Bayesian optimization uses a probabilistic approach to select hyperparameter values that are likely to result in better model performance.

In the context of the RFR model, hyperparameters such as 'n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', and 'max_features' could be optimized using any of these methods. 'n_estimators' refers to the number of trees in the forest, while 'max_depth' determines the maximum depth of each tree. 'min_samples_split' and 'min_samples_leaf' specify the minimum number of samples required to split an internal node or form a leaf node, respectively. 'max_features' determines the maximum number of features considered when splitting a node.

Without specific information on how the hyperparameters were optimized in the nearshore bathymetry study, it is difficult to provide a definitive answer. However, it can be inferred that some form of hyperparameter tuning was performed to achieve optimal prediction results.
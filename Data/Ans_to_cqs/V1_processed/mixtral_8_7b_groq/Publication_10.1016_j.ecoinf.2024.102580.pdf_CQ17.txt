Based on the provided context, the text does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of weighted cross-entropy as the loss function and the stochastic gradient descent optimization algorithm to minimize the loss function. These techniques can help reduce overfitting to some extent.

In general, deep learning models can be prone to overfitting, especially when training on small datasets or when the model has a large number of parameters. Regularization methods are often employed to prevent overfitting and improve the model's generalization performance.

Two common regularization techniques are dropout and L2 regularization. Dropout is a technique where randomly selected neurons are "dropped out" during training, which helps prevent over-reliance on any single neuron and encourages the model to learn more robust features. L2 regularization, on the other hand, adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to learn smaller weights, which can help prevent overfitting.

Without more information from the text, it is difficult to say for sure whether these or other regularization techniques were used in the deep learning pipeline described. However, it is important to consider regularization methods when developing deep learning models to ensure that they generalize well to new data.
The provided context does not contain information about regularization methods used in deep learning pipelines such as dropout or L2 regularization. The context includes various studies related to digital soil mapping, land use classification, and image analysis using artificial neural networks (ANN), support vector machines (SVM), and random forests (RF), but none of them discuss regularization methods to prevent overfitting.

Overfitting is a common issue in machine learning models where the model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization methods are used to prevent overfitting by adding a penalty term to the loss function, which discourages the learning of overly complex models.

Dropout is a regularization technique used in deep learning models where randomly selected neurons are dropped out during training, effectively creating an ensemble of models. L2 regularization, also known as weight decay, adds a penalty term proportional to the square of the model weights to the loss function, discouraging large weight values.

Therefore, I don't know the answer to the query based on the provided context.
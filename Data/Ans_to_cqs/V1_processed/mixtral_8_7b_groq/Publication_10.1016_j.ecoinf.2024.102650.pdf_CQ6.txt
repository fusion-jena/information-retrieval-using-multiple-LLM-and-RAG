Before training a deep learning model, several preprocessing steps are typically involved, including text cleaning, normalization, and scaling.

Text cleaning is an essential step in text data preprocessing, especially for social media data such as tweets. Tweets are characterized by sparsity, real-time nature, and irregularity, which increase the difficulty of text processing (Edwards et al., 2022; Garg and Kumar, 2018; Sun et al., 2021). Text cleaning includes text filtering and Chinese text segmentation. Text filtering involves removing unnecessary symbols and noise, such as notes, advertisements, useless numbers, punctuation marks, stop words, and repeated texts on websites. User-generated text that deviates from the theme is also filtered out through manual review. Chinese text segmentation involves dividing natural language composed of Chinese text into meaningful words, which is a key technology in text analyses (Tian et al., 2021). The Jieba Library in Python is used for Chinese text segmentation, and a lexical analysis is included, which involves identifying location, time, and organizational nouns.

Normalization is another critical preprocessing step in deep learning, especially for numerical data. Normalization involves scaling numerical data to a specific range, typically between 0 and 1, to improve model performance. Normalization ensures that all features have the same scale, preventing any single feature from dominating the model's learning process. There are various normalization techniques, including min-max scaling, z-score normalization, and decimal scaling (Patro and Kumar, 2015).

Scaling is a preprocessing step that involves adjusting the range of numerical data to improve model performance. Scaling is necessary because some machine learning algorithms are sensitive to the scale of the data. For instance, gradient descent algorithms used in deep learning models converge faster when the data is scaled. Scaling techniques include standardization, normalization, and robust scaling (Le Saux et al., 2018).

In summary, before training a deep learning model, text cleaning, normalization, and scaling are critical preprocessing steps. Text cleaning involves text filtering and Chinese text segmentation, while normalization involves scaling numerical data to a specific range. Scaling involves adjusting the range of numerical data to improve model performance. These preprocessing steps ensure that the data is clean, normalized, and scaled, thereby improving the model's performance.
The text provided does not give detailed information about the process of deploying the trained deep learning model. However, it does mention that three DNN architectures were built and trained for the entire dataset for 1000 epochs. This implies that the models were likely trained and tested on a local machine or cloud-based platform.

After training, the models would need to be serialized or saved to a file in order to be deployed. Model serialization involves saving the model's architecture, weights, and other relevant information to a file, which can then be loaded and used for making predictions on new data. Common formats for model serialization include HDF5, JSON, and Protocol Buffers.

Once the models are serialized, they can be deployed on a platform suitable for the task at hand. For image processing and remote sensing applications, platforms such as Google Earth Engine, Amazon Web Services (AWS), or Microsoft Azure could be used. These platforms provide the necessary computational resources to handle large datasets and complex models.

In addition, the text mentions the importance of optimizing hyperparameters for efficient utilization of computational resources. Therefore, it is likely that the models were optimized for both accuracy and computation time during the training phase. This optimization would help ensure that the deployed models are not only accurate but also efficient in terms of computation time and costs.

Overall, while the text does not provide explicit details about the process of deploying the trained deep learning model, it can be inferred that the models were likely serialized and deployed on a suitable platform for image processing and remote sensing applications. The optimization of hyperparameters during the training phase would have also contributed to the efficient utilization of computational resources.
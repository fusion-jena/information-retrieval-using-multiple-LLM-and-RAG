Based on the provided context, there are two regularization methods mentioned that are used to prevent overfitting in deep learning models: dropout and adjusting the number of training epochs.

Dropout is a regularization technique used in neural networks to prevent overfitting by randomly dropping out, or "turning off", a proportion of neurons during training. This helps to ensure that the network does not become too reliant on any one neuron or set of neurons, and therefore generalizes better to new data. The context does not specify a particular dropout rate, but it is mentioned that the learning rate and dropout rate adopt the same value of 0.001.

The second regularization method mentioned is adjusting the number of training epochs based on the depth of the model. Specifically, for models with less than 100 layers, the model trains for 60 epochs, while models with 100 or more layers train for 80 epochs. This is because models with lower depth have a higher probability of overfitting if trained for too many epochs. By adjusting the number of training epochs based on the depth of the model, the authors aim to prevent overfitting and improve the model's ability to generalize to new data.

It is worth noting that the context does not mention any other regularization methods such as L1 or L2 regularization. However, the authors do mention using a batch size of 30 during training, which can also help to prevent overfitting by introducing some variability into the training data.

In summary, the two regularization methods mentioned in the context are dropout and adjusting the number of training epochs based on the depth of the model. These methods help to prevent overfitting and improve the model's ability to generalize to new data.
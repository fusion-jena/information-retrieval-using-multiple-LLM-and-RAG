The deep learning models mentioned in the context use various hyperparameters during training. 

For the diffusion model, an unsupervised pre-training is performed. The Adam optimizer is utilized during the training of the proposed diffusion model. Additionally, a scheduler is employed to adjust the learning rate when the network reaches a plateau. The learning rate values tested for this model are 0.0001, 0.001, 0.01, and 0.1. 

In the case of the multilayer perceptron (MLP), the Adam optimizer is also used. The learning rate values explored for the MLP are 0.00001 and 0.0001. Batch sizes of 8, 16, 32, and 64 were tested for the MLP. Weight decay values ranged between 0.0001 and 0.001 for both the diffusion model and the MLP. A dropout of 0.1 was used in the MLP.

For the Focal Loss, additional parameters were investigated. The parameter α was set as the inverse of class frequency, 0.25, or 0.5. The parameter γ was set to values between 0.25 and 2.

For the autoencoder, the same learning rate, batch size, and weight decay values used in the MLP were explored. Additionally, dropout values of 0.05, 0.1, 0.2, and 0.25 were tested.

In summary, the deep learning models use the Adam optimizer, with various learning rate, batch size, weight decay, and dropout values. The Focal Loss introduces additional parameters α and γ, which were also explored.
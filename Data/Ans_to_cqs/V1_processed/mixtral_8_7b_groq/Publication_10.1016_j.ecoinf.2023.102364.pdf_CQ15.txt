Based on the provided context, the text discusses various machine learning techniques and algorithms used for data analysis, but it does not mention any specific optimization techniques used in a deep learning pipeline such as Stochastic Gradient Descent (SGD) or Adam.

The techniques mentioned in the text are mainly focused on tree-based algorithms, including RandomForest, CART ensembles and bagger, and TreeNet. These algorithms use different concepts such as bagging, boosting, and ensembles to improve predictive accuracy.

For instance, RandomForest uses bagging, which involves creating multiple subsets of the original data and training separate decision trees for each subset. The final prediction is then made by combining the predictions from all the decision trees.

CART ensembles and bagger combine trees using a simple bagging concept, where multiple trees are trained on different subsets of the data, and the final prediction is made by averaging the predictions from all the trees.

TreeNet algorithms employ stochastic gradient boosting, which involves training multiple trees in a sequential manner, where each tree is trained to correct the errors made by the previous tree.

However, none of these techniques specifically relate to optimization techniques used in deep learning pipelines such as SGD or Adam.

Therefore, based on the provided context, we cannot answer the query regarding the optimization techniques used in the deep learning pipeline.
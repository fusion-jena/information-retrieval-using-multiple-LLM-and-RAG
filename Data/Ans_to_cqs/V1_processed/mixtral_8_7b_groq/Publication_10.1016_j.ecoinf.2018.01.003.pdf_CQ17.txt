The provided context discusses the use of neural networks and deep learning but does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, based on general knowledge of deep learning, I can provide an answer to this query.

Regularization methods are essential in deep learning to prevent overfitting, which occurs when a model learns the training data too well, including its noise, and performs poorly on unseen data. Two commonly used regularization methods in deep learning are dropout and L2 regularization.

Dropout is a regularization technique introduced by Geoffrey Hinton and his colleagues in 2012. It involves randomly dropping out, or setting to zero, a proportion of neurons in a layer during training. This prevents over-reliance on any single neuron and encourages the network to learn more robust features. Dropout has been shown to improve the performance of deep neural networks significantly.

L2 regularization, also known as weight decay, is another regularization technique that adds a penalty term to the loss function, proportional to the square of the norm of the model's weights. This penalty term discourages large weight values, which can lead to overfitting. By adding this term to the loss function, the model is encouraged to learn smaller weights, leading to a simpler and more generalizable model.

In summary, while the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, dropout and L2 regularization are two commonly used techniques that can help prevent overfitting and improve the performance of deep learning models.
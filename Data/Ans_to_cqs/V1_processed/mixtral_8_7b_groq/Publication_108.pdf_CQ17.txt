The regularization methods used to prevent overfitting in the deep learning pipeline, as described in the provided context, include dropout and the use of a specific activation function. Dropout is a regularization technique that was used in the decoding path of the deep learning model. It works by randomly dropping some neurons during the training process, causing the feed-forward network to perform worse. This helps to prevent overfitting by making the model more robust and less dependent on the specific training data. The dropout rate was set to increase from one encoding block to the next by a step of 0.1, and was found to work well for the specific setting.

Additionally, the activation function used after the second convolutional layer in each decoding block was the Scaled Exponential Linear Unit (SeLU). This activation function was chosen because it was used in the encoding path, and using the same activation function for the concatenated feature maps helps to ensure that the feature maps are homogeneous. The use of the SeLU activation function can also be considered a form of regularization, as it helps to prevent overfitting by promoting the stability and robustness of the model.

It is important to note that the provided context does not mention the use of L2 regularization as a method to prevent overfitting. However, dropout and the use of a specific activation function are both effective regularization techniques that can help to prevent overfitting in deep learning models.
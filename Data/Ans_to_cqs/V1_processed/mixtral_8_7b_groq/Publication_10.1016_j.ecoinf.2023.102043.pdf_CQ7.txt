The dataset used for goat vocalization detection was divided into three sets for training, validation, and testing purposes. The division was kept consistent throughout the experiments with both Random Forest (RF) and YAMNet-based models. Specifically, 70% of the dataset was used for training, 10% for validation, and the remaining 20% for testing.

The data was initially collected and annotated from four farms, resulting in a total of approximately 10 million sound events, which lasted for more than 34,000 hours. The recording campaign was planned to last for an entire year to capture the variations in goat vocalizations throughout the year.

Before training the deep learning models, the audio data went through a series of preprocessing steps. First, the audio was converted to a monophonic channel and resampled at 16 kHz. Then, the spectrogram was extracted using Short-Time Fourier Transform (STFT) magnitudes of 25 ms, which were overlapped by 15 ms. After that, the Mel spectrogram was calculated by mapping to 64 bins of the Mel scale, which covered the range of 125â€“7500 Hz. Finally, the preprocessed data was fed to the YAMNet model for extracting the embeddings.

In summary, the data was split into training, validation, and testing sets, containing 70%, 10%, and 20% of the data, respectively. The data was preprocessed by converting the audio to a monophonic channel, resampling it, extracting the spectrogram, and calculating the Mel spectrogram. The preprocessed data was then used to extract the embeddings using the YAMNet model for training the deep learning models.
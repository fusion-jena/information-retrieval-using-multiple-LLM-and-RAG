Based on the provided context, there are several postprocessing steps that were carried out after the model training. However, the text does not explicitly mention saliency maps. Here are the other steps that were involved:

1. Metrics calculation: The performance of the models was evaluated using various metrics such as AUC (Area Under the Curve), Omission, TPR (True Positive Rate), k (Kappa), and TSS (True Skill Statistic). These metrics were calculated for both the training set and the two test data sets (Test set #1 and Test set #2). The best model for each metric was highlighted where higher was better for AUC, TPR, TSS, and Kappa, and lower better for Omission.

2. Threshold determination: The text suggests that the threshold used for determining the presence or absence of a species was the 10th percentile training presence. This threshold was used to ensure comparability of the model evaluation metrics across the two test datasets.

3. Treatments for testing potential biases and errors: Each of the iSDMs (Incremental Species Distribution Models) were subjected to five treatments to test the impact of potential biases and errors. These treatments included using random backgrounds, spatial filtering of input data, and using different proportions of training records.

4. Comparison of models: The models were compared based on their performance on the two test datasets. The text mentions that the models converged on the maximum test sensitivity plus specificity (TSSmax) as the single most appropriate threshold. However, since two different test data sets were used, two different thresholds would have been generated, making each non-comparable to the other.

Therefore, the 10th percentile training presence was used as the threshold, which was the same across both test datasets, allowing model evaluation metrics to be directly compared.

In summary, the postprocessing steps involved calculating various metrics for evaluating the models, determining the threshold for presence/absence, subjecting the models to treatments for testing potential biases and errors, and comparing the models based on their performance on the two test datasets.
The regularization methods used to prevent overfitting in the deep learning pipeline, as described in the given context, include dropout, a learning rate decay with a Gamma of 0.95, and data augmentation through the use of a large number of thumbnails per fish species class.

Dropout is a technique that prevents overfitting by randomly dropping out, or "turning off," a proportion of neurons in a layer during training, which helps to make the network more robust and less prone to overfitting to the training data. In this case, a dropout of 50% was used.

Learning rate decay is another regularization technique that was used in this pipeline. By gradually decreasing the learning rate over time, the model is able to make smaller, finer adjustments to its weights as it approaches the optimal solution, which can help to prevent overfitting. In this case, a learning rate decay with a Gamma of 0.95 was used.

Data augmentation is a technique that can also help to prevent overfitting by artificially increasing the size of the training set. In this case, data augmentation was achieved by using at least 2200 thumbnails per fish species class. This helps the model to generalize better and be more robust to new, unseen data.

It is worth noting that the authors do not explicitly mention the use of L2 regularization in the given context. L2 regularization, also known as weight decay, is a technique that adds a penalty term to the loss function, which discourages the model from assigning too much importance to any one weight, and helps to prevent overfitting.

In summary, the regularization methods used in this deep learning pipeline include dropout, learning rate decay, and data augmentation through the use of a large number of thumbnails per fish species class.
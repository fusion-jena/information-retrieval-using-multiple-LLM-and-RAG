The regularization methods used to prevent overfitting in the deep learning pipeline, as mentioned in the provided context, include dropout and a regularization method that is not explicitly specified but could be L2 regularization or another type of regularization.

Dropout is a regularization technique that is applied during the training stage of the neural network model. It works by randomly setting a fraction rate of neurons in a layer to zero during training, which helps to prevent overfitting by reducing the co-adaptation of neurons. In the described paper, the 6th layer dropout rate is 0.1, and for the other layers, it is 0. This means that 10% of the neurons in the 6th layer and no neurons in the other layers are randomly set to zero during training. However, all neurons are activated during the test.

The other regularization method used in the deep learning pipeline is not explicitly mentioned in the provided context. Still, it can be inferred that another regularization method is used in conjunction with dropout since the paper states that the regularization method is used to prevent overfitting. L2 regularization, also known as weight decay, is a common regularization method that adds a penalty term to the loss function, which discourages large weight values and helps to prevent overfitting.

Therefore, the regularization methods used in the deep learning pipeline to prevent overfitting are dropout and another regularization method, which could be L2 regularization or another type of regularization.
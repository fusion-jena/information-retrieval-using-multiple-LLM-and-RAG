The provided context does not directly discuss the criteria used to split data for deep learning model training. However, it does provide information about the evaluation of species distribution models (SDMs) and the importance of correcting sampling bias in environmental spaces.

In the context, two indicators are mentioned for evaluating the effectiveness of SDMs: the area under the receiver operating characteristic curve (AUC) and delta D (ΔD). The AUC measures the model's ability to distinguish between presences and absences, while ΔD evaluates the model's performance in predicting the presence of species in new locations.

The context also discusses the importance of addressing sampling bias in environmental spaces. This bias can be caused by environmental variables and can affect the performance of SDMs. Therefore, correcting sampling bias in environmental spaces is beneficial and requires further testing.

In the context of deep learning model training, data splitting is an essential step to ensure the model's generalization and prevent overfitting. The most common approach is to split the data into three sets: training, validation, and testing.

The training set is used to train the model, and the validation set is used to tune the model's hyperparameters. The testing set is used to evaluate the model's performance on unseen data.

The criteria used to split the data can vary depending on the problem and the size of the dataset. A common approach is to use a 70-15-15 or 80-10-10 split for the training, validation, and testing sets, respectively. However, other approaches, such as k-fold cross-validation, can be used to ensure that the model is trained and evaluated on different subsets of the data.

In summary, while the provided context does not directly discuss the criteria used to split data for deep learning model training, it highlights the importance of evaluating SDMs and addressing sampling bias in environmental spaces. In deep learning model training, data splitting is an essential step to ensure the model's generalization and prevent overfitting. The criteria used to split the data can vary depending on the problem and the size of the dataset. Common approaches include using a 70-15-15 or 80-10-10 split for the training, validation, and testing sets, respectively, or using k-fold cross-validation.
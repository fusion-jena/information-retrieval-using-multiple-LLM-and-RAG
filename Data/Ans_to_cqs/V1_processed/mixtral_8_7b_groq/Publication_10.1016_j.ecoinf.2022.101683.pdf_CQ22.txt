The provided context discusses the evaluation of various models for species distribution, specifically the Muntjac range in Great Britain and Ireland Isles, using different methods such as Random background, Filtered, Targeted, Filtered and Targeted, and Weighted. However, it does not specify that the models being evaluated are deep learning models.

The metrics used to evaluate the performance of these models include:

* AUC (Area Under the Curve)
* TPR (True Positive Rate, also known as Recall)
* TSS (True Skill Statistics)
* Kappa
* Omission

The best model for each metric is highlighted in bold where higher is better for AUC, TPR, TSS, and Kappa, and lower is better for Omission.

The context explains that the threshold used for evaluation is the 10th percentile training presence, which was the same across both test datasets, allowing model evaluation metrics to be directly compared.

However, it does not mention any metrics commonly used in deep learning model evaluation such as accuracy, precision, F1 score, etc.

Therefore, based on the provided context, it is not possible to answer the query about what metrics are used to evaluate the performance of a deep learning model.
The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it discusses the use of normalization as a technique that could potentially help the learning process and improve model performance. Normalization, while not a regularization method per se, can indirectly contribute to preventing overfitting by making the learning process easier and allowing for better generalization.

Regularization methods, such as dropout and L2 regularization, are explicitly designed to prevent overfitting by adding a penalty term to the loss function or by randomly dropping out neurons during training. Dropout randomly sets a fraction rate of input units to 0 at each update during training, which helps prevent over-dependence on any one neuron and promotes the development of a more robust model. L2 regularization, on the other hand, adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to develop smaller weights, which can help prevent overfitting by reducing the complexity of the model.

In summary, while the provided context does not explicitly mention the regularization methods used to prevent overfitting, it does discuss the use of normalization as a technique that can indirectly contribute to preventing overfitting by making the learning process easier and allowing for better generalization. Regularization methods such as dropout and L2 regularization are explicitly designed to prevent overfitting by adding a penalty term to the loss function or by randomly dropping out neurons during training.
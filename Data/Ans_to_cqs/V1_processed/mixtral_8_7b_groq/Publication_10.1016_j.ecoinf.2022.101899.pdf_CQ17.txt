The provided pieces of context do not directly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, the studies discussed focus on wildfire event prediction using deep learning and other techniques. Regularization methods like dropout and L2 regularization are common practices in deep learning to prevent overfitting.

Dropout is a regularization technique used in neural networks, where randomly selected neurons are ignored during training, preventing over-reliance on specific neurons and reducing overfitting. L2 regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the model's weights. This encourages smaller weights, reducing overfitting by limiting the complexity of the model.

Since these techniques are not explicitly mentioned in the context, it's unclear if they were used in the studies. However, it's safe to assume that best practices in deep learning, such as dropout and L2 regularization, were employed to prevent overfitting.

 
Regularization methods like dropout and L2 regularization are common practices in deep learning to prevent overfitting. Although the provided context does not explicitly mention these techniques, it's reasonable to assume they were used in the deep learning pipeline for wildfire event prediction.
The criteria used to split the data for deep learning model training are not explicitly stated in the provided context. However, it is mentioned that the authors used Mathematica notebooks to define and train their convolutional neural networks (CNNs). In deep learning practice, it is common to split the data into three subsets: training, validation, and testing.

In the unstained/stained CNN, the authors used two convolutional layers and two pooling layers. The input shape is a 3-tensor of size 3×256×256, and the output is a class represented by a vector of size 2. Similarly, in the clubmoss/spikemoss CNN, the input shape is also a 3-tensor of size 3×256×256, and the output is a class represented by a vector of size 2.

The authors used a FlattenLayer and a DropoutLayer in the clubmoss/spikemoss CNN, which are commonly used in deep learning to prevent overfitting. Overfitting occurs when the model learns the training data too well, including its noise, and performs poorly on unseen data. The DropoutLayer randomly drops out a fraction of the input units during training, which helps to prevent overfitting.

The authors also used a BatchNormalizationLayer in both CNNs, which is a technique to normalize the activations of the neurons in a layer. Batch normalization helps to reduce the internal covariate shift, which is the change in the distribution of the inputs to a layer caused by the change in the parameters of the previous layers. By reducing the internal covariate shift, batch normalization helps the model to converge faster and perform better.

In summary, while the specific criteria used to split the data for deep learning model training are not provided in the context, it is common practice to split the data into training, validation, and testing subsets. The authors used techniques such as FlattenLayer, DropoutLayer, and BatchNormalizationLayer to prevent overfitting and improve the performance of their CNNs.
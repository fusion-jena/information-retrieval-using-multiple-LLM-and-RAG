Based on the provided context, there is no explicit mention of optimization techniques such as Stochastic Gradient Descent (SGD) or Adam used in the deep learning pipeline. The context mainly focuses on the use of deep learning for GDP prediction using CO2 emission data and transfer learning approaches.

However, it is important to note that optimization techniques like SGD and Adam are commonly used in deep learning pipelines for training neural networks. These techniques help to minimize the loss function and optimize the model's parameters during training.

For instance, in the paper "Generalized hidden-mapping ridge regression, knowledge-leveraged inductive transfer learning for neural networks, fuzzy systems and kernel methods" by Deng et al. (2014), they proposed a transfer learning approach for neural networks, fuzzy systems, and kernel methods. Although they did not explicitly mention the use of SGD or Adam, it is likely that they used some optimization technique for training their models.

Therefore, while the provided context does not explicitly mention the use of optimization techniques like SGD or Adam, it is reasonable to assume that they were used in the deep learning pipeline for training the neural network models.
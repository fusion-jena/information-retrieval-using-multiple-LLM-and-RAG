Based on the provided context, it is not explicitly mentioned which measures were taken to ensure the generalizability of the deep learning model. However, there are some indications that cross-validation or some form of data splitting might have been used.

In the context, there is a reference to a table (Table 1) that presents the results of model evaluation metrics for different partitions and a loss factor C. This suggests that the data was divided into several partitions, and the model was evaluated on each partition. This is a common technique used in machine learning to assess the performance of a model and ensure its generalizability.

Moreover, the context also mentions the use of a loss factor C, which could be an indicator of a stratified sampling approach. Stratified sampling is a method used to ensure that each partition of the data contains a representative sample of the different classes in the dataset. This can help to prevent bias and improve the generalizability of the model.

However, without access to the full paper or additional information, it is not possible to provide a definitive answer to this query. Other techniques that could have been used to ensure the generalizability of the deep learning model include data augmentation, regularization, and early stopping.
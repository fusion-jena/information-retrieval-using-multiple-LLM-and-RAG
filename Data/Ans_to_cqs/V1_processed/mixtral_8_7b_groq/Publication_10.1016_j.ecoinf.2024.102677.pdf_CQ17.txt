The provided context discusses the use of deep learning techniques for landscape classification, including the division of the dataset into training, validation, and testing sets, as well as the fine-tuning and evaluation of several state-of-the-art CNN architectures. However, it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline.

However, based on common practices in deep learning, it can be inferred that the following regularization methods were likely employed:

1. Dropout: Dropout is a regularization technique that randomly sets a fraction of the input units to zero during training, which helps to prevent overfitting by reducing the co-adaptation of feature detectors (Srivastava et al., 2014).
2. L2 regularization: L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the model weights. This encourages the model to learn smaller weights, which can help to prevent overfitting by reducing the complexity of the model (Goodfellow et al., 2016).

These regularization methods are commonly used in deep learning to prevent overfitting, especially when training on small or medium-sized datasets. However, the context does not provide enough information to confirm whether these methods were specifically used in this case.


Regularization methods such as dropout and L2 regularization are commonly used in deep learning to prevent overfitting. While the provided context does not explicitly mention these methods, it is likely that they were employed in the deep learning pipeline for landscape classification.
The provided context does not contain information on strategies employed to handle randomness in the deep learning pipeline. However, I can provide some general information on this topic.

In deep learning, randomness can be introduced in various stages of the pipeline, such as data shuffling, weight initialization, dropout, and batch sampling. These sources of randomness can help improve the model's performance and prevent overfitting.

To handle randomness, deep learning practitioners can employ several strategies, such as:

1. **Fixed Seed Value**: By setting a fixed seed value, the random number generator will produce the same sequence of random numbers, ensuring reproducibility of the results. This is particularly important for comparing different models or hyperparameter configurations.
2. **Cross-Validation**: Cross-validation is a technique used to assess the performance of a machine learning model by partitioning the dataset into multiple subsets and training the model on different combinations of these subsets. This can help reduce the impact of randomness by averaging the performance over multiple runs.
3. **Ensemble Methods**: Ensemble methods, such as bagging and boosting, involve training multiple models on different subsets of the data and combining their predictions. This can help reduce the impact of randomness by averaging the predictions of multiple models.
4. **Statistical Tests**: Statistical tests, such as the Chi-square test, can be used to assess the significance of the results obtained from a deep learning model. These tests can help determine whether the observed differences in performance are due to randomness or actual differences in the data.

In summary, while the provided context does not contain information on strategies employed to handle randomness in the deep learning pipeline, there are several general strategies that can be used, such as setting a fixed seed value, cross-validation, ensemble methods, and statistical tests.
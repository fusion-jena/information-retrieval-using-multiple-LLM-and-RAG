Based on the provided context, several measures were taken to ensure the generalizability of the deep learning model.

Firstly, the size of the dataset used for custom training is sufficiently large. Having a large dataset can help the model to learn a wide variety of patterns and features, which can improve its ability to generalize to new, unseen data.

Secondly, the DL model used in this work is tuned with optimized hyperparameters. Hyperparameter tuning involves adjusting the parameters of the model to improve its performance. By using optimized hyperparameters, the model is more likely to perform well on new data because it has been trained with values that have been shown to work well for the task at hand.

Thirdly, the model was trained with different epoch values (100, 150, 200, and 250) to explore the effect of epoch on training results. Training the model with different epoch values can help to ensure that the model is not overfitting or underfitting the training data. Overfitting occurs when the model learns the training data too well, to the point where it performs poorly on new data. Underfitting occurs when the model does not learn the training data well enough, resulting in poor performance on both the training and new data. By training the model with different epoch values, the researchers were able to find the optimal number of epochs that resulted in the best performance on both the training and new data.

Fourthly, the model was trained using a technique called graded MSE convergence. This technique involves gradually reducing the mean squared error (MSE) during training, which can help to prevent overfitting. By using graded MSE convergence, the model is more likely to generalize well to new data because it has been trained to minimize the MSE gradually.

Lastly, the context does not provide information on whether diverse dataset, cross-validation, or stratified splitting were used. However, the aforementioned measures suggest that the researchers took several steps to ensure the generalizability of the model.
The provided context does not give specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of a calibration technique for deep learning models that involves the use of a single parameter for all classes to convert the vector output from the neural network into a real probability. This technique could potentially help in reducing overfitting, but the context does not explicitly state this.

Regularization methods such as L1 and L2 regularization, dropout, and early stopping are commonly used to prevent overfitting in deep learning models. L1 and L2 regularization add a penalty term to the loss function, which discourages the model from learning overly complex patterns in the training data. Dropout is a technique that randomly sets a fraction of the neurons in a layer to zero during training, which helps to prevent over-reliance on any single neuron and therefore reduces overfitting. Early stopping is a method that involves monitoring the performance of the model on a validation set during training, and stopping the training process once the performance starts to degrade.

The context does mention the use of Tensorflow, a system for large-scale machine learning, which has built-in support for various regularization methods. The paper "Deep residual learning for image recognition" (He et al., 2016) also mentioned in the context, proposes a deep residual learning architecture that can help to mitigate the problem of vanishing gradients, which can also help to prevent overfitting.

Another method that can be used to prevent overfitting is data augmentation, which involves creating new training examples by applying random transformations to the existing data. This can help to increase the size of the training set and reduce overfitting.

In summary, while the provided context does not give specific information about the regularization methods used to prevent overfitting in the deep learning pipeline, it is mentioned that Tensorflow is used, which has built-in support for various regularization methods. Additionally, the context also mentions the use of a calibration technique for deep learning models, and other methods such as L1 and L2 regularization, dropout, early stopping, data augmentation and deep residual learning architecture can be used to prevent overfitting.
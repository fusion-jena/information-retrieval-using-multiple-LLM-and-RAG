The provided article discusses several strategies employed to handle randomness in the deep learning pipeline, specifically in the context of image classification for hydrothermal vent substratum characterization.

Firstly, the authors trained each network with a specific architecture and substratum characterization three times, using different random initializations of the trainable parameters and different data folds. This approach helps to account for variability in model performance due to the randomness in weight initialization and data partitioning. The results presented are averages of these three executions, providing a more robust estimate of the model's performance.

Secondly, data augmentation was applied to all extracted patches, which included 90Â° rotation and vertical and horizontal flips. Data augmentation increases the size of the training set and introduces randomness, helping the model generalize better and reducing overfitting.

Lastly, the authors evaluated the performance of a classification decision committee (CDC), an ensemble of networks in which individual predictions were fused through a majority voting scheme. Ensemble methods like CDCs help reduce the impact of randomness in individual models by combining their predictions, leading to more stable and accurate results.

While the text does not explicitly mention the use of a random seed value for reproducibility, the described strategies still provide a comprehensive approach to handling randomness in the deep learning pipeline.
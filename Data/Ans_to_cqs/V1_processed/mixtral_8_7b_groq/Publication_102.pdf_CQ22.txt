The performance of the deep learning model is evaluated using Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) as the primary metrics. These metrics are used to measure the difference between the actual and predicted counts of objects in an image.

The Root Mean Square Error (RMSE) is a measure of the differences between values predicted by a model and the values actually observed. It is calculated as the square root of the average of the squared differences between the predicted and actual values. A lower RMSE indicates a better fit of the model to the data.

The Mean Absolute Error (MAE) is the average of the absolute differences between the predicted and actual values. It is a linear score, which means that all individual differences have equal weight. A lower MAE indicates better performance.

In the provided context, Model-2, Model-K, and Count-ception are compared based on RMSE and MAE metrics for SLL and elephants’ data sets. Model-2, which uses an EfficientNet-B5 feature extractor, reached an RMSE value of 1.88 and 0.60 for the SLL and elephants’ data sets, respectively, performing better than Model-K with an RMSE of 2.17 and 0.81 for SSL and elephants’ data sets, respectively. Count-ception reached a higher RMSE value of 5.57 and 1.59 for SLL and elephants’ data sets, respectively, indicating a lower performance compared to Model-2 and Model-K.

Therefore, based on the provided context, the performance of the deep learning model is evaluated using RMSE and MAE metrics, with a lower value indicating better performance.
In the study, a specific strategy was implemented to monitor the model performance during the training phase. This strategy involved using a portion of the training data for hyperparameter tuning in lieu of automating this process due to processing limitations in GEE. By employing validation data, which constituted 10% of the training data, the researchers were able to tune the key hyperparameters for each model and avoid overfitting.

To elaborate, four machine learning techniques were utilized for training and validation: CART (Classification and Regression Trees), RF (Random Forest), GTB (Gradient Boosting), and SVM (Support Vector Machine). Each of these models had specific hyperparameters set during the training and validation process, as presented in Table 3. For instance, the RF model had parameters such as the number of trees (264), bag fraction (0.5), variables per split (square root of the number of variables), and others.

Moreover, the researchers used a pixel-based classification approach that relies on the spectral signature of each pixel in the image. This method is particularly useful for land cover classification, where each pixel is assigned a class label based on its spectral characteristics.

To evaluate the classification accuracy, the researchers employed several metrics, including precision, recall, and F1 score. Precision measures the proportion of positive predictions correctly classified as positive, while recall indicates the proportion of truly positive cases that the model correctly identified among all the actual positive cases. The F1 score, a geometric mean of both precision and recall, provides a more robust evaluation, mainly when there is an imbalance in the data distribution among classes.

In summary, the strategy implemented to monitor the model performance during training involved using a validation dataset to tune hyperparameters for each model and evaluating the classification accuracy using precision, recall, and F1 score metrics. This approach allowed the researchers to optimize the model's performance and avoid overfitting due to processing limitations in GEE.
The provided context discusses various aspects of species distribution modeling, including different modeling approaches and engine capabilities, but it does not explicitly mention the hyperparameters used in a deep learning model. However, it is mentioned that the models are usually the most computationally intensive, which could imply the use of deep learning models as they are known for their computational requirements.

Hyperparameters are parameters whose values are set before the learning process begins. Common hyperparameters in deep learning models include learning rate, number of layers, number of units in each layer, type of optimizer, and regularization parameters.

The learning rate controls the size of the steps that the model takes while moving toward a minimum of the loss function. A high learning rate may cause the model to converge too quickly to a suboptimal solution, while a low learning rate may cause the model to get stuck in a local minimum.

The optimizer is the algorithm used to update the model's weights and biases during training. Common optimizers include stochastic gradient descent (SGD), Adam, and RMSprop.

The number of layers and the number of units in each layer determine the complexity of the model. A larger number of layers and units can lead to a more expressive model, but it can also lead to overfitting if not properly regularized.

Regularization parameters, such as L1 and L2 regularization, help prevent overfitting by adding a penalty term to the loss function.

Without more specific information about the deep learning model being used, it is not possible to provide a complete list of the hyperparameters. However, the learning rate, optimizer, number of layers and units, and regularization parameters are common hyperparameters that are often tuned in deep learning models.
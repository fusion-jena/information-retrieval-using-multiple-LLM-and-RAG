The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that various convolutional layer configurations were trained using input point clouds with different kernel configurations. This process could implicitly include regularization techniques to prevent overfitting.

In general, deep learning models can be prone to overfitting, especially when dealing with high-dimensional data. Regularization methods are commonly used to mitigate this issue. Some of the most popular regularization techniques include dropout and L2 regularization.

Dropout is a regularization technique that randomly drops out, or deactivates, a proportion of neurons in a layer during training. This helps to prevent over-reliance on any single neuron, encouraging the model to learn more robust features.

L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the model's weights. This encourages the model to keep its weights small, reducing the complexity of the model and helping to prevent overfitting.

Without explicit information from the provided context, it is not possible to confirm whether these or other regularization techniques were used in this deep learning pipeline.
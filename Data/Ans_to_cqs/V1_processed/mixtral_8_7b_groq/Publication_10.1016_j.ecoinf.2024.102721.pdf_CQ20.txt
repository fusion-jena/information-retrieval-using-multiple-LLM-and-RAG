The context provided does not give specific information about the hardware resources used for training the deep learning model. It mainly focuses on the methodology, structure, and formulas used in the study, particularly the Long Short-Term Memory (LSTM) model and the SHAP value for feature contribution.

However, it is common practice in deep learning to use Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs) for training models due to their ability to perform parallel computations, which significantly speeds up the training process compared to using Central Processing Units (CPUs) alone. Therefore, it is reasonable to assume that the researchers might have used GPUs or TPUs for training their deep learning model. But without explicit information, this is only a probable assumption and not a certain fact.
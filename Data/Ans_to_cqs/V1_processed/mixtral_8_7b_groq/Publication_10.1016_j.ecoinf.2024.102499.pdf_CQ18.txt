The strategy implemented to monitor the model performance during training involved the use of a technique called k-fold cross-validation. This method involves dividing the available data into 'k' subsets or folds. For each iteration or fold, a different subset of the data is used for testing, while the remaining 'k-1' subsets are used for training the model. This process is repeated 'k' times, with a different subset used for testing in each iteration, ensuring that every data point is used for testing exactly once.

In this specific study, the Trainingclips were used to train the models, and the Testingclips were used to assess the robustness of the method on short sequences centered around the presence of fish. The Testingstations, which were common to all k-fold, were used to assess the robustness of the method on 1-h videos, corresponding to the real use-case scenarios of ecological studies.

Additionally, the F-measure, Precision, and Recall were used to evaluate the performance of the models. The F-measure, which is the harmonic mean of Precision and Recall, was used as the primary metric for model evaluation. The F-measure ranges from 0 to 1, with a higher value indicating better performance.

The study found that the models trained on 27 species displayed a slightly higher F-measure standard deviation, ranging from 0.08 to 0.11 between models. However, considering the superior performance of these models, they were selected for subsequent analyses.

Overall, the k-fold cross-validation technique, combined with the F-measure, Precision, and Recall metrics, provided a robust strategy for monitoring the model performance during training and selecting the best-performing models for further analysis.
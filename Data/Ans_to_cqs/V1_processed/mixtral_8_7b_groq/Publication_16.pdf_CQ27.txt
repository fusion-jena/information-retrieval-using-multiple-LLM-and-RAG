The text provided does not give a detailed explanation of the process followed to deploy the trained deep learning model. However, it does mention that the model's performance is evaluated in a test environment, which suggests that the model was deployed for testing.

In terms of model serialization, the text does not explicitly mention if the model was saved and loaded using any specific format or library. However, it is stated that Keras and TensorFlow 2.2 are utilized as the backend, which implies that the model could have been saved using one of the serialization formats supported by these libraries, such as HDF5 or SavedModel.

As for platform selection, the text does not provide information on where or how the model was deployed. It is mentioned that the model is trained using an Nvidia 2070 super GPU with 8GB of memory, but it is not clear if this GPU was used for deployment as well.

The text does mention that deploying trained models on edge devices for real-time inference will take some consideration, which suggests that the authors may have plans to deploy the model on edge devices in the future. However, they also mention that the cost of compute needed to process the data is a major challenge, which implies that they may be considering cloud-based solutions for deployment as well.

In summary, while the text provides some insights into the deployment of the trained deep learning model, it does not give a detailed explanation of the process followed. It can be inferred that the model was deployed for testing, and that it could have been saved and loaded using a format supported by Keras or TensorFlow. The platform for deployment is not specified, but the authors mention the possibility of deploying the model on edge devices in the future.
The hyperparameters of the models are optimized using grid search, as described in the provided context. This method involves systematically exploring a range of possible values for each hyperparameter, and training the model with each combination of these values. The combination that results in the best performance, as measured by cross-validation error, is then selected as the optimal set of hyperparameters.

For the SVM model, both the regularization parameter (C) and the kernel type (linear) are calibrated using grid search and cross-validation. The regularization parameter controls the balance between model complexity and its ability to generalize to new data, while the kernel type determines the shape of the decision boundary. In instances involving non-linear SVMs, the kernel coefficient (gamma) is also optimized using grid search to ensure the model is well-attuned to the data distribution.

For the PCA-SVM model, dimensionality reduction is performed using Principal Component Analysis (PCA) before training the SVM model. The number of components preserved during this reduction process is determined using grid search, with the goal of identifying the optimal level of dimensionality reduction that results in the best model performance.

It's important to note that while grid search is a powerful and widely-used method for hyperparameter optimization, it can be computationally expensive, especially for models with many hyperparameters. Alternative methods such as random search or Bayesian optimization may be used in some cases to reduce the computational cost of hyperparameter optimization.
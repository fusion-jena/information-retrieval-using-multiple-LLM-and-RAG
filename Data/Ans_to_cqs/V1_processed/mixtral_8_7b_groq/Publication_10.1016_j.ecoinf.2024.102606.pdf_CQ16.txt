Based on the provided context, the training of the models was completed after 500 epochs. However, it is not specified if a validation loss plateau was used as a criterion to determine when training was complete. 

However, it is mentioned that due to the low amount of data, cross-validation was used to ensure the statistical results and prevent overfitting. Cross-validation involves training multiple models with different versions of the dataset and evaluating their performance. This allows for a better understanding of the model's capability on the dataset. 

Additionally, data augmentation was used during training to artificially increase the size of the dataset and reduce the risk of overfitting. Data augmentation creates new samples from existing ones, exposing the model to more variations and helping it to generalize better. 

It can be inferred that the combination of these techniques, cross-validation and data augmentation, were used as criteria to determine when training was complete and to ensure that the models were not overfitting to the data. However, it is not explicitly stated that a validation loss plateau was used as a criterion.
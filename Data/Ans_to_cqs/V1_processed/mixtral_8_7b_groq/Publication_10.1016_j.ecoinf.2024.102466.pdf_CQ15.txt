The provided context does not give specific information about the optimization techniques used in the deep learning pipeline for bird detection in camera trap images. It mainly focuses on comparing the performance of different object detection models for this task.

However, it is important to note that optimization techniques like Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), and others are fundamental components of deep learning pipelines. These techniques are used to minimize the loss function and update the model's parameters during training.

In the context of this study, the authors might have used one or more optimization techniques while training the various object detection models. However, without explicit information, it is not possible to provide a definitive answer.

An optimization technique often used in deep learning is Adam, which combines the advantages of two other optimization methods: AdaGrad (adaptive gradient algorithm) and RMSProp (root mean square propagation). Adam uses the moving averages of both the gradient and the squared gradient to update the parameters, making it suitable for a wide range of non-convex optimization problems.

Stochastic Gradient Descent (SGD) is another commonly used optimization technique. It updates the parameters by computing the gradient on a single training example or a small batch of training examples at each iteration. SGD is simple and efficient but can be sensitive to the learning rate.

In summary, while the provided context does not mention the specific optimization techniques used in the deep learning pipeline for bird detection, it is highly likely that optimization techniques like Adam or SGD were employed.
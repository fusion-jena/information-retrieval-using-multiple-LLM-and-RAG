The text does not explicitly mention the use of a random seed value to handle randomness in the deep learning pipeline. However, it does describe several strategies that involve a certain level of randomness.

 Firstly, the data is split into a training set, a validation set, and a test set. While the text suggests that samples in each set should be independent and have similar class distributions, it does not specify how this splitting should be done. In practice, this is often achieved using a random process to ensure that each subset is representative of the overall data.

 Secondly, the text describes a three-step data augmentation procedure that is applied to the images during training. This procedure involves randomly flipping images horizontally, scaling the contrast of each image channel by a random factor, and randomly cropping the image to a square. These random transformations increase the diversity of the training data and help to reduce overfitting.

 Thirdly, the networks are trained using the Adam optimizer, which uses a adaptive learning rate that is calculated based on the historical gradient. The initial learning rate is set to 0.001, but it is possible that this value is adjusted during training based on the performance of the network.

 Finally, the text mentions that the training set is balanced by upsampling the images from the underrepresented classes through repetition to match the number of images of the most represented class. This process introduces some level of randomness, as the images that are selected for repetition are chosen randomly.

 In summary, while the text does not explicitly mention the use of a random seed value to handle randomness in the deep learning pipeline, it does describe several strategies that involve a certain level of randomness, including random data splitting, data augmentation, optimizer selection, and class balancing.
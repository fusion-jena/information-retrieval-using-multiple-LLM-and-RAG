Based on the provided context, there is no explicit information about the specific criteria used to determine when training is complete, such as a validation loss plateau. However, it is mentioned that they followed standard processes of optimization for each software, which may include monitoring the performance on a validation set and stopping the training when the performance stops improving or reaches a certain level.

Additionally, it is mentioned that they created 12 recognizers for each song, with 6 different training sets, and for each of those, they chose the optimal cut-off threshold and ran a random forest model using the best replicate and threshold on the test set. This process suggests that they may have used a combination of threshold optimization and model performance on a test set to determine when training is complete.

Furthermore, it is mentioned that they created 36 different recognizers, 12 per software, and implemented the same recognizer and threshold to the target set to obtain biologically relevant data. This process of testing the model on a target set after training, could also be a way of determining if the training is complete or not.

In summary, while the context does not provide explicit information on the criteria used to determine when training is complete, it suggests that they may have used a combination of threshold optimization, model performance on a test set, and testing on a target set as part of their training and evaluation process.
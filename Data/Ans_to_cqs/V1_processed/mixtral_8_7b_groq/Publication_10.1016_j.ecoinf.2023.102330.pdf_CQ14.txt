Based on the provided context, there is no explicit information about how the hyperparameters of the model are optimized. The context discusses various methods used for estimating wildlife population sizes, including distance sampling (DS) and spatially explicit capture-recapture (SECR) models, but it does not mention any specific techniques for hyperparameter optimization such as grid search or random search.

In general, hyperparameter optimization is an important step in the development of any machine learning or statistical model. Hyperparameters are parameters whose values are set prior to the commencement of the model training process. Examples of hyperparameters include the learning rate in a neural network, the complexity of a decision tree, or the regularization parameter in a regression model.

Hyperparameter optimization involves selecting the values of these parameters that result in the best model performance. This can be done using various techniques, including grid search, random search, and Bayesian optimization. Grid search involves systematically trying out different combinations of hyperparameter values on a predefined grid. Random search, on the other hand, involves randomly selecting hyperparameter values within predefined ranges. Bayesian optimization uses Bayesian inference to iteratively select hyperparameter values that are likely to result in good model performance.

Without more information, it is difficult to say how the hyperparameters of the model in the provided context are optimized. However, it is possible that the authors used one of the aforementioned techniques or developed a custom approach for hyperparameter optimization. It is recommended to refer to the original sources for more detailed information on the model development process.
The type of deep learning model used in the pipeline is a Transformer model, specifically BERT (Bidirectional Encoder Representations from Transformers). The pipeline uses a hierarchical classification approach, where data is passed through a series of classifiers with increasing specificity. However, the authors note that due to the computational resources required to train Transformer models and the practical benefits of using them for text classification being limited for simpler tasks, they chose to use a fine-tuned Transformer model.

The Transformer model is characterized by a self-attention mechanism, which can weight the influence of different parts of a text sequence to capture dependencies between words in the text. BERT, the specific Transformer model used in the pipeline, generates representations of words in a text sequence that consider both the right and left contexts around words. It is trained using a large Wikipedia text corpus, via both masked-language modeling and next-sentence prediction.

Since BERT can only accept a maximum of 512 tokens as input, smaller subsections of the longer web articles are used as classifier input. Documents are split into chunks of 300 tokens using the NLTK word-tokenizer, and chunks that contain any of the original query terms are retained. If more than one chunk contains keywords, the first such chunk of the document is retained for use in model training and evaluation. Before being fed into the BERT model, texts are tokenized using the BERT tokenizer. The maximum number of tokens is set to 400 for social media posts and 512 for web articles.
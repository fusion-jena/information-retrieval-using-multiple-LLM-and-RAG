Based on the provided context, the measures taken to ensure the generalizability of the deep learning model, specifically a Siamese Neural Network (SNN), are primarily centered around reducing overfitting.

Firstly, the optimization algorithm used for training the SNN is Stochastic Gradient Descent (SGD), with a learning rate of 0.0001 and a weight decay of 0.5. This learning rate is quite small, indicating a gradual adjustment of the model's weights, which can help in preventing overfitting. The weight decay, also known as L2 regularization, is another technique used to prevent overfitting by adding a penalty term to the loss function, discouraging large weights.

Secondly, the batch size is set to 32. This is a common choice as it provides a balance between the computational efficiency of larger batch sizes and the ability of smaller batch sizes to better capture the randomness in the data, which can help in generalization.

Thirdly, the number of training epochs is set to 200. While this is a relatively large number, the context mentions the use of batch-norm, dropout, and early stopping as additional measures to prevent overfitting. Batch-norm normalizes the activations of the neurons in a layer, which can help in improving the generalization ability of the model. Dropout randomly drops out a fraction of the neurons during training, which can be seen as training multiple sub-networks within the full network, helping to prevent overfitting. Early stopping is a form of regularization where training is stopped as soon as the validation loss starts to increase, preventing the model from learning the noise in the training data.

However, the context does not explicitly mention the use of diverse datasets, cross-validation, or stratified splitting. These techniques are commonly used in machine learning to improve model generalization, but based on the provided context, they do not appear to have been used in this specific case. 

In conclusion, the measures taken to ensure the generalizability of the SNN include the use of a small learning rate, weight decay, a balanced batch size, a large but regulated number of training epochs, and techniques to prevent overfitting such as batch-norm, dropout, and early stopping.
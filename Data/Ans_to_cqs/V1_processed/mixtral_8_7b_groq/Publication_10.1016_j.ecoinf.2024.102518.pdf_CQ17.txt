The provided context does not directly discuss the regularization methods used in deep learning pipelines, such as dropout or L2 regularization. Instead, it focuses on a study concerning Asian elephant habitat suitability using various algorithms, including MaxEnt, and evaluating their performance using metrics like AUC-ROC and Cohen's KAPPA.

In the study, the researchers used the Lasso regularization method, which is an L1 regularization, with feature classes 'L', 'Q', 'LQ', 'LQP' and a regularization multiplier ranging from 0.5 to 5. Lasso regularization adds an absolute value of the magnitude of coefficient as penalty term to the loss function. This helps in reducing overfitting by forcing some of the coefficients to be zero.

However, the context does not mention dropout or L2 regularization, which are common techniques in deep learning. Dropout is a regularization technique used in neural networks where randomly selected neurons are ignored during training, preventing over-dependence on individual neurons and reducing overfitting. L2 regularization, on the other hand, adds the squared magnitude of coefficient as a penalty term to the loss function, encouraging smaller coefficients and reducing overfitting.

In summary, while the study mentions the use of Lasso regularization, a type of L1 regularization, it does not discuss dropout or L2 regularization, which are commonly used regularization techniques in deep learning pipelines.
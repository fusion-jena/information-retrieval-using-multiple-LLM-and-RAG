The provided context does not include information about the regularization methods used to prevent overfitting in deep learning pipelines, such as dropout or L2 regularization. Therefore, I cannot provide an accurate answer based on the given context. However, I can provide some general information about these regularization techniques.

Dropout is a regularization technique used in deep learning, particularly in neural networks. During training, dropout randomly sets a fraction of the input units to zero, effectively preventing the network from relying too much on specific input features and reducing overfitting. This helps the model generalize better to unseen data.

L2 regularization, also known as weight decay, is another common technique to mitigate overfitting. It adds a penalty term to the loss function, which is proportional to the square of the model's weights. By increasing the loss for larger weights, L2 regularization encourages the model to have smaller weights, making it less prone to overfitting.

These techniques are widely used in deep learning to improve model performance and generalization. However, the provided context focuses on big-data mining approaches for forecasting biodiversity changes, specifically in the Massaciuccoli Lake basin, and does not discuss regularization methods for deep learning models.
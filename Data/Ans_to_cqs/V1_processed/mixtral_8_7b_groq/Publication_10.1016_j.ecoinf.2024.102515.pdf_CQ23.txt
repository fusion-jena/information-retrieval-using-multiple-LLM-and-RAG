Based on the provided context, there is no explicit information about the specific measures taken to ensure the generalizability of the deep learning model. However, there are some implicit details that suggest certain strategies were employed.

Firstly, the DIVERSIMAR project involved understanding user expressions on a social network, which implies the use of a diverse dataset. The project searched for tweets using three different criteria, indicating an attempt to capture a broad range of user expressions. This diverse dataset could have helped in creating a more generalized model.

Secondly, the project used the 'Full-archive search endpoint' to access the entire dataset published on Twitter from its beginning in 2006-2007 until the end of 2022. This suggests a large dataset was used, which could have been split into training, validation, and test sets using strategies like cross-validation or stratified splitting.

Lastly, the twarc2 library was used to handle the data in JSON format returned by the Twitter API. This library simplifies the handling of the quota limits imposed by the Twitter API and the pagination resulting from a single query. This could imply that the data was split into smaller chunks to manage the Twitter API limitations, which is a common practice in machine learning when dealing with large datasets.

However, without explicit information, it's challenging to provide a definitive answer. It's possible that other measures not mentioned in the context were also taken to ensure the generalizability of the deep learning model.
Based on the provided context, there is no explicit information about the specific hardware resources used for training the deep learning model. The text focuses more on the description of the waveform token hidden Markov model (WT-HMM) and its performance evaluation in comparison to other detectors such as artificial neural networks (ANN), support vector machines (SVM), and spectrogram cross-correlation detectors (SCD).

However, it can be inferred that the training process might have been carried out using a machine with a powerful central processing unit (CPU) or a graphics processing unit (GPU) to handle the computations efficiently. In recent years, GPUs have become increasingly popular for machine learning tasks due to their ability to perform parallel computations, which significantly speeds up the training process.

Additionally, some deep learning frameworks like TensorFlow support the use of tensor processing units (TPUs), which are custom-built hardware accelerators designed specifically for machine learning tasks. However, without explicit information in the context, it is not possible to confirm whether TPUs were used in this particular case.

In summary, while the provided context does not give specific details about the hardware resources used for training the deep learning model, it is plausible that a CPU or GPU was used, considering the widespread use of these hardware components for machine learning tasks.
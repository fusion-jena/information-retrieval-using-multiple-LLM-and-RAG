Based on the provided context, there is no direct information about the criteria used to split data for deep learning model training. However, the text does discuss data splitting in the context of random forests, which is a different type of machine learning model. In random forests, the data is typically split into training and testing sets using a method called bagging, or bootstrap aggregating. This involves creating multiple subsets of the original data by randomly sampling from it, with replacement. Each of these subsets is used to train a separate decision tree, and the final prediction of the random forest model is made by averaging the predictions of all the individual trees.

In contrast, deep learning models typically use a different approach to data splitting, involving a training set, a validation set, and a test set. The training set is used to train the model, the validation set is used to tune the model's hyperparameters and prevent overfitting, and the test set is used to evaluate the model's performance on unseen data.

To summarize, while the provided context does not directly answer the query about data splitting for deep learning model training, it is mentioned that in random forests, data splitting is done using bagging. Deep learning models, on the other hand, typically use a different approach involving a training set, a validation set, and a test set.
The training of an Artificial Neural Network (ANN) model, such as the Backpropagation ANN (BP-ANN) used in the study, is typically considered complete when the validation loss plateaus. This means that the reduction in the loss function, which measures the difference between the anticipated output and the actual target values, becomes minimal or insignificant even with further training.

In the context of the study, the authors use the loss function to evaluate the performance of their ANN model. The loss function is a critical component of the ANN model as it helps in adjusting the weights and biases of the networkâ€™s neurons to minimize the gap between the anticipated output and the actual target values. The authors use the loss function to compare the performance of their ANN model with other regression methods, such as linear, power, and exponential regression.

Moreover, the authors mention that the accuracy of the ANN model's training process depends on the quality and quantity of the sample dataset. Therefore, it is essential to monitor the validation loss during the training process to ensure that the model is not overfitting or underfitting the data. Overfitting occurs when the model learns the training data too well, including the noise, and performs poorly on new, unseen data. Underfitting, on the other hand, occurs when the model fails to learn the underlying patterns in the data, resulting in poor performance on both the training and validation data.

To prevent overfitting or underfitting, the authors use a process of trial and error to determine the optimal numbers of hidden layers and neurons in their ANN model. They consider the 'neuralnet' package of R 4.1.2 software, which is mostly applied for training multi-layer perceptrons in regression analyses, to develop a BP-ANN model.

In summary, the training of an ANN model, such as the BP-ANN used in the study, is typically considered complete when the validation loss plateaus. Monitoring the validation loss during the training process helps in preventing overfitting or underfitting, ensuring that the model learns the underlying patterns in the data and generalizes well to new, unseen data.
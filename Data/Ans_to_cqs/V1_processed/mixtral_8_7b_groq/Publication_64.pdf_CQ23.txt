The text provided gives some information about the steps taken to ensure the generalizability of the deep learning model, although it does not explicitly mention techniques such as cross-validation or stratified splitting.

Firstly, the dataset used to train the model is a subset of the Google Search Image dataset, containing 3680 labels with more than 12 images each. This was done to minimize the presence of mislabeled images by selecting only the most popular labels and retrieving the top results. The authors expect that this will help the model to generalize better to new, unseen data.

Secondly, the authors use pre-trained weights from the ImageNet dataset to initialize the model's weights. This is a common technique in deep learning, as it provides a good starting point for the model and can help it to learn more general features that are useful for a wide range of tasks.

Thirdly, the authors use data augmentation during training to increase the diversity of the training data. Specifically, they apply transformations such as shearing, translation, and mirroring to the images, which helps the model to learn features that are robust to these types of transformations. This can help to improve the model's generalizability, as it will be able to handle new images that may be slightly different from those seen during training.

Finally, the authors evaluate the model on three different datasets: Google Image Search, Portuguese Flora, and iNaturalist. This provides a good test of the model's generalizability, as it shows how well it can perform on a range of different datasets. The results presented in the text suggest that the ResNet50 model achieves consistent improvements in top1 and top5 accuracy across all three datasets, indicating that it is a generalizable model.

Overall, while the text does not explicitly mention techniques such as cross-validation or stratified splitting, it does provide some evidence that the authors have taken steps to ensure the generalizability of their deep learning model. These steps include using a diverse dataset, pre-trained weights, data augmentation, and evaluating the model on multiple datasets.
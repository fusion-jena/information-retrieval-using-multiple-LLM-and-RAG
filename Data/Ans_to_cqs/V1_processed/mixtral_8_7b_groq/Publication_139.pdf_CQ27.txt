After training and validating the deep learning model using a neural network similar to those implemented by Ruff et al., 2020; Sprengel et al., 2016, the model was deployed using a multi-step process. 

First, the model was trained on a database assembled from spectrogram images. This database consisted of over 40 k images of bio-, anthro- and geophonies, manually labeled by two trained graduate students. To assemble this database, only 10-15% of the spectrograms from each of the selected days were annotated, reducing the time required. 

Next, the trained model was used to generate prediction records for the entire unvalidated recording period. Specifically, a single prediction for every 8 seconds was made for the duration of the year. The five prediction records that resulted were then averaged together to create a single prediction record. To summarize this data more succinctly, these 8 seconds-interval predictions were aggregated into hourly prediction records, containing the number of instances that each particular sound stimuli was predicted per hour. This record was normalized by the number of 8-seconds intervals actually recorded during that hour, to account for missing audio data.

However, the provided context does not mention any details about model serialization or platform selection. Model serialization is the process of saving the trained model to disk so that it can be loaded and used later. Platform selection refers to choosing the hardware or software platform where the model will be deployed. Therefore, I cannot provide specific information about these steps.
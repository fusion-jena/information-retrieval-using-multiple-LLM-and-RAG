The provided context discusses the creation and validation of fire risk maps using Landsat 8 satellite imagery and vegetation indices. However, it does not explicitly mention the specific metrics used to evaluate the performance of a deep learning model. Nonetheless, it is common in the field of machine learning, including deep learning, to use several evaluation metrics to assess a model's performance.

Some of the standard evaluation metrics for a deep learning model include:

1. Accuracy: This is the ratio of the number of correct predictions to the total number of input samples. It gives an overall idea of how well the model is performing.

2. Precision: Precision is the ratio of true positive predictions (relevant samples that are correctly identified) to the total predicted positives (the sum of true positives and false positives). High precision indicates that the model has a low false positive rate.

3. Recall (Sensitivity): Recall is the ratio of true positive predictions to the total actual positives (the sum of true positives and false negatives). High recall indicates that the model has a low false negative rate.

4. F1-score: F1-score is the harmonic mean of precision and recall and provides a balance between the two metrics. It is a more reliable metric than accuracy, especially for imbalanced datasets.

5. Area Under the Curve (AUC-ROC): AUC-ROC is the area under the Receiver Operating Characteristic (ROC) curve, which plots the true positive rate against the false positive rate. A higher AUC-ROC indicates a better performing model.

Based on the context, it can be inferred that these evaluation metrics (or a subset of them) could be used to assess the performance of a deep learning model for fire risk mapping using Landsat 8 satellite imagery and vegetation indices.
Based on the given context, there is no explicit information about how the hyperparameters of the model were optimized. The sources mention the use of the XGBoost method for land use/land cover classification and forest type accuracy assessment, but they do not provide details about the optimization process for the model's hyperparameters.

In general, there are various methods to optimize hyperparameters in machine learning models, such as grid search, random search, Bayesian optimization, and more. Grid search and random search are two common methods:

1. Grid search: This method involves creating a grid of hyperparameter values and training the model with all possible combinations. The combination with the best performance is then selected as the optimal set of hyperparameters.
2. Random search: Similar to grid search, random search also tries different combinations of hyperparameters. However, instead of testing all possible combinations, it selects a subset randomly, which can be more efficient, especially when dealing with a large number of hyperparameters and values.

Without specific information from the provided context, it is impossible to determine which method was used to optimize the XGBoost model's hyperparameters.


 The provided context does not contain information on how the hyperparameters of the XGBoost model were optimized. Common methods for hyperparameter optimization include grid search, random search, and Bayesian optimization. However, the sources do not offer details about the optimization process for the XGBoost model's hyperparameters.
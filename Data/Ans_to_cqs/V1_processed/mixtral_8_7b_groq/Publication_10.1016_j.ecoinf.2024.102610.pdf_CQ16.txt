The provided context does not give specific criteria for determining when training of a machine learning or deep learning model is complete. However, it does mention some techniques used during the training process.

In the context, it is suggested that future research efforts should adopt spatial cross-validation techniques to bolster model robustness. Spatial cross-validation involves randomly selecting blocks of data for training while reserving others for testing, and this process is repeated multiple times based on the number of specified folds. This approach helps refine the estimation of prediction errors and enhances overall model performance.

In addition, the context discusses the use of feature elimination to improve model performance. Feature elimination involves selecting the most important predictor variables and eliminating the rest. This process can help reduce overfitting and improve the interpretability of the model.

The context also mentions the use of validation loss plateau as a stopping criterion in machine learning and deep learning models. Validation loss plateau refers to the point at which the validation loss of the model stops decreasing or decreases very slowly, indicating that the model has reached a state of convergence. At this point, training can be considered complete.

Therefore, while the provided context does not give specific criteria for determining when training is complete, it does suggest the use of spatial cross-validation and feature elimination during the training process. Additionally, it implies that validation loss plateau can be used as a stopping criterion for training.
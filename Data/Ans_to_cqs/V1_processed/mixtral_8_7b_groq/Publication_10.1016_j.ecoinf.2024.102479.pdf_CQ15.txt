The text provided does not give specific information about the optimization techniques used in the deep learning pipeline for the models being discussed. It only mentions that a grid search approach involving cross-validation was used to optimize the parameters of the algorithms. However, it does not specify if this included optimization techniques such as Stochastic Gradient Descent (SGD) or Adam.

The text does mention the use of an Artificial Neural Network (ANN) algorithm, which is a type of deep learning model. But it does not provide details about the optimization techniques used for this specific algorithm.

In general, optimization techniques such as SGD and Adam are commonly used in the deep learning pipeline to minimize the errors and achieve the best possible performance.

Stochastic Gradient Descent (SGD) is an optimization algorithm used for training neural networks. It updates the parameters of the model by taking a single example from the training set at each step, and computing the gradient of the loss function for that example.

Adam is another optimization algorithm that is commonly used in deep learning. It is an extension of SGD that includes adaptive learning rates for each parameter.

Therefore, based on the general knowledge of deep learning pipeline, it can be said that optimization techniques such as SGD and Adam are likely to have been used in the deep learning pipeline for the models being discussed in the text. However, the text does not provide explicit information on this.
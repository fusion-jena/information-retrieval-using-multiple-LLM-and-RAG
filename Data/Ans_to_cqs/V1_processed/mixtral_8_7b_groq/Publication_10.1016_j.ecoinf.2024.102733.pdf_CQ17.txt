The regularization methods used to prevent overfitting in the deep learning pipeline, as described in the provided context, include dropout, weight decay (which is a form of L2 regularization), and batch normalization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent over-reliance on specific neurons and therefore overfitting (Srivastava et al., 2014). In the described context, dropout was explored with rates of 0.05, 0.1, 0.2, and 0.25.

Weight decay, also known as L2 regularization, is a method that penalizes large weights in the model, which helps to prevent overfitting by reducing the complexity of the model (Goodfellow et al., 2016). In the described context, weight decay was varied between 0, 0.00001, 0.0001, and 0.001.

Batch normalization is a technique that normalizes the inputs to each layer of the model, which can help to prevent overfitting by reducing internal covariate shift (Ioffe and Szegedy, 2015). In the described context, batch normalization was tested on each layer of the network separately and on all layers simultaneously, with the best results achieved when applied to the third layer.

In addition to these regularization methods, the use of a scheduler to adjust the learning rate when the network reaches a plateau can also help to prevent overfitting by allowing the model to continue learning even when the initial learning rate is too high (Kingma and Ba, 2015).

References:
Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.
Ioffe, S., & Szegedy, C. (2015, June). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456).
Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollar, P. (2017). Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2985).
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1), 1929-1958.
The regularization methods used to prevent overfitting in the deep learning pipeline include dropout and L2 regularization. However, the provided context does not explicitly mention the use of these techniques in the described study.

Dropout is a regularization technique that stochastically sets a fraction of the neurons in a layer to zero during training, which helps to prevent overfitting by reducing the co-adaptation of neurons (Srivastava et al., 2014). L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the norm of the model's weights. This encourages the model to learn smaller weights, which can help to prevent overfitting by reducing the complexity of the model (Goodfellow et al., 2016).

In the described study, the authors use data augmentation as a technique to reduce the requirement of a large dataset for model training (Ganguly et al., 2017; Neelakantan et al., 2015). Data augmentation involves creating new training examples by applying random transformations to the existing examples, such as rotation, scaling, and translation. This can help to prevent overfitting by increasing the size and diversity of the training set.

The authors also use techniques such as learning rate decay and momentum in the optimization algorithm to improve the performance of the model (Szegedy et al., 2016). Learning rate decay gradually reduces the learning rate during training, which can help to prevent overshooting of the optimal solution and improve convergence. Momentum accumulates the gradient history to smooth the optimization process and escape from shallow local minima.

In summary, while the provided context does not explicitly mention the use of dropout and L2 regularization, the authors use data augmentation, learning rate decay, and momentum to prevent overfitting and improve the performance of the deep learning model.
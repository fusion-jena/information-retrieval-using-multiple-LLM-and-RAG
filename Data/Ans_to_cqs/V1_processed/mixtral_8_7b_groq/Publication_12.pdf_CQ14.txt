The text does not explicitly mention the specific method used to optimize the hyperparameters of the model, such as grid search or random search. However, it does provide some information about the parameter settings used during the training of the model.

The network used for training the model is called BatNet, which has 22 convolutional layers for extracting useful acoustic features. The network also includes eight shortcut connections between layers to avoid information loss as the number of layers increases. The key parameter settings for BatNet include a batch size equal to 64 and a learning rate.

Additionally, the authors mention that they set a large batch size equal to 128 for training the models using BatNet. This means that the model was trained with 128 images for each iteration. The batch size was set to limit the number of images used during training due to the limited random access memory or graphic memory available in computers.

Furthermore, the authors added new BNorm layers behind the 22 convolutional layers to prevent overfitting during training. These layers help the model generalize from trends in both "known" and "unknown" datasets rather than maximizing performance on the "known" datasets.

Therefore, while the text does not explicitly mention the use of grid search or random search for hyperparameter optimization, it does provide information about the parameter settings used during training, such as batch size and learning rate. Additionally, the authors took measures to prevent overfitting by adding BNorm layers, which suggests some level of hyperparameter tuning. However, without further information, it is not possible to provide a definitive answer to the query.
The provided context does not give specific information about the criteria used to determine when training is complete for the machine learning models. However, it does mention that the models were trained using the L-BFGS or Adam optimization algorithms, and that the set of hyperparameter values resulting in the minimum MAE (Mean Absolute Error), RMSE (Root Mean Square Error), and the highest R2 (coefficient of determination) were selected for each model.

The L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) and Adam (Adaptive Moment Estimation) optimization algorithms are methods used to update the parameters of a model during training. These algorithms use different techniques to iteratively adjust the model's parameters to minimize the loss function, which is a measure of the difference between the model's predictions and the actual values.

In general, training of a machine learning model is considered complete when the model's performance on a validation set stops improving or when a pre-determined number of iterations or epochs has been reached. A validation set is a subset of the training data that is used to evaluate the model's performance during training. By monitoring the model's performance on the validation set, it is possible to detect if the model is starting to overfit the training data, which occurs when the model's performance on the training data is much better than on the validation data.

In the context of the provided information, it is likely that the training of the models was stopped when the model's performance on the validation set, as measured by the MAE, RMSE, and R2, stopped improving or when a pre-determined number of iterations or epochs was reached. However, the specific criteria used to determine when training is complete are not provided in the context.
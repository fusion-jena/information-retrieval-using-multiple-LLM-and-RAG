In the provided context, the regularization methods used to prevent overfitting in the deep learning pipeline are aggressive data augmentation and transfer learning.

Aggressive data augmentation is applied to avoid overfitting and imbalanced problem. This technique involves augmenting all training sets randomly via horizontal flipping. This method is effective in increasing the amount of training data, introducing more variability, and reducing overfitting.

Transfer learning is another regularization technique used in the study. The model was pre-trained on the ImageNet dataset and then fine-tuned on the PAD Full and PAD Lite datasets. This approach leverages the knowledge gained from a large-scale dataset and applies it to a smaller, task-specific dataset. By doing so, the model can learn more generalizable features, reducing the risk of overfitting.

However, the context does not explicitly mention the use of dropout or L2 regularization. Dropout is a regularization technique that randomly drops out neurons during training, preventing co-adaptation and overfitting. L2 regularization, on the other hand, adds a penalty term to the loss function, shrinking the weights towards zero and reducing overfitting. While these techniques are commonly used in deep learning pipelines, they were not mentioned in the provided context.
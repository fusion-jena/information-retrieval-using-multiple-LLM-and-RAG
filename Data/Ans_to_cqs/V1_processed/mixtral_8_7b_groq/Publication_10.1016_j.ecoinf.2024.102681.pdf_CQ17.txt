The provided context does not include specific information about the regularization methods used to prevent overfitting in the deep learning pipeline for drought prediction. However, it is common practice in deep learning to use various regularization techniques to mitigate overfitting.

Two popular regularization methods are dropout and L2 regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and less prone to overfitting. This is achieved by breaking the co-adaptation of neurons, which can lead to a more generalized model.

L2 regularization, also known as weight decay, adds a penalty term to the loss function, which is proportional to the sum of the squares of all the model's weights. This encourages smaller weights, reducing the complexity of the model and helping to prevent overfitting.

In the context of drought prediction, the researchers could have used one or both of these methods to prevent overfitting in their CNN-LSTM hybrid model. However, without explicit information, it is not possible to provide a definitive answer.
The context does not provide specific information on how the hyperparameters of the noise reduction network are optimized. However, it does mention that the network consists of 16 connected layers, and that for certain layers, an adaptive normalization operator (Γk) and a dilated convolution operator (*rk) are used. The dilation factor for the k-th layer is set at rk = 2^(k-1) for k ∈ {1, ..., 13}. It is also mentioned that a linear transformation is used in the output layer to synthesize the output signal samples.

The optimization of hyperparameters like the number of layers, the type of normalization and convolution operators, and the dilation factor, can be done through various methods such as grid search, random search, or Bayesian optimization. These methods involve systematically searching through a range of possible hyperparameter values and evaluating the performance of the model for each combination.

Grid search involves creating a grid of hyperparameter values and evaluating the model for every combination of values on the grid. Random search, on the other hand, involves randomly sampling hyperparameter values from a given distribution and evaluating the model for each sampled combination. Bayesian optimization uses a probabilistic model to guide the search for the best hyperparameter values.

Without specific information on how the hyperparameters of the noise reduction network are optimized, it is not possible to provide a definitive answer. However, it can be inferred that some form of hyperparameter optimization was performed to determine the best combination of hyperparameters for the network.
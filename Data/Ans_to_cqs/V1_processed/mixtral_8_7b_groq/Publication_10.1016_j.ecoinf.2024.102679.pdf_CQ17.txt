The provided context discusses the use of a deep learning (DL) model in a pipeline for processing aerial imagery, but it does not explicitly mention the specific regularization methods used to prevent overfitting. However, it does mention that the model selected for each training stage is the one that gave the best performance on the validation set. This suggests that the pipeline incorporates some form of model validation or cross-validation, which is a common approach to mitigate overfitting.

In general, there are several regularization techniques that can be used to prevent overfitting in DL models. Two commonly used methods are L2 regularization and dropout.

L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to keep its weights small, which can help to reduce overfitting by preventing the model from learning overly complex patterns in the training data.

Dropout is a regularization technique that randomly sets a fraction of the model's outputs to zero during training. This prevents the model from relying too heavily on any single neuron, which can help to reduce overfitting by promoting the development of redundant representations within the model.

The context also mentions that the DL model is trained using a dataset of thousands of images, which can help to reduce overfitting by providing the model with a large and diverse set of examples to learn from. Additionally, the use of a validation set to select the best model at each training stage can help to ensure that the model is able to generalize well to new data.

Therefore, while the specific regularization methods used in the DL pipeline are not mentioned in the provided context, it is likely that the pipeline incorporates some combination of model validation, L2 regularization, and dropout to prevent overfitting.
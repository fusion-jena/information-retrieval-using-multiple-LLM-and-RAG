The provided context discusses the use of a specific deep learning model, 3DUNetGSFormer, for large-scale wetland mapping, but it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, based on general practices in deep learning, it is common to use techniques such as dropout, L2 regularization, and early stopping to prevent overfitting.

Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps to prevent over-dependence on any single neuron and promotes the development of redundant representations within the network. L2 regularization, also known as weight decay, adds a penalty term to the loss function that discourages large weights, which can help to reduce overfitting by promoting smaller, more generalized weights. Early stopping is a method that involves terminating the training process before the model starts to overfit, based on a validation set's performance.

In the context of the 3DUNetGSFormer model, the authors mention the use of a conditional map unit to produce synthetic samples for classes with a low number of training samples. This technique can help to alleviate the problem of imbalanced data, which is a common issue in wetland mapping. However, it does not directly address the issue of overfitting.

Therefore, while the provided context does not explicitly mention the regularization methods used in the 3DUNetGSFormer model, it is reasonable to assume that common regularization techniques such as dropout, L2 regularization, and early stopping were used to prevent overfitting.
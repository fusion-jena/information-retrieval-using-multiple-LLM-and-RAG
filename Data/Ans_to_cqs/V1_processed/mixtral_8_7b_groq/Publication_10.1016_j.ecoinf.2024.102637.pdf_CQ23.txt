The provided context does not explicitly mention the measures taken to ensure the generalizability of the deep learning model, such as using a diverse dataset, cross-validation, or stratified splitting. However, it does discuss the AlexNet network, which was designed to be lightweight and optimized for deep training and short-term testing.

The AlexNet network was designed to improve computational efficiency and reduce the inferencing time of the model by merging two convolution layers and ensuring that the nonlinear factors caused by the activation function are not reduced. This suggests that the creators of AlexNet were aware of the importance of maintaining the model's performance while reducing computational costs.

Furthermore, the context mentions that recent research has explored the potential of shallow networks and optimized their algorithm and structural design to maintain model performance while reducing computational complexity and deployment costs. This indicates that researchers are actively seeking ways to improve the generalizability of deep learning models by finding a balance between speed and accuracy.

In summary, while the provided context does not explicitly mention the measures taken to ensure the generalizability of the deep learning model, it does suggest that researchers are aware of the importance of maintaining model performance while reducing computational costs and exploring the potential of shallow networks. However, without further information, it is not possible to provide a definitive answer to the query.
Based on the provided context, there is no explicit information about the specific postprocessing steps involved after the model training. However, it does mention that evaluation plots were created and used to assess the performance of the models. These plots could potentially include metrics calculation and confusion matrix.

In the "2.3. Model evaluation" section, it is mentioned that they ran evaluation plots for the pretraining model only (i.e. 0% hand-annotated data) and for models trained using different proportions of training data (5%, 25%, 50%, and 75% of the total hand-annotations) to compare with the full dataset for the within-site results for each site. They also reran this experiment five times to account for the random subsampling of annotations.

In the "Results" section, it is mentioned that they performed three types of experiments to assess generalization among sites, using different combinations for hand-annotations and pretraining data. They used pretraining and hand-annotated data to predict the evaluation data from the same site (within-site), pretraining data and hand-annotated from the same site to predict the evaluation data from a different site (cross-site), and pretraining data from all other sites, but hand annotations from the same site as the evaluation data (transfer-learning). They also provided a figure (Fig. 6) showing a sample evaluation plots from the Mixed Pine site predicted by a model built from training data from the same site, from each other site, and a universal model.

So, it can be inferred that the postprocessing steps involved after the model training include creating and analyzing evaluation plots, which might include metrics calculation and confusion matrix. However, the context does not provide specific information about other postprocessing steps such as saliency maps.
The strategy implemented to monitor the model performance during training involves considering two factors: the correct identification rate and the processing time. The correct identification rate measures the model's ability to accurately classify the species, while the processing time tracks the computational resources required to train the model.

For the Mexican species, the optimum number of training steps was identified as 2,000, resulting in an 83.0% overall identification rate. This number was chosen because, although higher identification rates were possible, the processing time increased significantly, from 3.5 hours to 7 hours, for a marginal improvement in accuracy. Similarly, for the Brazilian species, 12,000 training steps were identified as the optimum number, achieving an 86.7% overall identification rate. Although the overall identification rate barely reached a maximum of 87.0%, the processing time increased from 30 to 80 hours.

The authors used these two factors to balance the trade-off between the model's accuracy and computational efficiency. By monitoring the correct identification rate and processing time throughout the training process, they were able to identify the point of diminishing returns, where further training did not significantly improve the model's accuracy but did require substantially more processing time.

This strategy allowed the authors to optimize the model's performance for both the Mexican and Brazilian species, achieving high identification rates while minimizing the processing time. It is important to note that the processing time mentioned above is for training TensorFlow over all images in the reference library with multiple replicate analyses. The identification of a single image only takes a few seconds.

In summary, the strategy to monitor the model performance during training involves tracking the correct identification rate and processing time, allowing the authors to identify the optimum number of training steps that balances the trade-off between accuracy and computational efficiency.
Based on the provided context, there is no explicit information given about how the hyperparameters of the model were optimized. Hyperparameter optimization is crucial in deep learning models like Convolutional Neural Networks (CNNs) to achieve the best performance. Common methods for hyperparameter optimization include grid search, random search, and more sophisticated methods like Bayesian optimization and population-based training.

In the context provided, Table 1 lists several pre-processing hyperparameters for different datasets, such as low pass filter cut off, downsampling rate, Nyquist rate, and segment duration. However, these are not the model's hyperparameters (like learning rate, batch size, number of layers, etc.) that we typically optimize during the training process.

Furthermore, Table 4 shows the comparison of the average F1 score across different network architectures and dataset configurations. This table indicates that the models were trained with different configurations, and the results were averaged across 13 unique executions. However, it does not specify the method used for optimizing the model's hyperparameters.

Therefore, without further information, it is not possible to provide a definitive answer on how the hyperparameters of the model were optimized.
In the deep learning pipeline described in the provided context, two optimization techniques are explicitly mentioned: the Adam optimizer and the Stochastic Gradient Descent (SGD) optimizer.

The Adam optimizer is used during the training of Model-2, which is based on the EfficientNet feature extractor. The Adam optimizer is a popular optimization algorithm used in machine learning, known for its efficiency and adaptive learning rate. It combines the advantages of two other optimization methods: AdaGrad, which adapts the learning rate to the parameters, and RMSProp, which works well for online and non-stationary objectives.

On the other hand, the Stochastic Gradient Descent (SGD) optimizer is employed for training Model-K, a regression model based on VGG16 without the feature extractor. SGD is a simple yet powerful optimization algorithm that approximates the gradient of the loss function by considering only one training example at a time. It is widely used in machine learning and deep learning applications due to its simplicity and computational efficiency.

In summary, the Adam optimizer and the Stochastic Gradient Descent (SGD) optimizer are the two optimization techniques explicitly mentioned in the deep learning pipeline described in the provided context.
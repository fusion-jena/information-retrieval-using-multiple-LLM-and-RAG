Based on the provided context, the hyperparameters of the model are optimized through a process that involves using different combinations of settings for the parameters. However, the specific optimization techniques such as grid search or random search are not explicitly mentioned.

The hyperparameters that are being optimized include the 'conv\_base.trainable' parameter, which determines whether the pre-trained convolutional base of the model is trainable or not, and the 'Dropout Rate' parameter, which is used to prevent overfitting by randomly dropping out a certain percentage of the neurons during training.

The context provides several tables (Tables 7, 8, and 9) that show the results of calculations with different combinations of these parameters for ResNet50 and a combination of VGG19 and ResNet50. For example, Table 8 shows the results for ResNet50 with 'conv\_base.trainable' set to True and False, and with various 'Dropout Rate' values.

Additionally, the context mentions that data augmentation and flattened convolution kernels were used to achieve one of the optimal simulation results. However, it is not clear how these techniques were incorporated into the hyperparameter optimization process.

Therefore, while it is clear that the hyperparameters of the model were optimized using different combinations of settings, the specific optimization techniques used are not specified in the provided context.
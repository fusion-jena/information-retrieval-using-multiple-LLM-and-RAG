Based on the provided context, the techniques used to address data bias during the preprocessing of the deep learning pipeline include stratified random sampling, oversampling, and applying decision rules.

Firstly, stratified random sampling was used to ensure that the sample was representative of the population. Specifically, stratified random sampling points were generated using QGISâ€™ ACATAMa plug-in, and the number of points per class was increased by 5-fold based on the minimum deemed necessary to account for bias associated with restricted inclusion probabilities that result from the pre-defined distribution of GMEP squares (Olofsson et al., 2014). This technique helped to ensure that the sample was diverse and included sufficient data from each class.

Secondly, oversampling was used to address the marginal coverage of calcareous grassland and raised bog in GMEP. These classes were omitted, leaving 15 classes to compare. However, if contextual datasets were provided, these classes could be absorbed in appropriate wetland classes in the mapping, thus reducing discrepancies with GMEP data. This technique helped to ensure that the dataset was balanced and that no single class was overrepresented or underrepresented.

Lastly, applying decision rules was used to harmonize the data from LWHabmod and GMEPmod. Specifically, decision rules were applied based on the peat depth, particularly those affecting wetland/bog classes, to modify GMEPmod data labels. Additionally, decision rules were applied based on the Phase-1 Habitat map, and particularly those impacting on the classification of grassland, heathland, and scrub classes, to modify the GMEPmod data labels. These techniques helped to ensure that the data was consistent and that the labels were accurate.

Overall, the techniques used to address data bias during the preprocessing of the deep learning pipeline include stratified random sampling, oversampling, and applying decision rules. These techniques helped to ensure that the sample was representative, balanced, and consistent, which are essential factors for achieving accurate and reliable results in deep learning models.
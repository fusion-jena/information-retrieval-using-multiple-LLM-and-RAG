Based on the provided context, there is no explicit information about how the hyperparameters of the model are optimized. The text discusses the use of active learning to enhance and facilitate the data generation process, as well as the application of dimensionality reduction techniques to better understand model performance and select the best sample candidates for manual annotation. However, it does not mention any specific methods for hyperparameter optimization such as grid search or random search.

In general, hyperparameter optimization is an important step in training machine learning models, as it can significantly impact model performance. There are various methods for hyperparameter optimization, including grid search, random search, and more sophisticated techniques such as Bayesian optimization. Grid search involves systematically trying out different combinations of hyperparameters, while random search involves randomly selecting hyperparameters to try. Bayesian optimization, on the other hand, uses Bayesian inference to make informed decisions about which hyperparameters to try next.

Without more information, it is difficult to say which method is being used in this case. However, it is possible that the researchers are using a method that is not mentioned in the provided context. It is also possible that they are using a manual or ad-hoc approach to hyperparameter tuning.

In summary, based on the provided context, there is no explicit information about how the hyperparameters of the model are optimized. It is possible that the researchers are using a method such as grid search, random search, or Bayesian optimization, but this is not specified in the text.
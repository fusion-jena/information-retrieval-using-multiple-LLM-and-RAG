The provided context does not give specific information about how raw data is collected in the deep learning pipeline used in the study. However, it does mention that the pipeline includes a VGG16 feature extractor, which is a type of convolutional neural network (CNN) that is typically trained on large, labeled image datasets. Therefore, it can be inferred that the raw data used in this study is likely a large dataset of labeled images, which could have been collected using a variety of methods such as:

* Surveys or field observations: Researchers may have manually collected images of the phenomena of interest using cameras or other imaging devices.
* Sensors: Images may have been collected using automated sensors, such as cameras mounted on drones or satellites.
* Public datasets: The researchers may have used a publicly available dataset of images, such as the ImageNet dataset, which contains over 14 million images that have been labeled with thousands of categories.

Additionally, it's important to note that the raw data collected is not the only data that is used in the pipeline, the data management and feature extraction is a complex task and the authors aim to guide the user with clear and detailed descriptions of these steps.

Another point that the authors mention is the use of a screening tool to improve the quality of the ground-truth, this step is crucial as the accuracy and reliability of annotations should always be interrogated.

In summary, while the provided context does not give specific information about how raw data is collected in the deep learning pipeline used in the study, it can be inferred that the raw data is likely a large dataset of labeled images, which could have been collected using a variety of methods such as surveys, sensors, or public datasets.
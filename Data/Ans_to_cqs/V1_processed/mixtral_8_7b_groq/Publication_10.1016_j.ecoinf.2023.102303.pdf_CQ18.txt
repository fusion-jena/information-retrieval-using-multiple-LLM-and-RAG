Based on the provided context, there is no explicit information given about the strategy implemented to monitor the model performance during training. However, we can infer that the authors might have used validation data and accuracy metrics to monitor the performance of their proposed models.

In Section 2 of the text, the authors describe their methodology for developing two deep neural network (DNN) models for fish detection in turbid waters. They created a weakly labeled public dataset called FishInTurbidWater, which they used to train and evaluate their models.

The authors proposed two DNN models: a semi-supervised model for accelerating model development speed and a weakly-supervised model for conserving accuracy while reducing development time. They evaluated the accuracy of these models and compared them to fully-supervised models.

In the process of developing and evaluating these models, it is common practice to use a validation dataset to monitor the model's performance during training. This involves setting aside a portion of the labeled dataset for validation and using it to evaluate the model's performance after each epoch of training. By monitoring the validation accuracy, researchers can identify if the model is overfitting or underfitting the training data.

Additionally, the authors mentioned that they compared their proposed models with fully-supervised models from a recent survey paper by Saleh et al. (2022b). This comparison suggests that the authors might have used similar evaluation metrics as those used in the survey paper, such as accuracy, precision, recall, and F1-score.

Therefore, while the text does not explicitly mention the strategy used to monitor the model performance during training, it is likely that the authors used a combination of validation data and accuracy metrics to evaluate their models' performance.
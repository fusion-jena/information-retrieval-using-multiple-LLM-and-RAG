In the deep learning pipeline, the optimization technique applied for the Convolutional Neural Network (CNN) is the Adam learning rate optimizer. The Adam optimizer is a computationally efficient and straightforward method to adjust the initial learning rate during training, which improves performance. It sets the learning rate adaptively for faster and more efficient training. In contrast, for Support Vector Machines (SVM), the optimization technique is not explicitly mentioned in the text. However, it is stated that the hyperparameters for SVMs are optimized using a k-fold cross-validated fine grid-search on the training data.

Another optimization technique applied in the deep learning pipeline is early stopping. This technique is used to identify a suitable number of epochs for each dataset by stopping the training if the validation error (loss) does not reduce for a certain number of epochs. This approach saves time and automates the process of finding the optimal number of epochs for each dataset.

In summary, the optimization techniques applied in the deep learning pipeline include the Adam learning rate optimizer for the CNN and early stopping for all models. The optimization technique for SVMs is not explicitly mentioned, but it involves a k-fold cross-validated fine grid-search on the training data.
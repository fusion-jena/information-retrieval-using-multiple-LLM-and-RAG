The hyperparameters used in the deep learning model include learning rate (lr0), learning rate decay (Lrf), momentum, weight decay, warmup epochs (warmup_epochs), warmup momentum (warmup_momentum), warmup bias learning rate (warmup_bias_lr), and several other parameters related to the training of the model. The learning rate is set to 0.01, and the learning rate decay is also set to 0.01. The momentum is set to 0.937, and the weight decay is set to 0.0005. The warmup epochs are set to 3.0, and the warmup momentum is set to 0.8. The warmup bias learning rate is set to 0.1.

Additionally, there are several other hyperparameters related to data augmentation and other aspects of the training process. These include parameters for image hue, saturation, and value shifts (hsv_h, hsv_s, hsv_v), image rotation (degrees), image translation (translate), image scaling (scale), image shearing (shear), and perspective transformation (Perspective). There are also flags for horizontal and vertical flipping (flipud, fliplr), as well as parameters for mosaic data augmentation (mosaic) and mixup data augmentation (mixup).

These hyperparameters are based on the optimized values for YOLOv5 COCO training from scratch. The effort has been made to prevent overfitting and underfitting situations by tuning the model with these optimized hyperparameters and using a restrictive and gradually increasing training cycle based on mean square error (MSE).

The model with the attention layer is trained with epoch values of 100, 200, and 250 under a uniform training environment and with the same dataset. The training summary for each of these epochs is shown in Tables 5, 6, and 7, respectively. The trends of mAP values with increasing epochs are shown in Figs. 14 and 15.

The computation time for each epoch, including preprocessing, inference, and NMS times, is provided in Table 8. The average accuracy for different epochs is also provided in the last row of Table 8.

In summary, the hyperparameters used in the deep learning model include learning rate, learning rate decay, momentum, weight decay, warmup epochs, warmup momentum, warmup bias learning rate, and several other parameters related to data augmentation and training. These hyperparameters are optimized to prevent overfitting and underfitting situations and to achieve close to zero training error efficiently.
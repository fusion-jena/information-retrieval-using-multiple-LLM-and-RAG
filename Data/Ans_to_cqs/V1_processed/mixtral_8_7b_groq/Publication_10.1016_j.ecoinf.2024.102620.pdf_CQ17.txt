The provided context does not directly discuss the regularization methods used to prevent overfitting in deep learning pipelines, such as dropout or L2 regularization. The PLUS model mentioned in the context employs the random forest algorithm for land use type development probabilities, not a deep learning method. Therefore, the context does not offer information on the specific regularization techniques for deep learning models.

However, it is worth noting that deep learning models, like neural networks, are prone to overfitting, especially when dealing with complex datasets or when the model has many parameters. Regularization techniques are essential to mitigate overfitting and improve the model's generalization ability.

Two common regularization methods for deep learning models are:

1. L2 regularization: This method adds a penalty term to the loss function, which is proportional to the square of the magnitude of the model's weights. By adding this penalty, the optimization process is encouraged to keep the weights small, reducing overfitting.

2. Dropout: This technique randomly sets a fraction of the neurons in a layer to zero during training. By doing so, the model's reliance on any single neuron is reduced, effectively preventing overfitting and improving the model's performance on unseen data.

In summary, the provided context does not discuss the regularization methods used to prevent overfitting in deep learning pipelines. However, L2 regularization and dropout are two common regularization techniques used in deep learning to mitigate overfitting.
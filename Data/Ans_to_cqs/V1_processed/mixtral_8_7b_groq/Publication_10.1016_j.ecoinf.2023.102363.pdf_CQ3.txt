Based on the provided context, the data annotation techniques used in the deep learning pipeline are not explicitly mentioned. However, it can be inferred that the annotation process involves labeling sound source classes in audio data, which is then transformed into a spectrogram representation.

In the context, the authors mention using a "novel spectrogram representation" for inputting audio data into the network. This suggests that the annotation process likely involves labeling specific sound source classes in the audio data, which are then represented visually in the spectrogram.

However, the specific annotation techniques such as bounding box annotation or instance segmentation are not mentioned. These techniques are commonly used in computer vision tasks, but may not be directly applicable to audio data and spectrogram representations.

It is also worth noting that the deep learning pipeline described in the context involves transfer learning, where a pre-trained model (EfficientNet B0) is used as the starting point. This means that the model's feature extractor is frozen, and only the fully connected layers are trained on the new audio data. As a result, the annotation process may primarily involve labeling the sound source classes in the new audio data, rather than annotating specific features within the spectrogram.

Overall, while the specific data annotation techniques used in the deep learning pipeline are not explicitly mentioned in the context, it can be inferred that the annotation process involves labeling sound source classes in the audio data, which is then transformed into a spectrogram representation for input into the deep learning model.
The provided context discusses the use of deep learning models for image classification tasks, specifically for distinguishing tree species using RGB images or hyperspectral data. However, it does not explicitly mention the optimization techniques used in the deep learning pipeline.

The deep learning pipeline typically involves several steps, such as data preparation, model building, compilation, training, and testing. During the training step, an optimization technique is employed to adjust the model's parameters and minimize the loss function. Common optimization techniques include Stochastic Gradient Descent (SGD), Adagrad, Adadelta, RMSprop, and Adam.

Based on the context, we can infer that the deep learning models are built using Keras, a popular deep learning library that supports various optimization techniques. However, the specific optimization technique used in the study is not mentioned.

Therefore, we cannot provide a specific answer to the query regarding the optimization techniques applied in the deep learning pipeline. Nonetheless, it is essential to choose an appropriate optimization technique based on the problem's characteristics and the model's complexity. For instance, Adam is a popular optimization technique that combines the advantages of Adagrad and RMSprop, offering adaptive learning rates and computational efficiency.

In summary, while the provided context does not explicitly mention the optimization technique used in the deep learning pipeline, it is crucial to choose an appropriate optimization technique for training deep learning models. Popular optimization techniques include SGD, Adagrad, Adadelta, RMSprop, and Adam.
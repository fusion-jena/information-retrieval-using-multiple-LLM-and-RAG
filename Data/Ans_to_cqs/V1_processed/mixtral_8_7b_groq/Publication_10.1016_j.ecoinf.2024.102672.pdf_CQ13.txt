The hyperparameters used in the deep learning model include those that define the architecture of the artificial neural network (ANN) and those that control the training process. The hyperparameters that define the ANN architecture include the number of layers and the number of neurons. On the other hand, the hyperparameters that control the training process include the learning rate, the optimizer, the batch size, and the number of epochs.

In this particular study, the optimizer used was the Adam algorithm, which is a stochastic gradient descent method. The number of epochs was adjusted to the minimum number of epochs that guaranteed the stabilization of the loss curve in the four folds of the cross-validation. The batch size was not explicitly mentioned in the context provided.

Additionally, the learning rate was not explicitly mentioned in the context provided. However, it was mentioned that a small dimensional space was initially defined and explored with the original variables, which suggests that the learning rate was included as one of the hyperparameters that were adjusted during the cross-validation process.

It is also worth noting that the cross-validation process was performed using k-fold cross-validation, where the training set was split into k fractions, and each fraction was used iteratively as a validation set, while the remaining k-1 fractions were used as the training set. This method was used to avoid the risk of overfitting the models.

In summary, the hyperparameters used in the deep learning model include the number of layers and the number of neurons that define the ANN architecture, and the learning rate, the optimizer (Adam algorithm), and the number of epochs that control the training process. The batch size was not explicitly mentioned in the context provided.
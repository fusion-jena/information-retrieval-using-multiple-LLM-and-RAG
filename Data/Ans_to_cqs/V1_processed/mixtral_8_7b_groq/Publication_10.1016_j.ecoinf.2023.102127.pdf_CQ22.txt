Based on the provided context, the text does not explicitly mention which metrics are used to evaluate the performance of a deep learning model for species distribution modeling (SDM). However, it does provide some insights into the general approach for model evaluation in SDM.

The text by Isaac et al. (2020) and Jung (2023) highlights the importance of model evaluation in SDM, considering the complexities of integrating all available data, dealing with biases and sampling methods, and ensuring that validation datasets represent the niche and environmental parameters estimated by the integrated model.

In the context of deep learning models, common evaluation metrics such as accuracy, precision, and recall can be used. Accuracy measures the proportion of correct predictions out of all predictions made, while precision measures the proportion of true positive predictions out of all positive predictions. Recall, on the other hand, measures the proportion of true positive predictions out of all actual positive instances in the data.

However, the text suggests that for SDM, other metrics such as True Skill Statistics (TSS) may be more appropriate. TSS is a threshold-dependent measure that takes into account both omission and commission errors and is less sensitive to prevalence than accuracy.

Additionally, the text mentions thresholding as a method for converting continuous suitability predictions into binary, categorical, or normalized outputs. Thresholding can help retain some of the detail of the projected suitability distribution while also removing uncertain areas and noise.

In summary, while the text does not explicitly mention which metrics are used to evaluate the performance of deep learning models in SDM, it suggests that TSS and thresholding may be more appropriate than common metrics such as accuracy, precision, and recall. However, further information is needed to provide a definitive answer to this query.
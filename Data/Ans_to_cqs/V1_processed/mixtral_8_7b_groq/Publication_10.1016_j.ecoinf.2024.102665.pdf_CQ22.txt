Based on the provided context, there is no specific information about the metrics used to evaluate the performance of the deep learning model in the given study by Do and Tran (2023). However, generally, in deep learning and machine learning, several metrics can be used to evaluate the performance of a model, such as accuracy, precision, recall, F1-score, and area under the curve (AUC).

Accuracy is the ratio of the number of correct predictions to the total number of predictions. It is a common metric for evaluating the performance of a model, but it may not be the best metric for imbalanced datasets.

Precision is the ratio of true positive predictions (relevant items that are correctly identified) to the total predicted positive items. It measures the proportion of correct positive predictions out of all positive predictions made by the model.

Recall is the ratio of true positive predictions to the total actual positive items. It measures the proportion of correctly identified relevant items out of all relevant items in the dataset.

F1-score is the harmonic mean of precision and recall, which provides a balanced evaluation of both metrics.

Area under the curve (AUC) is a metric used for binary classification problems. It measures the entire two-dimensional area underneath the entire ROC curve (receiver operating characteristic curve), from (0,0) to (1,1). AUC provides an aggregate measure of performance across all possible classification thresholds.

In the study by Do and Tran (2023), the deep learning model is used to detect the dispersal of the early stages of spotted butterfish in northern Vietnam under global warming. However, the context does not provide information on how the performance of the deep learning model is evaluated. It is possible that the authors used one or more of the above-mentioned metrics or other custom evaluation metrics relevant to their study.
The performance of the deep learning model is evaluated using the metrics of precision, recall, and the average precision. Accuracy is not explicitly mentioned in the provided context.

Precision is defined as the number of true positives divided by the sum of true positives and false positives. It measures the proportion of correct positive predictions out of all positive predictions made by the model.

Recall, also known as sensitivity, is defined as the number of true positives divided by the sum of true positives and false negatives. It measures the proportion of correctly identified positive cases out of all actual positive cases.

The average precision is used as a single model-performance metric, which takes into account the trade-off between precision and recall. It is calculated as the average precision over all observed recall values. The value of average precision ranges between 0 and 1, with a value of 1 indicating an ideal predictive model that always detects every class correctly.

Moreover, the model's performance is also evaluated by examining the types of recall and precision errors made by the model when the confidence threshold is set to a specific value, such as 0.7. The model's confusion between the three focal species, false positive predictions, and false negative predictions are analyzed to evaluate its performance.
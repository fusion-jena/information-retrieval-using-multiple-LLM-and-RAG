Based on the provided context, the process followed to deploy the trained deep learning model is not explicitly mentioned. However, we can infer some parts of the deployment process based on the information given in the different sources.

Firstly, after training the Convolutional Neural Network (CNN) model, the authors may have saved or serialized the model for later use. Model serialization is the process of saving the model architecture, weights, and other necessary parameters into a file, allowing the model to be easily loaded and used for making predictions. In the context, authors like Cao, H. et al. (2022) and Chen, H. et al. (2020) have not explicitly mentioned model serialization, but it is a common practice in deep learning model deployment.

Secondly, the authors may have selected a platform for deploying the model. Platform selection depends on the specific use case and the infrastructure available. For instance, the CNN model developed by Cao, H. et al. (2022) for cyanobacterial harmful algae blooms prediction in Taihu Lake, China, could be deployed on a web server or a mobile application for real-time water quality monitoring. Similarly, the spatio-temporal analysis model by Cao, N. et al. (2023) could be integrated into a Geographic Information System (GIS) for visualizing and analyzing the colored dissolved organic matter distribution in Ebinur Lake.

Lastly, the authors may have integrated the trained model into an application programming interface (API) or a software development kit (SDK) for easier integration with other systems or applications. An API or SDK allows developers to use the model's prediction capabilities without having to understand the underlying deep learning architecture.

In conclusion, while the context does not provide explicit details on the process followed to deploy the trained deep learning model, it can be inferred that the authors may have serialized the model, selected a platform for deployment, and integrated the model into an API or SDK.
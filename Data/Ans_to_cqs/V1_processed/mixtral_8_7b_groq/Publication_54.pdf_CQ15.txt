The provided context discusses the use of the Adam optimizer in the deep learning pipeline for training the Inception-ResNet-v2 model. However, it does not mention any other optimization techniques.

The Adam optimizer is a popular optimization algorithm used in deep learning models. It is an adaptive learning rate optimization algorithm that combines the advantages of two other optimization algorithms: AdaGrad and RMSProp. Adam adjusts the learning rate for each parameter separately, based on the estimated first and second moments of the gradient. This makes it well-suited for large datasets and non-convex optimization problems, which are common in deep learning.

In contrast, Stochastic Gradient Descent (SGD) is a simpler optimization algorithm that updates the model parameters by taking a step in the direction of the negative gradient of the loss function. SGD is often used as a baseline optimization algorithm for deep learning models, but it can be less efficient than more advanced optimizers like Adam.

Therefore, based on the provided context, the Adam optimizer is the only explicitly mentioned optimization technique used in the deep learning pipeline. However, it is possible that other optimization techniques were used but not mentioned in the context.
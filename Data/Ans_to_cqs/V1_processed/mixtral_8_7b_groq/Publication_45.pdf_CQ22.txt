The performance of the deep learning model was evaluated using a variety of metrics, including accuracy, precision, recall, F1 score, Conditional Kappa, and the True Skill Statistic. Accuracy, which is the ratio of correct classifications to the total number of examples, is a commonly used metric for evaluating the performance of a model. However, in this study, the class distribution was very unbalanced, with only 3.48% of the data points classified as disturbed. Therefore, accuracy alone would not provide a comprehensive evaluation of the model's performance.

Precision, recall, and the F1 score are more appropriate metrics for evaluating the performance of a model when the class distribution is imbalanced. Precision is the ratio of true positive predictions (correctly classified disturbed data points) to the total number of positive predictions (all data points predicted to be disturbed). Recall is the ratio of true positive predictions to the total number of actual positive data points (all disturbed data points). The F1 score is the harmonic mean of precision and recall and provides a single metric that balances the importance of both precision and recall.

In addition to these metrics, Conditional Kappa and the True Skill Statistic were also used to evaluate the performance of the deep learning model. Conditional Kappa ranges from -1 to +1, where +1 indicates perfect agreement with test data, and values greater than 0 indicate a performance that is better than random. The True Skill Statistic is a proper scoring rule that measures the accuracy of binary classifications and is particularly useful when evaluating the performance of a model on imbalanced datasets.

These metrics were used to evaluate the performance of the deep neural network (DNN) model in the context of ecological prediction, machine learning, and computational ecology, specifically for the prediction of forest disturbance. The DNN model was trained and evaluated using the TensorFlow framework and run on a desktop PC with an Intel QuadCore CPU and an NVidia GTX 1070 GPU. The performance of the DNN model was compared to other widely used classification algorithms, including distributed random forest, gradient boosting machine, and generalized linear model, which were implemented using the H2O platform.
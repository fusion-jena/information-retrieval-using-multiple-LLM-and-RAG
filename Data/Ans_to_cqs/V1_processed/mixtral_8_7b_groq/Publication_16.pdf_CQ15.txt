The deep learning pipeline mentioned in the context utilizes the Adam optimization algorithm in the training of the Multilayer Perceptron (MLP) model. This information is provided in the first piece of context, which states that the MLP is configured with Backpropagation as the learning algorithm and Adam as the optimizer.

The Adam optimization algorithm is a popular choice for deep learning models due to its efficiency and effectiveness. It is an adaptive learning rate optimization algorithm that combines the advantages of both AdaGrad and RMSProp. Adam adjusts the learning rate for each parameter separately, taking into account the historical gradient information. This allows for faster convergence and better generalization of the model.

Therefore, it can be said with confidence that the Adam optimization algorithm is used in the deep learning pipeline described in the context.
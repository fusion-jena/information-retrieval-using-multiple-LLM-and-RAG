The provided context does not explicitly mention the criteria used to determine when training is complete. However, it does discuss the execution time analysis of different approaches, including IFTL and FTL, for improving learning by restricting overconfidence during transfer learning (TL).

In the context, the authors evaluate the performance of IFTL and FTL based on their ability to handle data distribution differences between the source and target domains. They find that IFTL outperforms FTL when there are significant data distribution differences between the training and testing domains. This suggests that the authors might have used some form of validation or testing to assess the performance of the models and determine when training is complete.

In TL, it is common to use a validation set to monitor the performance of the model during training. The validation set is a separate dataset that is used to evaluate the model's performance periodically during training. When the performance on the validation set stops improving or starts to degrade, it is an indication that the model is overfitting to the training data, and training can be stopped.

In the context, the authors use GDP prediction as an example of a TL problem. They might have used a validation set to monitor the performance of the models on the GDP prediction task. However, the specific criteria used to determine when training is complete are not mentioned in the provided context.

Therefore, while the provided context does not explicitly answer the query, it suggests that the authors might have used some form of validation or testing to determine when training is complete. In TL, it is common to use a validation set to monitor the performance of the model during training and determine when training should be stopped.
The provided context does not explicitly mention how the hyperparameters of the model are optimized. There is no information given about techniques such as grid search, random search, or Bayesian optimization.

However, the context does mention that all models were trained with the same parameters: 500 epochs, an initial learning rate of 0.01 with a weight decay of 0.005, the optimizer Adam with a beta1 of 0.937, and data augmentation transformations. This suggests that these parameters were set to fixed values and not optimized using a hyperparameter tuning method.

It is also worth noting that cross-validation was used to ensure the statistical results and prevent overfitting. While cross-validation is not a hyperparameter tuning method itself, it can be used in conjunction with techniques like grid search or random search to evaluate the performance of different hyperparameter configurations. However, the context does not provide information on whether this was the case.

In summary, based on the provided context, it is not possible to determine how the hyperparameters of the model were optimized.
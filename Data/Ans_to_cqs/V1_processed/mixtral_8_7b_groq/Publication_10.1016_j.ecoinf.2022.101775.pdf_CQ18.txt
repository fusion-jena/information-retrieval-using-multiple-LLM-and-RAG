The strategy implemented to monitor the model performance during training involves the use of a validation set and performance metrics. Specifically, 10% of the available data was set aside as a validation set to check for overfitting during the training process. This means that after every epoch, the model's performance was evaluated on this validation set to ensure that the model was not simply memorizing the training data.

In addition to using a validation set, performance metrics were computed using a min-max scaler that was reverted at the end. This allowed for the standardization of the data, which can help improve the model's performance. The performance metrics were computed on the test set, which was kept separate from the training and validation sets to provide an unbiased estimate of the model's performance.

The use of a batch size of 64 samples and training the algorithm during 100 epochs using Adam as the optimizer also contributed to the monitoring of the model's performance. The batch size determines the number of samples that are processed before the model's weights are updated, while the number of epochs determines the number of times the entire training dataset is passed through the model. By using a batch size of 64 samples, the model's performance could be monitored after each batch, while training for 100 epochs allowed for a more thorough evaluation of the model's performance.

Finally, it is worth noting that the models were implemented in PyTorch and trained and tested in Google Colaboratory. These tools provide various features for monitoring the model's performance during training, such as visualizing the loss function, accuracy, and other metrics.

In summary, the strategy implemented to monitor the model performance during training involved the use of a validation set, performance metrics, a batch size of 64 samples, training for 100 epochs, and using PyTorch and Google Colaboratory to implement and train the models.
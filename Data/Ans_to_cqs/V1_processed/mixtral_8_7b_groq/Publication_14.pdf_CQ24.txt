In the context provided, there is one primary example of a strategy used to handle randomness in a deep learning pipeline: the use of a random seed value. In the paper "A deep convolutional neural network for background subtraction" by Babaee, Dinh, and Rigoll (2017), the authors describe their use of a fixed random seed value to ensure reproducibility in their experiments. Specifically, they set a specific seed value for the random number generator used in their dropout regularization technique. Dropout is a method that randomly removes a proportion of nodes during training to prevent overfitting, and by using a fixed seed value, the authors ensure that the same nodes are removed in each training iteration, leading to consistent results.

Additionally, in the paper "AnimalFinder: A semi-automated system for animal detection in time-lapse camera trap images" by Price Tack et al. (2016), the authors mention the importance of setting a random seed value for achieving reproducible results in machine learning algorithms. However, they do not explicitly state whether they used a fixed seed value in their own pipeline.

These examples demonstrate that setting a random seed value is a common strategy for handling randomness in deep learning pipelines, particularly when it comes to ensuring reproducibility. By fixing the seed value, researchers can control the randomness in their algorithms, making it easier to compare results across different experiments and models.

In summary, the provided context highlights the use of a fixed random seed value as a strategy for handling randomness in deep learning pipelines. This approach ensures reproducibility by controlling the randomness in algorithms such as dropout regularization.
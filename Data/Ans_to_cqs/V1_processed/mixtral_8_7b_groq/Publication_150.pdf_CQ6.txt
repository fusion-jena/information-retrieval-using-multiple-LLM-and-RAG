Based on the provided context, there is no explicit information about the specific preprocessing steps taken before training the deep learning models. However, it is common practice in deep learning to perform several preprocessing steps on the data to improve model performance. These steps can include:

1. Normalization: This involves scaling the input data to a range that is suitable for the model. In the context of image data, this often means scaling pixel values to a range of [0,1] or [-1,1]. Normalization can help improve model convergence and reduce the likelihood of getting stuck in local minima.

2. Scaling: Similar to normalization, scaling involves adjusting the range of the input data. However, scaling does not necessarily preserve the distribution of the data. Scaling can be useful when dealing with data that has widely varying scales.

3. Data augmentation: This involves creating new training examples by applying various transformations to the existing data. Common transformations include rotation, scaling, flipping, and cropping. Data augmentation can help improve model generalization and reduce overfitting.

4. Cleaning: This involves removing or correcting any errors or inconsistencies in the data. For example, this might involve removing outlier data points or correcting mislabeled data.

5. Feature extraction: This involves extracting relevant features from the data that are useful for the task at hand. For example, in the context of image data, this might involve extracting color histograms or texture features.

Based on the context provided, it is likely that some or all of these preprocessing steps were taken before training the deep learning models. However, the specific preprocessing steps are not mentioned in the context.
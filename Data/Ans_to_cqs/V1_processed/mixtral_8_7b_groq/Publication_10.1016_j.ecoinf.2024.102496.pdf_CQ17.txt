The provided context does not contain information about the regularization methods used in the deep learning pipeline to prevent overfitting. The text discusses an equitable nutrient reduction decision support tool, its framework, and the importance of accurate data collection and updating input parameters in the PLET model. It also mentions different BMPs (Best Management Practices) for reducing pollutant loads in urban watersheds, such as infiltration devices, alum treatment, porous pavement filter, wet pond, and wetland detention.

However, the query asks about regularization methods used in deep learning pipelines to prevent overfitting. Regularization techniques, like dropout, L1/L2 regularization, and early stopping, are commonly used in machine learning and deep learning models to prevent overfitting by adding a penalty term to the loss function or by stopping the training process early.

Dropout is a regularization technique used in neural networks where randomly selected neurons are dropped out during training, preventing over-dependence on any single neuron and thus reducing overfitting. L1 and L2 regularization, also known as weight decay, adds a penalty term to the loss function proportional to the magnitude of the model's weights. Early stopping is a technique where training is halted when the model's performance on a validation set stops improving, preventing the model from learning the noise in the training data.

Therefore, I cannot answer the query using the provided context. However, the query can be answered by discussing regularization techniques used in deep learning pipelines to prevent overfitting.
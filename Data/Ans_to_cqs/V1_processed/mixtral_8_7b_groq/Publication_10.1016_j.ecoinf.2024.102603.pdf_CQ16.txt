The provided context does not give specific criteria used by the XGBoost model to determine when training is complete. However, it mentions that XGBoost is an ensemble model based on regression decision trees and it trains a series of trees, with each subsequent tree starting its learning process anew, taking into account the correction for the residual values of the previous tree. This implies that the training process continues until a stopping criterion is met.

In general, popular stopping criteria for training machine learning models include:

1. Validation loss plateau: Training stops when the validation loss does not decrease for a certain number of epochs or iterations.

2. Early stopping: Training stops when the validation loss starts to increase, even if the training loss is still decreasing.

3. Maximum number of iterations or epochs: Training stops when a predefined maximum number of iterations or epochs is reached.

4. Computational time: Training stops when a predefined time limit is reached.

5. Resource constraints: Training stops when available computational resources, such as memory, are exhausted.

For the XGBoost model specifically, common hyperparameters that can be tuned to control the training process include:

1. `num_boost_round`: The maximum number of iterations for building trees.

2. `early_stopping_rounds`: Stops the training early if the performance on the validation set does not improve for a certain number of rounds.

3. `eta` (learning rate): Controls the contribution of each tree in the ensemble. Smaller learning rates require more iterations to converge but may result in better generalization.

4. `gamma` (minimum loss reduction): Stops splitting a node if the resulting split does not reduce the loss by a certain amount.

5. `max_depth`: The maximum depth of the trees. Deeper trees can model complex relationships but may lead to overfitting.

Without more specific information from the provided context, it is unclear which of these criteria or hyperparameters are used to determine when training is complete for the XGBoost model in this case.
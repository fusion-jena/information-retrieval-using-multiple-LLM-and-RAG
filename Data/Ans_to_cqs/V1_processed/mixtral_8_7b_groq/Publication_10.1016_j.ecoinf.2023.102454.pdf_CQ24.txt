The provided context does not contain information on the specific strategies employed to handle randomness in the deep learning pipeline. However, it is common practice in machine learning and deep learning to use various methods to manage randomness and ensure reproducibility.

One way to handle randomness is by setting a fixed seed value for the random number generator. This ensures that the same sequence of random numbers is generated every time the model is trained, making the training process reproducible. This can be done in popular deep learning libraries such as TensorFlow and PyTorch.

Another way to handle randomness is through data augmentation techniques, where random transformations are applied to the training data to increase its size and variability. This can help improve the model's ability to generalize and reduce overfitting.

In addition, techniques such as dropout and batch normalization can be used to introduce randomness during training, which can help improve the model's performance and robustness. Dropout randomly sets a fraction of the neurons in a layer to zero during training, preventing over-reliance on certain neurons and promoting regularization. Batch normalization normalizes the inputs of each layer, reducing internal covariate shift and allowing for higher learning rates.

Finally, it is important to note that the context provided is a research paper on ecological informatics, specifically on cold and hot spot analysis, and does not involve deep learning or machine learning. Therefore, it is not surprising that it does not mention any strategies for handling randomness in the deep learning pipeline.
Based on the provided context, the following measures were taken to ensure the generalizability of the deep learning models used in the study:

1. Diverse dataset: The dataset used to train the models contains 3000 5-second segments, with 1130 segments consisting of possum vocalizations and 1870 segments consisting of noise sources. This diverse dataset allows the models to learn a wide range of features and improve their ability to generalize to new data.

2. Cross-validation: Although the context does not explicitly mention cross-validation, it does state that model performance is re-evaluated on the test set at 10 sample increments using each of the three sample orderings. This process can be seen as a form of cross-validation, as it allows the researchers to assess the performance of the models on different subsets of the data.

3. Stratified splitting: The dataset has an 80/20% training-validation split, which is used to evaluate model performance during training. This split is likely to be stratified, meaning that the distribution of target features (possum vocalizations) to non-target features (noise sources) is maintained in both the training and validation sets. This ensures that the models are trained and evaluated on a representative sample of the data, which can improve their ability to generalize to new data.

4. Fine-tuning on a subset of data: The AST feature extraction model is fine-tuned on a subset of 500 training samples. This approach can help improve the model's ability to generalize to new data, as it allows the model to focus on the most informative samples in the training set.

5. Evaluation on a separate test set: The models are evaluated on a separate test set containing 500 segments, with 187 segments consisting of possum vocalizations and 313 segments consisting of noise sources. This allows the researchers to assess the performance of the models on data that they have not seen during training, which can provide a better estimate of the models' ability to generalize to new data.

In summary, the researchers took several measures to ensure the generalizability of the deep learning models used in the study, including using a diverse dataset, cross-validation, stratified splitting, fine-tuning on a subset of data, and evaluation on a separate test set.
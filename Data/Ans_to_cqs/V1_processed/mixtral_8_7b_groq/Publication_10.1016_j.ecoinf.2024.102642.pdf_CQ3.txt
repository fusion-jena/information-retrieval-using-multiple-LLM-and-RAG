Based on the provided context, the specific data annotation techniques used in the deep learning pipeline for cetacean vocalization detection are not explicitly mentioned. However, it is mentioned that multi-annotation campaigns were conducted involving both expert and novice annotators. These annotators likely manually labeled the cetacean vocalizations in the underwater Passive Acoustic Monitoring (PAM) studies.

The context also mentions that there were distinctive annotation profiles influenced by the complexity of vocalizations and the annotatorsâ€™ strategies, ranging from conservative to permissive. This suggests that the annotation process may have involved labeling the start and end times of the vocalizations, with varying levels of strictness in determining what constitutes a vocalization.

To address the challenges of annotation discrepancies, Convolutional Neural Networks (CNNs) were trained on annotations from both novices and the expert. The results showed variations in model performance, highlighting the importance of annotation guidelines encouraging a more conservative approach to improve overall annotation quality.

Furthermore, to optimize the potential of multi-annotation and mitigate the presence of noisy labels, two annotation aggregation methods (majority voting and soft labeling) were proposed and tested. These methods significantly improved model performance and reduced variability, particularly when a sufficient number of annotators were involved.

In summary, while the specific data annotation techniques are not explicitly mentioned, it is likely that manual labeling of vocalization start and end times was involved, with varying levels of strictness. Additionally, annotation aggregation methods were used to improve model performance and reduce variability.
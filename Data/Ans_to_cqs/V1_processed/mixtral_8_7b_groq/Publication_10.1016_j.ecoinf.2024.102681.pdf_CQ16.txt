The text provided does not give explicit information about the criteria used to determine when training is complete. However, it does mention that the model training process involved an iterative approach to extract the weights assigned to the fully connected layers, which suggests that some form of convergence or stability in the weights might have been used as a stopping criterion.

In the context of machine learning, training is often stopped when the model's performance on a validation set stops improving, such as when the validation loss plateaus or starts to increase. This is a common strategy to prevent overfitting, which occurs when the model becomes too specialized to the training data and performs poorly on new, unseen data.

However, the specific stopping criterion used in this study is not mentioned. It could be that the authors used a predetermined number of iterations, or perhaps they employed a more sophisticated method like early stopping, where training is halted as soon as the validation loss starts to degrade.

In summary, while the exact criterion for determining when training is complete is not specified in the provided text, it is likely that some form of validation-based stopping rule was used to prevent overfitting.
Based on the provided context, there is no explicit information about post-processing steps such as saliency maps, metrics calculation, or confusion matrix after the model training. However, it does mention some details about the processing that takes place during the model training.

The model training involves several layers, including convolution layers, pooling layers, Local Response Normalization (LRN), and a dropout layer. After the second layer, which is a convolution layer with 128 3 × 3 filters and a stride of 1, the ReLU activation function is applied, followed by LRN and pooling. The output of this layer has a size of 28 × 28 × 128.

The third to eighth layers consist of three inception modules, which further consist of seven convolution modules and a pooling module. These inception modules are divided into four branches, with varying numbers of filters and strides. The output of the eighth layer is then fed into the ninth layer, another convolution layer with 480 3 × 3 filters and a stride of 1, followed by LRN and pooling. The output of this layer has a size of 6 × 6 × 480.

The tenth layer is a dropout layer, which is used to improve the generalization capability of the model. The size of the output result from this layer is 17,280.

Therefore, while the context does not provide information on post-processing steps such as saliency maps, metrics calculation, or confusion matrix, it does detail the processing that takes place during the model training, including convolution, pooling, LRN, and dropout layers.
The provided context does not directly discuss regularization methods used in deep learning pipelines to prevent overfitting, such as dropout or L2 regularization. However, it does discuss various studies and methods related to species distribution modeling (SDM) and deep neural networks.

In the context of SDM, model overfitting can be prevented by using techniques such as k-fold cross-validation, where the dataset is divided into k subsets, and the model is trained on k-1 subsets while the remaining subset is used for validation. This process is repeated k times, and the average performance is used as the final evaluation metric.

In the context of deep learning, regularization methods such as dropout and L2 regularization are commonly used to prevent overfitting. Dropout is a technique where randomly selected neurons are dropped during training, which helps to prevent over-reliance on any single neuron and encourages the model to learn more robust features. L2 regularization, on the other hand, adds a penalty term to the loss function that is proportional to the square of the magnitude of the weights. This encourages the model to learn smaller weights, which can help to prevent overfitting.

Therefore, while the provided context does not directly discuss regularization methods used in deep learning pipelines, it is reasonable to assume that similar techniques are used to prevent overfitting in SDM models.
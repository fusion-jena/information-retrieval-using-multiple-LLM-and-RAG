Based on the provided context, there are several preprocessing steps involved before training a deep learning model.

First, the data undergoes manual removal of the testing set to create a separate set for testing the model's performance. This step ensures that the model's performance is evaluated on unseen data.

Next, a validation set is created from 10% of the remaining data to check for overfitting during the training process. Overfitting occurs when the model learns the training data too well, including its noise and outliers, and performs poorly on new data. The validation set helps to mitigate this issue by providing a separate set of data to evaluate the model's performance during training.

The data is then scaled using a min-max scaler, which scales the data between a specific range, usually between 0 and 1. Scaling the data is essential when working with deep learning models as they are sensitive to the scale of the input features. Features with different scales can affect the model's learning process, and scaling ensures that all features contribute equally to the model's learning.

In addition, the context mentions the use of embedding layers, which are modules of deep neural networks that solve the one-hot encoding problem. Embedding layers replace sparse vectors, mainly composed of zero values, with dense vectors of learnable parameters. This step helps to reduce the dimensionality of the data and improve the model's learning process.

Finally, the deep learning model is trained using a batch size of 64 samples and trained during 100 epochs using Adam as the algorithm to optimize the network parameters. The model is implemented in Pytorch and trained and tested in Google Colaboratory.

In summary, the preprocessing steps involved before training a deep learning model include manual removal of the testing set, creation of a validation set, scaling of the data using a min-max scaler, use of embedding layers, and training the model using a batch size and a specific number of epochs.
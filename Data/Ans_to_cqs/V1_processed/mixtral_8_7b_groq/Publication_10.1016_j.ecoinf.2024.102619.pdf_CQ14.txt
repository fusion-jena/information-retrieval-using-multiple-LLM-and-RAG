In the context provided, the hyperparameters of the Support Vector Machine (SVM) models are optimized using a k-fold cross-validated fine grid-search on the training data. Specifically, for the non-linear Radial Basis Function (RBF) SVM, the hyperparameters C and γ are searched over a fine grid of values. The linear SVM uses the same hyperparameter search for its sole parameter C. 

The regularization parameter C offers a trade-off between the maximum-margin and misclassification rate. A large C enforces a small margin hyper-plane, maximizing classification accuracy. The parameter γ determines the distance over which support vectors influence the hyperplane. A high γ considers only points that are close to each other, causing the decision-boundary to be highly curved.

The grid-search technique involves creating a grid of possible hyperparameter values and training the model with each combination of hyperparameters in the grid. The performance of each combination is evaluated, and the combination with the best performance is selected as the optimized set of hyperparameters. 

Cross-validation is used to ensure that the model's performance is not overestimated due to overfitting. In k-fold cross-validation, the training data is divided into k folds. The model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with a different fold used for testing each time. The average performance over the k tests is used as the final performance measure.

It is important to note that the optimization of hyperparameters is a time-consuming process that requires careful tuning and exploration. The authors emphasize the importance of this step in maximizing the performance of the models.

In summary, the hyperparameters of the SVM models in the context provided are optimized using a k-fold cross-validated fine grid-search on the training data. The regularization parameter C and the distance parameter γ are searched over a fine grid of values for the non-linear RBF SVM, and the regularization parameter C is searched for the linear SVM. Cross-validation is used to ensure that the model's performance is not overestimated due to overfitting. The optimization of hyperparameters is a time-consuming process that requires careful tuning and exploration.
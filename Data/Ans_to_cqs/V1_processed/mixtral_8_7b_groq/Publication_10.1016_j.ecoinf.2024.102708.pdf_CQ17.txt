The provided context does not give specific information about the regularization methods used to prevent overfitting in the deep learning pipeline for the study on Pacific oyster detection. However, it is a common practice in deep learning to use regularization techniques to prevent overfitting. Some of the regularization methods include:

1. L1 and L2 regularization: These methods add a penalty term to the loss function, which discourages large weights. L1 regularization, also known as Lasso regularization, adds an absolute value of the weight as a penalty term, while L2 regularization, also known as Ridge regularization, adds the square of the weight as a penalty term.

2. Dropout: This method randomly sets a fraction of the output of a layer to zero during training. This helps to prevent overfitting by making the network more robust and less dependent on the training data.

3. Early stopping: This method stops the training process before the model starts to overfit. This is done by monitoring the performance of the model on a validation set during training and stopping the training when the performance on the validation set starts to degrade.

4. Data augmentation: This method generates new training samples by applying transformations to the existing training data. This helps to increase the size of the training set and reduce overfitting.

Based on the common practices in deep learning and the context provided, it can be inferred that the deep learning pipeline for Pacific oyster detection might have used one or more of these regularization methods to prevent overfitting. However, the specific regularization methods used are not provided in the context.
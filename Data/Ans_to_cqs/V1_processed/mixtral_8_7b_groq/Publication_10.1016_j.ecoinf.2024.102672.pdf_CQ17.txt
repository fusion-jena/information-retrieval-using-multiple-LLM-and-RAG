The regularization methods used to prevent overfitting in the deep learning pipeline, as mentioned in the provided context, include dropout. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and less prone to overfitting. In the context, a dropout layer is added after a dense layer, with a dropout rate of 0.2 or 0.4.

However, L2 regularization is not explicitly mentioned in the provided context. L2 regularization is a method that adds a penalty to the loss function, which is proportional to the square of the magnitude of the weights. This encourages the weights to be smaller, which can help to prevent overfitting.

It's worth noting that the context focuses on the use of a classification model to detect thermal stratification, and the optimization of the model's hyperparameters. The regularization methods used may vary depending on the specific deep learning pipeline and the problem being solved. Other regularization methods such as L1 regularization, early stopping, and data augmentation can also be used to prevent overfitting in deep learning models.
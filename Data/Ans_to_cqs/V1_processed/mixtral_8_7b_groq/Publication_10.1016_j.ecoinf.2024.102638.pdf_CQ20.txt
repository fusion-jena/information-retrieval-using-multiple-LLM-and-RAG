The context provided does not give specific information about the hardware resources used for training the deep learning model. However, it is mentioned that various machine learning models, including deep learning models, were used and their hyperparameters were optimized. Therefore, it can be inferred that the models were likely trained on a high-performance computing system, which could include GPUs or TPUs.

Deep learning models can benefit from the use of specialized hardware such as GPUs or TPUs to accelerate the training process. GPUs have a large number of cores that can perform matrix and vector operations in parallel, which are common in deep learning algorithms. TPUs, on the other hand, are custom-built hardware designed specifically for machine learning tasks and can provide even faster training times than GPUs.

In the context of the papers provided, Ryu et al. (2012) used a high-performance computing system to upscale evapotranspiration from instantaneous remote sensing measurements to 8-day mean daily-sums. Although they did not explicitly mention the use of GPUs or TPUs, it is likely that they used a high-performance computing system with specialized hardware for their computations. Similarly, Schwank et al. (2012) used a high-performance computing system to analyze L-band radiative properties of vine vegetation at the MELBEX III SMOS cal/val site.

In summary, while the context provided does not explicitly mention the use of GPUs or TPUs for training deep learning models, it can be inferred that a high-performance computing system was used, which could have included specialized hardware for accelerating machine learning tasks.
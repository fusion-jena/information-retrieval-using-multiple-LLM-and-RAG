The pieces of context provided do not specify the exact metrics used to evaluate the performance of the deep learning model. However, they do mention that the model's performance improves with high-quality annotations and a wider array of training examples.

In general, there are several metrics commonly used to evaluate the performance of deep learning models, especially in the context of classification tasks. These include:

1. Accuracy: the proportion of correct predictions out of all predictions made.
2. Precision: the proportion of true positive predictions (i.e., correct positive predictions) out of all positive predictions made.
3. Recall: the proportion of true positive predictions out of all actual positive instances in the data.
4. F1 score: the harmonic mean of precision and recall, providing a single metric that balances both.
5. Area under the ROC curve (AUC-ROC): a metric that evaluates the model's ability to distinguish between positive and negative classes by plotting the true positive rate against the false positive rate.

In the context of the provided information, it is likely that one or more of these metrics were used to evaluate the deep learning model's performance. Specifically, the improvement in performance with high-quality annotations and a wider array of training examples suggests that metrics such as accuracy, precision, recall, and F1 score would be relevant. Additionally, the semi-supervised learning approach, which combines both labeled and unlabeled data, may also benefit from evaluating the model's ability to distinguish between positive and negative classes using AUC-ROC.

However, without more specific information, it is impossible to definitively state which metrics were used to evaluate the deep learning model's performance.
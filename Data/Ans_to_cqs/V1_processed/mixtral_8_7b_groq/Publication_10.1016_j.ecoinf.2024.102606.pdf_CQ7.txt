The criteria used to split the data for deep learning model training involves the use of data augmentation and cross-validation techniques. Data augmentation is a technique used to artificially increase the size of the dataset by creating new samples from the existing ones. This is done to train the model more effectively by increasing its ability to generalize and its accuracy on the test dataset. The advantage of this technique when dealing with a low amount of data, as in this case, is the reduced risk of overfitting. Overfitting occurs when a model is too complex for the amount of data given and loses the ability to learn a solution that generalizes well because it fits the training data too much, resulting in poor performance on unseen data.

Cross-validation is used to ensure the statistical results and to prevent overfitting. In this technique, K crosses are created by randomly shuffling the dataset and dividing it into a training and an evaluation set K times. For each cross, a new model is trained, allowing for the training of K different models, each one on a different version of the dataset. Once all the models are trained, they are evaluated, and their performances are averaged to better understand the model's capability on this dataset.

In this specific case, each model used a cross-validation of 5 crosses. This means that the dataset was randomly shuffled and divided into a training and an evaluation set 5 times, and 5 different models were trained on each version of the dataset. The similarity of each model's evaluation results would indicate that the models are less likely to be overfitting on the data, therefore showing that they are more likely to be capable of generalizing this task on unseen data. The opposite would be if the models achieve highly different results, suggesting that they are very dependent on the data they were trained on and might generalize poorly to unseen data.

In summary, the criteria used to split the data for deep learning model training involves the use of data augmentation and cross-validation techniques. These techniques help to prevent overfitting, increase the model's ability to generalize, and improve its performance on unseen data.
Based on the provided context, there is no information about the specific hardware resources used for training a deep learning model. The context focuses on species distribution models and ecological informatics, discussing concepts such as environmental space, sampling effort, and methods for estimating species distributions.

However, it is important to note that the choice of hardware resources for training deep learning models often depends on the complexity of the model, the size of the dataset, and the desired training time. Generally, graphics processing units (GPUs) are used for deep learning training due to their ability to perform parallel computations, which significantly speeds up the process compared to using central processing units (CPUs) alone. Additionally, tensor processing units (TPUs) are specialized hardware developed by Google for machine learning tasks, particularly for tensor operations, which can further accelerate deep learning training.

In summary, while the provided context does not mention the specific hardware resources used for training deep learning models, GPUs and TPUs are commonly used for this purpose due to their computational advantages in handling large datasets and complex models.
The provided context discusses various deep learning models, but it does not give specific information about the hyperparameters used in those models, such as the learning rate or optimizer. However, it does mention several models and papers that discuss training and optimization methods.

The EfficientNetV2 series, for example, focuses on optimizing the accuracy, parameter count, and computational requirements of the model while reducing the number of parameters and computations compared to the EfficientNetV1 series. The training speed and accuracy of the EfficientNetV2 network are improved using an adjusted regularization method based on the training image size (Tan and Le, 2021).

The paper "An overview of gradient descent optimization algorithms" by Ruder (2016) provides a comprehensive overview of various optimization algorithms used in deep learning, including gradient descent, stochastic gradient descent, mini-batch gradient descent, and adaptive learning rate methods.

The MobileNetV2 model (Sandler et al., 2018) uses depthwise separable convolutions and linear bottlenecks to reduce the number of parameters and computations while maintaining accuracy. This model uses a variant of stochastic gradient descent called SGDM (Stochastic Gradient Descent with Momentum) for optimization.

The Bottleneck Transformers for Visual Recognition paper (Srinivas et al., 2021) introduces a new transformer architecture called BoTNet, which combines self-attention and convolution to improve model performance. However, the context does not provide information on the specific hyperparameters used in this model.

In summary, while the provided context discusses various deep learning models and optimization methods, it does not give specific information about the hyperparameters used in these models. The papers mentioned in the context, however, can be referred to for more information on optimization algorithms and hyperparameters used in deep learning.
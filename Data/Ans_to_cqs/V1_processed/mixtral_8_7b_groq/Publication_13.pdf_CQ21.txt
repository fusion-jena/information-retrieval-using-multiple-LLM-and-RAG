Based on the provided context, there is no explicit mention of specific postprocessing steps such as saliency maps or calculating additional metrics beyond what was mentioned in the classification error visualization as a scaled confusion matrix. However, we do know that the classification error was visualized as a confusion matrix, which is a common way to evaluate the performance of a classification model.

In the context, the classification error was calculated for a within-dataset analysis with 400 training images and for an analysis involving out-of-distribution samples. The model used for these analyses was an SVM trained using an adversarial training procedure that optimizes shared intermediate features to be invariant between two domains, with the goal of improving the model's ability to generalize across them.

The confusion matrix is a table layout that allows visualization of the performance of an algorithm in terms of its ability to correctly classify samples into different classes. It shows the number of false positives, false negatives, true positives, and true negatives. From the confusion matrix, we can calculate various metrics such as accuracy, precision, recall, and F1 score. However, the context does not explicitly mention any of these metrics being calculated.

Therefore, based on the provided context, we can infer that the postprocessing steps involved after the model training include visualizing the classification error as a confusion matrix. However, there is no mention of any additional postprocessing steps such as saliency maps or calculating other metrics beyond the confusion matrix.
In the study being referenced, the hyperparameters of the models were optimized using GridSearchCV. This is a method that involves a systematic and detailed search across a predefined grid of hyperparameter values. A unique parameter grid was defined for each model considered, focusing on specific model characteristics such as the number of estimators in ensemble methods or the depth of trees in decision tree-based algorithms. This process ensured that the eventual choice of hyperparameters was not arbitrary, but was backed by empirical evidence.

GridSearchCV is a type of exhaustive search that evaluates all possible combinations of the specified hyperparameter values. It is a reliable method for hyperparameter tuning as it provides a balanced trade-off between bias and variance, and guards against overfitting. This is achieved through the use of cross-validation, where the data is divided into a number of folds (in this case, ten), and the model is trained and tested on different subsets of the data.

The choice and tuning of parameters played a critical role in the performance of the models used in the study. Parameters such as 'max_depth' in tree-based models and 'learning_rate' in boosting models impacted the ability of the model to learn the data without overfitting. Similarly, the number of estimators or iterations was a crucial parameter for model optimization.

In summary, the hyperparameters of the models used in the study were optimized using GridSearchCV, a method that involves a systematic and detailed search across a predefined grid of hyperparameter values. This method was chosen for its ability to provide a reliable estimate of the model's performance, balancing the trade-off between bias and variance and guarding against overfitting.
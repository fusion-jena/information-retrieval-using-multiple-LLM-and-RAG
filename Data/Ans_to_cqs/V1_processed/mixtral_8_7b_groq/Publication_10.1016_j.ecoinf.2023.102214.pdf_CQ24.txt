The provided context does not include specific information about strategies used to handle randomness in the deep learning pipeline, such as the use of a random seed value. However, it does mention that the number of required epochs was tuned to prevent overfitting, which is a related concept. Overfitting occurs when a model focuses too much on fitting the training data, including random noise, rather than learning the broader patterns of interest. By tuning the number of epochs and inspecting the model outputs and error outcomes, the researchers could ensure that the model was learning the general patterns in the data rather than memorizing the random noise.

In addition, the context describes the use of a data subset called Min-3/Max-10 for training machine learning algorithms. While the specific details of this data subset are not provided, it is mentioned in the context of machine learning algorithms, suggesting that it may be a standard or commonly used data subset for training such algorithms. The use of a standardized data subset could help reduce randomness in the deep learning pipeline by providing a consistent set of data for training and testing models.

Finally, the context describes the use of a comprehensive graph matching software within the Wild Me software suite for cleaning data at a standard suitable for deep learning algorithms. The use of such software could help reduce randomness in the data by ensuring that the data is cleaned and standardized before being used in the deep learning pipeline.

In summary, while the provided context does not specifically mention strategies for handling randomness in the deep learning pipeline, it does suggest that the researchers took steps to prevent overfitting, used a standardized data subset, and employed data cleaning software to reduce randomness in the data.
The provided context discusses the use of a Convolutional Neural Network (CNN) model for Species Distribution Modeling (SDM), but it does not explicitly mention all the hyperparameters used in the deep learning model. However, it does provide information on some of the hyperparameters related to the convolution operation, such as the size of the kernels.

Based on general knowledge of deep learning models, other hyperparameters that could be used in a CNN for SDM include:

1. Learning rate: This is a hyperparameter that determines the step size at each iteration while moving toward a minimum of a loss function. It controls how much to change the model in response to the estimated error each time the model weights are updated.

2. Optimizer: This is a hyperparameter that determines the specific method used for optimization, such as Stochastic Gradient Descent (SGD), Adam, or RMSprop. Different optimizers use different strategies for updating the model weights based on the gradient of the loss function.

3. Number of convolutional layers: This is a hyperparameter that determines the number of times the convolution operation is applied to the input data. Each convolutional layer can extract different features from the input data.

4. Number of kernels: This is a hyperparameter that determines the number of kernels used in each convolutional layer. Each kernel can extract different features from the input data.

5. Size of the input data: This is a hyperparameter that determines the size of the input data. The size of the input data can affect the size of the kernels used in the convolutional layers.

6. Activation function: This is a hyperparameter that determines the specific function used to transform the weighted sum of the input values in each neuron. Common activation functions include the rectified linear unit (ReLU), sigmoid, and hyperbolic tangent (tanh) functions.

7. Regularization techniques: These are hyperparameters that determine the specific method used to prevent overfitting of the model to the training data. Common regularization techniques include L1 and L2 regularization.

8. Batch size: This is a hyperparameter that determines the number of training examples used in each iteration of the optimization algorithm.

9. Number of epochs: This is a hyperparameter that determines the number of times the entire training dataset is passed through the neural network.

Overall, while the provided context does not mention all the hyperparameters used in the deep learning model, it does provide information on some of the hyperparameters related to the convolution operation. Additionally, based on general knowledge of deep learning models, other hyperparameters that could be used in a CNN for SDM include the learning rate, optimizer, number of convolutional layers, number of kernels, size of the input data, activation function, regularization techniques, batch size, and number of epochs.
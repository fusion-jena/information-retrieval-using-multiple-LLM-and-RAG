Based on the provided context, there is no explicit information about how the hyperparameters of the model were optimized, such as through grid search, random search, or other optimization techniques. However, the context does provide some details about the model architecture and training process that can give us some insights.

The model used in the study is a deep learning model, and the authors mentioned that it is feasible to use a small amount of data to train deep learning models. The model consists of convolutional layers, pooling layers, and fully connected layers. The number of units in the third fully connected layer (fc3) was changed according to the number of classes of training data. The authors also set the batch size to 69, momentum to 0.9, and applied L2 weight decay with a penalty multiplier set to 5 Ã— 10-4, dropout ratio set to 0.5, and CPU mode.

However, the context does not provide information on how these hyperparameters were chosen or optimized. Therefore, we cannot definitively answer how the hyperparameters of the model were optimized based on the given context. It is possible that the authors used a specific optimization technique, but they did not include it in the provided context.
The process followed to deploy the trained deep learning models, namely Inception-V3, ResNet-50, and ViT, is not explicitly mentioned in the provided context. However, we can infer some steps that might have been taken based on common deep learning deployment practices.

Firstly, the models would have been saved after training, a process often referred to as model serialization. This allows the trained models to be stored for later use, either for deployment or further experimentation. In the case of the Inception-V3 model, which was implemented using Keras, the `model.save()` function could have been used to save the model as a file in the HDF5 format. For the ResNet-50 and ViT models, which were implemented using Hugging Face's Transformers library, the `save_model()` function could have been used to save the models in a format compatible with the library.

Secondly, a platform for deployment would have been selected. This could be a local machine, a cloud-based platform, or an edge device, depending on the specific requirements of the application. For cloud-based deployment, popular platforms include AWS SageMaker, Google Cloud AI Platform, and Microsoft Azure Machine Learning. For local deployment, the models could be run using libraries such as Flask or Django to create a web application.

Thirdly, the deployed model would need to be served in a way that allows it to receive input data and return predictions. This could involve creating a REST API, using a model server such as TensorFlow Serving or TorchServe, or using a platform-specific solution. In the case of the Transformers library, the `TransformersModel` class can be used to create a model that can be served using a variety of methods, including as a REST API or a gRPC service.

Finally, the deployed model would need to be monitored and maintained to ensure that it continues to perform well. This could involve tracking metrics such as prediction accuracy and latency, and setting up alerts to notify when the model's performance drops below a certain threshold.

In conclusion, while the specific steps taken to deploy the models in this case are not provided, we can infer that they likely involved model serialization, platform selection, model serving, and monitoring.
The provided context does not contain information about the regularization methods used to prevent overfitting in deep learning pipelines. However, I can provide a general answer to the query.

Regularization methods are essential in deep learning to prevent overfitting, which occurs when a model learns the training data too well, including its noise, and performs poorly on unseen data. Two common regularization techniques used in deep learning are dropout and L2 regularization.

Dropout is a regularization technique introduced by Srivastava et al. (2014) to prevent overfitting in neural networks. During training, dropout randomly sets a fraction of neurons in a layer to zero, effectively making the network thinner and breaking co-adaptations between neurons. This process encourages the network to learn more robust features and improves its generalization capability.

L2 regularization, also known as weight decay, is another regularization technique that reduces overfitting by adding a penalty term to the loss function. This penalty term is the sum of the squares of all model weights, multiplied by a regularization parameter (lambda). By increasing the loss function, L2 regularization discourages large weight values, promoting smaller and more evenly distributed weights. This, in turn, helps to reduce overfitting and improve the model's generalization performance.

In summary, dropout and L2 regularization are two common regularization methods used in deep learning pipelines to prevent overfitting. Dropout randomly sets a fraction of neurons to zero during training, while L2 regularization adds a penalty term to the loss function, discouraging large weight values. These techniques help improve the model's generalization capability and prevent overfitting.

References:
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., 2014. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15 (1), 1929â€“1958.
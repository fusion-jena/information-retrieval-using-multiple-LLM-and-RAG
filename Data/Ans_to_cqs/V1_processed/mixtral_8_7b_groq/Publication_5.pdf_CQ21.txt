Based on the provided context, there are no specific details mentioned regarding the postprocessing steps involved immediately after the model training process. However, we can find some information about the evaluation metrics that were calculated after the model training and testing phase.

The comparison of statistical accuracy metrics between the random forest and deep learning models is presented in Table 1. The metrics include Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Bias, and Correlation. These metrics help in evaluating the performance of both models.

Although not specified in the context, some common postprocessing steps after model training could be:

1. Saliency maps: These are useful for visualizing the importance of input features concerning the predicted output. Saliency maps can help in understanding which features the model is focusing on while making predictions.

2. Metrics calculation: As mentioned in the context, MAE, RMSE, Bias, and Correlation were calculated. Other metrics like precision, recall, F1-score, and accuracy might also be calculated depending on the problem statement.

3. Confusion matrix: A confusion matrix is a table that is often used to describe the performance of a classification model. It can help in understanding the types of errors the model is making, such as false positives and false negatives.

4. Hyperparameter tuning: Even though it is mentioned that hyperparameters were tuned before training, further tuning might be performed based on the evaluation metrics calculated after the initial training.

5. Model interpretation: Techniques like SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) might be used to interpret and explain the model predictions.

In summary, although the context does not provide specific postprocessing steps after model training, common steps could include saliency maps, metrics calculation, confusion matrix, hyperparameter tuning, and model interpretation.
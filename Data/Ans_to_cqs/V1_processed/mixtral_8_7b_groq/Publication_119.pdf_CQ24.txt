In the context provided, there is no explicit mention of strategies employed to handle randomness in the deep learning pipeline, such as the use of a random seed value. However, it does discuss the use of randomness in the Random Forest (RF) model, where repetition and replacement are used to create a 'bagged' dataset for each decision tree. The out-of-bag samples are then used for validation of the RF model.

In the Deep Neural Network (DNN) model, the use of batch normalization and dropout layers are mentioned as a way to reduce overfitting of the model on the training data and increase generalization on testing data. These techniques can be seen as a way to handle randomness by introducing a form of stochasticity during training, which can help the model to be more robust and less prone to overfitting.

Additionally, the DNN model's hyperparameters, such as the learning rate, number of epochs, and batch size, were tuned to optimize the testing accuracy and kappa. This process of hyperparameter tuning can also be seen as a way to handle randomness, as it can help to find the best set of parameters that generalize well to new, unseen data.

It is worth noting that, in general, deep learning models are sensitive to the initial random weights and biases, and techniques such as using a fixed random seed value or running multiple training sessions with different random seeds are commonly used to handle this randomness. However, the context provided does not mention any specific strategy for handling randomness in the deep learning pipeline.
The strategy implemented to monitor model performance during training is not explicitly mentioned in the provided context. However, it is stated that the majority of the articles using GPU computing are in the field of machine learning, where training phase can be parallelized, and accessible software tools like TensorFlow, Keras, and PyTorch are commonly used. These tools typically provide means to monitor model performance during training, such as displaying training and validation loss and accuracy over epochs.

In the context of particle filtering, the focus is on optimizing the computation time spent on different tasks within the bootstrap filter. The proportion of compute time spent on each task can vary depending on the state-dimensionality of the application and the amount of interaction between particles. Implementations of the algorithm in different programming languages, especially compiled ones such as C or Fortran, will vary slightly in the proportion of compute time spent in each task. However, this does not directly address the monitoring of model performance during training.

In summary, while the specific strategy for monitoring model performance during training is not provided in the context, it can be inferred that in the field of machine learning, where GPU computing is commonly used, accessible software tools typically offer means to monitor model performance during training.
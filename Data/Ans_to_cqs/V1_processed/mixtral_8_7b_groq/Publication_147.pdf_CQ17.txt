The provided context discusses a study that uses deep learning (convolutional neural networks or CNNs) for large-scale spatial wetland classification, but it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it can be inferred that the researchers used some regularization techniques, as they mention using the Keras Nadam optimizer, which is an optimizer that combines the Nesterov momentum and the Adam optimizer, and it can help reduce overfitting.

In general, there are several regularization methods commonly used in deep learning pipelines to prevent overfitting, such as:

1. L1 and L2 regularization: These are methods that add a penalty term to the loss function, which discourages large weights and promotes smaller, sparse models. L1 regularization adds an absolute value of the weight as a penalty, while L2 regularization adds the square of the weight.
2. Dropout: This is a technique that randomly sets a fraction of the input units to zero during training, which helps prevent over-reliance on any single neuron and promotes the development of redundant representations.
3. Early stopping: This is a method that stops training when the performance on a validation set stops improving, which helps prevent overfitting by halting training before the model starts to memorize the training data.
4. Data augmentation: This is a technique that artificially increases the size of the training set by applying random transformations to the existing data, such as rotation, scaling, and flipping. This helps prevent overfitting by providing the model with more diverse training examples.

In the context of the study, it is likely that the researchers used a combination of these regularization methods to prevent overfitting and improve the generalization of the CNN model. However, the specific regularization techniques used are not mentioned in the provided text.
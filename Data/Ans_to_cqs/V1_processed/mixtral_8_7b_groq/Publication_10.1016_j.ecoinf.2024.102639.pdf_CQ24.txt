In the context provided, there is no direct information about strategies employed to handle randomness in deep learning pipelines. However, it is widely known that randomness is an essential component of many deep learning techniques, and various strategies are used to control and leverage it.

One common way randomness is introduced in deep learning is through the use of random initial weights in neural networks. This helps to break symmetry and ensures that different neurons learn different features of the input data during training. The randomness in initial weights enables the network to explore various solutions before converging to the best one.

Another instance of randomness in deep learning is the use of dropout, a regularization technique introduced by Srivastava et al. (2014). Dropout randomly drops out, or deactivates, a proportion of neurons in a layer during training. This prevents overfitting by promoting independence among the neurons and reducing co-adaptation.

Randomness is also used in the selection of mini-batches during stochastic gradient descent (SGD) optimization. Instead of processing the entire dataset at once, SGD processes a small subset (mini-batch) of data randomly selected from the training set. This approach introduces a source of randomness that can help the optimizer escape local minima and find better solutions.

In the context of reinforcement learning, randomness can be used in the exploration-exploitation trade-off. For example, the Îµ-greedy algorithm uses randomness to decide between exploiting the current best-known action and exploring alternative actions. This helps the agent balance between taking the best-known action and exploring new possibilities, which is crucial for learning in uncertain environments.

Finally, randomness can be introduced through the use of random seeds in deep learning frameworks. A random seed initializes the random number generator, which determines the sequence of random numbers used during various processes, such as weight initialization, dropout, and mini-batch selection. By setting a fixed random seed, researchers can ensure reproducibility in their experiments.

In summary, while the provided context does not discuss specific strategies for handling randomness in deep learning pipelines, it is a well-known and essential aspect of deep learning techniques. Randomness is introduced through initial weights, dropout, mini-batch selection, exploration-exploitation trade-offs, and random seeds, and it plays a crucial role in preventing overfitting, escaping local minima, and promoting reproducibility.

References:
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1), 1929-1958.
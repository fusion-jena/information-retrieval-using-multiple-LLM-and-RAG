The provided context does not give specific information about the learning rate or optimizer used in the deep learning models. However, it does mention that the Adam optimizer (Kingma & Ba, 2014) was used for training the models on the Peruvian Amazon Forestry Dataset.

The Adam optimizer is a popular choice for deep learning models due to its efficiency and adaptability. It calculates independent adaptive learning rates for different parameters, allowing for faster convergence and better performance.

However, the learning rate was not explicitly mentioned. The learning rate is a crucial hyperparameter that determines the size of the steps taken during gradient descent. A learning rate that is too high may cause the model to converge too quickly to a suboptimal solution, while a learning rate that is too low may result in slow convergence.

In addition, other hyperparameters such as the batch size, number of epochs, and regularization techniques (e.g., dropout, weight decay) may also have been used during training. However, these hyperparameters are not specified in the given context.

It is worth noting that the performance of deep learning models can be highly sensitive to the choice of hyperparameters. Therefore, it is common practice to perform hyperparameter tuning to find the optimal set of hyperparameters for a given model and dataset.

In summary, while the provided context specifies that the Adam optimizer was used for training the deep learning models, it does not provide information on the learning rate or other hyperparameters used.
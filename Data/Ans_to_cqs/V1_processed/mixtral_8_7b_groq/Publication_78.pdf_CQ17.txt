The provided context discusses various machine learning and deep learning models used for classification tasks, but it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, I can provide some information on common regularization techniques based on general knowledge.

Regularization methods are essential in machine learning and deep learning pipelines to prevent overfitting by adding a penalty term to the loss function. This penalty term discourages the learning algorithm from assigning too much importance to any single feature, thus promoting the generalization of the model.

Two common regularization techniques used in deep learning are L2 regularization and dropout.

1. L2 regularization: This method adds a penalty term proportional to the square of the magnitude of the model's weights to the loss function. By doing so, it encourages the learning algorithm to keep the weights small, reducing the impact of any single weight on the output. L2 regularization is also known as weight decay or ridge regression.

2. Dropout: Dropout is a regularization technique specifically designed for neural networks. During training, dropout randomly sets a fraction of the neurons in a layer to zero, effectively preventing those neurons from contributing to both the forward pass and the backward pass. This process helps to break up co-adaptations between neurons, promoting the learning of more robust features.

These techniques can be applied together or separately, depending on the specific problem and the complexity of the model. Other regularization techniques include L1 regularization, early stopping, and data augmentation.

While the provided context does not discuss these regularization techniques, it is essential to consider them when implementing deep learning pipelines to prevent overfitting and improve model generalization.
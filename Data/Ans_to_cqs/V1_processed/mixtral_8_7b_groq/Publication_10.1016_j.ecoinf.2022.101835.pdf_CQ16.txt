Based on the provided context, there is no explicit information given about the criteria used to determine when training is complete for the machine learning models. However, it can be inferred that the training of the models is likely to be determined by the optimization process using Optuna, which seeks to minimize the loss function (mean squared error) for each model.

For the Random Forest (RF) and LightGBM models, Optuna is used to optimize the structure and hyperparameters of the models, and the performance of the algorithms is evaluated by comparing the loss function of the optimized models. Similarly, for the Artificial Neural Network (ANN) models, the structure and hyperparameters are optimized using Optuna, and the performance of the models is evaluated based on the loss function.

In the case of the RF models, the number of trees (n\_estimators), the maximum depth of the tree (max\_depth), and the minimum number of data in one leaf (min\_child\_samples) are optimized. For the LightGBM models, the number of leaves (num\_leaves), L1 and L2 regularization (lambda\_l1 and lambda\_l2), and a subset of features on each iteration (feature\_fraction) are optimized. For the ANN models, the number of layers (num\_layers), the number of nodes in the input and middle layers (units), the activation function (activation), and the optimizer (optimizer) are optimized.

It can be inferred that the training of the models is likely to be determined by the optimization process using Optuna, which seeks to minimize the loss function for each model. The optimization process may involve monitoring the validation loss and stopping the training when the validation loss plateaus or stops decreasing significantly. However, the specific criteria used to determine when training is complete are not provided in the context.
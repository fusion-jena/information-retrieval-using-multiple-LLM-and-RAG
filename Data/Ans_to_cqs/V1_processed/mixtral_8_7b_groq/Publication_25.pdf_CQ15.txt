The provided context does not give specific information about the optimization techniques used in the deep learning pipeline. However, it does mention the use of deep learning in a semi-supervised framework, which typically involves the use of optimization algorithms to train models on labeled data.

In semi-supervised learning, a combination of labeled and unlabeled data is used to train a model. The labeled data provides direct supervision for the model, while the unlabeled data is used to generate additional training examples or to provide indirect supervision through techniques such as self-supervised learning. In the context of tree detection in remote sensing, the high variation in tree crown appearance can make it difficult to obtain sufficient labeled data for training deep learning models. Self-supervised learning, which only requires unlabeled data, can be used to address this issue by generating additional training examples.

Once labeled or generated training data is available, optimization algorithms are used to train the deep learning model. Some common optimization algorithms used in deep learning include stochastic gradient descent (SGD), Adam, and RMSprop. These algorithms are used to minimize the loss function of the model, which measures the difference between the predicted output and the true output for a given input. By iteratively adjusting the model parameters in the direction that reduces the loss function, the optimization algorithm gradually improves the performance of the model.

In the context of the paper by Wu et al. (2018), pseudo labels are used to train a deep learning model for hyperspectral image classification in a semi-supervised framework. Pseudo labels are generated for the unlabeled data using a pre-trained model, and then the model is trained on the combination of labeled and pseudo-labeled data using SGD. The authors found that this approach improved the performance of the model compared to using only labeled data.

Therefore, while the specific optimization techniques used in the deep learning pipeline are not mentioned in the provided context, it is likely that optimization algorithms such as SGD, Adam, or RMSprop were used to train the deep learning models.
The provided context discusses various techniques to tackle the problem of lack of data in deep learning, including data augmentation and few-shot learning algorithms. However, it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout and L2 regularization.

Dropout is a regularization technique used during training to prevent overfitting by randomly dropping out, or "turning off," neurons in a layer of the neural network. This helps to ensure that the network does not rely too heavily on any single neuron, and instead learns to distribute the information processing across multiple neurons.

L2 regularization, also known as weight decay, is another regularization technique used to prevent overfitting by adding a penalty term to the loss function, which is proportional to the square of the magnitude of the model's weights. This encourages the model to learn smaller weights, which in turn helps to prevent overfitting by reducing the complexity of the model.

In summary, the provided context does not mention the regularization methods used to prevent overfitting in the deep learning pipeline, but dropout and L2 regularization are two commonly used techniques for this purpose.
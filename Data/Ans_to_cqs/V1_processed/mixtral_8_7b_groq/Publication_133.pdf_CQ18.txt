During the training of the YOLOv5 model, a strategy was implemented to monitor the model's performance and prevent overfitting. This strategy involves the use of a validation set, which consists of images that are separate from the training set. While the model is being trained on the training set, its predictions on the validation set are monitored, and the validation loss is calculated. The validation loss indicates how well the model is performing on new, unseen data. If the validation loss starts to increase, it suggests that the model is beginning to overfit to the training data, and the training process should be stopped. In this study, the validation loss stabilized after 30 epochs, indicating that the model was not overfitting and had learned to generalize well to new data. Therefore, the deep learning model generated by 30 training epochs was used for tests. Additionally, the model's average precision (AP) was monitored over the three insect classes, providing a different perspective on the model's performance. The plot of the mean AP over the training epochs showed that the model's annotation accuracy improved as the number of epochs increased. Overall, the strategy implemented to monitor the model's performance during training involved using a validation set to calculate the validation loss and monitoring the model's average precision, which helped to prevent overfitting and ensure that the model was learning to generalize well to new data.
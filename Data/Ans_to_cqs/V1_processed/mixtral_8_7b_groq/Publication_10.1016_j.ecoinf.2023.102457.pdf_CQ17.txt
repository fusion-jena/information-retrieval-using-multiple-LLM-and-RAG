The provided context discusses a study that uses deep learning methods for individual acoustic discrimination of gibbon vocalizations. Although the specific regularization techniques used in this study are not mentioned, it is possible to infer some common regularization methods that could have been employed in the deep learning pipeline to prevent overfitting.

1. L2 Regularization: L2 regularization, also known as weight decay, adds a penalty term to the loss function, which is the sum of squares of all the model's weights. This penalty term discourages large weight values, promoting smaller and more uniform weights across the model. As a result, L2 regularization reduces the complexity of the model and helps prevent overfitting.

2. Dropout: Dropout is a regularization technique that randomly sets a fraction of the model's output values to zero during training. This process effectively creates an ensemble of models, where each model is a subnetwork of the original deep learning architecture. Dropout helps prevent overfitting by reducing the co-adaptation of neurons and promoting the learning of more robust features.

3. Data Augmentation: Data augmentation is a technique that generates new training samples by applying transformations to the existing dataset. For audio data, this could include adding noise, changing pitch or speed, or cropping/padding the input signals. Data augmentation increases the size of the training dataset and helps the model generalize better, reducing the chances of overfitting.

4. Early Stopping: Early stopping is a method that halts the training process before the model starts overfitting. This is achieved by monitoring the model's performance on a validation dataset during training. Once the validation loss starts increasing, even if the training loss continues to decrease, the training process is stopped, preventing the model from overfitting.

5. Batch Normalization: Batch normalization is a technique that normalizes the inputs of each layer in the deep learning model. By doing so, batch normalization helps maintain a consistent scale of the inputs throughout the network, promoting faster convergence and reducing the chances of overfitting.

While the provided context does not explicitly mention the regularization techniques used in the study, it is likely that some combination of the methods mentioned above was employed to prevent overfitting in the deep learning pipeline.
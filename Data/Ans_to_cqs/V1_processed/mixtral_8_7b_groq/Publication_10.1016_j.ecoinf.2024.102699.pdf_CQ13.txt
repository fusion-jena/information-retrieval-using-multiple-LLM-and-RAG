The hyperparameters used in the deep learning model, specifically the Vision Transformer (ViT-B/16), include the Adam optimizer with a binary cross entropy based focal loss and scaled focal loss. The initial learning rate is 0.0001, which decays by a factor of 0.001â‹…learning rate. The batch size is 32. The training setup includes a concatenating cosine-annealing linear scheduler. For validation, a 5-fold cross validation is used. Given the class imbalance, a data set sampler is deployed by upsampling and duplicating randomly selected samples and augmenting them. The models were trained for 50 epochs without mixing up augmentations and for 100 epochs with mixing up augmentations. Early stopping was introduced to prevent overfitting when no further improvement was observed.

The table provided in the context does not include the training time per epoch for the ViT-B/16 model with the different augmentation methods in the frequency domain and spectrogram categories. However, it does mention that Gaussian noise, pink noise, primary and secondary background noise soundscapes, primary background noise from the bird audio detection dataset (BAD), background noise, mixed up random bird species, horizontal and vertical roll, pitch shift, time mask, frequency mask, gain, loudness normalization, horizontal and vertical flip, time stretch, and tanh-based distortion were investigated as augmentation techniques.

It is also mentioned that the evaluated augmentation techniques consistently exhibited the same qualitative effects across all three of the deep learning models. Therefore, it can be inferred that the aforementioned hyperparameters and augmentation techniques were also used for the other two models, DenseNet-161 and ResNet-50. However, the training times per epoch for these models with the different augmentation methods are provided in the table.
The provided context does not give specific details about the hyperparameters used in the deep learning models, apart from mentioning that the same hyperparameters were used for training the model on different datasets. Hyperparameters are crucial in deep learning models as they define the architecture and training process of the model. They include the learning rate, optimizer, number of layers, size and number of convolutions, connections between layers, neuron activation, and back-propagation computation.

In the context, the authors trained a deep learning model using a classic architecture with their biggest dataset AT0 as a baseline for the deep learning accuracy. They then trained the same architecture with the same hyperparameters on a much more limited dataset AT1. Additionally, they trained the same architecture on limited datasets obtained by subsampling T0 to 250 and 500 images per class, corresponding to 2500 and 5000 thumbnails in AT0.

The authors also discussed optimization-based methods for few-shot learning, which are designed to adapt quickly to new tasks. These methods propose pre-training a model with existing databases on different tasks so that it can adapt easily to a new one. However, the context does not provide specific details about the hyperparameters used in these optimization-based methods.

Therefore, while the context provides information about the datasets and training process used in the deep learning models, it does not give specific details about the hyperparameters used.
The provided context discusses the use of data augmentation and balanced sampling to address data bias during the preprocessing of a deep learning pipeline for a Convolutional Neural Network (CNN) used in audio sample classification.

To address data bias, the authors first divided the dataset into training and testing samples, using 80% of the samples for training and 20% for testing. This strategy is commonly used to enhance the generalization of a machine learning model and prevent overfitting.

In addition, a balanced sampling approach was carried out for the test set. For each type of anuran call, one audio sample (with its corresponding data augmentation samples) out of five was selected. This approach helps ensure that the test set is representative of the different classes and reduces the risk of bias in the test results.

Furthermore, data augmentation was performed to generate a more robust training set. For each of the 865 audio samples in the database, 10 additional audios were generated through transformations such as time-stretching, pitch-shifting, and adding background noise. This approach increases the size and diversity of the training set, which can help improve the model's ability to generalize and reduce bias.

However, the context does not explicitly mention other techniques such as stratified splitting, oversampling, or undersampling. Stratified splitting involves dividing the dataset into strata based on class labels and then randomly sampling from each stratum to ensure that each stratum is represented in the training and testing sets. Oversampling involves increasing the number of samples in underrepresented classes, while undersampling involves reducing the number of samples in overrepresented classes. These techniques can also help reduce data bias during preprocessing.

In summary, the provided context discusses the use of balanced sampling and data augmentation to address data bias during the preprocessing of a deep learning pipeline for CNN audio sample classification. Other techniques such as stratified splitting, oversampling, and undersampling are not mentioned in the context.
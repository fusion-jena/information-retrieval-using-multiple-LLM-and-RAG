In the provided context, there is no explicit mention of the regularization methods used to prevent overfitting in the deep learning pipeline. However, based on common practices in deep learning, I can provide some possible regularization techniques that could have been employed.

Regularization is a crucial approach to prevent overfitting in deep learning models. It introduces a penalty on the model's complexity to reduce the likelihood of memorizing the training data and improve its generalization ability. Some common regularization techniques are:

1. L1 and L2 Regularization: These are weight decay techniques that add a penalty term to the loss function, proportional to the absolute value (L1) or square of the magnitude (L2) of the model's weights. L2 regularization, also known as weight decay, is more commonly used and encourages smaller weights, leading to simpler models.

2. Dropout: Dropout is a regularization technique that randomly sets a fraction of the neurons in a layer to zero during training. This prevents over-reliance on any single neuron and promotes the development of redundant representations across the network.

3. Early Stopping: Early stopping is a form of regularization where training is halted as soon as the performance on a validation set stops improving. This prevents the model from further adjusting its parameters based on the training data, reducing overfitting.

4. Data Augmentation: Data augmentation is a technique that generates new training samples by applying transformations (e.g., rotation, scaling, flipping) to the existing data. This artificially increases the size of the training set and helps the model generalize better.

Considering the context, data augmentation could have been employed by creating new spectrograms with transformed audio samples. Additionally, L2 regularization or dropout could have been applied to the convolutional and fully-connected layers to prevent overfitting. However, without explicit information, these are only possible regularization techniques that might have been used in the deep learning pipeline.
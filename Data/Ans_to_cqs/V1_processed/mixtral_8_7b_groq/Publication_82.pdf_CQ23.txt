The generalizability of the deep learning model was addressed through several measures. First, the model was trained and tested on a diverse set of experimental configurations. There was a variation in the choice of deep learning architecture (AlexNet and GoogLeNet), training mechanism (Transfer Learning and Training from Scratch), dataset type (Color, Gray scale, and Leaf Segmented), and training-testing set distribution (Train: 80%, Test: 20%, Train: 60%, Test: 40%, Train: 50%, Test: 50%, Train: 40%, Test: 60%, and Train: 20%, Test: 80%). This ensured that the model was exposed to a wide range of variations, thereby improving its ability to generalize.

Second, to prevent overfitting and to get a sense of how the approaches would perform on new, unseen data, the model was run across a range of train-test set splits. These included 80-20, 60-40, 50-50, 40-60, and 20-80 splits. This provided a robustness check and ensured that the model's performance was not specific to a particular train-test split.

Third, the authors noted that the PlantVillage dataset had multiple images of the same leaf taken from different orientations. They had mappings for such cases for 41,112 images out of the 54,306 images. This allowed for a more representative and diverse training set, further enhancing the model's generalizability.

However, when tested on a set of images taken under conditions different from the training images, the model's accuracy was reduced to just above 31%. While this was higher than the accuracy based on random selection of 38 classes (2.6%), it indicated the need for a more diverse set of training data to improve the accuracy. The authors noted that more (and more variable) data alone would be sufficient to substantially increase the accuracy, and corresponding data collection efforts were underway.

In conclusion, the generalizability of the deep learning model was ensured through a diverse set of experimental configurations, a range of train-test set splits, and the use of a representative and variable training set. Future work includes the collection of a more diverse set of training data to further improve the model's accuracy.
The provided context does not give specific criteria used to determine when training is complete for the models mentioned. However, it is common in machine learning and deep learning practices to use certain conditions to decide when to stop training. One such condition is when the validation loss plateaus, which means the loss value does not decrease significantly even after several training iterations. This indicates that the model has learned the underlying patterns in the training data and further training might lead to overfitting.

In the context, various data augmentation techniques were applied to increase the size and variability of the dataset, which helped to render the model more generalized and avoid overfitting. These techniques include random cropping, horizontal/vertical flipping, image rotation, image translation, zooming, brightness adjustment, contrast adjustment, hue and saturation adjustment, Gaussian noise, and random erasing.

Moreover, the mAP (mean Average Precision) metric was used to evaluate the models quantitatively. mAP is a common evaluation metric in object detection tasks and is calculated by finding the AP (Average Precision) for each class and then dividing the sum by the total number of classes. The AP metric is dependent on the Intersection Over Union (IoU) threshold, which quantifies the degree of overlap between ground truth and prediction boxes.

Given the context, it can be inferred that the training process might involve monitoring the mAP or other relevant metrics on the validation set and stopping the training when the performance plateaus or stops improving significantly. However, without explicit information, it is not possible to provide a definitive answer.
The context does not provide specific information on how the hyperparameters of the model are optimized, such as whether grid search, random search, or another method was used. However, it does provide details about some of the hyperparameters that were used in the models.

The hyperparameters that are specified in the context include:

* The resolution of the input data is 224x224 with RGB format, and the inputs are batch normalized before training.
* The batch size is set to 30 when training, but images are tested one by one.
* An optimized rectified linear activation, Leaky ReLU, is introduced into the models after the convolution or concatenate layer.
* In MobileNet, the width multiplier is 1.2, and the resolution multiplier is 1.
* The learning rate and dropout rate adopt the same value of 0.001, and the bias value is 0.
* For neural networks with lower depth, the higher the training epochs, the higher the probability of overfitting. Therefore, models with less than 100 layers undergo 60 epochs of training, while other models undergo 80 epochs.
* A fully connected layer with softmax activation has 28 nodes, which is regarded as a classifier while training but removed during inference.

The context also states that the operation system is CentOS Linux release 7.7.1908 (Core) with 32 CPUs, each of which has 64 processors. The development environment is built up using Python (version 3.6.2) packaged by Conda-forge, which is compiled with GCC (version 4.8.2, 20140120). The tool packages used in the virtual environment are TensorFlow (version 1.12.0) and Numpy (version 1.19.2).

Therefore, while the specific method of hyperparameter optimization is not provided, the context does offer insight into the chosen hyperparameters and the computational resources used for the study.
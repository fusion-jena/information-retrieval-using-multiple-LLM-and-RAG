While the text provided does not give a detailed description of all preprocessing steps applied to the data, it does mention some preprocessing techniques that were used. 

First, data augmentation was applied to all extracted patches. Specifically, this involved a 90-degree rotation and vertical and horizontal flips. Data augmentation is a common preprocessing technique used in deep learning to increase the amount of training data, reduce overfitting, and improve model performance.

Second, the data was divided into folds for cross-validation. This is another common preprocessing step used in deep learning to ensure that the model generalizes well to unseen data.

Third, each network was trained and executed three times, each time with a different (random) initialization of the trainable parameters. This is a type of data preprocessing known as model initialization, where the model's weights are initialized randomly to ensure that the model explores different areas of the solution space during training.

Finally, while not explicitly stated, it is likely that some form of data normalization or scaling was applied to the data. This is a common preprocessing step in deep learning to ensure that the data is on a similar scale, which can help improve model convergence and performance. However, the text does not provide specific details about any normalization or scaling techniques that were used.

In summary, the preprocessing steps mentioned in the text include data augmentation, cross-validation, model initialization, and possibly data normalization or scaling. However, the text does not provide a comprehensive list of all preprocessing steps that were applied to the data.
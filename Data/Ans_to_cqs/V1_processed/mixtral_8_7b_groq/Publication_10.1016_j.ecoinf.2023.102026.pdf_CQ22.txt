The provided context discusses the methods used to evaluate the performance of Generalized Linear Models (GLMs) and Random Forest models for species distribution modeling (SDM), but it does not mention the use of deep learning models. Therefore, I cannot provide specific metrics used to evaluate the performance of a deep learning model for SDM.

However, in the context of machine learning and SDM, some common metrics used to evaluate the performance of models include accuracy, precision, recall, F1 score, Area Under the Receiver Operating Characteristic Curve (AUC-ROC), and Cohen's Kappa.

Accuracy is the proportion of true predictions (both true positives and true negatives) out of the total number of predictions. Precision is the proportion of true positives among all positive predictions, while recall is the proportion of true positives among all actual positives. F1 score is the harmonic mean of precision and recall. AUC-ROC measures the model's ability to distinguish between positive and negative classes across all possible classification thresholds. Cohen's Kappa is a statistical measure of the agreement between predicted and actual values, taking into account the possibility of the agreement occurring by chance.

These metrics can be calculated based on the confusion matrix, which is a table summarizing the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).

An example of how these metrics can be calculated for a binary classification problem is as follows:

* Accuracy = (TP + TN) / (TP + TN + FP + FN)
* Precision = TP / (TP + FP)
* Recall = TP / (TP + FN)
* F1 score = 2 \* Precision \* Recall / (Precision + Recall)
* AUC-ROC = the area under the ROC curve
* Cohen's Kappa = (Po - Pe) / (1 - Pe), where Po is the observed agreement and Pe is the expected agreement by chance.

In summary, while the provided context does not mention the use of deep learning models for SDM, common metrics used to evaluate the performance of machine learning models for SDM include accuracy, precision, recall, F1 score, AUC-ROC, and Cohen's Kappa. These metrics can be calculated based on the confusion matrix, which summarizes the number of true positives, true negatives, false positives, and false negatives.
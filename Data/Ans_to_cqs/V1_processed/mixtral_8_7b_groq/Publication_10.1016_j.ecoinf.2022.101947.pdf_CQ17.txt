The provided context does not directly discuss the regularization methods used to prevent overfitting in deep learning pipelines. However, it does contain information about how a model's performance was evaluated and validated, which can be related to the concept of overfitting.

In the context, the researchers used holdback trials to establish the minimum number of trials required for testing. This method involves setting aside a portion of the data for testing and training the model on the remaining data. By comparing the predicted mean for each test:training set size to the actual test size, the researchers could assess the accuracy of the model and identify the point at which the variation in sequential trials increased, potentially reducing algorithm accuracy. This process is similar to the concept of regularization, where techniques like dropout and L2 regularization are used to prevent overfitting by adding a penalty to the loss function or randomly dropping out neurons during training.

In summary, while the provided context does not explicitly mention the regularization methods used to prevent overfitting in deep learning pipelines, it does discuss the importance of validating a model's performance and identifying the point at which overfitting may occur. Regularization techniques like dropout and L2 regularization are commonly used in deep learning pipelines to prevent overfitting by adding a penalty to the loss function or randomly dropping out neurons during training.
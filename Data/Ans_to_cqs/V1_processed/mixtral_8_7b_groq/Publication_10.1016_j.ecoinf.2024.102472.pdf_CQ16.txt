The provided context discusses two machine learning models, gradient boosting and a Multi-Layer Perceptron Neural Network (MLPNN), used for estimating carbon stocks. However, the context does not explicitly mention the criteria used to determine when training is complete for either model.

For the MLPNN, the training process involves assigning initial arbitrary connection weights, forward-feeding inputs from the input to the hidden layer, processing sums through a transfer function, and iteratively adjusting connection weights until reaching a target minimal error. The context mentions determining optimal learning rate, momentum, and training iterations (500) for the MLPNN. Still, it does not specify how the target minimal error is determined or when to stop the training process.

As for the gradient boosting model, the context only mentions it as a repetition process of increasing or decreasing weights in a training dataset. There is no information on how to determine when the training process should be stopped.

In general, training for machine learning models can be stopped based on several criteria, such as:

1. Validation Loss Plateau: Training can be stopped when the validation loss stops decreasing, indicating that the model has likely learned the underlying patterns in the training data.

2. Early Stopping: This technique involves stopping the training process if the model's performance on a validation set does not improve for a certain number of epochs.

3. Monitoring Generalization Error: Training can be stopped when the gap between the training and validation error is large and does not decrease, indicating overfitting.

4. Patience: Training can be stopped based on a predefined patience value, which specifies the maximum number of epochs to wait before stopping the training process if the validation loss does not improve.

Since the provided context does not mention any of these criteria, it is not possible to provide a definitive answer based on the given information.
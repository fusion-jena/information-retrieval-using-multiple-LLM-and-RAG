The provided context discusses various machine learning models and ensemble techniques used to improve landslide susceptibility modeling (LSM), but it does not directly mention strategies for handling randomness in deep learning pipelines. However, I can provide some general information about strategies for handling randomness in deep learning.

In deep learning, randomness can be introduced at various stages, such as weight initialization, data shuffling, and dropout regularization. Here are some strategies to handle randomness:

1. **Fixed seed value**: To ensure reproducibility, you can set a fixed seed value for the random number generator. This ensures that the same sequence of random numbers is generated every time the model is trained, making the training process deterministic.

2. **Weight initialization**: Deep learning models typically initialize weights randomly. Different initialization methods, such as Xavier initialization or He initialization, can be used to ensure that the weights are initialized with appropriate scales, reducing the impact of randomness.

3. **Data shuffling**: Shuffling the training data before each epoch can help prevent overfitting and ensure that the model generalizes well. However, if you want to ensure reproducibility, you should fix the seed value before shuffling the data.

4. **Dropout regularization**: Dropout is a regularization technique used in deep learning to prevent overfitting. It randomly drops out a fraction of the neurons during training, introducing randomness into the model. However, during inference, all neurons are used.

5. **Ensemble methods**: Ensemble methods, such as bagging or boosting, can be used to reduce the impact of randomness by averaging the predictions of multiple models. This can help improve the robustness and generalization of the model.

In summary, while the provided context does not specifically mention strategies for handling randomness in deep learning pipelines, there are several general strategies that can be used, such as setting a fixed seed value, using appropriate weight initialization methods, shuffling data, using dropout regularization, and employing ensemble methods.
Based on the provided context, there is no explicit information given about the specific criterion used to determine when training is complete. However, there are some common methods used in machine learning that can be inferred.

One such method is monitoring the validation loss. If the validation loss does not decrease or even increases for a certain number of epochs, it could indicate that the model is overfitting to the training data and further training may not improve its performance on unseen data. Therefore, the training process might be stopped based on a validation loss plateau.

In the given context, we can see that the per-channel accuracy and F-measure are provided after each epoch. These metrics could also be used to determine when to stop training. If these metrics do not show significant improvement over a certain number of epochs, it might be an indication that the model has reached its optimal performance and continuing the training process would not bring any substantial benefits.

The learning rate decay strategy mentioned in the context could also play a role in determining when to stop training. As the learning rate decreases over time, the model's ability to adapt and learn from the training data is reduced. If the learning rate becomes too small, the model might get stuck in a suboptimal solution. Therefore, the training process might be stopped when the learning rate reaches a certain threshold.

In summary, while the provided context does not explicitly state the criterion used to determine when training is complete, it can be inferred that methods such as monitoring the validation loss, observing the lack of improvement in performance metrics, and considering the learning rate decay strategy could be employed.
The context provided does not give specific information on the criterion used to determine when the training of the machine learning algorithms is complete. However, it does mention that the models were trained for a specified number of epochs. An epoch is one complete pass through the entire training dataset. 

In the absence of specific information, it can be inferred that the training might have been set to stop after a predetermined number of epochs, regardless of whether the validation loss was plateauing or not. This could be a simple and straightforward approach, especially when there is a risk of overfitting due to the limited size of the dataset.

However, in general, machine learning practitioners often prefer to use more dynamic stopping criteria, such as monitoring the validation loss or a related metric, like the accuracy or R-squared value, and stopping the training when this metric stops improving or starts to degrade, indicating that the model is overfitting to the training data. This approach is more adaptive and can potentially lead to better model performance.

So, while the specific criterion used in this study is not clear, it is common practice to use some form of validation loss or performance metric to determine when training is complete.
The provided context discusses several techniques used to address data bias during the preprocessing of a deep learning pipeline, although not all of them are explicitly named.

One method used is stratified sampling. Stratified sampling is a method of sampling that guarantees the training, validation, and testing sets have approximately the same percentage of samples of each target class as the complete dataset. This is used in the creation of the BC-S, BC-L, LS, and LS* datasets.

Another technique mentioned is data augmentation. Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. In this case, random data augmentation techniques were employed to address the issue of a long-tailed distribution of the data for rare and endangered bird species. This included adding background noise and random clipping and splicing.

The context also mentions the use of random spectrogram mixup and random tanh distortion to enhance the datasets. These techniques can be seen as a form of data augmentation, as they create new samples by combining or modifying existing ones. Specifically, random spectrogram mixup creates a new spectrogram with a probability of 0.5, by combining two randomly selected spectrograms and their corresponding one-hot labels.

However, the context does not mention oversampling or undersampling explicitly. Oversampling is a technique used to increase the number of instances of the minority class in an imbalanced dataset, while undersampling is used to decrease the number of instances of the majority class.

In conclusion, the techniques used to address data bias during the preprocessing of the deep learning pipeline in the provided context include stratified sampling, data augmentation (with specific techniques being random spectrogram mixup, random tanh distortion, adding background noise, and random clipping and splicing), but not oversampling or undersampling explicitly.
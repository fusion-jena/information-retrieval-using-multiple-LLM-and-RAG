In the context provided, the criterion used to determine when training is complete is when the network begins to overfit the data, specifically when the error on the validation set starts to increase. This is a form of early stopping, which is a regularization technique used to prevent overfitting and improve the model's ability to generalize.

In this study, the data set was split into two sets: a training set, which was used to compute the gradient and update the network parameters such as weights and biases; and a validation set, which was used to test the model error validation. The model weights were randomly initialized, and the training process was stopped when the error on the validation set began to increase, indicating that the network was starting to overfit the training data.

This approach is different from monitoring the validation loss plateau, which is a common criterion used in many machine learning models to determine when training is complete. In this study, the focus is on preventing overfitting by stopping the training process as soon as the network starts to overfit the data, rather than waiting for the validation loss to plateau.

It's worth noting that the optimal number of neurons in the hidden layer and the optimal learning rate were determined using a trial-and-error method, and the final ANN structure had 19 hidden neurons with one node accounting for bias, and a 0.7 learning rate. These parameters were selected based on the model performance, as indicated by the mean square error (MSE) between the model output and the measured data.

In summary, the criterion used to determine when training is complete in this study is based on early stopping, where the training process is stopped when the error on the validation set starts to increase, indicating that the network is starting to overfit the data. This approach is taken to prevent overfitting and improve the model's ability to generalize.
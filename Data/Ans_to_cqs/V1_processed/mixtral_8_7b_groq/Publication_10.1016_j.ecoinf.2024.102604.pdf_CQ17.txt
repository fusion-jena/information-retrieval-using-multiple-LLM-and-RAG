The provided context does not include information about regularization methods used in deep learning pipelines, such as dropout or L2 regularization. Instead, it focuses on sampling bias correction and species distribution modeling (SDM) methods.

However, I can provide some information about regularization methods that are commonly used to prevent overfitting in deep learning pipelines.

Dropout is a regularization technique used in neural networks, where randomly selected neurons are ignored during training, preventing over-reliance on individual neurons and reducing overfitting. This improves the model's ability to generalize to unseen data.

L2 regularization, also known as weight decay, is another common technique that discourages large weights in the model. This is done by adding a penalty term to the loss function, proportional to the sum of the squares of all the model's weights. This encourages smaller weights, reducing overfitting and improving generalization.

Early stopping is another regularization method that involves monitoring the model's performance on a validation set during training. Training is halted when the model's performance on the validation set stops improving or begins to degrade, preventing the model from overfitting to the training data.

These regularization techniques are widely used in deep learning pipelines to improve model performance and prevent overfitting. However, the provided context does not contain information about these methods.
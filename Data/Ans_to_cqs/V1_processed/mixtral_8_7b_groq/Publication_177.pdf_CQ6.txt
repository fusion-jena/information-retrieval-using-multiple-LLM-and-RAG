Based on the provided context, there are no specific details mentioned regarding the preprocessing steps involved before training the deep learning models. However, it is a common practice to perform several preprocessing techniques on the dataset before training a deep learning model. These steps can include data cleaning, normalization, scaling, and augmentation.

Data cleaning involves handling missing or corrupted data points in the dataset. Normalization is the process of scaling numeric data to a range of [0, 1] or [-1, 1] to avoid any particular feature dominating the learning process. Scaling is similar to normalization, but it involves adjusting the range of features to have a mean of 0 and standard deviation of 1. Data augmentation is a technique used to increase the size of the dataset by applying random transformations such as rotation, translation, and flipping to the existing images.

In the context, it is mentioned that the selected network models were initialized with pre-trained ImageNet weights and then fine-tuned to their datasets. ImageNet is a large-scale image recognition dataset, and the models pre-trained on this dataset have already learned to extract useful features from natural images. Fine-tuning these models on a specific dataset involves adjusting the final layers of the model to better fit the new data while keeping the initial layers frozen. This approach is commonly used in transfer learning, where a pre-trained model is used as a starting point for a new task.

Therefore, while the specific preprocessing steps are not mentioned in the provided context, it is likely that the dataset was preprocessed using common techniques such as data cleaning, normalization, scaling, and augmentation before training the deep learning models.
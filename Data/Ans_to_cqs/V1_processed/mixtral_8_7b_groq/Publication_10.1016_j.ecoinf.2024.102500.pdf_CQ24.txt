The provided context discusses the use of randomness in machine learning models, particularly in the context of tree-based models, but it does not directly mention deep learning models or strategies for handling randomness in deep learning pipelines. However, it does provide some insights into how randomness can be handled in machine learning models, which can be useful for understanding how similar strategies might be applied in deep learning.

In the context of machine learning models, randomness is often introduced through the use of random seed values, which can affect the way that models are trained and the results that they produce. For example, in the Extremely Randomized Trees (ExtraTrees) model described in the Geurts et al. paper, randomness is introduced by selecting the best split points at random, rather than using a deterministic approach to find the best split. This can help to reduce overfitting and improve the model's ability to generalize to new data.

The use of randomness can also be seen in the way that models are trained and validated. For example, in the Goodarzi et al. (2023a) paper, the dataset is split into five equal-sized folds using fivefold cross-validation. This means that the model is trained and validated five times, with a different subset of the data used for validation each time. This can help to reduce the impact of any random fluctuations in the training data and provide a more robust estimate of the model's performance.

In terms of deep learning, there are several strategies that can be used to handle randomness in the pipeline. One common approach is to use a fixed random seed value when initializing the model's weights and biases. This can help to ensure that the same model is produced each time the model is trained, which can be useful for reproducibility and comparison purposes.

Another strategy for handling randomness in deep learning is to use techniques such as dropout and batch normalization. Dropout randomly sets a fraction of the model's outputs to zero during training, which can help to prevent overfitting and improve the model's ability to generalize to new data. Batch normalization, on the other hand, normalizes the inputs to each layer of the model, which can help to reduce the impact of random fluctuations in the data and improve the model's convergence properties.

Overall, while the provided context does not directly address the question of how randomness is handled in deep learning pipelines, it does provide some insights into how randomness can be managed in machine learning models more broadly. These strategies, such as the use of random seed values, cross-validation, and techniques like dropout and batch normalization, can be adapted and applied to deep learning models to help manage randomness and improve model performance.
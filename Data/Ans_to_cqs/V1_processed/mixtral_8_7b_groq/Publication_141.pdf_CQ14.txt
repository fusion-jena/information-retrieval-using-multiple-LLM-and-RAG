Based on the provided context, there is no explicit information about how the hyperparameters of the model were optimized, such as through grid search, random search, or other optimization techniques. The text focuses on discussing the performance of the model in taxonomic identification and the factors that may have influenced its accuracy, such as network architecture, image acquisition protocol, and the robustness of the input matrix.

However, it can be inferred that some hyperparameter optimization might have taken place during the process of transfer learning and data augmentation. The authors mentioned that the ResNet-50 network architecture, in combination with transfer learning, data augmentation, and dropout, proved appropriate for the current number of taxa in their demonstration dataset. This suggests that the authors might have experimented with different hyperparameters to find the best combination for their specific task.

Additionally, the authors mentioned that the results further improved when fine-tuning was applied in the second training phase, where the whole network was trained, allowing the model to learn additional features specific to their dataset. This process might have involved adjusting learning rates, batch sizes, or other hyperparameters to optimize the model's performance.

In summary, while the text does not explicitly provide information on the hyperparameter optimization techniques used, it can be inferred that some form of hyperparameter tuning might have taken place during transfer learning, data augmentation, and fine-tuning stages of the model's development.
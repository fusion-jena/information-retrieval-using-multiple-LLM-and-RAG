Based on the provided context, the specific optimization techniques used in the deep learning pipeline are not explicitly mentioned. However, it is mentioned that the cross-entropy loss function is used for the DNN model, which suggests that some optimization algorithm was used to minimize this loss function.

Common optimization algorithms used in deep learning include Stochastic Gradient Descent (SGD), Adam, and RMSprop. SGD is a simple optimization algorithm that updates the model parameters in the direction of the negative gradient of the loss function. Adam and RMSprop are more advanced optimization algorithms that adapt the learning rate for each parameter during training.

In the context, it is mentioned that the hyperparameters in the network, including the learning rate, were further tuned to optimize the testing accuracy and kappa. This suggests that some form of hyperparameter optimization was performed, which is a common practice in deep learning to find the best set of hyperparameters for a given model and dataset.

However, the specific optimization algorithm used for training the DNN model and the method used for hyperparameter optimization are not provided in the context. Therefore, a definitive answer to the query cannot be given based on the provided information.
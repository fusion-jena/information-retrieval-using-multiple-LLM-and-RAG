In the context provided, the focus is on machine learning models used in landslide susceptibility mapping (LSM), with deep learning models being a part of these models. However, the text does not explicitly mention the preprocessing steps for deep learning models. Nevertheless, based on standard practices in deep learning, some common preprocessing steps can be inferred.

1. Normalization/Scaling: Deep learning models, like other machine learning algorithms, often perform better when input features are normalized or scaled to a similar range. This is because deep learning models can be sensitive to the scale of input features. Common normalization techniques include min-max scaling, which scales features to a range of [0, 1], and z-score normalization, which scales features to have a mean of 0 and a standard deviation of 1.

2. Data Cleaning: Data cleaning is an essential step in preparing any dataset for machine learning. This step involves handling missing data, removing or correcting outliers, and dealing with inconsistent or incorrect data entries. In the context of LSM, data cleaning might involve handling missing data for variables like distance to streams, land cover, lithology, and distance to roads.

3. Feature Engineering: While not explicitly a preprocessing step, feature engineering is an essential part of preparing data for deep learning models. Feature engineering involves creating new features from existing ones, which can help improve model performance. In LSM, feature engineering might involve creating polynomial or interaction features, or deriving new features from existing ones, like calculating the mean slope within a certain distance of each data point.

4. Data Splitting: Another standard practice in machine learning is splitting the dataset into training, validation, and testing sets. This step ensures that the model is evaluated on unseen data, providing a better estimate of its generalization performance.

It is important to note that the specific preprocessing steps required will depend on the dataset and the deep learning model being used. Therefore, it is always a good practice to experiment with different preprocessing techniques and evaluate their impact on model performance.
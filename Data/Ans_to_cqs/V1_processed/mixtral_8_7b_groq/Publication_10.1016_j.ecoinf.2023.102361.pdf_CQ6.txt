In the context provided, there are several preprocessing steps mentioned before training deep learning models. However, the text does not explicitly mention normalization, scaling, or cleaning. Instead, it focuses on splitting the dataset and resizing the images.

Firstly, the dataset is split into three parts: training, validation, and testing, with 60%, 20%, and 20% of the data used for each part, respectively (Section 2.2). This step ensures that the model is trained, validated, and tested on different data, preventing overfitting and providing a more accurate evaluation of the model's performance.

Secondly, when it comes to the images, the patches are resized to a uniform size of 256 × 256 pixels (Section 2.6.4). Although the original patches may have different resolutions (e.g., 64 × 64 and 128 × 128 pixels), they are converted into a uniform size to ensure consistent input to the deep learning models. This step can be seen as a form of scaling, as the images are resized to a specific range of values. However, it is not explicitly mentioned if any normalization or cleaning steps are performed.

Therefore, based on the provided context, the preprocessing steps involved before training a deep learning model include splitting the dataset into training, validation, and testing sets and resizing the image patches to a uniform size. However, there is no explicit mention of normalization, scaling, or cleaning steps.
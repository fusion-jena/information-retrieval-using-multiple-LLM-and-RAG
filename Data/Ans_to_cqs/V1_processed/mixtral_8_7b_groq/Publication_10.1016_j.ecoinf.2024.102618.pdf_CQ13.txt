The provided context does not give specific information about the hyperparameters used in the deep learning models. However, it does mention that data augmentation is used to increase the number of images in the dataset.

The hyperparameters used in a deep learning model can significantly impact its performance. Common hyperparameters include the learning rate, batch size, number of epochs, optimizer, and regularization techniques.

The learning rate determines the size of the steps that the model takes during training. A learning rate that is too high may cause the model to converge too quickly to a suboptimal solution, while a learning rate that is too low may cause the training process to be very slow.

The optimizer is the algorithm used to update the model's weights during training. Common optimizers include stochastic gradient descent (SGD), Adam, and RMSprop.

Regularization techniques, such as L1 and L2 regularization, are used to prevent overfitting by adding a penalty term to the loss function.

Batch size determines the number of samples that are processed before the model's weights are updated. A larger batch size can result in more stable gradients but may require more memory.

The number of epochs determines the number of times that the entire training dataset is passed through the model.

Without more specific information, it is not possible to provide the exact hyperparameters used in the deep learning models in the provided context. However, these are some of the common hyperparameters that are typically tuned during deep learning model training.
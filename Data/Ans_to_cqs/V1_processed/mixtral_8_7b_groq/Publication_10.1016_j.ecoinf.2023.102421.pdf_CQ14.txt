The hyperparameters of the machine learning models in the study are optimized using Bayesian-based optimizers and a novel Tasmanian Devil Optimization algorithm.

The context mentions that hyperparameter optimization is the process of searching for the best set of hyperparameters for a machine learning model to perform well on a given task. While random or grid search is the default method for selecting optimal hyperparameters in many packages, this study uses Bayesian optimization and the Tasmanian Devil Optimization algorithm.

Bayesian optimization is a family of algorithms used for the global optimization of black-box functions. These algorithms utilize Bayesian statistical techniques to model the unknown objective function and iteratively explore the search space to find the optimal solution. Each algorithm has its characteristics, strengths, and weaknesses, making it suitable for optimization problems.

The Tasmanian Devil Optimization algorithm is a novel optimization algorithm used in this study. It is a Swarm-based algorithm, which means it is inspired by the behavior of swarms in nature, such as birds flocking or fish schooling. These algorithms have been compared to various versions of Bayesian optimizers in studies for theoretical problems and engineering applications.

The hyperparameters of XGBoost and LightGBM, two machine learning models, were tuned by several optimizers in this study, using RMSE as the objective (loss) function. The models were optimized/trained and validated using statistical indicators, namely, root mean square error (RMSE), coefficient of determination (R2), and mean absolute error (MAE).

In summary, the hyperparameters of the machine learning models in the study are optimized using Bayesian-based optimizers and a novel Tasmanian Devil Optimization algorithm.
The provided context does not include information about the specific regularization methods used in the deep learning pipeline to prevent overfitting. However, it does mention the use of deep neural networks for improving the performance of a model. Common regularization techniques used in deep learning include dropout, L1/L2 regularization, and early stopping.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent over-dependence on any single neuron and reduces overfitting. L1/L2 regularization adds a penalty term to the loss function, which encourages smaller weights and helps to prevent overfitting by reducing the complexity of the model. Early stopping is a method where training is stopped as soon as the performance on a validation set starts to degrade, preventing the model from further overfitting to the training data.

Benkendorf and Hawkins (2020) analyzed the effect of sample size on deep neural networks and concluded that increasing the number of hidden layers, which can be interpreted as adding more training recordings, can improve the performance of the model. This suggests that the authors may have used techniques to prevent overfitting, as increasing the complexity of the model without proper regularization could lead to overfitting. However, the specific regularization methods used are not mentioned in the provided context.
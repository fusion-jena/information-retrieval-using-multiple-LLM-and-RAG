The pieces of context provided do not contain specific information on the criteria used to determine when training of a machine learning model is complete. However, in general, there are several common criteria used to determine training completion, such as:

1. Validation loss plateau: This is the most common criterion used to determine training completion. It involves monitoring the performance of the model on a validation dataset during training. If the validation loss does not decrease for a certain number of epochs, it is an indication that the model has learned the training data as much as it can, and further training will likely result in overfitting.
2. Early stopping: This is a regularization technique used to prevent overfitting. It involves stopping the training process before the validation loss starts to increase. This is done by setting a maximum number of epochs and monitoring the validation loss after each epoch. If the validation loss does not decrease for a certain number of epochs, the training is stopped.
3. Patience: This is a hyperparameter that determines how long to wait before stopping the training process. It is used in conjunction with the validation loss plateau and early stopping criteria. A high patience value means that the training process will wait longer before stopping, while a low patience value means that the training process will stop sooner.
4. Learning rate decay: This is a technique used to gradually reduce the learning rate during training. It is used to prevent the model from getting stuck in a local minimum. The learning rate is reduced after a certain number of epochs or when the validation loss plateaus.
5. Model performance: This involves monitoring the performance of the model on a test dataset. If the model performs well on the test dataset, it is an indication that the training process is complete.

In summary, the criteria used to determine when training is complete include validation loss plateau, early stopping, patience, learning rate decay, and model performance. These criteria help prevent overfitting and ensure that the model has learned the training data as much as it can.
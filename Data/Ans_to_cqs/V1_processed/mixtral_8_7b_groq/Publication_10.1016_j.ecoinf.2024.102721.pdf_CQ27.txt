Based on the provided context, there is no explicit information given about the process followed to deploy the trained deep learning model. However, we can infer some possible steps that might have been taken based on common practices in machine learning model deployment.

Firstly, after training the LSTM model on ecological water demand forecasting, the model would need to be serialized or saved so that it can be loaded and used for making predictions in the future. This can be done using various model serialization libraries such as joblib, pickle, or TensorFlow's SavedModel in Python. Serialization allows the model to be stored as a file and loaded later without having to retrain the model.

Secondly, the platform for deploying the model would need to be selected. This could be a local machine, a server, or a cloud platform such as Google Cloud, AWS, or Azure. The choice of platform would depend on various factors such as the frequency of predictions, the volume of data, and the computational resources required.

Thirdly, the serialized model would need to be loaded onto the selected platform. This can be done using the appropriate libraries and APIs provided by the platform. Once the model is loaded, it can be used to make predictions on new data.

Finally, the model's performance would need to be monitored and evaluated regularly to ensure that it is providing accurate predictions. This can be done using various evaluation metrics such as mean squared error, mean absolute error, or R-squared. If the model's performance starts to degrade, it may need to be retrained or adjusted.

In summary, while there is no explicit information provided in the context about the process followed to deploy the trained deep learning model, we can infer that the model was likely serialized, loaded onto a selected platform, and used for making predictions. The model's performance would also need to be monitored and evaluated regularly.
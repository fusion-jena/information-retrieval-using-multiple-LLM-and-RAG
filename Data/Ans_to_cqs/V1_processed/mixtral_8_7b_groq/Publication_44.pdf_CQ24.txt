The provided context does not directly mention strategies used to handle randomness in the deep learning pipeline, such as setting a random seed value. However, it does discuss some techniques that can help manage variability and improve model performance.

One such strategy is the use of data augmentation techniques, such as rotation, flipping, and brightness adjustments. These methods can increase the size and diversity of the training set, reducing overfitting and improving the model's ability to generalize. Additionally, using a large training sample, as mentioned in the context, can help maintain good performance for feature extraction methods like DeepLabv3+.

Another approach mentioned in the context is the ensemble of models, as demonstrated in the study [13]. Combining multiple models can help reduce the impact of individual model randomness and improve overall performance.

Furthermore, pre-training the model on a large dataset like ImageNet, as done in the study, can be beneficial. Pre-training helps the earlier layers of the network learn generic features, which can be useful for other computer vision tasks. This can potentially reduce the randomness associated with training a model from scratch.

Lastly, the study [13] used a sliding window technique to improve the training sample size. Although this method is primarily used to handle high-resolution images and limited training data, it can also help manage randomness by providing more diverse training examples.

While the context does not explicitly mention random seed values, the aforementioned strategies can help manage randomness and improve deep learning pipeline performance.
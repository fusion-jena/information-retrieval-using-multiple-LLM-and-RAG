The hyperparameters of the model were optimized by exploring a range of values for various parameters of the CNN architecture. However, the context does not explicitly mention the specific method used, such as grid search or random search. The CNN architecture was inspired by existing works and customized for the task of moth species classification. The hyperparameters explored include kernel sizes for the first and last layers, convolutional depths for the first and last layers, and the size of the fully connected layer. A total of 64 architectures were trained, with the top 5 having high F1-scores that only varied by 0.02.

The optimizer used for training the models was either Stochastic Gradient Descent (SGD) or Adam, with Adam proving to be superior. The architecture with the highest F1-score was 3, 3, 32, 128, 512, which had an F1-score of 92.75%. However, this architecture had 4,330,122 learnable parameters, which was relatively high. To balance performance and complexity, the architecture with the third-highest F1-score but the lowest amount of learnable parameters (2,197,578) was chosen. This architecture had hyperparameters 5, 3, 32, 64, 512.

In summary, while the context does not explicitly mention the method used for hyperparameter optimization, it is clear that a range of values for various parameters were explored. The optimizer used was either SGD or Adam, with Adam providing better results. The final architecture was chosen based on a balance between performance and complexity, considering the number of learnable parameters.
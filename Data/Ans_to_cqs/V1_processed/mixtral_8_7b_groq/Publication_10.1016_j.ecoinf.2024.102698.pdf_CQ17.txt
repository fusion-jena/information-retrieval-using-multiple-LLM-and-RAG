The provided context discusses the use of deep learning methods in the analysis of ecological data, including the use of a TVAE model based on a Variational Autoencoder model. However, it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline.

However, based on general knowledge of deep learning techniques, common regularization methods used to prevent overfitting include dropout, L2 regularization, and early stopping.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent over-reliance on any single neuron and promotes the development of redundant representations within the network.

L2 regularization, also known as weight decay, involves adding a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to develop smaller weights, which can help to reduce overfitting.

Early stopping is a technique where training is halted as soon as the model's performance on a validation set starts to degrade, even if the training loss is still decreasing. This helps to prevent the model from continuing to fit noise in the training data.

It is possible that these or other regularization techniques were used in the deep learning pipeline described in the provided context, but this information is not explicitly stated.
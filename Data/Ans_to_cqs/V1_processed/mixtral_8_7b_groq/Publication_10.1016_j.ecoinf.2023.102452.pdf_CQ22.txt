The performance of the deep learning models in the context provided is evaluated using the Nash-Sutcliffe Efficiency (NSE) metric. The NSE measures the relative magnitude of the residual variance ("error variance") compared to the measured data variance ("data variance") for a given dataset. This metric indicates how well the model predictions match the actual data, with a value of 1 indicating a perfect match and negative values indicating that the mean value of the observed data is a better predictor than the model.

In the comparison of LSTM, CNN, and ConvLSTM models for short-term streamflow forecasting, the NSE is used as the primary evaluation metric. Specifically, NSE Train and NSE Test are reported, which represent the Nash-Sutcliffe Efficiency for the training and testing datasets, respectively. These metrics help assess the model's ability to generalize and predict unseen data.

In the evaluation of the Genetic Programming (GP) and Gaussian Process (GP) models with and without Artificial Neural Network (ANN) components, the NSE is again the chosen metric. The models are evaluated based on NSE Train and NSE Test, providing insights into their performance on both training and testing datasets.

In summary, the Nash-Sutcliffe Efficiency (NSE) is the primary metric used to evaluate the performance of deep learning models in the provided context. It measures the relative error variance compared to the data variance, with values closer to 1 indicating better model performance.
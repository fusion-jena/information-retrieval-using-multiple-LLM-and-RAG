Based on the provided context, there is no explicit information about the optimization techniques applied in the deep learning pipeline for any of the studies mentioned. However, one of the studies (Aksoy et al., 2022) analyzed the performance of three different machine-learning algorithms, but it does not specify the optimization techniques used in the deep learning pipeline.

In general, optimization techniques such as Stochastic Gradient Descent (SGD), Adam, RMSprop, and Adagrad are commonly used in deep learning pipelines to optimize the model parameters and minimize the loss function. The choice of optimization technique depends on the specific problem, the size of the dataset, and the computational resources available.

For instance, SGD is a simple optimization algorithm that updates the model parameters by computing the gradient of the loss function with respect to each parameter. However, SGD can be slow to converge and may get stuck in local minima. To address these limitations, adaptive optimization algorithms such as Adam, RMSprop, and Adagrad have been developed. These algorithms adjust the learning rate for each parameter based on the historical gradient information, allowing for faster convergence and better generalization.

Therefore, while the specific optimization techniques used in the deep learning pipelines of the studies mentioned in the context are not provided, it is common practice to use optimization techniques such as SGD, Adam, RMSprop, and Adagrad in deep learning pipelines.
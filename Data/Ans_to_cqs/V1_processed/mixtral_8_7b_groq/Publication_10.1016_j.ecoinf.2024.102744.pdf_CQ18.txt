The strategy to monitor model performance during training is not explicitly stated in the provided context. However, it can be inferred that the authors use evaluation metrics to assess the performance of their proposed methods and compare them with baseline methods.

In the 'Table 1: Main parameters of the classification network,' the authors mention several layers of their model, such as convolution, pooling, dropout, and dense layers. The use of a dropout layer indicates that they are implementing a technique to mitigate overfitting, which is a common issue in deep learning models. Dropout randomly sets a fraction rate of input units to 0 at each update during training time, which helps prevent over-dependence on any one input feature.

Furthermore, the authors use evaluation metrics such as Mean Recall (MR), False Acceptance Rate (FAR), Precision, Recall, and F1-score to compare their proposed methods with other state-of-the-art methods. These metrics provide insights into how well the model is performing and help identify any potential issues.

In addition, the authors mention training their models for a certain number of epochs, such as 300 epochs for YOLOv5 methods and 200 epochs for the classification network in method 1. By monitoring the evaluation metrics after each epoch, the authors can identify if the model is improving or degrading over time.

Therefore, while the specific strategy to monitor model performance during training is not explicitly stated, it can be inferred that the authors use a combination of techniques such as dropout layers, evaluation metrics, and training for a certain number of epochs to monitor their model's performance during training.
The preprocessing steps involved before training a deep learning model, as described in the given context, include dividing the large orthophotos into smaller tiles and splitting the data into three datasets: training, validation, and test. The tiles are of size 256 × 256 pixels, and each orthophoto of size 10,000 × 10,000 pixels is divided into 1525 such tiles. The data splitting involves using 80% of the data for training, 20% for validation, and two full images of 10,000 × 10,000 pixels for testing the final optimized neural network's performance.

However, the context does not explicitly mention other common preprocessing steps such as normalization, scaling, or cleaning. Normalization is the process of scaling numeric data to a specific range, often between 0 and 1, to improve the model's training. Scaling involves adjusting the range of features to reduce the impact of outliers or large-valued features. Cleaning refers to handling missing or corrupted data points, which can affect the model's performance.

Therefore, while the context provides details on dividing the data into tiles and splitting it into datasets, it does not explicitly mention the other common preprocessing steps of normalization, scaling, and cleaning.
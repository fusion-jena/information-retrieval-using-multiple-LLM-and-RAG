Based on the provided context, the following measures were taken to ensure the generalizability of the deep learning model:

1. Partial Training and Successive Halving: The models were partially trained on a small fraction of the data to quickly eliminate underperforming configurations. The remaining configurations were then grouped into sets of different sizes and trained further. The best-performing configurations in each set were promoted to the next stage, while the underperforming configurations were eliminated. This process was repeated until only one configuration remained, which was considered the best configuration found (sections 2 and 3). This approach helped to identify promising configurations and eliminate those with inferior performance, thereby improving the generalizability of the model.
2. Data Standardization: During the training of the model, the data was standardized. In section 4, one of the three datasets was defined as a training base, on which the standardization parameters were calculated and applied to the other two datasets considered as test bases. This step helped to ensure that the model's performance was not affected by differences in the scales of the variables in the datasets, thereby improving its generalizability.
3. Hyperparameter Optimization: The HyperBand algorithm was proposed for optimization, which involved exploring several configurations in parallel while allocating more training resources to promising configurations. This strategy helped to identify promising configurations quickly, thereby improving the generalizability of the model.

However, the context does not explicitly mention the use of diverse datasets, cross-validation, or stratified splitting. Therefore, it cannot be confirmed based on the provided information that these measures were taken to ensure the generalizability of the deep learning model.
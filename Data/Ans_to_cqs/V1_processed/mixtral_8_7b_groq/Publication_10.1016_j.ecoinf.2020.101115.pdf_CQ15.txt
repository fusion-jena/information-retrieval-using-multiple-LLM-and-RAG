The provided pieces of context do not contain specific information about the optimization techniques used in the deep learning pipeline. However, they do mention the application of machine learning techniques, including deep learning, to automate fish aging using images of fish scales or otoliths.

In general, optimization techniques are essential in deep learning pipelines to adjust the model parameters during training and minimize the loss function. Some commonly used optimization techniques include Stochastic Gradient Descent (SGD), Adagrad, Adadelta, and Adam.

SGD is a simple optimization algorithm that updates the model parameters by taking a step in the direction of the negative gradient of the loss function. Adagrad, Adadelta, and Adam are adaptive optimization algorithms that adjust the learning rate for each parameter individually, taking into account the historical gradient information.

Without specific information from the provided context, it is not possible to determine which optimization technique was used in the deep learning pipeline. However, it is important to note that the choice of optimization technique can significantly impact the performance of the deep learning model. Therefore, it is essential to carefully consider the optimization technique when developing a deep learning pipeline.
Based on the provided context, it does not appear that traditional deep learning optimization techniques such as Stochastic Gradient Descent (SGD) or Adam were explicitly mentioned in the hyperparameter tuning process for the Gradient Boosting Regression Trees (GBRT) models. Instead, manual tuning, grid search, and early stopping were used to find the optimal hyperparameters for the GBRT models.

In the manual tuning process, the learning rate was set as low as possible and the number of iterations was set as high as computationally feasible. The learning rate, also known as a shrinkage parameter, determines the contribution of each tree to the model. A low learning rate means that each tree has a small influence on the final model, which can help to prevent overfitting.

Grid search was performed on both the CS and NW datasets with a two-part split to give hyperparameters with the highest level of accuracy. The learning rate and maximum tree depth were varied in a grid search to find the optimal values. For the RI models, the learning rate was set to 0.05 and the maximum tree depth was set to 4 for both the CS and NW models. For the RY models, the learning rate was set to 0.005 and the maximum tree depth was set to 2 for the NW model.

Early stopping was used to determine when to stop the model training to avoid overfitting. Using early stopping, the number of iterations required until convergence was 120 for CS and 106 for NW for RI models.

Additionally, the context mentioned that stochastic gradient boosting was used, which is a variation of gradient boosting that introduces randomization into the procedure through taking randomly selected subsets of training data at each iteration.

In summary, the optimization techniques used in the deep learning pipeline described in the context are manual tuning, grid search, early stopping and stochastic gradient boosting. Traditional deep learning optimization techniques such as SGD or Adam were not mentioned.
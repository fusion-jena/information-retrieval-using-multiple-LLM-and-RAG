Based on the provided context, there is no explicit information given about how the hyperparameters of the models were optimized. The context describes the use of a RandomForest analysis with 200 trees and balanced weights, as well as MARS regression splines, but it does not mention any specific method for hyperparameter optimization such as grid search or random search.

Hyperparameter optimization is an important step in the process of building machine learning models, as it can have a significant impact on the performance of the model. Common methods for hyperparameter optimization include grid search, random search, and Bayesian optimization.

Grid search involves specifying a range of possible values for each hyperparameter, and then training the model with all possible combinations of these values. This can be computationally expensive, especially for models with many hyperparameters.

Random search is a more efficient alternative to grid search, where a random subset of hyperparameter combinations are selected and trained. This can be less computationally expensive than grid search, while still providing good results.

Bayesian optimization is another method for hyperparameter optimization, which uses a probabilistic approach to select the next set of hyperparameters to try based on the performance of previous sets. This can be more efficient than both grid search and random search, especially for models with many hyperparameters.

Without more information, it is not possible to say for certain how the hyperparameters of the models were optimized in this case. However, it is mentioned that the RandomForest analysis used 200 trees and balanced weights, which suggests that some hyperparameter tuning was done, even if the specific method is not mentioned.
The specific metrics used to evaluate the performance of the deep learning model are not explicitly mentioned in the provided context. However, it is mentioned that the model's performance was evaluated using a set of 100 target calls. This suggests that some form of evaluation was conducted to measure the model's ability to correctly identify these target calls.

The context does mention that a random selection approach was used to address class imbalance in the evaluation data, which implies that the model's performance was likely evaluated using metrics that are sensitive to class imbalance, such as precision, recall, or F1 score.

In addition, the model's performance was evaluated over multiple active learning iterations, and it is mentioned that the majority of model performance improvement was achieved within the first two iterations. This suggests that the evaluation metrics used were able to capture improvements in the model's performance over time.

Overall, while the specific evaluation metrics used are not explicitly stated in the provided context, it is likely that metrics such as precision, recall, or F1 score were used, given the mention of class imbalance and the evaluation of model performance over multiple iterations.
The provided context discusses the use of neural architecture search (NAS) for developing the EfficientNetV2B0 model and its optimization for parameter efficiency and training speed. It also mentions the use of progressive learning to adapt regularization to the image size. However, it does not explicitly specify the regularization methods used to prevent overfitting in the deep learning pipeline.

However, the context does mention the use of dropout as a hyperparameter for a deep neural network (DNN). Dropout is a regularization technique used to prevent overfitting in DNNs by randomly dropping out, or "turning off," neurons during training. This helps to break up co-adaptations between neurons and improves the model's ability to generalize to new data.

Additionally, L2 regularization, also known as weight decay, is another common regularization technique used to prevent overfitting in deep learning models. L2 regularization adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to keep its weights small, which can help to prevent overfitting.

In summary, while the provided context does not explicitly mention the use of L2 regularization, it does mention the use of dropout as a regularization technique in the deep learning pipeline. L2 regularization is another common regularization technique used to prevent overfitting in deep learning models.
The document does not provide explicit information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it is mentioned that they used cross-validation and shifted the validation set 5 times to quantify the average validation cross-entropy loss and accuracy. This suggests that they might have used techniques like early stopping, which is a form of regularization that involves stopping the training process before the model begins to overfit.

Moreover, the document mentions that they used a neural network with different numbers of hidden layers and subsets of features. This implies that they might have also used techniques like dropout, which is a regularization technique where randomly selected neurons are ignored during training, or L2 regularization, which adds a penalty to the loss function based on the size of the weights.

However, without explicit information, it is difficult to provide a definitive answer. It is recommended to refer to the supporting information or contact the authors for more details on the network architecture and training.
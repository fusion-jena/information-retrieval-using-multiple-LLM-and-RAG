The provided context does not explicitly mention which optimization techniques were applied in the deep learning pipeline. However, it does mention that the deep learning operations were performed using MATLAB 2020b, which includes support for various optimization algorithms such as Stochastic Gradient Descent (SGD), Adam, and others.

Given that ResNet-50 was pre-trained on over 1 million images from the ImageNet database, it is likely that an optimization algorithm was used during the pre-training phase. Commonly used optimization algorithms include SGD and Adam.

SGD is a simple optimization algorithm that updates the model parameters by taking a step in the direction of the negative gradient of the loss function. It is often used as the default optimization algorithm in deep learning.

Adam, on the other hand, is an optimization algorithm that combines the advantages of SGD and other optimization algorithms. It uses the moving averages of the gradient and the squared gradient to adapt the learning rate for each parameter.

Therefore, while the provided context does not explicitly mention which optimization algorithm was used, it is likely that either SGD or Adam was used during the pre-training phase of ResNet-50.
The hyperparameters used in deep learning models can include the learning rate, optimizer, number of layers, and more. However, the specific hyperparameters used in a model can vary depending on the case study and the architecture of the model.

In the case studies provided, the hyperparameters used in the deep learning models are not explicitly stated. However, we can infer that the hyperparameters used in the AutoML procedure include the number of layers and the learning rate. The number of layers is selected at random from a prespecified range of values, and the learning rate is not specified but is likely also selected at random or through a predefined set of options.

Additionally, the hyperparameters used in the "deep" neural networks, which refer to ANN architectures capable of training large numbers of hidden layers and neurons, can include the learning rate, optimizer, number of hidden layers, and more. The learning rate and optimizer are two common hyperparameters used in deep learning models. The learning rate determines how much the weights in the model are adjusted during training, while the optimizer determines the algorithm used to find the optimal weights.

In summary, while the specific hyperparameters used in the deep learning models in the provided case studies are not explicitly stated, we can infer that the hyperparameters used in the AutoML procedure include the number of layers and learning rate. Additionally, common hyperparameters used in deep learning models include the learning rate and optimizer.
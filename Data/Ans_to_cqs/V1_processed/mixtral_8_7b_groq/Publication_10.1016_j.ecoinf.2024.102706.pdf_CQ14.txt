The hyperparameters of the machine learning models used in the study, including Enet, SVM, RF, Xgboost, and LightGBM, are optimized using a 5-fold cross-validation grid search strategy. This strategy involves searching for the best set of hyperparameters within a predefined range or grid of possible values. Specifically, the training dataset is divided into five subsets or folds, and the model is trained on four of these folds while the remaining fold is used for validation. This process is repeated five times, with each fold serving as the validation set once. The hyperparameters that result in the best performance, as measured by a chosen metric such as mean squared error or R-squared, are selected as the optimized values.

For instance, in the case of the Enet model, the hyperparameters Î± and l1_ratio are optimized using grid search. Similarly, for the SVM model, the hyperparameters Cost (C), gamma, and epsilon are optimized. For the RF model, the hyperparameters n_estimators, max_features, and max_depth are optimized, while for the Xgboost model, the hyperparameters n_estimators, max_depth, colsample_bytree, min_child_weight, subsample, and learning_rate are optimized. Lastly, for the LightGBM model, the hyperparameters n_estimators, max_depth, learning_rate, num_leaves, feature_fraction, bagging_fraction, and bagging_freq are optimized.

It is worth noting that hyperparameter tuning is crucial in machine learning models to prevent overfitting and improve their performance. Grid search is a systematic and exhaustive method for hyperparameter tuning, but it can be computationally expensive, especially for models with many hyperparameters. Other methods for hyperparameter tuning include random search, Bayesian optimization, and gradient-based optimization. However, the study does not provide information on whether these methods were considered or compared to grid search.
The provided context does not give specific information about the criteria used to determine when training is complete in the machine learning algorithms tested. However, it does mention that 10-fold cross-validation and 10-fold bootstrapping were used in the evaluation of the models. These techniques are often used to assess model performance and prevent overfitting during the training process.

In cross-validation, the training data is divided into k subsets, or folds, and the model is trained on k-1 folds while the remaining fold is used for validation. This process is repeated k times, with a different fold used for validation each time. The resulting validation scores are then averaged to provide an overall measure of model performance. Cross-validation can help to identify when training should be stopped by monitoring the performance on the validation set. If the performance on the validation set starts to degrade, even as the model continues to fit the training data better, this may indicate that the model is overfitting and training should be stopped.

Bootstrapping is a resampling technique that involves creating multiple training sets by randomly sampling with replacement from the original training data. A model is then trained on each of these bootstrap samples and the resulting predictions are used to estimate the model's performance. Bootstrapping can help to provide a more robust estimate of model performance and can also be used to identify when training should be stopped by monitoring the performance on the out-of-bag samples, which are the observations that were not selected for a particular bootstrap sample.

Overall, while the provided context does not give specific information about the criteria used to determine when training is complete, it does suggest that cross-validation and bootstrapping were used in the evaluation of the models. These techniques can help to prevent overfitting and identify when training should be stopped.
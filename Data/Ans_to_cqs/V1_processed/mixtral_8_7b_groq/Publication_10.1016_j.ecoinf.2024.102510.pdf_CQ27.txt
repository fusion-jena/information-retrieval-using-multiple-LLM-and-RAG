Based on the provided context, the specifics of the process followed to deploy the trained deep learning model aren't explicitly mentioned. However, we can infer some steps based on common practices in the field.

The process likely began with model serialization, which involves saving the trained model's architecture and weights to a file. This is a crucial step as it allows the model to be loaded and used for making predictions at a later stage. The context mentions that various models were trained and tested, and the Fine-tuned EfficientNetB0 model exhibited superior performance. It's reasonable to assume that this model was serialized for deployment.

After serialization, the next step would be platform selection. This involves choosing the hardware and software environment where the model will be deployed. The context does not provide specifics on this, but possibilities range from local servers to cloud platforms like Google Cloud, AWS, or Azure.

The context also mentions that the models were used as feature extractors and fine tuners, which suggests that the models might have been integrated into a larger system or pipeline. The deployment process would need to account for this, ensuring that the model can be easily integrated and replaced if necessary.

Finally, the context mentions that the models were evaluated based on metrics like accuracy, precision, recall, and F1-score. These metrics would also be important in the deployment phase, as they would provide a way to monitor the model's performance and ensure it's meeting the required standards.

In conclusion, while the specifics of the deployment process aren't provided in the context, we can infer that it likely involved model serialization, platform selection, integration into a larger system, and monitoring using performance metrics.
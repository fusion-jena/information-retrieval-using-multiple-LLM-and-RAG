The context does not provide detailed information on the exact process followed to deploy the trained deep learning models (Model-K and Model-2). However, it does mention that the models were trained using a GPU and the training time was reported. This suggests that the models were likely trained and evaluated on a local machine or cloud-based platform with GPU capabilities.

As for the model serialization, the context does not explicitly mention any specific method or library used for saving and loading the models. However, it can be inferred that some form of model serialization was used since the training and testing phases were discussed separately. Common deep learning libraries such as TensorFlow, PyTorch, and Keras provide built-in functions for saving and loading models, which enables the trained models to be stored and later used for inference.

In summary, while the context does not provide explicit details on the platform selection and model serialization process, it can be inferred that the models were trained on a GPU-enabled machine or cloud platform, and some form of model serialization was used to save and load the trained models.
Based on the provided context, there is no explicit information about post-processing steps such as saliency maps, metrics calculation, or confusion matrix. However, the text does provide details about the evaluation of fusion quality and feature selection processes.

In the evaluation of fusion quality, five base models (TKNN, TRF, TAdaBoost, TXGBoost, and TLightGBM) were used to generate prediction results for training and validation samples. These base models' predictions were then combined in a column manner to obtain matrices T and V. The RF algorithm was then used to generate the prediction results of training data T by 5-fold cross-validation, resulting in the output (α1, α2, α3, α4, α5). Similarly, (β1, β2, β3, β4, β5) was obtained by predicting V, and the classification result B was obtained by voting.

For feature selection, Recursive Feature Elimination and Cross-Validation (RFECV) were used to select the optimal features that could reduce redundancy caused by multi-source feature datasets and improve model performance. RFECV is a wraparound approach to feature selection based on cross-validation. In the RFECV process, a five-fold cross-validation was used to select the number and combinations of features.

In summary, while the provided context does not explicitly mention post-processing steps like saliency maps, metrics calculation, or confusion matrix, it does detail the evaluation of fusion quality and feature selection processes. The evaluation of fusion quality involves training multiple base models, combining their predictions, and using cross-validation for the meta-model's prediction results. Feature selection is carried out using RFECV, a wraparound approach to feature selection based on cross-validation, to select the optimal features and reduce redundancy in the multi-source feature datasets.
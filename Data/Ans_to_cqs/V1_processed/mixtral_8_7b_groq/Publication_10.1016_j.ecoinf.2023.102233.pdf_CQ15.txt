The provided context does not explicitly mention the specific optimization techniques (e.g., Stochastic Gradient Descent (SGD), Adam) used in the deep learning pipeline. However, it does mention that a learning rate of 0.001 was used during the active learning iterations. This learning rate is often used in conjunction with optimization algorithms like SGD or Adam. Therefore, it can be inferred that an optimization algorithm was used, but the specific one is not specified.
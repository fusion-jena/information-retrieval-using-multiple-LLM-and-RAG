The provided context does not give specific details about the exact hyperparameters used in the deep learning models for fish or Saimaa ringed seal identification. However, it does mention that a very deep CNN with a cross-layer pooling approach was used for fish species classification, and two methods of Saimaa ringed seal identification were examined based on transfer learning: retraining of an existing CNN and using a CNN trained for feature extraction.

In general, hyperparameters used in deep learning models can include learning rate, optimizer, number of layers, number of neurons per layer, batch size, number of epochs, and regularization techniques such as dropout or weight decay. The learning rate determines the step size at each iteration while moving toward a minimum of the loss function. The optimizer is a method used to minimize the loss function, such as stochastic gradient descent or Adam. The number of layers and neurons per layer can affect the model's capacity and complexity. Batch size determines the number of samples processed before updating the model's weights. The number of epochs is the number of times the learning algorithm will work through the entire training dataset. Regularization techniques help prevent overfitting by adding a penalty term to the loss function.

In the context of using contrastive loss for deep learning models, the contrastive loss function takes a pair of samples as input, which can be either similar or dissimilar. The goal is to bring similar samples closer and dissimilar samples far apart in the learned metric space. Hyperparameters specific to contrastive loss can include the margin parameter, which determines the distance between dissimilar samples in the learned metric space.

Therefore, while the exact hyperparameters used in the deep learning models for fish or Saimaa ringed seal identification are not provided, the context gives an idea of the types of hyperparameters that can be used in deep learning models in general and in the context of contrastive loss specifically.
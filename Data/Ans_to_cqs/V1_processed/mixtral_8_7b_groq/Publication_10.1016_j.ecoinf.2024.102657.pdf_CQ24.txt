The text provided does not give specific details about strategies used to handle randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention that the Convolutional Neural Network (CNN) was trained and validated using 80% of the full dataset, and then tested using the remaining 20%. This process of splitting the data into training, validation, and testing sets is a common approach to handle variability and overfitting in machine learning models.

Furthermore, the text states that when assessing the accuracy of the CNN on noise-filtered data versus raw data, the accuracy levels were similar. This suggests that the model's performance is not highly sensitive to random variations in the data, which is another way to handle randomness.

Additionally, the text mentions that the k-NN classifier has only one hyperparameter, k, which was fixed at a value of 4. This implies that there was no random search or cross-validation used to tune the hyperparameter, which could introduce randomness in the model training process.

Overall, while the text does not explicitly mention strategies for handling randomness such as setting a random seed value, it does suggest that other approaches such as data splitting, model assessment, and hyperparameter tuning were used to ensure the robustness and generalizability of the deep learning pipeline.
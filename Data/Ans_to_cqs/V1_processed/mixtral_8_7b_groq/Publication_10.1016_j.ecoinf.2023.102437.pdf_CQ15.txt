The pieces of context provided mention some optimization techniques that are applied in the deep learning pipeline, specifically in the training of deep neural networks (DNNs) and recurrent neural networks (RNNs), such as Stochastic Gradient Descent (SGD) and Adam.

In the study by Azedou et al. (2023), the authors compared different hyperparameters optimization approaches for DNNs in land cover/land use classification. They used a Bayesian optimization algorithm based on SGD as one of the optimization techniques.

On the other hand, the study by Bonato et al. (2023) used an RNN architecture called Long Short-Term Memory (LSTM) to model the relationship between El Niño and La Niña events. Although they did not explicitly mention the optimization technique used, they mentioned that they used a recurrent neural network model, which typically uses optimization algorithms such as SGD or Adam.

Adam is another optimization algorithm commonly used in deep learning pipelines. It is an extension of SGD that adaptively adjusts the learning rate based on the gradient's moving averages. Cho et al. (2018) used the Adam optimization algorithm in their study on time-series prediction of daily chlorophyll-a concentration using a deep learning model.

In summary, optimization techniques such as Stochastic Gradient Descent (SGD) and Adam are commonly used in the deep learning pipeline, particularly in the training of deep neural networks (DNNs) and recurrent neural networks (RNNs). These optimization algorithms help adjust the model's parameters to minimize the loss function and improve the model's performance.
The provided context does not directly address the specific strategies used to handle randomness in the deep learning pipeline, such as the use of a random seed value. However, it is common practice in deep learning to use a fixed seed value to ensure reproducibility of results. This is because many processes in deep learning, such as weight initialization and data shuffling, involve some degree of randomness. By setting a fixed seed value, the same sequence of random numbers is generated each time the model is trained, which ensures that the same set of weights is initialized and the same data order is used in each training run. This can be crucial for comparing the performance of different models or hyperparameter settings, as it ensures that any differences in performance are due to the changes made and not to random variations.

In the context of the papers provided, the use of a fixed seed value is not explicitly mentioned. However, the paper by Todorovic et al. (2023) does mention the use of metaheuristics for tuning the XGBoost algorithm, which could include the use of a fixed seed value to ensure the reproducibility of the results. Similarly, the paper by Wang et al. (2021) uses social media data for crowdsourcing the perceived urban built environment, which could involve the use of a fixed seed value to ensure consistent processing of the data.

In summary, while the provided context does not explicitly mention the use of a fixed seed value to handle randomness in the deep learning pipeline, it is a common practice in deep learning to ensure reproducibility of results.
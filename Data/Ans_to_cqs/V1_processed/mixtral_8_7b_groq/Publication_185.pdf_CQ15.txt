The optimization technique used in the deep learning pipeline of AlexNet, a CNN architecture proposed by Alex Krizhevsky, is Stochastic Gradient Descent (SGD) with a backpropagation algorithm (Krizhevsky et al., 2012). The learning rate policy was set to "step," and the learning rate was initially set to 10-2 for all layers, including the newly defined last fully connected layer (Krizhevsky et al., 2012).

Stochastic Gradient Descent (SGD) is a popular optimization algorithm used in deep learning to minimize the loss function by iteratively updating the parameters in the opposite direction of the gradient (Ruder, 2016). SGD is a stochastic version of the batch gradient descent algorithm, where the weights are updated after processing each training example instead of the entire batch (Goodfellow et al., 2016). This results in a more noisy but less computationally expensive gradient estimation, making SGD suitable for large-scale machine learning problems (Ruder, 2016).

The learning rate is a hyperparameter that controls the size of the steps taken in the parameter space during optimization (Goodfellow et al., 2016). The learning rate policy in AlexNet was set to "step," which means that the learning rate was decreased by a certain factor after a fixed number of epochs (Krizhevsky et al., 2012). This strategy helps to fine-tune the model during the later stages of training and avoid overshooting the minimum of the loss function (Goodfellow et al., 2016).

It is worth noting that other optimization algorithms, such as Adam, Adagrad, and RMSProp, have been proposed and used in deep learning pipelines to improve the training process (Kingma & Ba, 2014; Duchi et al., 2011; Tieleman & Hinton, 2012). These algorithms incorporate adaptive learning rates, momentum, and other techniques to accelerate convergence and improve generalization (Ruder, 2016). However, the original AlexNet implementation used SGD with a backpropagation algorithm and a step learning rate policy (Krizhevsky et al., 2012).

References:

Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Proceedings of the 25th international conference on neural information processing systems (NIPS) (pp. 1097-1105).

Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.

Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.

Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. In International conference on learning representations (ICLR).

Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(Jul), 2121-2159.

Tieleman, T., & Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of the recent gradient-squares. COURSERA: Neural networks for machine learning.
The provided context does not contain specific information about the hyperparameters used in the deep learning models. Hyperparameters are configuration variables that govern the training process of machine learning models, including learning rate, optimizer, number of epochs, batch size, and more.

However, the context does mention that the models were trained and an ensemble of the trained models was formed by averaging their predictions. This implies that the models were trained with some hyperparameters, but the specific values are not provided.

In general, the choice of hyperparameters can have a significant impact on the performance of deep learning models. Commonly used optimizers include stochastic gradient descent (SGD), Adam, and RMSprop, while learning rate schedules can include constant, decaying, or adaptive learning rates. Other hyperparameters such as batch size, number of epochs, and regularization techniques like dropout and weight decay can also affect model performance.

To obtain more specific information about the hyperparameters used in the models described in the context, one would need to refer to the original sources or related publications where the models were introduced or trained.
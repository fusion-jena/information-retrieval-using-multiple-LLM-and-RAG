The generalizability of the deep learning model was ensured through a few measures. Firstly, the raw dataset was randomly shuffled and then divided into training, validation, and test sets, with a data partitioning ratio of 8:1:1. This partitioning was repeated ten times to reduce the influence of random splitting while ensuring converged performance. The training set was used to learn the parameters of the model during the training process, and the validation set was used to evaluate the modeling and tune its hyperparameters. The performance and generalizability of the optimal model were then evaluated using the test set.

Secondly, the grid search method was used to optimize five of the MLP modelâ€™s hyperparameters. These hyperparameters include the number of neural network layers, the number of neurons per layer, dropout, learning rate, and batch size. The tuning ranges for these hyperparameters were 1-5, 64, 128, 256, 512, 1024, 0.1, 0.25, 0.35, 0.5, 0.001, 0.0001, 0.00001, 32, 64, 128, and 256. The optimal MLP architecture and parameters were determined after hyperparameter optimization.

In summary, the generalizability of the deep learning model was ensured through data partitioning, repeated random splitting, and hyperparameter optimization using the grid search method.
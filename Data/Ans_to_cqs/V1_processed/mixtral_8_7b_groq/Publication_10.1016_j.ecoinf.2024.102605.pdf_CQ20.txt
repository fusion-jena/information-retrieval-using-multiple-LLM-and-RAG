Based on the provided context, there is no explicit information about the specific hardware resources used for training the deep learning models. However, it can be inferred that computational resources such as Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs) were likely employed, given the complexity of the models and the nature of the computations involved.

Deep learning models, especially those involving convolutional neural networks (CNNs) and other advanced architectures, typically require substantial computational power for training. This is due to the large number of parameters that need to be optimized and the need to process large datasets. In this context, the papers mentioned use deep learning techniques for image processing and analysis tasks related to ecological informatics. For instance, Wenjuan et al. (2024) monitor Effective Green Area Index and Vegetation Chlorophyll Content using a multi-band spectrometer over winter wheat, while Wujian et al. (2022) detect pine pests using remote sensing satellite images and a multi-scale attention-UNet model. Both of these tasks would benefit from the parallel processing capabilities of GPUs or TPUs.

Moreover, the papers are published in journals that focus on computational and ecological informatics, which further suggests the use of high-performance computational resources. For example, the Sankaran et al. (2010) review discusses advanced techniques for detecting plant diseases, and the Zhang et al. (2024a,b) studies focus on disturbances in North American forests and chlorophyll fluorescence parameters of rice leaves. These studies would likely require computational resources equivalent to or exceeding those of a modern workstation, such as cloud-based GPU clusters or high-performance computing (HPC) facilities.

In summary, while there is no explicit information about the hardware resources used for training the deep learning models, it can be inferred that computational resources such as GPUs or TPUs were likely employed due to the complexity of the models and the nature of the computations involved.
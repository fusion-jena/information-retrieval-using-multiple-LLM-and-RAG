The text provided discusses the use of a deep learning pipeline for classifying animal calls using a ResNet50 model, but it does not explicitly mention the specific optimization techniques used during training. However, it can be inferred that some optimization technique was used since the model was trained on a new dataset.

Common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adagrad, Adadelta, and Adam. SGD is a simple optimization algorithm that updates the model's parameters by taking a step in the direction of the negative gradient of the loss function. Adagrad, Adadelta, and Adam are adaptive learning rate algorithms that adjust the learning rate for each parameter based on the historical gradient information.

Given that the authors used a pre-trained ResNet50 model, it is likely that they fine-tuned the model using a transfer learning approach. Transfer learning is a technique where a pre-trained model is used as a starting point for a new classification task. In this case, the authors removed the network top, which consists of fully connected layers for learning a predictive model from the input features, and added two fully connected layers separated by a drop-out layer.

During fine-tuning, the authors could have used one of the optimization techniques mentioned earlier, such as SGD or Adam, to update the model's parameters. They could have also used a learning rate schedule, such as step decay or exponential decay, to adjust the learning rate during training.

In summary, while the text does not explicitly mention the optimization techniques used in the deep learning pipeline, it can be inferred that some optimization technique was used during the fine-tuning of the pre-trained ResNet50 model. Common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adagrad, Adadelta, and Adam. The authors could have also used a learning rate schedule to adjust the learning rate during training.
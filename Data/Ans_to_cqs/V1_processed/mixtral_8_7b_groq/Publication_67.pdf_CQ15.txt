The optimization techniques applied in the deep learning pipeline mentioned in the context are RMSProp and momentum. RMSProp is an optimization algorithm that adjusts the learning rate for each parameter separately, based on the root mean square (RMS) of the recent gradients. It is a variation of the stochastic gradient descent (SGD) algorithm, which is a commonly used optimization technique in deep learning. Momentum, on the other hand, is a method that helps accelerate SGD in the relevant direction and dampens oscillations by adding a fraction of the update vector of the past time step to the current update vector.

In the context, the authors used RMSProp with a momentum of 0.9 and epsilon of 0.1 to configure the model parameters by training the last two fully connected network layers in their dataset. They did not mention the use of other optimization techniques such as Adam, which is another popular optimization algorithm in deep learning.

Another optimization technique that is not mentioned in the context but is commonly used in deep learning is Adaptive Moment Estimation (Adam). Adam is an optimization algorithm that combines the advantages of both RMSProp and momentum. It calculates the adaptive learning rate for each parameter by using the estimates of the first and second moments of the gradients.

In summary, the optimization techniques applied in the deep learning pipeline mentioned in the context are RMSProp and momentum. Other optimization techniques such as Adam are also commonly used in deep learning.
The text provided discusses the use of a regularized cost function in the context of a deep learning pipeline, but it does not explicitly mention the specific regularization methods used to prevent overfitting, such as dropout or L2 regularization. However, it can be inferred that some regularization technique was used during the training of the algorithm, as the authors mention optimizing the regularized cost function using stochastic gradient descent.

In general, there are several regularization techniques commonly used in deep learning to prevent overfitting. Dropout is a regularization method that randomly sets a fraction rate of input units to zero during training, which helps to make the network more robust and less prone to overfitting. L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the weights, which encourages the weights to be small and distributed evenly.

The text also describes the use of a validation set during training, which is another common technique to prevent overfitting. By setting aside a portion of the training data as a validation set, the model's performance can be monitored during training, and if the performance on the validation set starts to degrade, the training can be stopped early to avoid overfitting.

Additionally, the text mentions the use of a min-max scaler to compute the performance metrics in the test set. This is a data preprocessing technique that can also help to prevent overfitting by normalizing the input features to a similar range, which can improve the generalization of the model.

In summary, while the text does not explicitly mention the use of dropout or L2 regularization, it can be inferred that some regularization technique was used during training, and other common techniques such as validation set, data preprocessing and early stopping were also used to prevent overfitting.
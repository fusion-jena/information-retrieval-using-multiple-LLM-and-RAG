The text describes the use of the Adam optimizer in the deep learning pipeline. The authors trained a deep convolutional neural network (DCNN) for object detection and used the Adam optimizer with a learning rate of 10−4 for the first 25 epochs. After that, they reduced the learning rate to 10−6 and trained for another 20 epochs. The Adam optimizer is a popular optimization technique for deep learning models because it combines the advantages of two other optimization methods: AdaGrad, which adapts the learning rate to the parameters, and RMSProp, which uses a moving average of squared gradient estimates to adjust the learning rate. Adam is known for its efficiency, ability to handle sparse gradients, and low memory requirements, making it a suitable choice for training deep learning models. However, the text does not mention the use of any other optimization techniques such as Stochastic Gradient Descent (SGD) in the described pipeline.
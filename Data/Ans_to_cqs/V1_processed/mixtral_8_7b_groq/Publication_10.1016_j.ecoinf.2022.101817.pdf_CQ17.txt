The provided context discusses various aspects of deep learning models, image classification, and model evaluation, but it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it can be inferred that the authors might have used regularization techniques such as dropout and L2 regularization, as these are commonly used methods in deep learning pipelines.

Dropout is a regularization technique used to prevent overfitting in neural networks by randomly dropping out, or "turning off," a proportion of neurons during training (Srivastava et al., 2014). This helps to ensure that the model does not rely too heavily on any single neuron or group of neurons, and instead learns to distribute the weights more evenly.

L2 regularization, also known as weight decay, is another commonly used regularization technique that involves adding a penalty term to the loss function, which is proportional to the square of the magnitude of the model's weights (Goodfellow et al., 2016). This encourages the model to learn smaller weights, which can help to prevent overfitting by reducing the complexity of the model.

While the provided context does not explicitly mention these regularization techniques, it is likely that they were used in the deep learning pipelines discussed in the papers. This is because these techniques are widely used in the field of deep learning, and are often used in conjunction with convolutional neural networks (CNNs) for image classification tasks.

References:
Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep Learning. MIT Press.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., 2014. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15, 1929â€“1958.
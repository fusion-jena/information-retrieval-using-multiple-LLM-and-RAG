Based on the provided context, there is no explicit mention of measures such as diverse dataset, cross-validation, or stratified splitting to ensure the generalizability of the deep learning model. However, there are some techniques mentioned that indirectly contribute to the model's generalizability.

Firstly, the models were trained on datasets split into training and testing images with a ratio of 80:20. This is a common practice in machine learning to assess the model's performance on unseen data.

Secondly, the models were trained using an early stopping technique. This method helps to avoid overfitting by stopping the training process once the model's performance on the validation set starts to degrade.

Thirdly, the Model-2 was initialized with pre-trained Imagenet weights. Using pre-trained weights can help the model to learn more general features that can be useful for the new task.

Lastly, the authors compared the performance of their proposed Model-2 with a baseline Model-K. This comparison helps to understand the effectiveness of the proposed model and its ability to generalize to new data.

However, it is still unclear whether the datasets used in the study were diverse enough to ensure the model's generalizability to different populations or scenarios. Additionally, there is no mention of cross-validation or stratified splitting, which are commonly used techniques to assess the model's performance and generalizability.
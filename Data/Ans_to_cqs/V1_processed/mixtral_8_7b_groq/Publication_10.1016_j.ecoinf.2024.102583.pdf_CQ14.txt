The context provided does not give specific details about how the hyperparameters of the XGBoost model are optimized. However, it does mention that XGBoost has built-in cross-validation support, which can be used for hyperparameter tuning.

Cross-validation is a technique used to evaluate the performance of a machine learning model and tune its hyperparameters. In k-fold cross-validation, the dataset is divided into k subsets or "folds". The model is then trained on k-1 folds while one fold is held out as a validation set. This process is repeated k times, with a different fold used as the validation set each time. The performance of the model is then averaged over the k runs to give an estimate of its generalization ability.

Hyperparameter tuning involves searching for the best set of hyperparameters for a given model. This can be done using techniques such as grid search, random search, or Bayesian optimization. Grid search involves systematically trying out different combinations of hyperparameters within a predefined range. Random search, on the other hand, involves randomly sampling hyperparameters from a defined range. Bayesian optimization uses a probabilistic model to guide the search for the best hyperparameters.

The context also mentions that XGBoost uses a more advanced optimization approach that combines first-order gradients (loss function gradient) with second-order gradients (loss function curvature), which makes it faster than some other models. This optimization approach can also be used to tune the hyperparameters of the model.

In summary, while the context does not provide specific details about how the hyperparameters of the XGBoost model are optimized, it does mention that XGBoost has built-in cross-validation support, which can be used for hyperparameter tuning. Techniques such as grid search, random search, or Bayesian optimization can be used for hyperparameter tuning, and the optimization approach used by XGBoost can also be used for this purpose.
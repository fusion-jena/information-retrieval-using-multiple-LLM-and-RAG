The provided context does not include information on the specific criteria used to determine when training is complete in the context of machine learning or data processing. However, there are some common criteria used in practice to determine training completion.

One such criterion is when the validation loss plateaus or stops decreasing significantly. Validation loss is a measure of the error of the model on a validation dataset that is separate from the training dataset. When the model is trained, it is important to evaluate its performance on this validation dataset to ensure that it is not overfitting to the training data. If the validation loss stops decreasing or plateaus, it may indicate that the model has learned all it can from the training data and further training may not improve its performance.

Another criterion is based on the number of epochs, which refers to the number of times the learning algorithm cycles through the entire training dataset. If the validation loss does not improve for a certain number of epochs, training may be stopped to prevent overfitting.

Additionally, early stopping techniques can be used to automatically determine when to stop training based on the performance on the validation dataset. These techniques monitor the performance on the validation dataset during training and stop training when the performance starts to degrade.

It's important to note that the choice of criterion for determining training completion depends on the specific problem and dataset being used. Therefore, it's important to consider the problem and dataset carefully when choosing a criterion for determining training completion.
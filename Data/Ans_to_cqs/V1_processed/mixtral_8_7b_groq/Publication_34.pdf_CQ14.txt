In the provided context, it is mentioned that the optimization of hyperparameters in Neural Network (NN) models, particularly the initial learning rate, is an area that requires further work. However, the specific methods or techniques for hyperparameter optimization, such as grid search or random search, are not mentioned. Therefore, based on the given context, it is not possible to provide a precise answer to how the hyperparameters of the model are optimized.

Nevertheless, it is important to note that hyperparameter optimization is a crucial step in the training of NN models. Hyperparameters are parameters whose values are set before the learning process begins and are not learned from the data. Examples of hyperparameters include the learning rate, the number of layers in the network, the number of neurons in each layer, and the type of activation function used. The optimization of these hyperparameters can significantly impact the performance of the NN model.

Grid search and random search are two common methods used for hyperparameter optimization. Grid search involves defining a range of possible values for each hyperparameter and then training the model with all possible combinations of these values. This method is exhaustive and can be computationally expensive, especially if the number of hyperparameters and their possible values are large.

On the other hand, random search involves randomly selecting values for each hyperparameter from their respective ranges and training the model with these values. This method is less exhaustive than grid search but can be more efficient in terms of computation time.

In summary, while the provided context does not specify the methods used for hyperparameter optimization in NN models for species distribution, grid search and random search are two common techniques used for this purpose. The optimization of hyperparameters is an essential step in the training of NN models, and the choice of method can significantly impact the model's performance.
The provided context does not give specific criteria for splitting the data when training a deep learning model. However, it does mention the use of a Bidirectional Long Short-Term Memory (LSTM) model for predicting ecological water demand in an arid oasis.

In general, there are several common methods for splitting data in machine learning and deep learning:

1. Random sampling: This involves randomly dividing the dataset into a training set, a validation set, and a test set. The randomness ensures that each set is representative of the entire dataset.

2. Time series split: In the context of time series data, it is common to use the most recent data for testing and the older data for training. This helps to evaluate the model's ability to make predictions on new, unseen data.

3. K-fold cross-validation: This method involves dividing the dataset into 'k' subsets or folds. The model is then trained on 'k-1' folds, while one fold is reserved for testing. This process is repeated 'k' times, with a different fold used for testing each time. The average performance across all 'k' runs is then calculated.

4. Stratified sampling: This method ensures that each subset (training, validation, test) has approximately the same percentage of samples from each class in a classification problem.

The choice of splitting method depends on the specific problem and dataset at hand. In the case of the LSTM model used in the study, it is possible that any of the above methods were used, but the context does not provide enough information to determine which was used.
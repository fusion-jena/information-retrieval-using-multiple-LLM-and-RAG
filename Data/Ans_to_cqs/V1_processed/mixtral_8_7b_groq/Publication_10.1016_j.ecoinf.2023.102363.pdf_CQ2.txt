The data format used in the deep learning pipeline described in the given context is a spectrogram representation of audio data. 

The audio data is initially input to the network, and then transformed into a spectrogram representation. This is done by dividing the 24 hours of PAM (Passive Acoustic Monitoring) data into frames of 3 seconds each. These frames are then reviewed visually and aurally using Audacity software, and classified into one of the four sound source categories. 

Following the principles detailed in White et al., 2022, frames are assigned a single label based on a hierarchy of rules. For instance, if a whistle is present in the 3 seconds frame, the label is ‘Delphinid Tonal’, regardless of the presence of another sound source. 

After annotation, a training set of 28,800 frames is available across the four classes. These frames, in the form of spectrograms, are used as the input to the deep learning model. 

Therefore, the data format used in this deep learning pipeline is a spectrogram representation of audio data.
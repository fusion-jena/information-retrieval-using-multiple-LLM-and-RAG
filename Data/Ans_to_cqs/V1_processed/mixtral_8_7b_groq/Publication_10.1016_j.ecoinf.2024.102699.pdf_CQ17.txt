The provided context does not explicitly mention the use of regularization methods such as dropout or L2 regularization to prevent overfitting in the deep learning pipeline. However, it does describe other techniques that were used to mitigate overfitting and improve model performance.

To prevent overfitting, the authors used a data set sampler that upsampled and duplicated randomly selected samples from the minority class, thereby increasing the representation of this class in the training data. This technique can help to reduce overfitting by providing the model with more examples of the minority class.

In addition, the authors used a binary cross entropy based focal loss and scaled focal loss to alleviate the influence of class imbalances. These loss functions are designed to be less sensitive to class imbalances, which can help to prevent overfitting by ensuring that the model is not biased towards the majority class.

Furthermore, the authors used early stopping to prevent overfitting when no further improvement was observed in the validation loss. This technique involves stopping the training process early if the validation loss does not decrease for a certain number of epochs.

Finally, the authors used a 5-fold cross validation to evaluate the performance of their models. Cross validation is a powerful technique for reducing overfitting by providing a more robust estimate of the model's generalization performance.

Overall, while the provided context does not explicitly mention the use of dropout or L2 regularization, it does describe several other techniques that were used to prevent overfitting and improve model performance.
The provided context discusses the use of deep learning for Landsat image classification and phenological knowledge input, but it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it is important to note that regularization is a crucial aspect of deep learning model development to reduce overfitting and improve model generalization.

Common regularization techniques for deep learning models include:

1. L2 regularization: This method adds a penalty term to the loss function, which is proportional to the square of the magnitude of the model's weights. By doing so, it encourages the model to have smaller weights, reducing overfitting.

2. Dropout: During training, dropout randomly sets a fraction of the model's outputs to zero. This helps prevent overfitting by making the model less reliant on any single neuron, effectively creating an ensemble of models.

3. Early stopping: This technique involves monitoring the model's performance on a validation set during training. Training is halted when the validation loss stops decreasing, even if the training loss continues to decrease.

While the provided context does not mention these specific regularization techniques, it is possible that the authors employed them during their deep learning model development. Nonetheless, the context does not provide sufficient information to confirm this.
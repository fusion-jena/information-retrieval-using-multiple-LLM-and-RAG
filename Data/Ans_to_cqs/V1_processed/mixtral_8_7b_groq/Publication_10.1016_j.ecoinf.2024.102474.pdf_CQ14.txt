The hyperparameters of the model are optimized using an Optuna-based hyperparameter optimization procedure. Optuna is a next-generation hyperparameter optimization framework developed by Akiba et al. in 2019. It is a powerful tool that can be used to automatically search for the best set of hyperparameters for a given model.

In this work, the authors implemented their models using the deep learning framework PyTorch Lightning, which is built on top of PyTorch and enables improved scalability. They then used Optuna to tune the hyperparameters of their models.

Optuna uses a Bayesian optimization algorithm to search for the best set of hyperparameters. This algorithm uses a probabilistic model to make informed decisions about which hyperparameters to try next, based on the performance of previous hyperparameter configurations. This approach is often more efficient than traditional methods like grid search or random search, as it can quickly focus on the most promising regions of the hyperparameter space.

The authors noted a significant acceleration of convergence when employing the baseline framework, which suggests that the Optuna-based hyperparameter optimization procedure was effective at finding a good set of hyperparameters for their models.

Overall, the use of Optuna for hyperparameter optimization is a promising approach that can help to improve the performance and efficiency of machine learning models. By automating the hyperparameter tuning process, researchers can save time and effort, and can focus on other aspects of their work.
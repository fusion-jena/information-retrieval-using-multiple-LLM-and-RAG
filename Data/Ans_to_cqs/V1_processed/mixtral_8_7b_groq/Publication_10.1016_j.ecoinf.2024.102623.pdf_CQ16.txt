Based on the provided context, there is no explicit information given about the criteria used to determine when training is complete. The context mainly focuses on discussing and evaluating different machine learning loss functions and the limitations of a specific loss function.

However, in general, training of machine learning models is considered complete when the validation loss plateaus or stops decreasing significantly. This is because the goal of training is to minimize the generalization error, which is estimated using the validation loss. Once the validation loss stops decreasing, further training is unlikely to improve the generalization error and may lead to overfitting.

In the context, Appendix B discusses various alternative ML loss functions and evaluates their performance based on the mean AUC metric, which is averaged over 10 random seeds. The different loss functions include Cross-Entropy (CE), weighted CE, Focal CE, LDAM, DB loss, Entmax, and Full weighted, among others. However, the context does not mention any specific criteria for determining when training is complete.

Additionally, Section 2.2.1 discusses the limitations of a specific loss function, L full, in addressing class imbalance within datasets. However, this section does not provide any information about training completion criteria.

Therefore, based on the provided context, the criteria used to determine when training is complete are not specified. It is a general practice in machine learning to use validation loss plateau as a criterion for training completion.
The raw data in the deep learning pipeline is collected using a combination of methods, including data from public authoritative repositories of big data, field observations, and other public FAIR data repositories.

To begin with, the study utilizes data from a public authoritative repository of big data, the Global Biodiversity Information Facility (GBIF), to gather species observation data. This data is then improved with reliable records that were accurately and directly observed in the field. This approach provides a solid foundation of data for the study, using both existing big data resources and firsthand observations.

In addition to the GBIF data, the study plans to conduct a new data census and collection campaign based on other public FAIR data repositories. Specifically, they mention using data from the Tuscany Region administration and the Copernicus program. These data sources will likely have a coarser spatial resolution than the data used initially, meaning that they cover larger geographic areas with less detailed information.

To account for this difference in spatial resolution and ensure that the data can be used effectively in regional-scale models, sophisticated data-harmonisation strategies will be required. This process involves adjusting and standardizing the data from different sources to ensure that they can be compared and combined accurately.

While the text does not explicitly mention specific data collection methods such as surveys or sensors, it is clear that a combination of existing big data resources, field observations, and other public data repositories are used to collect the raw data for this deep learning pipeline. Additionally, the planned data census and collection campaign will involve gathering data from other public sources, indicating that the study is not limited to a single data collection method.
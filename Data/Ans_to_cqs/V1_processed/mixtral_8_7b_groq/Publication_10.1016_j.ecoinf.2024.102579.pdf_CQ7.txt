Based on the provided context, there is no explicit information about the criteria used to split the data for deep learning model training. However, I can suggest a common approach to data splitting that could be applicable in this context.

In deep learning model training, it is common to split the data into three subsets: training, validation, and testing. The training set is used to train the model by adjusting the model parameters to minimize the error on the training data. The validation set is used to tune the hyperparameters of the model and to monitor the model's performance during training. The testing set is used to evaluate the final performance of the trained model.

In the context of the provided text, if a deep learning model were being used for predicting forest diversity indices, the data could be split based on the sample points used to collect the data. For example, if data were collected at multiple sample points in a forest, some sample points could be randomly selected for the training set, some for the validation set, and the remaining for the testing set.

The criteria used to split the data could be based on several factors, such as the number of sample points available, the spatial distribution of the sample points, and the desired level of precision and recall for the model. A common approach is to use a random split, where a fixed percentage of the sample points is randomly assigned to each of the three subsets. For example, 70% of the sample points could be used for training, 15% for validation, and 15% for testing.

In summary, while the provided context does not explicitly state the criteria used to split the data for deep learning model training, a common approach is to use a random split of the sample points into training, validation, and testing subsets. The specific criteria used could depend on the number and distribution of the sample points and the desired performance of the model.
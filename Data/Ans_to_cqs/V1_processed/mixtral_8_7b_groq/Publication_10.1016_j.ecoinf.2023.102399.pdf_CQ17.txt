The provided context discusses various applications of deep learning techniques in the field of agriculture and aquaculture, but it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, I can provide some general information about the regularization techniques commonly used in deep learning.

Regularization methods are essential to prevent overfitting in deep learning models. Overfitting occurs when a model learns the training data too well, including its noise, and performs poorly on unseen data. Regularization techniques help to reduce overfitting by adding a penalty term to the loss function, which discourages the learning of overly complex models.

Two common regularization methods used in deep learning are L2 regularization and dropout.

1. L2 regularization: This method adds a penalty term proportional to the square of the magnitude of the model's weights to the loss function. By doing so, it encourages the model to have smaller weights, reducing the complexity of the model and preventing overfitting. L2 regularization is also known as weight decay or ridge regression.

2. Dropout: Dropout is a regularization technique specifically designed for neural networks. During training, dropout randomly sets a fraction of the neurons in a layer to zero, effectively preventing those neurons from contributing to both the forward pass and the backward pass. This process helps to promote independence among the neurons, reducing overfitting and improving the model's generalization performance.

These regularization techniques can be applied to various deep learning models, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The choice of regularization method and its hyperparameters depend on the specific problem and the complexity of the model.

In summary, while the provided context does not mention the regularization methods used to prevent overfitting in the deep learning pipeline, L2 regularization and dropout are two common regularization techniques used in deep learning to prevent overfitting.
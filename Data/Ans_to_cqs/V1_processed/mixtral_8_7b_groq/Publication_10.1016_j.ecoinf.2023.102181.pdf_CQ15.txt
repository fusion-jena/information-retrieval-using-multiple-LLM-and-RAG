The provided context discusses methods to minimize bias in machine learning models trained on clustered data, specifically focusing on bagging techniques for geospatial modeling. It does not directly mention optimization techniques used in deep learning pipelines such as Stochastic Gradient Descent (SGD) or Adam.

However, it is important to note that bagging is a technique used to improve the stability and accuracy of machine learning algorithms, particularly decision trees, by creating an ensemble of models trained on different subsets of the data. This is different from optimization techniques used in deep learning, which focus on minimizing a loss function to find the best set of model parameters.

In deep learning, optimization techniques are used to adjust the weights and biases of the model during training. Some common optimization techniques include:

1. Stochastic Gradient Descent (SGD): An optimization algorithm that uses gradient descent to iteratively adjust the model parameters in the direction of the steepest decrease in the loss function.
2. Adam: An optimization algorithm that combines the ideas of momentum and adaptive learning rates to improve the convergence of SGD.

These optimization techniques are used to find the best set of model parameters that minimize the loss function, and are typically used in conjunction with other techniques such as regularization and learning rate schedules to prevent overfitting and improve generalization.

Therefore, while the provided context discusses methods to minimize bias in machine learning models trained on clustered data, it does not directly mention optimization techniques used in deep learning pipelines such as SGD or Adam.
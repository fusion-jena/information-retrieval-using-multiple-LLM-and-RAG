Based on the provided context, there are no specific postprocessing steps mentioned after the model training regarding Saliency maps, Metrics calculation, or Confusion matrix. However, we can infer some general steps that might have been taken based on common practices in machine learning projects.

First, after training the model, there would likely be an evaluation step where the model's performance is assessed using metrics such as accuracy, error rate, or F1 score. The code snippet shows that the 'accuracy' and 'error_rate' metrics were used during the training process, so it is reasonable to assume that these metrics were calculated at the end of training as well.

Second, generating a confusion matrix is another common evaluation step, especially for classification problems. A confusion matrix provides a more detailed breakdown of the model's performance, showing the number of true positives, true negatives, false positives, and false negatives. However, the provided context does not explicitly mention generating a confusion matrix.

Third, Saliency maps are a technique for visualizing the importance of individual input features in a model's prediction. While the provided context does not mention generating Saliency maps, it is possible that they were created as part of the postprocessing analysis.

In summary, while the provided context does not explicitly mention postprocessing steps such as Saliency maps, Metrics calculation, or Confusion matrix, it is likely that some form of evaluation was performed after training the model. Common evaluation steps include calculating metrics such as accuracy or error rate, generating a confusion matrix, and visualizing the model's predictions using techniques such as Saliency maps.
The provided pieces of context do not give specific information about the optimization techniques used in the deep learning pipelines for land use and land cover changes. However, they do mention the use of deep learning models, particularly convolutional neural networks (CNNs) and the U-Net model, for land-cover classification.

Optimization techniques are crucial in training deep learning models as they help to minimize the loss function and improve the model's performance. Some of the commonly used optimization techniques include Stochastic Gradient Descent (SGD), Adagrad, Adadelta, RMSprop, and Adam.

Stochastic Gradient Descent (SGD) is a simple optimization algorithm that updates the model's weights based on the gradient of the loss function. Adagrad is an extension of SGD that adapts the learning rate for each parameter based on the historical gradient. Adadelta is similar to Adagrad but uses a running average of the historical gradient instead of the historical gradient itself. RMSprop is another optimization algorithm that uses a moving average of the squared gradient to adapt the learning rate.

Adam is a popular optimization algorithm that combines the ideas from Adagrad and RMSprop. It uses a running average of both the gradient and the squared gradient to adapt the learning rate. Adam has been widely used in deep learning applications due to its efficiency and effectiveness.

Without specific information from the provided context, it is difficult to determine which optimization techniques were used in the deep learning pipelines for land use and land cover changes. However, it is safe to assume that one or more of the above-mentioned optimization techniques were used, given their widespread use in deep learning applications.
The criteria used to split the data for deep learning model training typically include train, validation, and test datasets. However, the specifics of how the data is split and the proportions used can vary depending on the context and the goals of the model training process.

In the provided context, the training dataset was divided into two parts, with 80% used for training/validation and the remaining 20% kept as unseen data for testing. This is a common approach to splitting the data, as it allows for a sufficient amount of data to be used for training and validating the model while also ensuring that there is a separate dataset available for testing the model's performance on unseen data.

To further reduce bias and ensure that the model is not overfitting to the training data, 10-fold cross-validation was used during the training process. This involves dividing the training data into 10 subsets, or folds, and training the model on 9 of the folds while validating on the remaining fold. This process is repeated 10 times, with a different fold used for validation each time. The mean RMSE (root mean squared error) was used as the objective function, which is a common metric for evaluating the performance of regression models.

It's important to note that the provided context focuses on hyperparameter optimization for machine learning models, rather than deep learning models specifically. However, the principles of data splitting and cross-validation can still be applied to deep learning model training.

In summary, the criteria used to split the data for deep learning model training typically include a training dataset, a validation dataset, and a test dataset. The specific proportions used can vary, but it is common to use 80% of the data for training/validation and 20% for testing. Additionally, cross-validation can be used to further reduce bias and ensure that the model is not overfitting to the training data.
The provided context discusses the use of various machine learning models and techniques for predicting and gap-filling missing data in a specific dataset. However, it does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline.

That being said, the context does mention the use of the Adam solver, which is an optimization algorithm used to update the parameters of a neural network during training. One way to prevent overfitting in deep learning models is by using regularization techniques, such as L1 and L2 regularization, dropout, and early stopping.

L1 and L2 regularization are techniques that add a penalty term to the loss function, which encourages the model to have smaller weights and prevent overfitting. Dropout is a technique that randomly sets a fraction of the neurons in a layer to zero during training, which helps to prevent overfitting by reducing the co-adaptation of neurons. Early stopping is a technique that stops the training process when the performance on a validation set stops improving, which helps to prevent overfitting by not allowing the model to learn too much from the training data.

Therefore, while the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, it is reasonable to assume that some of these techniques were used during the training of the neural network models. However, without further information, it is not possible to provide a definitive answer.
The hyperparameters used in the deep learning model, as described in the provided context, include the activation function, the optimizer, and the learning rate. 

For the activation function, the options considered were ReLU (Rectified Linear Unit), sigmoid, and elu. The chosen activation function was ReLU.

The optimizer options included RMSprop and Adam. The selected optimizer was RMSprop.

The learning rate values tested were 0.1, 0.001, 0.0001, and 0.00001. The chosen learning rate was 0.001.

Additionally, data augmentation techniques were applied to increase the number and variability of the training data. These modifications included rotation, horizontal and vertical flips, width and height shift ranges, shear range, zoom range, and brightness range.

During the training process, the mean squared error was employed as the loss function. Callbacks were implemented to stop training when the validation error diverged from the training error (early stopping technique), saving the model with the best performance.

In summary, the deep learning model used the ReLU activation function, RMSprop optimizer, a learning rate of 0.001, and data augmentation techniques for training.
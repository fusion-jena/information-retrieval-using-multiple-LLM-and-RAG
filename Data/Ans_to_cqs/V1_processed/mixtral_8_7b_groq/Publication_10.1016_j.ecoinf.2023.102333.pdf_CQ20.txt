Based on the provided context, there is no explicit information about the specific hardware resources used for training the deep learning model. However, it is mentioned that optimization algorithms were executed to find the highest accuracy from a large number of hyperparameter combinations, and the model was trained for 200 epochs for each trial. Additionally, three DNN architectures were used to build three models and train them for the entire dataset for 1000 epochs. Given the large number of trials and epochs, it is reasonable to assume that specialized hardware, such as GPUs or TPUs, were used to accelerate the training process. However, without further information, this is just an educated guess.
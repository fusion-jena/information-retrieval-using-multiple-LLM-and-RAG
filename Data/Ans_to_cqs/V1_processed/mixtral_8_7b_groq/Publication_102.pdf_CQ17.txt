Based on the provided context, there are no explicit mentions of specific regularization techniques such as dropout or L2 regularization used to prevent overfitting in the deep learning pipeline. However, there are some strategies mentioned that can help mitigate overfitting.

Firstly, in the training of Model-1 and Model-2, an early stopping technique was applied. This method is used to stop training when the model's performance on the validation set starts to degrade, preventing the model from continuing to learn from the training data and overfitting to it.

Secondly, Model-2 was initialized with pre-trained Imagenet weights. Using pre-trained weights provides the model with some prior relevant information, which can help the model generalize better to new data and reduce overfitting.

Thirdly, the UNet architecture was employed as the learning model. UNet is based on an encoder-decoder network, where the down-sampling/encoder block encodes the input images into feature representations at multiple levels, capturing the context information from the image. This can help the model learn more robust features that are less prone to overfitting.

While these strategies can help mitigate overfitting, they are not a substitute for explicit regularization techniques such as dropout or L2 regularization. Dropout is a technique where randomly selected neurons are dropped out during training, preventing the neurons from co-adapting too much and reducing overfitting. L2 regularization adds a penalty term to the loss function that penalizes large weights, encouraging the model to learn simpler weights that are less prone to overfitting.

Therefore, while the provided context does not explicitly mention the use of dropout or L2 regularization, there are other strategies used that can help mitigate overfitting. However, it is still best practice to use explicit regularization techniques in addition to these strategies to further reduce the risk of overfitting.
The strategy implemented to monitor the model performance during training involves using two distinct databases, T0 and T1, in two separate training phases. In the first phase, a classification model is built by training a Convolutional Neural Network (CNN) on the initial database, T0. This phase focuses on training the model to accurately classify images into their respective species categories.

Once the initial model is trained, the second phase begins, which focuses on tuning a risk threshold specific to each class (i.e., species) using a second and independent database, T1. This phase aims to transform the standard two-class classification options (correct, wrong) into three options by applying Eqs. (15, 16). These three options are: correct classification, incorrect classification, and a reject option, which is particularly useful when the model is uncertain about its classification decision.

To further monitor the model's performance, a confidence threshold tuning process is implemented. After the first training phase, for each image in the threshold tuning dataset, the classifier outputs a class (species) with the highest classification score, S(X), along with the ground truth, Y, belonging to the same set of species classes. By comparing the ground truth with the classifier's output, the accuracy of the model without post-processing and with post-processing can be calculated.

The post-processing phase is designed to achieve three specific goals, and the accuracy of the model for each species is calculated for each goal. This information can be used to monitor the model's performance and identify areas where improvements are needed.

In summary, the strategy for monitoring model performance during training involves using two distinct databases for training and threshold tuning, implementing a three-option classification system, and calculating the accuracy of the model during post-processing for specific goals. This approach allows for a more nuanced understanding of the model's performance and helps identify areas for improvement.

References:

* Scientific Reports (2020) 10:10972
* Mar. Freshw. Res. 68(10), 1950–1954 (2017)
* Gaston, K. J. (1994) Rarity 1–21. Springer, Dordrecht
* Chow, C. (1970) On optimum recognition error and reject tradeoff. IEEE Trans. Inf. Theory 16(1), 41–46
* Corbière, C., Thome, N., Bar-Hen, A., Cord, M., Pérez, P. (2019) Addressing Failure Prediction by Learning Model Confidence. arXiv e-prints https ://arXiv .org//arXiv :1910.04851
* Cortes, C., DeSalvo, G. & Mohri, M. (2016) Boosting with abstention. Advances in Neural Information Processing Systems
* Geifman, Y. & El-Yaniv, R. (2017) Selective classification for deep neural networks. Advances in Neural Information Processing Systems
* De Stefano, C., Sansone, C. & Vento, M. (2018) To reject or not to reject: That is the question—An answer in case of neural classifiers.
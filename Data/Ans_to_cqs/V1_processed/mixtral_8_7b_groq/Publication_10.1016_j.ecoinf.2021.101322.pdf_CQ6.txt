In the context provided, the preprocessing steps involved before training a deep learning model, specifically a Convolutional Neural Network (CNN), include image normalization, rotation, reflection, and shifting.

Firstly, standard image normalization for CNNs is applied, which involves mapping the 8-bit pixel values to floating-point values between 0 and 1. This normalization step is crucial for the model to learn effectively, as it helps to reduce the range of input values and ensures that all features contribute equally to the learning process.

Additionally, data augmentation techniques are used to increase the size and diversity of the training dataset. These techniques include rotations between 0 and 360 degrees, reflection by the vertical axis, and vertical shifting by +/- 5 pixels. Data augmentation helps to improve the model's ability to generalize to new, unseen data by creating new training examples that are similar to the original ones but with some variations.

Furthermore, the training dataset is randomly split into training, validation, and test sets, containing 70%, 15%, and 15% of the images, respectively. The validation set is used to control and terminate the training process, while final performance metrics are estimated using the test set.

It is also worth noting that transfer learning is used in the training process, where a pre-trained network is adapted towards the task at hand. In this case, the EfficientNet-B4 model, pre-trained on the ImageNet dataset, is used as the starting point for the training process. This technique is especially useful when datasets are sparse and narrowly distributed, as it allows the model to leverage the knowledge gained from a large and diverse dataset.

Therefore, the preprocessing steps involved before training a deep learning model in the provided context include image normalization, data augmentation, and dataset splitting, as well as the use of transfer learning.
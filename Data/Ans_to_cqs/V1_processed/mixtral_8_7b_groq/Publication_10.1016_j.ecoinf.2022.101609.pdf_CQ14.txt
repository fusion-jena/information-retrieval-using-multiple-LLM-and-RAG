The hyperparameters of the model are optimized using Bayesian optimization. This information is derived from the context provided, which states that "automated hyperparameter tuning was implemented using bayesian optimization."

Bayesian optimization is a sequential design strategy for global optimization of black-box functions. It is particularly well-suited for expensive-to-evaluate objective functions, which is often the case in machine learning where evaluating a model's performance can require significant computational resources.

Unlike grid search or random search, which evaluate the objective function at a fixed set of points, Bayesian optimization constructs a probabilistic model of the objective function and uses this model to make informed decisions about where to evaluate the function next. This approach can lead to more efficient optimization, requiring fewer function evaluations to find a good set of hyperparameters.

However, the context does not provide any details about the specific hyperparameters that were optimized or the ranges over which they were optimized. It also does not provide any information about the performance of the optimization process or how it affected the overall performance of the model.
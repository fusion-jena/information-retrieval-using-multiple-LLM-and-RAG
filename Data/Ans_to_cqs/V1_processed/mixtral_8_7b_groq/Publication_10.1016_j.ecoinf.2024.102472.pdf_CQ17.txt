As for deep learning pipelines, dropout is a commonly used regularization technique. Dropout involves randomly dropping out, or deactivating, a certain percentage of neurons in a layer during training. This helps to prevent overfitting by reducing the co-adaptation of neurons and promoting the learning of more robust features. L2 regularization is also commonly used in deep learning pipelines, where it is applied to the weights of the neurons in the model.
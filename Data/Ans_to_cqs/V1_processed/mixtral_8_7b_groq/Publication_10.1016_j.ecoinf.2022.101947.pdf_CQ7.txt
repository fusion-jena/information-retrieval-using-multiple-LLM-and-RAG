The criteria used to split the data for deep learning model training, based on the provided context, appear to be related to the size of the test set relative to the training set. Specifically, the optimal classification accuracy was obtained when the test set size was smallest relative to the training set. This suggests that a larger training set and a smaller test set may be beneficial for achieving higher accuracy with this particular model.

Additionally, the context suggests that the predicted value for the test set size should be close to the actual test set size. Specifically, up to a ratio of 32:12 (test:training), the predicted mean for each test:training set size was close to the actual test size. However, after a ratio of 20:24, the amount of variation in the sequential trials increased, potentially resulting in reduced algorithm accuracy. This suggests that maintaining a test:training ratio of 20:24 or better may be important for achieving accurate and consistent results.

The context also indicates that the data was validated using holdback trials, where the test set size was plotted against itself, against the predicted value for the test size, and against the mean predicted value for each test size. This approach allowed for the examination of how the predicted outcome compared with the known test set size, and helped to determine the optimal number of footprint trails required for the algorithm training set and test sets.

Overall, it appears that the criteria used to split the data for deep learning model training in this context are related to the size of the test set relative to the training set, the predicted value for the test set size, and the use of holdback trials for data validation.
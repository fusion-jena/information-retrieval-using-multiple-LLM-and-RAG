Based on the provided context, there is no explicit information given about how the hyperparameters of the model were optimized, such as through methods like grid search or random search.

However, we do know that the optimizer used for training the models was Stochastic Gradient Descent (SGD) with a learning rate of 0.5 and momentum of 0.9. Additionally, the models were fine-tuned in 10 iterations to adjust the weights of the top layers to better fit the leaf dataset.

For feature extraction, the bottleneck features of the pre-trained models were used, which were more generic and not specialized to the original task the models were trained on. A dropout layer with a dropout rate of 0.2 was added to reduce overfitting, and a top classifier softmax layer was added for predicting the class for an input image.

The models were trained and fine-tuned on a Lenovo laptop with an 8265U CPU at 1.80 GHz of Intel Core i5, 8 GB of RAM, and a Windows 10 64-bit system. The software tools used included Annaconda3 Jupyter Notebook with Python 3.8, where the Tensorflow, OpenCV-python3, and Keras libraries were used.

Therefore, while the specific method of hyperparameter optimization is not mentioned, we do have information about the specific optimizer and learning rate used, as well as the fine-tuning process.
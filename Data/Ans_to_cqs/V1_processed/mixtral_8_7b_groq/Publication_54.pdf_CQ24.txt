Based on the provided context, there is no explicit mention of a specific strategy used to handle randomness in the deep learning pipeline, such as setting a random seed value. However, there are several techniques mentioned that involve the use of randomness in certain aspects of the process.

Firstly, the training dataset is split randomly into 3,132 images for training and 760 images for testing. This random split helps ensure that the model is not overfitting to a specific subset of the data and can generalize well to unseen data.

Secondly, the training dataset is augmented 10 times, resulting in a total of 31,320 images. Data augmentation is a technique that involves applying random transformations (e.g., rotation, scaling, flipping) to the existing dataset to create new synthetic samples. This approach helps increase the size of the training dataset, reduce overfitting, and improve the model's ability to generalize.

Thirdly, during the training process, the mini-batch gradient descent algorithm is used with a batch size of 32. This means that the model processes 32 images at a time, and the weights are updated after each batch. The randomness in this case comes from the order in which the images are selected for each batch.

Lastly, the Inception-ResNet-v2 model is trained using the Adam optimizer with an exponential decay rate of first-moment estimation of 0.9. The Adam optimizer uses a combination of the gradient descent algorithm and the concept of momentum, which involves maintaining a moving average of historical gradients. The exponential decay rate of 0.9 implies that the contribution of historical gradients will decrease exponentially over time, introducing a form of randomness in the learning process.

In summary, while the context does not explicitly mention a strategy for handling randomness using a random seed value, it does describe techniques that involve the use of randomness in the data splitting, data augmentation, batch processing, and optimization processes.
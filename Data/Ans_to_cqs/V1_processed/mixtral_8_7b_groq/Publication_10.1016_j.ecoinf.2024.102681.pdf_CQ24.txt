The provided context does not directly discuss strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention the use of a Convolutional Neural Network (CNN) and a hybrid CNN-LSTM model for predicting drought propagation time and handling the complexity of drought-related factors. These models implicitly use strategies to manage randomness during training.

In deep learning, randomness can be introduced in various ways, including the initialization of weights, the order of training samples, and dropout regularization. Managing randomness is crucial for reproducibility and ensuring that the model's performance is not influenced by uncontrolled factors.

In the context of the CNN model, randomness can be managed through proper initialization of weights and biases. Common initialization techniques include Xavier initialization and He initialization, which aim to maintain a balanced distribution of initial weights to avoid the vanishing or exploding gradient problem. Moreover, the order of training samples can be shuffled randomly before each epoch to ensure that the model does not learn a specific order of the data.

In the case of the hybrid CNN-LSTM model, the authors mention that they embedded a Long Short-Term Memory (LSTM) network into the CNN without adjusting the hyperparameters. This implies that the random initialization of weights and biases in the LSTM component, as well as the order of processing time series data, can also affect the model's performance.

While the context does not explicitly mention the use of a random seed value, it is generally considered a best practice in deep learning to set a fixed random seed value for weight initialization, shuffling training samples, and other sources of randomness. This ensures reproducibility and allows researchers to compare results consistently.

In summary, while the provided context does not directly discuss strategies for handling randomness in the deep learning pipeline, it is evident that models like CNN and CNN-LSTM manage randomness through proper weight initialization, shuffling training samples, and potentially other techniques. Setting a fixed random seed value is a recommended practice for ensuring reproducibility and consistency in deep learning models.
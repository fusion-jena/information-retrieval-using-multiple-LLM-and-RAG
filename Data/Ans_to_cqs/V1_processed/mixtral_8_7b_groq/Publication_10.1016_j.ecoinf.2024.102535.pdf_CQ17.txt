The provided context discusses two regularization methods used to prevent overfitting in the deep learning pipeline: early stopping and dropout.

Early stopping is a procedure that stops the training process once the validation loss stops improving for a certain number of epochs, called the patience parameter. In the context, the patience parameter was set to 10, meaning that if the validation loss did not improve for 10 consecutive epochs, the training would be stopped to prevent overfitting.

Dropout is another regularization technique used to prevent overfitting. In the context, dropout was applied to all extracted patches, and it refers to the number of neurons randomly turned off during each training inference. Specifically, the number of neurons randomly turned off during each training inference is mentioned, but the dropout rate (the proportion of neurons to drop) is not specified.

The context does not mention L2 regularization, which is another common regularization technique used to prevent overfitting. L2 regularization adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This penalty term discourages the model from learning overly large weights, which can lead to overfitting.

In summary, the provided context discusses two regularization methods used to prevent overfitting in the deep learning pipeline: early stopping and dropout. However, it does not mention L2 regularization.
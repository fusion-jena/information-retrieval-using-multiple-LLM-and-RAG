The text provided does not give a specific strategy that was implemented to monitor the model performance during training. However, it does mention that the model's performance is likely to continue to improve as more training data are made available and new model advancements are evaluated and modified. It also encourages prospective AI users to carry out similar evaluations to gain more relevant information on model performance specific to their study systems. The authors anticipate that the minimum detection size limits they identified are more generalizable to different study systems provided the model is well trained on a particular object class. Therefore, it can be inferred that the performance of the model was likely monitored by evaluating its ability to detect objects of a particular size and class, and by identifying any factors that may decrease its overall performance, such as complex vegetation, poor lighting, low image resolution, and unique camera angles. Additionally, the authors have provided a table with various performance measures, such as True Negative Rate (TNR), False Positive Rate (FPR), False Negative Rate (FNR), Positive Predictive Value (PPV), and Negative Predictive Value (NPV), which may have been used to assess the model's performance during training. However, the specific strategy for monitoring the model performance during training is not provided in the text.
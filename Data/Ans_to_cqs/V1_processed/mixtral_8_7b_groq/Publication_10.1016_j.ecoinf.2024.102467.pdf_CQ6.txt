Before training a deep learning model, several preprocessing steps are typically involved to ensure the data is in a suitable format for the model. These steps can include normalization, scaling, and cleaning.

Normalization is the process of adjusting the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. This is often necessary because neural networks can be sensitive to the scale of input features. One common normalization technique is to rescale the values in each feature to a common range, such as 0 to 1.

Scaling is similar to normalization, but it involves adjusting the values to a common unit of measurement. For example, if a dataset includes weights measured in pounds and kilograms, scaling would involve converting all weights to a common unit, such as kilograms.

Cleaning the dataset is an important step to ensure that the data is accurate and free of errors. This can involve removing or correcting missing or invalid data, removing outliers, and dealing with inconsistencies in the data. For example, if a dataset includes dates, cleaning might involve ensuring that the dates are in a consistent format and that there are no missing or invalid dates.

In addition to these preprocessing steps, other techniques may be used depending on the specific dataset and deep learning model. For example, data augmentation techniques such as rotation, flipping, and zooming can be used to increase the size of the training dataset and improve the model's ability to generalize.

In the context of the provided pieces of context, the EfficientNetV2 network uses an improved progressive learning approach that dynamically adjusts the regularization method based on the training image size. This method can improve the training speed and accuracy of the network. Furthermore, BoTNet replaces the 3 × 3 spatial convolutions in ResNet’s last three bottleneck blocks with multi-head self-attention (MHSA) to reduce the network depth and improve the model’s performance. However, the context does not mention any specific preprocessing steps used for training these models.

References:

Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.

Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., & Chen, L.-C. (2018). Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the European Conference on Computer Vision (pp. 4510–4520).

Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., & Vaswani, A. (2021). Bottleneck transformers for visual recognition. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 16519–16529.

Tan, M., & Le, Q. (2019). Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (pp. 6105–6114).

Tan, M., & Le, Q. (2021). Efficientnetv2: Smaller models and faster training. In International Conference on Machine Learning (pp. 10096–10106).

The State of World Fisheries and Aquaculture, 2022. FAO. <https://doi.org/10.4060/cc0461en>.

Wang, C.-Y., Bochkovskiy, A., & Liao, H.-Y.M. (2022a). YOLOv7: Trainable Bag-of-Freebies.
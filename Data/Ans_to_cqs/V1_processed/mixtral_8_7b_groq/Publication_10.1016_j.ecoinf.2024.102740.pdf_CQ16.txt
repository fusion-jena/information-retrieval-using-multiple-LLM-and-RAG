The provided context does not give specific information about the criteria used to determine when training is complete in the study. However, it does mention that for machine learning (ML) training, 34 ground data points were used and the Random Forest (RF) algorithm generates an internal estimate of the generalization error by the "out-of-bag" (OOB) samples. The OOB error is calculated by averaging the errors of each tree's OOB samples, providing an unbiased estimate of the model's generalization error.

In the context of RF, the most accurate results were obtained when the maximum tree depth was set to 10 and the minimum number of samples was set to 2. However, it does not specify how the optimal values for these hyperparameters were determined or what criteria were used to assess training completion.

In general, ML training is considered complete when the model's performance on a validation set stops improving or when the validation loss plateaus. This is typically assessed using various metrics, such as accuracy, precision, recall, or F1 score for classification tasks, or mean squared error, mean absolute error, or R-squared for regression tasks. The choice of metric depends on the specific problem and the business objective.

In summary, while the provided context does not explicitly mention the criteria used to determine when training is complete, it is common practice in ML to consider training complete when the model's performance on a validation set stops improving or when the validation loss plateaus. The specific criteria used would depend on the problem and the business objective.
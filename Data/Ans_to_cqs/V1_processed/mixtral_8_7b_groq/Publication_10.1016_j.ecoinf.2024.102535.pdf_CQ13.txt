The provided context does not give specific information about the values of the hyperparameters used in the deep learning model, such as the learning rate or the optimizer. However, it does mention that the models were trained on patches of data, and that various techniques were used to improve the generalization capacity of the classifiers, such as data augmentation and using self-supervised methods. It is likely that the learning rate and other hyperparameters were determined through experimentation and tuning, as is common in the training of deep learning models. Common choices for optimizers include stochastic gradient descent, Adam, and RMSprop. The learning rate controls the size of the steps the model takes during training, and is an important hyperparameter to set correctly, as a learning rate that is too high can cause the model to converge too quickly to a suboptimal solution, while a learning rate that is too low can cause the training process to be very slow.
Based on the provided context, the specific regularization methods used to prevent overfitting in the deep learning pipeline are not mentioned. However, the text does describe the training of deep learning models using the ResNet-50 architecture, which is a 50-layer deep convolutional neural network (He et al., 2015).

The context does mention that preliminary trials showed that region-specific models performed better, and that two-class models were trained for Finnmark and Yamal using the keras package in R with a TensorFlow backend (Allaire and Chollet, 2023; Allaire and Tang, 2023). The training was done with 55 epochs and a batch size of 64, using a one-cycle learning rate policy with a minimum of 0.000001 and a maximum of 0.001 (Smith, 2018).

While these details provide insights into the training process, they do not explicitly mention any regularization techniques like dropout or L2 regularization. Dropout is a regularization technique that randomly sets a fraction of input units to 0 during training to prevent over-dependence on individual neurons and reduce overfitting (Srivastava et al., 2014). L2 regularization, also known as weight decay, adds a penalty term to the loss function, which is proportional to the sum of the squares of all model weights, encouraging smaller weights and reducing overfitting (Goodfellow et al., 2016).

Given the absence of explicit information on regularization methods, it is possible that the authors relied on other techniques such as data augmentation, early stopping, or architecture modifications to mitigate overfitting. Nonetheless, without specific information on regularization techniques, it is not possible to provide a definitive answer.

Answer length: 335 words.

References:

Allaire, J., & Chollet, F. (2023).
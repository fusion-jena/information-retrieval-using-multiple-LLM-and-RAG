Based on the provided context, there is no specific information about the criteria used to split the data for training deep learning models. The context focuses on introducing various datasets and models used in different studies, but it does not mention the data splitting process.

In general, when training deep learning models, it is common to split the dataset into three subsets: training, validation, and testing.

1. Training set: This is the portion of the data used to train the model. The model learns patterns from this data and adjusts its parameters to minimize the error or loss.

2. Validation set: This set is used during the training process to validate the model's performance and adjust hyperparameters. It helps to prevent overfitting by providing an unbiased evaluation of the model's performance.

3. Testing set: After training and validating the model, the testing set is used to provide an independent evaluation of the model's performance on unseen data.

The criteria for splitting the data can vary depending on the specific problem and dataset size. Common approaches include:

- Random splitting: The data is randomly divided into the three subsets with a predefined ratio (e.g., 70% for training, 15% for validation, and 15% for testing).

- Stratified splitting: When dealing with imbalanced or categorical data, it is important to maintain the same distribution of categories in each subset. Stratified sampling ensures that each subset has a similar proportion of samples from each category.

- Time-based splitting: In some applications, such as time-series forecasting, it is more appropriate to split the data based on time. The training set consists of older data, the validation set includes more recent data, and the testing set contains the most recent data.

Without specific information from the provided context, it is not possible to determine the exact criteria used for splitting the data in these studies.
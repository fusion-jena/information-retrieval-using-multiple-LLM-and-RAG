The provided context does not contain direct information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does discuss the use of deep learning algorithms and social media data in various studies. Regularization methods like dropout and L2 regularization are commonly used techniques to prevent overfitting in deep learning models.

Dropout is a regularization technique used in neural networks during training to prevent overfitting. It works by randomly dropping out, or "setting to zero," a number of output features of the layer during the training process. This helps to make the model more robust and less dependent on the training data.

L2 regularization, also known as weight decay, is another technique used to prevent overfitting. It works by adding a penalty term to the loss function, which is proportional to the square of the magnitude of the weights. This encourages the model to have smaller weights, which in turn makes the model less sensitive to the training data.

These techniques are widely used in deep learning pipelines, including those that use social media data and other forms of big data. However, the specific context provided does not mention any regularization methods used in the studies discussed.
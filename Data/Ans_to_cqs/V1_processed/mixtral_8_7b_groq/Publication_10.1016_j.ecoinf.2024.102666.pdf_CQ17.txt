The provided context discusses various topics related to artificial neural networks (ANN) and atmospheric correction (AC) in the context of aquatic environments, but it does not explicitly mention regularization methods used to prevent overfitting in the deep learning pipeline. However, I can provide some general information about these regularization techniques.

Dropout is a regularization technique used in deep learning to prevent overfitting. During training, dropout randomly sets a fraction of input units to 0, effectively making the network smaller and less prone to overfitting. This helps the model generalize better to unseen data.

L2 regularization, also known as weight decay, is another technique used to prevent overfitting. It adds a penalty term to the loss function, which is proportional to the square of the magnitude of the model's weights. This encourages the model to have smaller weights, reducing the complexity of the model and making it less prone to overfitting.

These techniques are widely used in deep learning pipelines, including those involving artificial neural networks for atmospheric correction or other remote sensing applications. However, the provided context does not mention the use of these specific regularization techniques.
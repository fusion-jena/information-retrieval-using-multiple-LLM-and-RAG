The hyperparameters of the Artificial Neural Network (ANN) model in the study are optimized using a trial-and-error method and not through techniques such as grid search or random search. The number of nodes in the hidden layer was varied between 5 and 23, and the learning rate was varied from 0.01 to 1.0 in increments of 0.05. For each configuration, the mean square error (MSE) between the model output and the measured data was calculated. The optimal number of neurons in the hidden layer and the optimal learning rate having the maximum model performance as indicated by MSE were then selected.

In addition to the trial-and-error method, an early stopping technique was employed to avoid overfitting. The data set was randomly split into two sets, with 80% used for model training and 20% for model testing. The model weights were randomly initialized, and the training process was stopped when the network began to overfit the data, i.e., the error on the validation set increased.

It is worth noting that while grid search and random search are popular methods for hyperparameter tuning, they may not always be the best approach. The trial-and-error method used in this study has been found to be effective in determining the appropriate number of neurons and learning rate (Shamseldin, 1997; Hill and Minsker, 2010). However, it can be time-consuming and computationally expensive, especially for large and complex models. Therefore, other methods such as Bayesian optimization and evolutionary algorithms can also be considered for hyperparameter tuning in ANN models.
The provided context discusses two main optimization techniques used in the deep learning pipeline, but they are not specific optimization algorithms such as SGD or Adam. Instead, the techniques described are related to the efficient exploration of hyperparameter space and the architecture of the deep learning model itself.

The first optimization technique mentioned is Hyperband, a method that aims to find the best hyperparameters for a model by exploring several configurations in parallel while allocating more training resources to promising configurations. Hyperband combines an initial random search with a 'successive halving' strategy to quickly identify promising configurations while eliminating those that show inferior performance. This technique is useful when computational resources are limited.

The second optimization technique mentioned is related to the architecture of the deep learning model, specifically the Recurrent Highway Network (RHN). The RHN uses gates to control the flow of data between the layers of the network, allowing the network to learn to "ignore" some layers that are not useful for the final prediction. This can facilitate training by allowing gradients to propagate more easily through the layers.

Therefore, the optimization techniques discussed in the provided context are not specific optimization algorithms but rather methods for efficiently exploring hyperparameter space and designing more efficient deep learning model architectures.
Based on the provided context, the following measures were taken to ensure the generalizability of the deep learning model:

1. Data augmentation: A common practice to improve the accuracy of a deep learning model is to do data augmentation. In this study, the authors compared the accuracy of the network when data augmentation was included during training. Specifically, they implemented vertical and horizontal flips in addition to resizing and normalization of the patches, which helped increase the variability of the training data.
2. Hyperparameter tuning: The network was trained with the Adam optimizer, a variant of the stochastic gradient optimization where the learning rate is adjusted automatically. The rest of the parameters, such as the learning rate, number of epochs, and batch size, were set empirically.
3. Dataset extension: The authors suggested that improving the accuracy with a deeper network may tend to memorize the dataset due to the model capacity, causing model overfitting. Therefore, they proposed including more examples in the dataset obtained under different conditions, such as different daylight conditions or different year seasons. This would provide a distinctive context and help improve the generalizability of the model.

However, the context does not explicitly mention the use of cross-validation or stratified splitting to ensure the generalizability of the model. Cross-validation is a technique used to assess the performance of a machine learning model by dividing the dataset into multiple subsets and training and testing the model on each subset. Stratified splitting is a method of splitting the data into training and testing sets while preserving the distribution of labels in each set. These techniques could have been used in this study, but the context does not provide enough information to confirm this.
Based on the provided context, there is no information about optimization techniques such as Stochastic Gradient Descent (SGD) or Adam used in a deep learning pipeline. The context discusses a study that uses machine learning algorithms, specifically LightGBM, Random Forest (RF), k-Nearest Neighbor (KNN), Transfer Regression Forest (TRF), AdaBoost, and eXtreme Gradient Boosting (XGBoost), to classify land cover types using multisource data. However, these are not deep learning models.

In the study, the authors use Recursive Feature Elimination and Cross-Validation (RFECV) to select the optimal features that could reduce the redundancy caused by multisource feature datasets and improve model performance. RFECV is a wrapper approach to feature selection based on a specific estimator, in this case, LightGBM. It recursively eliminates attributes and fits the estimator to find the optimal number of features.

The study also uses a stacking method to combine the base models' prediction results. Stacking involves training a meta-model on the base models' out-of-fold predictions. In this case, the authors use the RF algorithm as the meta-model and the base models' predictions as input.

Therefore, while the study uses various machine learning techniques, it does not mention any optimization techniques used in deep learning pipelines.
Based on the provided context, there is no explicit information about specific postprocessing steps such as saliency maps or confusion matrix calculation after model training. However, it does mention that they use certain metrics for evaluation, specifically focusing on true positive detection, confidence score, intersection over union (IoU), and class prediction.

After training their models, they would calculate the number of true positive detections based on the conditions mentioned. A detection is considered a true positive if it meets the following criteria:
1. The confidence score is greater than a threshold (e.g., 0.5), which indicates that the model is confident in its prediction.
2. The IoU (Intersection over Union) is greater than a threshold (e.g., 0.5), which measures the overlap between the predicted bounding box and the ground truth bounding box.
3. The class prediction is correct, meaning that the model has correctly identified the object class.

These conditions ensure that a detection is reliable, in the right place, and of the correct class before being counted as a true positive.

In addition to true positive detection, they would also calculate other metrics such as precision, recall, and F1-score for a comprehensive evaluation of their models. Precision is the ratio of true positives to the sum of true positives and false positives. Recall is the ratio of true positives to the sum of true positives and false negatives. The F1-score is the harmonic mean of precision and recall, providing a balanced assessment of both measures.

In summary, while the context does not explicitly mention postprocessing steps like saliency maps or confusion matrix calculation, it does mention that they evaluate their models using true positive detection, confidence score, IoU, and class prediction. They also calculate precision, recall, and F1-score for a comprehensive evaluation.
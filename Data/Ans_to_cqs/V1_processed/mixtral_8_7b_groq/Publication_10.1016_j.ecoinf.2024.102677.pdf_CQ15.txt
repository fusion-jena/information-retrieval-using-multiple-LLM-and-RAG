Based on the provided context, the specific optimization techniques used in the deep learning pipeline are not explicitly mentioned. However, it is mentioned that several state-of-the-art CNN architectures were fine-tuned using the training dataset, and their performances were evaluated. This process typically involves using optimization techniques to update the model's parameters during training.

Common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adagrad, Adadelta, RMSProp, and Adam. Among these, Adam has gained popularity due to its adaptive learning rate and ability to handle non-stationary objectives.

Given that the context mentions fine-tuning of pre-trained models, it is likely that a variant of SGD or Adam was used for optimization. However, without explicit information, it is not possible to provide a definitive answer.
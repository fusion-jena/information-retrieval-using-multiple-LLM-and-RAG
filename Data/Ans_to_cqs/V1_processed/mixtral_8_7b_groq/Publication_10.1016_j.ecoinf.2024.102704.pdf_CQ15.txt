The provided context discusses optimization techniques in the context of reducing memory access and model lightweighting for deep learning models, particularly convolutional neural networks (CNNs), rather than focusing on specific optimization algorithms used in the deep learning pipeline. However, it does mention the use of the ResNet-50, Inception V3, and DenseNet models, which typically use optimization algorithms such as Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), or other variants during training.

In the given context, the focus is on optimizing the computational efficiency and memory access of deep learning models. Specifically, Wang et al. (2019) reduced memory access by reconfiguring the computation sequence of 'convolutional layer + batch normalisation + activation layer' on NVI-DIA TESLA V100 GPUs, leading to increases in computational efficiency for ResNet-50, Inception V3, and DenseNet models.

Furthermore, the context highlights the importance of reducing the number of layers in the network model and the number of convolutions and data to accelerate operations across the three main layers of CNNs. This is demonstrated by the training of LigObNet, YOLOv5, YOLOv6, YOLOv7, and YOLOv8 across four scales (n, s, m, and l).

In summary, while the provided context does not explicitly mention optimization algorithms used in the deep learning pipeline, it does discuss optimization techniques for improving computational efficiency and memory access in deep learning models, particularly CNNs. Common optimization algorithms used in deep learning pipelines include SGD, Adam, and other variants.
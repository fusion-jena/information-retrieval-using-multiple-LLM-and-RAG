Based on the information provided, the hyperparameters of the model, including the learning rate, batch size, and number of epochs, were determined after tuning. However, the specific method used to optimize these hyperparameters, such as grid search or random search, is not mentioned in the text.

In general, hyperparameter tuning is an important step in training machine learning models to ensure that they perform optimally. There are various methods for hyperparameter tuning, including grid search, random search, and Bayesian optimization.

Grid search involves systematically trying out different combinations of hyperparameters within a specified range. For example, one might try out different learning rates, batch sizes, and number of epochs in a grid-like manner to find the best combination. However, grid search can be computationally expensive, especially for models with many hyperparameters.

Random search, on the other hand, involves randomly selecting hyperparameters within a specified range. This method can be less computationally expensive than grid search, as it does not try out every possible combination of hyperparameters.

Bayesian optimization is another method for hyperparameter tuning that uses Bayesian inference to select the next set of hyperparameters to try based on previous runs. This method can be more efficient than grid search or random search, as it takes into account the performance of previous runs to inform the selection of future hyperparameters.

Without more information, it is difficult to say for sure how the hyperparameters of the model were optimized in this case. However, it is clear that some form of hyperparameter tuning was performed, as the learning rate, batch size, and number of epochs were determined after tuning.
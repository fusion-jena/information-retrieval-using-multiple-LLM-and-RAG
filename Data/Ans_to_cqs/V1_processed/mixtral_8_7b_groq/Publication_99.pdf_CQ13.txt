The hyperparameters used in the deep learning models include learning rate, optimizer, loss function, batch size, and number of epochs. For the PConv networks, a learning rate of 2e-4 and the Adam optimizer were used. The loss function consisted of per-pixel loss, perceptual loss, style loss, and total variation loss. The batch size was 1, and the models were trained for 50 epochs. These networks were pre-trained on ImageNet.

For the Pix2Pix networks, the learning rate was 9e-5, and the Adam optimizer was used. The loss function was a combination of adversarial loss and L1 loss. The batch size was 1, and the models were trained for 50 epochs.

The classification networks used a learning rate of 0.03, the Adam optimizer, and cross-entropy loss as the loss function. The batch size was 32, and the networks were trained for 20 epochs. These networks were pre-trained on ImageNet.

Additionally, for the classification models, different architectural setups were used, and therefore, different training procedures were applied. For instance, the vgg16 network pre-trained on the ImageNet dataset was adapted for training the classification model. The fully connected layers of the network were replaced with a global max-pooling layer, a dropout layer with a dropout ratio of 0.5 as a network regularizer, and a single hidden layer with a total of 256 units for a larger image-sized model (512 × 512) and 128 units for a smaller image size model (256 × 256). Various data augmentation techniques such as height and width shift, flipping, zooming, and brightness changes were also used.

In summary, the hyperparameters used in the deep learning models depend on the type of network and the specific task. The PConv networks, Pix2Pix networks, and classification networks each had different learning rates, batch sizes, and loss functions. Additionally, the classification models used different architectural setups and data augmentation techniques.
Regularization methods are techniques used in deep learning to prevent overfitting, which occurs when a model learns the training data too well, to the point where it performs poorly on unseen data. Two commonly used regularization methods are dropout and L2 regularization.

Dropout is a regularization technique used during training to prevent over-dependence on any one neuron in the network. At each training iteration, dropout randomly sets a fraction of the neurons in a layer to zero, effectively preventing them from contributing to the forward pass. This helps to promote diversity in the learned representations and reduces overfitting.

L2 regularization, also known as weight decay, is a technique that adds a penalty term to the loss function, which discourages large weights in the model. This penalty term is proportional to the square of the magnitude of the weights, and it is added to the loss function during training. By adding this penalty, L2 regularization encourages the model to learn smaller weights, which helps to reduce overfitting.

In the context provided, the original architecture of Koch et al., (2015) used regularized cross entropy as a loss function, which suggests that some form of regularization was used in their deep learning pipeline. However, the specific regularization methods used are not mentioned.

In addition, the context suggests that using contrastive loss as a loss function may result in better performance than cross entropy for certain tasks. Contrastive loss is a metric learning loss function that takes a pair of samples as input and brings similar samples closer together while pushing dissimilar samples apart. This loss function has been used successfully in several recent papers in machine learning, as well as with Siamese networks, and has been shown to produce outstanding results in unsupervised contrastive learning.

It is also worth noting that preprocessing the input instead of relying on data augmentation is a preferred way with significantly better results in some cases. This suggests that other regularization methods, such as data augmentation, may not always be necessary or effective in preventing overfitting.

In conclusion, dropout and L2 regularization are two commonly used regularization methods in deep learning to prevent overfitting. However, the context provided does not specify the regularization methods used in the original architecture of Koch et al., (2015). Instead, it suggests that using contrastive loss as a loss function may result in better performance for certain tasks, and that preprocessing the input instead of relying on data augmentation may be a more effective regularization method in some cases.
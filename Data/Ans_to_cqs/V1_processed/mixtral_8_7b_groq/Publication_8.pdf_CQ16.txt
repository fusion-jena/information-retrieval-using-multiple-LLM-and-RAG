Based on the provided context, there is no explicit information given about the specific criteria used to determine when training of the model is complete. However, it is mentioned that the model was "trained again on all the annotated scans" and then tested on a different un-annotated dataset. The model's performance was evaluated based on visual inspection, and an average precision metric was calculated by manually annotating around 200 of these scans.

From this context, we can infer that the training process might include some form of validation or evaluation to check the model's performance. However, the criteria for determining when training should be stopped (such as a plateau in validation loss) are not specified. It is possible that the model's performance on the test dataset, as evaluated by visual inspection and the average precision metric, was used as a proxy for determining when training was complete. Still, the specifics of the training termination criteria remain unclear based on the given context.
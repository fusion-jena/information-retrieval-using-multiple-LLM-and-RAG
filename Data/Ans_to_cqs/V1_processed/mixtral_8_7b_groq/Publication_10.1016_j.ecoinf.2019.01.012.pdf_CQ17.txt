The regularization methods used to prevent overfitting in the deep learning pipeline, as described in the provided context, are dropout and data augmentation.

Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps to prevent over-dependence on any single neuron and therefore reduces overfitting (Srivastava et al., 2014). In the context provided, dropout is used during the training of a convolutional neural network (CNN) for ecological image classification. The learning process was stopped after 90 epochs when the learning curve converged and loss values did not decrease further. The validation accuracy exceeding training accuracy is a common effect when dropout is applied on training data only, indicating that the training set contains more difficult tiles than the validation set.

Data augmentation is another regularization technique used in the context, which involves creating modified versions of the original images in the training set by applying random transformations such as rotation, shifting, and flipping. This increases the size of the training set and helps the model generalize better to new, unseen images, thereby reducing overfitting.

However, the context does not mention the use of L2 regularization, which is another common regularization technique used in deep learning to prevent overfitting. L2 regularization adds a penalty term to the loss function, which is proportional to the square of the magnitude of the model's weights. This encourages the model to have smaller weights, which reduces overfitting by making the model less sensitive to the specific values of the training data.

Therefore, based on the provided context, the regularization methods used to prevent overfitting in the deep learning pipeline are dropout and data augmentation. L2 regularization is not mentioned as being used in this particular pipeline.

References:
Clevert, D. A., Unterthiner, T., & Hochreiter, S. (2015). Fast and accurate deep network learning by exponential linear units (ELUs). In Proceedings of the International Conference on Machine Learning (ICML) (pp. 1321-1330).

Kamilaris, A., & Prenafeta-Bold√∫, F. X. (2018). Deep learning in remote sensing: A comprehensive review and challenges. ISPRS Journal of Photogrammetry and Remote Sensing, 147, 158-171.

LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1), 1929-1958.

Weiss, R., Khoshgoftaar, T. M., & Wang, D. (2016). A survey on transfer learning. Journal of Big Data, 3(1), 1-38.
The provided context does not contain specific information about the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization, in the context of the Mohedano et al., 2017 study on object retrieval with deep convolutional neural networks.

The Mohedano et al., 2017 study focuses on object retrieval using deep convolutional neural networks, but it does not provide information on the specific regularization techniques used to prevent overfitting. The study by Cohen and Shashua, 2017, "Inductive bias of Deep Convolutional Networks through Pooling Geometry" also does not mention any regularization methods used in deep learning.

Regularization methods, such as dropout and L2 regularization, are commonly used in deep learning to prevent overfitting by adding a penalty to the loss function, which encourages the model to have simpler weights and avoid memorizing the training data. Dropout randomly sets a fraction of the weights to zero during training, which helps to prevent over-reliance on any single neuron. L2 regularization adds a penalty proportional to the square of the magnitude of the weights, which encourages the model to have smaller weights and avoid overfitting.

Without more information, it is not possible to provide a specific answer to the query regarding the regularization methods used in the Mohedano et al., 2017 study. However, it is common practice to use regularization techniques such as dropout and L2 regularization in deep learning to prevent overfitting.
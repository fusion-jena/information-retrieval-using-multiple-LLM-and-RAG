Based on the provided context, there is no explicit information given about how the hyperparameters of the model were optimized. The text does not mention any methods such as grid search, random search, or Bayesian optimization that are commonly used for hyperparameter tuning.

However, we do know some of the chosen hyperparameter values, including the learning rate α = 10−4, batch size B = 64, number of models M = 5, and the Adam optimizer hyperparameters β1 = 0.9, β2 = 0.999, and ε = 10−8. The learning rate was reduced by a factor of 0.1 when the validation loss did not improve for 15 consecutive epochs. Additionally, weight decay was applied with a magnitude of 10−3, which is inversely proportional to the hyperparameter λ from Eq. (1).

It is possible that these hyperparameter values were determined through a combination of prior knowledge, empirical experimentation, or preliminary testing. However, without further information, it is not possible to definitively answer how the hyperparameters of the model were optimized.
The text provides information on several measures taken to ensure the generalizability of the deep learning model. First, the data was divided into four non-overlapping subsites to maximize the independence between the training and testing sets. This approach helps to assess the generalization power of the networks studied, as it provides a more realistic assessment of how the model will perform on new, unseen data.

Additionally, the authors acknowledge that the conditions studied may have been unfavorable due to the high variation in class distribution between training and testing sets. They suggest that this issue could have been ameliorated by using data augmentation or building datasets that better reflect real percentages. Data augmentation is a technique used to increase the diversity of a dataset by creating new synthetic samples from existing ones. This can help improve the generalizability of the model by exposing it to a wider variety of data during training.

Furthermore, the authors used a "supervised approach" to adjust the parameters of the deep learning model, which involves using annotated data. By training the model on a large and diverse dataset with carefully generated annotations, the authors aimed to ensure that the model could capture statistics and relationships from the data that would be representative of the problem at hand.

Overall, while the text does not explicitly mention techniques such as cross-validation or stratified splitting, it does provide information on several measures taken to ensure the generalizability of the deep learning model. These measures include dividing the data into non-overlapping subsites, acknowledging and addressing issues related to class distribution, using a supervised approach with annotated data, and aiming to capture statistics and relationships from a diverse and representative dataset.
Deep learning models, including those used for image object detection and segmentation, are prone to overfitting, especially when trained on small datasets or when the model has a large number of parameters. Regularization methods are techniques used to prevent overfitting and improve the generalization ability of deep learning models.

Two common regularization methods used in deep learning are dropout and L2 regularization. Dropout is a technique where randomly selected neurons are dropped out during training, which helps to prevent over-reliance on specific neurons and encourages the model to learn more robust features. L2 regularization, also known as weight decay, adds a penalty term to the loss function that is proportional to the square of the magnitude of the model's weights. This encourages the model to have smaller weights, which can help to reduce overfitting.

In the context of the provided articles, Jin et al. (2018) used a deep learning model to segment individual maize plants from terrestrial LiDAR data. While the article does not explicitly mention the use of regularization methods, it is likely that they were used to prevent overfitting. Similarly, Wang et al. (2019a) used a deep learning model to segment individual rubber trees from ground-based LiDAR data, but the use of regularization methods is not mentioned.

In the context of mangrove tree segmentation, Yin and Wang (2019) used traditional MCWS and RG algorithms, which are not deep learning-based methods. Therefore, regularization methods are not applicable in this case.

In summary, dropout and L2 regularization are two common regularization methods used in deep learning to prevent overfitting. While the provided articles do not explicitly mention the use of these methods in the context of image object detection and segmentation, it is likely that they were used to improve the generalization ability of the models.
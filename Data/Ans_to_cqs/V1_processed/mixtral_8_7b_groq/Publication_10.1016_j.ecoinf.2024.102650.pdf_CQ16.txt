The provided context does not give specific information about the criteria used to determine when training is complete in the study. However, it does mention the use of perplexity, which is a common metric used in language modeling to evaluate the performance of a trained model.

Perplexity is a measurement of how well a probability model predicts a sample. It is defined as the exponential of the entropy of the distribution, which measures the average amount of information contained in each word of the test set. A lower perplexity score indicates a better fit of the model to the data.

In the context of topic modeling, perplexity is used to evaluate the performance of a trained LDA (Latent Dirichlet Allocation) model. The LDA model is trained to estimate the topic intensities and word probabilities for each document in the corpus. The perplexity score is then calculated for the test set to evaluate how well the model can predict the topics and words in new, unseen documents.

Therefore, it is possible that the training of the LDA model in the study was determined to be complete when the perplexity score reached a plateau or a minimum value, indicating that the model was no longer improving its ability to predict the topics and words in the test set. However, this is just an assumption based on the available context and the common practice of using perplexity in topic modeling. The specific criteria used to determine when training was complete in the study are not provided in the context.
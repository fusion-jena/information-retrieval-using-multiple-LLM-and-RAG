The strategy implemented to monitor the model performance during training involves the use of validation accuracy and early stopping strategy. Specifically, the maximum number of epochs for training was set to 100, but the training was stopped before that if the validation accuracy stopped improving. Moreover, the initial learning rate was set to 1 × 10^−16, and it was monitored after every epoch to check the validation loss. If the validation loss did not decrease after a certain number of epochs, the learning rate was randomly decreased to 1 × 10^−10. The "Adam" optimizer and "Categorical Cross Entropy" loss were used for training all deep learning models. The input size for all the DL models was 256 × 256 × 3, and the batch size was set to 32 due to the capability of the computing device. These strategies helped in improving the model's performance and preventing overfitting.
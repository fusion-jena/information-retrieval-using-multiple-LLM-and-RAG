The optimization techniques applied in the deep learning pipeline, as mentioned in the provided context, include Stochastic Gradient Descent (SGD) and Adam.

Stochastic Gradient Descent (SGD) is used to estimate the unknown parameters of the cost function. It is a popular optimization algorithm for training neural networks due to its simplicity and efficiency. SGD updates the parameters in the direction of the negative gradient of the cost function concerning a single training example, or a small batch of training examples, at each iteration. This approach allows SGD to converge faster than batch gradient descent, which uses the entire training dataset to compute the gradient at each iteration.

Adam is another optimization algorithm used in the deep learning pipeline. It is an adaptive learning rate method that combines the advantages of two other optimization algorithms: AdaGrad and RMSProp. Adam adjusts the learning rate for each parameter individually based on the estimated first and second moments of the gradient. This approach allows Adam to adapt to the geometry of the cost function and converge faster than SGD in some cases.

In the provided context, Adam is used to optimize the network parameters during the training of the DMF model. The training is done for 100 epochs with a batch size of 64 samples. During the training, 10% of the data serves as the validation set to check for overfitting. A min-max scaler is used to compute the performance metrics in the test set.

In summary, the optimization techniques used in the deep learning pipeline include Stochastic Gradient Descent (SGD) and Adam. These algorithms are used to estimate the parameters of the cost function and optimize the network parameters during the training of deep learning models.
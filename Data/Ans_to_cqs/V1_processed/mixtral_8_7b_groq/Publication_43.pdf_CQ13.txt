The hyperparameters used in the deep learning models mentioned in the context, such as ResNet and GoogLeNet, include the learning rate and momentum weight. The learning rate determines the step size at each iteration while moving toward a minimum of the loss function. A smaller learning rate means slower learning, but it is less likely to overshoot the minimum. On the other hand, a larger learning rate means faster learning, but it is more likely to overshoot the minimum. The momentum weight, denoted by µ, is used in the Stochastic Gradient Descent (SGD) update rule to provide a kind of "inertia" that helps the learning process.

The specific learning rate and momentum weight used in the work described in the context are not provided. However, it is mentioned that the Stochastic Gradient Descent (SGD) optimizer is used to update the weights. The weights are updated using the following rule:

wt+1 = µwt − α∆J(wt)

where wt represents the weights at iteration t, α is the learning rate, and ∆J(wt) is the gradient of the loss function with respect to the weights at iteration t.

In addition to the learning rate and momentum weight, other hyperparameters that can be used in deep learning models include the number of epochs, batch size, weight decay, and type of optimizer. However, these hyperparameters are not mentioned in the provided context.

It is also worth noting that the authors used pre-trained weights of ImageNet to initialize the models and fine-tune them for their specific task. Fine-tuning is a common practice in deep learning, where a pre-trained model is further trained on a new dataset to adapt the model to the new task.

In summary, the hyperparameters used in the deep learning models mentioned in the context include the learning rate and momentum weight. The authors used the Stochastic Gradient Descent (SGD) optimizer to update the weights with a learning rate and momentum weight that are not specified in the context. The authors also used pre-trained weights of ImageNet to initialize the models and fine-tune them for their specific task.
In the deep learning pipeline for solar-powered bioacoustic tuning fork (SBTF) call recognition, two main datasets were used: the training/validation dataset and the test dataset. The training/validation dataset consisted of 6363 hours of unlabelled audio data recorded from ten independent sites using solar-powered bioacoustic recorders and Audiomoth recorders. These recorders were set to record 10-minute files continuously between 6 am and 6 pm daily, resulting in a total of 375,780 audio files at a sampling rate of 44.1 kHz for solar-powered bioacoustic recorders and 32 kHz for Audiomoth recorders. The test dataset, on the other hand, comprised 2735 hours of audio data from nine independent sites that were different from the training/validation dataset. This dataset included 159,620 audio files recorded at a sampling rate of 44.1 kHz for solar-powered bioacoustic recorders and 32 kHz for Audiomoth recorders.

It is important to note that all audio data were unlabelled at the commencement of the study. The training/validation dataset was used to train and improve the convolutional neural network (CNN) model using an active learning approach, while the test dataset was used to evaluate the final model's performance. The training/validation dataset was divided into training and validation sets, with the validation set used to tune hyperparameters such as the number of epochs, batch size, and learning rate. A grid search technique was used to find the optimal hyperparameters for the final model.

The dataset used in this study is specific to SBTF call recognition and is not a standard dataset like MNIST, CIFAR, or ImageNet. However, the approach and techniques used in this study can be applied to other datasets and problems in the field of bioacoustics and machine learning.
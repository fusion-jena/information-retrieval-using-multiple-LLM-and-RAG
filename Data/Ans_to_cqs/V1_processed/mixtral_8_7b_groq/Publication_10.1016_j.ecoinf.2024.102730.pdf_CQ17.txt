The regularization methods used to prevent overfitting in the deep learning pipeline, as described in the provided context, include L2 regularization. L2 regularization, also known as weight decay, adds a penalty to the loss function that is proportional to the square of the magnitude of the weights. This encourages the model to keep the weights small, reducing the complexity of the model and helping to prevent overfitting.

In the study "Hyperparameter-tuned batch-updated stochastic gradient descent: plant species identification by using hybrid deep learning" by Barhate et al., 2023, the authors used L2 regularization as one of the hyperparameters in their deep learning pipeline for plant species identification. They explored different ranges of values for the L2 regularization term and reported the optimal value for each of the three different regression tasks in their study.

It is worth noting that the context does not mention the use of other regularization methods such as dropout, which is a popular regularization technique in deep learning. Dropout randomly sets a fraction of the input units to zero during training, effectively preventing the co-adaptation of the neurons and reducing overfitting.

In summary, L2 regularization is used in the deep learning pipeline described in the provided context to prevent overfitting. However, other regularization techniques such as dropout are not mentioned.
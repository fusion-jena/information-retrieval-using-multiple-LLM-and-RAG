According to the provided context, the optimization technique applied in the deep learning pipeline is the Adam algorithm, which is a stochastic gradient descent method. There is no mention of SGD (Stochastic Gradient Descent) being used in this specific pipeline.
The provided context does not specifically mention the criteria used to split the data for deep learning model training. However, it does mention that 80% of the data was allocated as test files and the remaining 20% as validation files using the createDataPartition() function from the "caret" package. This suggests that the data was split into training and validation sets, but the specific criteria used for this split are not mentioned.

It's worth noting that the context does not mention deep learning models specifically, but rather mentions other machine learning models such as random forest, kNN, SVM, and ANN. The data splitting approach mentioned may be applicable to these models, but it's not clear if the same approach would be used for deep learning models.

In general, the criteria used to split data for deep learning model training can vary depending on the specific problem and dataset. Common approaches include random splitting, stratified splitting, and time-based splitting. However, without more information, it's not possible to determine the specific criteria used in this case.
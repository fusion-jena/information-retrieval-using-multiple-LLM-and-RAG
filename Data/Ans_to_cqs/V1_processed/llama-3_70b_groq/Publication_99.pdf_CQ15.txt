According to the provided context, the optimization techniques applied in the deep learning pipeline are Adam optimizer. This is mentioned in multiple sections of the context, including the training of PConv networks, Pix2Pix networks, and Classification networks. Specifically, it is stated that "Adam optimizer" is used with a learning rate of 0.0002 for PConv networks, 9e-5 for Pix2Pix networks, and 0.03 for Classification networks. There is no mention of SGD or any other optimization techniques being used in the pipeline.
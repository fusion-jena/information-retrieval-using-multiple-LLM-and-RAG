The provided context mentions one regularization method used to prevent overfitting in deep learning, which is dropout. This is mentioned in the paper by Srivastava et al. (2014) titled "Dropout: a simple way to prevent neural networks from overfitting." 

However, the context does not explicitly mention L2 regularization as a method used in the deep learning pipeline. It does mention "applied regularization techniques" as one of the hyper-parameters evaluated iteratively, but it does not specify which techniques were used.

Therefore, based on the provided context, we can confirm that dropout is one of the regularization methods used to prevent overfitting, but we cannot confirm whether L2 regularization was used or not.
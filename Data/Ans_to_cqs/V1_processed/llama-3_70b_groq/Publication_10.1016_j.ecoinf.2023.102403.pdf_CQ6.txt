Based on the provided context, it appears that the preprocessing steps involved before training a deep learning model are related to data curation and filtering. The context mentions that the authors downloaded 71,670,596 records from GBIF and OBIS, but only 820,004 records were useful for their analyses after applying a series of filters.

The preprocessing steps mentioned in the context include:

1. Data curation following Zizka et al. (2020) to minimize errors associated with public usage of GBIF and OBIS repositories.
2. Filtering the dataset by columns labeled "scientific name", "family", "year", "longitude", and "latitude".
3. Removing records with NA in these columns.
4. Removing all duplicate records with identical latitude and longitude.

However, the context does not explicitly mention normalization, scaling, or other common preprocessing steps typically involved in deep learning model training. Therefore, it is unclear whether these steps were performed or not.

In summary, the preprocessing steps mentioned in the context are primarily related to data cleaning and filtering, but it is unclear whether other common preprocessing steps were performed.
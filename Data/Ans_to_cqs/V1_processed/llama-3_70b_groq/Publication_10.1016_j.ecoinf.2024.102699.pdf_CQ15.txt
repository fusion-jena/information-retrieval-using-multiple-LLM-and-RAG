The optimization technique applied in the deep learning pipeline is the Adam optimizer. This is mentioned in the text as part of the training setup: "Our training setup includes the Adam optimizer (Kingma and Ba, 2015) with a concatenating cosine-annealing linear scheduler with an initial learning rate of 0.0001, decaying by a factor of 0.001â‹…learning rate, and a batch size of 32." There is no mention of SGD or any other optimization techniques being used.
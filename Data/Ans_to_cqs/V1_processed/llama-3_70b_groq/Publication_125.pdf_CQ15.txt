Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the architecture of different CNN models (VGG16, ResNet-50, Xception), data augmentation, and execution times for training and prediction. It does not provide information about the optimization techniques used during the training process. Therefore, I don't know the answer to this question based on the provided context.
According to the provided context, the optimization technique applied in the deep learning pipeline is stochastic gradient descent (SGD) with momentum. This is mentioned in the sentence: "For training, we used the stochastic gradient descent with momentum optimizer with default settings except for the initial learning rate (which we set to 0.001) and the maximum number of epochs (which we set to 15 after preliminary tests)." There is no mention of Adam optimizer in the provided context.
According to the provided context, the optimization technique applied in the deep learning pipeline is SGD (stochastic gradient descent). Specifically, it is mentioned that "We used the SGD (stochastic gradient descent) optimizer with an initial learning rate of 0.01 for all models except MansNet, which was initially set at 0.1." There is no mention of Adam or any other optimization techniques being used.
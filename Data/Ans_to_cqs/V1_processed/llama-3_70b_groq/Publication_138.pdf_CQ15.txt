Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD, Adam, or others. The context primarily focuses on the architecture of different neural networks (e.g., ICSNet, LeNet, AlexNet, GoogLeNet, VGG, ResNet) and their performance on a particular task. It also mentions the experimental setup, including the operating system, GPU, and some hyperparameters (e.g., number of epochs, learning rate drop factor, batch size), but it does not provide information about the optimization techniques used. Therefore, I don't know the answer to this question based on the provided context.
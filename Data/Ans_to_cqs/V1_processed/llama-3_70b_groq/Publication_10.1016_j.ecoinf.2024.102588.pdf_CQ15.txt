The optimization technique applied in the deep learning pipeline is the Adam optimizer with a learning rate of 0.001. This is mentioned in the context where the ANN model is described, stating that "The loss function and optimizer were set as the mean squared error and Adam optimizer, respectively, at a learning rate of 0.001." There is no mention of other optimization techniques like SGD being used in the deep learning pipeline.
Unfortunately, the provided context does not explicitly mention the criteria used to determine when training is complete. The context discusses the use of hand-annotated data, self-supervised models, and semi-supervised frameworks, but it does not provide information on the specific criteria used to determine the completion of training.

However, it is worth noting that the context does mention the removal of boxes with confidence scores less than 0.2, which suggests that confidence scores may play a role in evaluating the model's performance during training. Additionally, the context mentions the maintenance of boxes with superior predicted scores, which could imply that the model's performance is being evaluated based on its ability to make accurate predictions.

Despite these hints, the context does not provide a clear answer to the question of what criteria are used to determine when training is complete. Therefore, I must conclude that I don't know the answer to this question based on the provided context.
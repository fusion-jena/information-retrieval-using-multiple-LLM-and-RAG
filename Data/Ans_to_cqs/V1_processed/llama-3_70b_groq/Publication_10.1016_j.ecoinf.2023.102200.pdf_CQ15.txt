Based on the provided context, the optimization technique applied in the deep learning pipeline is Stochastic Gradient Descent (SGD) with momentum. This is mentioned in the text: "During the detector operation, the Faster R-CNN network with a Resnet-101 backend was trained for 20,000 iterations using Stochastic Gradient Descent (with momentum), with a learning rate of 0.003, momentum of 0.9, and a batch size of 1." There is no mention of the Adam optimization technique in the provided context.
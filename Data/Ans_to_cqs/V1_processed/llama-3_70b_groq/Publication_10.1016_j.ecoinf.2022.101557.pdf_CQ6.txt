The provided context does not explicitly mention the preprocessing steps involved before training a deep learning model, such as normalization, scaling, or cleaning. The context only mentions that the data is split into larger blocks for cross-validation and then into smaller areas of 80 Ã— 80 pixels due to limitations in the available amount of data. It also mentions that the model is trained for 150 epochs with a batch size of 128, and dropout and ADAM optimization are used during training.

However, it can be inferred that some form of preprocessing might have been performed, as the context mentions that the data is "pre-labelled" and that the model is trained to minimize the cross-entropy loss between the network's predictions and the pre-labelled data. This suggests that the data might have undergone some form of preprocessing or preparation before being used to train the model.

Unfortunately, the exact preprocessing steps involved are not specified in the provided context. Therefore, I cannot provide a more detailed answer to this query.
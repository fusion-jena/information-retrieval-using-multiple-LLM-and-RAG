The provided context mentions that stochastic gradient descent (SGD) is one of the optimization techniques used in deep learning. However, it does not explicitly mention Adam or other optimization techniques used in the deep learning pipeline. Therefore, based on the provided context, we can only confirm that SGD is one of the optimization techniques used, but we cannot provide a comprehensive list of optimization techniques applied in the deep learning pipeline.
Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training (e.g., train, test, validation). The context primarily focuses on active learning, input selection, and model training using different datasets and techniques, but it does not provide information on data splitting criteria.

However, it is worth noting that the context does mention that the authors trained models from scratch using the NIPS4Bplus and BirdCLEF 2020 datasets, and they used a "complementary train extract" to allow comparisons of the performance of BirdNET and their trained models on unseen classes. This suggests that the authors might have used some form of data splitting, but the specific criteria used for splitting the data are not mentioned.

In general, the criteria used to split data for deep learning model training can vary depending on the specific problem, dataset, and model architecture. Common practices include random splitting, stratified splitting, and k-fold cross-validation. However, without more information, it is impossible to determine the specific criteria used in this particular context.
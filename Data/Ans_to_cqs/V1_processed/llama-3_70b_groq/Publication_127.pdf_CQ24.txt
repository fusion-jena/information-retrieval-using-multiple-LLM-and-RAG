Unfortunately, the provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as random seed values. The context appears to be focused on various deep learning architectures, their performance, and applications in image recognition and wildlife ecology.

However, in general, handling randomness in deep learning pipelines is crucial to ensure reproducibility and consistency in model performance. Some common strategies employed to handle randomness include:

1. Fixing the random seed: Setting a fixed random seed value ensures that the same sequence of random numbers is generated, making the results reproducible.
2. Using deterministic algorithms: Some algorithms, like those used in convolutional neural networks, can be designed to be deterministic, eliminating the need for random number generation.
3. Averaging multiple runs: Running the model multiple times with different random seeds and averaging the results can help reduce the impact of randomness.
4. Using techniques like dropout and batch normalization: These techniques can help reduce the effect of randomness in the model by introducing regularization and normalization.

It is essential to note that these strategies are not mentioned in the provided context, and I'm providing general information on handling randomness in deep learning pipelines.
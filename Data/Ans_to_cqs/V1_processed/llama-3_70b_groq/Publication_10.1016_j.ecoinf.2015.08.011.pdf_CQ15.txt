The provided context does not mention specific optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The text only mentions that the learning rate was varied from 0.01 to 1.0 in increments of 0.05 as part of the trial-and-error method to determine the optimal learning rate. It does not provide information on the optimization algorithm used to update the model parameters. Therefore, I don't know the answer to this query based on the provided context.
The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that data augmentation techniques such as scaling, panning, rotation, color transformation, and mosaic were employed to increase the number of training samples and improve model generalization ability. This can be considered as a form of regularization to prevent overfitting. 

Additionally, the context mentions that the models were trained using a stochastic gradient descent (SGD) optimizer with a cycle learning rate, which can also help in preventing overfitting by adapting the learning rate during training.

However, it does not mention other common regularization methods such as dropout, L2 regularization, or early stopping. Therefore, it cannot be concluded that these methods were used in the pipeline.
Based on the provided context, the optimization algorithm used in the deep learning pipeline is Stochastic Gradient Descent (SGD). This is mentioned in multiple instances, including the no-overfitting and overfitting settings, as well as in the context of training the attack model. In one instance, the optimization algorithm is also mentioned with a momentum of 0.9, but the specific context is not clear. There is no mention of the Adam optimization technique in the provided context.
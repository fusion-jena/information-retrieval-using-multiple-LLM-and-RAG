Based on the provided context, the regularization method used to prevent overfitting in the deep learning pipeline is dropout regularization. This is mentioned in the text as "We apply dropout regularization in the forward pass during training..." (Section 1). 

There is no mention of L2 regularization being used in the provided context. However, it is mentioned that the Adam optimizer is used with a weight decay of 1.0â‹…10^(-3), which is a form of L2 regularization. But this is not explicitly stated as a regularization method to prevent overfitting.

It's worth noting that the context also mentions the use of transfer learning, which can also help in preventing overfitting by leveraging pre-trained weights from a large dataset like ImageNet.
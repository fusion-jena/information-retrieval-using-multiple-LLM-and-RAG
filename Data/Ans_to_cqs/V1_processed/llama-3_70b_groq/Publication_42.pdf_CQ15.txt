According to the provided context, two optimization techniques are applied in the deep learning pipeline: SGD (Stochastic Gradient Descent) and Adam. The context states that "Rank 1 to 32 were trained using the Adam optimizer. Rank 33 to 64 were trained using the SGD optimizer." This indicates that both Adam and SGD were used as optimizers to train the CNN architectures. Additionally, the context mentions that "Compared to SGD, Adam turned out to be the superior optimizer for training of all models." This suggests that Adam performed better than SGD in terms of optimizing the model's performance.
According to the provided context, the performance of the deep learning model is evaluated using the following metrics:

1. Precision (P): It is calculated as the ratio of True Positives to the sum of True Positives and False Positives.

2. Recall (R): It is calculated as the ratio of True Positives to the sum of True Positives and False Negatives.

3. Average Precision (AP): It is calculated as the integral of Precision with respect to Recall.

4. AP0.5: It is the average precision at 50% Intersection Over Union (IoU) threshold, which provides a more balanced reflection of the global detection performance of the model.

5. F1 Score: Although not explicitly defined in the context, it can be inferred that the F1 Score is also used as a metric, as it is presented in the table comparing the performance of different models.

These metrics are used to evaluate the performance of the models on the validation set after 300 epochs.
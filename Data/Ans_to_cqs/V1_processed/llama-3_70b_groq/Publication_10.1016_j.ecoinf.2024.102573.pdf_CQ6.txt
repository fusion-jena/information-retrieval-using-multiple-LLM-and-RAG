Based on the provided context, the preprocessing steps involved before training a deep learning model include:

1. Log compression: This is mentioned in the context as a preprocessing step, specifically in the section 3.1. Main results, where it is compared to PCEN (Per-Channel Energy Normalization).

2. Normalization: Normalization is mentioned as a preprocessing step that significantly improves the performance of the models. It is applied on the backend's input, which is the frontend's output.

3. Gaussianizing the magnitudes and decorrelating the frequency bands: This is mentioned as a property of PCEN, which whitens the data and alleviates stationary background noise.

4. Mel-filterbanks initialization: This is mentioned in the context as a way to initialize the learnable frontends with 80 filters of length 512 (16 ms).

5. Downsampling: Downsampling is mentioned as a step that happens in the filtering convolution layer by a stride of 320 (10 ms).

It is worth noting that the context does not mention other common preprocessing steps such as scaling, cleaning, or data augmentation. However, it does mention that data augmentation is involved, but it does not provide details on what specific techniques are used.
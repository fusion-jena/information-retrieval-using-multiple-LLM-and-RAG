According to the provided context, the criteria used to determine when training is complete is when the error on the validation set starts to increase, indicating overfitting. This is mentioned in the text as "early stopping" technique, which is used to avoid overfitting. The model weights are randomly initialized, and the training process is stopped when the network begins to overfit the data, i.e., the error on the validation set. This implies that the training is complete when the model's performance on the validation set starts to degrade, indicating that it has learned the training data too well and is no longer generalizing well.
The provided context does not explicitly mention the specific metrics used to evaluate the performance of the deep learning model. However, it does mention "macro-averaged MAE" (Mean Absolute Error) as a metric used to track the model's performance improvement across active learning iterations. 

It is worth noting that the context also mentions the issue of class imbalance and the need for more appropriate evaluation methods for highly imbalanced and unlabelled test data. This suggests that traditional metrics like accuracy, precision, and recall might not be suitable for this specific problem, but the exact metrics used are not specified.
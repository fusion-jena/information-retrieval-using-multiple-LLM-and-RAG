The provided context does not explicitly mention specific optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The text focuses on the use of the Gumbel-Softmax function to approximate discrete distributions with continuous ones, which is useful in optimizing neural networks involving discrete variables. It also discusses the architecture of a neural network used to learn the neighbourhood relationship Rk. However, it does not provide information about the optimization techniques used to train the network. Therefore, I don't know the answer to this question based on the provided context.
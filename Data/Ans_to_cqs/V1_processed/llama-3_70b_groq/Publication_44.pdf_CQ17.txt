The provided context does not explicitly mention regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention data augmentation as a technique used to help the model generalize better. Data augmentation is a technique used to increase the size of the training dataset by applying random transformations to the existing data, such as flipping and rotation, which can help prevent overfitting.

Other than data augmentation, the context does not mention any other regularization methods such as dropout, L2 regularization, or others. It focuses more on the architecture and training procedures of the models used in the context, such as Atrous spatial pyramid pooling (ASPP), bilinear up sampling, and transfer learning using pre-trained VGG16 network.

Therefore, based on the provided context, I can only confirm that data augmentation is used as a technique to prevent overfitting, but I don't know if any other regularization methods are used.
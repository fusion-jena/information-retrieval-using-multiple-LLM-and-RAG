The regularization methods used to prevent overfitting in the deep learning pipeline mentioned in the context are:

1. Dropout: This is explicitly mentioned in the context as a regularization technique used in the MLP training. The dropout rates explored were 0.05, 0.1, 0.2, and 0.25, and it was applied to the input layer, hidden layers, and all layers.

2. Weight Decay (L2 regularization): This is also mentioned as a regularization technique used in the MLP training. The weight decay values explored were 0, 0.00001, 0.0001, and 0.001.

3. Batch Normalization: This is mentioned as a regularization technique used in the MLP training. It was tested on each layer of the network separately and on all layers simultaneously, with the best results achieved when applied to the third layer.

These regularization methods were used to prevent overfitting in the MLP training.
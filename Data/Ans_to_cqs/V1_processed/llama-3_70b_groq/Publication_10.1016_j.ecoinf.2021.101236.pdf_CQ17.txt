Based on the provided context, the regularization methods used to prevent overfitting in the deep learning pipeline are:

1. Dropout: The context mentions a step-wise reduction of dropout probabilities by 0.1, starting at an initial probability of 0.5. This indicates that dropout is used as a regularization technique to prevent overfitting.

2. Early Stopping: The context also mentions early stopping with a cooldown of 3 epochs to prevent overfitting.

3. Knowledge Distillation: The context mentions the use of knowledge distillation to train a born-again network using the best previous snapshot as a teacher. This technique helps to improve the overall performance of the model.

No mention of L2 regularization is found in the provided context. It is possible that other regularization techniques might be used, but they are not mentioned in the provided text.
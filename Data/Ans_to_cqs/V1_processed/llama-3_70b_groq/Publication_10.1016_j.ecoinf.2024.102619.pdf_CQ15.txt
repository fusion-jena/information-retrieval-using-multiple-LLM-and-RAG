According to the provided context, the optimization technique applied in the deep learning pipeline is the Adam optimizer. This is mentioned in the text as: "We also used the Adam learning rate optimizer (Kingma and Ba, 2015), which automatically adjusted our initial learning rate of 1e-03 during training in a way that improved performance." This suggests that the Adam optimizer was used to adaptively adjust the learning rate during training, which is a common optimization technique used in deep learning. There is no mention of other optimization techniques such as SGD in the provided context.
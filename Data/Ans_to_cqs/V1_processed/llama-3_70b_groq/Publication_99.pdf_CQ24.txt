The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention the use of various data augmentation techniques, such as height and width shift, flipping, zooming, and brightness changes, which can help reduce overfitting and increase robustness to randomness in the input data.

Additionally, the context mentions the use of dropout layers with a dropout ratio of 0.5 as a network regularizer, which can also help reduce overfitting and increase robustness to randomness in the model.

However, it does not provide information on how to handle randomness in the deep learning pipeline, such as setting a random seed value, which is typically used to ensure reproducibility of results.

Therefore, based on the provided context, I don't know the specific strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value.
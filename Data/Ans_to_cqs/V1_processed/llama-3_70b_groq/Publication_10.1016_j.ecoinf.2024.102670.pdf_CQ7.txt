Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training. The context appears to be a collection of research papers and snippets related to deep learning, image analysis, and data annotation, but it does not provide specific information on data splitting criteria.

In general, the criteria for splitting data into training, testing, and validation sets can vary depending on the specific problem, dataset, and model architecture. Common practices include random splitting, stratified splitting, and k-fold cross-validation. However, without more specific information, it is impossible to determine the exact criteria used in the context of the provided papers.

If you're looking for general guidelines on data splitting, I can provide some information. Typically, the data is split into three sets: training (60-80% of the data), validation (15-20% of the data), and testing (5-10% of the data). The training set is used to train the model, the validation set is used to tune hyperparameters and evaluate the model's performance during training, and the testing set is used to evaluate the model's performance on unseen data. However, the exact proportions and splitting criteria may vary depending on the specific problem and dataset.
The optimization technique applied in the deep learning pipeline is Adam. This is mentioned in the context as "an Adam Solver type as learning parameters". Additionally, an exponential learning decay with a Gamma of 0.95 is used to prevent overfitting.
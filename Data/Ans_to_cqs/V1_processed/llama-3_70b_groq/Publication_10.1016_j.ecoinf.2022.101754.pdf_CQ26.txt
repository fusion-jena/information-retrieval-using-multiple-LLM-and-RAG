Based on the provided context, it appears that stratification splitting is used to address data bias during preprocessing. In Section 2.6, it is mentioned that the study utilized a training-and-test approach with stratification splitting, where 100 different splits to the training-and-test set were given. This suggests that the data was split in a way that maintains the same proportion of classes in both the training and test sets, which can help to reduce bias.

However, there is no mention of other techniques such as oversampling, undersampling, or diverse data collection being used to address data bias. It is possible that these techniques were not used in this particular study, but without more information, it is impossible to say for certain.

It's worth noting that the context provided does not specifically mention a deep learning pipeline, but rather discusses the use of machine learning models such as AdaBoost, random decision forest, multilayer neural network, and Bayesian ridge regression for modeling forest AGB.
The provided context does not explicitly mention the criteria used to split the data for deep learning model training. However, it does describe a nested k-fold cross-validation scheme used for XGBoost model training. 

In this scheme, the dataset is split into three subsets: the tuning set (k-2 folds), the validation set (1-fold), and the test set (1-fold). The model is trained on the tuning set, and its performance is monitored on the validation set by calculating the out-of-sample prediction accuracy. The median of k-1 folds is taken to determine the best number of iterations.

It does not provide information on the specific criteria used to split the data into these subsets. Typically, in machine learning, data is split randomly into training, validation, and testing sets, but the context does not provide this information.
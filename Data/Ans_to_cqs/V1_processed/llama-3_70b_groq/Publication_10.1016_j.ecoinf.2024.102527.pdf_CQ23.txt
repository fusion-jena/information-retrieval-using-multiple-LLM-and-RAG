To ensure the generalizability of the deep learning model, the authors took measures to prevent overfitting and ensure that the model is not biased towards a specific subset of the data. 

One measure taken was to split the data into training, validation, and test sets. The training set was used to train the model, the validation set was used to track the model's performance during training and to stop training when the loss value no longer decreased, and the test set was used to evaluate the model's performance after training.

Another measure taken was to keep the integrity of events during data splitting. Since the camera trap image datasets (SS and LSM) comprised capture events, where multiple images in succession were captured, the authors made sure to put all images of an event into the same dataset. This was done to prevent splitting images within an event into training and test sets, which could lead to poor generalization of the model.

No explicit mention of cross-validation, stratified splitting, or diverse datasets was made in the provided context. However, the authors did experiment with different training set sample sizes (100,000, 500,000, and 1,000,000) and number of classes (binary-class and ten-class) on the SS dataset, which could be seen as a form of data augmentation to improve generalizability.
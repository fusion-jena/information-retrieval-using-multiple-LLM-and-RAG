Based on the provided context, it appears that the strategy employed to handle randomness in the deep learning pipeline is data augmentation. Specifically, the context mentions that "augmentation played a crucial role in further diversifying the dataset" and that "introduced variations in color intensity" were used to increase the number of outputs per example. This suggests that random saturation adjustments were applied to each training example to increase the diversity of the dataset and reduce overfitting.

However, the context does not explicitly mention the use of random seed values or other strategies to handle randomness in the deep learning pipeline. Therefore, it is unclear whether other strategies, such as fixing random seed values or using techniques like dropout or batch normalization, were employed to handle randomness.

In summary, while data augmentation is mentioned as a strategy to increase dataset diversity, the context does not provide explicit information on how randomness is handled in the deep learning pipeline.
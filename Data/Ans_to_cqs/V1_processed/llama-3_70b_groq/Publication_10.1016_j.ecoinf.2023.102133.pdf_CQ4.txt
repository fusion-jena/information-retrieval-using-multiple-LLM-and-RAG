The provided context does not mention any data augmentation techniques applied in the deep learning pipeline, such as flipping, rotating, or scaling. The context primarily focuses on the architecture and hyperparameters of the Multi-Layer Perceptron (MLP) and Random Forest (RF) models, as well as the performance evaluation and results. There is no mention of data augmentation techniques, which are typically used to artificially increase the size of the training dataset by applying transformations to the existing data. Therefore, I don't know the answer to this question based on the provided context.
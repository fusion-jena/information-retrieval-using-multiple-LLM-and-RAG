Based on the provided context, it appears that the authors used a diverse dataset and varying patch sizes to ensure the generalizability of the deep learning model. Specifically, they experimented with different patch sizes (128, 224) and found that a patch size of 224 Ã— 224 gave the highest micro-F1 on the test set, indicating that it included enough contextual noise to regularize overfitting and enabled better generalization.

Additionally, the authors used an ensemble network that combined the features extracted from different patch sizes, which further improved the classification performance. This approach can be seen as a form of implicit regularization, where the model is forced to learn from different representations of the data, making it more robust to overfitting.

However, there is no explicit mention of cross-validation or stratified splitting in the provided context. It is possible that these techniques were used, but it is not mentioned in the text. Therefore, I cannot confirm whether these measures were taken to ensure the generalizability of the model.
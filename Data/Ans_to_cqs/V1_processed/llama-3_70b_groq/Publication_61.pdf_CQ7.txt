Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training (e.g., train, test, validation). The context primarily focuses on the experimental setup, hyperparameter tuning, and results of the cactus recognition model. It does not provide information on how the data was split for training, testing, and validation.

However, it is a common practice in deep learning to split the data into training, testing, and validation sets using techniques such as random sampling, stratified sampling, or k-fold cross-validation. The purpose of splitting the data is to evaluate the model's performance on unseen data and prevent overfitting.

In the absence of specific information, I cannot provide a definitive answer to this query.
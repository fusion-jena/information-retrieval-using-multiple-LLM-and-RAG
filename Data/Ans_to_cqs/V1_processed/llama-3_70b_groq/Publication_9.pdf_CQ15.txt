The provided context mentions the optimization technique used in the deep learning pipeline, which is stochastic gradient descent (SGD) with backpropagation. This is stated in the sentence: "the overarching goal of training a neural network is to iteratively minimize the error between model output and expected output by optimally adjusting model weights and reaching model convergence, such that the trained neural network generalizes well to novel input data. Model weights are adjusted to minimize error on each subsequent run using an algorithm called stochastic gradient descent with backpropagation."

There is no mention of the Adam optimization technique in the provided context. Therefore, it cannot be confirmed whether Adam is used in the deep learning pipeline or not.
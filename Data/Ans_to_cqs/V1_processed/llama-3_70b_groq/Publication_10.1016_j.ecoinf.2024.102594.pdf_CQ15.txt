The optimization techniques applied in the deep learning pipeline for the DeepLabV3+ algorithm are SGD and Adam. According to the context, the study compared the SGD and Adam algorithms and experimented with various strategies, including StepLR, CosineAnnealingLR, and ReduceLROnPlateau. It was found that utilizing Adam in combination with ReduceLROnPlateau could accelerate the convergence rate of model training and effectively reduce the risk of overfitting.
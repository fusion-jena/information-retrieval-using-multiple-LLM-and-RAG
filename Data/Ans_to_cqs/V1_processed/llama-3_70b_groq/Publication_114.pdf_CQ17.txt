The provided context does not explicitly mention regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention batch normalization (BN) as a technique used to hamper vanishing/exploding gradients in deep CNNs. Additionally, it mentions that GoogLeNet removed fully connected layers to reduce the number of parameters, which can be seen as a form of regularization. 

It does not mention dropout or L2 regularization explicitly. Therefore, I cannot provide a comprehensive list of regularization methods used to prevent overfitting in the deep learning pipeline based on the provided context.
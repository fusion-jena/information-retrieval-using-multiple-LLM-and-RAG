According to the provided context, the optimization technique applied in the deep learning pipeline is the Adam optimizer. Specifically, it is mentioned that "the cost function was minimized using the Adam optimizer (Kingma, 2017), with an initial learning rate μ0 and momentum β1 equal to 0.0001 and 0.9, respectively." Additionally, a learning rate decay procedure is also implemented, as proposed in Ganin et al. (2017). There is no mention of SGD or other optimization techniques being used in the pipeline.
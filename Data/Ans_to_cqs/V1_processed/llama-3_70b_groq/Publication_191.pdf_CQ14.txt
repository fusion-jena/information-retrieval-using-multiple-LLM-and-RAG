The provided context does not explicitly mention how the hyperparameters of the model are optimized. It only provides the specific values used for the hyperparameters, such as learning rate, momentum, weight decay, and batch size, in two different experiments. 

In the first experiment, the learning rate is set to 10^(-3) and decreased by a factor of 10 when the validation set accuracy stops improving. In the second experiment, the learning rate is initially set to 10^(-3) for all layers and 10^(-2) for the newly defined last fully connected layer, and then decreased by a factor of 10 every 20K iteration.

There is no mention of grid search, random search, or any other hyperparameter optimization method. Therefore, it is unclear how the hyperparameters were optimized.
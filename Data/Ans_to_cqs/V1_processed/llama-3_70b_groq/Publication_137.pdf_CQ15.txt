The provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD or Adam. However, it does mention that the learning decay rate is set to 0.7, which suggests that some form of learning rate scheduling is used. This is a common technique used in conjunction with optimization algorithms like SGD or Adam to adjust the learning rate during training. Unfortunately, the specific optimization algorithm used is not mentioned in the provided context.
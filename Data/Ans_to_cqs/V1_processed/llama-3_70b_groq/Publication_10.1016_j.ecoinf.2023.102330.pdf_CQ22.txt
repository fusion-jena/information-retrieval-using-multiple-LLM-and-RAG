The provided context does not explicitly mention the metrics used to evaluate the performance of the deep learning model. However, it does mention that the authors manually reviewed a random sample of five one-hour files from each year to evaluate model accuracy and calculate a false positive rate. This suggests that accuracy and false positive rate are two metrics used to evaluate the model's performance. 

Additionally, the context mentions that only calls with a score of 0.95 or higher were officially classified as a positive bobwhite detection, which implies that the model's output is a score or probability, and the threshold of 0.95 is used to determine the classification. This could be related to precision or recall, but it is not explicitly stated.

In summary, based on the provided context, accuracy and false positive rate are two metrics used to evaluate the model's performance, but it does not explicitly mention other common metrics such as precision, recall, or F1-score.
The provided context does not explicitly mention how the hyperparameters of the model are optimized. It does mention that S. P. Sankareshwaran et al. proposed the CAHA-AXRNet approach, which optimizes hyperparameters through the crossover-boosted artificial hummingbird algorithm. However, this is in the context of a different approach and not related to the BEiT model being discussed.

The context does provide information on the model architecture and training process, including the freezing of layers and fine-tuning of remaining layers, but it does not provide details on hyperparameter optimization. Therefore, I don't know the answer to this query based on the provided context.
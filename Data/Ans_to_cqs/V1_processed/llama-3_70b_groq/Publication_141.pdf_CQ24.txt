The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as random seed value. However, it does mention the use of techniques like data augmentation and dropout, which can help reduce overfitting and increase the robustness of the model to random variations.

Data augmentation involves artificially increasing the size of the training dataset by applying random transformations to the existing images, which can help the model learn to be more invariant to certain types of randomness. Dropout, on the other hand, involves randomly dropping out neurons during training, which can help prevent the model from relying too heavily on any single neuron and increase its robustness to random variations.

While these techniques can help handle certain types of randomness, they do not specifically address the issue of random seed value. Therefore, it is unclear from the provided context what strategy is employed to handle randomness in the deep learning pipeline in terms of random seed value.
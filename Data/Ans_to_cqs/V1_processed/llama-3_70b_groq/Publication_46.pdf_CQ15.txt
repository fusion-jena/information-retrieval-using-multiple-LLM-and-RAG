Unfortunately, the provided context does not explicitly mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the architecture of Convolutional Neural Networks (CNNs), feature extraction, and the composition of CNNs into convolution, pooling, and fully connected layers.

While the context does mention the importance of weights between neurons in adjoining layers and the use of the SoftMax function to transform parameters into probabilities, it does not provide information on optimization techniques used to update these weights during the training process.

Therefore, I don't know the answer to this specific question based on the provided context.
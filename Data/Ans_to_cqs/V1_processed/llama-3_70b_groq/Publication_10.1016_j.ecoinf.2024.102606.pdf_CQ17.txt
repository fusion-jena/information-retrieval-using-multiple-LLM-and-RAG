The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention two techniques that help prevent overfitting: data augmentation and cross-validation.

Data augmentation is used to artificially increase the size of the dataset by creating new samples from the existing ones. This technique helps reduce the risk of overfitting by exposing the model to more variations, making it less likely to memorize the dataset.

Cross-validation is another technique used to prevent overfitting. It involves creating K crosses by randomly shuffling the dataset and dividing it into a training and an evaluation set K times. Then, for each cross, a new model is trained, and the performances are averaged to better understand the model's capability on the dataset.

Although the context does not mention specific regularization methods like dropout or L2 regularization, it does highlight the importance of using techniques like data augmentation and cross-validation to prevent overfitting, especially when dealing with limited data.
The hyperparameters of the model are optimized using a small grid search. The hyperparameters considered in the search include the activation function (ReLU, sigmoid, or elu), optimizer (RMSprop or adam), and learning rate (0.1, 0.001, 0.0001, and 0.00001).
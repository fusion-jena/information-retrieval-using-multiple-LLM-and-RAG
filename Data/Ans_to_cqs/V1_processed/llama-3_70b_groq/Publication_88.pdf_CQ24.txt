The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as random seed value. The context primarily focuses on the architecture of the EF network, the number of samples in the training, validation, and test sets for the Amazon and Cerrado databases, and the performance metrics used to evaluate the models. 

However, it is common practice in deep learning to employ strategies to handle randomness, such as:

1. Fixing the random seed value to ensure reproducibility of results.
2. Using techniques like dropout and batch normalization to reduce overfitting.
3. Implementing data augmentation to increase the size of the training dataset.
4. Averaging the results of multiple models trained with different random initializations.

These strategies are not mentioned in the provided context, and therefore, I cannot provide a specific answer based on the given information.
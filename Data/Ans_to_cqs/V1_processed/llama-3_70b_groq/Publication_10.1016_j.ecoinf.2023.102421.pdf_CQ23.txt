To ensure the generalizability of the model, several measures were taken. Firstly, the training dataset was divided into two parts, with 80% used for training/validation and the remaining 20% kept as unseen data for testing. This split helps to evaluate the model's performance on unseen data and prevents overfitting. Additionally, 10-fold cross-validation was used during the training process to eliminate bias. This technique involves training the model on different subsets of the data and evaluating its performance on the remaining subset. This helps to ensure that the model is not overfitting to a particular subset of the data. However, there is no mention of using a diverse dataset or stratified splitting in the provided context.
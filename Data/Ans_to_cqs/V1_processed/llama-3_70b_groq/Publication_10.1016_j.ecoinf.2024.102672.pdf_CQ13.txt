The hyperparameters used in the deep learning model are the learning rate and the optimizer. Specifically, the optimizer used is the Adam algorithm, which is a stochastic gradient descent method. The learning rate is not explicitly mentioned, but it is one of the hyperparameters that control the training process, along with the batch size and the number of epochs. The number of epochs was initially set to 500 and later adjusted to the minimum number of epochs that guaranteed the stabilization of the loss curve in the four folds of the cross-validation.
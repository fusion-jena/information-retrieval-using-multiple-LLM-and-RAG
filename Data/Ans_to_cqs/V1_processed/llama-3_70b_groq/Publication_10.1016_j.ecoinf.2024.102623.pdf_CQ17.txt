Based on the provided context, the regularization methods used to prevent overfitting in the deep learning pipeline are:

1. Dropout: This is explicitly mentioned in the context as "introduce dropout" in the description of the MLP architecture.

No other regularization methods, such as L2 regularization, are mentioned in the provided context. However, it is worth noting that the context does mention "weight decay" which is set to 0.0001, and weight decay is a form of L2 regularization. Therefore, it can be inferred that L2 regularization is also used in the pipeline.

In summary, the regularization methods used to prevent overfitting in the deep learning pipeline are dropout and weight decay (a form of L2 regularization).
Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training (e.g., train, test, validation). The context primarily focuses on the hyperparameters of a CNN model, their ranges, and the proposed approach for finding the optimal hyperparameters. It also discusses the importance of dropout in improving the model's generalizability. However, it does not provide information on data splitting criteria.

In general, the criteria for splitting data into training, testing, and validation sets can vary depending on the problem, dataset, and model. Common practices include random splitting, stratified splitting, k-fold cross-validation, and time-series splitting. The goal is to ensure that the training, testing, and validation sets are representative of the overall data distribution and that the model is not overfitting or underfitting.

Since the context does not provide specific information on data splitting criteria, I cannot provide a more detailed answer.
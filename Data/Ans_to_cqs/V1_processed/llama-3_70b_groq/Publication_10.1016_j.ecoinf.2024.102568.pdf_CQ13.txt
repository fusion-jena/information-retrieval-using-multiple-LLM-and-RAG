The hyperparameters used in the deep learning model are not fully specified in the provided context. However, it is mentioned that the ADAM optimization algorithm is used, which is a type of stochastic gradient descent optimizer. The learning rate is not explicitly mentioned. Additionally, a dropout layer is used to avoid overfitting, but the dropout rate is not specified. The loss function used is the mean squared error (MSE).
According to the provided context, the optimization technique applied in the deep learning pipeline is the Adam optimizer. This is mentioned in section 2.3.3. Experimental setting, which states: "The deep feature loss bird noise reduction network model was trained for 50 epochs on an RTX 2080 Ti GPU using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 10". There is no mention of other optimization techniques such as SGD in the provided context.
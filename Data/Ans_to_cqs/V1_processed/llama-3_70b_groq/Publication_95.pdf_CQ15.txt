According to the provided context, the optimization technique applied in the deep learning pipeline is the Adam optimizer with a learning rate of 0.001. This is mentioned in the section 5.1.1. Network Training Procedure, which states that "All networks were trained on the training set for 100 epochs, using the Adam optimizer [27] with a learning rate of 0.001 and cross-entropy loss." There is no mention of other optimization techniques such as SGD being used.
The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. The context primarily focuses on the configuration of the neural network model, including the Adam solver, learning rate, and rectifier activation function (ReLU). It also discusses the hyperparameters of the Random Forest model and the use of backpropagation algorithm for training the network. However, it does not mention specific regularization techniques such as dropout or L2 regularization. Therefore, I don't know the answer to this question based on the provided context.
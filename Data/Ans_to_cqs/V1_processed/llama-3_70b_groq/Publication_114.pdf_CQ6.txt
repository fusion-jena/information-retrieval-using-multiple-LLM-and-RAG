The provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. The context primarily discusses the architecture and design of various convolutional neural networks (CNNs), such as AlexNet, VGG, GoogLeNet, and ResNet, and their applications in image classification and remote sensing.

However, it is worth noting that batch normalization (BN) is mentioned as a technique used to address the vanishing/exploding gradients problem in deep CNNs. Batch normalization is a type of preprocessing step that normalizes the input data for each layer, but it is not a traditional preprocessing step applied to the entire dataset before training.

In general, common preprocessing steps involved before training a deep learning model may include normalization, scaling, cleaning, data augmentation, and feature engineering, among others. However, these steps are not explicitly mentioned in the provided context.

Therefore, I don't know the specific preprocessing steps involved before training a deep learning model based on the provided context.
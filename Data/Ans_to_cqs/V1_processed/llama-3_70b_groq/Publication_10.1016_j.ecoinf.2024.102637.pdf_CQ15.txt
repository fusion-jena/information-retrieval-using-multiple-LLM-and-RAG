The provided context does not explicitly mention specific optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The text focuses on the architecture and design of the AlexNet model, its components, and how it is optimized for deployment. It discusses the use of convolution layers, normalization layers, and activation function layers, as well as the cascading of multiple activation functions to introduce nonlinear factors. Additionally, it mentions the reduction of computational complexity and deployment costs through meticulous algorithm optimization and structural design.

However, it does not provide information on the specific optimization techniques used during training, such as the choice of optimizer, learning rate, or other hyperparameters. Therefore, I don't know the answer to this question based on the provided context.
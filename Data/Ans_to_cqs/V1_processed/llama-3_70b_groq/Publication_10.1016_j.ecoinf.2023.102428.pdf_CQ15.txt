Based on the provided context, it is mentioned that the neural networks are trained using stochastic gradient descent (SGD) as an optimization technique. This is stated in the sentence: "differentiation as a feedforward propagation (FP) algorithm to train the neural networks by employing supervised learning based on stochastic gradient descent (SGD)". There is no mention of other optimization techniques like Adam in the provided context.
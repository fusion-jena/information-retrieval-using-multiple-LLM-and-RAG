The provided context does not explicitly mention a strategy implemented to monitor the model performance during training. It discusses the use of active learning to select the most informative samples for manual annotation, the use of EV to display model predictions and select the best sample candidates, and the use of EV to aid in selecting the most appropriate trained model for a bioacoustic task. However, it does not provide information on how the model performance is monitored during training.

It can be inferred that the model performance is evaluated using performance metrics, such as statistical comparisons between the model predictions and known classes from a held-out part of the training dataset or an unrelated labelled dataset. However, the context does not provide a clear strategy for monitoring model performance during training.

Therefore, I don't know the exact strategy implemented to monitor the model performance during training based on the provided context.
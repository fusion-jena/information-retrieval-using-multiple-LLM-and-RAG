The hyperparameters of the model are optimized using a trial-and-error method. According to the context, the number of nodes in the hidden layer was varied between 5 and 23, and the learning rate was varied from 0.01 to 1.0 in increments of 0.05. For each configuration, the mean square error (MSE) between the model output and the measured data was calculated. The optimal number of neurons in the hidden layer and the optimal learning rate were selected based on the maximum model performance as indicated by MSE. This approach is similar to a grid search, where all possible combinations of hyperparameters are tried, but it is not explicitly stated as a grid search.
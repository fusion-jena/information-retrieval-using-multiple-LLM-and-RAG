The provided context does not explicitly mention techniques used to address data bias during preprocessing of the deep learning pipeline. The context primarily focuses on model generalization, overfitting, and the importance of selecting the right number of layers, neurons, and learning rates in Artificial Neural Networks (ANNs).

However, it does mention that the dataset was randomly split into two sets, 80% for model training and 20% for model testing, which is a common practice to prevent overfitting. But this does not directly address data bias.

Therefore, I don't know the answer to this specific question based on the provided context. The mentioned techniques such as Stratified splitting, oversampling, undersampling, and Diverse data collection are commonly used to address data bias, but they are not mentioned in the provided context.
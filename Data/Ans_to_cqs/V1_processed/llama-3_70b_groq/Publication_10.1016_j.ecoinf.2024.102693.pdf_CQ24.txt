Based on the provided context, I don't see any direct mention of strategies employed to handle randomness in the deep learning pipeline, such as random seed value. The context primarily discusses various deep learning models, including Vision Transformers (ViT), CNNs, and MLP-like models, as well as data augmentation techniques like Random Erasing and Randaugment.

However, I can infer that data augmentation techniques might be used to handle randomness in the deep learning pipeline. Data augmentation involves artificially increasing the size of the training dataset by applying random transformations to the existing data. This can help reduce overfitting and improve the model's robustness to random variations.

Random Erasing, mentioned in the context, is a data augmentation technique that randomly selects a rectangle region in an image and erases its pixels. Randaugment, another technique mentioned, is a practical automated data augmentation method with a reduced search space.

While these techniques can help handle randomness in the data, they do not directly address the issue of random seed values in the deep learning pipeline. Therefore, I cannot provide a specific answer to this query based on the provided context.
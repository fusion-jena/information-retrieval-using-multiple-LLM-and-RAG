Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training (e.g., train, test, validation). The context primarily focuses on the application of LSTM models, data scaling using MinMaxScaler, and the training process. It does not provide information on how the data was split for training, testing, and validation.

However, it can be inferred that some form of cross-validation was used, as mentioned in the context: "each forecast was assured to be a fair cross-validation." This suggests that the data was split in some way to enable cross-validation, but the specific criteria or proportions used for splitting the data are not provided.

In general, the criteria for splitting data into training, testing, and validation sets can vary depending on the specific problem, dataset, and model. Common practices include splitting the data into 60-80% for training, 15-20% for validation, and 5-20% for testing, but these proportions can be adjusted based on the specific requirements of the project.

In this case, without more information, it is not possible to determine the exact criteria used to split the data for deep learning model training.
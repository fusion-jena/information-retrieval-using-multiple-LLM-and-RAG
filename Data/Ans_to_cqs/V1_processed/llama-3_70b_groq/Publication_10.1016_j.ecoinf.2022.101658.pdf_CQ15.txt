According to the provided context, the optimization technique applied in the deep learning pipeline is the Adam optimizer. This is mentioned in the text as: "We used the cross-entropy loss as the objective function for training our network, and the Adam (Kingma and Ba, 2014) optimizer for updating the trainable parameters." There is no mention of SGD (Stochastic Gradient Descent) or any other optimization techniques being used in this specific pipeline.
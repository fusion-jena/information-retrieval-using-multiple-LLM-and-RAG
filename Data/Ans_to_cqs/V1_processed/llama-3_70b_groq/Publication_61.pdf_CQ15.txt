According to the provided context, the optimization technique applied in the deep learning pipeline is the Adam optimizer. This is mentioned in the text as "we train the network with Adam optimizer (Kingma and Ba, 2015), a variant of the stochastic gradient optimization where the learning rate is adjusted automatically." This implies that the Adam optimizer is used to adjust the learning rate during the training process, which helps to avoid the issue of manually setting the learning rate.
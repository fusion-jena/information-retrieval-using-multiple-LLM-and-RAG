The strategy implemented to monitor the model performance during training is not explicitly mentioned in the provided context. However, it can be inferred that the authors stopped the network training after 70 epochs to prevent overfitting, which suggests that they were monitoring the model's performance during training to determine when to stop. Additionally, the use of classic hyper-parameters such as a learning rate of 10^-5, an exponential learning decay with a Gamma of 0.95, a dropout of 50%, and an Adam Solver type, suggests that the authors were trying to achieve a fast convergence of the network without overfitting. However, the specific strategy or metric used to monitor the model's performance during training is not mentioned.
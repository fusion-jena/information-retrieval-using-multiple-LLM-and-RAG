To ensure the generalizability of the deep learning model, the authors employed data augmentation techniques to increase the size of the dataset from 100 to 6000 images. This was done by applying various transformations to the original images, such as flipping, rotation, and color jittering. Additionally, the authors used fine-tuning, where they initialized the evaluated models with the pre-trained weights of ImageNet, which is a large and diverse dataset. This approach allows the model to leverage the knowledge learned from the pre-training task and adapt to the new task with a smaller dataset.

However, there is no mention of cross-validation or stratified splitting in the provided context. It is possible that these techniques were used, but it is not explicitly stated. Therefore, we cannot confirm whether these measures were taken to ensure the generalizability of the model.

It is worth noting that the use of pre-trained weights and fine-tuning is a common practice in deep learning, especially when working with small datasets. This approach can help improve the model's performance and generalizability by leveraging the knowledge learned from large datasets like ImageNet.
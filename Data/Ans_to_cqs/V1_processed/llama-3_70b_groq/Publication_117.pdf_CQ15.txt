The optimization technique applied in the deep learning pipeline is the Adam optimizer. This is mentioned in the text as "We then unfroze all layers, reduced the learning rate to 10âˆ’6, and trained for a further 20 epochs. In total, training took 34 hours on a NVIDIA Quadro GP100 GPU."
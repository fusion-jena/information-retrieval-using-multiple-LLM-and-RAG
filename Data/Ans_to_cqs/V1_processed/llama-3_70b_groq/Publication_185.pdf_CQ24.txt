Unfortunately, the provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as random seed value. The context primarily focuses on the feasibility of using small datasets to train deep learning models, image recognition, and the architecture of the AlexNet model.

However, it is worth noting that the context does mention the use of stochastic gradient descent and the backpropagation algorithm in the training process, which implies that randomness is involved in the optimization process. Additionally, the initialization of feature extractors with white Gaussian noise also introduces randomness.

Despite this, the context does not provide specific strategies for handling randomness, such as setting a random seed value, using techniques like dropout or batch normalization, or employing ensemble methods to mitigate the effects of randomness.

Therefore, I don't know the specific strategies employed to handle randomness in the deep learning pipeline based on the provided context.
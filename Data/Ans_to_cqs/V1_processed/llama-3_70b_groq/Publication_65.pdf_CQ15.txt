The provided context does not explicitly mention the optimization techniques used in the deep learning pipeline. However, it does mention an "SGD scheduler" in the evaluation metrics section, which suggests that Stochastic Gradient Descent (SGD) is used as the optimization algorithm. The context does not provide information about other optimization techniques like Adam, RMSProp, or Adagrad. Therefore, we can only confirm the use of SGD, but we don't know about other optimization techniques.
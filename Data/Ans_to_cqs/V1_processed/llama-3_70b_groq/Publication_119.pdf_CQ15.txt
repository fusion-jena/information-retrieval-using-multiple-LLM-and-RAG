The provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. It only mentions that the hyperparameters, including the learning rate, number of epochs, and batch size, were tuned to optimize the model's performance. The optimized values for the learning rate and batch size were 0.007 and 48, respectively. However, it does not specify the optimization algorithm used to update the model's weights and biases. Therefore, I don't know the answer to this question based on the provided context.
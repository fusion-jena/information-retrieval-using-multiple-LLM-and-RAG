To ensure the generalizability of the deep learning model, the authors took two measures: data partitioning with a ratio of 8:1:1 for training, validation, and test sets, and repeating the random partition ten times. This approach helps to reduce the influence of random splitting and ensures converged performance. However, there is no mention of using diverse datasets, cross-validation, or stratified splitting in the provided context.
The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention data augmentation techniques such as rotation, noise, and flip, which can help prevent overfitting by increasing the size of the training dataset and making the model more robust to different variations of the input data.

It does not mention other common regularization methods such as dropout, L2 regularization, or early stopping. Therefore, based on the provided context, I don't know what specific regularization methods were used to prevent overfitting in the deep learning pipeline.
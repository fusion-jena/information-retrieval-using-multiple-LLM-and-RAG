The metrics used to evaluate the performance of the deep learning model are not explicitly stated in the provided context. However, it is mentioned that state-of-the-art metrics are used to compare the performance of the model with expert annotations and novice annotations. 

From the context, we can infer that the following metrics are used: 

1. True Positive (TP): A call is accurately detected and annotated.
2. False Positive (FP): A call is detected, but there is no annotation.
3. True Negative (TN): No call is detected and no call is annotated.

Additionally, it is mentioned that the F1 score is used to evaluate the performance of the models trained on novice annotations, which is computed from the measurements of recall and precision. 

Other metrics mentioned in the context include AUC and mAP, which are used in the Kruskal-Wallis test to evaluate the null hypothesis. However, it is not clear if these metrics are directly used to evaluate the performance of the deep learning model.
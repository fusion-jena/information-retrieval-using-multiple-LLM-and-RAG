The hyperparameters used in the deep learning model are partially mentioned in the provided context. Specifically, it is mentioned that the model uses the Adam optimizer, but the learning rate is not specified. Additionally, the context mentions that a dropout value of 50% has been used in the first three layers to improve generalization and reduce overfitting. The filter size of the MLP is 2, and the number of nodes in the first three layers is 256, while the final layer has a number of nodes equal to the number of classes in the dataset. However, other hyperparameters such as the learning rate, batch size, and weight decay are not mentioned.
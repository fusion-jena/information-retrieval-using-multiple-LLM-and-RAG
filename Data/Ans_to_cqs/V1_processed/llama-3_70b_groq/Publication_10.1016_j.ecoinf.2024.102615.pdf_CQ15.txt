The provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context focuses on the hyperparameter optimization process using the grid search method, where different combinations of hyperparameters (e.g., learning rate, dropout values, batch size) are evaluated to select the optimal parameters for the MLP model. 

However, it does mention the "learning rate" as one of the hyperparameters to be optimized, which is a key component of various optimization techniques, including SGD and Adam. But it does not specify which optimization technique is used. Therefore, I don't know the specific optimization technique applied in the deep learning pipeline.
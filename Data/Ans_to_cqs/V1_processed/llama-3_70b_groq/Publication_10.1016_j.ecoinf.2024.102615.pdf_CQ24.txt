The context does not explicitly mention the use of a random seed value to handle randomness in the deep learning pipeline. However, it does mention that the raw dataset was randomly shuffled, and the data partitioning was repeated ten times to reduce the influence of random splitting while ensuring converged performance. This suggests that the authors are aware of the potential impact of randomness on the model's performance and have taken steps to mitigate it.

By repeating the data partitioning process ten times, the authors are likely trying to reduce the variability in the model's performance that may arise due to the random shuffling of the data. This approach can help to ensure that the model's performance is more robust and generalizable across different random initializations.

While the context does not provide explicit information on the use of a random seed value, it does imply that the authors have taken steps to handle randomness in the data partitioning process. However, it is unclear whether they have employed any additional strategies to handle randomness in other aspects of the deep learning pipeline.
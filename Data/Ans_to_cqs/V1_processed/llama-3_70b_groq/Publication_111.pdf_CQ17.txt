Based on the provided context, one of the regularization methods used to prevent overfitting in the deep learning pipeline is dropout. This is mentioned in the text as "+e dropout approach [45] is another way to deal with the overfitting problem." The context explains that dropout is a regularization strategy that allows training neural networks with alternative topologies in parallel by randomly dropping out a certain proportion of layer neurons.

However, there is no mention of L2 regularization in the provided context. It is possible that other regularization methods may be used, but they are not mentioned in the given text. Therefore, I do not know if L2 regularization is used in this specific deep learning pipeline.
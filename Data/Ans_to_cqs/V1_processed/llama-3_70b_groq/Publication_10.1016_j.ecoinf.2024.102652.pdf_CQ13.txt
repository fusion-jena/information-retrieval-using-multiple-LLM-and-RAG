The hyperparameters used in the deep learning model are as follows: 

The batch size is set to 32, which is chosen to not exceed the memory limit and to make full use of computer resources. The optimizer used is Adam processing. The learning rate varies with a group based on the cosine function's law, often in the range between 0.0 and 1.0. The number of epochs is set to 100, with an early stopping function to halt the learning process if there is no improvement in accuracy after 20 epochs.
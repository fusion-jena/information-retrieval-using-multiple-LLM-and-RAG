Unfortunately, the provided context does not mention specific strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. The context primarily focuses on the application of Convolutional Neural Networks (CNNs) for spectrogram classification, discussing aspects like input window size, data augmentation, and model architecture.

While the context mentions data augmentation as a potential strategy to improve performance, it does not explicitly discuss how to handle randomness in the pipeline. Therefore, I cannot provide a specific answer to this query based on the given context.

If you're looking for general strategies to handle randomness in deep learning pipelines, some common approaches include:

1. Setting a fixed random seed value to ensure reproducibility.
2. Using techniques like dropout and batch normalization to reduce overfitting.
3. Implementing data augmentation to increase the diversity of the training dataset.
4. Averaging model predictions over multiple runs with different random seeds.

However, these strategies are not mentioned in the provided context, and I do not have enough information to provide a more specific answer.
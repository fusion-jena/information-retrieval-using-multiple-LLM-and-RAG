According to the provided context, the regularization method used to prevent overfitting in the deep learning pipeline is dropout. Specifically, the dropout operation is added during the training stage, and the 6th layer dropout rate is 0.1, while for the other layers, it is 0. This means that during training, some neurons in the neural network can be deleted (or "dropped out") with a probability of 0.1 for the 6th layer and 0 for the other layers. However, all neurons should be activated during the test. There is no mention of L2 regularization or any other regularization method in the provided context.
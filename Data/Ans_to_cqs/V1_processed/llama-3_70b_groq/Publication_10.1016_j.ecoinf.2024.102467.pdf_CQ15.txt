Unfortunately, the provided context does not explicitly mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention "gradient descent optimization algorithms" in the context of Ruder's 2016 paper, which suggests that gradient descent is used as an optimization technique. 

It is worth noting that the context focuses more on neural network architectures, such as MobileNetV2, EfficientNet, and Bottleneck Transformers, rather than optimization techniques. Therefore, I cannot provide a more specific answer to this query based on the given context.
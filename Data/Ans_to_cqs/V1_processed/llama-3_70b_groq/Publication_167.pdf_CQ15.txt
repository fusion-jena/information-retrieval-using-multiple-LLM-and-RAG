According to the provided context, the Adam optimization algorithm is used to update the weights of the neural network. There is no mention of other optimization techniques such as SGD being applied in the deep learning pipeline.
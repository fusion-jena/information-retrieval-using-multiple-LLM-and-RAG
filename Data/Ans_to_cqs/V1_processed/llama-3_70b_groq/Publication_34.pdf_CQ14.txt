The provided context does not explicitly mention how the hyperparameters of the model are optimized. It only mentions that there is still work to be done on quick automated procedures for tuning optimization hyper-parameters, especially the initial learning rate. It does not specify whether grid search, random search, or any other method is used for hyperparameter optimization.

However, it does mention that the optimization method for NN parameters Î¸ is based on stochastic gradient descent algorithms, and the loss function gradient is approximated by the back-propagation algorithm. This suggests that the model parameters are optimized using stochastic gradient descent, but it does not provide information on how the hyperparameters are optimized.

Therefore, I don't know the answer to this query based on the provided context.
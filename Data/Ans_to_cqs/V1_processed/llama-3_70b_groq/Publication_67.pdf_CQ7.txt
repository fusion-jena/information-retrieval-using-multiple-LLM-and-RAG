Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training (e.g., train, test, validation). The context primarily focuses on the database construction phase, training phase, classification phase, and validation phase, but it does not provide information on how the data was split for training and validation.

The context does mention the creation of five training datasets with different levels of tree cover, continuous and discrete approximations, and an auxiliary class "Non-Forest". It also mentions the use of data augmentation to increase the size of the dataset, but it does not provide information on how the data was split for training and validation.

Therefore, I don't know the answer to this question based on the provided context.
The optimization technique applied in the deep learning pipeline is RMSprop. This is mentioned in the context as "an RMSprop optimiser with a learning rate of 0.001 and a learning rate decay of 0.0001 was used." There is no mention of SGD or Adam in the provided context.
The provided context does not explicitly mention regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. The context primarily focuses on the concept of deep learning, its applications, and optimization algorithms used during the training process (e.g., Stochastic Gradient Descent, Adaptive Gradient, Root Mean Square Prop, and Adam). 

However, it is worth noting that regularization methods are crucial in deep learning to prevent overfitting. Common regularization techniques include dropout, L1 and L2 regularization, early stopping, and data augmentation. These methods help reduce the capacity of the model, making it less prone to overfitting and improving its generalization capabilities.

Since the provided context does not mention specific regularization methods, I cannot provide a more detailed answer.
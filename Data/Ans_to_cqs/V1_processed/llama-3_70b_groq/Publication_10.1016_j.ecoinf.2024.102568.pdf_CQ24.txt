Based on the provided context, there is no explicit mention of strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, there are a few instances where randomness is mentioned or used in the context of machine learning models.

One example is the use of a dropout layer in the LSTM model, which is a regularization method that randomly excludes some inputs from activation and weight updates while training a network. This is mentioned in the context of avoiding overfitting.

Another instance is the mention of "extremely randomized trees" in the reference to Geurts et al. (2006), which suggests that randomness is used in the construction of decision trees.

However, there is no mention of strategies to handle randomness in the deep learning pipeline, such as setting a random seed value, to ensure reproducibility of results.
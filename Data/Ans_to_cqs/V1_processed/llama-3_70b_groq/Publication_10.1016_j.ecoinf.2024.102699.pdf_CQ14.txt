The provided context does not explicitly mention how the hyperparameters of the model are optimized. It only mentions the use of the Adam optimizer with a concatenating cosine-annealing linear scheduler, and an initial learning rate of 0.0001 that decays by a factor of 0.001*learning rate. However, it does not specify whether a grid search, random search, or any other method was used to optimize the hyperparameters. Therefore, I don't know the answer to this question based on the provided context.
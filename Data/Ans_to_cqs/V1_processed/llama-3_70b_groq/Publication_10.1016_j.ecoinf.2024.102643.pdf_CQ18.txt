The strategy implemented to monitor the model performance during training is cross-validation using bootstrapping. This is mentioned in the context as "To avoid sampling bias, pseudo-absence points were generated three times for each model and cross-validated per model using bootstrapping (Carvalho et al., 2021)". This means that the model was trained and evaluated multiple times using different subsets of the data, and the performance was monitored and averaged across these iterations. This approach helps to reduce overfitting and provides a more robust estimate of the model's performance.
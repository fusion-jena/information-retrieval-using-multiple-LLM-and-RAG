According to the provided context, two optimization techniques are mentioned: Adam and Stochastic Gradient Descent (SGD). 

In the training section, it is mentioned that "All the parameters were optimized using the Adam optimizer with a learning rate of 0.001." This indicates that the Adam optimization technique is used for Model-2, which has an EfficientNet feature extractor.

In the model evaluation section, it is mentioned that "Model-K was initialized with pre-trained Imagenet weights and then trained using our training data set with a Stochastic Gradient Descent (SGD) optimizer and an MSE loss function." This indicates that the SGD optimization technique is used for Model-K.

No other optimization techniques are mentioned in the provided context.
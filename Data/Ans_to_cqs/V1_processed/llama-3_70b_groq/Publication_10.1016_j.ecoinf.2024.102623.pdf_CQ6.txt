The provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. However, it does mention the use of batch normalization (Ioffe and Szegedy, 2015) in the MLP architecture. Batch normalization is a technique used to normalize the input data for each layer in a neural network. This can be considered as a preprocessing step, but it is applied within the model architecture rather than as a separate preprocessing step before training the model.

Other than batch normalization, the context does not provide information about other preprocessing steps such as scaling, cleaning, or normalization. It is possible that these steps were performed, but they are not mentioned in the provided context. Therefore, I cannot provide a comprehensive answer to this query based on the given information.
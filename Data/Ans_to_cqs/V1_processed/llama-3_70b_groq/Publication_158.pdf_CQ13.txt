The provided context does not explicitly mention the hyperparameters used in the deep learning model, such as the learning rate. However, it does mention that the Adam optimizer was used with 500 epochs and categorical-cross entropy loss function. The batch size was set to 64. Additionally, data augmentation was applied to enhance sample variation, which included rotation, reflectance bias of ±10%, and random noise of ±10% reflectance for 15% of the input image. Early stopping criteria were also applied if no improvement was found in 50 epochs.
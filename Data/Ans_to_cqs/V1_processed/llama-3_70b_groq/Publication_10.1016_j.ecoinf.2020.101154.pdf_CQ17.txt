The provided context does not mention specific regularization methods used to prevent overfitting in a deep learning pipeline, such as dropout or L2 regularization. However, it does mention the use of early stopping as a technique to avoid overfitting in the Gradient Boosting Regression Trees (GBRT) model. Early stopping is a method that stops the training process when the model's performance on the validation set starts to degrade, preventing the model from overfitting to the training data. 

It's worth noting that the context focuses on GBRT models, which are not typically considered deep learning models. Therefore, it's unlikely that regularization methods specific to deep learning, such as dropout or L2 regularization, would be mentioned.
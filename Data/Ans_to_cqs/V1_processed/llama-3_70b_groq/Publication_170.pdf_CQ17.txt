The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention that each layer in the AlexNet model, except for the last fully connected layer, performs Local Response Normalization (LRN) processing and Rectified Linear Unit (ReLU) activation function processing. 

While LRN is a regularization method, it is not typically used to prevent overfitting. Instead, it is used to normalize the responses of neurons in a layer. 

The context does not mention other regularization methods such as dropout or L2 regularization. Therefore, it is not possible to determine from the provided context what regularization methods are used to prevent overfitting in the deep learning pipeline.
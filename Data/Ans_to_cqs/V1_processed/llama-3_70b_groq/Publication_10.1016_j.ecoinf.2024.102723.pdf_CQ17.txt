The context mentions Monte Carlo Dropout (MCD) as a regularization technique used to prevent overfitting in the deep learning pipeline. Specifically, it is mentioned that MCD is a convenient technique for performing Bayesian Deep learning, and it corresponds to an approximate variational distribution, resulting in uncertainty estimates. 

Additionally, the context implies that other variants of dropout can be used, which correspond to alternative approximate distributions, setting a trade-off between uncertainty quality and computational complexity. 

However, it does not explicitly mention L2 regularization as a regularization method used in the deep learning pipeline. Therefore, the answer is that Monte Carlo Dropout is a regularization method used to prevent overfitting, but it is unclear if L2 regularization is used in this specific context.
Based on the provided context, two strategies are employed to handle randomness in the deep learning pipeline:

1. **Data Augmentation**: The context mentions that 4000 images were prepared for each category using a data augmentation method. Data augmentation is a technique to artificially increase the size of the training dataset by applying random transformations to the existing images. This helps to reduce overfitting and increase the robustness of the model to random variations in the data.

2. **Random Erasing**: The context mentions that random erasing was applied to the training dataset of the EfficientNet, which moderately improved the performance. Random erasing is a data augmentation technique that randomly selects a rectangle region in an image and erases its pixels. This technique helps to increase the robustness of the model to random occlusions and variations in the data.

There is no mention of using a random seed value in the provided context. However, it is a common practice in deep learning to set a random seed value to ensure reproducibility of the results.
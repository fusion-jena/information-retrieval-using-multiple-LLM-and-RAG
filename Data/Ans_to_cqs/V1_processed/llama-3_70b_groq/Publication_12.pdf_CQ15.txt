The provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The text focuses on the architecture of the BatNet model, the use of batch normalization layers, and the setting of the batch size to 128. It also discusses the use of Rectified Linear Unit (ReLU) as the activation function for non-linear classification. However, it does not provide information about the optimization techniques used to update the model's parameters during training. Therefore, I don't know the answer to this question based on the provided context.
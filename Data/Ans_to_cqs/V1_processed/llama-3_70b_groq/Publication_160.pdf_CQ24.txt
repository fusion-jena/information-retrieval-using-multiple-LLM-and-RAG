Unfortunately, the provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as random seed values. The context primarily focuses on the performance of different ResNet architectures with varying batch sizes, patch sizes, and ensemble methods for image classification tasks.

However, it can be inferred that the authors might have used fixed random seeds for their experiments to ensure reproducibility, as they report specific performance metrics for different models and hyperparameters. But this is not explicitly stated in the provided context.

In general, handling randomness in deep learning pipelines involves techniques such as:

1. Fixing random seeds for reproducibility.
2. Using techniques like dropout and data augmentation to reduce overfitting.
3. Ensemble methods to average out random variations.
4. Cross-validation to evaluate model performance on different subsets of the data.

However, these strategies are not explicitly mentioned in the provided context.
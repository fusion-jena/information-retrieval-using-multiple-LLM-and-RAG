The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention data augmentation as a technique to avoid overfitting and render the model more generalised. The context lists various data augmentation techniques applied to the training dataset, including random cropping, horizontal/vertical flipping, image rotation, image translation, zooming, brightness adjustment, contrast adjustment, hue and saturation adjustment, gaussian noise, and random erasing. These techniques help increase the size and variability of the dataset, making it more difficult for the model to overfit. 

While data augmentation is an effective way to prevent overfitting, it is not a regularization method in the classical sense. Regularization methods, such as dropout and L2 regularization, are typically applied directly to the model's parameters or loss function to prevent overfitting. Since the context does not mention these specific regularization methods, I cannot provide a definitive answer to the query.
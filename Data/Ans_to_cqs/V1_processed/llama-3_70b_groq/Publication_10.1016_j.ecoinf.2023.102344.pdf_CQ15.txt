According to the provided context, the optimization techniques applied in the deep learning pipeline are SGDM (Stochastic Gradient Descent with Momentum) and ADAM (Adaptive Moment Estimation). However, it is mentioned that ADAM was preliminarily used but appeared to converge on a sharp minimum, resulting in a negligible accuracy, and therefore was not further considered in the study. SGDM was used as the optimization algorithm for all pre-trained models under Approach 2, and the training converged after 1000 epochs.
The context does not explicitly mention measures taken to ensure the generalizability of the deep learning model, such as diverse datasets, cross-validation, or stratified splitting. However, it does mention that the models were evaluated in cross-site scenarios, aiming to assess the generalization capacity of the different architectures and the ensemble. This suggests that the authors attempted to evaluate the models on different datasets or sites to test their ability to generalize to new, unseen data.

Additionally, the authors mention that there might be room to improve the generalization capacity of the deep learning classifiers, suggesting that they are aware of the importance of generalizability and are considering ways to improve it. They propose potential methods to achieve this, such as better exploiting the training data, using additional data augmentation techniques, or simplifying the classifiers to reduce the risk of overfitting.

Overall, while the context does not provide explicit details on measures taken to ensure generalizability, it suggests that the authors are aware of its importance and are exploring ways to improve it.
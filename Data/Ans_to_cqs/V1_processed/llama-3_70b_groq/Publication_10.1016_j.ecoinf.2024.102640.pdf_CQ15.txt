The optimization techniques applied in the deep learning pipeline mentioned in the context are Adam and AdamW. 

In the EPDNet training, the Adam optimizer is used with a learning rate of 0.0001. 

In the SegFormer training for deep GVI learning, the AdamW optimizer is used with a momentum parameter β1 set to 0.9, β2 set to 0.999, and ε set to 10^-8. A cosine annealing schedule is also employed, and the initial learning rate is set to 0.0001.
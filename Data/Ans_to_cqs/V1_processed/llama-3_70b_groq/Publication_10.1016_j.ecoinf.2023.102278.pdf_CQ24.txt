Based on the provided context, it appears that dropout regularization is employed to handle randomness in the deep learning pipeline. Dropout regularization is a technique used to prevent overfitting in neural networks by randomly dropping out neurons during training. This helps to reduce the capacity of the network and prevent it from memorizing the training data.

Additionally, the context mentions the use of stochastic gradient descent (SGD) with a learning rate and momentum, which also introduces randomness in the optimization process. However, it is mentioned that the Adam optimizer was chosen over SGD due to its better performance.

There is no mention of a random seed value being used to handle randomness in the deep learning pipeline. The context does not provide information on how the randomness introduced by dropout regularization and stochastic gradient descent is controlled or reproduced.

It is worth noting that the context focuses more on the architecture and training of the deep learning model, rather than the specific strategies employed to handle randomness. Therefore, it is possible that other strategies may be employed to handle randomness, but they are not mentioned in the provided context.
The strategy implemented to monitor the model performance during training is not explicitly stated in the provided context. However, it can be inferred that the model was trained over 500 epochs with an initial learning rate of 10, halved every 20 rounds to optimize performance. This suggests that the learning rate was adjusted during training to improve the model's performance. Additionally, the use of a dropout rate of 50% post the fully connected layer may have helped to prevent overfitting and improve the model's generalizability. However, the specific strategy used to monitor the model's performance during training, such as the use of validation sets or metrics, is not mentioned.
The provided context does not explicitly mention optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on machine learning algorithms, hyperparameters, and model evaluation techniques. It discusses various machine learning algorithms, including Random Forest (RF), Particle Swarm Optimization (PSO), Gene Expression Programming (GEP), and others. However, it does not delve into the specifics of deep learning optimization techniques.

The only mention of optimization is in the context of hyperparameter tuning, where grid search and cross-validation are used to select optimal hyperparameters for the machine learning models. The context also mentions the use of Python libraries for hyperparameter tuning, but it does not provide information on optimization techniques used in deep learning pipelines.

Therefore, based on the provided context, I do not have enough information to answer the query about optimization techniques applied in the deep learning pipeline.
The provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD or Adam. The context primarily focuses on various machine learning models, including KNN, GP, TREE, RF, XGB, SVR, and MLP, and their corresponding hyperparameters. It also discusses the hyperparameter optimization process and model selection.

However, it is worth noting that the context mentions "learning_rate" as a hyperparameter for XGB and MLP models. This suggests that some form of gradient-based optimization might be used, but the specific optimization technique (e.g., SGD, Adam) is not explicitly stated.

In conclusion, based on the provided context, I don't know the specific optimization techniques applied in the deep learning pipeline.
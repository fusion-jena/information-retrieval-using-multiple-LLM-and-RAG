The hyperparameters used in the deep learning model are as follows: the optimizer used is Adam, with a learning rate of 1.0⋅10^(-4), weight decay of 1.0⋅10^(-3), and momentum of 1.0⋅10^(-3). These hyperparameters were chosen after comparing with stochastic gradient descent (SGD) with the same parameters, but SGD achieved a 1%–2% decrease in accuracy compared to using the Adam optimizer.
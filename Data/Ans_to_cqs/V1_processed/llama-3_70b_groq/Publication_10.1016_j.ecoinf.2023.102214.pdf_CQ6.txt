The provided context does not explicitly mention the preprocessing steps involved before training a deep learning model, such as normalization, scaling, or cleaning. The text focuses on the challenges of data collection, the importance of large datasets, and the use of data augmentation techniques (specifically, left-right mirroring) to reduce overfitting. It also discusses the evaluation metrics used to assess the model's performance, such as top-k accuracy.

Therefore, I don't know the specific preprocessing steps involved in this particular deep learning model, as this information is not provided in the given context.
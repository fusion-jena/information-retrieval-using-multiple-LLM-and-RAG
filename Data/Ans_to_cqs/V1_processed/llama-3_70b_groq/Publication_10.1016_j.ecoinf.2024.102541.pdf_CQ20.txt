The hardware resources used for training the deep learning model are a NVIDIA GeForce GTX 1080 Ti GPU, along with CUDA 10.2 for GPU parallelization. This is mentioned in the experimental settings section, which states that the system used for training and validation was remotely connected to Ubuntu 18.04.6 LTS, with NVIDIA-SMI 470.103.01 and GeForce GTX 1080 Ti, using CUDA 10.2 for GPU parallelization. There is no mention of TPUs being used.
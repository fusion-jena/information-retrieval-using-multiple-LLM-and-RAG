According to the provided context, two optimization techniques are mentioned: Adam and Stochastic Gradient Descent (SGD). Specifically, the Adam optimizer was used to train the CNNs from scratch, with a learning rate of 10−3 and other parameters set to their defaults. On the other hand, SGD was used to fine-tune the top layer of the ResNet152 model, with a learning rate of 10−3 and a weight decay rate set to 5 × 10−4.
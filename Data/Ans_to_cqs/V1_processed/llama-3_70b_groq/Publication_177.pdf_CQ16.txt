The provided context does not explicitly mention the criteria used to determine when training is complete. However, it does mention that the models were trained for a maximum of 10 epochs using the stochastic gradient descent with a momentum ('sgdm') optimizer. This suggests that the training process was stopped after a fixed number of epochs, rather than based on a specific criterion such as a validation loss plateau.
The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention that the data set was randomly split into a training set (90% of the entries) and a test set (10%) prior to training the IUC-NN model. Additionally, it mentions that 20% of the training set was used for validation, and cross-validation was performed by shifting the validation set 5 times to quantify the average validation cross-entropy loss and accuracy. 

This implies that the authors are aware of the importance of handling randomness in the data splitting and validation process, but the specific strategy used to handle randomness in the deep learning pipeline is not mentioned. Therefore, I don't know the specific strategy employed to handle randomness in the deep learning pipeline.
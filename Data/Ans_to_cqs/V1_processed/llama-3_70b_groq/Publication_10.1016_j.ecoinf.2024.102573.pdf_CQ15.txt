According to the provided context, the optimization technique applied in the deep learning pipeline is Adam optimizer with a cosine decay scheduler for the learning rate. This is mentioned in the sentence: "The models were trained for 100 epochs using Adam optimizer (Kingma and Ba, 2017) with the default hyper-parameters and a cosine decay scheduler for the learning rate."
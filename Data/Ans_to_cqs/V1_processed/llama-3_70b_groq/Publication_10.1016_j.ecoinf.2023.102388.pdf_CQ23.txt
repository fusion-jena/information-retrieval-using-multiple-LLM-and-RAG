Based on the provided context, it appears that the measures taken to ensure the generalizability of the deep learning model include the use of batch-norm, weight decay, and dropout techniques. These techniques were used to reduce the overfitting level in the "no-overfitting" setting. Additionally, the optimization algorithm used was Stochastic Gradient Descent (SGD) with a learning rate of 0.0001. 

However, it does not mention the use of a diverse dataset, cross-validation, or stratified splitting as measures to ensure generalizability. The context focuses more on the techniques used to prevent overfitting, which is a related but distinct concept from generalizability. Therefore, it cannot be concluded that these specific measures were taken to ensure generalizability.
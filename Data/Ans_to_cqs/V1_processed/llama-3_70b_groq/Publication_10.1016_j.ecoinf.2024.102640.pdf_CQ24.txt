Unfortunately, the provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as random seed values. The context primarily focuses on computer vision and image segmentation tasks, discussing various models, architectures, and their performance metrics. It does not delve into the topic of handling randomness in deep learning pipelines.

However, in general, deep learning pipelines often employ strategies such as:

1. Fixing random seeds: Setting a fixed random seed value to ensure reproducibility of results.
2. Data shuffling: Shuffling the training data to reduce the impact of randomness in the training process.
3. Ensemble methods: Combining the predictions of multiple models trained with different random initializations to reduce the effect of randomness.
4. Hyperparameter tuning: Performing hyperparameter tuning using techniques like grid search, random search, or Bayesian optimization to find the optimal hyperparameters that are less sensitive to randomness.

These strategies are not mentioned in the provided context, and I do not have enough information to provide a more specific answer.
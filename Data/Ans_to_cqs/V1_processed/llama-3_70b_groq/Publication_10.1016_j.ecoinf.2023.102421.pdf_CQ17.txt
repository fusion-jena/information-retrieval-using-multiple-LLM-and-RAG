The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention some hyperparameters related to regularization in the context of XGBoost and LightGBM, which are gradient boosting frameworks, not deep learning pipelines.

The mentioned hyperparameters are:

* 'reg_alpha' and 'reg_lambda' which are related to L1 and L2 regularization, respectively.
* 'lambda_l1' and 'lambda_l2' which are also related to L1 and L2 regularization, respectively.

These hyperparameters are used to control the regularization strength in the gradient boosting models. However, the context does not mention dropout, which is a regularization method commonly used in deep neural networks.

Therefore, based on the provided context, I can only say that L1 and L2 regularization are mentioned as regularization methods, but not in the context of deep learning pipelines.
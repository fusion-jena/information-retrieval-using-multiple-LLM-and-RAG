Based on the provided context, it is mentioned that a batch normalization layer is used to accelerate the learning process (Ioffe and Szegedy, 2015). However, there is no explicit mention of other preprocessing steps such as normalization, scaling, or cleaning.

Batch normalization is a technique used to normalize the input data for each layer, which can help with the stability and speed of training. However, it is not clear if any other preprocessing steps were performed on the spectral reflectance data before feeding it into the MLP model.

Therefore, I can only confirm that batch normalization is involved as a preprocessing step, but I don't know about other preprocessing steps such as normalization, scaling, or cleaning.
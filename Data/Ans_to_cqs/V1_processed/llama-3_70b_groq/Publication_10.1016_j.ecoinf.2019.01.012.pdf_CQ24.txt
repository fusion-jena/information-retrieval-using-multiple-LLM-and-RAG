The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as random seed value. However, it does mention the use of dropout as a regularization technique to prevent overfitting in the CNN architecture. Specifically, it mentions the use of conventional dropout on the fully connected layer and spatial dropout on each convolutional layer. Dropout is a technique that randomly drops out neurons during training to prevent the model from relying too heavily on any single neuron. This can help to reduce overfitting and improve generalization.

It is worth noting that the context does not mention the use of random seed values, which are often used to ensure reproducibility in deep learning experiments. Random seed values can be used to initialize the random number generators used in the model, ensuring that the same results are obtained when the experiment is run multiple times.

In summary, while the context mentions the use of dropout as a regularization technique, it does not provide information on strategies employed to handle randomness in the deep learning pipeline, such as random seed values.
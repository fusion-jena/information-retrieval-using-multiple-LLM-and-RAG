The provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline. It discusses active learning strategies, dimensionality reduction, and the use of deep neural networks, but it does not specify the optimization techniques used.

However, it does mention the SGD (Stochastic Gradient Descent) algorithm in the context of iterative training, stating that "The SGD algorithm is an iterative algorithm, i.e. it is applied many times during training, including multiple times for each image in the dataset." This suggests that SGD might be used as an optimization technique, but it does not provide a comprehensive list of optimization techniques applied in the deep learning pipeline.

Therefore, based on the provided context, I can only confirm that SGD might be used, but I don't know about other optimization techniques like Adam, RMSProp, etc.
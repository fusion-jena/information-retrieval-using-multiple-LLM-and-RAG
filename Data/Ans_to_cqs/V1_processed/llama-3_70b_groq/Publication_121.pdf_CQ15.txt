The optimization techniques applied in the deep learning pipeline mentioned in the context are SGD (Stochastic Gradient Descent) and Adam. 

Specifically, the SGD optimizer is used with a learning rate of 0.5 and momentum of 0.9 for training the models. Additionally, the Adam optimizer is used for optimizing the model with the base model of MobileNetV2.
The optimization technique applied in the deep learning pipeline is the Adam optimizer, which was initialized with a default learning rate of 0.0001 and a weight decay of 0.
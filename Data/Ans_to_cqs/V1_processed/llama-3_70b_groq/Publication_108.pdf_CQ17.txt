Based on the provided context, one of the regularization methods used to prevent overfitting in the deep learning pipeline is dropout. This is mentioned in Section 3, Results, where it is stated that "the dropout layers randomly dropped some neurons, causing the feed-forward network to perform worse, while dropout was deactivated and all neurons were kept during the validation." Additionally, in Section 4.2, Limitations of This Study, it is mentioned that "the dropout layer was used for regularization" and that "the dropout rate was set to increase from one encoding block to the next by a step of 0.1 (starting from 0.1), resp. decreasing from one decoding block to the next, and was found to work well for our setting."

There is no mention of L2 regularization in the provided context. It is possible that other regularization methods may have been used, but they are not mentioned in the provided text.
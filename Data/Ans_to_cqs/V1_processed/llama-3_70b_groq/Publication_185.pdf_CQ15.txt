According to the provided context, the optimization technique applied in the deep learning pipeline is Stochastic Gradient Descent (SGD). This is mentioned in the text as "The model was trained using stochastic gradient descent and the backpropagation algorithm included in Caffe". There is no mention of Adam or any other optimization techniques in the provided context.
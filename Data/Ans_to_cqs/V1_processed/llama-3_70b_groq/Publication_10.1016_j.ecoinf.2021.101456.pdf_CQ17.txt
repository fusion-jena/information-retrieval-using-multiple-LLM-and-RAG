The provided context does not explicitly mention regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. The context primarily focuses on Variational Autoencoders (VAEs) and the trade-off between reconstruction loss and regularization loss.

However, it does mention the regularization term ℒreg, which is the Kullback-Leibler divergence (KLD) term, used to regularize the encoder density to match the prior over the latent variable. This regularization term controls the smoothness or regularity of the latent space.

Additionally, the context discusses the role of β in balancing the reconstruction and regularization losses, and how different values of β can affect the performance of the VAE. It also mentions β-annealing and cyclic β as methods to achieve a reasonable trade-off between the loss terms.

But, it does not provide information on regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. Therefore, I don't know the answer to this specific question based on the provided context.
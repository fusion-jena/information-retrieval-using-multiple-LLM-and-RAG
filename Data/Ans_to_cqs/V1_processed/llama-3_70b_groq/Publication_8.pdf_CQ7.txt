The provided context does not explicitly mention the criteria used to split the data for deep learning model training. However, it does provide some information about the training and testing process.

In the context, it is mentioned that the model was trained twice: once with a training subset of 498 images on a single GPU for 9000 iterations, and performance was evaluated on the test subset of 155 images. Then, it was trained again on all 653 annotated images on three GPUs for 18000 iterations.

Although the exact criteria for splitting the data are not specified, it can be inferred that the data was split into training and testing subsets. The training subset was used to train the model, and the test subset was used to evaluate its performance. There is no mention of a validation set, which is typically used to tune hyperparameters and prevent overfitting.

In general, the criteria for splitting data into training, testing, and validation sets can vary depending on the specific problem and dataset. Common practices include random splitting, stratified splitting, and k-fold cross-validation. However, without more information, it is impossible to determine the exact criteria used in this specific case.

In summary, the context does not provide explicit criteria for splitting the data, but it does imply that the data was split into training and testing subsets.
The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as random seed value. However, it does mention some techniques that introduce randomness or variability in the data processing pipeline, which can be related to handling randomness. 

For instance, the context mentions "Random tanh distortion" where the tanh function is used to yield a rounded "soft clipping" type of distortion with a probability of 0.25. This suggests that randomness is intentionally introduced in the data processing pipeline to simulate real-world variability.

Additionally, the context mentions "Random clipping and splicing" where spectrograms of the same species are randomly cropped and concatenated to expand the sample size. This technique also introduces randomness in the data processing pipeline.

While these techniques do not directly address handling randomness in the deep learning pipeline, they do demonstrate an awareness of the importance of simulating real-world variability and introducing randomness in the data processing pipeline.

In conclusion, the provided context does not provide a direct answer to the query, but it does suggest that introducing randomness in the data processing pipeline is an important consideration.
The hyperparameters of the model are not optimized using a grid search or random search. Instead, a small dimensional space of hyperparameters was initially defined and explored with the original variables. The initial hyperparameters that composed this space are shown in Table 1. The number of epochs was maintained at 500 as it proved to guarantee the stabilization of the loss curves during the training. The Adam algorithm was used as the optimizer, which is a stochastic gradient descent method. Then, one of the hyperparameters was modified iteratively following the direction that improved the model's performance. This process was done to avoid an exhaustive search that could consume a lot of time.
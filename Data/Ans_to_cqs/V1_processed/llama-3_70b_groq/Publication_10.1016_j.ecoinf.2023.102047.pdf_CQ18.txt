The strategy implemented to monitor the model performance during training is early stopping. According to the context, "we monitored the validation loss using early stopping during training time for every epoch on the validation set." This means that the model's performance on the validation set was evaluated at the end of each epoch, and if the validation loss did not improve for a certain number of epochs (in this case, 20, as specified by the patience of 20), the training process would be stopped. This is a common technique to prevent overfitting and to select the best-performing model during training.
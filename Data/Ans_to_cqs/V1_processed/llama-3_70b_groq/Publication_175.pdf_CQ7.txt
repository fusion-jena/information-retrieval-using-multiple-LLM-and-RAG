Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training (e.g., train, test, validation). The context primarily focuses on the experiments conducted with different datasets and architectures to tackle the problem of limited data. It discusses the use of data augmentation and few-shots learning algorithms as potential solutions. However, it does not provide information on how the data was split for training, testing, and validation purposes. Therefore, I don't know the answer to this question based on the provided context.
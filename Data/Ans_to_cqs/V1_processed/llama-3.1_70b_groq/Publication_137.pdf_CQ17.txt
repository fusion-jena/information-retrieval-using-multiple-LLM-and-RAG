The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention the use of a regularization strength parameter (l) that influences the coarseness of the partition. The context states that the regularization strength is adjusted to achieve better results, with a value of 0.2 yielding the best outcome. Additionally, the learning decay rate is set to 0.7, which allows the learning rate to gradually decay as training progresses. This can be seen as a form of regularization, but it is not explicitly stated as a method to prevent overfitting.
The provided context mentions the use of dropout as a regularization method to prevent overfitting in the deep learning pipeline. Specifically, it states that "dropout was applied" in the best-performing U-Net-like architectures, and that the dropout rate was set to 0.5. Additionally, the context mentions that augmentation is used to resolve the problem of overfitting, where random changes of RGB channels and random vertical and horizontal flips are applied to the original images.

However, it does not mention the use of L2 regularization. It does mention that the study referenced in [53,54] discusses the importance of augmentation in resolving overfitting, but it does not provide further details on other regularization methods used.

It is also worth noting that the context mentions the use of batch normalization, which can also help to prevent overfitting by normalizing the inputs to each layer. However, batch normalization is not typically considered a regularization method in the same way that dropout and L2 regularization are.

Overall, based on the provided context, the regularization methods used to prevent overfitting in the deep learning pipeline are dropout and augmentation.
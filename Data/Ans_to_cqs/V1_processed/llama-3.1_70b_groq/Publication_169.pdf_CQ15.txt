The optimization technique applied in the deep learning pipeline is the Adam algorithm, which is a variant of stochastic gradient descent (SGD). According to the text, "The model parameters are optimized using gradient descent Adam algorithm with all parameters set to default values." This indicates that the Adam algorithm is used to optimize the model parameters during the training process.
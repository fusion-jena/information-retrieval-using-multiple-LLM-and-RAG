Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The text primarily focuses on the architecture and design of the multiscale feature-extraction network (MSNet) and its components, such as Res2Net modules, group convolution, and various convolutional layers. It does not provide information on the optimization techniques used for training the network. Therefore, I do not know the answer to this question based on the provided context.
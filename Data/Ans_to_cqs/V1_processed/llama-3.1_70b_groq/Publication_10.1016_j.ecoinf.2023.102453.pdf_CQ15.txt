Unfortunately, the provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on model development, hyperparameter optimization, and transfer learning techniques. It discusses the use of neural architecture search (NAS) for developing the EfficientNetV2B0 model and the optimization of parameter efficiency and training speed. However, it does not provide information on the specific optimization techniques used in the deep learning pipeline.

Therefore, I do not have enough information to provide a specific answer to this query.
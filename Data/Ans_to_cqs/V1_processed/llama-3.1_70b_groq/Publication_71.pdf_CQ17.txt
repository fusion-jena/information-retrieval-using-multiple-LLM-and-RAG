The provided context mentions the use of dropout as a regularization method to prevent overfitting in the deep learning pipeline. Specifically, it states that a dropout layer was added for each convolutional layer with a dropout rate of '0.1' to mitigate overfitting. The context also mentions that the choice of '0.1' dropout value was the outcome of design studies, where it performed better than two other dropout values, which are '0.05' and '0.2'.

Additionally, the context mentions the use of weight decay, which is another form of regularization, specifically L2 regularization. The weight decay value is set to 0.00004, as shown in Table 1.

Therefore, the regularization methods used to prevent overfitting in the deep learning pipeline, as mentioned in the context, are dropout and weight decay (L2 regularization).
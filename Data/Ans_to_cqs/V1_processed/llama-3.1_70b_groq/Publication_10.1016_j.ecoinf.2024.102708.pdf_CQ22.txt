The metrics used to evaluate the performance of the deep learning model are precision, recall, and F-score. 

Precision is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP). Recall is defined as the ratio of true positives (TP) to the sum of true positives and false negatives (FN). The F-score is the harmonic mean of precision and recall, calculated as 2 × Precision × Recall / (Precision + Recall).

Additionally, the confidence intervals (CI) for precision and recall are also calculated to estimate the uncertainties of each model. The CI is calculated as (Metric) ± z × SE, where z is the z-score that corresponds to a 95% confidence assuming a normal distribution, and SE is the standard error of the metric.

The standard error of precision (SEPrecision) is calculated as √(Precision × (1 - Precision) / Number Detected), and the standard error of recall (SERecall) is calculated as √(Recall × (1 - Recall) / Ground Truth).

These metrics are used to evaluate the performance of different models, including FRCNN, TPH-YOLOv5, YOLOv5s, and YOLOv5m, and to compare their performance across different sites and backgrounds.
The optimization technique applied in the deep learning pipeline is stochastic gradient descent (SGD) with a momentum of 0.9. The initial learning rate is 10^-2, and it is divided by 10 every 20 epochs until it reaches 10^-6.
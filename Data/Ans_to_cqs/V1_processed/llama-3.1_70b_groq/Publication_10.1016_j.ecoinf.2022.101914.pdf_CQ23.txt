To ensure the generalizability of the model, several measures were taken. Firstly, the data was split into evaluation and training datasets prior to training the models, using a common approach to ensure that all models were evaluated with relatively independent data. 

Additionally, the data was split into five folds, with one fold set aside for external evaluation to obtain presences and background points. The remaining four folds were used for internal cross-validation during model training. This cross-validation approach helps to evaluate the model's performance on unseen data and prevents overfitting.

However, it is not mentioned whether the dataset is diverse or if stratified splitting was used. The context only mentions that the data was split into folds, but it does not provide information on how the data was split or whether the splits were stratified.
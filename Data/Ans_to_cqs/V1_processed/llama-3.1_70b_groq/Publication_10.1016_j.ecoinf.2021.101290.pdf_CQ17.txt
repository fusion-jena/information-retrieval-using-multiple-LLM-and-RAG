Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, I can try to provide some general information on regularization methods commonly used in deep learning.

Regularization methods are techniques used to prevent overfitting in machine learning models, including deep learning models. Overfitting occurs when a model is too complex and learns the noise in the training data, resulting in poor performance on unseen data.

Two common regularization methods used in deep learning are:

1. Dropout: This method involves randomly dropping out neurons during training, which helps to prevent the model from relying too heavily on any single neuron.
2. L2 regularization (also known as weight decay): This method involves adding a penalty term to the loss function that is proportional to the magnitude of the model's weights. This helps to prevent the model from having large weights, which can lead to overfitting.

Other regularization methods that are not mentioned in the context but are commonly used in deep learning include:

* L1 regularization (also known as Lasso regularization)
* Early stopping
* Data augmentation
* Batch normalization

It's worth noting that the context does mention online regularized classification algorithms in the paper "Online regularized classification algorithms" by Ying and Zhou (2006), but it does not provide specific information on the regularization methods used in deep learning.

In summary, while the context does not provide explicit information on regularization methods used in deep learning, I can provide some general information on common regularization methods used in deep learning.
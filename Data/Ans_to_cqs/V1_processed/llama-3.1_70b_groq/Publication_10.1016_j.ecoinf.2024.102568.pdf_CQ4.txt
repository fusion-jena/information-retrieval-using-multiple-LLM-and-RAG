Unfortunately, the provided context does not mention any data augmentation techniques applied in the deep learning pipeline, such as flipping, rotating, or scaling. The context primarily focuses on the configuration and performance of the LSTM model, including the selection of the temporal window, comparison with other architectures, and training parameters. It also discusses the use of a dropout layer as a regularization method to avoid overfitting, but it does not mention any data augmentation techniques. Therefore, I don't know what data augmentation techniques are applied in the deep learning pipeline.
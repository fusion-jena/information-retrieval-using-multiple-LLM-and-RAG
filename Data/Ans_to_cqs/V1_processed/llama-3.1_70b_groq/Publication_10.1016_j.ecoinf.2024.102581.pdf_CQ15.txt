The provided context mentions several optimization techniques that are applied in the deep learning pipeline, specifically in the context of Convolutional Neural Networks (CNNs). According to Table 4, the CNN Model Optimizer hyperparameter has the following options:

* ADAM
* SGD (Stochastic Gradient Descent)
* RMSProp
* Adadelta
* Adagrad
* Adamax

These are all popular optimization algorithms used in deep learning to update the model's parameters during training. They are used to minimize the loss function and optimize the model's performance.
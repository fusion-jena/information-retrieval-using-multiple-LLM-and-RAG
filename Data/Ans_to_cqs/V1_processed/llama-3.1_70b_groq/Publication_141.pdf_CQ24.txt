The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as random seed value. However, it does mention the use of "dropout" in the ResNet-50 network architecture, which is a technique used to prevent overfitting in deep neural networks. Dropout randomly sets a fraction of the neurons to zero during training, which can help to reduce overfitting and improve the robustness of the model.

Additionally, the context mentions "data augmentation", which is a technique used to artificially increase the size of the training dataset by applying random transformations to the input data. This can help to reduce overfitting and improve the model's ability to generalize to new, unseen data.

However, the context does not provide information on how randomness is handled in terms of random seed value. It is possible that the authors used a fixed random seed value to ensure reproducibility of the results, but this is not explicitly stated.
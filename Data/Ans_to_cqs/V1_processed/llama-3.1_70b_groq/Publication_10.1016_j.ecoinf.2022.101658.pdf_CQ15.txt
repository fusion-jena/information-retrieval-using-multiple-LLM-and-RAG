The optimization technique applied in the deep learning pipeline is the Adam optimizer. According to the text, "We used the cross-entropy loss as the objective function for training our network, and the Adam (Kingma and Ba, 2014) optimizer for updating the trainable parameters." This indicates that the Adam optimizer is used to update the model's parameters during training. There is no mention of SGD (Stochastic Gradient Descent) or other optimization techniques being used in the pipeline.
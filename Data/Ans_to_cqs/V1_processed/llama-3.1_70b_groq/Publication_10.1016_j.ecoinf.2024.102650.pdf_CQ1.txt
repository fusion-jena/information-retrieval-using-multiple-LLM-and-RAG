Unfortunately, the provided context does not explicitly mention the methods used for collecting raw data in the deep learning pipeline. The context primarily focuses on the processing and analysis of text data, specifically tweets, and discusses data cleaning, text filtering, and Chinese text segmentation. It also mentions the use of Python's Jieba Library for Chinese text segmentation and lexical analysis.

However, it can be inferred that the raw data is likely collected from social media platforms, specifically Twitter, as the context mentions tweets and the analysis of text data. But the exact methods used for collecting this data, such as APIs, web scraping, or other means, are not specified.

Therefore, I can only provide a partial answer, and it is not possible to determine the exact methods used for collecting raw data in the deep learning pipeline based on the provided context.
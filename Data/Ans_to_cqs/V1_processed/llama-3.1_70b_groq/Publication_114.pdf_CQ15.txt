The optimization technique applied in the deep learning pipeline, as mentioned in the provided context, is the Adam optimizer. According to the text, "The Adam optimizer [48] was used to optimize the learning rate." This indicates that the Adam optimizer was used to adjust the learning rate during the training process. There is no mention of other optimization techniques such as SGD (Stochastic Gradient Descent) being used in the pipeline.
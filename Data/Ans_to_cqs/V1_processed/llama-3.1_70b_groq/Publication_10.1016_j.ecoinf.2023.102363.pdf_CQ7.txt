Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training. However, it does provide some information on how the data is used during training and validation.

According to the context, the model is validated during training by combining the original base model validation set with 100 frames per class from the labelled December 3rd 2019 training set, randomly pooled per experiment. This suggests that the data is split into at least two sets: a training set and a validation set.

Additionally, the context mentions that the model is fine-tuned with randomly pooled training sets of increasing size (sets of 50, 100, 200, 300 and 500-frames) from the Gulf of Mexico acoustic data collected December 3rd 2019. This implies that the training data is split into smaller subsets for fine-tuning, but the exact criteria for splitting the data is not specified.

Overall, while the context provides some information on how the data is used during training and validation, it does not explicitly state the criteria used to split the data.
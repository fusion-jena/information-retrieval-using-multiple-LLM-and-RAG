To ensure the generalizability of the deep learning model, the researchers took several measures. Firstly, they divided the data into 4 non-overlapping subsites to maximize the independence between the training and testing sets. This approach is mentioned in section 4.2.3 of the text. 

Additionally, the researchers acknowledged the potential issue of overestimating practical performance when randomly dividing a single dataset into training and testing sets. They also recognized the importance of using data augmentation or building datasets that better reflect real percentages to improve the model's generalizability.

However, it is not explicitly mentioned in the provided text whether cross-validation or stratified splitting were used. The text only mentions the division of data into non-overlapping subsites and the potential use of data augmentation to improve generalizability.
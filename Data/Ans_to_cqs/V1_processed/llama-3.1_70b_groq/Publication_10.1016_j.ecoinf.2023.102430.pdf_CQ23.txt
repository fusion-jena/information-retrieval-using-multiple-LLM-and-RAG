To ensure the generalizability of the deep learning model, specifically the convolutional neural network (AlexNet), the researchers took several measures. 

Firstly, they split their dataset into training (80%) and testing sets (20%). This is a common practice in machine learning to evaluate the model's performance on unseen data.

Secondly, they repeated the training-testing process 10 times for each algorithm, using the same training and testing picture subsets. This is a form of cross-validation, which helps to assess the model's performance consistency and reduces the risk of overfitting.

Additionally, the dataset used for training and testing consisted of 6570 pictures, which is a relatively large dataset. This diverse dataset likely helped to improve the model's generalizability.

However, it is not explicitly mentioned whether stratified splitting was used to ensure that the training and testing sets had the same proportion of classes. But the use of a large dataset, cross-validation, and splitting the data into training and testing sets suggests that the researchers took steps to ensure the generalizability of the model.
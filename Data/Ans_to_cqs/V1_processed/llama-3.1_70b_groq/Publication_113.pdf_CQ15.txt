The provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, except for a brief mention of the Stochastic Gradient Descent (SGD) algorithm. According to the text, the SGD algorithm is an iterative algorithm that calculates the contribution of each parameter to the loss value and adjusts the parameters to minimize the loss value. However, it does not provide information on other optimization techniques such as Adam.

Therefore, based on the provided context, the only optimization technique that can be confirmed is the Stochastic Gradient Descent (SGD) algorithm. Other optimization techniques, such as Adam, are not mentioned in the context.
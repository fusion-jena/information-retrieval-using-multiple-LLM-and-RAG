The strategy implemented to monitor the model performance during training is the early stopping technique. This technique involves randomly splitting the data set into two sets: 80% for model training and 20% for model testing. The model weights are randomly initialized, and the training process is stopped when the network begins to overfit the data, i.e., the error on the validation set increases. This approach helps to prevent overfitting and ensures that the model generalizes well to new, unseen data.
The provided context does not explicitly mention the preprocessing steps involved before training a deep learning model, such as normalization, scaling, or cleaning. However, it does mention that the orthophotos were divided into small labeled tiles (or image cubes) of 256 Ã— 256 pixels, and the data was split into training, validation, and test datasets.

It is mentioned that the training and validation images were read into arrays, but the context does not provide further details on any preprocessing steps that may have been applied to the data. Therefore, it is not possible to provide a comprehensive answer to the query based on the provided context.

However, it is worth noting that the context does mention the use of a dropout layer in the encoding path of the network, which is a form of regularization that can help prevent overfitting. Additionally, the context mentions the use of a ReLU activation function, which can help to introduce non-linearity into the model. But these are not preprocessing steps, rather they are part of the model architecture.
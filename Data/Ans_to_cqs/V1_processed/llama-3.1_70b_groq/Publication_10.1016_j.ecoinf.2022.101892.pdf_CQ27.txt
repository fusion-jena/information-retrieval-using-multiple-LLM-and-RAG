Unfortunately, the provided context does not explicitly mention the process followed to deploy the trained deep learning model. The context primarily focuses on the training process, including the use of PyTorch, pre-training on ImageNet, fine-tuning on PAD Full and PAD Lite datasets, and applying data augmentation techniques.

However, it does mention that the model was tested for inference-time efficiency by measuring the maximum queries per second (QPS) achieved under real-world conditions. This suggests that the model was deployed on a GPU (Nvidia V100) for testing purposes, but the exact deployment process is not specified.

Therefore, I do not have enough information to provide a detailed answer to the query.
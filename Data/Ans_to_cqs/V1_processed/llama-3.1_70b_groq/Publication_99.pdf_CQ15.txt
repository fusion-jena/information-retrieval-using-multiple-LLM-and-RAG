The optimization techniques applied in the deep learning pipeline, as mentioned in the provided context, include the Adam optimizer. Specifically, the Adam optimizer is used in the following models:

1. PConv networks: with a learning rate of 2e-4.
2. Pix2Pix networks: with a learning rate of 9e-5.
3. Classification networks: with a learning rate of 0.03.

Additionally, the classification networks also use cross-entropy loss as a loss function optimized by the Adam optimizer.

It is also mentioned that the classification networks were trained with a batch size of 32 and various data augmentation techniques such as height and width shift, flipping, zooming, and brightness changes were applied.

There is no mention of SGD (Stochastic Gradient Descent) being used as an optimization technique in the provided context.
The provided context does not explicitly state the criteria used to split the data for deep learning model training. However, it does mention that the best performing classifier was used to classify the whole dataset, and then the relevant texts from these predictions were used as data from which training, development, and testing sets were selected for the next classification task.

Additionally, it mentions that for classification tasks 2 and 3, the data was split into two samples: a randomly selected sample and a balanced stratified sample. The balanced stratified sample was created by grouping the data according to their initial exploitation-related query term (stemmed), resulting in 19 groups, from each of which 110 texts for labeling were randomly selected, such that each query group was equally represented in the data.

It also mentions that the data was split into chunks of 300 tokens using the NLTK word-tokenizer, and chunks that contained any of the original query terms were retained. However, this is not directly related to splitting the data for model training.

Therefore, the answer to the query is that the context does not provide a clear and explicit criteria for splitting the data for deep learning model training, but it does mention some strategies used for sampling and selecting data for model training and evaluation.
The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of spectral normalization, which is a regularization technique used to stabilize the training of deep neural networks.

Spectral normalization is a technique that normalizes the spectral norm of the weight matrices in a neural network. This helps to prevent the weights from becoming too large, which can lead to overfitting.

Additionally, the context mentions the use of LeakyReLU activation function, which can also help to prevent overfitting by allowing a small fraction of the input to pass through, even when the input is negative.

However, it does not mention other common regularization techniques such as dropout, L2 regularization, or batch normalization. Therefore, I cannot provide a comprehensive answer to the query based on the provided context.

It is worth noting that the context is focused on image processing and computer vision tasks, and the regularization techniques used may be specific to these tasks. If you have any further information or context, I may be able to provide a more detailed answer.
Unfortunately, the provided context does not mention specific optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on explaining various loss functions and metrics used for bounding box estimation and classification, such as YOLO loss, box loss, and classification loss. It also discusses the use of k-fold cross-validation for determining the ideal training-test split and the application of Intersection over Union (IoU) and distributional focal loss (DFL) for measuring errors in bounding box coordinates and dimensions.

Therefore, I do not have enough information to provide a specific answer to the query about optimization techniques used in the deep learning pipeline.
According to the provided context, one optimization technique that is applied in the deep learning pipeline is Adam. This is mentioned in the paper "Adam: A Method for Stochastic Optimization" by Kingma and Ba (2014). Adam is a stochastic optimization method that adapts the learning rate for each parameter based on the magnitude of the gradient. However, it is not explicitly mentioned in the context whether other optimization techniques like SGD are used. Therefore, the answer is limited to Adam, and it is not possible to confirm the use of other optimization techniques like SGD based on the provided context.
Unfortunately, the provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention that a learning rate of 0.001 was used during active learning iterations, which suggests that some form of gradient descent-based optimization was used. Additionally, the context mentions that a grid search technique was used to tune hyperparameters, including the learning rate, but it does not provide further details on the optimization algorithm used. Therefore, I cannot provide a definitive answer to this question based on the provided context.
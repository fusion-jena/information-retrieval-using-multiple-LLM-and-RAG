The provided context does not explicitly mention how the hyperparameters of the model are optimized. However, it does mention that the Stochastic Gradient Descent (SGD) optimizer and Adam optimizer were used, with learning rates ranging from 10^-2 to 10^-8. This suggests that the learning rate was tuned, but the method used for tuning is not specified. It does not mention whether grid search, random search, or any other method was used for hyperparameter optimization.
Unfortunately, the provided context does not mention specific strategies employed to handle randomness in the deep learning pipeline, such as random seed values. The context primarily focuses on the challenges of annotation discrepancies, inter-annotator variability, and methods to address these issues, such as majority voting and soft labeling. It also discusses the importance of annotation guidelines and the impact of annotator strategies on model performance.

However, it does mention that Convolutional Neural Networks (CNNs) are trained on annotations from both novices and experts, and that the results show variations in model performance. This suggests that the study may have involved some level of randomness or variability in the training process, but the specific strategies used to handle this randomness are not mentioned.

Therefore, I do not have enough information to provide a detailed answer to the query.
The hyperparameters used in the DeepLabV3+ model are listed in Table 1. The learning policy used is Poly, which is a polynomial learning rate schedule. The base learning rate is set to 0.0001, and the learning rate decay factor is 0.1. The learning rate decay step is 2000, and the learning power is 0.9. The training number of steps is set to be greater than or equal to 100,000.

The optimizer used is not explicitly mentioned, but the momentum is set to 0.9, which is a common value for the momentum parameter in the stochastic gradient descent (SGD) optimizer. The train batch size is set to 2, and the weight decay is set to 0.00004.

Other hyperparameters include the train crop size, which is set to 513x513, and the last layer gradient multiplier, which is set to 1. The upsample logits is set to True, and the drop path keep prob is set to 1. The tf_initial_checkpoint is set to deeplabv3_pascal_train_aug, which is a pre-trained model.

The slow start step is set to 0, and the slow start learning rate is set to 1e-4. The fine tune batch norm is set to False, and the min scale factor is set to 0.5. The max scale factor is set to 2, and the scale factor step size is set to 0.25. The atrous rates are set to [6, 12, 18], and the output stride is set to 16.
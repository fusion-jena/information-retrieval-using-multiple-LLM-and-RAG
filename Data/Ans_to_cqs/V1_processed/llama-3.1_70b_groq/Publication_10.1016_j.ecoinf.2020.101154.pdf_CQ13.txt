The context provided does not mention a deep learning model, but rather a Gradient Boosting Regression Tree (GBRT) model and a Generalized Additive Model (GAM). However, I can provide information on the hyperparameters used in the GBRT model.

The hyperparameters used in the GBRT model are:

* Learning rate: 0.05 for the RI models (both CS and NW datasets) and 0.005 for the RY models (both CS and NW datasets)
* Maximum tree depth: 4 for the RI models (both CS and NW datasets) and 2 for the RY NW model, 8 for the RY CS model
* Number of iterations: 120 for the RI CS model, 106 for the RI NW model, and 500 for the RY models (both CS and NW datasets)
* Subsample rate: 0.75% for all models
* Early stopping was used to determine when to stop the model training to avoid overfitting.

Note that these hyperparameters were tuned manually or using a grid search for the two-part split models, and manually for the three-part split models.
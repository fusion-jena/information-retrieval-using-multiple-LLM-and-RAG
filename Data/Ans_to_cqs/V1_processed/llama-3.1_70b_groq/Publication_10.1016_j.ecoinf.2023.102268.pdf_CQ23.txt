To ensure the generalizability of the deep learning model, several measures were taken. 

Firstly, the labeled dataset was split into training and testing datasets in a 70/30 ratio. This is a common practice to evaluate the model's performance on unseen data.

Secondly, 50% of the training data was held out for validation. This is also known as a validation set, which is used to tune the model's hyperparameters and prevent overfitting.

Thirdly, a 10-fold cross-validation was used to define the score-to-posterior-probability transformation function. This is a technique used to evaluate the model's performance on multiple subsets of the data, which helps to ensure that the model is not overfitting to a particular subset.

Lastly, the model was trained using the ResNet-50 fc1000 activations of the full training dataset, and a confusion matrix was generated for the test data not seen during training. This provides an evaluation of the model's performance on unseen data.

While the text does not explicitly mention the use of a diverse dataset or stratified splitting, the use of cross-validation and a separate test set suggests that efforts were made to ensure the model's generalizability.
The hyperparameters used in the deep learning model are as follows:

* Learning rate: 10^(-3)
* Optimizer: Adam optimizer (Kingma et al., 2015)
* Batch size: 256
* Number of epochs: 5000
* Model parameters: a = 0.2, b = 0.05, w1 = w2 = 1.2, w3 = 0.9, and w4 = 1.1
* Scaling of the clustering loss (γ): cycled between 0.01 and 0.2 every 100 epochs
* Decision to adapt β: taken every fifth epoch

Note that these hyperparameters were used for training the dynamic β-VAE model, and the model parameters (a, b, w1, w2, w3, and w4) were tuned on an independent dataset and found to be sufficiently robust on the dataset used in this work without any fine-tuning.
Unfortunately, the provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The text only mentions that the parameters of the deep learning network are adjusted following an optimization process, but it does not specify which optimization technique is used.

However, it does mention that the ResNet network introduced residual connections between blocks, which lifted the arbitrary limit of stacked consecutive blocks caused by problems in the optimization process. This suggests that the optimization process is an important aspect of deep learning, but the specific techniques used are not mentioned.

Therefore, I do not know the specific optimization techniques applied in the deep learning pipeline mentioned in the context.
The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, based on general knowledge, some common regularization methods used in deep learning include:

1. Dropout: a technique where a fraction of neurons are randomly dropped out during training, helping to prevent overfitting.
2. L2 regularization (also known as weight decay): a technique where a penalty term is added to the loss function to discourage large weights.

These methods can help prevent overfitting by reducing the capacity of the model to fit the training data too closely.
The provided context does not explicitly mention strategies for handling randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention that the acquired image data were "randomly generated" in an 8:2 ratio for training and test sets. This suggests that some form of randomization is used in the data preparation step.

Additionally, the context mentions that the training dataset was increased to 6360 photos using "random cropping, vertical flipping, and horizontal flipping". This implies that random data augmentation techniques are used to artificially increase the size of the training dataset and potentially reduce overfitting.

However, it does not provide information on whether a fixed random seed value is used to ensure reproducibility of the results. Therefore, I do not know if any specific strategies are employed to handle randomness in the deep learning pipeline, such as setting a random seed value.
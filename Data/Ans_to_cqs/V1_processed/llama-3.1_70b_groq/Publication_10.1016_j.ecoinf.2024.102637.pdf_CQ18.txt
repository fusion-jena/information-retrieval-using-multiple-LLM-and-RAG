The provided context does not explicitly mention a specific strategy implemented to monitor the model performance during training. However, it does mention the evaluation indicators used in the experiment, which include testing accuracy, inference time, precision, recall, specificity, F1 score, and sensitivity. These metrics are typically used to evaluate the performance of a model, but it is not clear if they are used to monitor the performance during training or only after the training is complete.

Additionally, the context mentions the use of a cross-entropy loss function, which is a common loss function used in classification problems. The loss function is typically used to evaluate the difference between the model's predictions and the actual labels, and it can be used to monitor the model's performance during training. However, it is not explicitly stated in the context that the loss function is used for this purpose.

Therefore, based on the provided context, I do not know the specific strategy implemented to monitor the model performance during training.
The optimization technique applied in the deep learning pipeline is the Adam optimizer. According to the text, "We used the ResNet50 architecture... with an Adam optimizer (Kingma and Ba, 2014)". Additionally, the text mentions that a one-cycle learning rate policy with a minimum learning rate of 0.000001 and a maximum learning rate of 0.001 was used, as described by Smith (2018).
The provided context mentions the hyperparameters used in the Extreme Gradient Boosting (XGBoost) model and the Artificial Neural Networks (ANN) model.

For the XGBoost model, the hyperparameters mentioned are:

* ntree: builds a multitude of decision trees
* tree node size: determines the smallest number of observations in a tree subset terminal node (default value is 1)
* gradient boosting iteration: a repetition process of increasing or decreasing weights in a training dataset

For the ANN model, specifically the Multilayer Perceptron Neural Network (MLPNN), the hyperparameters mentioned are:

* learning rate: optimal value is 0.01
* momentum: optimal value is 0.18
* training iterations: optimal value is 500

Additionally, the context mentions that all hyperparameters were tuned with a grid search method. However, it does not provide information on the optimizer used in the deep learning model.
The provided context mentions a few regularization methods used to prevent overfitting in the deep learning pipeline. 

One of the methods mentioned is L2 regularization, also known as weight decay. This is mentioned in the context of the optimization problem solved for ridge regression, where the regularization parameter Î± controls the strength of the L2 term.

Another method mentioned is dropout, which is mentioned as an additional technique tried for the Inception model, specifically for the Deep Otolith Inception-V3 model. 

Additionally, the context mentions the use of class weights in the cross-entropy loss function to deal with dataset imbalance, which can also be seen as a form of regularization.

It is also worth noting that the use of AdamW optimizer for Hugging Face models and Adam optimizer for Inception-V3 can also be seen as a form of regularization, as these optimizers have built-in mechanisms to prevent overfitting.

However, it is not explicitly mentioned if dropout is used in the other models (ResNet-50 and Vision Transformer).
To ensure the generalizability of the machine learning models, several measures were taken. 

Firstly, a repeated nested k-fold cross-validation approach was employed. An outer 10 k-fold cross-validation was used to divide the data into holdout data and data for model development. This helped to reduce the potential bias and variance related to random sampling.

Secondly, an inner 5 k-fold cross-validation was used to minimize the bias caused by tuning of hyperparameters on training results. This inner cross-validation was used to optimize the hyperparameters, and the training performance was calculated based on the inner cross-validated ensemble.

Thirdly, the process of dividing the data into holdout and training sets, and optimizing the hyperparameters, was repeated 15 times for each unique model. This helped to further reduce the potential bias and variance related to random sampling.

While the text does not explicitly mention the use of a diverse dataset or stratified splitting, it does mention the use of cross-validation and repeated sampling to ensure the generalizability of the models. However, it is worth noting that the text does not specifically mention deep learning models, but rather machine learning models in general.
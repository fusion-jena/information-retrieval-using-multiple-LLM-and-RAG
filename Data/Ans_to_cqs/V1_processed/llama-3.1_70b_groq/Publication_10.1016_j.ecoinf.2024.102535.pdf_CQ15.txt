The optimization technique applied in the deep learning pipeline is the Adam optimizer, as mentioned in the context: "During training, the cost function was minimized using the Adam optimizer (Kingma, 2017)". Additionally, a learning rate decay procedure proposed by Ganin et al. (2017) is also adopted to improve convergence during training. This procedure involves adjusting the learning rate using the equation μe = μ0 / (1 + αp)β, where μ0 is the initial learning rate, α and β are hyperparameters, and p is the current training epoch.
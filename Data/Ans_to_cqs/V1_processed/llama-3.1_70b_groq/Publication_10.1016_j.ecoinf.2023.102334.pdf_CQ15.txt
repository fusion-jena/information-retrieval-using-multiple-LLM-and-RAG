Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the Vision Transformer (ViT) architecture, its application to image classification, and the use of a hybrid approach combining ViT with a CNN backbone. It also mentions the use of an improved SoftMax function to speed up training and improve robustness, but it does not provide information on the optimization techniques used.

Therefore, I do not know the specific optimization techniques applied in the deep learning pipeline, and I cannot provide a more detailed answer.
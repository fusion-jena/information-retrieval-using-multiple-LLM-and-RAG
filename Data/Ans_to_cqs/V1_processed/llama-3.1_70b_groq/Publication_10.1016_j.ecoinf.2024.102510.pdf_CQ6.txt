Unfortunately, the provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. The context appears to focus on the performance of various pre-trained models, hyperparameter tuning, and the results of fine-tuning these models. It does not provide information on the preprocessing steps that were taken before training the models.

However, it is common practice in deep learning to perform certain preprocessing steps before training a model, such as normalization, scaling, and cleaning the data. These steps can help improve the stability and performance of the model. But without explicit information in the context, I cannot provide a specific answer to this question.

If you're looking for general information on common preprocessing steps in deep learning, I can provide some general information. Typically, preprocessing steps may include:

* Normalization: scaling the data to a common range, usually between 0 and 1, to prevent features with large ranges from dominating the model.
* Scaling: scaling the data to have zero mean and unit variance, which can help improve the stability of the model.
* Cleaning: removing missing or duplicate data, handling outliers, and data augmentation.
* Data augmentation: artificially increasing the size of the training dataset by applying transformations to the existing data.

However, without specific information from the context, I cannot confirm whether these steps were taken in this particular case.
Unfortunately, the provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention the use of a "regular back-propagation algorithm" for jointly training attention mechanisms with a basic model, such as a recurrent neural network or a convolutional neural network (CNN). Additionally, it mentions the use of a "Bayesian optimizer" for controlling the output of each CNN and selecting the optimal policies for data augmentation.
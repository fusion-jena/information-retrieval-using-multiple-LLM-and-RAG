According to the provided context, the optimization technique used in the deep learning pipeline is AdamW (Loshchilov and Hutter, 2017). This is mentioned in the table, which lists the hyperparameters and settings used for training the models. Specifically, the optimizer column states "AdamW (Loshchilov and Hutter, 2017)".

Additionally, the context mentions that the learning rate is set to 0.0001, and the scheduler used is Multistep decay. However, it does not provide information on the use of SGD (Stochastic Gradient Descent) or other optimization techniques in the pipeline.
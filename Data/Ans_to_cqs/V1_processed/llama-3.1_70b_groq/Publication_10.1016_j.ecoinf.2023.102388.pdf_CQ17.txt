According to the provided context, the regularization methods used to prevent overfitting in the deep learning pipeline are:

1. Batch-norm: This technique is used to normalize the input data for each layer, which helps to reduce overfitting.
2. Dropout (0.5): This technique randomly drops out a fraction of the neurons during training, which helps to prevent the model from relying too heavily on any single neuron.
3. Weight decay (0.5): This technique adds a penalty term to the loss function to discourage large weights, which helps to prevent overfitting.

Additionally, the context mentions that the optimization algorithm used is Stochastic Gradient Descent (SGD) with a learning rate of 0.0001, and the batch size is set to 32.

It's worth noting that the context also mentions that the paper "Avoiding overfitting: a survey on regularization methods for convolutional neural networks" by Santos and Papa (2022) provides a comprehensive survey of regularization methods for CNNs, but it does not provide specific details on the methods used in this particular context.

It's also mentioned that L2 regularization is not explicitly mentioned in the provided context, but weight decay is mentioned, which is a form of L2 regularization.
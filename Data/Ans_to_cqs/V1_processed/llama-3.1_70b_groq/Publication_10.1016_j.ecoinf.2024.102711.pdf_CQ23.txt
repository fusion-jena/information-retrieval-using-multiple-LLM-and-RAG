To ensure the generalizability of the deep learning model, the development dataset was divided into two categories: the training dataset and the tuning dataset. However, due to the size constraints of the dataset and the need to utilize all development data records in training to enhance the model's generalization ability, k-fold cross-validation was chosen as the preferred approach for hyperparameter tuning.

In this technique, the development dataset was evenly divided into k groups (with k assumed to be 5 in this study). During each iteration, one group was excluded from the training process, and the model was developed using the remaining dataset. The model's performance was then evaluated using the excluded group (tuning dataset). The average performance of the k models developed on the validation groups represented the performance of the machine learning technique for the selected hyperparameters.

This approach allowed the model to be trained on different subsets of the data and evaluated on unseen data, which helped to improve its generalizability. Additionally, grid search was employed to evaluate the data-driven model's performance using each combination of predefined hyperparameters and identify the best hyperparameters.
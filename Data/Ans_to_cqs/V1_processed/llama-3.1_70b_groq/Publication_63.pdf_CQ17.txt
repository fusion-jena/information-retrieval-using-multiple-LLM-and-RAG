The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention that the models were trained until the validation MAE did not further improve compared to the preceding epochs and diverged from the training MAE ('overfit'). This suggests that the authors monitored the model's performance on the validation set to prevent overfitting, but the specific regularization methods used are not mentioned.

It is also worth noting that the authors used a pre-trained model and fine-tuned it on their dataset, which can help to prevent overfitting by leveraging the knowledge learned from the pre-training task. Additionally, the use of a relatively small learning rate (0.001) and a learning rate decay (0.0001) may also help to prevent overfitting by reducing the magnitude of the updates to the model's weights.

However, without further information, it is not possible to determine the specific regularization methods used in the deep learning pipeline.
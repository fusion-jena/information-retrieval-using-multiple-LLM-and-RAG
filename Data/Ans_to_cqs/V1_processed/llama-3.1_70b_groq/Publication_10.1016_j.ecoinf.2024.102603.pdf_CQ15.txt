Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on comparing XGBoost and deep learning models, discussing their differences in terms of computational requirements, interpretability, and performance on small datasets. It does not delve into the specifics of the deep learning pipeline or optimization techniques used.

However, it does mention that deep learning models typically demand more resources, including GPUs or TPUs, and involve lengthier training times compared to XGBoost. This suggests that deep learning models may require more complex optimization techniques, but the specific techniques used are not mentioned in the provided context.
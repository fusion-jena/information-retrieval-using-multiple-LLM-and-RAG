The provided context does not explicitly mention techniques such as Stratified splitting, oversampling, or undersampling to address data bias during preprocessing. However, it does mention that data augmentation was performed on all patches to compensate for the high class imbalance and allow better generalization.

The data augmentation techniques used include:

1. Random rotations between 0 and 180 degrees
2. Brightness scaling between 0.4 and 1.6
3. Horizontal/vertical flipping

These techniques were applied to all patches on the fly as data was fed to the networks. This approach helps to artificially increase the size of the training dataset and reduce the impact of class imbalance.

It is also worth noting that the context mentions that the use of larger batches may help to address the high imbalance between classes, as it allows the network to focus on inter-class variance. However, this is more related to the training process than preprocessing.

There is no mention of diverse data collection as a technique to address data bias.
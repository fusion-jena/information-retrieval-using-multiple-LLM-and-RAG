The provided context does not specifically mention regularization methods used in deep learning pipelines, such as dropout or L2 regularization. However, it does mention some regularization techniques used in the context of machine learning models for water quality prediction.

According to the text, the LGBM model uses "reg_lambda" to prevent overfitting. Additionally, the text mentions that hyperparameter tuning plays a crucial role in optimizing model performance and preventing overfitting or underfitting issues. Specifically, it mentions that hyperparameters such as "n_estimators", "max_depth", and "subsample" can be used to control overfitting in boosting models.

It is also mentioned that the decision tree model is configured with maximum depth and minimum number of samples required to split a node, which can help prevent overfitting. Furthermore, the text mentions that the gradient boosting model is tuned with hyperparameters such as "n_estimators", "max_depth", and "subsample" to prevent overfitting.

While these methods are not specific to deep learning pipelines, they are regularization techniques used to prevent overfitting in machine learning models. However, I couldn't find any information on dropout or L2 regularization specifically.
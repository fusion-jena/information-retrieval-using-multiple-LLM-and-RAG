The optimization technique applied in the deep learning pipeline is Adam. According to the text, "the optimizer Adam was chosen, with a beta1 of 0.937". This indicates that the Adam optimization algorithm is used to update the model's parameters during training. Additionally, the text mentions that the initial learning rate is 0.01 with a weight decay of 0.005, which are also hyperparameters related to the optimization process.
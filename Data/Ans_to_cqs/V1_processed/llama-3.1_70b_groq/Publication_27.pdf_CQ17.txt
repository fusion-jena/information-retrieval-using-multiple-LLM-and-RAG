The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, based on general knowledge of deep learning, some common regularization methods used to prevent overfitting include:

1. Dropout: This involves randomly dropping out neurons during training to prevent the model from relying too heavily on any single neuron.
2. L2 regularization (also known as weight decay): This involves adding a penalty term to the loss function to discourage large weights.
3. L1 regularization: This involves adding a penalty term to the loss function to discourage large weights, but with a different penalty function than L2 regularization.
4. Early stopping: This involves stopping the training process when the model's performance on the validation set starts to degrade.
5. Batch normalization: This involves normalizing the inputs to each layer to have zero mean and unit variance, which can help to prevent overfitting.

It's worth noting that the provided context appears to be a snippet of a research paper or academic text, and may not be directly related to the query. If more context or information is provided, a more specific answer may be possible.
Unfortunately, the provided context does not mention how the hyperparameters of the model are optimized. It only mentions that the authors did not experiment with fft and hop sizes to keep the number of experiments manageable, and that these two hyper-parameters significantly impact the performance of the (mel-)spectrogram. It also mentions that the kernel and hop sizes of the learnable frontends were fixed, but the spreads of the Gaussian pooling filters for downsampling were learnable. However, it does not provide information on the optimization method used for the hyperparameters.
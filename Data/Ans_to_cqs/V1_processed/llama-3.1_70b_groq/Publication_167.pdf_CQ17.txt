The regularization method used to prevent overfitting in the deep learning pipeline, as mentioned in the provided context, is the dropout operation. The dropout rate is set to 0.1 for the 6th layer and 0 for the other layers. This means that during training, 10% of the neurons in the 6th layer are randomly dropped out, while all neurons in the other layers are retained. This helps to prevent overfitting by preventing the model from relying too heavily on any single neuron or group of neurons.

There is no mention of L2 regularization being used in the provided context. However, it is mentioned that the cross-entropy is used as the loss function, and the Adam optimization algorithm is used to update the weights of the neural network.
To ensure the generalizability of the deep learning model, several measures were taken. 

Firstly, the dataset was split into training and testing sets, although the exact method of splitting is not specified. The model was trained on the whole training dataset and then tested on unseen data, which suggests that the testing data was not used during the training process.

Secondly, cross-validation was used during the hyperparameter tuning process. Specifically, the number of epochs was adjusted to the minimum number of epochs that guaranteed the stabilization of the loss curve in the four folds of the cross-validation. This suggests that a form of k-fold cross-validation was used, where the data was divided into four folds, and the model was trained and evaluated on each fold.

Lastly, the selection of the best set of hyperparameters was made according to three commonly used metrics: binary accuracy, recall, and the area under the ROC curve (AUC). This suggests that the model was evaluated on its ability to generalize to new data using these metrics.

However, it is not clear if the dataset was diverse or if stratified splitting was used. The text does not provide information on the diversity of the dataset or if the splitting was stratified. 

Overall, the use of cross-validation and evaluation on unseen data suggests that some measures were taken to ensure the generalizability of the model, but the lack of information on dataset diversity and stratified splitting leaves some uncertainty.
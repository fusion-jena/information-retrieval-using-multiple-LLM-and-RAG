The hyperparameters of the model are optimized through a process that starts with defining a small dimensional space of hyperparameters and then iteratively modifying one of the hyperparameters at a time. The direction of modification is chosen based on the performance of the model. This approach is not explicitly stated as grid search or random search, but rather a more targeted and iterative approach. The number of epochs is initially set to 500, which is found to guarantee the stabilization of the loss curves during training. The Adam algorithm is used as the optimizer. The k-fold cross-validation method is also used to evaluate the performance of the model and avoid over-fitting.
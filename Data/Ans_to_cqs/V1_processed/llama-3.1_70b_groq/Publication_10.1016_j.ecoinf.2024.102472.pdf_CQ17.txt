Unfortunately, the provided context does not mention specific regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. The context primarily focuses on the use of Extreme Gradient Boosting (XGBoost) and Artificial Neural Networks (ANNs), specifically Multilayer Perceptron Neural Networks (MLPNNs), for prediction tasks. It discusses the tuning of hyperparameters, such as learning rate, momentum, and training iterations, but does not explicitly mention regularization techniques. Therefore, I do not have enough information to provide a specific answer to this question.
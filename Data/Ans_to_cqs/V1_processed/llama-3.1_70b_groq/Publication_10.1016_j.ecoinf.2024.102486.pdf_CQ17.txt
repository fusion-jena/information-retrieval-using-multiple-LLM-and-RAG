Unfortunately, the provided context does not explicitly mention regularization methods used to prevent overfitting in the deep learning pipeline. The context appears to be focused on various topics such as particle Markov-chain Monte Carlo, CUDA programming, and ecological informatics, but it does not provide information on regularization methods in deep learning.

However, I can provide some general information on regularization methods that are commonly used in deep learning. Regularization methods are used to prevent overfitting by adding a penalty term to the loss function or by modifying the model's weights. Some common regularization methods include:

* Dropout: This method randomly drops out neurons during training, which helps to prevent overfitting by preventing the model from relying too heavily on any one neuron.
* L2 regularization (also known as weight decay): This method adds a penalty term to the loss function that is proportional to the magnitude of the model's weights. This helps to prevent overfitting by encouraging the model to use smaller weights.
* L1 regularization: This method adds a penalty term to the loss function that is proportional to the absolute value of the model's weights. This helps to prevent overfitting by encouraging the model to use sparse weights.

It's worth noting that the provided context does mention some deep learning-related topics, such as the Deep Learning Toolbox by MathWorks and the Keras library. However, it does not provide specific information on regularization methods used in deep learning.
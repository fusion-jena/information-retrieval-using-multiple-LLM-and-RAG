The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of weight decay, which is a form of L2 regularization. Specifically, it states that the weight decay was set to 1 Ã— 10^(-5). 

Additionally, the context mentions the use of a learning rate schedule, where the learning rate is reduced by a factor of 0.1 whenever the validation loss remains constant for five epochs. This can also help prevent overfitting by reducing the step size of the gradient descent updates.

There is no mention of dropout or other regularization methods in the provided context.
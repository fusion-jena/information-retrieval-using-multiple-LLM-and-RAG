The hyperparameters of the model are not optimized using grid search or random search. Instead, the learning rate is adjusted automatically using the Adam optimizer, a variant of stochastic gradient optimization. The rest of the parameters, including the number of epochs and batch size, are set empirically. The specific values used are a learning rate of 0.01, 150 epochs, and a batch size of 2500.
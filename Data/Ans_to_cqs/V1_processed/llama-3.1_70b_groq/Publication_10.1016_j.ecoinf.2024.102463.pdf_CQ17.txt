Unfortunately, the provided context does not explicitly mention regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. The context primarily focuses on hyperparameter optimization using the HyperBand algorithm, model architectures (e.g., Recurrent Highway Network), and the distinction between parameters and hyperparameters.

However, it does mention the "vanishing gradient" phenomenon in classical recurrent networks, which can be related to overfitting. But it does not provide information on regularization methods used to address this issue.

Therefore, I do not have enough information to provide a specific answer to the query about regularization methods used to prevent overfitting in the deep learning pipeline.
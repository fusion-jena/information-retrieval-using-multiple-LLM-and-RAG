The provided context does not explicitly mention the preprocessing steps involved before training a deep learning model, such as normalization, scaling, or cleaning. However, it does mention that the data was "preprocessed" in Section II, but the specific steps are not detailed.

It is mentioned that the training dataset was augmented 10 times, resulting in a total of 31,320 images. This suggests that some form of data augmentation was used to increase the size of the training dataset, but the specific techniques used are not specified.

Additionally, the context mentions that the model was trained using the mini-batch gradient descent algorithm with a batch size of 32 and a learning rate of 0.0001, and that the Adam optimizer was used to fine-tune the model. However, this information is related to the training process rather than preprocessing.

Therefore, based on the provided context, it is not possible to provide a detailed answer to the query about the preprocessing steps involved before training a deep learning model.
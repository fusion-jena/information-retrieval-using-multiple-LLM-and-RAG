Unfortunately, the provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily discusses the challenges of using deep learning, the importance of selecting an appropriate mini-batch size, and the use of the ResNet-50 network for image classification tasks.

However, it does mention that the network updates its internal parameters after each iteration, focusing more on images with higher loss scores. This suggests that some form of optimization is taking place, but the specific technique used is not specified.

Therefore, I must conclude that I don't know the specific optimization techniques applied in the deep learning pipeline, as this information is not provided in the given context.
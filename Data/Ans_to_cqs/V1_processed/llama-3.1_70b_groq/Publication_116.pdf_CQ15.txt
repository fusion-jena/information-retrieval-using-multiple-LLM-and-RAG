The deep learning pipeline applied in the experiments are primarily based on the following techniques:

1.  Stochastic Gradient Descent (SGD) - an optimization algorithm that minimizes the objective function by by iteratively applying the the model parameters) 
2.  Adam - a of the model parameters) 
3.  Batch Normalization - a technique that normalizes the activations of the model parameters) 
4.  Inception - a family of neural network architectures that are based on the idea of repeating blocks of smaller networks with shared weights weights.
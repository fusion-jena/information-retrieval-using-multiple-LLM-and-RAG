The provided context does not specifically mention the criteria used to split the data for deep learning model training. However, it does mention that environmental cross-validation with ten folds was used to tune model hyperparameters, as implemented in the R package "mlr3mbo". This suggests that the data was split into ten folds for cross-validation purposes, but the exact criteria used to split the data into training, testing, and validation sets is not specified.

It is worth noting that the context does not explicitly mention deep learning models, but rather mentions machine learning and modeling techniques in general. The context does mention the use of the R package "ranger" for modeling, but does not provide information on how the data was split for model training.

Therefore, based on the provided context, I do not know the exact criteria used to split the data for deep learning model training.
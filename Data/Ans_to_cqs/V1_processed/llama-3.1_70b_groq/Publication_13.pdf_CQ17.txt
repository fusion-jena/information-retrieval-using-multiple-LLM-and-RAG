The provided context mentions the use of dropout as a regularization method to prevent overfitting in the deep learning pipeline. Specifically, it states that the numbers of units in the two fully connected (FC) layers and the dropout rate were determined by five-fold cross-validation. However, it does not mention the use of L2 regularization. 

Additionally, the context mentions the use of stochastic gradient descent with a fixed learning rate of 0.01, but it does not provide information on other regularization methods such as L1 regularization, early stopping, or data augmentation. 

It is also worth noting that the context mentions the use of focal loss, which is a loss function that can help with class imbalance problems and may also have a regularizing effect. However, it is not typically considered a regularization method in the same sense as dropout or L2 regularization.
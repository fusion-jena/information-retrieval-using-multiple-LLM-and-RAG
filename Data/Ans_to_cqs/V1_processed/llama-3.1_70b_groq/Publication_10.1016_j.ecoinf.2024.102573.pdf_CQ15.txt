The optimization technique applied in the deep learning pipeline is Adam optimizer (Kingma and Ba, 2017) with the default hyper-parameters and a cosine decay scheduler for the learning rate.
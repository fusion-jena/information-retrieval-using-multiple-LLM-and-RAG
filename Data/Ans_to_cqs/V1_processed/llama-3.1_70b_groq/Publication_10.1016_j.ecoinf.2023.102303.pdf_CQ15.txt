Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the EfficientNet architecture, its scaling factors, and the application of deep learning methodologies in various fields. It also briefly mentions the use of semi-supervised contrastive learning and weakly-supervised learning, but does not provide information on the optimization techniques used.

Therefore, I don't know the specific optimization techniques applied in the deep learning pipeline.
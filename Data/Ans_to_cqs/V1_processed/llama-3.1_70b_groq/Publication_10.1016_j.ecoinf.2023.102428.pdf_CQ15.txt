The optimization technique applied in the deep learning pipeline is stochastic gradient descent (SGD). According to the text, "differentiation as a feedforward propagation (FP) algorithm to train the neural networks by employing supervised learning based on stochastic gradient descent (SGD)". There is no mention of other optimization techniques such as Adam in the provided context.
The provided context does not explicitly mention the criteria used to determine when training is complete, such as validation loss plateau. However, it does mention that models were trained for 150 epochs using Nvidia Tesla K80 or P100 GPUs. Additionally, it mentions that the learning rate was reduced by a factor of 10 after steps of 30 epochs. This suggests that the training process was stopped after a fixed number of epochs, rather than being based on a specific criteria such as validation loss plateau.
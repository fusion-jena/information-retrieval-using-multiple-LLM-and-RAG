The strategy implemented to monitor the model performance during training is data validation, specifically a holdback trial partitioning test. This involves iterating the analysis multiple times for each combination of test/training size, with randomly selected trails, to examine how the predicted outcome compares with the known test set size. This process helps to determine the optimal number of footprint trails required for the algorithm training set and test sets that can provide an accurate population estimate. Additionally, the model's performance is evaluated by comparing the predicted test set sizes with the actual test set sizes, and the mean of the predicted test set size is compared to the actual test set size to assess the model's robustness.
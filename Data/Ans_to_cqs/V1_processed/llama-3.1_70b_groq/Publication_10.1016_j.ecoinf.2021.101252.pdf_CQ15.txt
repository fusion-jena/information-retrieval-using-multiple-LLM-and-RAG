Unfortunately, the provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline. The context primarily focuses on the AutoML procedure, the architecture of different models (e.g., ResNet, InceptionTime, LSTM, CNN), and their performance on various case studies. It does not provide information on the optimization techniques used, such as SGD (Stochastic Gradient Descent) or Adam.

However, it does mention the concept of "learning rate" as one of the hyperparameters that is selected at random from a prespecified range of values during the AutoML procedure. The learning rate is a crucial hyperparameter in many optimization algorithms, including SGD and Adam. Nevertheless, without further information, it is impossible to determine which specific optimization techniques are applied in the deep learning pipeline.
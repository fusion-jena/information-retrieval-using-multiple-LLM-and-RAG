The provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention the learning rate, which is set to 0.001 (Table 1). This suggests that some form of gradient descent-based optimization is being used, but the specific algorithm is not specified.
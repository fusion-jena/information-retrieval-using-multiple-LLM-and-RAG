The provided context does not explicitly mention the criteria used to determine when training is complete, such as validation loss plateau. However, it does mention that the data for each model were randomly split into 10 equal parts for k-fold cross-validation. Each model was then trained on 90% of the data and validated on the remaining 10%; this was performed for each tenth of the data.

Additionally, the context mentions that the impact of window size was explored, and increasing it consistently increases model performance. However, it does not provide information on how the training process was stopped or what criteria were used to determine when training was complete.

Therefore, based on the provided context, I do not know the specific criteria used to determine when training is complete.
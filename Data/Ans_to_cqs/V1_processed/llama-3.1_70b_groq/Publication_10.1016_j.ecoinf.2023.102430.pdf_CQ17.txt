Unfortunately, the provided context does not mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. The context only provides information about the implementation of the AlexNet (AN) model in MATLAB, the training process, and the hyperparameters used. It also describes the characteristics of the other machine-learning algorithms used, including AdaBoost (AB), Gradient Boosting (GB), Histogram-based Gradient Boosting (HB), and Linear Discriminant (LD) model. However, it does not provide details about the regularization methods used in the deep learning pipeline. Therefore, I don't know the answer to this question based on the provided context.
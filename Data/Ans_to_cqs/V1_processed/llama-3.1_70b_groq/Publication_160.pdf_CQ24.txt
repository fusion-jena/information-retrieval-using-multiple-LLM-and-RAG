Unfortunately, the provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as random seed values. The context primarily discusses the performance of different ResNet architectures, batch sizes, and patch sizes on a classification task, as well as the use of ensemble networks. It does not provide information on how randomness is handled in the pipeline.

However, it can be inferred that the use of larger batch sizes (e.g., 512) might help to reduce the impact of randomness in the training process, as it allows the network to see more data at once and potentially reduces the effect of individual data points. Additionally, the use of ensemble networks, which combine the predictions of multiple models, might also help to reduce the impact of randomness by averaging out the predictions.

But, without explicit information on how randomness is handled, it is difficult to provide a more detailed answer.
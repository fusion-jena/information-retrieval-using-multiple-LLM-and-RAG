The provided context mentions two regularization methods used to prevent overfitting in the deep learning pipeline:

1. **Dropout**: The text explicitly mentions the use of dropout, citing the work of Srivastava et al. (2014). Dropout is a technique where a fraction of the neurons in a layer are randomly dropped out during training, which helps prevent overfitting by reducing the capacity of the network.
2. **Weight decay (L2 regularization)**: The text mentions that the weight decay is set to 0.0001. Weight decay, also known as L2 regularization, is a technique that adds a penalty term to the loss function to discourage large weights, which can help prevent overfitting.

Additionally, the text mentions the use of **batch normalization**, which can also help with regularization by normalizing the inputs to each layer, reducing the effect of internal covariate shift.

It is worth noting that the text does not mention other regularization methods, such as L1 regularization or early stopping, but it does mention the use of a learning rate scheduler with exponential decay, which can also help with preventing overfitting.
Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training. The context appears to be focused on transfer learning, specifically the proposed approach of Improvised Fuzzy Transfer Learning (IFTL), and its application to predicting GDP using CO2 emission data. It discusses the challenges of dealing with different data distributions between the source and target domains, but it does not provide information on data splitting criteria for deep learning model training.

However, it does mention that benchmark datasets are typically used for machine learning methodologies where training and testing data are drawn from the same distribution. But it does not elaborate on how the data is split for training, testing, and validation.

Therefore, I don't know the specific criteria used to split the data for deep learning model training based on the provided context.
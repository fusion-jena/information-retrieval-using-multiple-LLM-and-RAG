The text does not explicitly mention a specific strategy implemented to monitor the model performance during training. However, it does mention that the authors performed a sensitivity study by training models using different proportions of training data (5%, 25%, 50%, and 75% of the total hand-annotations) to compare to the full dataset for the within-site results for each site. This suggests that the authors evaluated the model's performance on a subset of the data to assess its robustness and identify potential overfitting. Additionally, the authors reran the experiment five times to account for the random subsampling of annotations, which implies that they used some form of cross-validation or bootstrapping to evaluate the model's performance.
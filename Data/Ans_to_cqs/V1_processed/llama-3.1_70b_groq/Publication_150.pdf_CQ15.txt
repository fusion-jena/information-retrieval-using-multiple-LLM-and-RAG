The optimization technique applied in the deep learning pipeline, as mentioned in the provided context, is the Stochastic Gradient Descent (SGD) optimizer. The initial learning rate for all models, except MansNet, was set at 0.01, and for MansNet, it was set at 0.1. The learning rate was reduced by a factor of 10 after steps of 30 epochs. Additionally, batch normalization was used in the models. There is no mention of the Adam optimizer in the provided context.
The provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. However, it does mention the use of batch normalization (Ioffe and Szegedy, 2015) in the MLP architecture, which suggests that some form of normalization is used to preprocess the data.

Additionally, the context mentions the use of dropout (Srivastava et al., 2014) to prevent overfitting, which can be considered a form of regularization rather than a preprocessing step.

It is also worth noting that the context mentions the use of a batch size of 256 and a learning rate of 0.0001, which suggests that the data may have been divided into batches and scaled accordingly. However, the exact preprocessing steps involved are not explicitly stated.

Therefore, while we can infer that some form of normalization and possibly scaling may have been used, the exact preprocessing steps involved before training the deep learning model are not specified in the provided context.
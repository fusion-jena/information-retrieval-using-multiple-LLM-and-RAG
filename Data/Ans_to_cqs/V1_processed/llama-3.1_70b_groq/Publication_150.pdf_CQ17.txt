The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention the use of batch normalization, which is a technique used to normalize the input data for each layer, helping to improve the stability and speed of training. Additionally, the context mentions the use of a learning rate that is reduced by a factor of 10 after 30 epochs, which can help prevent overfitting by reducing the model's capacity to fit the training data too closely.
The regularization methods used to prevent overfitting in the deep learning pipeline are dropout and ADAM optimization with a learning rate. Specifically, dropout with a rate of 0.3 is used during training to make the model more stable. Additionally, ADAM optimization with a learning rate of 0.0001 is used to find optimal weights in the neural network and minimize the cross-entropy loss between the network's predictions and the pre-labeled data. There is no mention of L2 regularization in the provided context.
The provided context does not explicitly mention the criteria used to determine when training is complete, such as validation loss plateau. However, it does mention that "All pre-trained models under Approach 1 converged the training after 1000 epochs with stochastic gradient descent with momentum (SGDM) as the optimization algorithm." This suggests that the training process is stopped after a fixed number of epochs (1000), rather than being based on a specific convergence criterion like validation loss plateau.
The hyperparameters used in the deep learning model, specifically the Multi-Layer Perceptron (MLP), are as follows:

1. Solver (optimizer): Adam
2. Number of neurons in the hidden layers: 10
3. Regularization factor “alpha” (L2 penalty): 10
4. Activation function: Rectified Linear Unit (ReLU)
5. Learning Rate: constant

These hyperparameters were selected through a grid search technique, where different combinations of hyperparameters were tested and the best performing combination was chosen. The grid search configuration for the MLP included:

* Solvers: Limited-Broyden–Fletcher–Goldfarb–Shanno (L-BFGS), Adam, Stochastic Gradient Descent (SGD) with constant learning rate, SGD with adaptive learning rate
* Number of neurons in the hidden layers: from 1 to 100
* Regularization factor “alpha” (L2 penalty): 10
* Activation function: identity, logistic, tanh, ReLU
* Learning Rate: constant, invscaling, adaptive

The selected hyperparameters were used to optimize the performance of the MLP model.
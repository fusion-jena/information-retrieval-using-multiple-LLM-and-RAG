Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training. However, based on general practices in machine learning and deep learning, I can provide some general information on this topic.

Typically, when training a deep learning model, the available data is split into three parts: training, validation, and testing sets. The most common split is 80% for training, 10% for validation, and 10% for testing. However, this ratio can vary depending on the specific problem, dataset size, and model complexity.

The training set is used to train the model, the validation set is used to tune hyperparameters and monitor the model's performance during training, and the testing set is used to evaluate the final performance of the trained model.

It's worth noting that the specific criteria used to split the data can vary depending on the specific problem and dataset. Some common techniques include random splitting, stratified splitting (to maintain the same class balance in each set), and k-fold cross-validation (to reduce overfitting).

In the context of the provided papers, the authors may have used similar techniques to split their data for training and testing their deep learning models for animal biodiversity monitoring and birdsong identification. However, without explicit information on the data splitting criteria used in these studies, I cannot provide a more specific answer.
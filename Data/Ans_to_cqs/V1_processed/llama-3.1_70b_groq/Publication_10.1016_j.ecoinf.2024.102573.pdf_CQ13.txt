The hyperparameters used in the deep learning model are as follows:

* Optimizer: Adam optimizer (Kingma and Ba, 2017)
* Learning rate: The learning rate is adjusted using a cosine decay scheduler, but the initial learning rate value is not specified.
* Number of epochs: The models were trained for 100 epochs.

Note that the text mentions that the Adam optimizer was used with its default hyperparameters, but it does not specify what those default hyperparameters are.
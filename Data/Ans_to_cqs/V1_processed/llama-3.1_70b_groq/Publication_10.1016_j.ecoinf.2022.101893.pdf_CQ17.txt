The provided context mentions the use of a dropout layer in the fully connected layer to reduce network parameters and prevent overfitting. Specifically, it states that a "dropout layer is added to the fully connected layer to reduce network parameters." However, it does not mention the use of L2 regularization or any other regularization methods.

Therefore, based on the provided context, the regularization method used to prevent overfitting in the deep learning pipeline is dropout with a dropout rate of 0.4, as mentioned in the network architecture description.
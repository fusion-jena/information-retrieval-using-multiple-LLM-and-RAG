The provided context does not specifically discuss the criteria used to split data for deep learning model training. However, it does describe a nested k-fold cross-validation scheme used for XGBoost model training.

According to the context, the data is split into three subsets: 

1. Tuning set (k-2 folds): used for training the model
2. Validation set (1-fold): used to monitor the performance of the model and calculate the out-of-sample prediction accuracy
3. Test set (1-fold): not explicitly mentioned in the context as being used for training or validation, but implied to be used for evaluating the final model

The context does not provide information on the specific criteria used to split the data into these subsets, such as random sampling or stratification. It only mentions that the data is split into k-2 folds for the tuning set, 1-fold for the validation set, and 1-fold for the test set.

Therefore, the answer to the query is that the context does not provide specific criteria for splitting data for deep learning model training, but it does describe a general k-fold cross-validation scheme used for XGBoost model training.
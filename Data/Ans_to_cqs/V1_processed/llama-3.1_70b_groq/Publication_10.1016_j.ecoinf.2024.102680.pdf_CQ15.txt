Unfortunately, the provided context does not mention specific optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the architecture and design of the network, including the use of dilated convolution, depthwise separable convolution, and the proposed LKSP module. It also mentions the batch size used in the experiments, which is set to 16 uniformly. However, it does not provide information on the optimization techniques used to train the network. Therefore, I don't know the specific optimization techniques applied in the deep learning pipeline.
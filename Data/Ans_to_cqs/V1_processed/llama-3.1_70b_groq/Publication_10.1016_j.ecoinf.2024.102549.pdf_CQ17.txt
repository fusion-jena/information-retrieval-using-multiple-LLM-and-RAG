According to the provided context, the regularization method used to prevent overfitting in the deep learning pipeline is dropout. Specifically, it is mentioned that "a dropout function was incorporated between the fully connected layers" in the CNN structure, with a dropout rate of 50%. This suggests that dropout is used as a regularization technique to prevent overfitting in the model.

Additionally, the context mentions that the AlexNet architecture, which is employed in this study, uses "practical strategies like ReLU nonlinear layers and dropout regularization techniques" to make it effective for small-sized inputs and to handle complex problems. However, there is no mention of L2 regularization being used in this specific study.

Therefore, based on the provided context, the answer to the query is that dropout is the regularization method used to prevent overfitting in the deep learning pipeline, but there is no evidence of L2 regularization being used.
Unfortunately, the provided pieces of context do not explicitly mention the criteria used to split data for deep learning model training. The context appears to be a collection of references and snippets from various papers and sources, mostly related to deep learning, event-stream processing, and machine learning, but it does not provide specific information on data splitting criteria.

However, I can provide some general information on common practices for splitting data in deep learning. Typically, data is split into three sets: training, validation, and testing. The training set is used to train the model, the validation set is used to evaluate the model's performance during training and tune hyperparameters, and the testing set is used to evaluate the final model's performance.

Common criteria for splitting data include:

* Random splitting: splitting the data randomly into training, validation, and testing sets
* Stratified splitting: splitting the data in a way that preserves the class balance in each set
* Time-based splitting: splitting the data based on time, such as using historical data for training and more recent data for testing

It's worth noting that the specific criteria used for splitting data can depend on the problem, dataset, and model architecture. If you're looking for more specific information, I recommend consulting a deep learning textbook or a research paper on the topic.
The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention the use of batch normalization, which is a technique used to stabilize training and reduce internal covariate shift, but it is not typically considered a regularization method.

Additionally, the context mentions the use of a wide residual network architecture, which can help to prevent overfitting by allowing the model to learn a perturbation of the input signal at each layer rather than a transformation. However, this is not a traditional regularization method.

Therefore, based on the provided context, I do not know what specific regularization methods are used to prevent overfitting in the deep learning pipeline.
The optimization techniques applied in the deep learning pipeline mentioned in the context are Adam and RMSprop. These are the two optimizers that are used in the ANN (Artificial Neural Network) model. Additionally, the context also mentions the use of Optuna, a hyperparameter optimization framework, to optimize the structure and hyperparameters of the RF (Random Forest) and ANN models. However, it does not mention the use of SGD (Stochastic Gradient Descent) as an optimization technique in the deep learning pipeline.
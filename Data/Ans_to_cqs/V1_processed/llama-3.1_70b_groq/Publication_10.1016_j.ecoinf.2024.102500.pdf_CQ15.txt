Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on machine learning algorithms, hyperparameter tuning, and model evaluation, but it does not delve into the specifics of optimization techniques used in deep learning.

However, it does mention "learning rate" as one of the hyperparameters, which is a common parameter in many optimization algorithms, including SGD and Adam. But without further information, it is unclear which specific optimization techniques are being used.

Therefore, I do not have enough information to provide a definitive answer to this question.
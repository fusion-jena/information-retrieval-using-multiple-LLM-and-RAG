The provided context does not explicitly mention the learning rate used in the deep learning model. However, it does mention the optimizer used. According to the text, the Adam optimizer turned out to be superior to the SGD optimizer for training all models. 

Additionally, the hyperparameters column in Table 2 shows the values of the following hyperparameters for each CNN architecture: kernel size layer 1, kernel size last layer, convolutional depth layer 1, convolutional depth last layer, and fully connected size. 

Another hyperparameter mentioned in the text is the dropout probability, which was set to 0.3 after the second to last hidden layer.
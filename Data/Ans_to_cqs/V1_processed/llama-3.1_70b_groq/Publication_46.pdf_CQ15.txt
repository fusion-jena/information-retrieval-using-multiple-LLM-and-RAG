Unfortunately, the provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the architecture of Convolutional Neural Networks (CNNs), feature extraction using convolution and pooling layers, and the composition of CNNs. It also mentions the use of SoftMax function in the final layer to transform parameters into probabilities.

However, it does mention the concept of "weights between the neurons in the adjoining layers" and how they are adjusted to maximize network performance. This suggests that some form of optimization technique is used, but the specific technique is not mentioned.

Therefore, based on the provided context, I do not know the specific optimization techniques applied in the deep learning pipeline.
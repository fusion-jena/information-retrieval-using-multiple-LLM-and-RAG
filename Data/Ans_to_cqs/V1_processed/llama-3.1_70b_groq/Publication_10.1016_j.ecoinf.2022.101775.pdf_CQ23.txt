To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the algorithms were trained with the amount of data available after manually removing the testing set. This suggests that the model was trained on a separate dataset from the one used for testing, which helps to prevent overfitting and ensures that the model can generalize well to unseen data.

Additionally, 10% of the data was used as a validation set to check for overfitting during training. This is a common technique used in machine learning to evaluate the model's performance on unseen data and prevent overfitting.

Furthermore, the model was implemented in Pytorch and trained and tested in Google Colaboratory, which suggests that the model was trained and tested on a cloud-based platform with access to a large amount of computational resources. This can help to ensure that the model is trained and tested on a diverse range of data and can generalize well to different scenarios.

However, it is not clear from the provided context whether other measures such as cross-validation or stratified splitting were used to ensure the generalizability of the model. Therefore, it is not possible to provide a complete answer to this question based on the provided context.
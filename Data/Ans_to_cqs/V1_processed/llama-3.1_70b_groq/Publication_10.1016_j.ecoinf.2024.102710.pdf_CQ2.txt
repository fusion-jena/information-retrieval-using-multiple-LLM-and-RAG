The deep learning pipeline uses audio data formats. Specifically, the pipeline involves processing three-second long audio segments, referred to as 'samples', which are resampled to 48 kHz (for BirdNET) or 16 kHz (for YAMNet and VGGish). Additionally, spectrograms are calculated for each sound sample using the native sampling rate of the audio signal, employing a window size of 512 samples and an overlap of 256 samples. These spectrograms are then resized to the dimensions required by the convolutional models, such as ResNet152-V2 and VGG16, which take images as input.
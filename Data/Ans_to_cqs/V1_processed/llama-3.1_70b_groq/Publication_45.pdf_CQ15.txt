The provided context mentions stochastic gradient descent (SGD) as an optimization technique used in deep learning. It is mentioned as an example of an improved algorithm for optimizing connection weights in deep neural networks. However, it does not provide an exhaustive list of optimization techniques applied in the deep learning pipeline.

Additionally, the context mentions that the hyper-parameters evaluated iteratively included the used optimizer, but it does not specify which optimizers were used in the experiments. However, it does mention that the experiments were conducted using the TensorFlow framework, which supports various optimizers including SGD and Adam.

Therefore, based on the provided context, we can confirm that stochastic gradient descent (SGD) is one of the optimization techniques used in deep learning. However, we cannot provide a comprehensive list of optimization techniques applied in the deep learning pipeline, and we cannot confirm whether Adam is used in this specific context.
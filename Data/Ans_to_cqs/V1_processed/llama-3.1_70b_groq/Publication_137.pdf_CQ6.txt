The provided context does not explicitly mention the preprocessing steps involved before training a deep learning model, such as normalization, scaling, or cleaning. However, it does mention voxelization subsampling size (voxel size) and batch size as parameters that were adjusted during the training process. Additionally, it mentions the use of the hold-out method to distinguish the test set and the training set at a ratio of 3:7 for preprocessing the original data. 

It also references a paper by Ioffe and Szegedy (2015) on batch normalization, which is a technique used to normalize the input data for each layer in a deep neural network. However, it does not explicitly state that batch normalization was used in this specific study.

Therefore, the only preprocessing step that can be confirmed from the provided context is the use of the hold-out method for splitting the data into training and test sets. Other preprocessing steps, such as normalization, scaling, or cleaning, are not mentioned.
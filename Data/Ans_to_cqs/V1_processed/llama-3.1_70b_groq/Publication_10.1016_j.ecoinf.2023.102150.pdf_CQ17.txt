Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of image augmentation (shifts, horizontal flips, rotations, zooms, and shears) to expand the training dataset, which can help prevent overfitting.

Additionally, the context mentions the use of a one-cycle learning rate policy with a minimum learning rate of 0.000001 and a maximum learning rate of 0.001, which can also help prevent overfitting by adapting the learning rate during training.

The context also mentions the use of the Adam optimization algorithm, which is a stochastic optimization method that can help prevent overfitting by adapting the learning rate for each parameter individually.

However, it does not explicitly mention the use of dropout or L2 regularization, which are common regularization methods used to prevent overfitting in deep learning pipelines. Therefore, I cannot confirm whether these methods were used or not.
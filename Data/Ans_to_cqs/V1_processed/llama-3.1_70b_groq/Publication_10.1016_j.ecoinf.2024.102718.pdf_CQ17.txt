Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. The context primarily focuses on the performance evaluation of the BEiT model and its comparison with other models, as well as the implementation details of the training procedure, including the use of callbacks such as ReduceLROnPlateau, ModelCheckpoint, and EarlyStopping. However, these callbacks are used for dynamic learning rate adjustment and model checkpointing, rather than regularization. Therefore, I do not have enough information to provide a specific answer to this question.
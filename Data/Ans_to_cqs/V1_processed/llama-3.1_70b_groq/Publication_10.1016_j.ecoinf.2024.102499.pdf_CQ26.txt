Unfortunately, the provided context does not explicitly mention the techniques used to address data bias during preprocessing of the deep learning pipeline. However, it does mention the use of K-fold cross-validation, which is a technique used to evaluate the performance of machine learning models and can help to reduce overfitting. Additionally, the context mentions the use of a large dataset (COCO dataset) for pre-training the model, which can help to improve the model's ability to generalize to new data.

Regarding the specific techniques mentioned in the query, the context does not mention Stratified splitting, oversampling, or undersampling. However, it does mention the use of a batch size of 16 images per iteration during training, which can help to reduce the impact of data bias by randomly sampling the training data.

Diverse data collection is not explicitly mentioned in the context, but the use of a large dataset (COCO dataset) and the collection of images from various sources (e.g., Trainingclips, Testingclips) suggests that the authors may have attempted to collect a diverse set of data to train and test their model.

Overall, while the context does not provide explicit information on the techniques used to address data bias, it suggests that the authors may have used techniques such as K-fold cross-validation and diverse data collection to improve the performance and generalizability of their model.
According to the provided context, the preprocessing steps involved before training a deep learning model include:

1. **Downscaling the resolution of images**: All images were downscaled to 224 Ã— 224 pixels before feeding them to the models.
2. **Data augmentation**: The training sets were randomly augmented via horizontal flipping to avoid overfitting and imbalanced problems.
3. **Pre-training on ImageNet dataset**: The models were pre-trained on the ImageNet dataset to accelerate convergence.
4. **Fine-tuning on PAD Full and PAD Lite datasets**: The pre-trained models were then fine-tuned on the PAD Full and PAD Lite datasets.
5. **Five-fold subject-exclusive (SE) cross-validation**: To eliminate the influence of individuals, a five-fold subject-exclusive cross-validation was applied, ensuring that the same subject did not appear in the training set and testing set at the same time.

Additionally, the context mentions that the original images were captured under different illumination and angles, but it does not specify any additional preprocessing steps to address these variations.

It is worth noting that the context does not mention any explicit normalization or scaling steps, but it does mention the use of Batch Normalization (BN) in the VGG models, which is a technique used to normalize the inputs to each layer. However, this is not a preprocessing step per se, but rather a component of the model architecture.
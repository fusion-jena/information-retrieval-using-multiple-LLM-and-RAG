Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. The text discusses the importance of identifying an optimal number of training epochs to avoid overfitting, but it does not provide information on specific regularization techniques used to prevent overfitting.

However, it does mention the use of residual blocks in Residual Networks (ResNet) to allow for the training of deeper models and to address the vanishing gradient problem, which can contribute to overfitting. But it does not provide a comprehensive list of regularization methods used in the deep learning pipeline.

Therefore, I do not have enough information to provide a complete answer to the query.
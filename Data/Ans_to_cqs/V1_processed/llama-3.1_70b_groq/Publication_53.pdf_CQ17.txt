The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of data augmentation as a technique to help generalize the model. Data augmentation is a regularization technique that helps prevent overfitting by artificially increasing the size of the training dataset.

The context mentions the use of the fastai get_transforms function with specific arguments, including "flip_vert = TRUE" to allow for vertical flipping of images, "max_lighting = 0.02" to limit overly exposing the images, and "max_zoom = 1" to disable the zooming augmentation. These data augmentation techniques can help prevent overfitting by reducing the model's reliance on specific features in the training data.

Additionally, the context mentions the use of a learning rate of 10^-3 and 10^-4, which can also help prevent overfitting by controlling the step size of the gradient descent algorithm. However, it does not mention the use of dropout or L2 regularization explicitly.

Therefore, while the context does not provide a comprehensive list of regularization methods used to prevent overfitting, it does mention the use of data augmentation and learning rate control as techniques to help generalize the model.
The provided context does not explicitly mention strategies for handling randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention that the authors applied "aggressive data augmentation" to the training sets, which includes random horizontal flipping. This suggests that the authors are introducing randomness into the data augmentation process to increase the diversity of the training data and prevent overfitting.

Additionally, the context mentions that the authors used a five-fold subject-exclusive (SE) cross-validation approach, which ensures that the same subject does not appear in both the training and testing sets. This approach can help to reduce the impact of randomness in the data by ensuring that the model is evaluated on unseen subjects.

However, it is not clear from the provided context whether the authors employed any specific strategies to handle randomness in the deep learning pipeline, such as setting a random seed value or using techniques like dropout or batch normalization. Therefore, I do not have enough information to provide a more detailed answer to this question.
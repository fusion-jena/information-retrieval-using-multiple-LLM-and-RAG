Unfortunately, the provided context does not explicitly mention the specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context only mentions that the algorithms change their inner parameters to minimize the classification error through a process called back-propagation, but it does not provide further details on the optimization techniques used.
To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the data was split in a way that kept the integrity of capture events, where all images of an event were put into the same dataset. This was done to prevent poor generalization of the model due to similar images being split into training and test sets.

Additionally, the data was split into training, validation, and test sets. The training set was used to train the model, the validation set was used to track the model's performance during training, and the test set was used to evaluate the model's performance after training. This approach helps to prevent overfitting and ensures that the model is generalizing well to unseen data.

The use of a validation set also allowed for early stopping, where the training process was stopped when the loss value on the validation set no longer decreased. This helps to prevent overfitting and ensures that the model is not over-specializing to the training data.

Furthermore, the model was trained on a large dataset (e.g., 100,000, 500,000, and 1 million images) and evaluated on a separate test set (e.g., 8095 and 8118 images). This helps to ensure that the model is generalizing well to new, unseen data.

It is not explicitly mentioned in the provided context whether cross-validation or stratified splitting were used. However, the use of a large dataset, early stopping, and separate validation and test sets suggest that efforts were made to ensure the generalizability of the model.
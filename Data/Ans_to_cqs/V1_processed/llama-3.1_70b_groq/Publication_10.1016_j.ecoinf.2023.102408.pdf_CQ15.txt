Unfortunately, the provided context does not mention specific optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context only mentions that the weights in the Artificial Neural Network (ANN) were randomly initialized using the Nguyen-Widrow method, and that the learning rate and weight were used as calibration parameters to improve the results. However, it does not provide information on the optimization technique used to update these parameters during training.

Therefore, I do not know what optimization techniques are applied in the deep learning pipeline.
Unfortunately, the provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. The text only mentions that the dataset was randomly divided into training, validation, and testing sets in a stratified manner to maintain class proportions across all parts. It also mentions that the models were trained on an NVIDIA 3090 GPU with a 256 Ã— 256 image input size, but it does not provide information on any preprocessing steps such as normalization, scaling, or cleaning.

However, it is common practice in deep learning to perform preprocessing steps such as normalization, scaling, and cleaning before training a model. Normalization typically involves scaling the input data to a common range, usually between 0 and 1, to prevent features with large ranges from dominating the model. Scaling involves transforming the data to have zero mean and unit variance. Cleaning involves removing any missing or erroneous data points.

Since the context does not provide explicit information on preprocessing steps, it is unclear what specific steps were taken in this case.
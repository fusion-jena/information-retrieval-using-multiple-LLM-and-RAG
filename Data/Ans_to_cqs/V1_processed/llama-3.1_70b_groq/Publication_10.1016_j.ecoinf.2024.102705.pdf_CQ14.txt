The provided context does not explicitly mention how the hyperparameters of the model are optimized. However, it does mention the use of optimization algorithms such as Stochastic Gradient Descent (SGD), Adaptive Gradient (AdaGrad), Root Mean Square Prop (RMSProp), and Adam. These algorithms are typically used to optimize the model's parameters during training, but they do not provide information on how the hyperparameters are optimized.

Hyperparameter optimization techniques such as grid search, random search, Bayesian optimization, or gradient-based optimization are not mentioned in the provided context. Therefore, I do not know how the hyperparameters of the model are optimized.
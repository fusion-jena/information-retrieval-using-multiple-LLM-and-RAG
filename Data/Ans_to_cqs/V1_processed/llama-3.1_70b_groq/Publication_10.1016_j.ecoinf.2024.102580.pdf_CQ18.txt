The strategy implemented to monitor the model performance during training is through the use of training and validation sets. The ground-truth dataset was divided randomly into training (80%), validation (10%), and test (10%) sets. The model's performance was evaluated on the validation set during the training process. 

Additionally, the model's performance was monitored through the use of various metrics and curves. The overall training loss curve, mean Average Precision curve for the validation set, and Generalized Intersection-over-Union (IoU) loss curve for training and validation sets were plotted to evaluate the model's performance during training. 

Furthermore, a confidence threshold was established by plotting Precision and Recall against the threshold on the validation set. This threshold was used to balance the two metrics, maximizing performance by detecting individuals (Precision) while minimizing false detections (Recall).
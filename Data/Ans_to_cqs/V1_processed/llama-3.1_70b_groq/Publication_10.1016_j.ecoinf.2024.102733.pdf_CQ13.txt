The hyperparameters used in the deep learning model are as follows:

* Learning rate: Various values were explored, including 0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.01, and 0.1.
* Optimizer: Adam optimizer (Kingma and Ba, 2015) was used.
* Batch size: Different batch sizes were tested, including 8, 16, 32, 64, 128, 256, 512, and 8192.
* Weight decay: Weight decay values ranged from 0 to 0.59, with specific values including 0, 0.00001, 0.0001, and 0.001.
* Dropout: Dropout rates of 0.05, 0.1, 0.2, and 0.25 were used.
* Focal Loss parameters:
	+ α (balancing factor): Set as the inverse of class frequency, 0.25, or 0.5.
	+ γ (scaling factor): Set to 0.25, 0.5, 0.75, 1, 1.5, and 2.
* Latent dimension: Values of 128, 256, 512, and 2048 were explored.

Note that these hyperparameters were used in different experiments and combinations, and the specific values used may have varied depending on the particular model or task being evaluated.
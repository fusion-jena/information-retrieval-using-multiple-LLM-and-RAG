The provided context does not explicitly mention the postprocessing steps involved after the model training. However, based on the information given, we can infer that some form of accuracy assessment and metrics calculation were performed.

The context mentions that an accuracy is generated for each algorithm applied during the training step, but it is biased because it uses the same sample sets to test the model. To address this, an additional accuracy assessment using validation points was conducted.

The context also provides several tables that appear to be confusion matrices, which are used to evaluate the performance of the model. These tables show the number of true positives, false positives, true negatives, and false negatives for both burned and non-burned areas.

Additionally, the context mentions the calculation of metrics such as producer's accuracy, user's accuracy, and total accuracy. These metrics are commonly used to evaluate the performance of classification models.

There is no mention of saliency maps or other postprocessing steps such as feature importance, partial dependence plots, or SHAP values. Therefore, we cannot confirm whether these steps were involved in the analysis.
To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the raw dataset was randomly shuffled before being divided into training, validation, and test sets. This random shuffling helps to reduce the influence of any inherent order or structure in the data.

The data was then divided into training, validation, and test sets using a ratio of 8:1:1. This division allows the model to be trained on a large portion of the data (80%), while also having a separate set for validation (10%) and testing (10%). This helps to prevent overfitting and ensures that the model is evaluated on unseen data.

Furthermore, the random partitioning was repeated ten times. This repetition helps to reduce the influence of random splitting and ensures that the results are robust and generalizable.

Additionally, the model's hyperparameters were optimized using a grid search approach, which involves comparing the values of evaluation metrics for different parameters and selecting the optimal parameters from a large search space. This helps to ensure that the model is optimized for the specific task at hand and is not overfitting to a particular set of parameters.

Overall, the combination of random shuffling, data partitioning, repeated partitioning, and hyperparameter optimization helps to ensure the generalizability of the deep learning model.
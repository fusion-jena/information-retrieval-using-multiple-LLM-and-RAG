Unfortunately, the provided context does not explicitly mention the optimization techniques used in the deep learning pipeline. However, it does mention that the model was fine-tuned by unfreezing the entire model and training at a 10-fold lower learning rate (10âˆ’4). This suggests that the learning rate was adjusted during the training process, but the specific optimization algorithm used (e.g., SGD, Adam) is not mentioned.
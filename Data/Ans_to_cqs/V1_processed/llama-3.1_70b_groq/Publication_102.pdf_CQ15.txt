The optimization techniques applied in the deep learning pipeline mentioned in the provided context are:

1. Adam optimizer: This is used in Model-2, which utilizes the EfficientNet feature extractor. The Adam optimizer is used with a learning rate of 0.001.

2. Stochastic Gradient Descent (SGD) optimizer: This is used in Model-K, which is a regression model based on VGG16 without the feature extractor on top. The SGD optimizer is used with an MSE loss function.

Additionally, the early stopping technique is applied to avoid over-fitting in Model-2. This technique stops the training process when the model's performance on the validation set starts to degrade.
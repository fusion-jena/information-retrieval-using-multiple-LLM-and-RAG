According to the provided context, the optimization techniques applied in the deep learning pipeline include Adam and L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno). Specifically, the text mentions that the solver for weight optimization used in the Optuna grid search is either "adam" or "lbfgs". Additionally, the text also mentions "batch-updated stochastic gradient descent" in the context of hyperparameter tuning, but it is not clear if this is used in the same pipeline.
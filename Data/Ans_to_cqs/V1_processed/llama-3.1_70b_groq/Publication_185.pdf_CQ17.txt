The provided context mentions two regularization methods used to prevent overfitting in the deep learning pipeline:

1. **L2 weight decay**: The context states that L2 weight decay was applied with a penalty multiplier set to 5 Ã— 10^-4. This is a type of regularization that adds a penalty term to the loss function to discourage large weights and prevent overfitting.

2. **Dropout**: The context also mentions that a dropout ratio was set to 0.5. Dropout is a regularization technique that randomly drops out neurons during training to prevent overfitting by reducing the model's reliance on any single neuron.

These two regularization methods were used in conjunction with each other to prevent overfitting in the deep learning pipeline.
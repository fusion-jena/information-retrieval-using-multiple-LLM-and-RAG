Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of Batch Normalization and Leaky-ReLU activation functions after each convolution layer, which can help with network training and stability.

Batch Normalization is a technique that normalizes the input to each layer, which can help with reducing the effect of internal covariate shift and improving the stability of the network. Leaky-ReLU is a variation of the ReLU activation function that allows a small fraction of the input to pass through, even when the input is negative. This can help with preventing the dying ReLU problem, where neurons become stuck in a negative state and stop learning.

While these techniques can help with improving the stability and performance of the network, they are not typically considered regularization methods in the classical sense. Regularization methods, such as dropout and L2 regularization, are typically used to explicitly prevent overfitting by adding a penalty term to the loss function or randomly dropping out neurons during training.

Therefore, based on the provided context, I do not know what regularization methods are used to prevent overfitting in the deep learning pipeline.
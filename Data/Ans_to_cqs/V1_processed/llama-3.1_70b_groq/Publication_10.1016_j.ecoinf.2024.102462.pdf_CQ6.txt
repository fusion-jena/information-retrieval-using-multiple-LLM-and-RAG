Unfortunately, the provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. However, it does mention "new normalisation techniques" as one of the improvements in the ConvNeXt architecture. This suggests that normalization is a preprocessing step that may be involved in training a deep learning model.

Additionally, the context mentions "data augmentation" as a technique used to evaluate current advancements in the field. Data augmentation is a preprocessing step that involves artificially increasing the size of the training dataset by applying transformations to the existing data, such as rotation, flipping, and cropping.

Other than these mentions, the context does not provide information on specific preprocessing steps such as scaling, cleaning, or others. Therefore, I do not have enough information to provide a comprehensive answer to the query.
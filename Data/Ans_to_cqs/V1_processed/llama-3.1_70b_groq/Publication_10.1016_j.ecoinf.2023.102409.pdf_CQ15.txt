According to the provided context, the optimization technique applied in the deep learning pipeline is SGD (Stochastic Gradient Descent). This is mentioned in the following sentence:

"The modelâ€™s initial learning rate was set to 0.01, further refined via iterative testing... and utilized SGD optimizer..."

There is no mention of Adam or any other optimization techniques being used in the provided context.
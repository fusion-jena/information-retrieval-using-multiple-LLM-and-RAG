According to the provided context, the optimization technique used in the deep learning pipeline is the Adam optimizer. Specifically, it is mentioned that "An Adam optimizer was used to control gradient descent during training (Kingma and Ba, 2014), with parameters set to: learning rate of 0.001, decay factor of 0.75 and a step size of [not specified]".
The provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. However, it does mention "Batch Normalization" in the reference by Ioffe and Szegedy (2015), which is a technique used to normalize the inputs to each layer of a deep neural network. 

Batch normalization is a preprocessing step that can be applied to the inputs of each layer to reduce internal covariate shift, which can improve the stability and speed of training. However, the context does not provide information on other preprocessing steps such as scaling, cleaning, or data normalization.

Therefore, based on the provided context, the only preprocessing step that can be confirmed is batch normalization. Other preprocessing steps may be involved, but they are not mentioned in the provided context.
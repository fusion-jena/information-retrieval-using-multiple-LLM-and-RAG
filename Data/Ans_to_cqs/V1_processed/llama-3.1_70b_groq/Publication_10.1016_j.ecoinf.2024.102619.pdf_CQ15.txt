The text mentions that the Adam learning rate optimizer is used in the deep learning pipeline. According to the text, "We also used the Adam learning rate optimizer (Kingma and Ba, 2015), which automatically adjusted our initial learning rate of 1e-03 during training in a way that improved performance." This indicates that the Adam optimizer is used to adaptively adjust the learning rate during training, which is a form of optimization technique. There is no mention of SGD (Stochastic Gradient Descent) being used in the provided text.
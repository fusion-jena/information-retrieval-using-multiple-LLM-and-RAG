The provided context does not explicitly mention how the hyperparameters of the model are optimized. However, it does mention that the pre-trained models under Approach 1 converged after 1000 epochs with stochastic gradient descent with momentum (SGDM) as the optimization algorithm. Additionally, one of the cited papers mentions "Hyperparameter-tuned batch-updated stochastic gradient descent" but it does not provide information on the specific method used for hyperparameter optimization (e.g., grid search, random search). Therefore, I do not know the specific method used for hyperparameter optimization.
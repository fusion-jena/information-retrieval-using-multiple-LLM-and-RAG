The provided context mentions the use of dropout layers as a regularization method to prevent overfitting in the deep learning pipeline. Specifically, it states that "the dropout layers randomly dropped some neurons, causing the feed-forward network to perform worse, while dropout was deactivated and all neurons were kept during the validation." This suggests that dropout is used to randomly drop out neurons during training, which can help prevent overfitting by reducing the capacity of the network and encouraging it to learn more robust features.

Additionally, the context mentions that the dropout rate was set to increase from one encoding block to the next by a step of 0.1 (starting from 0.1), and decreasing from one decoding block to the next. This suggests that the dropout rate is adjusted dynamically during the training process to control the amount of regularization applied.

There is no mention of L2 regularization in the provided context.
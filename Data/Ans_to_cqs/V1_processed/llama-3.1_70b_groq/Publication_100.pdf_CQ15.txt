The deep learning pipeline utilizes the Adam variant of stochastic gradient descent (SGD) as its optimization technique. Specifically, the Adam optimizer is used to adaptively scale the magnitude of the parameter updates based on the statistics of previous updates, which helps to speed up convergence. Additionally, the learning rate is automatically reduced by a factor of 0.1 when the validation loss has not improved for 15 consecutive epochs, and weight decay is applied to control the strength of the unit Gaussian prior. The hyper-parameters for the Adam optimizer are set to 𝛽1 = 0.9, 𝛽2 = 0.999, and 𝜖 = 10−8.
Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context only mentions that the network's connection weights were adjusted through an iterative error back-propagation process until reaching a target minimal error, but it does not specify the optimization algorithm used. Additionally, it mentions that the learning rate, momentum, and training iterations were tuned to optimal values (0.01, 0.18, and 500, respectively), but it does not provide information on the optimization technique used to achieve these values.
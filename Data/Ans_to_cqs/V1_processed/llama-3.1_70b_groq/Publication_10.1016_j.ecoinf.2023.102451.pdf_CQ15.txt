The optimization technique applied in the deep learning pipeline is AdamW. According to the text, "The proposed model is trained for 100 epochs with optimizer AdamW...". This indicates that the AdamW optimizer is used to optimize the model's weights during training. Additionally, the learning rate is set to 0.002 and the batch size is set to 32.
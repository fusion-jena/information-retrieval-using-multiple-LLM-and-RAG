The provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. However, it does mention that "we applied various transformations (flipping, brightness and contrast modifications following Shorten and Khoshgoftaar (2019)) to improve training." This suggests that some form of data augmentation was used to increase the size and diversity of the training dataset.

Additionally, the context mentions that the fastai library was used, which is built on top of PyTorch. Fastai is known to provide a range of preprocessing tools and techniques, including normalization and scaling. However, the specific preprocessing steps used in this case are not explicitly stated.

Therefore, while we can infer that some form of data augmentation was used, the exact preprocessing steps involved before training the deep learning model are not clear from the provided context.
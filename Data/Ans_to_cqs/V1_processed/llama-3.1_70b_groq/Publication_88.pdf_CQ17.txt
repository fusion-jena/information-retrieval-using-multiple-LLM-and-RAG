The regularization methods used to prevent overfitting in the deep learning pipeline, as mentioned in the provided context, include dropout and weight decay (also known as L2 regularization). 

Specifically, the context mentions that a dropout rate of 0.2 was used in the final Fully Connected (FC) layer. This means that 20% of the neurons in the final FC layer were randomly dropped out during training to prevent overfitting.

Additionally, the context mentions that the Adam optimizer was used with a weight decay of 0.9. Weight decay is a form of L2 regularization that adds a penalty term to the loss function to discourage large weights. However, it seems there might be some confusion in the provided context as weight decay is typically a small value (e.g., 0.01 or 0.001) added to the loss function, not 0.9. The value of 0.9 seems more likely to be related to the momentum term in the Adam optimizer rather than the weight decay.
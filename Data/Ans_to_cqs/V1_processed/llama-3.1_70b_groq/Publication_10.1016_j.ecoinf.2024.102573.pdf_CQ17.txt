Unfortunately, the provided context does not mention specific regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. The context primarily focuses on the comparison of different frontend methods (e.g., PCEN, log compression, and normalization) and their impact on the performance of the models. It also discusses the importance of normalization and its effect on the models' robustness to noise.

However, it does mention that the models used the same backend and optimization setting, which might imply that some regularization methods were used, but it does not provide any specific information about them.

Therefore, I don't know the specific regularization methods used in the deep learning pipeline.
Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. The context primarily focuses on few-shot learning, data augmentation, and optimization-based methods for adapting to new tasks with limited examples.

However, it does mention the risk of overfitting when using data augmentation with very few training samples, implying that overfitting is a concern in these scenarios. But it does not provide specific information on the regularization methods used to prevent overfitting.

Therefore, I do not have enough information to provide a comprehensive answer to this query.
The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention the use of a technique called "dropout" in the context of deep learning. Dropout is a regularization technique that randomly drops out units during training to prevent overfitting. This is mentioned in reference [66], which is a paper by Srivastava et al. titled "Dropout: A simple way to prevent neural networks from overfitting."

Additionally, the context mentions the use of a statistical feature importance test (SFIT) to explain the importance of input variables in the deep learning model. This test involves randomly shuffling a single feature while keeping all other features constant, and then measuring the decrease in model performance. This process is repeated for each feature to determine its importance.

However, it does not provide information on how randomness is handled in the deep learning pipeline, such as setting a random seed value. Therefore, I do not have enough information to provide a complete answer to this question.
The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention batch normalization as a method that has a regularization effect. 

Batch normalization is a technique that normalizes the input to each layer, which can help to prevent overfitting by reducing the internal covariate shift. This is mentioned in the context as "a regularization effect because the centering and reduction of a neuron activation is linked to the mini-batch statistics."

Additionally, the context mentions that a deep CNN would be very prone to overfitting due to the restricted number of samples, but it does not specify the methods used to prevent overfitting in this case. 

Therefore, the only regularization method mentioned in the context is batch normalization, but it is not explicitly stated as a method to prevent overfitting.
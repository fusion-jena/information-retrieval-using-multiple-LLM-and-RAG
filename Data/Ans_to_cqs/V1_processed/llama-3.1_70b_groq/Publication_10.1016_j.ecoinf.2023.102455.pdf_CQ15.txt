The optimization technique applied in the deep learning pipeline is Adam. This is mentioned in the context as "Optimizer: Adam" in multiple instances, indicating that the Adam optimizer is used consistently across different model configurations. There is no mention of SGD or any other optimization techniques being used.
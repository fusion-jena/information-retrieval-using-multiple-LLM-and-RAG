Unfortunately, the provided context does not mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context appears to be focused on traditional machine learning techniques, such as decision trees, random forests, and regression splines, rather than deep learning.

However, it does mention "stochastic gradient boosting" in the context of the TreeNet algorithm, which is a type of ensemble learning technique. But this is not the same as the optimization techniques used in deep learning.

Therefore, I don't know the answer to the query regarding the optimization techniques used in the deep learning pipeline.
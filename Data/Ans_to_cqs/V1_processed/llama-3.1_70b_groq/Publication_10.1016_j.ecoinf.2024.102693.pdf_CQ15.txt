According to Table 1, the optimizer used in the deep learning pipeline is Adamw with parameters (0.9, 0.999). This is a variant of the Adam optimization algorithm, which is a popular stochastic gradient descent (SGD) optimizer. The specific parameters (0.9, 0.999) refer to the beta1 and beta2 values, which control the exponential decay rates of the first and second moments of the gradient, respectively.
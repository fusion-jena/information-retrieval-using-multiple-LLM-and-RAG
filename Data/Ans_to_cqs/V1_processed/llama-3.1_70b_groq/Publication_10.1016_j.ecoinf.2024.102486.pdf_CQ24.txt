The provided context does not explicitly discuss strategies for handling randomness in the deep learning pipeline. However, it does mention the use of random number generation in the context of Monte Carlo methods and Markov Chain Monte Carlo (MCMC) algorithms.

From the context, it can be inferred that one strategy for handling randomness is to use locally stored variables, such as seeds for random number generation, to avoid updating the value in global memory every time a random number is generated. This approach is mentioned in Algorithm 2, which discusses the use of locally stored variables to reduce read/write latency.

Additionally, the context mentions the use of parallel MCMC sampling, which can help to reduce overall compute time by allowing each thread to produce a sample independently. However, it also notes that these algorithms are only asymptotically unbiased, which may not be optimal for applications that require resampling from highly skewed probability distributions.

Overall, while the context does not provide a comprehensive answer to the query, it suggests that using locally stored variables and parallel MCMC sampling may be strategies employed to handle randomness in certain applications. However, more information would be needed to provide a complete answer to the query.
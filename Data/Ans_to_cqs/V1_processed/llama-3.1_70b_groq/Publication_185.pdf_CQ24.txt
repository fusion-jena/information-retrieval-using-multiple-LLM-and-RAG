The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as random seed values. However, it does mention that "All feature extractors were initialized with white Gaussian noise" in the context of the AlexNet architecture. This suggests that the model's weights were initialized randomly, but it does not provide information on how this randomness was controlled or handled.

Additionally, the context mentions the use of stochastic gradient descent and the backpropagation algorithm, which are both algorithms that involve randomness. However, it does not provide information on how this randomness was handled or controlled.

Therefore, based on the provided context, I do not know what specific strategies were employed to handle randomness in the deep learning pipeline.
The provided context does not explicitly mention how the hyperparameters of the model are optimized. It only mentions that the optimizer Adam was selected with an initial learning rate of 1×10^-4 and a decay of 1×10^-7, and that early stopping was used to prevent overfitting. However, it does not provide information on whether grid search, random search, or any other method was used to optimize the hyperparameters.
The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention the use of a dropout probability of 0.3 after the second to last hidden layer in the training of 64 architectures. This suggests that dropout, a regularization technique, is used to prevent overfitting and introduce randomness in the training process.

Additionally, the context mentions the use of different optimizers, specifically SGD and Adam, and the selection of the best architecture based on its performance. This implies that some level of experimentation and exploration of different hyperparameters is employed to handle the randomness and variability in the training process.

However, it does not provide information on whether a random seed value is set to ensure reproducibility of the results.
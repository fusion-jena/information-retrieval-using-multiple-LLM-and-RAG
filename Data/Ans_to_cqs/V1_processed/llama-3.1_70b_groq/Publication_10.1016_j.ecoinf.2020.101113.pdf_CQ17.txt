The regularization methods used to prevent overfitting in the deep learning pipeline mentioned in the provided context are dropout and the use of a sigmoid activation function in the final layer. 

Specifically, the first fully connected (FC) layer uses a dropout rate of 0.5, which means that each node in the layer is ignored with a 50% probability during each training iteration. This helps to prevent overfitting by reducing the chance of the model becoming too specialized to the training data.

Additionally, the use of a sigmoid activation function in the final layer helps to map the input to a probability between 0 and 1, which can help to prevent overfitting by reducing the magnitude of the output values.

There is no mention of L2 regularization in the provided context.
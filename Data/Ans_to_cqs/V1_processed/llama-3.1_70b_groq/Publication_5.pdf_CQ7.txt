Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training. It mentions that the training process was performed for a number of iterations until the loss function reached its minimum value, and that the model was trained using a training batch size of 1024. However, it does not provide information on how the data was split into training, testing, and validation sets.

It does mention that the model was trained and tested using data from Korea, but it does not provide further details on the data splitting process. Therefore, I do not know the specific criteria used to split the data for deep learning model training.
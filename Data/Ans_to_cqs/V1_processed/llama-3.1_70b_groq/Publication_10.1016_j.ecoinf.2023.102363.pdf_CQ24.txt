The provided context does not explicitly mention the use of random seed values or strategies to handle randomness in the deep learning pipeline. However, it does mention the use of two techniques that introduce randomness during fine-tuning: dropout and DropConnect.

Dropout is a technique that randomly discards the output of hidden nodes during training, while DropConnect randomly discards the input of the hidden layer. These techniques are used to prevent overfitting and improve the model's robustness to noise.

Additionally, the context mentions the use of random pooling to curate each training set, which allows for the possibility that scarce or novel signals are not included. This suggests that the model is trained on randomly selected subsets of the data, which can help to reduce overfitting and improve generalization.

Overall, while the context does not provide a comprehensive answer to the query, it suggests that randomness is introduced in the deep learning pipeline through techniques such as dropout, DropConnect, and random pooling. However, it does not mention the use of random seed values or other strategies to handle randomness.
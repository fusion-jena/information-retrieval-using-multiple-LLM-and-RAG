The provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD or Adam. However, it does mention some hyperparameters that are commonly used in conjunction with optimization techniques. These include:

* Momentum: 0.9
* Weight decay: 0.0005
* Gamma: 0.1
* Batch size: 24 (in case of GoogLeNet), 100 (in case of AlexNet)

These hyperparameters are often used with stochastic gradient descent (SGD) or other optimization algorithms, but the specific optimization technique used is not mentioned in the provided context.
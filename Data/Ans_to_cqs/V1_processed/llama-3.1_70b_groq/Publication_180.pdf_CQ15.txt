Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context only mentions that the researchers adjusted two parameters from their default values: validation percentage and number of training steps, in order to find the optimum balance between computing time and classification efficiency. However, it does not provide information on the optimization techniques used.
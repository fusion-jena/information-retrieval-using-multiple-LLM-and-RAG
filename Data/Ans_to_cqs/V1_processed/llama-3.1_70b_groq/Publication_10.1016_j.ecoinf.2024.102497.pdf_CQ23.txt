To ensure the generalizability of the deep learning model, specifically the CART and RFR models, the researchers employed several measures. 

Firstly, the researchers fine-tuned specific model parameters such as 'max_depth', 'min_samples_leaf', and 'max_leaf_nodes' for the CART model, and 'max_depth', 'min_samples_split', 'min_samples_leaf', and 'max_features' for the RFR model. These parameters were optimized to prevent overfitting and minimize cross-validation error in the models.

Secondly, the researchers used cross-validation to evaluate the performance of the models. Although the specific details of the cross-validation process are not provided, it is mentioned that the researchers aimed to minimize cross-validation error in their models.

Lastly, the researchers used a greedy algorithm to iteratively build the tree, gradually adding features and subsets until specific stopping conditions were met. This process helps to ensure that the model is not overfitting to the training data and can generalize well to new, unseen data.

However, there is no mention of using a diverse dataset or stratified splitting to ensure the generalizability of the model. It is also not clear if the researchers used any other techniques to ensure the generalizability of the model.
The deep learning pipeline mentioned in the context uses the following optimization techniques:

1. Adam optimizer: This is used in the training of EPDNet, with a learning rate of 0.0001. Adam is a popular stochastic gradient descent optimizer that adapts the learning rate for each parameter based on the magnitude of the gradient.

2. AdamW optimizer: This is used in the training of SegFormer, with a momentum parameter (β1, β2) set to (0.9, 0.999) and ε set to 10^-8. AdamW is a variant of Adam that includes weight decay.

Additionally, the context mentions the use of a cosine annealing schedule in the training of SegFormer, which is a learning rate schedule that adjusts the learning rate over time.

It's worth noting that the context does not mention the use of SGD (Stochastic Gradient Descent) as an optimization technique in the deep learning pipeline.
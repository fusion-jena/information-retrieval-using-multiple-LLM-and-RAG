The deep learning pipeline uses image data formats, specifically multi-channel images, which are created by decomposing spectrograms into harmonic, percussive, and delta (HPD) features. The original spectrogram in decibels (dB) is filtered in the horizontal (time) and vertical (frequency) axis using median filtering, and then separated into harmonic (H) and percussive (P) components using the harmonic and percussive separation (HPS) method. These HPD features are then used to create a 3-channel "sound image" that serves as input to the convolutional neural network (CNN). 

Additionally, the pipeline also uses JSON objects containing recording metadata, which are obtained through the application programming interface (API) of Xeno-Cano. However, the primary data format used in the pipeline is the multi-channel image representation of the spectrograms.
Unfortunately, the provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. The context primarily focuses on the architecture and training details of various deep learning models, such as the use of Adam optimizer, batch size, learning rate, and hyper-parameter settings. It also discusses the incorporation of attention mechanisms, LSTM networks, and label hierarchy transformation.

However, it does mention that data augmentation was avoided in all models to ensure a fair comparison. This implies that some form of preprocessing might have been performed, but the specific steps are not mentioned.

Therefore, I do not have enough information to provide a detailed answer to the query about the preprocessing steps involved before training a deep learning model.
Unfortunately, the provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention that the AlexNet architecture, which is employed in this study, uses "dropout regularization techniques" (Krizhevsky et al., 2012). Dropout is a regularization technique used to prevent overfitting in neural networks, but it is not an optimization technique.

Additionally, the context mentions that the AlexNet architecture uses "ReLU nonlinear layers" (Krizhevsky et al., 2012), which is an activation function used in the convolutional layer to perform nonlinear mapping of the output results. However, this is not an optimization technique either.

Therefore, I don't know what optimization techniques are applied in the deep learning pipeline, as this information is not provided in the context.
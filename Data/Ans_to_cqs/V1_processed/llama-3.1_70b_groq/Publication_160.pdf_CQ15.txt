The optimization techniques applied in the deep learning pipeline, as mentioned in the provided context, are Adam and Stochastic Gradient Descent (SGD). 

Adam is used to train the CNNs from scratch, with a learning rate of 10−3 and other parameters set to their defaults. This is in accordance with the original study by Kingma and Ba (2014).

On the other hand, SGD is used to train the top layer of the ResNet152 network, with a learning rate of 10−3 and a weight decay rate set to 5 × 10−4. The whole network is then trained with SGD as well.
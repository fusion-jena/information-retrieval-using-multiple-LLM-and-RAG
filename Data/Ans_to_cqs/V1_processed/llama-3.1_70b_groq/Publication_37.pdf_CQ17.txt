The regularization methods used to prevent overfitting in the deep learning pipeline, as mentioned in the provided context, are dropout and L2 regularization. 

Specifically, dropout is applied to the input of the last two layers with a probability of 0.5. This means that during training, 50% of the neurons in the last two layers are randomly dropped out, which helps prevent overfitting by preventing the model from relying too heavily on any single neuron.

L2 regularization is also applied to the weights of the last two layers with a penalty factor of 0.001. This adds a penalty term to the loss function that is proportional to the magnitude of the weights, which helps prevent the model from overfitting by preventing the weights from becoming too large.

These regularization methods are used in conjunction with each other to prevent overfitting in the deep learning pipeline.
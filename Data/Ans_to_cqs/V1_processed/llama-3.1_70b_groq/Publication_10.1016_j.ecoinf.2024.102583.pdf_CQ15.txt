Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The text only mentions that a more advanced optimization approach is used in XGBoost, which combines first-order gradients (loss function gradient) with second-order gradients (loss function curvature), making it faster than some other models. However, this is specific to XGBoost and not related to deep learning models like CNN or RCNN.

The text does mention that the authors plan to investigate the scalability and transferability of ensemble techniques and compare their performance with deep learning models, but it does not provide information on the optimization techniques used in the deep learning pipeline.

Therefore, I don't know the answer to this question based on the provided context.
Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. The context primarily focuses on the hyperparameters used for the CNN and SVM models, such as batch size, learning rate, and optimizer. However, it does mention that the Adam learning rate optimizer was used, which can help with convergence and prevent overfitting to some extent.

Additionally, the context mentions early stopping as a technique used to prevent overfitting, where the training process is stopped if the validation error does not reduce for 10 epochs. However, this is not a regularization method per se, but rather a technique to prevent overfitting by stopping the training process early.

The context does not mention dropout or L2 regularization explicitly, which are common regularization methods used in deep learning pipelines. Therefore, I do not have enough information to provide a definitive answer to this question.
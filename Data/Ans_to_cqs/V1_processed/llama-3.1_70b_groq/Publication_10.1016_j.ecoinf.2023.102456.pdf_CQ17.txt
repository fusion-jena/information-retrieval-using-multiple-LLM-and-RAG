Unfortunately, the provided context does not mention specific regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. The context appears to be focused on various topics, including missing data in machine learning, forest fires, and cellular automata, but does not provide information on regularization methods.

However, it does mention the use of the sigmoid activation function, which can help to prevent overfitting by introducing non-linearity and bounding the output of the network. Additionally, the context mentions the use of the Gumbel softmax function, which can help to make the update criterion differentiable, but it is not a regularization method per se.

Therefore, I do not have enough information to provide a specific answer to the query.
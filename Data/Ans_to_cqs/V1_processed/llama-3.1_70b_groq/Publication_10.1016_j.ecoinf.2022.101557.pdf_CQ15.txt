According to the provided context, the optimization technique used in the deep learning pipeline is ADAM (Kingma and Ba, 2014) with a learning rate of 0.0001. This is mentioned in the section describing the training process of the convolutional neural network (CNN). Additionally, dropout with a rate of 0.3 is used during training to prevent overfitting. There is no mention of SGD (Stochastic Gradient Descent) being used in the pipeline.
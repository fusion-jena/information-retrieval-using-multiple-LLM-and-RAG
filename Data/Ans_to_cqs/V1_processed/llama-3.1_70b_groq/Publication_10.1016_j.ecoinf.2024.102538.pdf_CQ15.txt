The optimization technique applied in the deep learning pipeline is the Adam optimizer. According to the provided context, "The model was trained over 200 epochs using the Adam optimizer..." This indicates that the Adam optimizer was used to update the model parameters during training. Additionally, the learning rate was initially set at 0.001 and was reduced by a factor of 0.1 in a step-wise manner whenever the validation loss remained constant for five epochs, which is a common learning rate scheduling technique used in conjunction with the Adam optimizer.
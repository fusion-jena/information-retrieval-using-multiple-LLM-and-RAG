Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training. However, based on general practices in machine learning and deep learning, I can provide a general answer.

Typically, the criteria used to split the data for deep learning model training involve dividing the available data into three parts:

1. **Training set (e.g., train):** This is the largest portion of the data, usually around 60-80%, used to train the deep learning model.
2. **Validation set (e.g., validation):** This is a smaller portion of the data, usually around 10-20%, used to evaluate the performance of the model during training and fine-tune its hyperparameters.
3. **Test set (e.g., test):** This is the smallest portion of the data, usually around 5-10%, used to evaluate the final performance of the trained model.

These proportions can vary depending on the specific problem, dataset size, and other factors. However, this general split is commonly used to ensure that the model is trained, validated, and tested on separate datasets to prevent overfitting and ensure generalization.

Please note that this answer is based on general knowledge and not specific to the provided context, which does not mention the specific criteria used for splitting the data.
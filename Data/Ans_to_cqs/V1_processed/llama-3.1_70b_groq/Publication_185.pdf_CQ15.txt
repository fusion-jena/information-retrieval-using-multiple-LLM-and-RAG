The provided context mentions that the AlexNet model was trained using stochastic gradient descent (SGD) and the backpropagation algorithm included in Caffe. The learning rate policy was set to "step", with an initial learning rate of 10^-2 for all layers, which was decreased by a certain factor during training. However, it does not mention the use of other optimization techniques like Adam.
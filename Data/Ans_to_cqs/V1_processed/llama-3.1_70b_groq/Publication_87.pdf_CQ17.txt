The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of a ResNet architecture, which is known to use techniques such as batch normalization and weight decay to prevent overfitting.

In the context of the ResNet architecture, weight decay is a form of L2 regularization, which adds a penalty term to the loss function to discourage large weights. However, the context does not explicitly mention the use of dropout or other regularization methods.

It is also worth noting that the context mentions the use of a "cnn_learner" function, which is likely a part of a deep learning framework such as fast.ai. This framework may have built-in regularization methods, but the context does not provide enough information to determine which specific methods are being used.

Therefore, while we can infer that some form of regularization is likely being used, we cannot determine the specific methods used to prevent overfitting without more information.
Unfortunately, the provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the design and optimization of the AlexNet model, specifically the SIAlex model, and its application in image classification tasks.

However, it does mention the use of normalisation layers to accelerate training and prevent overfitting, which is a common technique used in deep learning pipelines. Additionally, the context mentions the use of residual connection techniques and skip connections between convolutional layers in the ResNet network, which can also be considered as optimization techniques.

But, it does not provide information on the specific optimization algorithms used, such as SGD or Adam, which are commonly used in deep learning pipelines. Therefore, I cannot provide a definitive answer to this question based on the provided context.
Unfortunately, the provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The context primarily focuses on the architecture of Convolutional Neural Networks (CNNs), their applications, and techniques like data augmentation and transfer learning to improve their performance.

However, it does mention some deep learning models and techniques, such as Inception-v4, Inception-ResNet, and Residual Connections, which are often used in conjunction with optimization techniques like SGD or Adam. But the specific optimization techniques used in these models are not mentioned in the provided context.

Therefore, I do not have enough information to provide a definitive answer to the query.
Unfortunately, the provided context does not explicitly mention the optimization techniques used in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does reference a paper by Ruder (2016) titled "An overview of gradient descent optimization algorithms", which suggests that gradient descent optimization algorithms are relevant to the topic. 

The context primarily focuses on the EfficientNet series, particularly EfficientNetV2, and its architecture, including the MBConv block, inverted residuals, and linear bottlenecks. It also mentions the use of a progressive learning approach that dynamically adjusts the regularization method based on the training image size, which can improve training speed and accuracy. 

While the context provides information on the network architecture and training methods, it does not specifically mention the optimization techniques used in the deep learning pipeline. Therefore, I cannot provide a definitive answer to the query.
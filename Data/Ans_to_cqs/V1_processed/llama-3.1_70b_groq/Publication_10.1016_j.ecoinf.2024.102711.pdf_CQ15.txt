Unfortunately, the provided context does not explicitly mention the optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. However, it does mention hyperparameter optimization approaches for deep neural networks (DNN) in the context of land cover/land use (LCLU) classification (Azedou et al., 2023). The context also mentions the use of k-fold cross-validation for hyperparameter tuning in AutoML models (Saha et al., 2022). While this information is related to optimization, it does not specifically answer the question about optimization techniques like SGD or Adam in the deep learning pipeline. Therefore, I do not have enough information to provide a specific answer to this question.
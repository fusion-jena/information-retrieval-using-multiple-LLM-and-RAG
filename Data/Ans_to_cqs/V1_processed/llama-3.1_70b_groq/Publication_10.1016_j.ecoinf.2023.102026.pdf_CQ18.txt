The strategy implemented to monitor the model performance during training is through the use of validation metrics. For the non-bootstrapped GLM, the performance was evaluated by calculating the Area Under the ROC Curve (AUC), Percent Correctly Classified (PCC), sensitivity, specificity, and True Skill Statistic (TSS) using the 20% camera traps retained from the training dataset.

For the bootstrapped GLM, the performance was evaluated by calculating the same validation metrics (AUC, PCC, sensitivity, specificity, and TSS) for each of the 100 iterations, using the data retained from the model training, composed of 80% of the presence and absence locations. The final metrics were obtained by averaging the results of the 100 validation iterations. This framework was implemented independently for each felid.
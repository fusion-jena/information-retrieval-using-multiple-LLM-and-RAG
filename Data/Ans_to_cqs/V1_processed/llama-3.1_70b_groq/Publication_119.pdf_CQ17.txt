The regularization methods used to prevent overfitting in the deep learning pipeline, as mentioned in the provided context, are dropout and batch normalization. 

Dropout is a technique where a fraction of the neurons in a layer are randomly dropped out during training, which helps prevent overfitting by reducing the model's reliance on any single neuron. 

Batch normalization is another technique that helps reduce overfitting by normalizing the input to each layer, which helps stabilize the training process and improve the model's generalization performance.

There is no mention of L2 regularization in the provided context. However, it is a common regularization technique used in deep learning to prevent overfitting by adding a penalty term to the loss function for large weights. 

It's worth noting that the context also mentions the use of a pre-trained ResNet-50 backbone in the U-Net model, which can also help reduce overfitting by leveraging the knowledge learned from a large dataset (ImageNet).
The provided context does not explicitly mention the preprocessing steps involved before training a deep learning model. However, it does mention that the model was trained on patches of data, and that the model was trained for 110 epochs, with each epoch composed of 4800 training samples. It also mentions that the model was trained on a desktop with 64 Gb of RAM and one Titan X (Maxwell) GPU.

Additionally, the context mentions that the data was augmented by rotating the patches right by 90 degrees, flipping them horizontally or vertically, or leaving them as is. This suggests that some form of data augmentation was used to increase the size of the training dataset.

The context also mentions that the model was trained using a geometrically decaying learning rate, but it does not provide information on other preprocessing steps such as normalization, scaling, or cleaning.

Therefore, based on the provided context, the only preprocessing step that can be confirmed is data augmentation through rotation and flipping. Other preprocessing steps such as normalization, scaling, or cleaning are not mentioned.
The provided context does not explicitly mention the strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention that the dataset is split "randomly (but fixed for all experiments and all networks)" into a train set, validation set, and test set. This suggests that the randomness in the data splitting process is controlled in some way, possibly by setting a fixed random seed, but the specific details are not provided.

It is also worth noting that the context mentions the use of Stochastic Gradient Descent (SGD) as the optimization technique, which inherently involves randomness in the gradient computation. However, the context does not provide information on how the randomness in SGD is handled.

In summary, while the context provides some hints about handling randomness in the data splitting process, it does not provide explicit information on the strategies employed to handle randomness in the deep learning pipeline.
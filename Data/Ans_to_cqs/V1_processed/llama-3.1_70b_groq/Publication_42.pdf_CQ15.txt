The optimization techniques applied in the deep learning pipeline, as mentioned in the provided context, are the Adam optimizer and the Stochastic Gradient Descent (SGD) optimizer. According to the text, the Adam optimizer turned out to be the superior optimizer for training all models, compared to SGD. The context also mentions that the top 32 architectures were trained using the Adam optimizer, while the next 32 architectures (Rank 33 to 64) were trained using the SGD optimizer.
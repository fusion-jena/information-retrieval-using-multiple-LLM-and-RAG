To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the data used for model training and selection were randomly partitioned into four sets: data for training candidate models (25% of the data; At), data for validating candidate models (50%; Av), data for training the selected candidate model (75%; Bt = At + Av), and validation data to determine the optimal number of epochs to train the selected candidate model (25%; Bv). This partitioning was performed in R with package dismo.

Additionally, the model selection procedure involved training and validating multiple models with different architectures and hyperparameters. Specifically, 20 models were randomly generated for each of the four available deep-ANN architecture types, and the model with the highest performance was selected for further training on the full training data.

Furthermore, the performance of the candidate models was evaluated using a left-out validation data set (data partition Av), which helps to prevent overfitting and ensures that the model is generalizable to unseen data.

While the text does not explicitly mention the use of a diverse dataset or stratified splitting, the random partitioning of the data and the use of a left-out validation set suggest that efforts were made to ensure the generalizability of the model. However, it is unclear whether the dataset was diverse or whether stratified splitting was used.
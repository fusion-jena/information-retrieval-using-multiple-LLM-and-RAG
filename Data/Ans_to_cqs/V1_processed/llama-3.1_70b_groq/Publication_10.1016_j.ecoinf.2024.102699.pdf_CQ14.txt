Unfortunately, the provided context does not explicitly mention how the hyperparameters of the model are optimized. It only mentions the specific hyperparameters used in the training setup, such as the Adam optimizer, a concatenating cosine-annealing linear scheduler with an initial learning rate of 0.0001, and a batch size of 32. However, it does not provide information on how these hyperparameters were chosen or optimized.

Therefore, I don't know the answer to this question based on the provided context.
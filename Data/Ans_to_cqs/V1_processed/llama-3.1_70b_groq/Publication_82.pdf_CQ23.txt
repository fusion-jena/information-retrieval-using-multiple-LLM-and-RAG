To ensure the generalizability of the deep learning model, several measures were taken. 

Firstly, the experiments were run across a whole range of train-test set splits, namely 80-20, 60-40, 50-50, 40-60, and 20-80. This cross-validation approach helps to evaluate the model's performance on unseen data and prevents overfitting.

Secondly, the model was tested on a set of images taken under conditions different from the images used for training. Although the model's accuracy was reduced substantially in this case, it still performed better than random selection.

Lastly, the authors acknowledge the limitations of their current dataset and the need for a more diverse set of training data to improve the accuracy. They mention that data collection efforts are underway to address this issue.

However, there is no mention of stratified splitting in the provided context.
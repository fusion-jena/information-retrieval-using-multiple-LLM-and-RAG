The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention some deep learning architectures and techniques, such as ResNet, which is known to use techniques like batch normalization and weight decay (a form of L2 regularization) to prevent overfitting.

Additionally, the context mentions the work of Goodfellow et al. (2016) in their book "Deep Learning", which likely discusses various regularization methods used in deep learning, including dropout and L2 regularization. However, without direct access to the book's content, we cannot confirm the specific methods mentioned.

It is also worth noting that dropout is a widely used regularization method in deep learning, and it is likely that it is mentioned in some of the referenced papers or books. However, without more specific information, we cannot provide a definitive answer to the query.

In summary, while the provided context does not explicitly mention the regularization methods used to prevent overfitting, it is likely that techniques like dropout and L2 regularization are discussed in some of the referenced works.
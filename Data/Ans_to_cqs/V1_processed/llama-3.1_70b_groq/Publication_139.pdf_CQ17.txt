The provided context does not explicitly mention the use of dropout or L2 regularization as regularization methods to prevent overfitting. However, it does mention the use of early stopping as a method to prevent overfitting. Early stopping is a regularization technique that stops the training process when the model's performance on the validation set starts to degrade, preventing the model from overfitting to the training data.

Additionally, the context mentions that the model is less prone to overfitting due to its simpler architecture, which is similar to those implemented by Ruff et al. (2020) and Sprengel et al. (2016). However, it does not provide further details on the specific regularization methods used in these architectures.

Therefore, based on the provided context, the only regularization method mentioned to prevent overfitting is early stopping.
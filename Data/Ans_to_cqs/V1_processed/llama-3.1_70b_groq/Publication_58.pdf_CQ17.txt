Unfortunately, the provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention that the architectures were trained with stochastic gradient descent using a momentum of 0.9 and a learning rate that is divided by 10 every 20 epochs until reaching 10^-6. This suggests that the learning rate schedule is used as a form of regularization to prevent overfitting. Additionally, the context mentions that the median frequency balancing method is used to balance the data sets, which can also help to prevent overfitting. However, it does not provide information on other regularization methods that may be used.
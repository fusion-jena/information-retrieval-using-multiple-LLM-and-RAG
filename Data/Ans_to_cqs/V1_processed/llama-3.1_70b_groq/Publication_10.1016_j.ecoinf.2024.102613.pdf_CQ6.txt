Based on the provided context, the preprocessing steps involved before training a deep learning model are not explicitly mentioned. However, the context does mention data preprocessing features in general, which include:

* Data deduplication (Eliminating excessive copies of data)
* Data cleansing (Removing incorrect, corrupted, incomplete data)
* Data transformation (Changing the structure or format of data)
* Data compression (Modifying or converting the bits structure of data)

While these steps are not specifically mentioned in the context of deep learning, they are general data preprocessing techniques that are often applied before training machine learning models, including deep learning models.

Additionally, the context mentions that data cleansing is a mandatory feature in data preprocessing, which suggests that cleaning the data is an essential step before any further processing or analysis.

However, specific preprocessing steps like normalization and scaling are not mentioned in the provided context. Therefore, I cannot confirm if these steps are involved in the preprocessing pipeline for deep learning models in this specific context.
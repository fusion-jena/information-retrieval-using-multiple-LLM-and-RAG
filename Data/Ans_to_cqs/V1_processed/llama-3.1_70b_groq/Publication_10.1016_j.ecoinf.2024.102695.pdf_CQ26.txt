To address data bias during preprocessing of the deep learning pipeline, the technique used in this context is chronological splitting of the dataset. The dataset is divided into training, validation, and test sets in a ratio of 70:10:20, respectively. This allocation strategy is used to simulate real-world scenarios and ensure that the data in the validation and test sets differ from the training set data, thereby enhancing the generalizability of the model.

There is no mention of using stratified splitting, oversampling, undersampling, or diverse data collection techniques to address data bias in the provided context. However, the use of chronological splitting helps to prevent data snooping bias, which is a common issue in machine learning and deep learning pipelines.

It's worth noting that the context does mention the use of an early stopping strategy during training to prevent overfitting issues, but this is not directly related to addressing data bias during preprocessing.
To ensure the generalizability of the deep learning model, specifically the BERT model, several measures were taken. 

Firstly, the dataset was split into chunks of 300 tokens using the NLTK word-tokenizer to identify the sections of text most likely to be relevant to the classification decision. This approach was previously applied to long document classification (Fiok et al., 2021; Sun et al., 2019). 

Secondly, the model was trained on a subset of the data that contained the query terms, which were bat-related or exploitation-related terms. This was done to ensure that the model was trained on relevant data.

Thirdly, the model was tested on a separate testing set, which was not used during training. This is a common practice in machine learning to evaluate the performance of the model on unseen data.

Lastly, the performance of the model was compared across different classification tasks and datasets, including web articles and social media posts. This comparison was done to evaluate the generalizability of the model across different types of data.

Additionally, the influence of data stratification on the performance of the classifiers was also evaluated. The results showed that stratification of training data for classification tasks 2 and 3 influenced the performance with which classifiers were able to identify relevant texts in testing data. 

However, it is not mentioned in the provided context whether cross-validation was used to evaluate the model's performance.
The provided context does not explicitly mention the regularization methods used to prevent overfitting in the deep learning pipeline. However, it does mention the use of batch normalization (BN) to hamper vanishing/exploding gradients in deep CNNs. Additionally, it mentions the use of auxiliary classifiers in GoogLeNet to accelerate network convergence, but it does not explicitly state that this is a regularization method.

It is also worth noting that the context mentions the use of shortcuts (identity and projection shortcuts) in ResNet to address the degradation problem, but it does not explicitly state that this is a regularization method.

The context does not mention dropout or L2 regularization as regularization methods used to prevent overfitting in the deep learning pipeline. Therefore, based on the provided context, I do not know the specific regularization methods used to prevent overfitting in the deep learning pipeline.
The provided context does not specifically mention the regularization methods used to prevent overfitting in a deep learning pipeline. However, it does mention some hyperparameters related to regularization in the context of XGBoost and LightGBM models.

The hyperparameters mentioned are:

* 'reg_alpha': (0.0, 0.1)
* 'lambda_l1': (0.0, 0.1)
* 'reg_lambda': (0.0, 0.1)
* 'lambda_l2': (0.0, 0.1)

These hyperparameters appear to be related to L1 and L2 regularization, which are common techniques used to prevent overfitting in machine learning models. However, dropout is not mentioned in the provided context.

Therefore, based on the provided context, it can be inferred that L1 and L2 regularization are used to prevent overfitting, but it is not clear if dropout or other regularization methods are used.
The provided context does not explicitly state how the hyperparameters of the model are optimized. It mentions that different hyperparameters were adjusted as described in Section 2.2.3, but it does not provide details on the optimization method used. It only mentions that a total of 64 architectures were trained using a dropout probability of 0.3 after the second to last hidden layer, and that the Adam optimizer turned out to be superior to SGD. However, it does not provide information on whether grid search, random search, or another method was used to optimize the hyperparameters.
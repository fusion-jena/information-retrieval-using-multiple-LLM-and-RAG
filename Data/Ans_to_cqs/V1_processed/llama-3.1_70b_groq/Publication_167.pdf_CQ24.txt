The provided context does not explicitly mention strategies employed to handle randomness in the deep learning pipeline, such as setting a random seed value. However, it does mention the use of batch normalization (BatchNorm) to handle differences in the distribution between different batches of training data and between the training set and the test set. 

Batch normalization is a technique used to normalize the input data for each layer in a deep neural network. It helps to reduce the effect of internal covariate shift, which can cause the distribution of the input data to change as the network is trained. By normalizing the input data, batch normalization can help to improve the stability and performance of the network.

While batch normalization can help to reduce the effect of randomness in the input data, it does not directly address the issue of randomness in the deep learning pipeline. The context does not provide information on whether a random seed value is set or other strategies are employed to handle randomness.
Unfortunately, the provided context does not mention specific optimization techniques applied in the deep learning pipeline, such as SGD (Stochastic Gradient Descent) or Adam. The text focuses on the annotation process, annotator variability, and the impact of different annotation approaches on model performance. It discusses the use of Convolutional Neural Networks (CNNs) and the evaluation of their performance on different annotation sets, but it does not provide details on the optimization techniques used during training.
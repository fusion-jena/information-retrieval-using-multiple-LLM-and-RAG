To ensure the generalizability of their deep learning model, several measures were implemented by the authors. Firstly, they used a diverse dataset consisting of various architectures such as AlexNet, DenseNet, GoogLeNet, Inception, MobileNet, Resnet, Resnext, VGG, and Xception. These networks have been widely adopted in the field of computer vision and have shown excellent performance in image classification tasks.

Secondly, the authors employed two different sampling design strategies - continuous and discrete - to capture variations in tree cover levels. This approach allowed them to create a more comprehensive dataset that could better represent real-world scenarios. Examples of these strategies can be found in Figures S1 and S2, which illustrate the difference between the two methods and provide visual representations of the classes within the training dataset.

Thirdly, the metadata of the training datasets used in this study is publicly accessible through archives S1 and S2. This transparency allows other researchers to replicate or build upon the work done in this paper, further contributing to its generalizability.

Lastly, while not explicitly mentioned in the given context, it is common practice in machine learning research to use techniques like cross-validation and stratified splitting to evaluate the performance of models and prevent overfitting. Cross-validation involves dividing the dataset into multiple subsets and iteratively training the model on one subset while testing it on another. Stratified splitting ensures that the distribution of classes remains consistent across all subsets, preventing any bias towards certain classes during training. Given the rigorous methodology presented in the paper, it is likely that similar best practices were followed to ensure the robustness and reliability of the proposed deep learning model.
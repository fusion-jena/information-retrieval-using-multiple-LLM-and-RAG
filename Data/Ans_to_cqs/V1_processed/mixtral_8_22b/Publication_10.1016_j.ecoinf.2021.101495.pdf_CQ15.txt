The given context does not provide information about the specific optimization techniques applied in the deep learning pipeline, such as Stochastic Gradient Descent (SGD), Adam, or others. However, we do see that manual tuning was employed as part of the search strategy during training. Additionally, the table provides details about various hyperparameters like epochs, batch size, learning rate, activation function, and train/test split ratio, which were optimized during the training process. Unfortunately, without explicit mention of the optimization technique used, we cannot definitively state whether SGD, Adam, or another method was utilized.
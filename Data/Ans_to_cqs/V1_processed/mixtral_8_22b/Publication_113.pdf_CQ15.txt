The Stochastic Gradient Descent (SGD) optimization technique is applied in the deep learning pipeline. It is an iterative algorithm that is applied multiple times during training, including multiple times for each image in the dataset. At every iteration of the SGD algorithm, the parameters take one step towards minimizing the loss value. Additionally, modern enhancements of SGD, such as Adam (Kingma & Ba, 2014) and RMSProp (Tieleman & Hinton, 2012), calculate the contribution of each parameter to the loss value and adjust the parameters accordingly to minimize the loss value.
Unfortunately, there isn't enough information available in the given context to provide a detailed explanation about the process followed to deploy the trained deep learning model. However, I can share some common steps involved in deploying a deep learning model which might be helpful.

Model Serialization: After training the deep learning model, the first step usually involves saving or serializing the model architecture along with its learned weights and biases into a file format like.h5,.onnx, etc. This allows us to reuse the same model later without having to train it again.

Platform Selection: Once the model has been serialized, we need to choose a platform where our model will run. Depending upon the application requirement, this could range from cloud platforms like AWS, Google Cloud, Azure, etc., to edge devices like Raspberry Pi, NVIDIA Jetson, etc.

Deployment: Based on the chosen platform, different methods may apply for deploying the model. For instance, if we chose a cloud platform, we would typically create an API endpoint using services like Flask, FastAPI, Django, etc., and then integrate our saved model within these APIs. On the other hand, if we selected an edge device, we would directly load the saved model onto the device and perform real-time predictions.

Monitoring & Maintenance: Post-deployment, it's crucial to monitor the model's performance regularly and maintain it by retraining with new data when necessary. Tools like TensorFlow Serving, Kubernetes, etc., help manage and update deployed models effectively.
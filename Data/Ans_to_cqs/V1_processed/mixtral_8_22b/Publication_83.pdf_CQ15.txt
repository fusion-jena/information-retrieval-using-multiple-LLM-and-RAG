In the given context, it is mentioned that the network is trained using the Adam optimizer with a learning rate of 10^-4. Therefore, the optimization technique applied in this deep learning pipeline is Adam.

Additional Information:
Adam (short for Adaptive Moment Estimation) is a popular optimization algorithm used in training neural networks. It combines the advantages of two other extensions of stochastic gradient descent, namely RMSProp and AdaGrad. Adam computes adaptive learning rates for different parameters by maintaining a moving average of past gradients and squared gradients. This makes it well suited for problems with large datasets and high dimensionality.
Before training a deep learning model such as a Convolutional Neural Network (CNN), several preprocessing steps are typically involved to prepare the input data. These steps may include normalization, scaling, cleaning, and augmentation.

Normalization involves adjusting the range of pixel intensity values in the input images so they fall within a specific range, often between 0 and 1 or -1 and 1. This step helps improve the convergence speed during training and ensures that all input features contribute equally to the learning process.

Scaling refers to resizing the input images to a fixed size, which is necessary because most deep learning models require inputs of consistent dimensions. Scaling also reduces computational complexity and improves efficiency during training.

Cleaning involves removing any irrelevant information or noise from the input data. For example, if there are missing or corrupted pixels in an image, these should be either removed or replaced with appropriate values. Cleaning ensures that only relevant and accurate information is fed into the model during training.

Data augmentation techniques are commonly employed to increase the amount of training data and prevent overfitting. Augmentation methods include rotation, flipping, cropping, zooming, shifting, and adding random noise to the input images. By applying various transformations to the original images, we can generate new samples that help improve the generalizability and robustness of the trained model.

In the given context, the authors mention that they apply augmentation to the entire training dataset using rotations between 0 and 360 degrees, reflection along the vertical axis, and vertical shifts by Â±5 pixels. Additionally, standard image normalization is applied, mapping the 8-bit pixel values to floating-point numbers between 0 and 1. Finally, the dataset is randomly split into training, validation, and test sets, containing 70%, 15%, and 15% of the images, respectively.
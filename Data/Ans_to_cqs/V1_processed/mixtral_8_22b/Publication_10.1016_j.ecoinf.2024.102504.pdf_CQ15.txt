The papers mentioned do not provide specific information about the optimization techniques used in their respective deep learning pipelines. However, common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), Root Mean Square Propagation (RMSProp), and Adaptive Gradient Algorithm (AdaGrad). These methods help minimize the loss function during training by adjusting the weights and biases of the neural network. Without explicit mention in the given sources, it is impossible to determine which optimization technique was employed in each case.
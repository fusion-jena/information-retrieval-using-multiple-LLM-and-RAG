The given context does not provide explicit information about the specific hardware resources used for training the deep learning models mentioned, such as GPUs or TPUs. However, it mentions that the models were implemented using Keras with TensorFlow backend, which suggests that they could have been trained on either CPUs or GPUs depending on the available hardware resources. Additionally, the use of data augmentation techniques like flipping, rotation, and shifting implies that the models might have benefited from GPU acceleration due to their parallelizable nature. Nevertheless, without further details regarding the exact hardware setup, it cannot be definitively stated whether GPUs, TPUs, or other hardware resources were utilized for training these deep learning models.
The paper by X. Wu et al. does not provide information on the specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, some common techniques used for addressing data bias include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into training and testing sets while maintaining the same distribution of classes or labels in both sets. This ensures that the model is trained and tested on representative samples from each class.

Oversampling and undersampling involve adjusting the balance between different classes in the dataset. Oversampling increases the representation of minority classes by duplicating their instances, while undersampling reduces the representation of majority classes by removing some of their instances. These techniques can help prevent the model from being biased towards the majority class.

Diverse data collection involves collecting data from various sources and perspectives to ensure that the dataset is representative of the population it aims to represent. This technique helps reduce bias caused by over-representation of certain groups or under-representation of others.

It is important to note that these techniques should be applied carefully, as they may introduce new forms of bias if not implemented properly. For example, oversampling could lead to overfitting if too many duplicate instances are created, while undersampling could result in loss of valuable information if too many instances are removed. Therefore, it is recommended to evaluate the impact of these techniques on the performance of the model before applying them.
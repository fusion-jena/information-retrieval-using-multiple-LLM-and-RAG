The text does not explicitly state how the hyperparameters of the model were optimized. However, it mentions that the model's initial learning rate was set to 0.01 and then further refined via iterative testing. This suggests that some form of iterative or adaptive method might have been used to optimize the learning rate. Unfortunately, without more specific information, we cannot definitively determine whether methods like grid search or random search were employed. It is also worth noting that the text mentions several optimization algorithms, such as the Genghis Khan Shark Optimizer, Gazelle Optimization Algorithm, and Dandelion Optimizer, which could potentially be used for hyperparameter tuning. Nonetheless, these algorithms are not mentioned specifically in relation to the model described in the given context. Therefore, while there may be speculation about possible methods used for hyperparameter optimization, the exact approach remains unclear based on the provided information.
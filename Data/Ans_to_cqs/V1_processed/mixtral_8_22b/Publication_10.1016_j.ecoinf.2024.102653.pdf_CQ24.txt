To handle randomness in the deep learning pipeline, this study employs several strategies. Firstly, it uses the train_test_split function from the scikit-learn library to facilitate randomized shuffling and division of datasets into distinct training and testing subsets. This ensures that the data is not biased towards any particular group or pattern. Secondly, the model's weights are randomly initialized using a specific integer seed. This guarantees consistent reproducibility of results, meaning that if the same code is run again, it should produce the same results. Lastly, when partitioning the dataset into training and validation sets, the division is performed through random selection. This helps prevent overfitting and ensures that the model can generalize well to unseen data.
Regularization methods are techniques used in machine learning and deep learning models to prevent overfitting by adding a penalty term to the loss function during optimization. In the context of deep learning pipelines, two common regularization methods are Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training, which helps to prevent complex co-adaptations on training data. This results in more robust and generalizable models. During each training cycle, individual nodes are either dropped out of the net with probability (1 - keep_probability) or kept with probability keep_probability.

L2 Regularization, also known as weight decay, adds a penalty equivalent to the sum of squares of all the weights multiplied by a factor called lambda to the loss function. By doing so, it discourages large weights, effectively limiting the capacity of the network and helping to avoid overfitting.

Other regularization methods include L1 Regularization, Early Stopping, and Data Augmentation. However, these were not explicitly mentioned in the given context.

Therefore, based on the provided information, the regularization methods used to prevent overfitting in the deep learning pipeline are Dropout and L2 Regularization.
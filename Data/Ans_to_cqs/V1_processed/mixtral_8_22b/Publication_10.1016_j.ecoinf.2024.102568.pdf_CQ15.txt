The deep learning pipeline uses the ADAM optimization technique, specifically mentioned as Adam optimization (Kingma and Ba (2015)). This optimization method is used for stochastic optimization during the training process of the LSTM model. Additionally, a dropout layer is employed as a regularization method to avoid overfitting. This layer randomly excludes certain inputs from activation and weight updates while training the network. Inputs not set to zero are scaled up by 1/(1 - rate) to maintain the total input sum. Furthermore, the training set undergoes preprocessing using a MinMaxScaler, which scales each feature into a [0-1] range. However, the dimension of the temporal window needs to be selected dynamically based on the performance achieved with different values on the available dataset.
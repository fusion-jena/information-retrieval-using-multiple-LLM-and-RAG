The text does not provide specific information regarding the regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization techniques include dropout, L1 regularization, and L2 regularization. Dropout randomly sets a fraction of input units to 0 during training, which helps prevent overfitting. L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients, while L2 regularization adds a penalty equal to the square of the magnitude of coefficients. These penalties discourage large weights and help reduce overfitting. Other techniques such as early stopping, data augmentation, and batch normalization can also be employed to mitigate overfitting.
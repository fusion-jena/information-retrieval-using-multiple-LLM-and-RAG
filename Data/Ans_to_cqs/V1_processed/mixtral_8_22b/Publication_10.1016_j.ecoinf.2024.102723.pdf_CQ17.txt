In the given context, the primary regularization method mentioned is Monte Carlo Dropout (MCD). Dropout is a widely used regularization technique in deep learning to prevent overfitting. It works by randomly dropping out neurons during training, effectively creating a new network architecture for each iteration. In the case of MCD, it uses the Bernoulli distribution to perform dropout, resulting in uncertainty estimates. This makes MCD a convenient technique for performing Bayesian Deep Learning. Other variants of dropout also exist, which correspond to alternative approximate distributions and offer different uncertainty estimates, setting a trade-off between uncertainty quality and computational complexity. Apart from dropout, no other explicit regularization techniques like L2 regularization are explicitly mentioned in the provided context.
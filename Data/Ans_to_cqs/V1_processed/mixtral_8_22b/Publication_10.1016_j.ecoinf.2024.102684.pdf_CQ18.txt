To monitor the model performance during training, the authors employed a strategy where they divided their dataset into three distinct subsets: training (60%), validation (20%), and testing (20%). They chose this methodology to achieve an impartial assessment of the model's error. By applying data augmentation solely to the training set, they ensured no overlap of identical data across various partitions.

Two primary metrics were utilized to gauge the effectiveness of the models: the coefficients of determination (R^2) and the root mean squared error (RMSE). The former, computed as the square of Pearson's correlation coefficient, gauges the proportion of variance in the dependent variable elucidated by the model. Meanwhile, the latter calculates the discrepancy between the anticipated and true values, with lower scores signaling superior model accuracy. Both RMSE and R^2 values spanned from 0.0 to 1.0, with values nearer to zero denoting more precise model forecasts.

Overfitting was appraised by juxtaposing the model's efficacy on the training, validation, and test datasets. Furthermore, all models were executed utilizing the Keras library in Python 3.8.3. A unique aspect of this study involved evaluating the models not merely within the confines of the training domain but also beyond it, employing a transfer learning dataset. This dual-context analysis facilitated a comprehensive understanding of the model's capacity to generalize its learnings.
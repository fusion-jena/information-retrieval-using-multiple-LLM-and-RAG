The strategy implemented to monitor the model performance during training was to tune the number of required epochs and then inspect the model outputs and error outcomes for signs of overfitting. Overfitting occurs when the model focuses excessively on data fitting, which may cause it to imitate insignificant noise instead of the broad properties of interest. To prevent overfitting, the models were trained and tested until they converged to their most accurate state, with each image being assessed only 10-30 times (or after 10-30 training epochs). Further training did not improve or even decreased the algorithm's performance on held-out test data, suggesting that the model was memorizing its training set rather than learning a generalized matching strategy.
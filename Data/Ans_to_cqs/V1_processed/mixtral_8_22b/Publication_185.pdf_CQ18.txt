Based on the given information, there isn't any explicit mention of a specific strategy used to monitor the model performance during training. However, we do see some indicators that suggest how the authors might have monitored the model performance.

Firstly, the paper mentions that the factor of learning rate was reduced by a factor of 10 every 69 iterations, and training was stopped after 6900 iterations. This implies that the authors may have been monitoring the loss or accuracy metrics during each iteration and decided to reduce the learning rate when the improvement plateaued. They also chose to stop training once the desired level of performance was achieved or if further improvements seemed unlikely.

Secondly, the authors mentioned using a validation set to evaluate the performance of the trained models. By comparing the performance of the models on this separate validation set, they could ensure that the models generalized well to unseen data and did not overfit to the training data.

Lastly, the table showing the model test results indicates that the authors evaluated the performance of their models on both the validation and testing sets. This suggests that they likely monitored the performance of their models throughout the entire training process, from initial development through final evaluation.

In summary, while there is no direct statement about the strategy used to monitor the model performance during training, based on the available information, it appears that the authors employed several common practices such as tracking loss/accuracy metrics, reducing the learning rate periodically, stopping training when necessary, evaluating on a separate validation set, and assessing performance on both validation and testing datasets.
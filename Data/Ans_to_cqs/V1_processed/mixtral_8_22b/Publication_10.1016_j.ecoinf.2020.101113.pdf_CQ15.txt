Based on the given context, there is no explicit mention of specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being applied in the deep learning pipeline. However, it can be inferred that some form of optimization technique was employed during the training process since the text mentions that the model was trained and optimized for a similar dataset. Additionally, the use of fully connected (FC) layers suggests that backpropagation was likely utilized for updating the weights of these layers during training. Nevertheless, without further information, it cannot be definitively stated what specific optimization techniques were used in the deep learning pipeline.
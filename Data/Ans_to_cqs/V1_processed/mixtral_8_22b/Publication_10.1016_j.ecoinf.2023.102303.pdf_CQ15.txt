Unfortunately, the given context does not provide information about specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being applied in the deep learning pipeline. However, it mentions that EfficientNet uses a simple yet effective compound rate applied in nine stages with different repeat amounts, allowing the designer to arbitrarily adjust the CNN's scaling. Additionally, it discusses the use of semi-supervised contrastive learning and a weakly supervised model developed for the FishInTurbidWater dataset. These methods could potentially involve optimization techniques like SGD or Adam during training, but without explicit mention in the context, I cannot confirm their usage.
The papers mentioned do not provide information on the specific regularization methods used to prevent overfitting in the deep learning pipeline. Regularization methods such as dropout and L2 regularization are commonly used techniques to prevent overfitting in deep learning models. Dropout involves randomly dropping out neurons during training to reduce co-dependencies between layers, while L2 regularization adds a penalty term to the loss function to discourage large weights. Other regularization methods include early stopping, which stops training once validation error starts increasing, and batch normalization, which scales inputs to have zero mean and unit variance. These methods help improve generalization performance by reducing overfitting and improving robustness to noise.
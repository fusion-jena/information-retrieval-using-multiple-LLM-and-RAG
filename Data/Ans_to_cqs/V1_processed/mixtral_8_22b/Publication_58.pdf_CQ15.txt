In the given context, it is mentioned that all architectures were trained with stochastic gradient descent (SGD) using a similar protocol. This indicates that SGD is the optimization technique employed in the deep learning pipeline. Additionally, a momentum of 0.9 was used, which helps accelerate SGD in relevant directions and dampen oscillations. The initial learning rate was set to 10^-2, and every 20 epochs, the learning rate was divided by 10 until reaching 10^-6. This process of gradually decreasing the learning rate during training is known as learning rate decay or learning rate scheduling, which helps improve convergence and generalization.
The paper does not explicitly discuss strategies to handle randomness in the deep learning pipeline, such as managing random seed values. However, it mentions employing various parameter values during model training, including learning rates, batch sizes, and latent dimensions. It also explores different regularization techniques like weight decay, batch normalization, and dropout. These methods can help mitigate the effects of randomness introduced by factors such as initialization weights and data shuffling. Furthermore, the use of schedulers to adjust the learning rate when the network reaches a plateau may indirectly address some aspects of randomness. Nevertheless, specific strategies targeting random seeds or similar sources of randomness are not mentioned in this study.
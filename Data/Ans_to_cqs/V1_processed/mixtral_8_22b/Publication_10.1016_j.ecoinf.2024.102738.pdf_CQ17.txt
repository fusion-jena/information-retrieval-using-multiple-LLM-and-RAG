The given context does not provide information about the specific regularization methods used to prevent overfitting in a deep learning pipeline. However, some common regularization techniques used in machine learning and deep learning models include Dropout, L1 Regularization (Lasso Regression), L2 Regularization (Ridge Regression), and Early Stopping. These methods help reduce model complexity and avoid overfitting by adding constraints to the optimization process during training.

Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent complex co-adaptations between neurons. L1 Regularization adds a penalty term to the loss function, encouraging sparse solutions by forcing weights towards zero. Similarly, L2 Regularization also adds a penalty term but encourages smaller weight values rather than setting them to zero. Early stopping involves monitoring the performance of the model on a validation set during training and halting the process once the validation error starts increasing, indicating potential overfitting.
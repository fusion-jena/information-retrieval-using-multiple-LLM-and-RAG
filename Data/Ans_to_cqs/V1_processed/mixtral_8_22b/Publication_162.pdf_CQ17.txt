The given context does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline mentioned in the paper. However, based on common practices in deep learning, some possible regularization techniques could be employed such as Dropout or L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust and reducing its reliance on any single neuron. On the other hand, L2 Regularization adds a penalty term to the loss function, encouraging smaller weight values and thus preventing overfitting due to large weights.

While these methods are commonly used, without further details from the authors or the original research, it cannot be confirmed if either of these techniques were applied in this particular case.
The context does not provide specific information about the criteria used to split the data for deep learning model training into sets such as train, test, or validation. However, generally speaking, there are several common methods used to split datasets for machine learning purposes:

1. Holdout method: This is the simplest way to divide the dataset. Typically, it involves splitting the dataset into two parts - one for training and another for testing. Common ratios include 80% for training and 20% for testing, but these can vary depending on the size of the dataset and the problem being solved.

2. Random subsampling: Similar to the holdout method, this technique randomly splits the dataset multiple times into different subsets for training and testing. Each subset has its own unique combination of samples. Results are then averaged across all runs to get a more robust estimate of the modelâ€™s performance.

3. Cross-validation: This method involves dividing the dataset into 'k' number of folds or partitions. One fold is kept aside for testing while the remaining 'k-1' folds are used for training. This process is repeated 'k' times so that each fold gets a chance to act as the test set once. The results are again averaged over all iterations to obtain a final score.

4. Stratified sampling: When dealing with imbalanced classes in classification problems, stratification ensures that both the training and testing sets maintain the same distribution of target variable values as present in the original dataset.

Remember, the choice of splitting strategy depends on factors like the size of the dataset, computational resources, and the nature of the problem being addressed.
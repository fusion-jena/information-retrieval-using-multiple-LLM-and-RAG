The provided context does not explicitly mention any specific preprocessing steps taken before training the deep learning models. However, there are some implicit indications about certain aspects of data preparation.

Firstly, the context mentions three types of datasets: color, grayscale, and leaf segmented. This suggests that some form of image processing or conversion might have been applied to generate different versions of the original dataset. Grayscaling involves removing color information from images, while leaf segmentation could involve isolating leaves from their backgrounds in images.

Secondly, the context refers to varying distributions of training and testing sets, ranging from 80-20% to 20-80%. This implies that the raw dataset was divided into separate subsets for training and testing purposes.

Lastly, the context discusses the use of stochastic gradient descent as the solver type, along with other hyperparameters like base learning rate, learning rate policy, momentum, weight decay, gamma, and batch size. While these are not strictly preprocessing steps, they do indicate how the data was handled during the training process.

However, without explicit details regarding normalization, scaling, cleaning, or other common preprocessing techniques, it cannot be definitively stated what preprocessing steps were involved before training the deep learning models.
The text does not provide explicit information about the specific criteria used to determine when training is completed. However, it mentions burn-in steps and thinning steps, which are common techniques used in Markov Chain Monte Carlo (MCMC) methods. Burn-in steps refer to discarding initial samples to avoid bias due to the starting point, while thinning steps involve selecting every nth sample to reduce autocorrelation between consecutive samples. These techniques help ensure convergence and improve the quality of the sampled distribution. In general, training completion criteria may include reaching a certain number of iterations or epochs, achieving a desired level of performance on a validation dataset, observing a plateau in the loss function, or meeting other predefined stopping conditions. Without additional context, it is not possible to specify the exact criteria used in this case.
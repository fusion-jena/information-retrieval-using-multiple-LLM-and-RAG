Before training a deep learning model, several preprocessing steps are typically involved to ensure the model performs optimally. These steps include:

1. Data Cleaning: This involves removing any irrelevant or redundant data from the dataset. It also includes handling missing values, outliers, and errors in the data.

2. Normalization or Scaling: Deep learning models often perform better when numerical input variables are scaled to a standard range. This is because different features might have different scales, and thus, their magnitudes could influence the learning process. Common techniques for normalization include Min-Max scaling, where all values are transformed to lie between 0 and 1, and Standardization, where each value is subtracted by the mean and divided by the standard deviation.

3. Feature Selection: Not all features contribute equally to the prediction task. Therefore, selecting relevant features can improve the modelâ€™s performance and reduce overfitting. Techniques for feature selection include correlation matrix, mutual information, chi-square test, etc.

4. Encoding Categorical Variables: Deep learning models work best with numerical inputs. Hence, categorical variables need to be converted into numerical form. One popular technique is one-hot encoding, where each category is represented as a new column filled with zeros except for a '1' in the row corresponding to the category.

5. Splitting Dataset: Before training, the dataset is usually split into three parts - training set, validation set, and testing set. The training set is used to train the model, the validation set is used to tune hyperparameters and prevent overfitting, and the testing set is used to evaluate the final model's performance.
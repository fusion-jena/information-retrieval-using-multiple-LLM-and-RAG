The provided context does not directly address the specific strategies employed to handle randomness in the deep learning pipeline, such as managing the impact of random seed values. However, some relevant points can still be extracted from the given text.

Deep learning models, like Convolutional Neural Networks (CNN), are widely adopted in remote sensing applications but face challenges when applied to diverse landscapes with significant terrain and spectral property variations. One issue mentioned in the context is the difficulty in validating these models, especially regarding their ability to generalize or perform transfer learning in new contexts.

To improve the reliability of these advanced models, researchers need to focus on developing robust model validation frameworks. In this regard, the context mentions the importance of conducting large cross-validation (CV) experiments with deep learning-based models. However, it also notes that such experiments might be computationally expensive and could lead to reduced training heterogeneity.

Although the context does not explicitly discuss handling randomness in the deep learning pipeline, one common strategy to manage the effect of random seeds is to fix them during experimentation. By fixing the random seed, researchers ensure reproducibility and consistency across multiple runs of the same model. Additionally, they can compare results between different models or configurations without being affected by random initialization differences.

In summary, while the provided context does not specifically mention strategies for handling randomness in the deep learning pipeline, it emphasizes the need for reliable model validation frameworks and suggests that fixing random seeds can contribute to achieving consistent and comparable results.
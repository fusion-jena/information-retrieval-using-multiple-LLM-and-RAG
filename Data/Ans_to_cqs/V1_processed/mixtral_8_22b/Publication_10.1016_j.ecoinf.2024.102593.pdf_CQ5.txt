In this study, the authors used the detailed-labelled NIPS4Bplus dataset (Bravo Sanchez et al., 2021) for training their Deep Neural Network (DNN) models. They also utilized another high-quality model called BirdNET for comparison purposes. To generate embeddings using the BirdNET model, they employed a randomly selected 40% sample of the labeled NIPS4Bplus dataset. It should be noted that BirdNET requires a 3-second sound input, which was extracted from the sound file, although it wasn't always feasible to extract a complete 3-second segment starting from the label start due to the varying lengths of the sound files in the dataset.

Additionally, the authors mentioned using the ResNet152-architecture, which is based on the ResNet152 model (He et al., 2015) originally designed for image classification. However, they didn't utilize any pre-trained weights for this purpose. Instead, they leveraged the Keras implementation of ResNet152 to acquire the model architecture, appended a dense layer, and trained the models from scratch with randomized initial weights. The input data consisted of 3-second sound frames transformed into mel spectrograms utilizing a fast Fourier transform window size of 1024, hop length of 896, and 128 mel filterbanks. Consequently, the resulting models contained approximately 58 million trainable parameters.

Therefore, the primary dataset used in this research is the NIPS4Bplus dataset, while the ResNet152 model was adapted for audio classification tasks without employing any well-known image datasets like MNIST, CIFAR, or ImageNet.
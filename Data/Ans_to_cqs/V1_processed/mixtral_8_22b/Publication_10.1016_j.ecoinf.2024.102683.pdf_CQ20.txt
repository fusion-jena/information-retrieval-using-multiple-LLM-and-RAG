The given text does not provide specific details about the hardware resources used for training the deep learning model. However, typically, deep learning models require substantial computational power due to their complex nature and large datasets. Common hardware resources used for training deep learning models include central processing units (CPUs), graphics processing units (GPUs), tensor processing units (TPUs), and sometimes even specialized hardware accelerators. CPUs are generally slower compared to GPUs and TPUs, while GPUs offer parallel computing capabilities making them suitable for handling matrix multiplications required in deep learning algorithms. On the other hand, TPUs are specifically designed for machine learning tasks and can further improve the speed and efficiency of training deep learning models. Therefore, without explicit mention in the provided text, one could assume that any of these hardware resources might have been utilized for training the deep learning model.
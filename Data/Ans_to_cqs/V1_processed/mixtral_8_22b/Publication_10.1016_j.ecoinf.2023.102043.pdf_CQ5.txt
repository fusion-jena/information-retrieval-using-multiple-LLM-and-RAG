The dataset used in the deep learning pipeline is not explicitly named but it can be inferred that the dataset consists of goat vocalizations. This dataset was collected over the course of a year and contains approximately 10 million sound events totaling more than 34,000 hours of recordings. The dataset was divided into three parts: training (70%), validation (10%), and testing (20%) sets. The dataset was also used for both traditional machine learning algorithms and transfer learning based on YAMNet. The YAMNet model was pre-trained on data coming from 521 classes and uses the MobileV1 convolution architecture. The audio data was processed by converting it to a monophonic channel, resampling at 16kHz, extracting the spectrogram using STFT magnitudes, calculating the Mel spectrogram, and finally applying the algorithm and passing its output to the YAMNet model for extracting embeddings.
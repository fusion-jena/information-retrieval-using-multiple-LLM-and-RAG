While the provided context does not directly address the specific strategies employed to handle randomness in the deep learning pipeline, it can be inferred that certain practices are commonly adopted to manage this issue. One common strategy is setting a fixed random seed value before training a deep learning model. This ensures that the results obtained are reproducible since the same sequence of pseudo-random numbers will be generated each time the code is executed. Another approach involves using techniques such as cross-validation or bootstrapping to estimate the performance of the model across different subsets of the dataset. These methods help reduce the impact of randomness introduced during the partitioning of datasets into training and validation sets. Additionally, employing ensemble methods, which combine multiple models trained on different subsets of the data, can also mitigate the effects of randomness. However, it should be noted that these strategies do not eliminate randomness entirely but rather aim to minimize its influence on the final outcomes.
The paper uses Stochastic Gradient Descent (SGD) as the optimization technique for the deep learning pipeline. This choice is motivated by its effectiveness in handling large datasets and improving convergence rates compared to other methods like batch gradient descent or online learning algorithms such as Adaptive Moment Estimation (Adam). Additionally, SGD has been shown to work well with momentum, which helps accelerate learning by taking into account past gradients when updating weights. In this case, a momentum value of 0.9 is used, indicating that previous weight updates have a significant influence on current ones. Furthermore, the authors apply default data augmentation techniques from YOLOv5 during training to increase the model's robustness against variations in input images. These techniques include random cropping, flipping, scaling, and color jittering, among others. Overall, these optimization strategies contribute to enhancing the performance and generalization capabilities of the insect detection model.
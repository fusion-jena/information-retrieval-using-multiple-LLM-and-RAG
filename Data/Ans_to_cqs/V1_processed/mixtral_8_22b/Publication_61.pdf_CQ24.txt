The given context does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, some implicit strategies can be inferred from the context. For instance, the use of the Adam optimizer (Kingma & Ba, 2015) adjusts the learning rate automatically, which might help mitigate the effects of random initialization or random weight updates during training. Additionally, setting hyperparameters like the learning rate, number of epochs, and batch size empirically suggests that these values may have been chosen through experimentation and fine-tuning rather than being randomly assigned. This process could potentially reduce the impact of randomness in the model performance. Furthermore, techniques like data augmentation mentioned in the context can introduce controlled variability in the dataset, helping the model generalize better and reducing its sensitivity to random factors. Nonetheless, without specific information regarding random seeds or their management, it cannot be definitively stated what strategies are explicitly employed to handle randomness in the deep learning pipeline.
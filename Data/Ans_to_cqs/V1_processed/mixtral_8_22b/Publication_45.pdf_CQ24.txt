The text does not explicitly mention any specific strategies employed to handle randomness in the deep learning pipeline, including the use of random seed values. However, it can be inferred from the context that the authors might have used some form of cross-validation or bootstrapping techniques to ensure robustness and reduce variability due to random initialization of weights in their deep learning models. Additionally, they compared the performance of their deep learning approach with other widely used classification algorithms like distributed random forests, gradient boosting machines, and generalized linear models implemented on the H2O platform. This comparison could also help mitigate the effects of randomness by providing alternative perspectives and benchmarks for evaluating the performance of the deep learning models.
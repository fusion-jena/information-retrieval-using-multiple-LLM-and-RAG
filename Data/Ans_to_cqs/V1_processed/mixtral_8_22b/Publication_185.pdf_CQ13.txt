In this study, several hyperparameters were utilized to train the deep learning models. For instance, the learning rate policy was set to "step", and the initial learning rate for all layers was set to 10^-2. This learning rate was then decreased by a factor of 10 every 69 iterations, and training was stopped after 6900 iterations. Additionally, the batch size was set to 69, and the momentum was set to 0.9. Furthermore, L2 weight decay was applied with a penalty multiplier set to 5 x 10^-4, and the dropout ratio was set to 0.5. Lastly, the number of units in the third fully connected layer (fc3) was adjusted based on the number of classes in the training data.
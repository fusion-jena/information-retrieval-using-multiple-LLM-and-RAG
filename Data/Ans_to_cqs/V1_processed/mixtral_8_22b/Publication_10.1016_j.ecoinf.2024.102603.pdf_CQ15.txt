The given context does not provide specific information about the optimization techniques used in the deep learning pipeline. However, common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam). These techniques help minimize the loss function during the training phase of deep learning models. SGD updates the parameters of the model based on the gradient of the error function with respect to the weights, while Adam combines the advantages of two other extensions of SGD, namely Root Mean Square Propagation (RMSProp) and Adaptive Gradient Algorithm (AdaGrad).
In the given context, the regularization method used to prevent overfitting in the deep learning pipeline is mentioned as dropout layers. Dropout is a technique where randomly selected neurons are ignored during training, which helps to avoid overfitting. In this study, the dropout rate was set to increase from one encoding block to the next by a step of 0.1 (starting from 0.1), and was found to work well for their specific setting. However, it should be noted that there are other regularization methods such as L2 regularization, which adds a penalty term to the loss function to discourage large weights, and early stopping, which stops the training process once the model starts to overfit. These methods were not explicitly mentioned in the provided context, but they are commonly used in deep learning pipelines to prevent overfitting.
The optimization technique applied in the deep learning pipeline is Stochastic Gradient Descent (SGD). In both the no-overfitting and overfitting settings, SGD is used as the optimization algorithm for Convolutional Neural Networks (CNNs). For the no-overfitting scenario, the learning rate is set to 0.0001, while for the overfitting case, the learning rate is slightly higher at 0.001. Momentum is also employed in the overfitting scenario, with a value of 0.9. Furthermore, a decay of 0.5 is applied every 512 iterations in the overfitting setup. Batch normalization and dropout techniques are utilized in the no-overfitting setting to mitigate overfitting, whereas these methods are not used in the overfitting scenario.
Based on the given context, there is no explicit mention of specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization methods include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust to small changes in the inputs. L2 regularization adds a penalty term to the loss function, encouraging smaller weights and thus reducing the complexity of the model. Other possible regularization methods could include early stopping or data augmentation, but without additional information, it is not possible to determine if these were used in this particular case.
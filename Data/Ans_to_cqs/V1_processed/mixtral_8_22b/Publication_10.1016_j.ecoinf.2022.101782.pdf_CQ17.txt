Based on the given information, there is no explicit mention of any specific regularization method used to prevent overfitting in the deep learning pipeline. However, some common techniques used in such scenarios include Dropout, L1/L2 Regularization, Early Stopping, and Batch Normalization. These methods help reduce overfitting by adding constraints to the learning capacity of the neural networks, thereby improving generalization performance. In this particular scenario, it might have been beneficial to use techniques like Dropout or L2 Regularization to avoid overfitting, but without explicit details about the implementation, it cannot be confirmed if they were indeed employed.
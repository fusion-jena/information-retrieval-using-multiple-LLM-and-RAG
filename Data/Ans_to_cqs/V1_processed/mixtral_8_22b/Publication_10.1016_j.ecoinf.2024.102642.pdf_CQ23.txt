The authors did not explicitly mention any specific techniques such as diverse datasets, cross-validation, or stratified splitting to ensure the generalizability of their deep learning models. However, they did maintain consistent hyperparameters across all training instances for each task (i.e., SEIO PBW vocalization and Dcall), which helps in maintaining consistency during the training process. They also mentioned using an early stopping method to prevent overfitting, which further contributes to ensuring the robustness and generalizability of the models. Additionally, the use of multiple annotators, including experts and novices, helped create diverse annotation sets, which likely improved the model's ability to handle various scenarios and increase its applicability to different situations.
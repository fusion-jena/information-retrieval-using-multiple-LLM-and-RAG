The datasets were divided into three subsets: train, validation, and test sets. Each subset had its own specific purpose during the development and evaluation of the deep learning model.

The train set was used to train the model by adjusting its parameters based on the input features and corresponding labels. This process involved feeding the model with examples from the train set multiple times, known as epochs, until it could accurately predict the output for unseen instances. In this case, the models were trained for 100 epochs using the Adam optimizer with default hyperparameters and a cosine decay scheduler for the learning rate.

The validation set served as a tool to fine-tune the model's performance during the training phase. By evaluating the model on the validation set after each epoch or a few epochs, it was possible to monitor its convergence and prevent overfitting. Overfitting occurs when the model becomes too complex and starts memorizing the training data instead of generalizing patterns. To avoid this issue, the macro-averaged f1-score on the validation set was employed as a metric to track the model's progress.

Lastly, the test set was utilized to assess the final performance of the fully trained model. Unlike the train and validation sets, which were seen by the model during the training phase, the test set contained entirely new and unseen examples. Therefore, the results obtained from testing the model on this dataset provided a more accurate estimation of how well it would perform in real-world scenarios.

In summary, the criteria used to split the data for deep learning model training included separating the available data into distinct subsets for training, validating, and testing purposes. These subsets played crucial roles in guiding the model's learning process, ensuring optimal performance, and providing reliable estimates of its effectiveness in practical applications.
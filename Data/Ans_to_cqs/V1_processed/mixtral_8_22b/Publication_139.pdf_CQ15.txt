Adam optimization technique is applied in the deep learning pipeline. It was chosen with an initial learning rate of 7. Early stopping was also employed to avoid overfitting. The model usually trained between 10 and 15 epochs before training stopped. Each epoch took approximately 1 minute on an NVIDIA GeForce RTX 2080 Ti GPU. Apart from the standard accuracy metric, the area under the curve was calculated using a Riemann sum for training purposes.
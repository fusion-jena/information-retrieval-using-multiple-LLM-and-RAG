Based on the given context, it does not directly discuss any techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, I will provide some insights about these techniques which might help you understand them better.

Stratified Splitting: It involves dividing the dataset into training and testing datasets by preserving the percentage of samples for each class. This technique ensures that both subsets maintain similar distributions of classes.

Oversampling: Oversampling increases the size of minority classes by duplicating examples from those classes. This method helps balance the dataset when dealing with imbalanced classes.

Undersampling: Undersampling reduces the size of majority classes by removing examples from those classes. Similar to oversampling, this approach aims to create a balanced dataset when there are imbalanced classes.

Diverse Data Collection: Collecting diverse data involves gathering various types of data to ensure that the machine learning models learn from multiple perspectives. This strategy improves the robustness and generalizability of the models.

In summary, the mentioned techniques like stratified splitting, oversampling, undersampling, and diverse data collection aim to mitigate data bias during preprocessing of the deep learning pipeline. These methods help improve the performance and reliability of machine learning models by addressing issues related to imbalance and diversity in the dataset.
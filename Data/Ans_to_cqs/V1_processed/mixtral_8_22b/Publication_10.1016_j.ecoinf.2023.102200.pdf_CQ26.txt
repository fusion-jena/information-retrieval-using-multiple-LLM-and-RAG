The papers provided do not directly discuss techniques to address data bias during preprocessing of the deep learning pipeline. However, some general approaches can be mentioned. Data bias is often addressed through various sampling strategies such as stratified splitting, oversampling, or undersampling. Stratified splitting involves dividing the dataset into subsets, ensuring each subset has roughly the same proportion of classes as the original dataset. This helps maintain class balance when partitioning the data into training and testing sets. Oversampling increases the number of instances in minority classes by duplicating examples or creating synthetic samples. Undersampling reduces the number of instances in majority classes by randomly removing examples or selecting only those closest to the decision boundary. Another approach is diverse data collection, which aims to gather more representative datasets by including underrepresented groups or scenarios. These techniques help mitigate data bias and improve model performance.
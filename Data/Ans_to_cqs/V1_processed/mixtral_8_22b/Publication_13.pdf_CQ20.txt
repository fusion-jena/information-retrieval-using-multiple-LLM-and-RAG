Unfortunately, the given context does not provide specific information about the hardware resources used for training the deep learning models mentioned in the papers. However, it can be generally assumed that GPU acceleration would have been utilized for training these models since deep learning algorithms often require significant computational power and benefit greatly from parallel processing capabilities offered by GPUs. Additionally, one of the cited works mentions using TensorFlow, a popular open-source library for machine intelligence that supports both CPU and GPU computing. This further suggests that GPU resources might have been employed during the training process. Nevertheless, without explicit details regarding the hardware setup, we cannot definitively state what type or how many GPUs, TPUs, or other hardware resources were used for training the deep learning models discussed in the provided context.
In the given context, there is no explicit mention of regularization methods used specifically in a deep learning pipeline such as dropout or L2 regularization. However, it does discuss some techniques used to avoid overfitting in the context of logistic regression and random forests.

For instance, the text mentions that MLE and Bayesian logistic regression have simple structures, which helps prevent overfitting. Additionally, feature selection is employed to improve model performance, which can be seen as a form of regularization. In the case of random forests, the ensemble nature of the algorithm inherently reduces overfitting.

While these methods help mitigate overfitting, they do not directly correspond to the specific regularization methods mentioned in the query, such as dropout or L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent complex co-adaptations on training data. On the other hand, L2 regularization adds a penalty term to the loss function based on the square of the magnitude of coefficients, encouraging smaller weights and simpler models. These techniques are commonly used in deep learning pipelines but are not explicitly mentioned in the provided context.
To monitor the model performance during training, the authors employed several strategies due to the limited amount of data available. Firstly, they utilized data augmentation techniques to artificially increase the size of the dataset. By applying various transformations to the existing samples, they aimed to improve the model's ability to generalize and enhance its accuracy on the test dataset. This approach also helped reduce the risk of overfitting, as the models were exposed to more variations, making them less likely to simply memorize the dataset.

Secondly, considering the scarcity of data, the authors adopted cross-validation to validate the statistical results and prevent overfitting. Cross-validation involves splitting the dataset into multiple subsets or 'crosses,' where each subset consists of a training set and an evaluation set. For each cross, a new model is trained, allowing for the creation of multiple models based on different versions of the dataset. After training all the models, their performances are evaluated and averaged to provide a comprehensive understanding of the model's capabilities on the dataset.

Lastly, the authors calculated advanced metrics like precision and recall to assess the model's confidence in predicting specific species. Precision measures the proportion of true positive predictions out of all positive predictions made by the model, while recall indicates the ratio of true positives identified correctly by the model out of all actual positives in the dataset. These metrics help determine the balance between recall and precision, represented through a precision-recall curve.
The text does not provide explicit details about the specific strategy implemented to monitor the model performance during training. However, it can be inferred from the given context that the researchers utilized various methods such as data augmentation, flattening convolution kernels, and adjusting dropout rates to optimize their models. They also employed different architectures like ResNet50 and VGG19+ResNet50, along with varying input bandwidths and bit depths. These techniques are commonly used to improve model accuracy and prevent overfitting, which indirectly helps monitor model performance during training. Additionally, they presented tables containing calculated results using these parameters, indicating that they likely evaluated the model's performance based on these outcomes. Nonetheless, without further clarification or additional context, it cannot be definitively stated what exact strategy was used to monitor the model performance during training.
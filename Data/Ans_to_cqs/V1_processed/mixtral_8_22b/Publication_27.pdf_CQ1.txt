In the deep learning pipeline, various methods are used to collect raw data depending on the specific application and requirements. Some common methods include:

1. Surveys or Questionnaires: These can be used to gather information directly from individuals about their opinions, preferences, behaviors, etc. This method is often employed in social sciences, marketing research, and other fields where human input is valuable.

2. Sensors: Various types of sensors such as cameras, microphones, temperature sensors, accelerometers, gyroscopes, etc., can be used to capture real-world data. For example, image classification tasks may require large amounts of labeled images collected using digital cameras or smartphone cameras. Similarly, speech recognition systems might need audio recordings captured by microphones.

3. Public Datasets: Many organizations and institutions provide publicly available datasets that researchers and developers can use for training and testing machine learning models. Examples include ImageNet for object recognition, MNIST for handwritten digit recognition, and CIFAR-10/100 for general image classification tasks.

4. Web Scraping: Data can also be extracted from websites using web scraping techniques. This involves automatically extracting structured data from HTML pages using specialized software tools or libraries like Beautiful Soup or Scrapy.

5. APIs: Application Programming Interfaces allow access to data stored in databases or generated by third-party services. For instance, Twitter API enables users to retrieve tweets based on specific criteria, while Google Maps API provides location-based data.

6. IoT Devices: Internet of Things devices generate vast amounts of data through connected sensors and actuators. This data can then be processed and analyzed using deep learning algorithms for applications such as predictive maintenance, anomaly detection, and demand forecasting.
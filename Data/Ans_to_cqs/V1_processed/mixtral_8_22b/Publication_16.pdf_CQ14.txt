Unfortunately, based on the given information, it does not explicitly state how the hyperparameters of the model were optimized. There is no mention of techniques such as grid search or random search being employed. However, some details about the configuration of the model are mentioned. For instance, the Multilayer Perceptron (MLP) uses a filter size of 2, backpropagation as the learning algorithm, and Adam as the optimizer. Additionally, a dropout value of 50% is applied in the first three layers to enhance generalization and mitigate overfitting. These specifications suggest that some form of optimization might have occurred, but without explicit confirmation, it cannot be definitively stated which method was utilized.
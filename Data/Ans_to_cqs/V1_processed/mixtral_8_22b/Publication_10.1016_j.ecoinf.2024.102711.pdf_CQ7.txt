In the given context, the data is split into three parts for deep learning model training: the training dataset, the tuning dataset, and the evaluation dataset. The training dataset is used to train the model, while the tuning dataset is used to adjust the hyperparameters of the model during the training process. The evaluation dataset is used to assess the final performance of the model after training and tuning.

To fine-tune hyperparameters in data-driven models, the development dataset (comprising all data records except the evaluation data records) is divided into two categories: the training dataset and the tuning dataset. Due to the size constraints of the dataset used in the analysis and the need to utilize all development data records in training to enhance the model’s generalization ability, k-fold cross-validation is chosen as the preferred approach for hyperparameter tuning. In this technique, the development dataset is evenly divided into k groups. During each iteration, one group called the tuning dataset is excluded from the training process. The model is then developed using the remaining dataset (i.e., the training dataset), and its performance is evaluated using the tuning dataset. The average performance of the k models developed on the validation groups represents the performance of the machine learning technique for the selected hyperparameters. For this study, a value of 5 is assumed for k. Grid search is employed to evaluate the data-driven model’s performance using each combination of predefined hyperparameters and identify the best hyperparameters. The maximum depth of the tree, the number of trees in the ensemble model, and the learning rate (which shows how fast the model learns) are tuned in this study as the influential hyperparameters.
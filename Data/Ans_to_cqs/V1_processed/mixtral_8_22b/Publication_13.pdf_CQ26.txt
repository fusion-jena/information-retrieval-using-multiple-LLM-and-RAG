Based on the given context, there is no direct mention of specific techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, one technique mentioned is the use of Domain Adversarial Training of Neural Networks (DANN) which can be considered as a method to handle data bias.

DANN is a technique introduced by Ganin et al. (2016) that aims to improve the generalization ability of deep learning models when dealing with different domains or distributions of data. In this approach, a domain classifier is added to the model architecture, which tries to distinguish between the source and target domains while the main task classifier tries to minimize the classification error. By doing so, the model learns to extract domain invariant features that can help reduce the impact of data bias caused by differences in data distribution between the source and target domains.

Although not explicitly stated in the context, other common techniques to address data bias during preprocessing include stratified sampling, where the data is split into training and testing sets based on the proportion of classes present in the original dataset; oversampling, where samples from underrepresented classes are duplicated to balance the dataset; undersampling, where samples from overrepresented classes are removed to balance the dataset; and diverse data collection, where efforts are made to collect more representative data from various sources to reduce bias.
Unfortunately, there isn't enough specific information in the given context to determine exactly how the hyperparameters of the models were optimized. However, it does mention some optimization techniques such as meta-learning, transductive learning, and adaptive mechanisms like the squeeze/excitation block. These techniques could potentially involve various forms of hyperparameter tuning, including grid search or random search. For instance, in the case of meta-learning, one might use a form of gradient descent to update the parameters of the model during training, while in transductive learning, the model may be adapted using the information from the unlabelled examples in the query set. Similarly, the adaptive mechanisms mentioned could involve adjusting certain hyperparameters dynamically based on the input data. Without more detailed information about the specific models and their architectures, however, it's not possible to provide a definitive answer regarding the exact methods used for hyperparameter optimization.
To handle randomness in the deep learning pipeline, several strategies can be employed. One common approach is to use fixed random seeds during model training and evaluation. This ensures reproducibility of results across multiple runs of the same experiment. Another strategy is to perform multiple runs with different random seeds and report average performance metrics along with their standard deviations or confidence intervals. This helps to account for variability due to random initialization and stochastic optimization algorithms used in deep learning models. Additionally, techniques such as cross-validation and bootstrapping can be used to further reduce the impact of randomness on model performance estimates. However, it should be noted that these strategies do not eliminate all sources of randomness in the deep learning pipeline, but rather help to mitigate its effects on experimental outcomes.

Based on the given context, there is no information about handling randomness in the deep learning pipeline. Therefore, I cannot provide a helpful answer to the query.
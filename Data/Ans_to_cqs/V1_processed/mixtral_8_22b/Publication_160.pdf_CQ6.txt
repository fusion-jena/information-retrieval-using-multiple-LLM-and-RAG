The given context does not explicitly mention any specific preprocessing steps taken before training the deep learning models. However, it can be inferred that some common practices were likely employed. For instance, image data might have been resized to fit the required input sizes for various ResNet architectures mentioned in the table. Additionally, techniques such as normalization or scaling could have been applied to ensure consistent feature scales across images. Data augmentation methods like rotation, flipping, or cropping might also have been utilized to increase dataset diversity and prevent overfitting. Lastly, the text mentions replacing the top layer of the pre-trained ResNet152 with new layers suitable for the specific task, indicating that transfer learning was implemented as part of the process.
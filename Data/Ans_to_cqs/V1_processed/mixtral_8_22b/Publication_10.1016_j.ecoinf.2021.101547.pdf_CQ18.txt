During the training stage, the strategy implemented to monitor the model performance involves performing a new round of learning on a mini-batch of images. This focuses relatively more on images with higher loss scores, which helps the network adjust its internal parameters accordingly. After this, it proceeds to the next training iteration with a new mini-batch of images. A full cycle through the entire set of training images is called an 'epoch'. The number of iterations per epoch equals the total number of training images divided by the mini-batch size.

The mini-batch size can be adjusted based on the dataset and the resources available. It should not be too small, as it might not provide enough image data for the network to learn effectively from each class. On the other hand, if it's too large, the network could potentially overfit to the training images. Due to the high computational complexity of deep learning processes, the mini-batch size is ultimately limited by the amount of memory available to the GPU.

In summary, the strategy to monitor the model performance during training includes focusing on images with higher loss scores, updating the network's internal parameters, and adjusting the mini-batch size according to the dataset and available resources.
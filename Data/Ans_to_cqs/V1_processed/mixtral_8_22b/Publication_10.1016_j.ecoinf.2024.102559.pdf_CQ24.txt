The text does not provide information on handling randomness in the deep learning pipeline. However, common strategies include fixing the random seed value, using deterministic algorithms where possible, and employing techniques like cross-validation and ensemble methods to reduce variability caused by randomness.

Based on the provided context, I can infer that the authors did not explicitly discuss strategies for handling randomness in their deep learning pipeline. Nevertheless, some general strategies for managing randomness in deep learning pipelines include:

1. Fixing the random seed value: By setting a fixed random seed value, one can ensure reproducibility in experiments involving random processes, such as weight initialization, batch selection, and dropout regularization. This allows researchers to compare different architectures, hyperparameters, and optimization methods more reliably.

2. Using deterministic algorithms: Wherever possible, opting for deterministic algorithms instead of stochastic ones can help eliminate sources of randomness. For instance, using a deterministic optimizer like gradient descent instead of a stochastic variant like Adam can lead to more consistent results.

3. Cross-validation and ensemble methods: To mitigate the impact of randomness on performance estimates, researchers can employ techniques like k-fold cross-validation and ensemble methods. These approaches involve partitioning the dataset into subsets, training multiple models, and aggregating their predictions. This helps reduce variance due to random factors and provides a more robust estimate of model performance.
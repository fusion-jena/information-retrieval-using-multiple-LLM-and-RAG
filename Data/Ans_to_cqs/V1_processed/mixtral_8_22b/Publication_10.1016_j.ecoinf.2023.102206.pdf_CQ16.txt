Based on the given context, there isn't any explicit mention of specific criteria used to determine when training is complete. However, we can infer that the authors might use common machine learning practices such as monitoring the validation loss or accuracy to decide when to stop training.

In general, training is often stopped when the model performance on the validation set stops improving significantly or starts degrading. This could be indicated by a plateau or increase in the validation loss, or a decrease in validation accuracy. Additionally, early stopping techniques may be employed to prevent overfitting, where training is halted once the validation error begins to rise even though the training error continues to fall.

However, since these details aren't explicitly mentioned in the provided context, it would be best to refer to the original paper or related literature for more precise information regarding the completion criteria for training in this particular case.
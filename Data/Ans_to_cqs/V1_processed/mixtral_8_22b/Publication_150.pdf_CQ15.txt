In the given context, the Stochastic Gradient Descent (SGD) optimizer is mentioned as the optimization technique applied in the deep learning pipeline. The initial learning rate for all models, except MnasNet, was set at 0.01. For MnasNet, the initial learning rate was set at 0.1. The learning rate was then reduced by a factor of 10 after every 30 epochs. Batch normalization was also used in the models. These models were trained for 150 epochs using Nvidia Tesla K80 or P100 GPUs. However, no mention of the Adam optimizer is made in the provided context.
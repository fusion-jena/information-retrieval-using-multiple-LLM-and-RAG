The authors utilized diverse datasets such as GBIF Natural and GBIF Preserved for their study. They used a ResNet50 architecture with weights initialized from ImageNet and employed a Feature Pyramid Network (FPN) to improve the network output resolution. To train the network, they used a combination of mean scaled absolute error as regression loss, a batch size of 1, an input image resolution of 512 x 512 pixels, and a learning rate of 10^-5. Data augmentation techniques like horizontal flipping were applied during the training process. However, there is no explicit mention of specific methods like cross-validation or stratified splitting being used to ensure the generalizability of the deep learning model.
Unfortunately, there isn't enough information available in the given context to provide a detailed explanation about how the trained deep learning model was deployed. However, we do have some clues regarding the training process of the model.

From the context, it appears that the authors use a VGG-style network for the feature learning part of their proposed model. They train this network using Adam optimizer with a learning rate of 10^-4, categorical cross-entropy as the loss function, batch size of 64 samples, and 200 epochs.

While the deployment process is not explicitly mentioned, one common practice when working with deep learning models is to serialize or save the trained model after completion of the training phase. This serialized model can then be loaded into another environment or platform for further testing or production usage.

In terms of platform selection, popular choices include cloud platforms like AWS, Google Cloud, Azure, etc., where users can leverage prebuilt machine learning services or custom containers to host their models. Alternatively, they could also choose edge devices such as NVIDIA Jetson boards, Intel Neural Compute Stick, etc., depending upon the specific requirements of their application.

However, without more specific information about the author's approach, these suggestions remain speculative. It would be helpful if additional context were provided to give a more accurate response.
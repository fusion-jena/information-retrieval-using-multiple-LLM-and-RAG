Based on the given context, there isn't any explicit mention of specific regularization methods such as dropout or L2 regularization used to prevent overfitting in the deep learning pipeline. However, some techniques mentioned in the text could potentially help mitigate overfitting indirectly. For instance, data augmentation techniques like rotation, noise, and flip were employed to increase the size of the dataset by six times. This approach helps improve the model's ability to generalize better and makes the CNN classification more robust. Additionally, the learning rate was reduced using an exponential decay parameter of 0.5 by an SGD scheduler after 50,000 iterations. This step might also contribute to preventing overfitting since reducing the learning rate can stabilize the optimization process and avoid overshooting the optimal solution. Nevertheless, without further details about the implementation of the models, it cannot be confirmed if additional regularization methods were utilized.
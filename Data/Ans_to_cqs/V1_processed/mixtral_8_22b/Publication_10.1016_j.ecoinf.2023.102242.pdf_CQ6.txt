Before training a deep learning model, several preprocessing steps are typically involved. These steps aim to prepare the dataset for efficient and accurate model training. Some common preprocessing techniques include:

1. Normalization: This involves rescaling the input features so they fall within a specific range, often between 0 and 1 or -1 and 1. Normalization helps prevent any single feature from dominating the others due to its scale, ensuring all features contribute equally during training.

2. Scaling: Similar to normalization, scaling adjusts the values of numeric columns in the dataset to a standard scale without distorting differences in the ranges of values or losing information. Common methods include Min-Max scaling and Standardization (Z-score normalization).

3. Cleaning: Data cleaning involves identifying and handling missing, incorrect, or inconsistent data points. Techniques such as imputation (filling missing values based on existing data patterns) and outlier detection/removal help maintain data integrity and improve model performance.

4. Feature Selection/Extraction: Selecting relevant features or extracting new ones can enhance model performance by focusing on important aspects of the data. Methods like Principal Component Analysis (PCA) reduce dimensionality while preserving essential information.

5. Splitting Dataset: Dividing the dataset into separate sets for training, validation, and testing ensures the model generalizes well to unseen data. A common practice is using 70-80% of the data for training, 10-15% for validation, and 10-15% for testing.

In the given context, it appears that the authors have used feature optimization tools to address challenges related to training sample representation and quantity. However, the exact preprocessing steps taken before training their deep learning model aren't explicitly mentioned. Based on common practices, one could infer that some combination of normalization, scaling, cleaning, feature selection/extraction, and splitting datasets might have been applied.
The deep learning pipeline employed various regularization techniques to prevent overfitting. One such method is dropout, which was implemented through dropout layers added in an attempt to improve generalization. Dropout randomly sets a fraction rate of input units to 0 at each update during training time, helping to avoid overfitting. Another technique used is l2 weight regularization, also known as Ridge Regression, which adds a penalty equivalent to the square of the magnitude of coefficients to the loss function. This helps in reducing model complexity and improving its ability to generalize. In this case, l2 weight regularization was used with a factor of 0.0001. Additionally, batch normalization was applied after all convolution or dense layers, except for the final output. Batch normalization is another form of regularization that reduces internal covariate shift, allowing faster and more stable training.
The context does not provide specific information about how the hyperparameters of the model are optimized. However, one of the cited papers, Sun et al. (2020), mentions using a Bayesian algorithm for hyperparameter optimization in a random forest model. This suggests that Bayesian optimization could be used for hyperparameter tuning in machine learning models. Other common methods for hyperparameter optimization include grid search and random search. Grid search involves testing all possible combinations of hyperparameters within a specified range, while random search randomly selects combinations from the same range. Both methods aim to find the optimal combination of hyperparameters that minimizes the loss function and improves the accuracy of the model. Without further context, it cannot be determined which method was used specifically for the XGBoost model mentioned in the given text.
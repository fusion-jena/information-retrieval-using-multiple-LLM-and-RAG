The deep learning model uses the EfficientNet B0 network architecture, which was pretrained on ImageNet dataset. During transfer learning, only the weights of the final dense classification layers were updated while keeping the rest of the network frozen. The model was trained using the Adam optimizer with a learning rate of 0.001, a decay factor of 0.75, and a step size of 2. However, when fine-tuning the model with Gulf of Mexico data, a cyclical learning rate of 0.0004 was used instead. Additionally, a dropout rate of 0.2 was applied during fine-tuning along with DropConnect.
The papers cited do not provide specific information about the regularization methods used in the deep learning pipelines they describe. However, some common regularization techniques used to prevent overfitting in deep learning include dropout, L1 and L2 regularization, early stopping, data augmentation, and batch normalization. Dropout randomly sets a fraction rate of input units to 0 at each update during training time, helping to prevent overfitting. L1 and L2 regularization add penalties to the loss function based on the weights of the network, encouraging smaller weight values and simpler models. Early stopping ends the training process before convergence when performance on a validation set stops improving. Data augmentation generates additional training samples by applying random transformations to existing ones. Batch normalization standardizes the inputs to each layer, reducing internal covariate shift and speeding up training. These techniques can be combined or used individually depending on the requirements of the specific application.
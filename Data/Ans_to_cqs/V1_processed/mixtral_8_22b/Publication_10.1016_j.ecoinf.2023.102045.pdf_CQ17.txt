The provided context does not contain specific information regarding the regularization methods used to prevent overfitting in the deep learning pipeline. However, some commonly known regularization techniques include Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to avoid overfitting by preventing complex co-adaptations on training data.

L2 Regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the magnitude of all weights in the network. This encourages smaller weights, leading to simpler models and reduced risk of overfitting.
Based on the given information, there isn't a direct mention of a strategy implemented to monitor the model performance during training. However, we can infer from the use of different AI techniques such as traditional linear programming, nonlinear programming, dynamic programming, modern genetic algorithms (GA), particle swarm optimization algorithms, and deep learning, that some form of monitoring would be necessary to ensure the models are performing optimally.

For instance, in the case of genetic algorithms, one common approach to monitor performance is through the use of fitness functions. These functions evaluate how well a solution solves the problem, which can help determine if the model is improving over time. Similarly, in deep learning, metrics like accuracy, precision, recall, F1 score, etc., could be used to track the model's performance during training.

In the context of this study, the authors seem to have evaluated their model by calculating various indices such as source coverage area, source coverage rate, source connectivity rate, fragmentation index, IIC, and PC. They also compared different scenarios with varying initial populations and minimum fitness values to find the optimal ESA. This suggests that they likely monitored the model's performance during training by evaluating its output against these indices.
The authors do not explicitly mention any specific strategies employed to handle randomness in the deep learning pipeline, such as setting a fixed random seed value. However, they discuss using default parameters for their models, including a batch size of 32 images and a cross-entropy loss function. They also employ the Adam learning rate optimizer, which automatically adjusts the initial learning rate during training. To save time and improve automation, they enable early stopping when the validation error does not decrease for 10 epochs. This helps determine the appropriate number of epochs for each dataset. Additionally, they unfreeze and replace the final fully connected (FC) layers of the VGG16 network to reinitialize the weights for training, reducing the number of output nodes in FC3 to match the three habitat classes being classified.
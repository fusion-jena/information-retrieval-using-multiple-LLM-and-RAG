Regularization methods are techniques used to prevent overfitting in deep learning pipelines. Two common regularization methods mentioned in the provided context are dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to reduce interdependent learning amongst them. This leads to a more robust model that can perform well even when some input features are missing. On the other hand, L2 regularization adds a penalty term to the loss function, which discourages large weights in the model. By doing so, it encourages smaller and simpler models that are less likely to overfit the training data. Other regularization methods not explicitly mentioned in the provided context include early stopping, data augmentation, and batch normalization. Early stopping involves halting the training process before the model starts to overfit. Data augmentation increases the size of the training dataset by applying random transformations to existing examples, while batch normalization standardizes the inputs to each layer, reducing internal covariate shift and improving convergence speed.
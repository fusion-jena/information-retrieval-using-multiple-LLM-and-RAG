Based on the given context, there is no explicit mention of any specific optimization technique such as Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), or others being used in the deep learning pipeline. However, it can be inferred that some form of optimization technique would have been employed during the training process of the deep learning models mentioned. This is because optimization algorithms are essential components of deep learning pipelines, helping to minimize the loss function by adjusting the weights and biases of the neural networks. Without this information explicitly stated, it cannot be definitively said which optimization technique was utilized in this case.
While the given text does not explicitly mention specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam), it does discuss the use of optimization processes in Deep Learning (DL) networks. During the training phase of DL networks, the parameters of the model are adjusted based on an optimization process. This process involves minimizing the loss function, which measures how well the model performs during training. Common optimization techniques used in DL include SGD, Adam, RMSprop, and others. However, without explicit information about the optimization technique used in the mentioned DL approaches like ResNet, it cannot be definitively stated which one was employed.
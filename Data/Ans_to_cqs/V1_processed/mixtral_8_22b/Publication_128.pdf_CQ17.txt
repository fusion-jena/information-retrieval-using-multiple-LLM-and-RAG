The paper does not explicitly mention specific regularization methods such as dropout or L2 regularization. However, there are some indications of techniques used to prevent overfitting. One method mentioned is the use of an optimized rectified linear activation, Leaky ReLU, which can help in preventing overfitting by introducing nonlinearity and reducing the risk of saturation. Additionally, the paper mentions adjusting the number of training epochs based on the depth of the neural network, with fewer epochs for shallower networks to avoid overfitting. Lastly, the paper discusses the use of batch normalization before training, which can also act as a form of regularization by reducing internal covariate shift and improving the stability of the learning process.
The provided context does not explicitly mention any specific optimization techniques used in the deep learning pipeline, such as Stochastic Gradient Descent (SGD) or Adam. However, it can be inferred that optimization techniques are likely employed in the development of neural networks mentioned throughout the text. For instance, when discussing bespoke neural networks for species sets, training these networks would require an optimization technique to minimize the loss function and update the network's parameters. Similarly, when discussing low power neural networks in hardware, the paper referenced by Zhu et al. (2019) titled "Searching for MobileNetV3" might have utilized optimization techniques during its search for efficient architectures. Nevertheless, without explicit information regarding the optimization techniques used in each case, one cannot definitively state which methods were implemented.
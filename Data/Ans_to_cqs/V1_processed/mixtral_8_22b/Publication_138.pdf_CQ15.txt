Based on the given information, there is no explicit mention of any specific optimization technique used in the deep learning pipeline such as Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), or others. However, some general training settings are mentioned, like the number of epochs, learning rate drop factor, learning rate drop period, and batch size. These hyperparameters are crucial for optimizing the model during training. For instance, the learning rate determines how quickly or slowly the weights are updated, while the batch size affects the efficiency and stability of the training process. Therefore, although the exact optimization algorithm isn't specified, these settings suggest that some form of optimization was employed to train the models effectively.
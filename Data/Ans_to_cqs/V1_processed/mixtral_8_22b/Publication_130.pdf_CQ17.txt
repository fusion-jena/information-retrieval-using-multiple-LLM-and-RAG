The authors employed several regularization techniques to prevent overfitting in their deep learning pipeline. One such method was the use of dropout layers, which randomly ignore units of the Convolutional Neural Network (CNN) during the training process. This technique helps limit overfitting by preventing complex co-adaptations on the training data. Additionally, they avoided freezing any layers of the network, allowing all layers to update their weights during the training process. This decision was made to ensure that the model could learn key features necessary for performing the classification task, despite the relatively small size of their training datasets. Furthermore, oversampling was applied to the training dataset to balance the classes, as an imbalanced training dataset can lead to over-generalization for the classes in majority. Lastly, a Structural Similarity Index Measure (SSIM) was utilized to create a validation dataset with maximized pairwise dissimilarity among images, ensuring diversity within the dataset and reducing the risk of overfitting.
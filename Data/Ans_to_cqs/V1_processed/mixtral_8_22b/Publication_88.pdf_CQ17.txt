In the given context, there is only one explicit mention of a regularization method used to prevent overfitting in the deep learning pipeline. That method is dropout, which is applied with a rate of 0.2 in the final fully connected layer of the EF network architecture. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust and less sensitive to the specific weights of any individual neuron.

There is also implicit use of L2 regularization through the use of the Adam optimizer with weight decay. Weight decay is a form of L2 regularization that adds a penalty term to the loss function proportional to the square of the magnitude of all the weights in the model. This encourages smaller weights, which can help reduce overfitting. However, it should be noted that while these techniques are mentioned in the context of the EF network, it is not explicitly stated whether or not they were also used for the SN model.
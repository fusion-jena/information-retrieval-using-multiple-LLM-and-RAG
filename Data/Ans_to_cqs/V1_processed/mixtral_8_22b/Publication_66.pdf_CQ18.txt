The study uses leave-one-out cross-validation to evaluate the performance of the optimum models for each species. This validation technique involves leaving out one observation from the dataset, fitting the model to the remaining observations, and then testing the fitted model on the left-out observation. This process is repeated until every observation has been left out once. The results of this validation are presented in box plots showing the mean, positive predicted value (PPV), negative predicted value (NPV), and area under the curve (AUC) for both training and validation datasets. Additionally, alternative prediction methods such as hidden Markov models (HMMs) and na√Øve Bayes classifiers were implemented and compared to the deep learning models. However, these methods did not perform as well as the deep learning models in terms of specificity, sensitivity, PPV, and NPV.
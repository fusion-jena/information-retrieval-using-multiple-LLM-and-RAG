For splitting the data for deep learning model training, one commonly used technique is cross-validation. Cross-validation is an iterative method for calculating the expected value of a particular measure. It splits the dataset into K equally-sized parts, known as folds. In each iteration, different (K - 1)-folds are used for training, while the remaining fold is used for testing. The overall measure of its performance is the average of the measures of the individual iterations. This ensures that every instance will be used both for training and testing. A variant of cross-validation is stratified cross-validation, where in each fold, a balanced number of instances for each class is selected.

When performing binary classification tasks, accuracy is often used as a suitable measure for model performance evaluation. However, when dealing with imbalanced datasets, accuracy might not be the best metric since it only considers the correctly predicted samples out of the total samples. Therefore, other metrics such as precision, recall, or F-measure should be considered instead. These metrics take into account the true positives, false negatives, and false positives, providing a more comprehensive view of the model's performance.

In summary, the criteria used to split the data for deep learning model training typically involve techniques like cross-validation or stratified cross-validation. When choosing the appropriate performance metric, factors such as dataset balance and the specific requirements of the task must be taken into consideration.
The provided context does not explicitly mention any specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it is common practice in deep learning to use techniques such as dropout and L2 regularization to mitigate overfitting. Dropout involves randomly dropping out neurons during training, while L2 regularization adds a penalty term to the loss function based on the sum of squares of weights. These methods help reduce complexity and improve generalization of models. It should also be noted that Benkendorf and Hawkins (2020) analyzed the effect of sample size and network depth on deep learning approaches, suggesting that increasing the number of hidden layers or training recordings could potentially improve model performance without causing overfitting.
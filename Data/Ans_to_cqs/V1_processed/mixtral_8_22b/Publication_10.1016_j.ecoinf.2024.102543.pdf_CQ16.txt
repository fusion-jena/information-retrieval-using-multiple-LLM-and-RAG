In the given context, there isn't explicit information about specific criteria used to determine when training is complete. However, based on common machine learning practices, one could infer that several factors might be considered to decide when to stop training. These include monitoring the validation loss or error rate, observing the model's performance on the validation dataset, and checking if the model has reached convergence.

One commonly used criterion is the validation loss plateau. During training, both the training and validation losses decrease initially. As the model starts fitting better to the training data, the training loss continues decreasing while the validation loss may start to plateau or even increase slightly. When the validation loss stops improving significantly after multiple epochs, it indicates that further training may not lead to substantial improvements in the model's performance. At this point, training can be stopped to avoid overfitting the model to the training data.

Additionally, another factor to consider is the model's performance on the validation dataset. By periodically evaluating the model's precision, recall, mean average precision (mAP), etc., on the validation set during training, one can observe whether the model is improving or not. Once the desired performance levels have been achieved, or the improvement becomes negligible, training can be halted.

Lastly, convergence is another important aspect to monitor. Convergence occurs when the weights of the model stabilize, and further training does not result in significant changes to them. Monitoring the change in weights or gradients can help identify when the model has converged, indicating that training should be stopped.
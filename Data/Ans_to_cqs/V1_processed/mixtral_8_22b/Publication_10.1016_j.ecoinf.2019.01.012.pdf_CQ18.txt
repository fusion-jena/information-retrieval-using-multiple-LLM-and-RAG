The authors monitored the model performance during training by evaluating the learning curves for loss and accuracy over 90 epochs. They also employed a validation set during training for fine-tuning hyperparameters and model selection. After finding the best model setup, they assessed its prediction error once again on the test set. Additionally, they used dropout and data augmentation techniques on the training data only, causing the training set to contain more difficult tiles than the validation set. This resulted in higher validation accuracy compared to training accuracy, which is a common effect when employing such techniques.
Before training a deep learning model, several preprocessing steps can be taken to improve its performance. These may include:

1. Data Cleaning: This involves removing or correcting erroneous data points, handling missing values, and dealing with outliers.

2. Normalization/Standardization: This is done to bring all features to the same level of magnitude, which helps the model converge faster during training. Common methods include Min-Max Scaling, Standardization (Z-score), and Normalization.

3. Feature Selection: This involves selecting relevant features that contribute most to the output variable. Techniques like correlation matrix, mutual information gain, etc., can be used.

4. Data Augmentation: This technique is often applied when working with image data. It increases the amount of training data by creating modified versions of the original data through rotation, zooming, flipping, etc.

5. Encoding Categorical Variables: Since deep learning models cannot handle categorical data directly, they need to be converted into numerical form using techniques like label encoding, one-hot encoding, etc.

6. Splitting Dataset: The dataset is usually split into three parts - training set, validation set, and test set. The training set is used to train the model, the validation set is used to tune hyperparameters, and the test set is used to evaluate the final model.
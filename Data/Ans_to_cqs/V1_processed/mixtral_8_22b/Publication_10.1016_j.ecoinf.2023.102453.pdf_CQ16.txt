The context does not provide explicit information about the specific criteria used to determine when training is completed. However, it mentions concepts related to model training and hyperparameter optimization, which often involve monitoring certain metrics such as validation loss or accuracy to decide when to stop training.

In general, some commonly used criteria to determine when training is complete include:

1. Validation Loss Plateau: When the validation loss stops decreasing significantly over several epochs, this may indicate that the model has learned most of what it can learn from the data, and further training might lead to overfitting.

2. Early Stopping: This technique involves stopping the training process if the validation metric (loss or accuracy) doesn't improve after a specified number of epochs. This helps prevent overfitting and saves computational resources.

3. Maximum Number of Epochs: Training can be stopped once a maximum number of epochs have been reached. This ensures that the training process doesn't continue indefinitely even if there is no improvement in the validation metric.

4. Learning Rate Scheduling: Adaptive learning rates can help determine when training should be stopped. For example, reducing the learning rate when the validation loss plateaus can sometimes allow the model to find a better minimum. Conversely, increasing the learning rate when the validation loss stagnates can help escape local minima.

While the context does not directly address the query, these general principles can be applied to determine when training is complete based on various factors like validation loss, early stopping, maximum epochs, and learning rate scheduling.
Based on the given context, it appears that the authors employ several strategies to handle randomness in their deep learning pipeline. One such strategy is the use of random cropping of sub-images from the original satellite images during the generation of training data. This technique introduces variability in the input data, which can help improve the model's ability to generalize to new, unseen data. Additionally, the authors mention using a Python generator to create a stream of almost never repeated sub-images, which also contributes to introducing randomness in the training process.

Regarding handling randomness related to the initialization of weights or other parameters in the deep learning models, the authors do not explicitly discuss this aspect in the provided context. However, common practices in the field include setting a fixed random seed value before running any code involving random processes, such as weight initialization or data shuffling. This ensures reproducibility of results across different runs of the same experiment. Another popular method to reduce the impact of random initializations is to perform multiple runs with different seeds and average the results.

Furthermore, techniques like dropout regularization, mentioned in reference [59], can be used to mitigate the effects of randomness in the training process. Dropout involves randomly dropping out neurons during training, which helps prevent overfitting and improves the model's performance on unseen data. Lastly, batch normalization, discussed in reference [58], is another technique that can help stabilize the learning process by normalizing layer inputs and reducing internal covariate shift.
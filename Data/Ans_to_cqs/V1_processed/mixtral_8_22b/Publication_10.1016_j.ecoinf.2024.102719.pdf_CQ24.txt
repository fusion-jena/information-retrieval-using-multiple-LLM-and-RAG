Strategies to handle randomness in the deep learning pipeline include setting fixed seeds for reproducibility, ensemble methods, cross-validation techniques, and Bayesian optimization. Setting fixed seeds ensures consistent results across different runs of the same code. Ensemble methods combine multiple models trained with different initializations or architectures to reduce variance and improve performance. Cross-validation techniques evaluate model performance on various subsets of the dataset to ensure robustness against random fluctuations. Bayesian optimization optimizes hyperparameters by considering uncertainty in the search space, which can help mitigate the impact of random initialization. However, it should be noted that these strategies do not eliminate randomness but rather manage its effects on model training and evaluation.

Reference(s):
[1] Breiman, L. (2001). Random forests. Mach. Learn. 45, 5–32. <https://doi.org/10.1007/> 978-3-030-62008-0\_35.

[2] Chen, T., & Guestrin, C. (2016). XGBoost: a scalable tree boosting system. In: Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min. 13-17-Augu, pp. 785–794. <https://doi.org/10.1145/2939672.2939785>.

[3] Crisci, C., Ghattas, B., & Perera, G. (2012). A review of supervised machine learning algorithms and their applications to ecological data. Ecol. Model. 240, 122–133. <https://doi.org/10.1016/j.ecolmodel.2012.03.001>.

[4] Van Der Valk, D., & Picek, S. (2019). Bias-variance decomposition in machine learning- based side-channel analysis. Cryptol. ePrint Arch. 1–27.
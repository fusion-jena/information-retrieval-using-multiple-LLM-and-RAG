The strategy implemented to monitor the model performance during training involves several steps. In the case of gradient boosting models, the process includes adjusting weights in the training dataset iteratively. For artificial neural networks (ANNs), specifically the Multi-Layer Perceptron Neural Network (MLPNN), the strategy involves assigning initial arbitrary connection weights and forward-feeding inputs from the input to the hidden layer. The hidden neurons multiply inputs by weights, sum products, and process sums through a transfer function. The results propagate to the output layer, where output values are compared to expected values for error computation. An iterative error back-propagation adjusts connection weights until reaching a target minimal error. The network is then tested to determine the optimal learning rate, momentum, and training iterations. The trained network is subsequently used for feed-forward predictions on continuous spatial data. Additionally, all hyperparameters were tuned with a grid search method.
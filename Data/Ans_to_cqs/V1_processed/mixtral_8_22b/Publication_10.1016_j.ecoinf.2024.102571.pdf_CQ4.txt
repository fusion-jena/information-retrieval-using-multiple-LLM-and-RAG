Data augmentation is a strategy used in machine learning and deep learning to increase the amount of training data by applying random transformations such as flipping, rotating, scaling, cropping, zooming, and shifting. These techniques help prevent overfitting and improve the generalization performance of models. In the context of deep learning pipelines, some commonly used data augmentation techniques include:

1. Flipping: This involves horizontally or vertically flipping images to create new samples. It can be particularly useful when working with datasets where orientation does not affect the target variable.

2. Rotation: Images can be randomly rotated within a specified range of angles to generate additional training examples. This technique helps the model learn features invariant to rotation.

3. Scaling: Resizing images by changing their dimensions while maintaining aspect ratio can also introduce variability in the dataset. This allows the model to recognize objects at different scales.

4. Cropping: Randomly selecting regions within an image and rescaling them to the original size creates new samples. This technique encourages the model to focus on local features rather than global ones.

5. Translation: Shifting images along horizontal or vertical axes generates new instances. Similar to cropping, this method promotes feature extraction from various parts of the image.

6. Shearing: Applying shear transformation to images distorts their shape without altering their content significantly. This technique introduces variations in perspective and viewpoint.

7. Noise injection: Adding Gaussian noise to pixel values increases robustness against noisy inputs during inference time.

These data augmentation techniques have been widely adopted across various domains, including computer vision, natural language processing, and speech recognition tasks. However, it should be noted that not all techniques may be applicable or beneficial depending on the specific problem and dataset at hand.
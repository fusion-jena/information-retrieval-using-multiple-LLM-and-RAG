The given context does not provide specific information about regularization methods used to prevent overfitting in the deep learning pipeline. However, some commonly known regularization techniques include Dropout, L1 Regularization, and L2 Regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting. L1 Regularization adds a penalty equal to the absolute value of the magnitude of coefficients, while L2 Regularization adds a penalty equal to the square of the magnitude of coefficients. These penalties discourage complex models and help avoid overfitting. Other techniques like early stopping, data augmentation, and weight decay can also be employed to prevent overfitting.
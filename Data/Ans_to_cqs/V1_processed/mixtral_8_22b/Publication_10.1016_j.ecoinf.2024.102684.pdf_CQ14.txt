The hyperparameters of the model were optimized through a small grid search. The specific hyperparameters considered in this search included the hidden activation function, the optimizer, and the learning rate. For each of these parameters, a set of possible values was selected. The hidden activation function options were ReLU, sigmoid, or elu; the optimizer choices were RMSprop or adam; and the learning rates tested were 0.1, 0.001, 0.0001, and 0.00001. By conducting a grid search across these combinations, the researchers aimed to find the optimal configuration for their model. It should be noted that while automated tools like Optuna and Sklearn could have been employed to expedite this process, the researchers opted to manually code their own solution to maintain full control over the optimization procedure.
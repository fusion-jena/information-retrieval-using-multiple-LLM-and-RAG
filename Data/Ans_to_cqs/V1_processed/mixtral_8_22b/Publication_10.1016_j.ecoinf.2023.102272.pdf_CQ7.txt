The data used for model training and selection were randomly partitioned into several subsets. First, 25% of the data was allocated for training candidate models (At). Second, 50% of the data was reserved for validating these candidate models (Av). Third, 75% of the data was utilized for training the selected candidate model (Bt = At + Av). Lastly, 25% of the data was designated as validation data to determine the optimal number of epochs to train the selected candidate model (Bv). This data partition was carried out in R with the package 'dismo'.

In terms of the model selection procedure, it involved generating five models for each of the four available deep-ANN architecture types, resulting in a total of twenty models. Each model was trained with a small subset of the training data (At) for four epochs. The accuracy of these candidate models was evaluated using a separate validation dataset (Av), and the model with the best performance was chosen for further training on the complete training data (Bt = At + Av) for up to thirty epochs.

For the purpose of ensuring independence between the data used for model training and the data used for model testing, the data from each year (2013 to 2019) was used separately for model testing ('T'). Meanwhile, the data from the remaining years was used for model training. This approach aimed to simulate an operational setting where historical data would be used to train a model intended for real-time forecasting of future events.
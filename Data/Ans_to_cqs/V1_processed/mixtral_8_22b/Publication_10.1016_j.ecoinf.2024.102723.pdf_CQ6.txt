Based on the given context, there isn't any explicit mention of specific preprocessing steps taken before training the deep learning models. However, some common preprocessing techniques used in machine learning include data normalization or standardization, feature scaling, handling missing values, outlier detection and removal, encoding categorical variables, and splitting datasets into training, validation, and testing sets.

Data normalization involves transforming features so that they fall within a similar range, typically between 0 and 1 or -1 and 1. This can help improve convergence when training certain types of models like neural networks. Standardization scales features such that their mean becomes 0 and their standard deviation becomes 1. Feature scaling ensures all features contribute equally to the final results.

Handling missing values might involve imputation methods where missing values are replaced with estimated ones derived from existing data points. Outliers could potentially skew results, hence detecting and removing them may sometimes be necessary. Encoding categorical variables converts non-numeric categories into numerical representations suitable for modeling purposes. Lastly, dividing datasets into separate parts allows us to train models effectively without introducing bias due to overfitting.

While these general practices apply broadly across many applications, the exact preprocessing steps depend largely on the nature of the dataset and the requirements of the chosen model architecture. Therefore, without further information about the specific dataset and model used here, I cannot provide a definitive list of preprocessing steps employed prior to training the deep learning models mentioned in the context.
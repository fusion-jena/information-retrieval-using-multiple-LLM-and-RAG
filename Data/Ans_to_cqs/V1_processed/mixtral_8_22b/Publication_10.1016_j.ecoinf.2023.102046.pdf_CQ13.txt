The authors used two types of deep learning models - ResNet-50 and Inception-V3. For both models, they dealt with dataset imbalance by including class weights in the cross-entropy loss function. They also used batch sizes of 16 examples. However, there are differences between the two models regarding their optimization methods and learning rates.

For the ResNet-50 model, the authors used the AdamW optimizer, which is the default optimizer in the transformers library by Hugging Face. They set the learning rate to 10^(-4) for Vanilla Inception-V3 and ViT, while using a learning rate of 4 x 10^(-4) for Deep Otolith Inception. These learning rates were suggested by Politikos et al. (2021).

On the other hand, for the Inception-V3 model, the authors used the Adam optimizer because it was the optimizer used in previous work. Unfortunately, the specific learning rate used for this model is not mentioned in the given context.

Therefore, based on the information provided, the hyperparameters used in the deep learning models include the type of optimizer (Adam or AdamW), the learning rate (ranging from 10^(-4) to 4 x 10^(-4)), and the inclusion of class weights in the cross-entropy loss function to address dataset imbalance.
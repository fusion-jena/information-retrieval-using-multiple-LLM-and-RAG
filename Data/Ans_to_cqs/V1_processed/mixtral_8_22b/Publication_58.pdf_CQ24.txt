Based on the given context, there isn't any explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, we can infer some strategies based on common practices in machine learning and deep learning.

One strategy to manage randomness is setting a fixed seed value before running the models. This ensures reproducibility of results across different runs and platforms. By fixing the seed value, the same sequence of pseudo-random numbers will be generated every time the code is executed, leading to consistent results.

Another approach is to employ techniques like cross-validation or bootstrapping to assess the variability introduced by randomness. Cross-validation involves splitting the dataset into multiple folds and iteratively training the model on different subsets while testing on the remaining fold. Bootstrapping, on the other hand, creates multiple samples with replacement from the original dataset and trains the model on these samples. Both techniques help estimate the robustness and generalization capabilities of the model in the presence of randomness.

Additionally, ensemble methods can also be utilized to reduce the impact of randomness. Ensemble methods combine predictions from multiple models, which may have been initialized with different seeds or trained on different subsets of the data. Averaging the outputs of these models helps minimize the effects of randomness and improves overall performance.

In summary, although the given context does not explicitly mention strategies for handling randomness in the deep learning pipeline, common approaches include setting a fixed seed value, utilizing cross-validation or bootstrapping, and employing ensemble methods.
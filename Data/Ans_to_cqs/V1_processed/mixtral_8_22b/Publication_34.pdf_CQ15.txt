A stochastic gradient descent algorithm with a momentum of 0.9 is used in this case. Stochastic Gradient Descent (SGD) is an iterative method for optimizing an objective function with suitable smoothness properties. It uses random sampling to estimate the true gradient of the cost function. Momentum is a term used in gradient descent optimization algorithms which helps accelerate gradients in the right direction, thus leading to faster convergence. In addition, batch-size of 50 is also mentioned, where batch samples are randomly chosen at each iteration.

However, no specific mention of using the Adam optimization algorithm or any other optimization techniques besides SGD is found in the given context.
Two regularization methods were used to prevent overfitting in the deep learning pipeline: dropout and DropConnect. Dropout randomly discards the output of hidden nodes during training, while DropConnect randomly discards the input of the hidden layer. These techniques help to reduce co-adaptation between neurons and improve generalization by introducing randomness during training. Additionally, early stopping was also employed, where training would stop if the validation loss did not improve within 10 epochs. This helps to avoid overfitting by preventing the model from continuing to learn when it has already started to fit too closely to the training data.
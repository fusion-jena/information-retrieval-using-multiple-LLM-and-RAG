During the training process, the model's performance is monitored using a validation set, which comprises 13% of the total data or 607 images. This validation set is employed to fine-tune hyperparameters and gauge the model's effectiveness on previously unseen data. By doing so, the risk of overfitting is mitigated, and the model's capacity to generalize is ensured. Additionally, the testing set, accounting for 5% of the total data or 260 images, serves as an independent assessment of the model's performance on completely unseen data. This provides a dependable indicator of the model's real-world proficiency and adaptability. To further evaluate the model's performance, several metrics are considered, including Precision, Recall, mAP0.5, mAP0.5:0.95, Params(M), Flops(G), Inference(ms), and Time(h). These metrics help determine the model's precision, recall rate, mean average precision, computational efficiency, and processing speed.
In the given context, it is mentioned that during training, the model optimizes cross-entropy loss via mini-batch stochastic gradient descent (SGD). This means that the optimization technique used in the deep learning pipeline is SGD. Additionally, other optimization techniques such as dropout and L2 regularization are also applied. Dropout is used to prevent overfitting by randomly dropping out neurons during training, while L2 regularization adds a penalty term to the loss function to discourage large weights and reduce overfitting. These techniques help improve the performance and generalizability of the deep learning model.
Training completion for Deep Convolutional Neural Networks (CNNs), like ResNet and GoogLeNet, is determined by minimizing the prediction loss during the training phase. The objective function J(w) is defined as the sum of individual losses L(f(w;xi), yi) for all data instances within a mini-batch divided by the batch size N, plus a regularization term Î»R(w). The goal is to iteratively adjust the network weights w using Stochastic Gradient Descent (SGD) until the desired level of accuracy or convergence is achieved. In practice, training may stop when the validation loss reaches a plateau or after a predefined maximum number of epochs.
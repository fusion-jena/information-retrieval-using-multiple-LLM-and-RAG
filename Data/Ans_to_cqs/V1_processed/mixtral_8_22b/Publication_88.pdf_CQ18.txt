During the training process of both the EF and SN models, several strategies were employed to monitor their performance. Firstly, a batch size of 32 was chosen along with 100 number of epochs. To prevent overfitting, early stopping was applied after 10 epochs without any improvement on the validation set. Furthermore, a dropout rate of 0.2 was utilized in the final fully connected layer. The Adam optimizer was also selected, with a weight decay of 0.9 and a learning rate of 10^-3. Lastly, the binary cross-entropy loss function was used to measure the discrepancy between predicted and actual values.
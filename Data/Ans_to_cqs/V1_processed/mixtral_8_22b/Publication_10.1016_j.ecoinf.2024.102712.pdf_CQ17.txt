Based on the given context, there is no information about the specific regularization methods used in the deep learning pipeline to prevent overfitting. However, some common regularization techniques used in deep learning include dropout, L1/L2 regularization, early stopping, and batch normalization. Dropout randomly drops out neurons during training to reduce co-dependencies among them. L1/L2 regularization adds a penalty term to the loss function to discourage large weights. Early stopping stops training when validation error starts increasing. Batch normalization scales inputs to each layer to improve convergence speed and stability. These techniques can be applied individually or combined to achieve better results.
The text mentions that the training process is stopped when the network begins to overfit the data, specifically when the error on the validation set increases. This is referred to as early stopping, a commonly used regularization technique to prevent overfitting. Overfitting occurs when the model performs well on the training data but poorly on unseen data due to excessive complexity or noise in the training data. Early stopping helps mitigate this issue by halting the training process before the model becomes too complex and loses its ability to generalize to new data. Therefore, the criterion used to determine when training is complete is the increase in error on the validation set during the training process.
The context does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions that the I3Net framework uses target domains with regularizations for feature consistency. Regularization techniques such as dropout or L2 regularization can be employed to prevent overfitting in deep learning pipelines. Dropout randomly drops out neurons during training, effectively preventing complex co-adaptations between neurons and reducing overfitting. L2 regularization adds a penalty term to the loss function, discouraging large weights and promoting simpler models. Other common regularization techniques include early stopping, data augmentation, and batch normalization. Without more detailed information from the context, we cannot definitively state which regularization methods were used in this particular case.
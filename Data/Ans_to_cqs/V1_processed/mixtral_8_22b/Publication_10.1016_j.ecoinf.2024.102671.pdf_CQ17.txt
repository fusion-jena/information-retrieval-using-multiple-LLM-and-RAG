The query asks about regularization methods used to prevent overfitting in deep learning pipelines. Regularization is a technique used to reduce overfitting by adding a penalty term to the loss function during training. Two common types of regularization techniques mentioned in the query are Dropout and L2 regularization.

Dropout is a regularization method that randomly drops out neurons from the network during training. This helps prevent overfitting by reducing co-dependencies between neurons and making the network more robust to noise. During each iteration of training, some percentage of nodes are dropped out, forcing the remaining nodes to learn more generalized features.

L2 regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the weights. This encourages smaller weights, leading to simpler models that are less likely to overfit. By penalizing large weights, L2 regularization can help improve the generalization performance of the model.

Other regularization methods not explicitly mentioned in the query include L1 regularization, which adds a penalty term proportional to the absolute value of the weights, and early stopping, which stops training once validation error starts increasing. These methods can be combined or used independently depending on the specific requirements of the problem being solved.

Based on the given context, it appears that the authors did not discuss regularization methods used to prevent overfitting in deep learning pipelines. Therefore, this information cannot be directly extracted from the provided sources. However, the above explanation provides a helpful response to the query.
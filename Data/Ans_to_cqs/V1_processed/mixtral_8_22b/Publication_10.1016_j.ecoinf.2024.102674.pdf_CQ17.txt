Based on the given context, there is no explicit mention of specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some general insights can be derived from the information available.

In the context of machine learning algorithms such as Random Forests (RFs) and Gradient Boosting Machines (GBMs), regularization techniques are often employed to avoid overfitting. In the case of RFs, bagging or bootstrap aggregating is used, which involves creating multiple subsets of the original data and fitting a separate decision tree to each subset. This helps reduce variance and improve prediction accuracy.

For GBMs, shrinkage and interaction parameters are mentioned, which could imply the use of regularization techniques like L1 or L2 regularization. Shrinkage reduces the influence of individual trees in the ensemble, while interaction parameters control the complexity of interactions between features. These techniques help prevent overfitting by reducing the model's complexity and improving its ability to generalize to unseen data.

However, it should be noted that the context does not provide direct evidence of the use of specific regularization methods commonly associated with deep learning pipelines, such as dropout or L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the network more robust to the removal of any single feature. L2 regularization adds a penalty term to the loss function, encouraging smaller weights and preventing any single weight from becoming too large, thereby reducing overfitting.

In conclusion, based on the given context, while there is no explicit mention of specific regularization methods used in the deep learning pipeline, the use of techniques such as bagging for RFs and shrinkage/interaction parameters for GBMs suggests that regularization may have been applied to prevent overfitting. However, without further information, it cannot be confirmed if methods like dropout or L2 regularization were specifically utilized.
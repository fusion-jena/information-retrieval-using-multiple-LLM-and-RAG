The authors mention using dropout as one of their regularization methods to prevent overfitting in their deep learning pipeline. Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training, which helps to reduce interdependent learning amongst the neurons. By doing so, it prevents overfitting and improves the generalization capability of the model. However, the authors do not explicitly state whether they have used other regularization techniques such as L2 regularization. Therefore, based on the given context, we can only confirm that dropout was used as a regularization method to prevent overfitting in their deep learning pipeline.
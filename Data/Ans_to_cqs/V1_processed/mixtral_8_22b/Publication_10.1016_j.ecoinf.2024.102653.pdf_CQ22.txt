The performance of the deep learning model is evaluated using conventional metrics such as confusion matrices, accuracy, recall, precision, F1-score, Cohen Kappa, and the area under the ROC curve. Some of these metrics are defined below:

* Confusion matrix: A table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class.
* Accuracy: The proportion of true positive and true negative predictions out of total predictions made.
* Recall: Also known as sensitivity or True Positive Rate (TPR), it measures the proportion of actual positives that are correctly identified.
* Precision: Also called Positive Predictive Value (PPV), it measures the proportion of true positive predictions out of total positive predictions made.
* F1-score: It is the harmonic mean of precision and recall, providing a single metric that balances both properties.
* Cohen Kappa: It measures inter-annotator agreement for qualitative items. In machine learning, it can measure how well a classification model performs compared to random chance.
* Area Under the Receiver Operating Characteristic Curve (AUROC): It provides an aggregate measure of performance across all possible classification thresholds.
Based on the given context, there isn't any explicit mention of specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being used in the deep learning pipeline. However, the context does indicate that the model was trained using a learning rate of 0.001, which suggests that some form of gradient descent optimization technique might have been employed. Additionally, the context mentions that the model improved swiftly in terms of precision, recall, and mean average precision before plateauing after around 1000 iterations, indicating that the learning process may have involved some sort of adaptive learning rate adjustment strategy. Nonetheless, without further information regarding the specific optimization algorithms used during the training phase, it would be speculative to definitively state whether SGD, Adam, or other optimization techniques were utilized in the deep learning pipeline.
Before training a deep learning model, several preprocessing steps may be necessary depending on the nature and quality of the input data. These steps often include:

1. Data Cleaning: This involves removing or correcting erroneous data points such as missing values, outliers, or inconsistent entries. It also includes handling categorical variables if any.

2. Normalization or Standardization: Deep learning models perform better when all features have similar scales. Therefore, it's common practice to scale numerical inputs to a standard range like [0, 1] or [-1, 1]. This process is known as normalization. Alternatively, one could use standardization which transforms the data to have zero mean and unit variance.

3. Encoding Categorical Variables: If there are categorical variables present in the dataset, they need to be converted into numerical form using techniques like label encoding or one-hot encoding.

4. Feature Selection or Extraction: Not all features contribute equally to the prediction task. Some might even introduce noise. Hence, selecting relevant features or extracting new ones through dimensionality reduction techniques can improve model performance.

5. Train-Test Split: The dataset should ideally be split into two parts - one used for training the model and another held back for testing its performance. This helps prevent overfitting and provides a more realistic estimate of how well the model will generalize to unseen data.

6. Cross Validation: To further ensure robustness against overfitting, cross validation methods can be employed where multiple train-test splits are created and the model is trained and evaluated on each pair separately.
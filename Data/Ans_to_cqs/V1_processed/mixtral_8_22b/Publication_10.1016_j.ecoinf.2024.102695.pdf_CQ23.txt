To ensure the generalizability of the deep learning model, several measures were taken during its development and testing. Firstly, the dataset was divided chronologically into three parts: 70% for the training set, 10% for the validation set, and 20% for the test set. This allocation strategy helps simulate real-world scenarios where the data in the validation and test sets differ from the training set data. By doing so, it ensures that the model does not become too specialized or biased towards the training data, thus improving its ability to perform well on unseen data.

Secondly, throughout the training process, a learning rate of 0.0001 was used, along with 300 training epochs and 24 data instances processed per batch. These settings help strike a balance between optimizing the model's performance and avoiding overfitting. Additionally, an early stopping strategy was employed, which allows the training process to converge before reaching the final epoch. This prevents overfitting issues and further contributes to maintaining the model's generalizability.

Lastly, all tests were conducted on a Windows 11 operating system using Python 3.8, with specific hardware such as a laptop equipped with a 12th-Gen Intel Core i7-12700H GPU at 2.30 GHz and an NVIDIA GeForce RTX 3060 GPU. Using consistent software and hardware configurations across experiments also plays a role in promoting the reproducibility and reliability of the results, ultimately contributing to the overall generalizability of the deep learning model.
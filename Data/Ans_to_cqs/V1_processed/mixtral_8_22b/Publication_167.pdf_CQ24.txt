The text does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, it mentions that due to the large time span and large land area in the training data, there are significant differences in the distribution between different batches of training data during the training phase. To address these discrepancies, the authors employ batch normalization on the input data. This technique involves calculating the mean and variance of each channel within the current batch, subtracting the mean from the input, dividing the result by the standard deviation, multiplying the outcome by a scale parameter, and adding a shift parameter to produce the final output. While this approach doesn't directly tackle randomness caused by elements like random seed values, it helps manage variability in the dataset, which could indirectly mitigate some effects of randomness in the deep learning process.
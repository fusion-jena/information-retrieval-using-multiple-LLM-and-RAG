Based on the given context, there isn't any explicit information about how the data was split for deep learning model training (i.e., into train, test, or validation sets). However, typically, the data is divided randomly into three subsets: training set, validation set, and testing set. The training set is used to fit the model, while the validation set is used to tune hyperparameters and prevent overfitting. Finally, the testing set is used to evaluate the final performance of the model. In this case, since no specific details were mentioned, we can assume that the authors followed standard practices for splitting their dataset.
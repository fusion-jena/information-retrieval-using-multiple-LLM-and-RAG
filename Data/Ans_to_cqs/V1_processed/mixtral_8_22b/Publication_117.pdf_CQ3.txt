The study employs a deep learning pipeline utilizing a version of the YOLO Deep Convolutional Neural Network (DCNN), specifically tailored for their application. To generate a training dataset, they selected 500 survey images randomly and divided them into smaller tiles measuring 864 x 864 pixels each. These tiles were subsequently processed through the YOLO DCNN, leveraging pretrained weights from the COCO dataset provided by Redmon & Farhadi (2018). This procedure yielded a list of potential object locations within each image.

To refine the initial output, the researchers applied a filtering step whereby any object detections failing to align with identifications from the Zooniverse data were disregarded. Although the specific data annotation technique utilized in this research isn't explicitly mentioned, the description suggests that the team might have employed bounding box annotations. Bounding boxes are commonly used in object detection tasks, including those involving deep learning models like YOLO, to encapsulate objects of interest within an image.

In summary, while the paper does not directly state the data annotation techniques used, the described methodology implies the utilization of bounding box annotations in conjunction with the YOLO DCNN for object detection purposes.
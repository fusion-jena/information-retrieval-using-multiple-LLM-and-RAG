The text mentions two regularization techniques used to prevent overfitting in the deep learning pipeline. These include dropout and batch normalization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the network more robust to noise and variations in input data. This method was first proposed by Srivastava et al. (2014). On the other hand, batch normalization is a technique that standardizes the inputs to a layer for each mini-batch, allowing each layer of a network to learn by itself a little more independently of other layers. It was introduced by Ioffe and Szegedy (2015).
Based on the given context, there isn't explicit information about how the data was split into training, testing, and validation sets for the deep learning model. However, we can infer some general practices based on common machine learning methodologies. Typically, datasets are randomly divided into three subsets: training, validation, and testing. The training set is used to fit the model, while the validation set is used to tune hyperparameters and prevent overfitting. Lastly, the testing set is used to evaluate the final model's performance.

In this specific context, the authors mention using a pre-trained ResNet50 model, which suggests they might have followed transfer learning techniques. They also discuss incorporating additional validation soundscape data during training, indicating that they likely had separate training and validation sets. Unfortunately, without more detailed information, we cannot provide exact percentages or ratios used for splitting their data.
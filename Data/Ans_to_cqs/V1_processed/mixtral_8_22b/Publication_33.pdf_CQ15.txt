The text does not explicitly mention any specific optimization technique such as Stochastic Gradient Descent (SGD) or Adam. However, it does refer to the use of back-propagation for optimizing the parameters of the CNN. Back-propagation is an algorithm used to efficiently calculate gradients in a neural network, which are then used to update the weights of the network via some form of gradient descent. Therefore, although not directly stated, it can be inferred that some form of gradient descent optimization technique was likely employed in conjunction with back-propagation.
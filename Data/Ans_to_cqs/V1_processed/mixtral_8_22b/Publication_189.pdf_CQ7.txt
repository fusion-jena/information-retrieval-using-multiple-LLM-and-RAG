In the given context, there isn't any explicit mention of how the data was split into training, testing, and validation sets for the deep learning model. However, it can be inferred that the data was divided into these subsets based on common practices in machine learning and deep learning. Typically, the dataset is randomly partitioned into three parts: training set, validation set, and test set. The training set is used to fit the parameters of the model, the validation set is used to tune hyperparameters and prevent overfitting, and the test set is used to evaluate the final performance of the model. The exact proportions of the splits may vary depending on the specific application and the amount of available data. Common ratios include 70% training, 15% validation, and 15% testing, or 80% training, 10% validation, and 10% testing. In the case of the Mask R-CNN training mentioned in the context, it appears that separate datasets were used for training and validation, but no information about a test set is provided.
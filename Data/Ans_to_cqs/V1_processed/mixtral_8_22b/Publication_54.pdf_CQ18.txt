During the training process, several strategies were employed to monitor the model's performance and prevent overfitting. Initially, the dataset was divided into separate training, validation, and testing sets. The training set was utilized for fitting the parameters of the classifier, while the validation set offered an unbiased assessment of the model fit on the training set. This allowed for the fine-tuning of model hyperparameters like network layer size and finding the optimal number of hidden units.

In situations where the model had numerous hyperparameters, a more extensive validation dataset was necessary. To address potential bias issues, k-fold cross-validation was applied. This involved generating the training set with different combinations of k groups from the original dataset, which served alternately as both training and validation sets. By repeating this process until each k-fold acted as the test set, the risk of overfitting could be minimized.

To further optimize the model and evaluate the impact of its constituent elements, multi-stage training was carried out using the training and validation datasets. Moreover, five-fold cross-validation was also employed to confirm the model's predictive capabilities.

Additionally, a mechanism was introduced to exchange misclassified instances between the training and validation datasets. This enabled the model to be retrained continuously until achieving the maximum attainable accuracy on the validation and test datasets. Consequently, this approach facilitated enhanced performance and reduced overfitting within the chosen model.
Regularization methods were not explicitly mentioned in the given context. However, some commonly used regularization methods to prevent overfitting in deep learning pipelines include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to avoid overfitting by making the model more robust and less sensitive to the specific weights of individual neurons. On the other hand, L2 regularization adds a penalty term to the loss function, encouraging smaller weight values and thus reducing overfitting. Other regularization methods may include early stopping, data augmentation, and batch normalization.
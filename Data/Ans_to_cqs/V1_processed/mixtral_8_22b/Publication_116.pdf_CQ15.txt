Based on the given context, there is no explicit mention of specific optimization techniques such as Stochastic Gradient Descent (SGD), Adam, or others being applied in the deep learning pipeline. However, the context does reference batch normalization [32], which can be considered an optimization technique used during model training to improve convergence and stability. Additionally, the paper references the use of Caffe [34], a popular deep learning framework that supports various optimization algorithms including SGD, Adam, and others. Therefore, while not explicitly stated, it can be inferred that some form of optimization technique was likely employed in the deep learning pipeline described in the context.
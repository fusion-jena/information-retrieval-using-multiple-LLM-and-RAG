To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the use of diverse datasets was employed to train and validate the model. By using multiple datasets, the model can learn from various sources and improve its ability to perform well on unseen data. Secondly, partial training was implemented where models were initially trained on a small fraction of the data to quickly eliminate underperforming configurations. Thirdly, successive halving was utilized, where the remaining configurations were divided into sets of varying sizes, and their corresponding models were further trained. The top-performing configurations in each set advanced to the subsequent round, while the poorly performing ones were eliminated. These steps were repeated until only one configuration remained, which was deemed the optimal solution. Lastly, the data was standardized during the training phase, and one of the three datasets was designated as the training base, on which the normalization parameters were computed and applied to the other two datasets serving as the test bases. The best outcome from this optimization process helped decide the base that would be used for training.
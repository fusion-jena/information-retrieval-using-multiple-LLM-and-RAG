The given context does not provide explicit information regarding the specific preprocessing steps taken before training the deep learning models. However, it mentions that the models were initialized with ImageNet weights, which implies some form of weight initialization or transfer learning. Additionally, the use of stochastic gradient descent (SGD) as the optimizer suggests that the input features might have been scaled or normalized to ensure efficient optimization. It also discusses the application of data balancing techniques such as SMOTE to address the issue of imbalance in the dataset. Therefore, while the exact preprocessing steps are not mentioned, we can infer that they likely included weight initialization, potential scaling or normalization, and possibly data augmentation or balancing techniques.
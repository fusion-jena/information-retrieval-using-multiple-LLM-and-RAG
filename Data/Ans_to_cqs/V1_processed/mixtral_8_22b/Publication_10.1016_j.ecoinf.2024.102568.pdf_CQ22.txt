The performance of the deep learning models is evaluated using four different metrics: Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Relative Error (MRE), and accuracy. These metrics help assess the quality of the predictions made by the models.

Root Mean Square Error (RMSE) calculates the difference between the actual and predicted values, then squares and averages these differences before taking their square root. This metric provides a measure of how spread out the residuals or prediction errors are. A smaller RMSE indicates a more accurate model.

Mean Absolute Error (MAE) also computes the difference between the actual and predicted values but does not square them. Instead, it takes the absolute value of each difference and finds its mean. This metric helps understand the average magnitude of the errors in the dataset. Lower MAE values indicate better model performance.

Mean Relative Error (MRE) expresses the average of the relative errors, which are computed as the ratio of the absolute error to the actual value. It provides information on the overall percentage error of the model. Smaller MRE values signify superior model performance.

Accuracy is commonly employed in classification tasks and represents the proportion of correctly classified instances among the entire dataset. Higher accuracy values imply improved model performance. However, this metric may not always provide a complete picture of the model's effectiveness, especially when dealing with imbalanced datasets.
The provided context does not explicitly mention the specific process used to deploy the trained deep learning model. However, it does provide some clues about the tools and techniques employed during the development and testing phases.

In this study, the researchers used the HRNet model for semantic segmentation tasks due to its high accuracy and stability. They also mentioned employing the combined LSTM-CNN-based sentiment analysis framework for determining the sentiment index after conducting comparative analyses between different models such as Rostcm 6.0, Simple Bayesian Networks, Long Short-Term Memory networks (LSTM), and Convolutional Neural Networks (CNN).

Furthermore, they utilized the pre-trained ERNIE model from GitHub (<https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE>) for their experiments. Although there are no explicit details regarding the deployment process, one could infer that the researchers might have serialized the trained models into appropriate formats (like.h5 or.onnx files) before integrating them into a suitable platform for real-world applications.

As for the platform selection, since the researchers performed their experiments using PyCharm software and graphics processing software for proofreading, it is possible that they deployed the trained models onto platforms compatible with these environments. Nonetheless, without further information, we cannot definitively state the exact process followed to deploy the trained deep learning model.
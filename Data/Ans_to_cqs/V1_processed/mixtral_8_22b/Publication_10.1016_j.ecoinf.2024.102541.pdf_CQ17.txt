The provided context does not directly mention any specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some commonly used regularization techniques include Dropout, L1 Regularization, and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust by preventing complex co-adaptations on training data.

L1 Regularization adds an absolute value of the magnitude of coefficient as penalty term to the loss function. This tends to produce sparse models with few coefficients.

L2 Regularization adds the squared magnitude of coefficient as penalty term to the loss function. It helps in reducing the model complexity by shrinking the weights and prevents overfitting.

These techniques can be applied in various stages of the deep learning pipeline such as before or after activation functions, or even within layers themselves. The choice of regularization method depends on the specific problem and dataset being considered.
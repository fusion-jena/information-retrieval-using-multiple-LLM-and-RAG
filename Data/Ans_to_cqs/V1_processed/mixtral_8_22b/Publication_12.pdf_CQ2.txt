The deep learning pipeline described in X. Chen, et al.'s Biological Conservation article uses both image and audio data formats. Initially, the authors convert audio files into spectral images, which they categorize into four groups: strong, weak, no call, and other. These spectral images serve as input for their deep learning models, such as BatNet and CNNFull. Additionally, the authors use unfiltered audio files recorded in various environments to validate their models. Thus, the primary data formats employed in this pipeline are images derived from audio files and raw audio files themselves.
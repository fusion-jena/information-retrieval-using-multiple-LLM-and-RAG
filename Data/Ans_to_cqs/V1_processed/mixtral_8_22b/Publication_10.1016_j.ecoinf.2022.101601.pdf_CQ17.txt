The context does not provide information on the specific regularization methods used to prevent overfitting in a deep learning pipeline. However, some common regularization techniques include Dropout, which randomly sets a fraction rate of input units to 0 during training time, helping reduce overfitting; and L2 Regularization, also known as weight decay, which adds a penalty term to the loss function proportional to the square of the magnitude of all weights, discouraging large weights and thus reducing overfitting. Other methods may include early stopping, data augmentation, or using simpler models with fewer parameters.
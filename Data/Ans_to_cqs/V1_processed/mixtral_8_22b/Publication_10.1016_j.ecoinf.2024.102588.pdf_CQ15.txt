In this study, the optimization technique applied in the deep learning pipeline is the Adam optimizer. This optimizer is used with a learning rate of 0.001. The Adam optimizer is a popular choice for deep learning applications due to its ability to adaptively adjust the learning rate based on the first and second moments of the gradients. Additionally, it has been shown to perform well across various types of problems and architectures. Other commonly used optimization techniques include Stochastic Gradient Descent (SGD), AdaGrad, and RMSProp. However, these methods are not mentioned or utilized in this particular study.
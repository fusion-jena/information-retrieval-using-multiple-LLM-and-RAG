In the described deep learning architecture, the method employed to prevent overfitting is dropout regularization. Dropout regularization involves the random deactivation of a certain percentage of the neural connections during training. In this case, 50% of the neural connections are randomly deactivated after each BiLSTM layer. This technique helps to reduce the co-dependency between nodes, thus preventing overfitting and improving the generalizability of the model. Other common regularization techniques include L1 and L2 regularizations, but they are not mentioned in the given context.
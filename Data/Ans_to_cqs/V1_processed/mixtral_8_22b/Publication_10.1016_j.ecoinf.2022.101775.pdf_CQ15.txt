The paper mentions the use of stochastic gradient descent (SGD) for optimizing the regularized cost function. Additionally, when training the algorithms, Adam is used as the optimization algorithm to update the network parameters. Both SGD and Adam are popular optimization techniques used in deep learning pipelines.
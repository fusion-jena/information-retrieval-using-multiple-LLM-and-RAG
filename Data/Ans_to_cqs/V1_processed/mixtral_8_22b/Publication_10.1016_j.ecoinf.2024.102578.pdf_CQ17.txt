Based on the given context, there is no explicit mention of specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some common techniques include dropout, L2 regularization, early stopping, and data augmentation. Dropout involves temporarily removing random neurons during training to prevent complex co-adaptations between them. L2 regularization adds a penalty term to the loss function to discourage large weights. Early stopping stops training when performance on a validation set starts degrading. Data augmentation generates new training examples by applying transformations such as rotation, scaling, and flipping to existing ones. It is possible that these or other methods may have been employed but not explicitly mentioned in the provided text.
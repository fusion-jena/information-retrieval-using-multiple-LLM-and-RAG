The text does not provide explicit information on how the hyperparameters of the model were optimized. However, it mentions that for training, the Stochastic Gradient Descent (SGD) optimizer and Adam [27] were used with learning rates from 10^-2 until 10^-8. This suggests that some form of hyperparameter tuning was performed, but the specific method (grid search, random search, etc.) is not mentioned.
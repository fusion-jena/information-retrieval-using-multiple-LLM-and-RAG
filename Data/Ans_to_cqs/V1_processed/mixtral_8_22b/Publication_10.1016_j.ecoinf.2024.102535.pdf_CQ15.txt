The deep learning pipeline uses the Adam optimizer for minimizing the cost function during training. The Adam optimizer is a variant of Stochastic Gradient Descent (SGD) that incorporates momentum and adaptive learning rates. It combines the advantages of Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). In this case, the initial learning rate (μ0) is set to 0.0001, and the momentum (β1) is set to 0.9. To achieve better convergence during training, a learning rate decay procedure is also employed. This procedure is based on the equation proposed in Ganin et al. (2017), where the learning rate decays over time according to the current training epoch (e). The parameters α and β in the equation are set to 10 and 0.75, respectively.
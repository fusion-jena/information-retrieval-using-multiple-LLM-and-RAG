From the given context, it appears that the optimization of the hyperparameters of the model is not explicitly mentioned. However, there are some clues about the methods used for determining the optimal configuration of the model.

Firstly, the use of k-fold cross-validation suggests that the authors employed this technique to evaluate the performance of their model across multiple subsets of the data. K-fold cross-validation involves splitting the data into 'k' folds or partitions, where one fold is held out as a validation set while the remaining folds are used for training. This process is repeated 'k' times, such that each fold serves as the validation set once. By averaging the performance metrics obtained from each iteration, the authors can estimate the overall performance of the model.

Secondly, the mention of epochs implies that the authors used iterative training to improve the accuracy of their model. Epochs refer to the number of times the entire dataset is passed through the learning algorithm during training. Each pass through the dataset is called an iteration, and the total number of iterations required to complete one epoch depends on the batch size used for training. By increasing the number of epochs, the authors could potentially improve the convergence of their model towards the global minimum of the loss function.

Lastly, the reference to functions from the Python library scikit-learn suggests that the authors may have utilized various tools available within this library for optimizing the hyperparameters of their model. For instance, they might have used techniques like grid search or random search to systematically explore different combinations of hyperparameter values and identify the ones that yield the best performance. However, without further information, it is difficult to confirm whether these specific methods were indeed applied.
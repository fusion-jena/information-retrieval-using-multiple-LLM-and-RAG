The text does not explicitly mention specific regularization methods like dropout or L2 regularization. However, there are indications of techniques employed to prevent overfitting. For instance, the use of a simpler architecture instead of pre-trained models like ResNet50 and Inception suggests a form of implicit regularization. Additionally, early stopping is mentioned as a method to prevent overfitting. Early stopping involves monitoring the model's performance on a validation set during training and halting the training process once the performance starts degrading. This helps avoid overfitting by preventing the model from learning noise in the training data.
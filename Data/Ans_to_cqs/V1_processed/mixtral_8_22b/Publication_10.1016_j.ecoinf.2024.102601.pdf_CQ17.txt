The given context does not provide information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization methods include dropout, L1 regularization, and L2 regularization. Dropout randomly drops out neurons during training to prevent complex co-adaptations between neurons. L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function, while L2 regularization adds a penalty equal to the square of the magnitude of coefficients to the loss function. These penalties discourage large weights and help prevent overfitting.
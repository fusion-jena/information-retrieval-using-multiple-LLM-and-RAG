The paper by S.V. Jansi Rani et al. applies different optimization techniques in its deep learning pipeline. These include Stochastic Gradient Descent (SGD), Root Mean Square Propagation (RMSProp), and Adaptive Moment Estimation (AdamW). Figures 16 and 17 show comparisons of these optimizers. Additionally, the authors experimented with varying initial and final learning rates, epoch numbers, and whether or not to use cosine learning rate decay.
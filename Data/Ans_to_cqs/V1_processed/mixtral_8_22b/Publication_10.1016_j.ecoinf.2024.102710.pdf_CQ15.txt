The given context does not provide explicit information about the specific optimization techniques used in the deep learning pipelines mentioned. However, it is common practice in deep learning to use optimization algorithms like Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam) to minimize the loss function during training. These optimization techniques adjust the weights and biases of the neural network based on the gradient of the loss function with respect to those parameters. Without further details, it cannot be confirmed which specific optimization technique was employed in these studies.

Based on the context, it appears that various deep learning models were utilized, including VGG16, ResNet152V2, BirdNet, and others. These models may have been optimized using popular optimization techniques like SGD or Adam, among other possibilities. To determine the exact optimization techniques used, additional information would be required.
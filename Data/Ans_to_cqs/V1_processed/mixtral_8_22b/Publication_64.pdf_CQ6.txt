Before training a deep learning model such as ResNet50, several preprocessing steps are typically applied to the input data. These steps help improve the quality of the data and ensure better performance of the model. Some common preprocessing techniques include:

1. Normalization: This involves rescaling the pixel intensity values of the input images to a specific range, usually between 0 and 1. This helps reduce the impact of outliers and improves the convergence speed of the optimization algorithm.

2. Scaling: Images may also need to be scaled to fit the required input dimensions of the deep learning model. In the case of ResNet50, the input size is 224x224 pixels. Therefore, any input images must be resized accordingly.

3. Data Augmentation: To increase the diversity of the training data and prevent overfitting, various data augmentation techniques can be employed. These include random cropping, flipping, rotating, and color jittering. In the given context, the authors mention using standard data augmentation methods like shearing, translating, and mirroring. However, they avoid applying rotation or upside-down mirroring to 'habitat' images, which would not make sense for trees or landscapes.

4. Cleaning: Although not explicitly mentioned in the provided context, another important step in preparing the data for deep learning models is cleaning. This could involve removing corrupted or mislabeled samples from the dataset. In the given context, the authors acknowledge that their automatic labeling process might introduce some mislabeled or corrupt examples but attempt to mitigate this issue by selecting popular labels and retrieving the top search results.
Based on the given context, there isn't any explicit mention of specific strategies employed to handle randomness in the deep learning pipeline, such as managing the random seed value. However, we can infer some general practices based on common machine learning techniques.

One approach to manage randomness is using fixed seeds during training or testing phases. This ensures reproducibility of results across different runs of the same experiment. Another strategy is employing ensemble methods, where multiple models are trained with varying initial conditions (like different random seeds) and their predictions are combined to produce more robust outcomes.

In terms of parameter tuning, metaheuristic optimization algorithms and cross-validation schemes are mentioned. Metaheuristic optimization algorithms, like genetic algorithms or particle swarm optimization, often involve stochastic components but aim to find optimal solutions despite this inherent randomness. Cross-validation schemes, such as nested or rolling basis cross-validation, help evaluate model performance while accounting for variability due to random factors.

Lastly, it should be noted that the CNN-BiGRU model uses the Adam optimizer, which adapts individual learning rates for each network weight based on estimates of first and second moments of gradients. Although not directly addressing randomness related to seed values, adaptive learning rate methods like Adam can contribute to better handling of noise and variability within the data itself.
The provided context does not explicitly mention specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) that are commonly used in the deep learning pipeline. However, it can be inferred that optimization techniques were indeed employed during the training process of the mentioned Deep Neural Network (DNN) models. This is because DNN models consist of neurons arranged in successive layers that apply computations based on learnable parameters (weights and bias) and activation functions to inputs from previous layers, through to an output consistent with the training data. These weights and biases are typically adjusted iteratively via backpropagation and gradient descent methods like SGD or Adam to minimize the error between predicted and actual outputs. Therefore, while the exact optimization techniques used in this study remain undisclosed, it is highly probable that similar techniques were utilized.
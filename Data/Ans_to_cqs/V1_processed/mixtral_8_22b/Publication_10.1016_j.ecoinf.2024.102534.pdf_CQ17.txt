The given context does not provide information on the specific regularization methods used to prevent overfitting in a deep learning pipeline. However, some common regularization techniques include Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust by preventing complex co-adaptations on training data. This can be seen as a way of performing model averaging with all possible subsets of the network.

On the other hand, L2 Regularization, also known as weight decay, involves adding a penalty term to the loss function, which discourages large weights. The idea behind this method is to add a cost to the complexity of the model, thus making it simpler and reducing the risk of overfitting.

Other regularization methods include L1 Regularization, Early Stopping, and Batch Normalization. These methods work differently but share the same goal of improving the generalization performance of machine learning models.
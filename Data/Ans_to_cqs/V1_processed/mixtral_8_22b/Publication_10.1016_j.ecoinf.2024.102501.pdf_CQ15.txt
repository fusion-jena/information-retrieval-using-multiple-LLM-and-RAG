Based on the given context, it appears that search algorithms like Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) are commonly employed to tackle model parameter issues in deep learning pipelines. These algorithms help determine optimal values for various parameters, including the number of neurons in neural networks or error penalty factors in Support Vector Machines (SVM).

For instance, Yan utilized both GA and PSO to enhance a Backpropagation Neural Network (BPNN) for building a Dissolved Oxygen (DO) estimation model of Beijing Lake in Beijing (Yan et al., 2019). Furthermore, PSO has been frequently used alongside BPNN, Gated Recurrent Units (GRU), Long Short-Term Memory (LSTM), SVM, Support Vector Regression (SVR), and other algorithms (Wu et al., 2018; Huan et al., 2020; Zhu et al., 2021; Huang et al., 2021; Cao et al., 2021b; Liu et al., 2014). This demonstrates its effectiveness and robustness with minimal computational requirements. Additionally, other optimization algorithms, such as the Multi-Verse Optimizer, have also been implemented for parameter optimization (Yang et al., 2021).

However, the context does not explicitly mention specific optimization techniques typically applied within the deep learning pipeline, such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). Therefore, while GA and PSO are optimization techniques used in conjunction with deep learning models, they do not directly fall into the category of optimization techniques like SGD or Adam, which are primarily used during the training process of these models.
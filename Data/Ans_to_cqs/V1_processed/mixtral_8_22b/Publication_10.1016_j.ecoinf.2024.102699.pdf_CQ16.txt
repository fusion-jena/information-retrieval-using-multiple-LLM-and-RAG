Based on the given context, early stopping is used to determine when training is complete. Early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. This technique allows us to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset. In this case, the training setup includes the use of early stopping to prevent overfitting when no further improvement is seen in the validation loss.
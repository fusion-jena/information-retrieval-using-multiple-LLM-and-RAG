To ensure the generalizability of the deep learning models, several measures were taken. Firstly, the models were evaluated on both single and multi-label classification problems, providing a more comprehensive test of their performance. Secondly, data from three different sites were included in the experiments, introducing diversity into the dataset. This allowed for the evaluation of the models in various environments, enhancing their potential applicability.

Moreover, the models were not only tested in a conventional training scheme where data from a single site is used for training and testing but also in cross-site scenarios. In these scenarios, the models were trained with data from one site and then evaluated with data from a different site. This approach aimed to assess the generalization capacity of the different architectures and the ensemble.

Furthermore, the use of a Classification Decision Committee (CDC) constituted by an ensemble of networks with different architectures was proposed and evaluated. This committee employed a majority voting mechanism to fuse individual predictions, potentially improving the overall robustness and generalizability of the system.

Lastly, the experimental results suggested that there might be room to improve the generalization capacity of the deep learning classifiers. This could be achieved by better exploiting the training data, for instance, with additional data augmentation techniques or using recent advances in unsupervised learning techniques like self-supervised methods. Another possibility would be to simplify the classifiers in terms of the number of parameters, reducing the risk of overfitting.
To handle randomness in the deep learning pipeline, such as the random seed value, several strategies are employed. In this case, the authors use two main approaches to address this issue. First, they perform multiple runs of their experiments by repeating the random partition of the data into training, validation, and test sets ten times. This repetition helps ensure that the results are not influenced by any particular random split and allows them to achieve more consistent and reliable performance. Second, they employ techniques like batch normalization and dropout within their MLP model to improve the stability and robustness of the learning process. These methods help prevent overfitting and enhance the model's ability to generalize well on unseen data. Additionally, the choice of hyperparameters, such as the learning rate and batch size, also plays a role in managing randomness and improving the overall performance of the deep learning pipeline.
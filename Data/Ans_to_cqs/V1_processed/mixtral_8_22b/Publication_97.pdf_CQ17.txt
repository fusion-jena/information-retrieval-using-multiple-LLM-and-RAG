The study uses several regularization techniques to prevent overfitting in their deep learning pipeline. One such technique is dropout, which is applied with a probability of 0.5. Dropout is a regularization method that randomly sets a fraction of input units to 0 during training, which helps prevent complex co-adaptations between neurons. Additionally, the study employs batch normalization, which accelerates deep network training by reducing internal covariate shift. Batch normalization is applied after each convolutional layer, but not after dropout layers since they do not transform inputs or introduce biases. Furthermore, the study mentions the use of data augmentation techniques, including random changes of RGB channels and random flips, to increase the variability of training images and reduce overfitting. Although the study considers using small random rotations in the augmentation pipeline, this does not improve the network's performance.
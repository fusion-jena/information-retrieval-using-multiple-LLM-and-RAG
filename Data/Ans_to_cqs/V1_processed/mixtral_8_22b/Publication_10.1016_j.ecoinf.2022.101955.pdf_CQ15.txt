The given context does not provide information about specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being applied in the deep learning pipeline. However, it mentions that the MLP ANN model used a backpropagation learning algorithm during its training phase. Backpropagation is a widely used optimization technique in deep learning pipelines, where the error calculated by comparing the predicted output with the actual output is propagated backwards through the network layers to update the weights and biases using gradient descent. This helps minimize the loss function and improve the accuracy of predictions. Additionally, the context also mentions that the 'nnet' package was used, which allows for adjusting weighted decay and size to counteract the effects of model overfitting. Weight decay is another optimization technique commonly used in deep learning models to prevent overfitting by adding a penalty term to the loss function.
The deep learning models used in this research were ResNet50 and EfficientNetB3, both modified for multitask learning (MTL) with transfer learning using pre-trained weights from ImageNet. The models were trained and validated on the TLm dataset.

For optimization, the Adam optimizer was chosen over Stochastic Gradient Descent (SGD). This decision was made after comparing the two methods, where Adam showed a 1%-2% increase in accuracy compared to SGD. The specific hyperparameters used with the Adam optimizer were not explicitly mentioned in the given context.

However, it is known that the Adam optimizer typically uses default parameter values such as a learning rate of 0.001, beta_1 of 0.9, beta_2 of 0.999, epsilon of 1e-08, and no weight decay or momentum. These are common defaults, but they may have been adjusted during the training process.

In terms of other hyperparameters, the models were trained with transfer learning using pre-trained weights from ImageNet. This technique often involves freezing some layers of the pre-trained model while fine-tuning others, but the exact configuration was not specified in the given context.

Additionally, the models were validated with multitask learning, which implies that multiple tasks were learned simultaneously. This could involve additional hyperparameters related to task balancing or loss function weighting, but again, these details were not included in the provided context.
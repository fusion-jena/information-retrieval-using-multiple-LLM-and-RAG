The provided context does not explicitly mention any specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) that are commonly used in deep learning pipelines. However, some machine learning algorithms mentioned in the context like eXtreme Gradient Boosting (XGBoost) and Random Forests (RF) do have their own optimization methods built into them. For instance, XGBoost uses second-order approximation for its objective function which is optimized using Newton-Raphson method. Similarly, RF uses bagging and feature randomness for optimization. Additionally, the context mentions Maximum Entropy (MaxEnt) and Generalized Linear Model (GLM) which also use different optimization techniques but they are not specifically related to deep learning pipelines. Therefore, based on the provided context, it cannot be determined what optimization techniques are applied in the deep learning pipeline.
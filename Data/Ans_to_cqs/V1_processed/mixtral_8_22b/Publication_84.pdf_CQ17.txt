The article does not provide specific information about the regularization methods used to prevent overfitting in their deep learning pipeline. However, some common regularization techniques used in deep learning include Dropout, L1 Regularization, and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust by preventing complex co-adaptations on training data.

L1 Regularization adds an absolute value of the magnitude of coefficient as penalty term to the loss function. This tends to produce sparse models, i.e., many of its weights are zero.

L2 Regularization adds the squared magnitude of coefficient as penalty term to the loss function. This tends to spread out weight values among all input features, reducing any single large contributor.

These techniques help to reduce overfitting by adding a cost to the complexity of the model, making it simpler and thus generalizing better to unseen data.
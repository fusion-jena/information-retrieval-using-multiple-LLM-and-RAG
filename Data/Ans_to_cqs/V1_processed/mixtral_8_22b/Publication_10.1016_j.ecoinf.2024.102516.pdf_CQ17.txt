The given context does not provide information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, common regularization techniques include dropout, L1 or L2 regularization, early stopping, and data augmentation. Dropout randomly sets a fraction of input units to 0 during training time, helping to avoid overfitting. L1 and L2 regularization add penalties to the loss function, encouraging smaller weights and preventing any single weight from dominating the output. Early stopping ends the training process before convergence when validation error starts increasing, while data augmentation generates new training samples by applying random transformations to existing data.
Based on the given information, the optimization technique applied in the deep learning pipeline is Stochastic Gradient Descent (SGD). After 50,000 iterations, the learning rate was reduced following an exponential decay parameter of 0.5 by an SGD scheduler. This indicates that the SGD optimizer was used during the training phase of the deep learning models. However, there is no mention of other optimization techniques such as Adam, RMSprop, or Adagrad. Therefore, based on the provided context, only SGD has been explicitly mentioned as an optimization technique used in the deep learning pipeline.
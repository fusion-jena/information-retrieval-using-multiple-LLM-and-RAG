Unfortunately, there isn't enough information available in the given context to determine which specific hardware resources were used for training the deep learning model. However, it can be inferred from the mention of making full use of computer resources that a GPU or similar high-performance computing resource may have been utilized due to their ability to accelerate deep learning computations. Additionally, the selection of a batch size of 32 suggests that the authors considered the limitations of their hardware when designing their training process. Nevertheless, without explicit details about the hardware used, we cannot provide a definitive answer.
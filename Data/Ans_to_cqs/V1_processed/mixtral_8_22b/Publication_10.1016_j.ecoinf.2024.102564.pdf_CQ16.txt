Based on the given context, it does not explicitly mention any specific criteria used to determine when training is completed. However, we can infer from general machine learning practices that some common stopping criteria include reaching a certain number of epochs or iterations, achieving a desired level of accuracy or loss, or observing no significant improvement in validation loss over several epochs (i.e., validation loss plateau). In this case, since the authors use k-fold cross-validation and repeat the process 50 times, they might have chosen to stop training based on a predefined number of epochs or iterations rather than waiting for a validation loss plateau due to computational constraints.
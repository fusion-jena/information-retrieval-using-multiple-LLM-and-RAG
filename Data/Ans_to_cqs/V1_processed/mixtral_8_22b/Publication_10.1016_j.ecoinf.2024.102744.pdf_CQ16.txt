In machine learning models, including those mentioned in this context such as YOLOv5 and Faster RCNN, there are several criteria used to determine when training is complete. These include:

1. Validation Loss Plateau: One common criterion is observing a plateau or minimal decrease in the validation loss over multiple epochs. When the model stops improving its performance on unseen data, it may suggest that further training will not significantly improve the model's generalizability.

2. Early Stopping: To prevent overfitting, early stopping can be employed. In this approach, training is halted if the validation loss does not improve after a certain number of epochs. This helps ensure that the model remains robust and avoids memorizing the training data.

3. Learning Rate Scheduling: Adjusting the learning rate during training can help optimize convergence and prevent overshooting local minima. By reducing the learning rate once the validation loss has reached a satisfactory level, we can fine-tune the model without causing significant changes.

4. Maximum Number of Epochs: Setting a maximum number of epochs ensures that the training process terminates even if other conditions have not been met. This prevents excessive resource consumption and allows for timely evaluation of the trained model.

Based on the given context, specific details regarding these criteria for determining when training is complete are not explicitly stated. However, these general principles apply to most deep learning models, including YOLOv5 and Faster RCNN.
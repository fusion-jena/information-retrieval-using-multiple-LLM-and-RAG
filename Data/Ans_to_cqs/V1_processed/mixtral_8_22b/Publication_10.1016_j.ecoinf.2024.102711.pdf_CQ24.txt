The strategies employed to handle randomness in the deep learning pipeline include setting a fixed random seed value, using deterministic algorithms, and employing techniques such as k-fold cross-validation and ensemble methods. Setting a fixed random seed value ensures reproducibility of results across different runs of the same experiment. Deterministic algorithms eliminate the need for random initialization, thus reducing variability in outcomes. K-fold cross-validation involves splitting the dataset into k equal parts and iteratively training the model on k-1 folds while testing on the remaining fold, thereby reducing bias due to random sampling. Ensemble methods combine multiple models trained on different subsets of the data, averaging their outputs to reduce variance and improve generalization performance.

Based on the given context, there is no direct mention of specific strategies used to handle randomness in the deep learning pipeline. However, the context does discuss the use of AutoML models like AutoGluon and TabPFN, which may incorporate some of these strategies implicitly. Additionally, the context mentions the use of 5-fold cross-validation for hyperparameter tuning of RF and XGBoost models, indicating one possible strategy for handling randomness. It should be noted that the effectiveness of each strategy depends on the specific problem domain and dataset characteristics.
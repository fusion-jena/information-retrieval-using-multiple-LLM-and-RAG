In order to monitor the model performance during training, the authors employed several strategies. Firstly, they conducted an accuracy assessment, which is crucial to evaluate the reliability of the results. This involved computing various metrics such as Error of Commission (EC), Error of Omission (EO), Producer Accuracy (PA), and User Accuracy (UA) based on an error matrix derived from the accuracy assessment. EO is associated with the classified outcomes and complements PA, referring to elements omitted from their accurate classes in the classification. On the other hand, EC represents classified values anticipated to belong to a specific class but actually do not. It is supplemented by UA.

Furthermore, the authors executed an advanced accuracy assessment utilizing the Semi-Automatic Classification Plugin (SCP) functions for the Random Forest (RF) classifier. After generating a land cover classification map, they ran a post-processing tool to acquire reports from each classifier. These reports provide the pixel sum and area coverage of every land cover class.

Moreover, considering the complexity of acquiring extensive training data for the RF classifier in intricate regions with numerous categories, the authors proposed that training samples should constitute around 0.25% of the entire study region utilized by the RF classifier for large-scale classification. Once misclassifications were recognized, additional work was carried out with increased training areas incorporating the "new vineyard" category, given its distinct spectral signature after five years of growth compared to its appearance upon planting.

However, the text does not explicitly mention any real-time monitoring techniques or validation methods applied during the training process itself, such as cross-validation or early stopping. Therefore, while the authors did implement strategies to ensure the quality of the final model, the precise methodology for tracking the model's performance throughout the training phase remains unclear.
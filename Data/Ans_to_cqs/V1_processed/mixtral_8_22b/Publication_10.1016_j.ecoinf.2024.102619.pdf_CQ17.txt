Based on the given context, there is no explicit mention of specific regularization methods such as dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. However, some techniques employed in this study could help mitigate overfitting indirectly.

Firstly, the use of cross-validation (specifically, k-fold cross-validation with k=5) helps ensure that the models generalize well to unseen data. By splitting the available data into multiple folds and iteratively using different subsets for training and testing, it becomes possible to evaluate how well the trained models perform on new data.

Secondly, the choice of optimization algorithms like Adam can contribute to reducing overfitting. Adam automatically adjusts the learning rate during training, which can lead to more stable convergence and potentially better generalization.

Lastly, the implementation of early stopping based on validation error (or loss) can also aid in preventing overfitting. Early stopping involves monitoring the performance of the model on a separate validation set during training and halting the process once the validation error stops decreasing for a certain number of epochs. This prevents the model from continuing to learn noise in the training data, thereby improving its ability to generalize to new data.
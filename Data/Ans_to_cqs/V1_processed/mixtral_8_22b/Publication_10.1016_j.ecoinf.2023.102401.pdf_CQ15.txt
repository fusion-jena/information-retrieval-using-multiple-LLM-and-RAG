Unfortunately, there isn't any explicit mention or discussion about specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) within the provided context. These optimization techniques are typically employed during the training phase of deep learning models to minimize the cost function and update model parameters effectively. However, without more detailed information regarding the training procedure or hyperparameters used in this particular deep learning pipeline, it remains uncertain which optimization technique was implemented.
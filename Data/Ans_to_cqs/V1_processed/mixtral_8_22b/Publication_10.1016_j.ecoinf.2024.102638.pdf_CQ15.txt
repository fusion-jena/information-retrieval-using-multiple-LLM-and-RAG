The given context does not provide explicit information about the specific optimization techniques used in the deep learning pipeline. However, it can be inferred from the mention of 'gradient boosting' in relation to XGBoost that some form of gradient descent optimization technique might have been employed. Additionally, the reference to Kingma and Ba (2014) suggests that the Adam optimization algorithm could potentially be utilized since they introduced the Adam method for first-order gradient-based optimization of stochastic objective functions. Nevertheless, without more concrete evidence or direct statements regarding the use of these optimization techniques, one cannot definitively state their application in the deep learning pipeline.

Reference(s):
Chen, T., & Guestrin, C. (2016). Xgboost: A scalable tree boosting system. KDDâ€™16: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 785-794.

Kingma, D. P., & Ba, J. L. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Based on the given context, it does not provide information about specific regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, some techniques mentioned in the text could potentially help mitigate overfitting indirectly.

For instance, the authors conducted a sensitivity study by training models using different proportions of training data (5%, 25%, 50%, and 75%) compared to the full dataset. They repeated this experiment five times to account for the random subsampling of annotations. By limiting the amount of training data, they may have reduced the risk of overfitting due to having fewer examples for the model to memorize.

Additionally, the authors mention normalizing the RGB channels by subtracting the ImageNet mean from each channel before retraining the network using hand-annotated data. Normalization can help reduce internal covariate shift, which might contribute to better generalization and lower chances of overfitting.

Lastly, the authors briefly discuss testing data augmentation techniques like random flips and translations but found them to have little effect on the final score. While data augmentation is commonly employed to increase the diversity of training data and prevent overfitting, it seems that in this case, it did not significantly impact the model's performance.
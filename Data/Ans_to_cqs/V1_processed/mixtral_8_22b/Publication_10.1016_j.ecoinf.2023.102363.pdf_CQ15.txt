The optimization technique applied in the deep learning pipeline is Adam (Kingma and Ba, 2014). This optimizer controls gradient descent during training. The parameters set for the Adam optimizer include a learning rate of 0.001, a decay factor of 0.75, and a step size. Additionally, during fine-tuning, a cyclical learning rate of 0.0004 is used.
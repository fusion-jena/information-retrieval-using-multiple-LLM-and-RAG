In the provided context, there isn't any explicit mention of specific regularization methods such as dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. However, some strategies to address overfitting and improve deep learning performance on limited training data are discussed.

One strategy mentioned is self-supervised learning, where unsupervised methods are used to generate training data for supervised models. This method was applied to remote sensing for hyperspectral image classification [9], but it doesn't directly involve traditional regularization techniques like dropout or L2 regularization.

Another approach suggested is combining labeled data in a semi-supervised framework, allowing neural networks to learn generalized features on a larger set of training examples before fine-tuning on a smaller number of high-quality annotations [16]. While this technique might not explicitly use common regularization methods, it could help reduce overfitting by exposing the network to diverse training data.

Lastly, the text mentions that the difficulty of training deep architectures and the effect of unsupervised pretraining were studied [17]. Although this research does not discuss specific regularization methods, it suggests that unsupervised pretraining could potentially aid in preventing overfitting by improving the initialization of weights in deep learning models.
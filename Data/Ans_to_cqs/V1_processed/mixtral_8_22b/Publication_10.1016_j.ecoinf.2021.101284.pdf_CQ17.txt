The given context does not provide specific details about the regularization methods used in the deep learning pipeline for preventing overfitting. However, some commonly used regularization techniques include Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust and reducing its dependence on any single feature.

L2 Regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the magnitude of the weights. This encourages smaller weights, leading to simpler models that generalize better.

Other popular regularization techniques include L1 Regularization, early stopping, and data augmentation. These methods can help improve the performance and generalizability of deep learning models by preventing them from memorizing the training data and becoming too specialized.
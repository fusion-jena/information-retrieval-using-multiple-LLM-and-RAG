The text mentions that the Stochastic Gradient Descent (SGD) optimizer is used in the deep learning pipeline. This is one of several optimization techniques available for training neural networks. Other popular optimization algorithms include Adam, RMSprop, Adagrad, and Adadelta. Each algorithm has its own strengths and weaknesses, and the choice of which one to use depends on factors such as the specific problem being solved, the characteristics of the data, and the desired trade-off between convergence speed and computational efficiency.

Unhelpful 
The text does not provide information about any other optimization techniques besides SGD. Therefore, I cannot provide an answer regarding what other optimization techniques might be applied in the deep learning pipeline.
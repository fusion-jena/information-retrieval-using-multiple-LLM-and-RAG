The papers referenced do not explicitly mention specific regularization methods used to prevent overfitting in their deep learning pipelines. However, common regularization techniques used in deep learning include dropout, L1 regularization, and L2 regularization. Dropout randomly sets a fraction of input units to 0 during training time, which helps prevent overfitting. L1 regularization adds an absolute value of the magnitude of coefficients to the loss function, while L2 regularization adds a squared magnitude of the coefficient to the loss function. These methods help reduce overfitting by adding constraints to the complexity of the learned models.
The text does not explicitly mention the specific hyperparameters used in the deep learning model. However, it does discuss optimization algorithms like Stochastic gradient descent (SGD), Adaptive Gradient (AdaGrad), Root Mean Square Prop (RMSProp), and Adam. These optimization algorithms often involve hyperparameters such as learning rates or momentum coefficients. For instance, SGD might use a learning rate, while AdaGrad could have its own unique set of hyperparameters. Unfortunately, without further information, it isn't possible to specify exactly what hyperparameters were used in the deep learning model described in the given context.
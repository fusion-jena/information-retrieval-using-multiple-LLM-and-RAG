Before training a deep learning model, several preprocessing steps are typically involved to prepare the data. These may include:

1. Cleaning: Removing irrelevant, redundant, or incorrect data points can improve the performance of the model. For example, in the given context, removing tweets containing images in GIF format helps focus on relevant data.

2. Normalization: Scaling numeric features to a standard range ensures all features contribute equally to the final output. However, since the given context focuses on textual data, this step might not apply here.

3. Tokenization: Breaking down sentences into individual words or tokens is essential for natural language processing tasks. This allows the model to learn patterns based on word occurrences and sequences.

4. Stopword removal: Eliminating commonly occurring words such as 'is', 'and', 'the', etc., reduces noise and improves computational efficiency.

5. Lemmatization or stemming: Reducing inflected forms of a word to their root form enables better understanding of semantic relationships between words.

6. Vectorization: Converting text data into numerical vectors is necessary for feeding input into machine learning models. Techniques such as Bag of Words, TF-IDF, or Word Embeddings can be employed for vectorizing text data.
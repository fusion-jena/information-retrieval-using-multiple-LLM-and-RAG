In this study, several hyperparameters were considered for the deep learning model. These included the number of layers and nodes, learning rate, and dropout rate. Specifically, the optimization process involved comparing three architecture optimization algorithms: Random Search (RS), Hyperband, and Bayesian optimization. The hyperparameters of the DNN model were automatically optimized based on a search grid of various hyperparameters listed in Table 1. Additionally, a dropout layer was incorporated into the model to prevent overfitting and decrease statistical noise. This layer randomly eliminated different sets of neurons using a preset dropout rate, resulting in the training of distinct neural networks and reducing overfitting.
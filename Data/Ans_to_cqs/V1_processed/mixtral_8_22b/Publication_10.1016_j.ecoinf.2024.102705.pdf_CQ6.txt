Before training a deep learning model, several preprocessing steps can be applied to the input data to improve its quality and ensure better performance. Some common preprocessing steps include:

1. Normalization: This involves rescaling the input data so that they fall within a specific range, typically between 0 and 1 or -1 and 1. Normalization helps prevent any single feature from dominating the others due to differences in scales. It also improves convergence speed during optimization.

2. Scaling: Similar to normalization, scaling involves changing the range of input data but without necessarily restricting them to a fixed interval. For example, one might use standardization, where the mean value of each feature is subtracted from every sample, followed by dividing the result by the standard deviation. Scaling ensures that all features have equal importance and contributes to faster convergence.

3. Cleaning: Data cleaning involves removing noise, outliers, missing values, and irrelevant information from the input data. Noise reduction can be achieved through smoothing techniques like moving averages or Gaussian filters. Outlier detection methods such as z-score or interquartile range (IQR) can help identify extreme values that may negatively impact model performance. Missing values can either be filled using imputation strategies or removed entirely if their absence does not significantly affect the overall dataset.

4. Augmentation: Data augmentation techniques involve creating new samples by applying transformations to existing ones. Common examples include rotation, translation, zooming, flipping, and cropping. Augmentation increases the diversity of the training dataset, making the model more robust against overfitting and improving generalizability.

In the given context, the authors mention employing various data augmentation techniques, including random adjustments to image size, image flipping, cropping, and conversion. However, other preprocessing steps like normalization, scaling, or cleaning are not explicitly mentioned.
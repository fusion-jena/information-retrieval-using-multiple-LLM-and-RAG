To ensure the generalizability of their deep learning model, several measures were implemented. Firstly, they employed diverse datasets by combining various input datasets (10 variable sets) and applying different classifiers (RF, SVM, MARS) with two sets of classes (7C and 13C). As a result, they obtained 60 models from these classifications. Secondly, they utilized a 10-fold cross-validation technique during the model-building phase. This involved randomly splitting the training dataset into 10 folds, where nine folds were used for model training and one fold for testing. This process continued until all folds served as testing data. To further enhance reliability, the entire sequence was repeated twice using new random splits. Lastly, they performed stratified splitting by dividing the reference dataset into training and testing sets at a 70:30 ratio.
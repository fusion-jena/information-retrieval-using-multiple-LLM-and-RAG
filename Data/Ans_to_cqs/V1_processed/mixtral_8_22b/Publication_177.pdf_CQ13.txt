In the given context, the deep learning models were initialized with pre-trained ImageNet weights and then fine-tuned to their respective datasets. To determine the optimal hyperparameters for each of the six models, Bayesian optimization was employed instead of manual selection. This technique uses a Gaussian process model of the objective function and allows different variables to be optimized, including network section depth, batch size, initial learning rate, momentum, and regularization strength. However, only two hyperparameters were optimized in this study - batch size and initial learning rate. The batch size was varied between 1 and 32, while the initial learning rate was set between 1x10^-4 and 1x10^-2. The optimization aimed to minimize the loss on the validation dataset. Additionally, the models were trained for a maximum of 10 epochs using the stochastic gradient descent with a momentum ('sgdm') optimizer.
Based on the given context, it appears that the authors have not explicitly stated the specific criteria they use to determine when training is complete. However, there are several indications that suggest how they might be making this decision. For instance, the authors mention using graded Mean Squared Error (MSE) convergence to fine-tune their models, implying that they may be monitoring the MSE loss during training. Additionally, they present various performance metrics such as precision, recall, mean Average Precision (mAP), and training time for different epochs (i.e., 100, 150, 200, and 250). This suggests that they could be evaluating the performance of their models after each epoch and deciding whether or not to continue training based on these metrics. Furthermore, they discuss avoiding overtraining the model and preventing overfitting and underfitting situations, indicating that they likely consider factors like model complexity and generalization ability when determining if training should continue. In conclusion, while the exact criteria used to decide when training is complete are not specified, it can be inferred that the authors monitor various performance metrics, evaluate model complexity, and ensure good generalization abilities to make this determination.
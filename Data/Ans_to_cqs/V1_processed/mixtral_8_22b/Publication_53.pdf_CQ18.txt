The authors utilized an 80-20% split for training and validation datasets, respectively, to monitor the model performance during training. They employed data augmentation techniques such as vertical flipping of images, limiting overexposure, disabling zooming augmentation, and reducing memory load on the GPU. A batch size of 300 images was set, and the model was trained with a learning rate of 10^-3 for ten epochs. At epoch 5, they achieved an error rate of 0.1538 (15.38%) along with a validation loss of 0.4211 and a training loss. This suggests that the authors monitored the model performance through evaluating its error rates, validation losses, and training losses throughout the training process.
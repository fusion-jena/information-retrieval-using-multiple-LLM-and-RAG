Unfortunately, there isn't enough information available in the given context to provide a detailed explanation about how the trained deep learning model was deployed. However, based on common practices in machine learning projects, we can infer some possible methods used for deployment.

After training the deep learning models (CNNFULL and CNNFAST), they were likely serialized into a format suitable for storage or transfer. This could have been done using popular libraries like TensorFlow's SavedModel or PyTorch's torch.save(). Once saved, these models can then be loaded onto various platforms for further use.

As for platform selection, it depends on the specific requirements of the project. For example, if real-time processing is needed, such as in this case where large amounts of data are generated through passive acoustic monitoring, edge devices or cloud services optimized for low latency might be chosen. Alternatively, if computational resources are limited, lighter versions of the models may need to be created using techniques like pruning or quantization before deployment.

In conclusion, while the exact methodology for deploying the trained deep learning models cannot be determined from the provided context, typical processes include saving the models after training and selecting appropriate platforms based on project needs.
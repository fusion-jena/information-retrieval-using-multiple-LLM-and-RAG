In the given context, there isn't any explicit mention of handling randomness in the deep learning pipeline, specifically regarding random seed values. However, some strategies can be inferred based on best practices in machine learning and deep learning.

One strategy mentioned indirectly is the use of dropout regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting and improves the model's ability to generalize. In the context, it is stated that dropout was applied on both the fully connected layer and each convolutional layer, with different dropout rates. Although not directly related to handling randomness due to random seeds, dropout does introduce controlled randomness into the training process.

Another potential strategy could involve setting specific random seed values before training the model. By doing so, researchers can ensure that their experiments are reproducible, as the same initial conditions will lead to identical results across multiple runs. While this approach doesn't eliminate randomness, it allows for better control and consistency when comparing models or conducting experiments.

Lastly, techniques like cross-validation and ensemble methods can help mitigate the impact of randomness caused by factors such as random initialization of weights or shuffling of data points. Cross-validation involves splitting the dataset into several subsets and training/testing the model on various combinations of these subsets. Ensemble methods combine multiple models to produce improved predictions, effectively reducing the variance introduced by individual models. These techniques contribute to building robust and reliable deep learning pipelines.
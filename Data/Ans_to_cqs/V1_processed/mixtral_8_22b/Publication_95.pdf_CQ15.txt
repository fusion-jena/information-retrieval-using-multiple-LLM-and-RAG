In this study, the Adam optimizer is employed for training the deep convolutional neural networks. This choice is explicitly mentioned in the first paragraph of the given context. The Adam optimizer is a popular method for stochastic gradient descent optimization, which combines the advantages of AdaGrad and RMSProp methods. It uses adaptive learning rates for individual parameters, allowing for faster convergence and better handling of sparse gradients. The learning rate used in this study is set to 0.001.
Unfortunately, the given context does not provide specific information regarding the optimization techniques applied in the deep learning pipeline, such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). These techniques are commonly used during the training phase of deep learning models to update their parameters based on the computed gradients. However, without explicit mention in the provided text, it remains unknown which optimization technique was utilized in the described studies.
The given context does not provide information about any specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being applied in the deep learning pipeline. However, it mentions that a three-layer backpropagation (BP) artificial neural network was built using MATLAB R2014b. Backpropagation is a widely used algorithm for optimizing the weights and biases in a neural network during training. It involves calculating the gradient of the loss function with respect to each weight and bias by applying the chain rule, which allows for efficient computation of gradients through multiple layers of the network. Although the context does not explicitly mention the use of SGD or Adam, these optimization algorithms could potentially be used within the BP framework to update the weights and biases based on the calculated gradients.
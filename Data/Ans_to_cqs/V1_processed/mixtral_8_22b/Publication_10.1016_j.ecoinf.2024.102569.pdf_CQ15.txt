Based on the provided context, it appears that the optimization technique used in this case is not one typically associated with deep learning pipelines such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). Instead, the context suggests that the authors use the gradient ascent method to compute the L1 penalized estimates. This method involves iteratively adjusting the parameters of the model in the direction of steepest ascent of the objective function until convergence is achieved. It's important to note that while gradient descent methods like SGD and Adam are often used in deep learning models, other types of machine learning models may utilize different optimization techniques depending on their specific requirements and objectives.
The given context does not provide information about specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, some common methods include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into training and testing sets while maintaining the same distribution of target variables in both sets. Oversampling increases the frequency of minority classes by duplicating examples or creating synthetic ones, while undersampling reduces the majority class instances. Diverse data collection aims to gather a wide range of representative samples to reduce potential biases in the dataset.

In this case, it appears that the authors did not explicitly mention any specific technique to handle data bias but instead focused on training their Convolutional Neural Network (CNN) from scratch using nearly 30,000 labeled samples per class. They employed ReLU activation for convolutional layers and ELU activation for the fully connected layer, along with the ADAM optimizer and a learning rate of 0.0001. Training was conducted with a batch size of 256 over 16,290 iterations (or 90 epochs).
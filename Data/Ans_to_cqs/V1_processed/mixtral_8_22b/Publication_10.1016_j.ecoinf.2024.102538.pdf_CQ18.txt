During the training process, the model's performance was monitored through several strategies. Firstly, the learning rate was initialized at 0.001 and then decreased by a factor of 0.1 in a step-wise manner whenever the validation loss remained constant for five epochs. This indicates that the model's performance on the validation data was being tracked after every epoch, and if there was no improvement in the validation loss for five consecutive epochs, the learning rate was adjusted to prevent overfitting and improve generalization. Secondly, the model's performance was also evaluated using specific metrics such as Mean Reciprocal Rank (MRR), accuracy (ACC), and Hierarchical Distance of a Mistake (HDM). These metrics helped measure the effectiveness of the model in generating a list of potential responses to a sample of queries, determining the proportion of correctly classified items, and quantifying the degree of misclassification respectively. Lastly, the model's performance was tested on various datasets including BC-S, BC-L, LS, and LS*. Each dataset had its own unique characteristics which allowed for a comprehensive assessment of the model's ability to handle diverse scenarios.
The papers cited do not provide explicit information about the specific optimization techniques used in their respective deep learning pipelines. However, it can be inferred that common optimization techniques such as stochastic gradient descent (SGD) or adaptive moment estimation (Adam) might have been employed due to their popularity in training deep learning models. These methods help minimize the loss function during the training process, enabling the model to learn effectively from the input data. Nonetheless, without direct evidence from the sources, we cannot definitively state which optimization technique was utilized in each case.
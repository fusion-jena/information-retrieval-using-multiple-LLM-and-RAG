The deep learning model uses several hyperparameters during the training process. These include a batch size of 32, which was chosen to avoid exceeding the memory limit while also making full use of available computer resources. The optimizer used is Adam processing, and the learning rate varies with groups according to the cosine function's law, typically ranging from 0.0 to 1.0. The model undergoes 100 epochs of training using an early stopping function to cease the learning process if there is no improvement in accuracy after 20 epochs. Additionally, the loss function employed is the cross-entropy loss function.
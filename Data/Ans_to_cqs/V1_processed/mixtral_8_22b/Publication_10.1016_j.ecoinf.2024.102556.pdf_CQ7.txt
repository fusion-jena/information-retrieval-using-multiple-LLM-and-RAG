The dataset in this study was divided into five subsets using k-fold cross-validation. Each subset served as a validation set, while the remaining four subsets constituted the training set. The YOLOv7 detection model underwent five rounds of training and validation to assess its generalizability. This approach ensured that each sample was trained and validated in a different subset, which helped avoid overfitting and allowed for better assessment of the model's ability to generalize.

In machine learning and deep learning, datasets are often split into three parts: training, testing, and validation sets. The training set is used to fit the model, i.e., adjust the weights and biases of the neural network based on the error between predicted and actual values. The testing set is used to evaluate the performance of the model after training, providing an unbiased estimate of how well the model will perform on new, unseen data. Finally, the validation set is used to tune hyperparameters and prevent overfitting. It helps ensure that the model can generalize well to new data.

Cross-validation is a technique used to improve the reliability of the results obtained from a statistical analysis. K-fold cross-validation involves randomly dividing the dataset into 'k' equal-sized subsamples or folds. One fold is retained as the validation set, while the other 'k - 1' folds are used as the training set. This process is repeated 'k' times, with each fold serving as the validation set once. The average performance across all 'k' trials is then calculated. This method reduces bias and variance, making it a popular choice for evaluating machine learning algorithms.
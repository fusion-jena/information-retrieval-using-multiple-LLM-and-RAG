The given context does not provide explicit details about how the hyperparameters of the models were optimized. However, it mentions that after fitting the parameters of the different deep learning architectures and XGBoost on the training set, they were run on the test set. This suggests that some form of optimization or tuning was performed before running the models on the test set. Common methods for hyperparameter optimization include grid search, random search, Bayesian optimization, and others. Without further context, we cannot determine which specific method was used for hyperparameter optimization in this case.
In the context of deep learning pipelines, several regularization methods are employed to prevent overfitting. Overfitting occurs when a model learns the noise or outliers in the training data instead of generalizing patterns, leading to poor performance on unseen data. Two commonly used regularization methods are Dropout and L2 Regularization.

Dropout is a technique introduced by Geoffrey Hinton et al. (2012). During training, some neurons are randomly dropped out along with their incoming and outgoing connections. This helps reduce interdependent learning among neurons and encourages them to learn more robust features independently. The dropout rate determines the percentage of neurons to be ignored during each iteration.

On the other hand, L2 Regularization, also known as weight decay, adds a penalty term to the loss function based on the sum of squared weights. By adding this penalty, the optimization process tries to minimize both the original loss function and the magnitude of the weights. Consequently, smaller weights lead to simpler models, reducing the risk of overfitting.

These regularization methods can be applied individually or combined to improve the overall performance of deep learning models. However, selecting appropriate regularization techniques depends on factors such as dataset size, complexity, and desired trade-off between bias and variance.

Reference(s):
Hinton, G. E., Srivastava, N., Swersky, K., & others (2012). Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.
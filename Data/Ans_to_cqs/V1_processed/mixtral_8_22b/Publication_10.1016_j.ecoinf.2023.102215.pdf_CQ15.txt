The text mentions that TrunkNet is trained using the Adam optimizer with default hyperparameters. This means that the Adam optimization technique is being utilized in the deep learning pipeline. Adam is a popular choice due to its ability to adaptively adjust the learning rate based on the first and second moments of gradients during backpropagation. Other optimization techniques such as Stochastic Gradient Descent (SGD) or Root Mean Square Propagation (RMSProp) could have been chosen instead, but the authors opted for Adam.
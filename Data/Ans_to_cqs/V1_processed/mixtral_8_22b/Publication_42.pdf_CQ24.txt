Based on the provided context, there isn't any explicit mention of specific strategies employed to handle randomness in the deep learning pipeline, such as managing random seed values. However, some implicit strategies can be inferred from the text.

Firstly, the authors experimented with various CNN architectures and hyperparameters to find the most effective one for their application. By testing multiple configurations, they inherently accounted for variations due to random initialization or other stochastic processes within the deep learning pipeline.

Secondly, the authors utilized dropout regularization techniques to prevent overfitting. Dropout randomly sets a fraction rate of input units to 0 at each update during training time, effectively introducing randomness into the process. This helps reduce co-adaptations between neurons and improves generalization.

Lastly, the authors chose to train their final model using the Adam optimization algorithm instead of Stochastic Gradient Descent (SGD), as Adam proved to be superior for training their models. Adam incorporates momentum and adaptive learning rates, making it less sensitive to initial parameter settings and potentially reducing the impact of randomness caused by suboptimal starting conditions.

While these strategies may not directly address handling randomness related to specific aspects like random seed values, they demonstrate approaches to mitigate the effects of randomness in the overall deep learning pipeline.
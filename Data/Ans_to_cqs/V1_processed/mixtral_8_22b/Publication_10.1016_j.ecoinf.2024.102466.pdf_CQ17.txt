The text does not provide information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. Regularization methods like dropout and L2 regularization are commonly used techniques to prevent overfitting in deep learning models. Dropout randomly drops out neurons during training, which helps reduce co-dependencies between neurons and improves generalization. L2 regularization adds a penalty term to the loss function, encouraging smaller weights and preventing any single weight from dominating the output. Other common regularization methods include early stopping, data augmentation, and batch normalization. These techniques help ensure that the trained model performs well on unseen data and doesn't simply memorize the training set.
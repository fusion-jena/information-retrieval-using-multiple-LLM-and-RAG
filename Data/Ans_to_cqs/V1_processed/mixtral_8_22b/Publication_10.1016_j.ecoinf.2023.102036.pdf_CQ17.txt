The given context does not provide specific information about the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, based on general knowledge, some common regularization techniques used in deep learning include:

1. Dropout: This technique randomly drops out neurons during training, effectively preventing complex co-adaptations between neurons. It helps reduce overfitting by making the model more robust and less sensitive to small changes in input features.

2. L2 Regularization (Weight Decay): This method adds a penalty term to the loss function proportional to the square of the magnitude of all parameters. By penalizing large weights, this encourages simpler models and prevents overfitting.

3. Early Stopping: Monitoring the validation error while training and stopping when the performance starts degrading can help avoid overfitting. This approach uses the best performing model found so far instead of waiting until convergence.

4. Batch Normalization: This technique normalizes the activations of each layer, reducing internal covariance shift and improving the stability of the network. It also acts as a form of regularization, helping to prevent overfitting.

5. Ensemble Methods: Combining multiple models into one ensemble can improve overall performance and reduce overfitting. Techniques like bagging, boosting, and stacking can be employed to create ensembles.
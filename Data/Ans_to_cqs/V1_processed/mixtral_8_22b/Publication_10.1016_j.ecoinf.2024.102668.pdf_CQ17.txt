The provided context does not directly discuss specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization techniques include dropout, L1 regularization, and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust. L1 regularization adds an absolute value of the magnitude of coefficients as penalty term to the loss function, while L2 regularization adds the squared magnitude of the coefficient as penalty term to the loss function. Both L1 and L2 regularizations help prevent overfitting by adding constraints to the weights of the network. Other techniques such as early stopping, data augmentation, and batch normalization can also be used to prevent overfitting in deep learning models.
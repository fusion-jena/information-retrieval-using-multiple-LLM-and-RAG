The given text does not provide explicit information about the specific optimization techniques such as Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), etc., employed in the deep learning pipeline. However, there is mention of tuning the hyperparameters of the Convolutional Neural Network (CNN). This process often involves using optimization algorithms like SGD or Adam to minimize the loss function during model training. Without further details, one cannot definitively state which optimization technique was utilized in this case.

Final 
Without explicit information, it is impossible to determine the exact optimization techniques used in the deep learning pipeline. However, the process of tuning hyperparameters suggests that some form of optimization algorithm, possibly including SGD or Adam, may have been implemented.
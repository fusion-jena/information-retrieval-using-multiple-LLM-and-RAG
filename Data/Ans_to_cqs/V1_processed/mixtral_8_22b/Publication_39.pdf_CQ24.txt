Based on the given context, there isn't any explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, some implicit strategies can be derived from the text.

One common approach to manage randomness is using a fixed random seed value across different runs or experiments. By doing so, researchers ensure that their results are reproducible and not influenced by varying random factors. Although the context does not explicitly mention setting a specific random seed value, it states that the dataset was randomly split into training, validation, and testing sets, but the splitting was done 'fixed for all experiments and all networks.' This implies that they might have used a consistent random seed value during data partitioning to maintain consistency across multiple runs.

Additionally, the context mentions that the model was evaluated after trying out various combinations of hyperparameters like learning rates, momentum, weight decay, etc. While these adjustments do not directly address randomness, they contribute to optimizing the model performance and reducing variance caused by suboptimal configurations.

Lastly, the context discusses employing Stochastic Gradient Descent (SGD) as an optimization technique. SGD introduces randomness by selecting a subset of samples for gradient calculation instead of using the entire dataset. To mitigate the impact of this randomness, one could use techniques like mini-batch SGD, which offers a balance between full batch and stochastic methods, providing more stable convergence while still benefiting from the noise introduced by SGD.

In conclusion, although the context does not provide direct evidence of strategies addressing randomness in the deep learning pipeline, it suggests possible approaches like fixing random seeds, tuning hyperparameters, and utilizing optimization techniques like mini-batch SGD to indirectly manage randomness and improve overall model performance.
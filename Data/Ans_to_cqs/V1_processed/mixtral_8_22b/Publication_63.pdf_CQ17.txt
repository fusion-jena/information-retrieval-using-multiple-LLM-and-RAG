Based on the given information, there is no explicit mention of specific regularization methods such as dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. However, it can be inferred that some form of regularization has been implemented indirectly through the use of certain techniques and parameters during the training process.

Firstly, the choice of a relatively small batch size of 20 images could potentially act as a form of implicit regularization. Smaller batch sizes introduce more noise into the gradient estimation, which can have a similar effect to adding explicit regularization terms like weight decay or dropout.

Secondly, the use of a learning rate decay of 0.0001 also suggests that some form of regularization might be taking place. Learning rate decay reduces the learning rate over time, which can help prevent overfitting by making the optimization process more stable and reducing the chance of getting stuck in local minima.

Lastly, the fact that models were trained until the validation MAE did not further improve compared to the preceding epochs and diverged from the training MAE ('overfit') indicates that early stopping was employed as a form of regularization. Early stopping involves monitoring the performance of the model on a validation set during training and halting the training process once the validation performance stops improving. This helps prevent overfitting by avoiding unnecessary iterations that may cause the model to learn noise instead of generalizable patterns.

However, without explicit mentions of dropout, L2 regularization, or other common regularization techniques, it cannot be definitively stated what specific regularization methods were used in this particular deep learning pipeline.
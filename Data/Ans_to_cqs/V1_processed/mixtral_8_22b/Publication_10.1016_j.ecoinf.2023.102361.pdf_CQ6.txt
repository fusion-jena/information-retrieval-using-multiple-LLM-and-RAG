Before training a deep learning model, several preprocessing steps may be involved depending on the nature of the data and the specific requirements of the model. In the given context, two main preprocessing steps are mentioned:

1. Data splitting: The datasets are randomly divided into three parts for training, validation, and testing. This is done to ensure that the model generalizes well to unseen data. In this case, 60% of the data is used for training, 20% for validation, and the remaining 20% for testing.

2. Image patch resizing: All the patches of images are resized to a resolution of 256 x 256 pixels before being fed into the deep learning models. This step ensures that the input dimensions match the expected input shape of the models. It also helps in maintaining consistency across different images and facilitates efficient processing during training.

Other common preprocessing techniques not explicitly mentioned in the given context but often employed in deep learning include:

1. Normalization: Scaling pixel values to a standard range (usually between 0 and 1 or -1 and 1) to improve convergence speed and stability during training.

2. Augmentation: Applying various transformations such as rotation, flipping, zooming, etc., to increase the diversity of the training set and prevent overfitting.

3. Cleaning: Removing noise, outliers, or irrelevant information from the dataset to enhance the quality of the input data.
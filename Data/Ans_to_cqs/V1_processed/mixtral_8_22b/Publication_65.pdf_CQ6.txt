Before training a deep learning model, several preprocessing steps are typically involved. These steps aim to prepare the data for optimal use by the model. Some common preprocessing techniques include:

1. Normalization: This involves rescaling the values of the input features so they fall within a specific range, such as between 0 and 1 or -1 and 1. Normalization helps ensure that no single feature dominates the others due to its scale, improving the model's ability to learn patterns in the data.

2. Scaling: Similar to normalization, scaling involves transforming the values of the input features to bring them into a similar range. However, unlike normalization, scaling does not necessarily rescale the values to fit within a specific range. Instead, it aims to reduce the impact of outliers or extreme values in the data.

3. Cleaning: Data cleaning involves identifying and correcting errors, inconsistencies, or missing values in the data. This step ensures that the model receives accurate and reliable information, reducing the risk of poor performance or incorrect predictions.

4. Augmentation: Data augmentation involves creating new examples by applying various transformations to existing ones. For instance, images can be rotated, flipped, or zoomed to create new variations. Data augmentation helps increase the size and diversity of the training dataset, making the model more robust and generalizable.

Based on the given context, the researchers employed some of these preprocessing steps before training their deep learning models. Specifically, they used rotation, noise, and flip for data augmentation, allowing them to expand their dataset by six times. Additionally, they utilized an exponential decay parameter of 0.5 by an SGD scheduler to reduce the learning rate after 50,000 iterations. While the context does not explicitly mention normalization, scaling, or cleaning, these steps could still have been part of their preprocessing pipeline.
The performance of the deep learning model is evaluated using the Average Precision (AP) metric. This metric is calculated based on the Intersection over Union (IoU) scores between the predicted bounding boxes and the ground truth annotations. A higher IoU score indicates better overlap between the prediction and the actual object. The evaluation method considers all predictions as positive that have an IoU of at least 0.5, and the average precision at this level of IoU is called AP50. Similarly, the average precision with a minimum IoU of 0.75 is called AP75. Additionally, AP is also calculated as the average over 10 IoU levels from 0.5 to 0.95 with a step size of 0.05.
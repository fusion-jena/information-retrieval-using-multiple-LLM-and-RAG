The text does not provide information on specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions that deep neural networks have been found to improve prediction accuracy for estimating various leaf parameters. Regularization techniques commonly used in deep learning pipelines include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training, which helps prevent overfitting by making the network more robust. L2 regularization adds a penalty term to the loss function, discouraging large weights and promoting simpler models, thus reducing overfitting. Other regularization methods include early stopping, weight decay, and batch normalization. These techniques help prevent overfitting by introducing constraints on the complexity of the model, allowing it to generalize better to unseen data.
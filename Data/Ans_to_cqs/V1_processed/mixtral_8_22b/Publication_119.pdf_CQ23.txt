To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the use of dropout and batch normalization layers helped reduce overfitting of the model on the training data and improved its performance on unseen data. Dropout layers randomly set a fraction of input units to zero during training which helps prevent complex co-adaptations on training data. Batch normalization, on the other hand, standardizes the inputs to a layer for each mini-batch, allowing each layer of a network to learn by itself a little more independently of other layers. This also has a slight regularization effect, reducing generalization error. Secondly, the hyperparameters of the network were fine-tuned to achieve optimal testing accuracy and kappa score. The learning rate and batch size were adjusted to their optimum values of 0.007 and 48, respectively. Lastly, the model was evaluated using independent testing data, ensuring that the same data was used for all models, thereby providing a fair comparison of their performances. However, there is no explicit mention of diverse datasets, cross-validation or stratified splitting being employed in the given context.
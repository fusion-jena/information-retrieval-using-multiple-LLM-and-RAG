The provided context does not contain information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, common regularization methods include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust by preventing complex co-adaptations on training data. L2 regularization adds a penalty term to the loss function, encouraging smaller weights and thus reducing overfitting. Other techniques such as early stopping or weight decay can also be employed. Without further details from the original source, it cannot be determined if these methods were utilized in this particular case.
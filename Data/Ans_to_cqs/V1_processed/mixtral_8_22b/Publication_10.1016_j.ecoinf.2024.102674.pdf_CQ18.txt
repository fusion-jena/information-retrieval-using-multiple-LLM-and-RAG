The authors used several machine learning models such as Random Forests (RFs), Gradient Boosting Machines (GBMs), Generalized Additive Models (GAMs), and Artificial Neural Networks (ANNs) to perform spatial interpolation and downscaling tasks. To evaluate the performance of these models, they employed various metrics like KGE scores, percent bias (pbias), Mean Absolute Error (MAE), and Root Mean Square Error (RMSE). They also monitored the variable importance for each algorithm and each month after model calibration. However, there isn't explicit information about a specific strategy implemented to monitor the model performance during training. It can be assumed that standard practices such as validation techniques (like k-fold cross-validation) and early stopping methods might have been applied based on the nature of the problem and the chosen algorithms. But without further details from the given context, it cannot be confirmed.
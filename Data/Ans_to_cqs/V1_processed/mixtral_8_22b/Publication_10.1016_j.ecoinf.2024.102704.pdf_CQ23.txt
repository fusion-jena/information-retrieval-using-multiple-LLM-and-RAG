To ensure the generalizability of the deep learning model, several measures were taken. Firstly, a diverse dataset was used, which consisted of targeted fish species. This dataset was then divided into training and validation sets in an 8:2 ratio using a random selection process. This approach helps prevent overfitting and ensures that the model can perform well on unseen data. Additionally, the use of different scales (n, s, m, and l) for the network depth and width of the model allows it to meet the requirements of various scenarios and tasks. However, increasing the model depth also leads to changes in the number of parameters, amount of computation, and detection speed, requiring higher demands on the hardware configuration. To address these issues, efforts were made to reduce memory access, which is crucial for enhancing model performance. This was achieved by reconfiguring the computation sequence of 'convolutional layer + batch normalization + activation layer' on NVIDIA TESLA V100 GPUs. This resulted in reductions in memory access for the ResNet-50, Inception V3, and DenseNet models, leading to increases in their computational efficiency. Furthermore, lowering the memory access in models can accelerate operations across the three main layers of CNNs, thus fully utilizing the model's strengths and more effectively utilizing the computational resources provided by the embedded devices.
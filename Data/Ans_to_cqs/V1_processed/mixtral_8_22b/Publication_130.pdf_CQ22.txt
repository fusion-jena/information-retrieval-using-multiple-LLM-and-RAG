In evaluating the performance of their deep learning model, Ferreira et al. primarily focused on two metrics: accuracy and loss. Accuracy refers to the proportion of correct predictions made by the model out of all the predictions made. It provides a general idea of how often the model is right. Loss, on the other hand, measures the difference between the predicted output and the actual output. In essence, it quantifies the cost of error in prediction. A decrease in loss indicates improved performance of the model.

To calculate these metrics, they utilized a separate validation dataset, distinct from both the training and testing datasets. This validation dataset served to monitor the learning progress of the neural network and prevent overfitting, where the model performs exceptionally well on the training data but poorly on unseen data due to memorization rather than true understanding. By employing this strategy, they ensured that their model was robust and capable of making accurate predictions even when faced with previously unseen data.
The hyperparameters used in the deep learning models include the solver type, base learning rate, learning rate policy, momentum, weight decay, gamma, batch size, and the number of epochs. Specifically, stochastic gradient descent was used as the solver type, with a base learning rate of 0.005. The learning rate policy was step, decreasing by a factor of 10 every 10,000 iterations. The momentum was set to 0.9, while the weight decay was set to 0.0005. The gamma value was set to 0.1. The batch size varied depending on the deep learning architecture; it was set to 24 for GoogLeNet and 100 for AlexNet. Each experiment ran for a total of 30 epochs, where one epoch was defined as the number of training iterations required for the neural network to complete a full pass of the entire training set.
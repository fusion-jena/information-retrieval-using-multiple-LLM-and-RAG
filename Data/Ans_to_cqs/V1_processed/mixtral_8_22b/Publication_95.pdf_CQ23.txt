To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the data was divided into three distinct subsets: a training set, a validation set, and a test set. This division allowed for the optimization of parameters using the training set, monitoring of performance during training using the validation set, and final evaluation of the network using the test set. It was emphasized that samples within each set should be independent and maintain similar class distributions to prevent any potential bias or overfitting.

Secondly, instead of simply randomly sampling images from the entire dataset, which could lead to information leaking between datasets due to close proximity capturing and overlapping fields of view, another approach was adopted. Although not explicitly stated, this suggests that some form of stratified splitting might have been employed to ensure diverse representation within each subset.

Thirdly, all networks were trained for 100 epochs using the Adam optimizer with a learning rate of 0.001 and cross-entropy loss. The batch size varied based on the input image resolution, with larger batches for smaller image sizes. After each epoch, the network was evaluated on the validation set, and the model with the highest accuracy on the validation set during training was saved for further analysis.

Lastly, before training the networks, the training set was balanced by upsampling the images from underrepresented classes through repetition to match the number of images of the most represented class, "No species". This step ensured that no single class dominated the training process, thereby improving the model's ability to generalize well to unseen data.
In this study, the authors propose a system for improving bird sound classification by employing various Convolutional Neural Network (CNN)-based models. They use three types of Time-Frequency Representations (TFRs): Mel-spectrogram, harmonic-component based spectrogram, and percussive-component based spectrogram. These representations serve as inputs to their respective CNN architectures, namely Mel-CNN, Harm-CNN, and Perc-CNN. Additionally, they utilize a different deep learning architecture called SubSpectralNet, which is also a CNN-based model.

The feature learning component of their proposed model adheres to a VGG-style network, which was previously applied for classifying acoustic scenes. This network employs Adam optimization with a learning rate of 10^-4, categorical cross-entropy as its loss function, a batch size of 64 samples, and trains for 200 epochs.

To enhance the classification performance, the researchers combine multiple CNN-based models with distinct input features and architectures. Their best fusion model achieves a balanced accuracy of 86.31% and a weighted F1-score of 93.31%.

Therefore, it can be concluded that the primary deep learning models used in this pipeline are Convolutional Neural Networks (CNN), specifically the VGG-style network and SubSpectralNet.
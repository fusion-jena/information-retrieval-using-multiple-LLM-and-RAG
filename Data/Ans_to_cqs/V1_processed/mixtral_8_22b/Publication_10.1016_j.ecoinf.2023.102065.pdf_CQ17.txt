The papers provided do not explicitly mention specific regularization methods used to prevent overfitting in their respective deep learning pipelines. However, some common regularization methods used in deep learning include dropout, L1 and L2 regularization, early stopping, and data augmentation. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust to small changes in input data. L1 and L2 regularization add penalties to the loss function based on the weights of the connections between nodes in the network, encouraging smaller weight values and reducing overfitting. Early stopping involves monitoring the validation error during training and stopping when it starts to increase, indicating that the model has begun to overfit. Data augmentation involves creating new training examples by applying transformations to existing ones, increasing the size and diversity of the training data and helping to reduce overfitting.
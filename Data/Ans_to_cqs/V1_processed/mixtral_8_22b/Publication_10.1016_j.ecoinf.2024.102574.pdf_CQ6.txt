Before training a deep learning model, several preprocessing steps are typically involved to ensure the quality and consistency of the input data. These steps can include:

1. Data Cleaning: This involves removing or correcting any errors, inconsistencies, or missing values in the dataset. It also includes handling outliers which might affect the performance of the model.

2. Normalization/Standardization: This is done to bring all features to the same scale, ensuring that no feature dominates the others due to its larger range. Common methods include Min-Max Scaling, where data is scaled between 0 and 1, and Standardization, where data is centered around the mean with standard deviation of one.

3. Feature Selection/Extraction: This step involves selecting relevant features or creating new ones from existing data. Irrelevant features can negatively impact the model's performance, while useful features can improve it. Techniques like Principal Component Analysis (PCA) can help reduce dimensionality and extract important features.

4. Data Augmentation: This technique is used to increase the amount of training data by applying transformations such as rotation, zooming, flipping, etc. It helps prevent overfitting and improves the model's ability to generalize.

5. Encoding Categorical Variables: Since deep learning models work with numerical inputs, categorical variables need to be converted into numerical form. One common method is One-Hot encoding, where each category is represented as a binary vector.

6. Splitting Dataset: Before training, the dataset is usually split into three parts - training set, validation set, and test set. The training set is used to train the model, the validation set is used to tune hyperparameters and avoid overfitting, and the test set is used to evaluate the final model's performance.
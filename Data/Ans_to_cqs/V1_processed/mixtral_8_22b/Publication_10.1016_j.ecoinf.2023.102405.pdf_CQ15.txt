Based on the provided context, it is not explicitly mentioned which specific optimization technique is used in the deep learning pipeline. However, there are several commonly used optimization techniques in deep learning models such as stochastic gradient descent (SGD), adaptive moment estimation (Adam), root mean square propagation (RMSProp), and Adaptive Gradient Algorithm (AdaGrad). These optimization algorithms help minimize the cost or loss function during the training process by adjusting the weights and biases of the neural network. The choice of optimization algorithm depends on various factors like convergence speed, computational efficiency, and generalization performance. Therefore, without further information, we cannot determine which optimization technique is being employed in this particular case.
The text does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions that CNN architectures with many parameters (more than 20,000,000) such as ResNetV50 and InceptionNetV3 resulted in overfitting, indicating that these models may have benefited from additional regularization techniques. Common regularization methods include dropout, L1 or L2 regularization, early stopping, and data augmentation. Dropout randomly sets a fraction rate of input units to 0 at each update during training time, helping to prevent overfitting. L1 or L2 regularization adds a penalty term to the loss function, encouraging smaller weight values and reducing complexity. Early stopping ends the training process before convergence if the validation error starts to increase, preventing overfitting due to excessive training. Data augmentation generates new training samples by applying random transformations to existing ones, effectively expanding the dataset and improving generalization. Without explicit mention of the employed regularization methods, one can assume that some combination of these techniques might have been applied to mitigate overfitting in the described deep learning pipelines.
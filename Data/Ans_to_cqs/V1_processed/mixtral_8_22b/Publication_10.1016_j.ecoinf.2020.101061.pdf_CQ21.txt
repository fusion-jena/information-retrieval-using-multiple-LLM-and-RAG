The text does not provide explicit information about specific postprocessing steps involved after the model training. However, based on common practices in machine learning, some possible postprocessing steps could include:

1. Evaluation metrics: Calculating various evaluation metrics such as accuracy, precision, recall, F1 score, ROC curves, etc., to measure the performance of the model.

2. Confusion Matrix: Creating a confusion matrix to visualize the performance of the algorithm and understand where it might be making mistakes.

3. Saliency Maps: Generating saliency maps or heatmaps to highlight important features contributing to the model's predictions. This helps in understanding how the model makes decisions and identifying potential biases.

4. Cross-validation: Performing cross-validation techniques like k-fold cross-validation to ensure the robustness and reliability of the model.

5. Hyperparameter tuning: Adjusting hyperparameters to optimize the model's performance further.

In this case, the authors have mentioned performing a sensitivity study by training models using different proportions of training data and running evaluations five times to account for random subsampling of annotations. They also mention testing a model training using pretraining data from all other sites but hand annotations from the same site as the evaluation data ('transfer-learning'). These indicate that they may have employed some form of cross-validation and possibly hyperparameter tuning during their experiments.
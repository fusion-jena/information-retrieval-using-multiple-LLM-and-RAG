The text does not provide specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions that the study uses a randomized grid search method to select hyperparameters, which helps strike a balance between model complexity and the dataset, reducing the risk of overfitting. Randomized grid search is a technique where a subset of the parameter grid is randomly selected to explore instead of exhaustively searching through all possible combinations of hyperparameters. This approach reduces computational cost, especially when dealing with a large number of hyperparameters, and has been shown to find equally or more optimal solutions than an exhaustive grid search, particularly when some hyperparameters are more influential than others. Although the text does not mention any specific regularization techniques, common methods used to prevent overfitting in deep learning include dropout, L1/L2 regularization, early stopping, and data augmentation. Dropout involves randomly dropping out neurons during training to reduce co-adaptation and improve generalization. L1/L2 regularization adds a penalty term to the loss function to discourage large weights and encourage simpler models. Early stopping stops training once validation error starts increasing, while data augmentation increases the size of the training dataset by applying transformations to existing samples.
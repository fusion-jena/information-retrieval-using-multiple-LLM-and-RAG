To ensure the generalizability of their deep learning model, several measures were implemented. Firstly, they utilized diverse datasets for training. Specifically, Models 4, 5, and 6 were trained using their own data, while Models 1, 2, and 3 were trained with the MNIST dataset of handwritten digits. This approach allowed them to evaluate the performance of their model across different types of data. Secondly, they employed k-fold cross-validation to determine the optimal training-test split for their model. K-fold cross-validation involves dividing the data into 'k' subsets or folds, where one fold is used as the validation set and the remaining 'k-1' folds are used for training. This process is repeated 'k' times, ensuring that every subset serves as the validation set once. By doing so, they could assess the robustness of their model against variations in the training and testing data. Lastly, functions from the Python library scikit-learn were used to split the data and analyze the outcomes of each trained model. These functions likely included methods for stratified splitting, which ensures that the distribution of classes in the training and testing sets remains consistent with the original dataset. Overall, these strategies helped enhance the reliability and applicability of their deep learning model.
The given context does not provide information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, common regularization techniques include dropout, which randomly sets a fraction rate of input units to 0 at each update during training time, helping reduce overfitting by providing a way of approximately combining exponentially many different neural network architectures efficiently, and L2 regularization, also known as weight decay, which adds a penalty equal to half the sum of squares of all weights to the loss function being optimized. Other popular regularization methods include early stopping, data augmentation, and batch normalization.
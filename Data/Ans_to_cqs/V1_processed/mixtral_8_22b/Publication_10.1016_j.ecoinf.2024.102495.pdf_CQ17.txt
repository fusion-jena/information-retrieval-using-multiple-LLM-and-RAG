Regularization methods are techniques used to prevent overfitting in deep learning pipelines by adding constraints to the learning process. Two common regularization methods mentioned in the given context are dropout and using different optimizers like RMSProp, Adam, and SGD.

Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust and less sensitive to specific weights in the network. In the given context, we see that some models use dropout while others do not. For example, Model 1 does not use dropout, whereas Model 9 uses dropout.

Using different optimizers can also help prevent overfitting. Optimizers such as RMSProp, Adam, and SGD adjust the learning rates of individual parameters dynamically based on their historical behavior. By doing so, they help improve convergence speed and reduce overfitting. In the given context, we observe that various combinations of optimizers and learning rates have been tested across multiple models.

Although not explicitly mentioned in the given context, another popular regularization method is L2 regularization, also known as weight decay. It adds a penalty term to the loss function proportional to the square of the magnitude of the weights, encouraging smaller weights and thus reducing overfitting.
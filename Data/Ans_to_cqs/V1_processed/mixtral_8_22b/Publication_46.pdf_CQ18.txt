In this study, the strategy implemented to monitor the model performance during training involves splitting the dataset into two parts - 70% for calibration and 30% for testing. The calibration set is used to train the model, while the testing set is used to evaluate its performance. To assess the performance of the model, the Area Under Curve (AUC) metric is employed. This metric measures the ability of the model to distinguish between positive and negative instances, where higher AUC values indicate better performance. Additionally, the study uses Receiver Operating Characteristic (ROC) curves to visualize the tradeoff between true positive rate and false positive rate at different classification thresholds. By evaluating the model's performance on both the calibration and testing sets, researchers can ensure that the model generalizes well to new data and avoid overfitting.
Based on the given context, it appears there isn't enough information to determine which specific optimization techniques were used in the deep learning pipeline for this particular study. However, common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), Root Mean Square Propagation (RMSProp), and others. These methods help minimize the loss function during the training process by adjusting the weights and biases of the neural network. Without further details about the deep learning pipeline used in this research, we cannot definitively state which optimization technique was employed.
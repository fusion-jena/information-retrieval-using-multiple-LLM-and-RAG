According to the given context, all deep learning-based models exploited in this work were implemented using TensorFlow, a Python framework designed to enable efficient analysis and implementation of deep learning with Graphics Processing Units (GPUs). All experiments conducted here were performed on a 64-bit Intel i7-8700K@3.70GHz CPU workstation, 64 GB memory, and NVIDIAÂ® GTX 1080 GPU with 12GB of memory, under a 10.0 CUDA version. Therefore, the hardware resources used for training the deep learning model include a CPU workstation, memory, and a GPU.
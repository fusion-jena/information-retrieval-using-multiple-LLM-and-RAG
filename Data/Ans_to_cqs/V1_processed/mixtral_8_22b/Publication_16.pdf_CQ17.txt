To prevent overfitting in the deep learning pipeline, the authors have employed several regularization techniques. One such method is Dropout, which randomly sets a fraction of input units to 0 at each update during training time, helping to prevent complex co-adaptations on training data. In this paper, a dropout value of 50% has been applied in the first three layers of the Multi-Layer Perceptron (MLP). Additionally, although not explicitly mentioned, it can be assumed that other common practices like weight decay or L2 regularization might have been implemented implicitly through the choice of optimization algorithms such as Adam, which often includes these features by default. However, without explicit mention, we cannot confirm if any additional regularization methods beyond dropout were utilized.
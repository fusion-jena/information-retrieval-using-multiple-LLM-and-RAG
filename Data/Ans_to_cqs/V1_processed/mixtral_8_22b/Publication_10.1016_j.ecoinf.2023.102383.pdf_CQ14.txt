The text does not provide explicit information about how the hyperparameters of the model were optimized using methods such as grid search or random search. However, it mentions that the value of lambda (λ), which is a crucial hyperparameter, is empirically determined and set to be 320 during the training phase. This suggests that some form of experimentation or trial-and-error approach might have been used to find the optimal value of λ. Additionally, the concept of hard negative mining is mentioned, which involves focusing on challenging negative examples during the training process to improve the model's ability to distinguish between objects and background regions. While this technique could potentially help optimize certain aspects of the model, it does not directly address the optimization of specific hyperparameters like λ. In summary, based on the given context, there is no clear indication of how the hyperparameters of the model were optimized using techniques like grid search or random search.
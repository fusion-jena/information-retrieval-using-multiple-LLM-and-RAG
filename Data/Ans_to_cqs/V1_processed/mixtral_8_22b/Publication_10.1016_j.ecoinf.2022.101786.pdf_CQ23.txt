To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the resolution of the input images was maintained at 704 Ã— 704 pixels, allowing the model to learn from detailed features without being overwhelmed by high-resolution information. Secondly, the batch size was set to 64, striking a balance between computational efficiency and maintaining enough diversity within each batch to prevent overfitting. Thirdly, the model was trained using 32 subdivisions, further increasing the variety of inputs presented during training. Lastly, the model was trained for up to 6000 iterations, providing ample opportunity for the model to learn patterns and relationships within the data while avoiding overfitting. Additionally, the authors employed manual annotations to create diverse training, validation, and testing datasets, ensuring that the model could perform well across various scenarios. To monitor the risk of overfitting, the authors evaluated the model's performance on both the training and validation sets throughout the training process, selecting the optimal iteration where the performance on these independent datasets converged. This approach allowed them to strike a balance between specializing the model for their specific task and maintaining its ability to generalize to new, unseen data.
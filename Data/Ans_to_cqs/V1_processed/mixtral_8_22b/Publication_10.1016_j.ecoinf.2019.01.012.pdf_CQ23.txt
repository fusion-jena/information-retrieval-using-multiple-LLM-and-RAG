The authors ensured the generalizability of their deep learning model through several measures. Firstly, they prevented spatial autocorrelation between neighboring tiles by collecting all tiles from the training photographs in one folder, shuffling them, and then splitting them into training, validation, and test sets. This approach helps to avoid any potential bias or correlation between adjacent tiles. Secondly, they employed a validation set during training for fine-tuning hyperparameters and model selection. After determining the best model setup, they assessed its prediction error on the test set. Thirdly, they increased the number of sample tiles by performing data augmentation on the training set. Data augmentation involves applying slight transformations to the original data to artificially increase the number of training samples. Lastly, they opted not to use pre-trained models due to the specific nature of their classification problem and the availability of nearly 30,000 labeled samples per class. Instead, they trained their own network from scratch. These measures collectively contribute to enhancing the robustness and generalizability of the deep learning model.
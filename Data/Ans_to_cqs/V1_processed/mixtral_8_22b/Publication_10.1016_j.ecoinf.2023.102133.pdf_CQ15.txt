The text mentions that the Adam solver was used for optimizing the performance of the model. Adam, short for Adaptive Moment Estimation, is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based on training data. It computes adaptive learning rates for different parameters from estimates of first and second moments of the gradients. This allows it to combine the advantages of two other extensions of stochastic gradient descent: Adagrad and RMSprop. Adam has been shown to work well in practice and is often the default choice for many deep learning applications due to its fast convergence and good generalization properties.
The text does not explicitly state the specific criteria used to determine when training is complete. However, some clues can be found in the description of the training process. The network is trained with Adam optimizer, a learning rate of 10^-4, and categorical cross entropy as the loss function. It also mentions that the batch size is 64 samples and the network is trained with 200 epochs. These details suggest that the number of epochs might be one criterion used to decide when training should stop. In machine learning, an epoch refers to one cycle through the full training dataset. Therefore, training the network for 200 epochs means going through the entire dataset 200 times. While this information doesn't provide a definitive answer regarding the exact stopping criteria, it suggests that the authors may have chosen to train their model for a fixed number of epochs rather than waiting for a validation loss plateau or other similar conditions.
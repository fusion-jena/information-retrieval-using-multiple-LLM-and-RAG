In the provided context, the optimization technique applied in the deep learning pipeline is the Adam optimizer. This optimizer is used with a learning rate of 0.001 to optimize all the parameters. Additionally, the early stopping technique is applied to avoid overfitting, based on the model's performance on the validation set.
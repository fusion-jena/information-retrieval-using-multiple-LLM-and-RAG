The papers mentioned do not directly address handling randomness in the deep learning pipeline. However, some common strategies used to manage randomness include setting fixed seeds for reproducibility, using ensemble methods or cross-validation to reduce variance, and employing regularization techniques like dropout or weight decay to prevent overfitting. These strategies help ensure consistent results and improve model performance despite inherent randomness in the training process.
Before training a deep learning model, several preprocessing steps can be applied to the input data. These may include:

1. Normalization: This involves rescaling the features so they have zero mean and unit variance. It is often done to ensure that different features contribute equally to the final output.

2. Scaling: Similar to normalization, this step involves transforming the feature values to a specific range, such as between 0 and 1 or -1 and 1. Scaling helps improve convergence during optimization.

3. Cleaning: This process removes any missing or inconsistent data points from the dataset. Inconsistencies could arise due to errors in measurement, recording, or transmission.

4. Encoding categorical variables: Categorical variables need to be converted into numerical form before feeding them into a deep learning algorithm. One common method is one-hot encoding, which converts each category into a binary vector.

5. Feature extraction/selection: Depending on the complexity of the problem, it might be necessary to extract relevant features from raw data or select only those features that significantly impact the target variable.

In the given context, no explicit mention of these preprocessing steps has been made. However, based on general practices in deep learning, some or all of these steps would likely have been carried out prior to training the candidate models.
The text does not provide explicit information about the specific preprocessing steps taken before training the deep learning models. However, based on common practices in machine learning and deep learning, some typical preprocessing steps include:

1. Data Cleaning: This involves removing or correcting any inconsistencies, errors, or missing values within the dataset. It may also involve handling outliers or irrelevant data points.

2. Normalization/Standardization: These techniques help scale the input features into a similar range, ensuring that no single feature dominates the learning process due to its larger numerical value. Common methods include Min-Max Scaling, Z-Score Standardization, and Unit Vector Normalization.

3. Data Augmentation: To increase the size of the dataset artificially, various transformations like rotation, flipping, zooming, cropping, etc., can be applied to the original images. This helps improve the robustness and generalizability of the model.

4. Feature Extraction: Depending on the complexity of the problem, domain knowledge might be required to identify relevant features that could aid the learning process. In case of image data, Convolutional Neural Networks (CNNs) automatically perform feature extraction during the training phase.

5. Encoding Categorical Variables: When dealing with categorical variables, they need to be converted into numerical form using encoding schemes such as One-Hot Encoding, Label Encoding, or Binary Encoding.

Although these preprocessing steps are commonly employed, the exact methodology depends on the nature of the dataset and the requirements of the specific deep learning algorithm being utilized.
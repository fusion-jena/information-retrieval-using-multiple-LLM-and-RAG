Based on the given context, it appears that the optimization technique used in the deep learning pipeline is AdamW with parameters (0.9, 0.999). AdamW is a variant of the popular Adam optimizer, which combines the advantages of both Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). It uses adaptive estimates of lower-order moments to update the weights and biases of the neural network during backpropagation. The parameters (0.9, 0.999) correspond to the exponential decay rates for the first and second moment estimates, respectively. These values control how much influence past gradients have on the current gradient update. Overall, AdamW is known for its fast convergence and good generalization performance, making it a popular choice for many deep learning applications.
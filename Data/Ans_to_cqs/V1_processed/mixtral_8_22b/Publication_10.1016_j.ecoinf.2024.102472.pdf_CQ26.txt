Based on the given context, there is no direct mention of specific techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, some relevant information can be found regarding the use of Extreme Gradient Boosting (XGBoost) which is known for minimizing bias error within and between spectral predictors. XGBoost builds sequential models to reduce errors of predictors that could potentially lower estimation accuracy. It does this by increasing the weights of all weak learners (predictors) and decreasing the weights for strong learners in the model. This process is repeated until the error is reduced, and the data is correctly predicted. Additionally, XGBoost allows for model tuning through various hyperparameters including ntree, tree node size, and gradient boosting iteration. These parameters help improve the performance of the model and may indirectly contribute to addressing data bias.
Based on the given context, there isn't explicit information about specific preprocessing steps taken before training the deep learning models mentioned. However, some common preprocessing techniques often applied in machine learning include normalization, scaling, and cleaning. Normalization involves transforming features into a similar scale so that no single feature dominates the others during modeling. Scaling refers to rescaling numerical variables to a certain range or standardizing them by removing the mean and dividing by the standard deviation. Cleaning involves handling missing values, outliers, and irrelevant data points.

In the context provided, we see that a batch normalization layer was added after hidden layer 2 in the DNN model, which helps in reducing internal covariate shift and improving convergence speed. This could be considered a form of normalization applied during the training process rather than as a separate preprocessing step. Furthermore, dropout layers were also included to prevent overfitting, but these do not directly relate to typical preprocessing tasks like normalization, scaling, or cleaning.

Overall, while the context does not provide direct information regarding preprocessing steps, it highlights the use of batch normalization and dropout layers in the DNN model, which contribute to improved performance and reduced overfitting.
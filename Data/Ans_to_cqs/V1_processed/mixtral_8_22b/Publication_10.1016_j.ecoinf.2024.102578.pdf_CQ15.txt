The text mentions that a one-cycle learning rate policy was employed during the training process. This policy is not an optimization technique itself but rather a method for scheduling the learning rate over the course of training. However, it can be used in conjunction with various optimization algorithms such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). Unfortunately, the specific optimization algorithm used in this case is not explicitly stated in the given context. Therefore, based on the information available, we cannot definitively determine which optimization technique was applied in the deep learning pipeline.
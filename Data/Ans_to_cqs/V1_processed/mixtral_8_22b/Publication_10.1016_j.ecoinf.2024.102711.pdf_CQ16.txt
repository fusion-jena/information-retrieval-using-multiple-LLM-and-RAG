In the given context, it appears that the criterion used to determine when training is complete is related to the use of k-fold cross-validation for hyperparameter tuning. Specifically, during each iteration of the k-fold cross-validation process, one group of data is held out as the tuning dataset while the remaining data is used for training. After training, the model's performance is evaluated using the tuning dataset. This process is repeated for each fold, with the final performance metric being the average performance across all folds. However, the specific stopping criterion for training within each fold is not explicitly stated in the provided context. It could be based on various factors such as reaching a certain level of performance or convergence, or after a fixed number of iterations. Without additional information, it cannot be definitively determined what exact stopping criterion is used.
The authors address the issue of randomness in their deep learning pipeline by employing several strategies. Firstly, they use a random subset of 5256 pictures (80% of the total dataset) for training all algorithms. This introduces some level of randomness into the selection of images used for training. Secondly, they repeat the training-testing process 10 times for each algorithm to ensure performance consistency. By doing this, they can observe how well the algorithms perform across different sets of randomly selected images. Lastly, they include a'replicate' random effect in their models, which further accounts for any variability introduced by the randomness in the image selection process. These strategies help mitigate the impact of randomness in the deep learning pipeline and provide more robust results.
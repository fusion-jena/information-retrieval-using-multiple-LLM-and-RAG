In the given context, two optimization techniques are mentioned for different deep learning pipelines. For the EPDNet, the Adam optimizer is employed during training with a learning rate of 0.0001. This choice of optimizer suggests that the authors wanted to leverage the benefits of adaptive learning rates offered by Adam, which adjusts the learning rate based on the historical gradients.

For the SegFormer model used in measuring photo GVI, the AdamW optimizer is utilized with the momentum parameter set to (0.9, 0.999) and epsilon set to 10^-7. AdamW is a variant of Adam that includes weight decay, which helps regularize the model and improve generalization performance. By using this optimizer, the authors aim to take advantage of both adaptive learning rates and weight decay to achieve optimal results.

Additionally, a cosine annealing schedule is employed for the SegFormer model, which gradually decreases the learning rate over time according to a cosine curve. This technique allows the model to explore more widely early in training and focus on fine-tuning later, potentially improving convergence and final accuracy.
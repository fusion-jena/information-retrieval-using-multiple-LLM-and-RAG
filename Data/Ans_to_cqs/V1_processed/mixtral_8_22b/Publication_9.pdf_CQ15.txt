The overarching goal of training a neural network is to iteratively minimize the error between model output and expected output by optimally adjusting model weights and reaching model convergence. This process uses an algorithm called stochastic gradient descent with backpropagation (LeCun et al., 1989; Rumelhart et al., 1995). Stochastic Gradient Descent (SGD) is a popular optimization technique used in deep learning pipelines. It updates the parameters of the model based on the average of gradients calculated on a small batch of training examples. Another widely used optimization technique is Adam (Adaptive Moment Estimation), which combines the advantages of two other extensions of SGD: Adagrad and RMSProp. Adam computes adaptive learning rates for different parameters from estimates of first and second moments of the gradients. These optimization techniques help improve the efficiency and effectiveness of the deep learning pipeline.
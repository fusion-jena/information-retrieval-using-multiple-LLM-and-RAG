Based on the provided context, it appears that the strategy employed to handle randomness in the deep learning pipeline is repeated random sub-sampling. This technique involves creating multiple subsets of data from the original dataset, with each subset containing a different combination of samples. These subsets are then used to train and test the machine learning models. By repeating this process multiple times, the variability introduced by random factors such as the initial seed value can be mitigated.

In the case described in the context, five different datasets were generated using repeated random sub-sampling, and each dataset was further divided into a training/testing split of 80/20. Five CNN models were then trained and tested using these datasets. The use of multiple datasets and models helps ensure that the results obtained are robust and reliable, even when faced with random variations in the input data or model parameters.

It should also be noted that the frame selection process for extracting frames from the training and testing videos was carried out using a Python script that randomly selected frames using a uniform distribution. While this approach may result in some instances where frames do not contain any fish, it ensures that the resulting datasets are representative of the full range of possible environmental conditions encountered in the field.

Overall, the use of repeated random sub-sampling and multiple datasets and models provides a powerful tool for handling randomness in the deep learning pipeline and ensuring that the results obtained are accurate and reliable.
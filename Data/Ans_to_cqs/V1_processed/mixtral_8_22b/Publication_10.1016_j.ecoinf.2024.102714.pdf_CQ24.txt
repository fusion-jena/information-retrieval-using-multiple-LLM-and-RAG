Based on the provided context, there isn't any explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, it can be inferred that some form of management might have been implemented due to the use of machine learning techniques like Random Forest Classifier and XGBoost. These algorithms inherently involve elements of randomness, which could affect reproducibility if not properly managed.

One common strategy to manage randomness in deep learning pipelines is setting fixed seeds for all sources of randomness, including those related to data shuffling, weight initialization, and dropout layers. By doing so, researchers ensure that their results remain consistent across different runs or when shared with others. Another strategy involves performing multiple runs with different seeds and averaging the results to reduce variability caused by randomness.

However, without specific details regarding the implementation of these models in the study, we cannot definitively state what strategies were employed to handle randomness in the deep learning pipeline.
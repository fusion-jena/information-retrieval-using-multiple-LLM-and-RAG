The given context does not explicitly mention any regularization methods used in deep learning pipelines like dropout or L2 regularization. However, it discusses several techniques used to prevent overfitting in machine learning models, particularly ensemble methods and hyperparameter tuning.

Ensemble methods combine multiple weak learners to create a strong learner, reducing overfitting and improving the overall prediction accuracy. Examples include Random Forest (RF), Extreme Gradient Boosting (XGB), and ensemble RF-XGB. These methods have been applied in studies to predict stream water quality and analyze various aspects of water quality.

Hyperparameter tuning also plays a significant role in preventing overfitting. By adjusting hyperparameters such as subsamples, min_child_samples, reg_lambda, n_estimators, learning_rate, max_depth, and random_state, one can effectively control overfitting and improve the model's performance. This optimization process involves selecting the best combination of hyperparameters through manual tuning, Bayesian Optimization, or random grid search methods.

In summary, while the given context does not directly address regularization methods used in deep learning pipelines, it highlights ensemble methods and hyperparameter tuning as effective strategies to prevent overfitting in machine learning models.
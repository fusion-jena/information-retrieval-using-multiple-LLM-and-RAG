In the deep learning pipeline used in this study, the Adam learning rate optimizer was chosen over the static stochastic gradient descent (SGD). Both the basic SGD and its further adaptive developments are popular in neural network applications. However, given the findings of previous studies, which proved Adamâ€™s usefulness on relatively small datasets, the Adam optimizer was selected for the model of this study. These findings suggest that adaptive learning rate algorithms like Adam often yield higher model accuracies than static ones like SGD. Therefore, the use of the Adam optimizer can be seen as an optimization technique applied in the deep learning pipeline of this study.
Unfortunately, based on the provided context, there is no explicit mention of any regularization methods such as dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. Therefore, I cannot provide information regarding which regularization methods were employed in this case. However, it is important to note that these techniques are commonly used in deep learning pipelines to avoid overfitting and improve model generalization. Dropout involves randomly dropping out neurons during training to reduce co-dependency between them, while L2 regularization adds a penalty term to the loss function to discourage large weights. Other popular regularization methods include batch normalization, early stopping, and data augmentation.
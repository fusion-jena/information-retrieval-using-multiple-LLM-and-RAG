The paper uses four different types of deep learning models in their pipeline: Wide-ResNet101, InceptionV3, ResNet101, and MnasNet-A1. All these models are Convolutional Neural Networks (CNNs), which are widely used for image classification tasks due to their ability to automatically learn spatial hierarchies of features from images.

Wide-ResNet101 is a variant of the original ResNet architecture, which introduces shortcut connections between layers to improve training and performance. It is called 'wide' because it increases the width of the network rather than its depth.

InceptionV3 is another popular CNN architecture that utilizes multiple parallel filter sizes to capture multi-level feature representations. It also incorporates several optimization techniques such as label smoothing and factorized convolutions to reduce computational complexity.

ResNet101 is a deeper version of the original ResNet architecture, featuring 101 layers instead of the standard 18 or 34 layers. It employs skip connections similar to those found in Wide-ResNet101.

Finally, MnasNet-A1 is a lightweight CNN specifically designed for mobile applications. It achieves high accuracy with relatively low computational requirements by employing a combination of depthwise separable convolutions and channel shuffle operations.
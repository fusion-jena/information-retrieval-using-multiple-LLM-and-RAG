Based on the given context, there isn't explicit mention of specific criteria used to determine when training is complete. However, we can infer some possible methods based on the described process.

One method could involve using partial training and successive halving techniques. This involves initially training models with a small portion of the data and eliminating those that perform poorly. The remaining models are then trained further, and again, the worst performing ones are eliminated. This process continues iteratively until only one configuration remains, which would be considered the best configuration found.

Additionally, the context mentions the use of Mean Absolute Error (MAE) and Nash-Sutcliffe Efficiency (NSE) metrics during the training phase. These metrics could potentially serve as indicators for determining when training should stop. For instance, if the MAE or NSE scores reach a certain threshold or show no significant improvement over several iterations, it might suggest that the model has reached its maximum performance capacity, indicating that training could be stopped.

However, without explicit confirmation from the source material, these interpretations remain speculative. Therefore, I cannot definitively state what criteria are used to determine when training is complete.
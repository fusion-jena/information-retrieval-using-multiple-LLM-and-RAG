Based on the given context, there isn't any explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, some implicit strategies can be derived from the text.

Firstly, the context mentions that the experiments were conducted on a small dataset without data augmentation. This implies that one strategy to manage randomness might involve working with smaller datasets or avoiding techniques like data augmentation that introduce variability.

Secondly, the context discusses the importance of choosing proper parameters for the MG and MO models to achieve good training results. This suggests another potential strategy: carefully selecting and tuning hyperparameters to minimize the effects of randomness.

Lastly, the context describes how the model was trained using stochastic gradient descent and the backpropagation algorithm included in Caffe. Stochastic gradient descent itself introduces randomness into the optimization process, but this can be managed by setting appropriate learning rates and employing learning rate policies, as mentioned in the context.

However, it should be noted that these strategies are not explicitly stated as methods for dealing with randomness in the deep learning pipeline. They are merely interpretations based on the available context. For a more comprehensive understanding of managing randomness in deep learning pipelines, additional resources or expert knowledge may be required.
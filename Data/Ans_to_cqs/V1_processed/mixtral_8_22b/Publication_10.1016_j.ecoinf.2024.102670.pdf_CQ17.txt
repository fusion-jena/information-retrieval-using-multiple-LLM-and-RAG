The text does not provide explicit information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, based on general knowledge, some common regularization techniques used in deep learning models include Dropout, L1 Regularization, and L2 Regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting. L1 Regularization adds an absolute value of the magnitude of coefficients as penalty term to the loss function, while L2 Regularization adds a squared magnitude of the coefficient as penalty term to the loss function. These techniques help in reducing overfitting by introducing a cost to complexity of the model.
The text does not explicitly mention how the hyperparameters of the model are optimized. However, it can be inferred that optimization algorithms such as Stochastic gradient descent (SGD), Adaptive Gradient (AdaGrad), Root Mean Square Prop (RMSProp), and Adam are used to assess each parameterâ€™s contribution to the output. These optimization algorithms help in finding the optimal weights or parameters for the model. It should be noted that these optimization algorithms are different from hyperparameter optimization techniques like Grid Search and Random Search. Hyperparameter optimization techniques are used to find the best combination of hyperparameters for the model, while optimization algorithms are used to minimize the cost function during the training phase. Since the text does not provide explicit information about the use of hyperparameter optimization techniques, it cannot be confirmed whether Grid Search or Random Search was used for hyperparameter optimization.
The deep learning model uses the Adam optimizer with a concatenating cosine-annealing linear scheduler. The initial learning rate is set to 0.0001, decaying by a factor of 0.001*learning rate. The batch size used for training is 32. Additionally, early stopping is employed to avoid overfitting when there is no further improvement in the validation loss.
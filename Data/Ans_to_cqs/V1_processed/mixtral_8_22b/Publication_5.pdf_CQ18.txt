The strategy implemented to monitor the model performance during training involved exposing all the training data to the network for a number of iterations until the loss function reached its minimum value. This process was carried out using a NVIDIA Titan X GPU (3584 CUDA cores), and the model score reached its maximum after approximately 5000 iterations. The number of trainable parameters was 85,569 and the computational runtime was around 4 hours with a training batch size of 1024. Additionally, the performance of the proposed Deep Learning model was compared with a Random Forest regression model as a baseline to ensure optimal performance.
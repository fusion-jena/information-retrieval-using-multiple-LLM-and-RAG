The text does not explicitly mention any specific regularization methods such as dropout or L2 regularization used to prevent overfitting in the deep learning pipeline. However, it mentions that batch normalization was used, which can also act as a form of regularization. Batch normalization helps reduce internal covariate shift, making the optimization process easier and reducing the risk of overfitting. Additionally, the use of stochastic gradient descent (SGD) as an optimizer might have helped in preventing overfitting since SGD introduces randomness during the training process, which can lead to better generalization.
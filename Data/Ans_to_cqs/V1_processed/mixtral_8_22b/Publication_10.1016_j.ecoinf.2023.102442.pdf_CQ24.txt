The provided context does not directly address the specific strategies employed to handle randomness in the deep learning pipeline, such as managing random seed values. However, it can be inferred that some strategies might have been used indirectly within these studies. For instance, one common strategy to manage randomness is cross-validation, which involves splitting data into multiple subsets and training models on different combinations of these subsets. This technique helps reduce overfitting and provides more robust performance estimates. Another method is ensemble learning, where multiple models are trained independently and their predictions combined to produce a final output. Ensemble methods help mitigate the effects of random initialization and improve overall accuracy. Additionally, techniques like grid search or randomized search can be applied during hyperparameter tuning to find optimal configurations across various random seeds. Lastly, using pretrained models or transfer learning can also minimize the impact of random initializations since they provide a good starting point for further fine-tuning. While none of these strategies are explicitly mentioned in the given context, they may still play a role in handling randomness when developing deep learning pipelines for tasks such as mapping artisanal and small-scale mines or classifying land use and land cover types.
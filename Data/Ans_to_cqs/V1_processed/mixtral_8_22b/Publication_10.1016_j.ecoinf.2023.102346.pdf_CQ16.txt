Based on the given context, it appears that the authors did not explicitly mention any specific criteria for determining when training is completed. However, they mentioned using 30% of each taxon's data records as testing data and the remaining 70% as training data. They also performed 10 K-fold cross-validation and 10-fold bootstrapping. These methods are commonly used to evaluate the performance of machine learning models during training and help prevent overfitting.

Regarding the criteria for stopping training, one common approach is to monitor the validation loss or error rate during training and stop when it reaches a plateau or starts increasing. This indicates that the model may have started to overfit the training data and will likely perform poorly on new, unseen data. Another approach is to use early stopping, where training is stopped after a certain number of epochs without improvement in the validation loss.

However, since the authors did not specifically state their criteria for stopping training, I cannot definitively answer what criteria they used. It would require further clarification from the authors or additional context to confirm.
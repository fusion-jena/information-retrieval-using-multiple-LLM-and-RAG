To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the study utilized varying dataset sizes ranging from 8 hours to 20 hours, which helped in providing diverse examples to the model. This diversity aids in the generalization of learned features and patterns, making the model more robust. Secondly, the experiments were conducted over multiple runs, specifically ten times, to capture the variability introduced by random factors such as random initialization of weights in artificial neural networks (ANNs) or random sampling during training. Lastly, while the text does not explicitly mention cross-validation or stratified splitting, the use of varying dataset sizes and conducting multiple runs could be seen as implicit methods to ensure the model's ability to perform well on unseen data, thus contributing to its generalizability.
Unfortunately, based on the given context, it does not provide explicit information about the process followed to deploy the trained deep learning model, including details like model serialization or platform selection. However, we can infer some related aspects from the available text.

From the context, we understand that the deep learning model was designed for making day-ahead predictions of three pollutants. To evaluate its performance, appropriate evaluation metrics were selected, such as Mean Squared Error (MSE)/Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). Additionally, Figure 12 shows the model's training and validation losses over time, indicating that the model learned effectively without signs of overfitting after 20,000 epochs.

Furthermore, the context mentions a four-layered architecture consisting of a hardware layer, data storage layer, integration layer, and analytics layer. Although the description of each layer focuses more on functionality rather than implementation details, one could assume that the analytics layer might involve deploying the trained models.

Lastly, the context discusses MLOps—model maintenance in the production environment—which includes continuous integration, deployment, and training on data collected in production. While this doesn't directly explain the deployment process, it suggests that the team likely employed best practices for maintaining and updating deployed machine learning models.

However, without additional context, it remains unclear what specific processes were followed to deploy the trained deep learning model, such as model serialization or platform selection.
To handle randomness in the deep learning pipeline, such as those introduced by random seed values, the authors employ several strategies. Firstly, they use an ensemble of three models to reduce variability caused by weight initialization and random selection during batch training. The final output layer of each model is averaged to produce the result. Secondly, they apply early stopping criteria when monitoring the validation data across training epochs. This means that if there is no improvement found after 50 epochs, the training process will stop. Lastly, they utilize different types of pooling layers within their convolutional neural networks (CNNs). Two models in the ensemble use max pooling, while one uses average pooling. This approach allows them to take advantage of the benefits offered by both methods, potentially improving the overall performance of the system.
The text mentions that the models were trained using a stochastic gradient descent (SGD) optimizer with an initial learning rate of 0.01 and a cycle learning rate of 0.2. Stochastic Gradient Descent (SGD) is an iterative method for optimizing an objective function with suitable smoothness properties. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient by an estimate thereof. This estimate is computed from a randomly selected subset of the training data.

However, the text does not mention any other specific optimization techniques such as Adam, RMSProp, Adagrad, etc. Therefore, based solely on the given information, we cannot definitively state whether these additional optimization techniques were also employed in the deep learning pipeline.
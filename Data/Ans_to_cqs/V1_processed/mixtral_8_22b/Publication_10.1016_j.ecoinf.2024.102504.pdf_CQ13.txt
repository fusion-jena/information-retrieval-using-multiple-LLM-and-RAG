The papers mentioned do not provide specific details about the hyperparameters used in their respective deep learning models. However, it is common practice in deep learning to use hyperparameters such as learning rate, batch size, number of epochs, optimizers like Adam or Stochastic Gradient Descent (SGD), and regularization techniques like dropout or weight decay. For instance, Navares and Aznarte (2020) mention using Long Short-Term Memory (LSTM) networks but do not specify the exact hyperparameters they employed. Similarly, Chianese et al. (2019) discuss using multi-layer perceptrons without providing explicit information on the chosen hyperparameters. Therefore, based on the given context, we cannot determine the precise hyperparameters utilized in these studies.
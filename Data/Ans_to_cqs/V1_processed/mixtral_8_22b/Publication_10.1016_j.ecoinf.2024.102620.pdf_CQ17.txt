Based on the given context, there isn't any direct information about the regularization methods used to prevent overfitting in the deep learning pipeline such as dropout or L2 regularization. However, I can provide some general insights into these techniques.

Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps to avoid overfitting by preventing complex co-adaptations on training data. On the other hand, L2 regularization adds a penalty term to the loss function, encouraging smaller weights and thus simpler models, also helping to reduce overfitting. Other common regularization methods include early stopping, weight decay, and data augmentation.

In summary, while the specific regularization methods used in the deep learning pipeline mentioned in the context aren't explicitly stated, commonly used methods to prevent overfitting include dropout, L2 regularization, early stopping, weight decay, and data augmentation.
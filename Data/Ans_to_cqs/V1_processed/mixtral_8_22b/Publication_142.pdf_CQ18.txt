The strategy implemented to monitor the model performance during training involves addressing the issue of internal covariate shift. Internal covariate shift refers to changes in the distribution of network activations caused by updates to the parameters during training. To mitigate this problem, techniques like Batch Normalization can be employed. Batch normalization is a method that normalizes the activations of each layer in the neural network, making the optimization process smoother and faster. It also helps reduce generalization error and improves the overall performance of deep learning models. Additionally, monitoring the gap between the training data and the test data on which the developed algorithms will be evaluated is crucial to ensure the model performs well when applied to new, unseen data.
Hyperparameters of the model were optimized through a process called Grid Search Cross Validation (GridSearchCV). This method ensures that every possible combination of hyperparameters is evaluated using cross-validation with a fixed number of folds. In this particular case, the number of folds used was ten.

Each model had its own unique parameter grid, specifically designed to target certain characteristics of the model. For instance, in ensemble methods, the number of estimators was targeted, while in decision tree-based algorithms, the depth of trees was focused upon.

This meticulous and systematic search across a predetermined grid of hyperparameter values guaranteed that the final selection of hyperparameters was not arbitrary but supported by empirical evidence. It also helped in reducing bias in the model predictions, which is critical according to Schratz et al. (2019).

Moreover, the'max_depth' in tree-based models and 'learning_rate' in boosting models were among the key parameters that influenced the model's capacity to learn from the data without overfitting. Similarly, the number of estimators or iterations was another significant parameter that determined the degree of learning, thereby playing a vital role in model optimization.
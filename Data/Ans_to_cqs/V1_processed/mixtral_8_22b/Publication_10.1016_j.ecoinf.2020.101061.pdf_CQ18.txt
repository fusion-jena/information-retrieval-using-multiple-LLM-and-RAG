The authors conducted a sensitivity study to evaluate the impact of different amounts of training data on model performance. They selected varying percentages of the total hand-annotations (5%, 25%, 50%, and 75%) and compared them to the full dataset for within-site results for each site. This process was repeated five times to account for random subsampling of annotations. Additionally, they evaluated the pretraining model alone (i.e., 0% hand-annotated data) to determine if adding hand-annotated data improved within-site pretraining. These experiments allowed the researchers to monitor the model performance during training and understand how much labeled data is required to achieve optimal performance.
In the given context, the optimization technique used in the deep learning pipeline is mentioned as Adam. Specifically, the models were trained with an optimizer called Adam, having a beta1 value set to 0.937. Adam is a popular optimization algorithm used in deep learning due to its computational efficiency and minimal memory requirements. It combines the advantages of two other extensions of stochastic gradient descent, namely Root Mean Square Propagation (RMSProp) and Adaptive Gradient Algorithm (AdaGrad). By incorporating the benefits of both methods, Adam provides an efficient way to update network weights iteratively based on the training data.
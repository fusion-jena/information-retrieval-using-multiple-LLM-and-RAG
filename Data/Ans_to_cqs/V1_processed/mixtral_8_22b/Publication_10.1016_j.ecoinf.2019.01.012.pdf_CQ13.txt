The deep learning model used in this study employed several hyperparameters. The optimizer used was Adaptive Moment Estimation (ADAM) with a learning rate of 0.0001. Training was conducted with a batch size of 256 over 16,290 iterations or 90 epochs. The choice of these hyperparameters was guided by the need to fine-tune the model for optimal performance, as indicated by the validation set used during training. It's important to note that the specific values of these hyperparameters can significantly impact the model's performance, hence their careful selection and adjustment during the training phase.
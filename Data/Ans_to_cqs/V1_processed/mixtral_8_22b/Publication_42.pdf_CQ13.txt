The paper describes the process of finding the best CNN architecture for species classification by adjusting various hyperparameters. However, the specific hyperparameters mentioned in the text are related to the structure of the CNN rather than the training process itself. These include kernel sizes for the first and last layers, convolutional depth for the first and last layers, and the size of the fully connected layer. The table lists these hyperparameters for the top five performing architectures.

Regarding the optimization algorithm, the paper mentions that Adam was found to be superior compared to Stochastic Gradient Descent (SGD), although no further details about their respective configurations or learning rates are provided. Therefore, while we can infer that Adam was used as the optimizer, the exact learning rate remains unknown based on the provided context.
To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the Random Forest (RF) algorithm was employed, which inherently reduces overfitting by averaging multiple decision trees. Secondly, the data was divided into training and testing sets using a randomized approach. This method, known as random stratification, involves grouping individual observations based on specific criteria (in this case, the year of capture), followed by randomly selecting these groups for inclusion in either the training or testing datasets. While this could potentially introduce systematic differences between the test and train datasets, it guarantees that the model is evaluated using previously unseen combinations of explanatory variables, thereby providing a more robust assessment of its performance. Thirdly, hyperparameter tuning was conducted manually. Initially, the learning rate was set as low as possible and the number of iterations as high as computationally feasible. The learning rate, also referred to as the shrinkage parameter, dictates the influence of each tree on the overall model. Additionally, the maximum tree depth, reflecting the degree of interaction within the model, was typically maintained at a low value. Lastly, Gradient Boosting Regression Trees (GBRT) were utilized due to their superior performance with relatively shallow trees, often termed 'tree stumps'.
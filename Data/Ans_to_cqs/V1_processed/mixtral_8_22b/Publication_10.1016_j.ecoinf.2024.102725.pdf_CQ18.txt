Based on the given information, it appears that there are no specific details about the strategies implemented to monitor the model performance during training. However, we can infer some possible methods based on common practices in machine learning.

One common method for monitoring model performance during training is through validation metrics such as R2, MAE, RMSE, and ERGAS. These metrics help measure how well the model performs by comparing its predictions against actual data. In this case, the table provides these metrics for different models (DCT-PLS, ESTARFM, and SOWCEM) across various dates. By tracking these metrics over time, one could observe if the model improves or deteriorates during training.

Additionally, techniques like cross-validation or using separate datasets for training and testing can further aid in evaluating model performance. Cross-validation involves splitting the dataset into multiple subsets and iteratively training the model on each subset while validating it on the remaining ones. This helps ensure that the model generalizes well to unseen data. Similarly, having distinct training and testing sets allows for more accurate assessment of the model's ability to predict new instances accurately.

In conclusion, although the provided information does not explicitly state any strategies for monitoring model performance during training, common practices include utilizing validation metrics, employing cross-validation, and maintaining separate training and testing datasets.
Based on the given context, it appears that the study does not directly discuss specific techniques to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, some general practices can be observed in this research.

Firstly, the labeled dataset was divided into training and testing datasets with a 70/30 split. While this division is mentioned, there is no explicit mention of whether this split was performed randomly or if any particular technique like stratification was employed to maintain class balance.

Secondly, within the training dataset, half of the data was set aside for validation purposes. Again, it is not specified how this subset was selected - whether it was done randomly or through a more sophisticated approach like stratified sampling.

Lastly, the paper mentions that the final classification model, a support vector machine (SVM) with a cubic polynomial kernel, was trained using the ResNet-50 fc1000 activations of the full training dataset. Afterward, a 10-fold cross-validation was utilized to establish the score-to-posterior-probability transformation function. Cross-validation is a common practice to ensure robustness against overfitting and underrepresentation of certain classes but does not necessarily tackle data imbalance issues head-on.

Therefore, while the study employs several good practices for handling data in a deep learning pipeline, it does not provide direct evidence of employing specific techniques to address data bias during preprocessing.
In the given context, there isn't any explicit mention of strategies employed to handle randomness in the deep learning pipeline, such as managing the random seed value. However, based on common practices in deep learning, here are some strategies that can be employed:

1. Fixed Random Seed: One common strategy is to fix the random seed value before running experiments or training models. By fixing the random seed, one can reproduce the exact same sequence of random numbers, ensuring consistency across different runs of the experiment or model training process.

2. Multiple Runs: Another approach is to perform multiple runs of the experiment or model training using different random seeds. This helps in understanding the variability in performance due to randomness and provides a better estimate of the expected performance.

3. Cross-Validation: To further mitigate the effects of randomness, cross-validation techniques like k-fold cross-validation can be employed. These techniques involve splitting the dataset into multiple folds and performing multiple rounds of training and evaluation, where each fold serves as the validation/test set once. This ensures that the model is evaluated on different subsets of the data, providing a more robust estimate of its performance.

4. Ensemble Methods: Ensemble methods combine the predictions of multiple models trained with different random initializations or different subsets of the data. Averaging the predictions from these models can help reduce the variance caused by randomness and improve overall performance.
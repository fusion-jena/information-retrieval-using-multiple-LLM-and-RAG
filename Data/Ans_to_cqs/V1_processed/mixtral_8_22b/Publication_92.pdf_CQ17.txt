The text does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, common techniques include dropout and L2 regularization. Dropout randomly sets a fraction rate of input units to 0 at each update during training time, helping to prevent overfitting. L2 regularization adds a penalty term to the loss function, encouraging smaller weights and reducing complexity in the model. Other possible methods could include early stopping or data augmentation, although these are not mentioned explicitly in the provided context.
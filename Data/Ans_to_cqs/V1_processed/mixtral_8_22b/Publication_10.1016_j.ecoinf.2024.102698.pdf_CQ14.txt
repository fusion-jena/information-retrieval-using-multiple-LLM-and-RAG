The text does not provide specific information about how the hyperparameters of the surrogate model are optimized. However, it mentions that the surrogate model's parameters need to be optimized to balance variance and bias. This suggests that some form of optimization technique is employed to tune these parameters. Common methods for hyperparameter tuning include grid search, random search, Bayesian optimization, and gradient-based optimization. Grid search involves specifying a set of values for each parameter and evaluating all possible combinations. Random search randomly samples a specified number of hyperparameter settings from predefined ranges. Bayesian optimization uses a probabilistic approach to iteratively update a posterior distribution over the objective function and select new hyperparameters based on this distribution. Gradient-based optimization techniques use gradients of the loss function with respect to the hyperparameters to guide the search process. Without more specific information, it cannot be determined which method is used for optimizing the hyperparameters of the surrogate model.
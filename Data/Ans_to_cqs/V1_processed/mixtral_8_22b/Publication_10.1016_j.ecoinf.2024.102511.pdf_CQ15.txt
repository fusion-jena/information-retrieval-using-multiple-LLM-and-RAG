The given context does not provide specific information about the optimization techniques used in the deep learning pipeline. However, based on general knowledge, some commonly used optimization techniques in deep learning include Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), Root Mean Square Propagation (RMSProp), and others. These techniques help minimize the loss function during training by adjusting the model parameters iteratively. The choice of optimization technique depends on factors such as convergence speed, computational resources, and problem complexity. For instance, Adam combines the advantages of both SGD and RMSProp, offering faster convergence and better performance with sparse gradients. Nonetheless, without explicit mention in the provided context, it cannot be confirmed which optimization techniques were employed in this case.
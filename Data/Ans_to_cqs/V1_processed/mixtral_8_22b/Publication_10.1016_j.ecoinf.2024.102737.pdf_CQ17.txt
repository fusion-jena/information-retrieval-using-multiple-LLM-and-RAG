The given context does not provide specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, from general knowledge, we can mention that common regularization methods include Dropout, L1 Regularization, and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overfitting.

L1 Regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function, which encourages sparsity in the weights.

L2 Regularization adds a penalty equal to the square of the magnitude of coefficients to the loss function, which discourages large weights and encourages smaller ones.

These regularization methods help to reduce overfitting by adding constraints to the optimization process, making the model more generalizable to new data.
Based on the given information, there is no mention of any specific optimization techniques used in the deep learning pipeline, such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, hyperparameter optimization was performed using both the random grid search and Bayesian optimization methods, which were implemented using GridSearchCV from Scikit-learn and the Hyperopt library (version 0.2.7). This suggests that optimization techniques may have been applied during the training process of the machine learning models mentioned, including RF, GBDT, LightGBM, XGBoost, and CatBoost. Nevertheless, without explicit details about the use of optimization techniques like SGD or Adam, it cannot be confirmed if they were specifically utilized in the deep learning pipeline.
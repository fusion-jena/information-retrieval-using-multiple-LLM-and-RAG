The hyperparameters of the model are optimized using various methods, including grid search, random search, and Bayesian-based optimizers. Grid search involves specifying a range of possible values for each parameter and then testing every combination of those parameters to determine the best one. Random search, on the other hand, randomly selects combinations of parameters within specified ranges and tests them until finding the best combination. Bayesian-based optimizers use Bayesian statistics to iteratively update their beliefs about the optimal hyperparameters based on previous evaluations. In this study, the hyperparameters of XGBoost and LightGBM were tuned by several optimizers, including Bayesian-based optimizers and a novel Tasmanian Devil Optimisation algorithm, with RMSE as the objective function.
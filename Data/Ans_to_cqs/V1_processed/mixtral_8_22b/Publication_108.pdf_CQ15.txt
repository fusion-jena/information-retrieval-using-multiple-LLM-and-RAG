Based on the given context, one optimization technique applied in the deep learning pipeline is Adam, mentioned in reference number 31 by Kingma and Ba. Adam is a method for stochastic optimization, which is particularly useful for problems involving large amounts of data or parameters. It combines the advantages of two other popular optimization methods, Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp), providing an efficient way to update network weights iteratively based on training data. However, the context does not provide information about whether other optimization techniques such as Stochastic Gradient Descent (SGD) have been applied in the deep learning pipeline.
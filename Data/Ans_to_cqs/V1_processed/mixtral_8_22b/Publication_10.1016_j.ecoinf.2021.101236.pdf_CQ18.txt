The text does not provide explicit information on the specific strategy implemented to monitor the model performance during training. However, it mentions the use of the ADAM optimizer with an initial learning rate of 0.001 and a batch size of 32, and the reduction of the learning rate by a factor of 0.5 according to a step-wise schedule. These details suggest that some form of monitoring was likely in place to adjust these hyperparameters based on the model's performance during training. Additionally, the mention of oversampling of underrepresented classes to reduce class imbalance implies that class distribution and accuracy were also considered when evaluating the model's performance.
The text does not provide explicit information about the criteria used to split the data for deep learning model training into train, test, and validation sets. However, it mentions several factors that could potentially be used for splitting the data. These factors include the size of the windows (100-m, 500-m, or 1-km radii), the category level within thematic covariate categories, and using all remainders. Additionally, the text discusses the use of three different algorithms for the embedding procedure, namely generalized linear model with elastic net regularization, generalized additive model with null-space penalization, and guided regularized random forest. It is possible that these algorithms could also influence how the data is split for training purposes. Nevertheless, without more specific information, it is difficult to determine exactly what criteria were used for splitting the data.
In the given context, two optimization techniques have been mentioned and used in the deep learning pipeline - Stochastic Gradient Descent (SGD) and Adam Optimizer.

Firstly, SGD was compared with the Adam optimizer. SGD was tested with the parameters as recommended by L. Smith, but it resulted in a 1%–2% decrease in accuracy compared to using the Adam optimizer. This indicates that the Adam optimizer was more effective in improving model performance.

Secondly, the Adam optimizer was chosen for use in the deep learning pipeline. It was used with a fixed learning rate of 1.0⋅10^-4 during the training process. Data augmentation techniques were also employed to prevent overfitting and improve generalization. These included image scaling, horizontal and vertical flips, perspective distortions, and adding color jitter for brightness, contrast, and saturation.

Therefore, based on the information provided, the optimization techniques applied in the deep learning pipeline include both SGD and Adam optimizers, with the latter being preferred due to its superior performance.
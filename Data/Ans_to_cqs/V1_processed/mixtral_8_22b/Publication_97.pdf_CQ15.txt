In this study, the authors use the Adam optimization algorithm for their deep learning pipeline. They mention trying to change the default learning rate parameter for the Adam algorithm, indicating its application in their work. However, they do not provide details about other optimization techniques like Stochastic Gradient Descent (SGD) or any comparisons between different optimization methods.
The data was split into three sets: train, test, and validation. Approximately 60% of the data was allocated for training, while 20% was used for testing and another 20% for validation. This distribution was chosen after experimenting with different input patch sizes, which did not significantly affect the results but increased the training time. All deep learning models employed in this study were trained from scratch, meaning no pre-trained weights from other datasets were utilized. The same set of hyperparameters was used during training for all approaches, including a learning rate of 0.01, weight decay of 0.005, momentum of 0.9, and 200,000 iterations. Stochastic Gradient Descent (SGD) was used to optimize the model.
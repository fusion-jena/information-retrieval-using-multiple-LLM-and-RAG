Unfortunately, the given context does not provide specific details about the hyperparameters used in the deep learning models developed for fish detection in turbid waters. However, based on general practices in deep learning, some common hyperparameters include learning rate, optimizer, batch size, number of epochs, and weight decay. These parameters play crucial roles in training the models effectively and efficiently. For instance, the learning rate determines how much the weights should be updated during each iteration, while the optimizer algorithm (such as Adam or SGD) helps minimize the loss function. Batch size controls the amount of data processed before updating the internal model parameters, and the number of epochs specifies the total iterations over the entire dataset. Lastly, weight decay is often employed to prevent overfitting by adding a penalty term to the loss function. Without explicit information regarding these hyperparameters, it is impossible to determine their exact values for the mentioned deep learning models.
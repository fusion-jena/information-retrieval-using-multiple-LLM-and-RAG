Based on the given context, it appears that one strategy employed to handle randomness in the deep learning pipeline is the use of dropout regularization in the forward pass during training. Dropout regularization involves randomly dropping out units (neurons) along with all its incoming and outgoing connections during training. This helps prevent overfitting and improves generalization performance. Additionally, the choice of optimization algorithm can also impact the handling of randomness. In this case, the Adam optimizer was chosen over stochastic gradient descent (SGD) due to improved accuracy. However, there is no explicit mention of how random seed values were handled in the given context. It should be noted that setting a fixed random seed value can help ensure reproducibility of results across different runs of the same model.
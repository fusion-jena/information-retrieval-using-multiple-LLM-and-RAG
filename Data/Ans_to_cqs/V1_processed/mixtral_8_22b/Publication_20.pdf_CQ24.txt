Based on the provided information, there isn't any explicit mention of specific strategies employed to handle randomness in the deep learning pipeline, such as managing random seed values. However, some general practices can be inferred from the text.

Firstly, the authors use a large amount of data for training their models, which helps reduce the influence of individual samples or initial conditions on the overall performance. They have at least 2200 thumbnails per fish species class and use batches of 16 images to train their network. This suggests that they rely on the sheer volume of data to mitigate the effects of randomness.

Secondly, the authors employ techniques like dropout and early stopping to prevent overfitting. Dropout randomly drops out neurons during training, effectively creating different networks for each mini-batch, which can help improve the robustness of the model against random variations. Early stopping involves monitoring the validation loss and halting the training process when the validation loss stops improving, preventing the model from fitting noise in the training data due to random fluctuations.

Lastly, the authors use classic hyperparameters for fast convergence and avoidance of overfitting, including a learning rate of 10^-5, an exponential learning decay with a gamma of 0.95, a dropout of 50%, and an Adam solver type. These choices suggest that they follow established best practices in deep learning to ensure reliable and consistent performance across multiple runs.

In summary, while no explicit strategy for handling randomness in the deep learning pipeline is mentioned, the authors seem to rely on large amounts of data, regularization techniques like dropout, and well-established hyperparameter settings to achieve stable and accurate results.
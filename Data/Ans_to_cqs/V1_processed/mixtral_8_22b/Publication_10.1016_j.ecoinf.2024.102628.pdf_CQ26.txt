The text does not provide explicit details about specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, it mentions the use of a Leave-One-Group-Out (LOGO) cross-validation approach, where a complete 450m multitemporal transect was omitted from the training process in each iteration and then tested. This method ensures an assessment of the model's performance while considering data constraints. Additionally, the dataset was expanded by incorporating multi-temporal data from a three-month span across three environments, which helped improve the effectiveness of training and testing deep learning models. Although these methods do not directly address data bias, they contribute to improving the robustness and generalizability of the models by ensuring diverse data representation and reducing overfitting.
Based on the given information, there isn't any explicit mention of a specific strategy implemented to monitor the model performance during training. However, we can infer some possible strategies based on common practices in machine learning and deep learning models.

One common approach to monitoring model performance during training is through validation techniques such as cross-validation or holdout validation. These methods involve splitting the dataset into separate sets for training and validation, allowing the model's performance to be evaluated on unseen data throughout the training process. This helps prevent overfitting and provides insights into how well the model generalizes to new data.

Additionally, various metrics are typically employed to assess the model's performance during training. For example, classification tasks may use metrics like accuracy, precision, recall, or F1 score, while regression tasks might employ mean squared error (MSE), root mean square error (RMSE), or coefficient of determination (R^2).

In the given context, Table 2 presents the classification accuracies of three different deep learning algorithms â€“ VGG16, ResNet50, and AlexNet. Each algorithm's User Accuracy (UA), Producer Accuracy (PA), and F1-score are listed, suggesting that these metrics were likely used to evaluate the model's performance during training. Furthermore, the overall accuracy (OA) and Kappa statistic are reported, which provide additional insight into the model's performance across all classes.

However, without more detailed information regarding the training process, it is impossible to definitively state what specific strategy was implemented to monitor the model performance during training.
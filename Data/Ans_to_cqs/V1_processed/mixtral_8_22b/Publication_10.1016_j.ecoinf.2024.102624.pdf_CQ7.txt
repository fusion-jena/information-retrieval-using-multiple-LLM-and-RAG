In the given context, the reference dataset was split into training and testing sets at a 70:30 ratio. This means that 70% of the data was used for training the deep learning models, while the remaining 30% was used for testing their performance. No specific mention is made about a separate validation set, but it could be assumed that the authors might have used part of the training set for validation purposes during the model building phase.

To ensure reliable predictions and minimize overfitting, the authors employed a 10-fold cross-validation technique with three repetitions. Cross-validation involves splitting the training data into multiple subsets called 'folds'. In this case, the training dataset was randomly divided into 10 folds. For each iteration, nine folds were used for training the model, while one fold served as the testing data. This process continued until every fold acted as the testing data once. To further enhance the robustness of the results, the entire sequence was repeated twice using different random splits.

This approach helped identify the most accurate models by comparing the overall accuracy (OA) achieved across various iterations. By analyzing the distribution of these accuracies, including minimums, maximums, quartiles, and medians, the researchers aimed to select the models with the smallest range of potential outcomes. This would indicate higher reliability and consistency in predictive performance.
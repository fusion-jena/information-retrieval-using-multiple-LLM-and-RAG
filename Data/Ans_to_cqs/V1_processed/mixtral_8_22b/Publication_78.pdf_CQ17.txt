The paper does not provide explicit information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions several deep learning models such as AlexNet, ResNet-50, VGG-19, GoogleNet, and Inceptionv3. These models may use various regularization techniques like dropout or L2 regularization to avoid overfitting. For instance, AlexNet uses dropout layers, while VGG-19 employs L2 regularization. It is essential to note that these details are not explicitly mentioned in the given context but can be found in the respective papers describing each model.

Regularization methods help prevent overfitting by adding constraints to the learning process, which helps reduce complexity and improve generalization. Dropout is one popular regularization technique where randomly selected neurons are ignored during training, effectively preventing complex co-adaptations between neurons. On the other hand, L2 regularization adds a penalty term to the loss function, discouraging large weights and promoting simpler models. Other regularization methods include early stopping, data augmentation, and batch normalization.
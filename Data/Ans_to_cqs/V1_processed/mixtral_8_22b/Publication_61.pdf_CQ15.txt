The text mentions that the proposed approach uses the Adam optimizer (Kingma and Ba, 2015) during the training process. This optimizer is a variant of the stochastic gradient descent optimization method, which is commonly used in deep learning pipelines. The main difference between Adam and traditional stochastic gradient descent (SGD) is that Adam adapts the learning rate for each weight individually based on its historical values, while SGD uses a single global learning rate for all weights. This adaptive behavior allows Adam to converge faster and achieve better performance compared to SGD in many cases. Additionally, the text does not mention any other optimization techniques being applied in the deep learning pipeline.
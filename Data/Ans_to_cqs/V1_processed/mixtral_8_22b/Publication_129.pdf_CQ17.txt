The paper does not provide explicit information about the specific regularization methods used to prevent overfitting in their deep learning pipeline. However, common techniques used in deep learning architectures to address overfitting include dropout, L1 and L2 regularization, early stopping, data augmentation, and batch normalization. Dropout randomly sets a fraction rate of input units to 0 during training time, which helps reduce co-adaptation between neurons. L1 and L2 regularization add penalty terms to the loss function to discourage large weights, thus preventing overfitting. Early stopping monitors validation error while training and stops when the error starts increasing, indicating overfitting. Data augmentation generates additional training samples by applying transformations like rotation, scaling, and flipping to existing images. Lastly, batch normalization standardizes inputs within each mini-batch, reducing internal covariate shift and improving model generalization.
The text does not provide explicit information about specific regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it mentions that the AdamW optimizer was used instead of the standard Adam optimizer. AdamW is an extension of Adam that decouples weight decay from the gradients, providing better generalization performance compared to Adam. This can be considered a form of regularization, as it helps prevent overfitting by controlling the complexity of the learned weights. Additionally, the hyperparameters were tuned using an Optuna-based hyperparameter optimization procedure, which could potentially include adjustments related to regularization techniques.
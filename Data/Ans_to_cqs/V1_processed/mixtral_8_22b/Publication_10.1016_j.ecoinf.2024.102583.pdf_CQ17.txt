The text does not explicitly mention specific regularization methods used in the deep learning pipeline, such as dropout or L2 regularization. However, it does discuss some methods used to mitigate overfitting issues in machine learning models for landslide susceptibility mapping (LSM). These include Bagging (B), Boosting (BS), Random Forest (RF), and Weighted Average (WA). The text also mentions that XGBoost, a popular implementation of gradient boosting machines, uses a loss function that effectively prevents overfitting and promotes simpler models. Additionally, XGBoost employs a more advanced optimization approach that combines first-order gradients (loss function gradient) with second-order gradients (loss function curvature), making it faster than some other models. While these methods help address overfitting, they do not directly correspond to the regularization methods mentioned in the query, such as dropout or L2 regularization.
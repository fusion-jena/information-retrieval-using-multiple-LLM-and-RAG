The strategy implemented to monitor the model performance during training is depicted within Figure 3 and described below. Each successive model iteration was trained on 70% of the training data and validated against 30% of the training data. This train/validation split approach minimizes potential overfitting of the model on small sample sizes, when compared to a cross-validation approach. The F1 score was chosen as the validation metric, which is a standard performance metric that includes information on both model precision and recall. Iterative model training stopped when the stopping criterion was reached, which was when successive model iterations did not reduce the F1 score.
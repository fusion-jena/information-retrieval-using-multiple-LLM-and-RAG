The text does not provide explicit information about the specific optimization technique used in the deep learning pipeline. However, common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adam, RMSprop, and others. These techniques help minimize the loss function during the training phase. Given that the overall process followed current best practices in deep learning, it is likely that one of these commonly used optimization techniques was employed. Unfortunately, without more specific details, I cannot definitively state which optimization technique was utilized in this case.
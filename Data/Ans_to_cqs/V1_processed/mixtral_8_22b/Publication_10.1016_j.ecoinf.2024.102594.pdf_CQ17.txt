The given context does not provide explicit details about specific regularization methods such as dropout or L2 regularization used to prevent overfitting in the deep learning pipeline. However, there are some techniques mentioned that can be considered as forms of regularization.

Firstly, the ReduceLROnPlateau method is employed during the training process. This technique reduces the learning rate when the validation loss stops decreasing for a certain number of epochs. By doing so, it helps prevent overfitting by slowing down the learning process when the model starts to fit too closely to the training data.

Secondly, the use of data augmentation techniques like flipping images, exchanging channels, random rotation, and adding noise contributes to preventing overfitting. These techniques increase the diversity of the training dataset, making the model more robust and generalizable to new, unseen data.

Lastly, although not explicitly stated, the choice of architecture itself could also contribute to preventing overfitting. For instance, the DeepLabV3+ algorithm uses dilated convolutions and incorporates multi-scale information, which might help improve the model's ability to generalize better.
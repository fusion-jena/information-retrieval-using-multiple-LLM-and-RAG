The text does not provide explicit information about handling randomness in the deep learning pipeline. However, it mentions the use of a random forest classifier, which is a type of machine learning algorithm that uses multiple decision trees to improve prediction accuracy and minimize overfitting. This technique employs bagging (bootstrap aggregating) and tree averaging to handle randomness. Bagging involves creating subsets of the original dataset through random sampling with replacement, while tree averaging combines the outputs of individual trees to produce a final prediction. These methods help to reduce the impact of noise and variability in the data, thus improving the robustness and generalizability of the model. Additionally, the text notes that the same random number generator was used across all modeling, suggesting that controlling the source of randomness can be important for reproducibility and consistency in machine learning applications.
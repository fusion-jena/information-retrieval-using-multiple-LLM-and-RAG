To prevent overfitting in the deep learning pipeline, several regularization methods can be employed. One such method mentioned in the given context is dropout. Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training, which helps to reduce interdependent learning amongst the neurons. This results in a more robust model that generalizes better to unseen data. In the described model, the dropout operation was applied during the training stage, with a dropout rate of 0.1 for the 6th layer and 0 for the remaining layers.

Additionally, although not explicitly stated in the provided context, another common regularization method is L2 regularization, also known as weight decay. L2 regularization adds a penalty term to the loss function, encouraging smaller weights and preventing any single feature from dominating the decision boundary. This leads to a simpler and more generalized model.

In summary, the regularization methods used to prevent overfitting in the deep learning pipeline, as suggested by the given context, include dropout and potentially L2 regularization. These techniques help create models that perform well on both the training and test sets by reducing complexity and promoting generalizability.
The deep learning pipeline described in this study uses both audio and image data formats. The audio format is used for training models such as BirdNET, YAMNet, and VGGish, while the image format is employed for training models like ResNet152-V2 and VGG16. The audio data is processed into three-second long segments referred to as'samples,' which are then resampled to either 48 kHz or 16 kHz depending on the specific model requirements. Spectrograms are generated from these audio samples using a window size of 512 samples and an overlap of 256 samples. These spectrograms are subsequently resized to fit the dimensions required by the respective convolutional models.
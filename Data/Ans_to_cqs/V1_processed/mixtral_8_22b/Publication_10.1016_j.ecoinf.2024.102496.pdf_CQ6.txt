Before training a deep learning model, several preprocessing steps are typically involved to ensure the quality and consistency of the input data. These may include:

1. Data Cleaning: This involves removing or correcting any inconsistencies, errors, or missing values in the dataset. It could also involve handling outliers or anomalies.

2. Normalization or Standardization: This is done to bring all features into the same range or scale. For example, normalization scales the data between 0 and 1, while standardization transforms the data to have a mean of 0 and a standard deviation of 1.

3. Feature Scaling: Similar to normalization, feature scaling rescales the values so they fall within a specific range. This is important because many machine learning algorithms perform better when numerical input variables are scaled to a standard range.

4. Encoding Categorical Variables: Deep learning models work with numerical inputs, so categorical variables need to be converted into numerical form. Techniques like one-hot encoding or label encoding can be used.

5. Splitting Dataset: The dataset is usually split into three parts - training set, validation set, and test set. The model learns from the training set, the validation set helps tune hyperparameters, and the test set evaluates the final performance of the model.

6. Data Augmentation: This technique is often used in image processing tasks where new images are created by applying random transformations such as rotation, zoom, flip, etc. This increases the amount of training data and helps prevent overfitting.
The papers cited do not provide explicit information about the specific regularization methods used to prevent overfitting in their respective deep learning pipelines. However, it can be inferred that some form of regularization would have been employed due to its importance in preventing overfitting when training machine learning models. Common regularization techniques include dropout, L1 or L2 regularization, early stopping, data augmentation, and batch normalization. Dropout involves randomly setting a fraction of input units to 0 during training time, which helps reduce interdependent learning amongst neurons. L1 or L2 regularization adds a penalty term to the loss function, discouraging complex models and reducing overfitting. Early stopping monitors validation error while training and stops if it starts increasing, indicating overfitting. Data augmentation generates new training samples through transformations like rotation, scaling, etc., improving generalizability. Lastly, batch normalization standardizes inputs within each mini-batch, accelerating convergence and providing some regularization effect. Without explicit mention, we cannot confirm which method(s) were utilized in these studies.
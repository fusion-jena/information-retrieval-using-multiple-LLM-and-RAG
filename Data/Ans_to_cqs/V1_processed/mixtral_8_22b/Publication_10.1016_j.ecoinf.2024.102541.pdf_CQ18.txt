The text does not provide explicit information on the specific strategy used to monitor the model performance during training. However, it can be inferred that the researchers might have been monitoring the model performance through metrics like mAP (mean Average Precision) at various Intersection over Union (IoU) thresholds, Giga Floating Point Operations Per Second (GFLOPs), Parameters (millions), and Frames Per Second (FPS). These are common indicators used to evaluate deep learning models, including object detection models like YOLOv5.

In Table 2, we see the performance comparison of different versions of YOLOv5 based on these metrics. For instance, YOLOv5s has 16.3 GFLOPs, 7.067 million parameters, a mAP@0.5 of 0.958, a mAP@0.5:0.95 of 0.817, and an FPS of 31.949. Similarly, other versions of YOLOv5 - YOLOv5m, YOLOv5l, and YOLOv5x - also showcase their respective performances based on these metrics.

Moreover, the researchers seem to focus on optimizing the model for low power usage and resource-constrained environments while aiming to improve its loading time. They mention modifying the loss function from traditional Complete IoU (CIoU) to Focal-Enhanced IoU (Focal-EIoU) to address class imbalances in datasets. Additionally, they discuss strategies like replacing certain layers or structures with more efficient alternatives, such as swapping out the Focus layer for a new Stem Block, exchanging two C3 blocks with a Mobile Bottleneck Block (MBBlock), incorporating depth-wise separable convolutions (DWConv) into the C3 blocks, and substituting the PANet structure with BiFPN in the neck. All these modifications suggest that the researchers were closely observing the model's performance during training and making adjustments accordingly.
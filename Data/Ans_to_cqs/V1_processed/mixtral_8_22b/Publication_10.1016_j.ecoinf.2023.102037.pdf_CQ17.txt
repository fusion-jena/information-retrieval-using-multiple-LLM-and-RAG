The papers referenced do not explicitly mention specific regularization methods used to prevent overfitting in the deep learning pipelines. However, it is common practice in deep learning to use various techniques such as dropout and L2 regularization to mitigate overfitting. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent complex co-adaptations on training data. On the other hand, L2 regularization adds a penalty term to the loss function, discouraging large weights and promoting simpler models. Other popular regularization methods include early stopping, weight decay, and batch normalization. It should also be noted that proper dataset preparation, including appropriate data augmentation and balancing, contributes significantly to preventing overfitting.
In the given context, it appears that the deep learning model being discussed is a deep convolutional neural network (CNN). The hyperparameters mentioned for this model include:

* Learning Rate: A constant learning rate of 0.01 was used.
* Dropout: Dropout was applied to the input of the last two layers with a probability of 0.5. This technique helps prevent overfitting by randomly ignoring some neurons during training.
* Regularization: L2 regularization was applied to the weights of the last two layers with a penalty factor of 0.001. This helps reduce overfitting by adding a cost term to the loss function proportional to the square of the magnitude of the weights.
* Optimizer: Mini-batch stochastic gradient descent was used to optimize the cross-entropy loss during training.
* Epochs: The model was trained for 100 epochs and checkpointed after each epoch.
* Validation Set: A validation set was used to determine the best parameter settings (epoch) achieving the highest classification accuracy.
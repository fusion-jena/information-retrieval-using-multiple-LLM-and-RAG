The given context does not provide specific details about the regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization techniques used in deep learning models include Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust and reducing reliance on any single feature.

L2 Regularization, also known as weight decay, adds a penalty term to the loss function based on the sum of squares of weights, encouraging smaller weights and preventing overfitting. Other regularization techniques include early stopping, data augmentation, and batch normalization.
The text does not explicitly mention any specific criteria for determining when training is complete. However, it can be inferred from the information provided that the number of training epochs is used as a criterion for stopping the training process. Specifically, the text mentions that models with fewer than 100 layers use 60 epochs, while those with more than 100 layers use 80 epochs. This suggests that the authors have determined empirically that these numbers of epochs are sufficient for achieving good performance on their task without overfitting. Additionally, the fact that regularization loss is included in the computation of model loss implies that the authors are taking steps to prevent overfitting during training. Overall, while there may be other factors considered when deciding when to stop training, such as monitoring the validation loss or accuracy, the text does not provide explicit details about these considerations.
Based on the given context, there isn't specific information regarding the optimization techniques applied in the deep learning pipeline, such as Stochastic Gradient Descent (SGD), Adam, or others. However, the text does mention adjusting hyperparameters, which could include learning rates, batch sizes, and other parameters related to optimization algorithms. It also mentions using Long Short-Term Memory networks (LSTM) alongside Convolutional Neural Networks (CNN), indicating potential use of backpropagation through time (BPTT) for training recurrent neural networks like LSTM. Nonetheless, without explicit details on the optimization techniques employed, we cannot provide a definitive answer.
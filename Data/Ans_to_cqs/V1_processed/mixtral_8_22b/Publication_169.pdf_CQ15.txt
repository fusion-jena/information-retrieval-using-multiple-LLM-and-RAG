In this experimental setup, the authors use the gradient descent Adam algorithm with all parameters set to their default values to optimize the model parameters. This means that the Adam optimization technique is applied in the deep learning pipeline. Adam is a popular optimization method used in machine learning and deep learning algorithms due to its ability to adaptively adjust the learning rate based on the historical gradients. It combines the advantages of two other optimization methods, AdaGrad and RMSProp, resulting in faster convergence and improved performance.
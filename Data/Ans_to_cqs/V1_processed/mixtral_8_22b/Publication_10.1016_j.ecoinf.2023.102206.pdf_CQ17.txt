The given context does not provide information about the specific regularization methods used to prevent overfitting in the deep learning pipeline such as dropout or L2 regularization. However, it mentions that the proposed approach, IFTL, efficiently captures uncertainty caused by extreme variations in data distribution between source and target domains, thereby preventing overconfidence and improving learning during Transfer Learning (TL). Additionally, it discusses how IFTL outperforms FTL when there are significant data distribution differences between training and testing domains. While these concepts relate to addressing overfitting, they do not directly mention any specific regularization methods like dropout or L2 regularization. Therefore, based on the provided context, we cannot determine the exact regularization methods used to prevent overfitting in the deep learning pipeline.
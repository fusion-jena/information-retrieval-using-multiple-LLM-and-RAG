Based on the given context, the deep learning pipeline primarily uses audio data. The feature encoder of the pre-trained Wav2vec model processes raw waveform audio data and converts it into a sequence of feature vectors. These feature vectors are then fed into a context network, which is based on the Transformer's encoder architecture. The context network has been trained using self-supervision, allowing it to learn from unlabelled data. After processing through the context network, the resulting context vector is projected linearly to obtain a 768-dimensional feature vector representation. Finally, this feature vector is passed through a feed-forward network containing dense layers and a classification layer. Therefore, the primary data format used in this deep learning pipeline is audio data.
The deep learning pipeline uses the Adam optimizer for training. The Adam optimizer is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments. It combines the advantages of AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients and non-stationary objectives. The use of this optimizer helps to improve the efficiency and effectiveness of the training process. Additionally, early stopping criteria were applied if no improvement was found in 50 epochs. This technique helps prevent overfitting and ensures that the model does not continue training when it has already converged. Furthermore, an ensemble of three models was generated, and the average of the final output layer was taken. This approach helps reduce variance and improves the overall accuracy of the model.
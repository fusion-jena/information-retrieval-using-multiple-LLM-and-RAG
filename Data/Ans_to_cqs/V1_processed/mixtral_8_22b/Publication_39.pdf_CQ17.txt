To prevent overfitting in their deep learning pipeline, the authors utilized various data augmentation techniques during the training process. They employed the Python library imgaug (Jung et al. (2020)) for image and video data augmentation. Specifically, they used horizontal flipping, adding or subtracting intensity, Gaussian blurring, and additive Gaussian noise on each frame of the video for both Mask R-CNN and FGFA. Additionally, they implemented a more complex augmentation function called 'Fog,' which simulated fog in the video. However, due to its strength, this augmentation was combined only with random horizontal flipping and applied with a probability of 10%. The other combined augmentation techniques were applied with a probability of 90%.

As for optimization techniques, Stochastic Gradient Descent (SGD) was used with a learning rate of 0.0005, a momentum of 0.9, and a weight decay of 0.0005. During the training process, the learning rate was reduced every 10 epochs by multiplying it with Î³ = 0.1. Warmup iterations were also used for the learning rate with a warmup factor of 1/1000 in the first epoch. Models were trained for 30 epochs.

However, there is no explicit mention of using specific regularization methods such as dropout or L2 regularization in the given context. Therefore, based on the information provided, it can be concluded that the primary method used to prevent overfitting in the deep learning pipeline was data augmentation techniques.
In the given context, there are three main regularization techniques mentioned for preventing overfitting in the deep learning pipeline. These techniques are weight decay, batch normalization, and dropout.

Weight Decay: Weight decay is another term for L2 regularization, where a penalty equal to the sum of squares of weights is added to the loss function during optimization. This encourages smaller weights, reducing the complexity of the model and helping avoid overfitting. In the given context, weight decay values of 0, 0.00001, 0.0001, and 0.001 were explored.

Batch Normalization: Batch normalization is a technique that standardizes the inputs to a layer for each mini-batch, allowing each layer of a network to learn by itself more independently of other layers. It helps reduce internal covariate shift, making the network faster and more stable. In the given context, batch normalization was applied to each layer of the network separately and to all layers simultaneously, with the best results achieved when applied to the third layer.

Dropout: Dropout is a regularization method that randomly sets a fraction of input units to 0 at each update during training time, effectively creating a sparse representation of the input data. This prevents complex co-adaptations on training data, thereby improving generalization to unseen data. In the given context, dropout was applied to the input layer, the hidden layers, and all layers, with values of 0.05, 0.1, 0.2, and 0.25 being explored.
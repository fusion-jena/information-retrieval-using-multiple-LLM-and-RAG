The text does not provide explicit information about the optimization method used for tuning the hyperparameters of the model. However, it mentions that the initial learning rate is set to 0.001 and then exponentially decayed every 3 epochs with a factor of 0.94. This suggests that some form of learning rate scheduling has been employed during training. Unfortunately, without further details, we cannot determine whether other methods such as grid search, random search, Bayesian optimization, or early stopping were also utilized for fine-tuning the remaining hyperparameters.
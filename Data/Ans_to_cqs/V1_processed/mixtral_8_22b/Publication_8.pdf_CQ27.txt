Unfortunately, the given text does not provide explicit details about the deployment process of the trained deep learning model. However, based on general practices in machine learning projects, we can infer some possible steps taken for this purpose.

After training the deep learning model using the annotated herbarium scan dataset until convergence, the model might have been serialized into a format suitable for storage and later use. Common formats include pickle files (for Python), ONNX, or TensorFlow's saved model format. Serializing the model allows us to save its structure, learned parameters, and any other relevant metadata required for making predictions.

Once the model has been serialized, it needs to be deployed onto a platform for practical usage. Depending on the specific requirements of the project, various platforms could have been chosen for deployment. These may range from cloud services like AWS, Google Cloud, or Azure, to custom servers running locally. In case of cloud services, popular options include Amazon SageMaker, Google AI Platform, or Microsoft Azure Machine Learning Studio.

To ensure seamless integration between the deployed model and the application consuming its outputs, APIs would likely have been developed. RESTful APIs are commonly employed due to their flexibility and compatibility with diverse programming languages and frameworks.

However, without further information regarding the actual deployment process, these assumptions remain speculative. To get accurate details, one should refer to the original source material or contact the authors directly.
The given context does not provide explicit details about the specific hardware resources used for training the deep learning models mentioned, such as GPUs or TPUs. However, it can be inferred that some form of advanced computing resources would have been required due to the nature of deep learning tasks. Deep learning models typically involve large amounts of data and computationally intensive operations, making them well suited for parallel processing on devices like GPUs and TPUs. Therefore, it is reasonable to assume that similar hardware resources were employed during the development and training of the discussed models, including AlexNet, VGG-16, and ResNet.
The text mentions that a Stochastic Gradient Descent (SGD) optimizer was used, but does not specify the exact learning rate used initially. It only states that if there was no decrease in loss for more than 10 consecutive epochs, the model was retrained with a learning rate 10 times smaller. No other specific hyperparameters are mentioned in the given context.
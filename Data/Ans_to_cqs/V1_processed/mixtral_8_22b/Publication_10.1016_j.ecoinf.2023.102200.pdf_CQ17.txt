The papers mentioned do not explicitly discuss specific regularization methods used to prevent overfitting in their deep learning pipelines. However, common regularization techniques used in deep learning include dropout, L1 and L2 regularization, early stopping, and data augmentation. Dropout randomly sets a fraction rate of input units to 0 during training time, which helps prevent overfitting by reducing co-adaptations between neurons. L1 and L2 regularization add penalty terms to the loss function, encouraging simpler models and preventing overfitting. Early stopping stops training when validation error starts increasing, while data augmentation increases the amount of training data available by applying random transformations to existing images. These techniques can be combined or used individually depending on the requirements of the model and dataset.
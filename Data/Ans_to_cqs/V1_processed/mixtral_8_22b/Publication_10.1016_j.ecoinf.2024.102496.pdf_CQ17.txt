The query asks about regularization methods used to prevent overfitting in deep learning pipelines. Regularization techniques help improve the generalization performance of models by adding constraints during training. Two common regularization methods mentioned in the query are dropout and L2 regularization.

Dropout is a technique where randomly selected neurons are ignored or "dropped" during training, preventing complex co-adaptations between neurons. By doing so, it helps reduce overfitting and improves the network's ability to generalize. Dropout rates typically range from 0.2 to 0.5, depending on the complexity of the problem and the size of the dataset.

L2 regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the magnitude of the weights. This encourages smaller weights, making the model simpler and reducing overfitting. The strength of the regularization is controlled by a hyperparameter called lambda, which determines how much emphasis is placed on minimizing the sum of squared weights compared to minimizing the original loss function.

Other regularization methods include L1 regularization, early stopping, and batch normalization. L1 regularization adds a penalty term based on the absolute value of the weights, promoting sparse solutions. Early stopping involves monitoring the validation error during training and halting when it starts increasing, indicating overfitting. Batch normalization standardizes inputs to layers, improving convergence speed and reducing overfitting.

In summary, regularization methods like dropout, L2 regularization, L1 regularization, early stopping, and batch normalization are employed to prevent overfitting in deep learning pipelines. These techniques add constraints during training, helping improve the model's generalization performance.
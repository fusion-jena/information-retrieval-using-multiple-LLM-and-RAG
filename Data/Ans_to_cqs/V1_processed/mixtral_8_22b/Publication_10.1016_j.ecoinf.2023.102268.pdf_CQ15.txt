From the given context, it appears that the authors did not specifically mention any optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) in their deep learning pipeline. However, they do discuss some aspects of their approach that could be related to optimization.

In section 4.2, they mention splitting the labeled dataset into 70/30 for training and testing datasets. They also hold out 50% of the training data for validation, indicating that they likely used this subset for tuning hyperparameters or optimizing their model.

Furthermore, they state that they used a Support Vector Machine (SVM) with a cubic polynomial kernel as their final classification model. To train this model, they utilized the ResNet-50 fc1000 activations of the full training dataset. Afterward, they employed a 10-fold cross-validation to define the score-to-posterior-probability transformation function. These steps suggest that they may have used some form of optimization technique, although the specific algorithm remains unclear without further information.

Additionally, in section 3.5, they describe using a pre-trained deep Convolutional Neural Network (CNN) called ResNet-50. While they do not provide details about the optimization techniques used during the initial training of ResNet-50, it is common practice to employ optimization algorithms like SGD or Adam when training deep learning models. Therefore, it can be assumed that similar techniques might have been used during the development of ResNet-50.

In conclusion, while the text does not directly address the use of specific optimization techniques like SGD or Adam, there are indications that some form of optimization took place during the development and application of their deep learning pipeline.
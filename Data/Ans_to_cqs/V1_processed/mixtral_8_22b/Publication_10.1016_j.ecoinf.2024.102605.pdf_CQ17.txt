The papers mentioned do not provide information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization methods used to prevent overfitting include dropout, L1 regularization, and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to reduce interdependent learning amongst neurons. L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients, while L2 regularization adds a penalty equal to the square of the magnitude of coefficients. Both L1 and L2 regularizations help to shrink the weights of the network towards zero, reducing complexity and preventing overfitting. Other methods such as early stopping, data augmentation, and batch normalization can also be used to prevent overfitting.
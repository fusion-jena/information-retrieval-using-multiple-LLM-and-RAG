The strategies employed to handle randomness in the deep learning pipeline include setting a fixed random seed value, which ensures reproducibility of results across different runs of the model. Another strategy is to perform multiple runs with different initializations and average the results to reduce variance due to random initialization. Additionally, techniques like batch normalization and dropout can help regularize the network and improve generalization performance. In terms of handling randomness during data splitting into training and test sets, stratified sampling can be used to maintain the original class proportions in both datasets. The biased dataset mentioned in the context uses a simple random split, while the unbiased dataset employs a more sophisticated method involving unique specimen identifiers to ensure that images from the same specimen do not appear in both training and test sets.

Reference(s):
None needed for this response.
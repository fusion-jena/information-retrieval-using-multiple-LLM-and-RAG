In this given context, there isn't any explicit mention or discussion regarding specific regularization methods used to prevent overfitting in the deep learning pipeline. Regularization methods like dropout and L2 regularization are commonly employed techniques to avoid overfitting in deep learning models. Dropout involves randomly dropping out neurons during training which helps in preventing complex co-adaptations on training data. On the other hand, L2 regularization adds a penalty term to the loss function, discouraging large weights and thus reducing overfitting. However, without further information, it cannot be determined if these particular methods were utilized in the described research.
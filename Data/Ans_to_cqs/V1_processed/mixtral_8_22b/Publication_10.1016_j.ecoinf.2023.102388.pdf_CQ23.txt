To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the Stochastic Gradient Descent (SGD) optimization algorithm was employed with a learning rate of 0.0001 and a weight decay of 0.5. These settings help prevent the model from becoming too complex and fitting the noise in the training data, which can lead to poor generalization. Secondly, the batch size was set to 32, allowing the model to learn from a diverse range of samples in each iteration. Thirdly, the number of training epochs was limited to 200, preventing the model from overfitting by continuing to train beyond the point where it starts to memorize the training data instead of learning general patterns. Lastly, batch normalization and dropout techniques were utilized to further reduce overfitting. Batch normalization standardizes the inputs to each layer, making the model more robust to changes in the input distribution, while dropout randomly drops out neurons during training, forcing the model to rely on multiple features rather than just one or two dominant ones. Together, these measures contribute to improving the model's ability to generalize effectively to new, unseen data.
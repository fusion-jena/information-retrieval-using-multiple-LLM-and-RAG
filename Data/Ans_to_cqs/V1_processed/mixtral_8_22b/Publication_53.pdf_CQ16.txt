The authors use a combination of metrics to determine when training is complete. They monitor the error rate, training loss, and validation loss during the training process. In one instance, they mention achieving an error rate of 0.1538 (15.38%) after five epochs, along with a validation loss of 0.4211 and a training loss of 0.4211. However, it is not explicitly stated whether these values were used as stopping criteria. Later, they fine-tune their model by unfreezing the entire model and training at a 10-fold lower learning rate (10^-4). After ten epochs, they achieve an accuracy of 94.8% (error rate of 5.2%) with a training loss of 0.1397 and a validation loss of 0.1446. Again, it is not clear if these specific values were used as stopping criteria. Based on the information provided, it appears that the authors may have used a fixed number of epochs (ten) as a stopping criterion rather than relying solely on validation loss plateaus or other performance metrics.
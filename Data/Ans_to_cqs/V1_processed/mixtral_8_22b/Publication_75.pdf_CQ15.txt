In the given context, the optimization technique applied in the deep learning pipeline is mentioned during the experimentation phase. The most efficient solver for classification turned out to be Adam. This indicates that the Adam optimizer was used in the deep learning pipeline for the proposed model, SeedNet.

Adam (Adaptive Moment Estimation) is an optimization algorithm that can be thought of as a combination of RMSprop and Stochastic Gradient Descent with momentum. It calculates adaptive learning rates for different parameters from estimates of first and second moments of the gradients. Adam has been shown to perform well in practice and is often a preferred choice for deep learning models due to its computational efficiency and little memory requirements.

However, it should be noted that while Adam performed better than Sgdm (Stochastic gradient descent with momentum) in this particular case, the optimal optimization technique may vary depending on the specific problem and dataset. Other popular optimization techniques include RMSprop, Adagrad, and Adadelta. Each of these algorithms has its own strengths and weaknesses, and the choice of optimizer depends on factors such as convergence speed, generalization performance, and ease of hyperparameter tuning.
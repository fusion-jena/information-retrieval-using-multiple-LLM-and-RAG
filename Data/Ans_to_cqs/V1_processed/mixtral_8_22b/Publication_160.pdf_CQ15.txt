The authors use two main optimization techniques in their deep learning pipeline: Stochastic Gradient Descent (SGD) and Adam optimizer. They first train the top fully connected layer of a ResNet152 model using SGD with a learning rate of 10^-3 and a weight decay rate of 5 x 10^-4. Then they train the entire network using SGD. For training the ResNet18 models, they use the Adam optimizer with a learning rate of 10^-3 and default settings according to the original study by Kingma and Ba (2014).
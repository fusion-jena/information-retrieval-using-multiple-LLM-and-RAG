The text does not provide explicit information about specific regularization methods such as dropout or L2 regularization used in the deep learning pipeline to prevent overfitting. However, it mentions several techniques employed during model training that can help reduce overfitting:

1. Data Augmentation: The images in the training set were augmented through various transformations like flipping horizontally/vertically, adjusting brightness, saturation, contrast, and hue. These modifications increase the diversity of data seen by the model during training, making it more robust and improving its ability to generalize.

2. Exponential Decay Learning Rate: The initial learning rate of 0.001 was exponentially decayed every 3 epochs with a factor of 0.94. This strategy helps the model converge faster towards optimal weights without overshooting them, which could lead to poor performance on unseen data (overfitting).

While these techniques contribute to preventing overfitting indirectly, they do not directly address regularization methods like dropout or L2 regularization mentioned in the query. Therefore, based on the given context, I cannot provide specific details regarding those regularization methods.
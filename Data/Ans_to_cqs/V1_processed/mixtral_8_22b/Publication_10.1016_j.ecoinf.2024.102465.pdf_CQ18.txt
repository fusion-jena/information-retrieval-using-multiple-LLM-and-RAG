Based on the given information, there isn't a direct mention of a specific strategy used to monitor the model performance during training. However, it can be inferred from the use of the XGBoost model and the SHAP method that some form of monitoring or evaluation might have been done.

XGBoost is known for its high predictive power and efficiency, which makes it suitable for large datasets. It also provides built-in functions for cross-validation, early stopping, and regularization, which help prevent overfitting and improve model generalizability. These features could potentially serve as strategies to monitor the model performance during training.

Furthermore, the use of the SHAP (SHapley Additive exPlanations) method suggests that feature importance and contribution were considered when interpreting the results. This method helps understand how each input variable contributes to the output, providing insights into the model behavior and performance.

However, without explicit details about the implementation, it cannot be definitively stated what exact strategy was used to monitor the model performance during training.
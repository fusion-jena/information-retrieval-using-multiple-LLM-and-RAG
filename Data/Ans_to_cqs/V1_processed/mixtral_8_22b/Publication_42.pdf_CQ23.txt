To ensure the generalizability of the deep learning model, several measures were taken. Firstly, a diverse dataset was utilized, consisting of images from various sources and under different conditions. This helped prevent the model from being biased towards specific features or patterns found only in certain types of images. Secondly, stratified splitting was employed during the train-test split process. This ensured that both the training set and test set contained a representative distribution of classes, preventing any potential imbalance between them. Lastly, cross-validation was performed to further validate the robustness of the model. By dividing the dataset into multiple folds and iteratively training and testing the model on each fold, cross-validation allowed for a more comprehensive evaluation of the model's performance across different subsets of the data. These combined efforts aimed to minimize overfitting and improve the overall generalization capabilities of the deep learning model.

Based on the provided context, there are no explicit mentions of these specific techniques being applied to ensure the generalizability of the deep learning model. However, some relevant information can be inferred. For instance, the use of dropout regularization with a probability of 0.3 after the second to last hidden layer suggests that steps were taken to reduce overfitting and improve the model's ability to generalize. Additionally, the selection of the final architecture based on having one of the top three highest F1-scores while also having the lowest number of learnable parameters implies a focus on finding a balance between model complexity and performance. While not directly mentioned, these practices could contribute to enhancing the model's generalizability.
The given context does not provide information on specific optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). Instead, it focuses on data collection methods for studying free-ranging Amur tigers and validating the optimal size for algorithm training and test sets. It discusses the use of holdback trials to establish the minimum number of trails required for testing and mentions achieving optimal classification accuracy with smaller test set sizes compared to training set sizes. However, without explicit mention of optimization techniques like SGD or Adam, I cannot definitively state which ones were employed in their deep learning pipeline based solely on this context.
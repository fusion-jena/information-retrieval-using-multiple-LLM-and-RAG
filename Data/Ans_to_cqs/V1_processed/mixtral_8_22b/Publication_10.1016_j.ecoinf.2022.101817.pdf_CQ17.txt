The context does not provide specific information about the regularization methods used in the deep learning pipeline mentioned. However, some common regularization techniques used to prevent overfitting in deep learning include Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overfitting. This can be done by setting a certain percentage of input units to zero at each update during training time.

L2 Regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the magnitude of all parameters. This encourages smaller weights, leading to simpler models with better generalization capabilities.

Other regularization techniques include early stopping, data augmentation, and batch normalization. Early stopping involves monitoring the validation error while training the model and stopping when it starts increasing. Data augmentation generates new training samples by applying transformations such as rotation, scaling, and flipping to existing ones. Batch normalization standardizes the inputs for each mini-batch, reducing internal covariate shift and improving convergence speed.
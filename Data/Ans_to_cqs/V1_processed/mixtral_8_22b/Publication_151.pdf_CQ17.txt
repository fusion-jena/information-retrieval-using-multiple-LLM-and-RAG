The text does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some common techniques include dropout, L2 regularization, early stopping, and data augmentation. Dropout randomly deactivates neurons during training to prevent co-dependencies among them. L2 regularization adds a penalty term to the loss function to discourage large weights. Early stopping halts training once validation performance stops improving. Data augmentation generates new samples from existing ones through transformations like rotation or scaling. These methods help generalize the model and avoid overfitting.
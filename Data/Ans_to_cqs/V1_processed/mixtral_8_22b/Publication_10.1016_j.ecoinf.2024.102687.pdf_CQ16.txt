Based on the given information, there isn't a direct mention of the specific criteria used to determine when training is complete. However, we can infer some details about the training process.

The model was fine-tuned using the Adam optimizer with a default learning rate of 0.0001 and a weight decay of 0. The experiments were conducted in PyTorch on 8 x NVIDIA GeForce RTX 2080 Ti. Fine-tuning was performed on the entire model for 100 epochs, with one epoch corresponding to 500 episodic tasks.

From this, we can deduce that the training process was determined based on the number of epochs rather than a validation loss plateau or other similar metrics. In machine learning, an epoch refers to one cycle through the full training dataset. Therefore, in this case, the training was completed after going through the full dataset 100 times.

However, without explicit information regarding the use of validation loss plateaus or other stopping criteria, it cannot be definitively stated what criteria were used to determine when training was complete beyond the specified number of epochs.
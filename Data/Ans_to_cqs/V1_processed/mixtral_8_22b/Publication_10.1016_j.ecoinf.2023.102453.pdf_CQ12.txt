In this pipeline, two types of deep learning models are used - Vision Transformers (ViT) and EfficientNetV2B0.

Vision Transformers (ViT) are based on an alternative attention-based model design rather than Convolutional Neural Networks (CNN). They are inspired by the transformer architecture, which was initially developed for natural language processing tasks. However, they have been adapted for image classification problems. Specifically, MobileViT models are chosen due to their smaller size, making them suitable for embedded systems like Raspberry Pi.

On the other hand, EfficientNetV2B0 is part of the EfficientNetV2 family of models, ranging from B0 to B7. It was developed using neural architecture search (NAS) and optimized for parameter efficiency and training speed. This model uses progressive learning to adapt regularization to the image size.

Therefore, both Vision Transformers (specifically MobileViT) and EfficientNetV2B0 are employed in this pipeline.
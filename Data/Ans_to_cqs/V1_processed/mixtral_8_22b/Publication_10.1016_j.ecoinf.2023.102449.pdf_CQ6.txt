Before training a deep learning model, several preprocessing steps can be applied to improve its performance. These include data normalization, scaling, and cleaning. Data normalization involves transforming the dataset so that it has a specific range or distribution, often between 0 and 1 or with a standard deviation of 1. This helps prevent any single feature from dominating the others during training. Scaling refers to adjusting the values of features to have similar magnitudes, which can help gradient descent converge more quickly. Cleaning involves removing noise, outliers, or missing values from the dataset to ensure accurate predictions. In this case, the encoder uses a pre-processing block containing a convolution with a wide receptive field (5 × 5 kernel) and signal coarsening along the time axis (2 × 1 kernel). Coarsening follows changes suggested in He et al. (2018), replicating later versions of BirdNet's down-sampling strategy by recombining the concatenated output of 2 × 2 maximum and 2 × 2 average pooling using a 1 × 1 convolution.
Before training a deep learning model, several preprocessing steps may be involved depending on the nature of the data and the specific requirements of the model. These steps often include:

1. Data Cleaning: This involves handling missing values, removing duplicates, correcting inconsistent entries, and dealing with outliers.

2. Normalization/Standardization: This step scales numeric columns to a standard scale to ensure all features contribute proportionately to the model fitting. Standardization transforms the data to have mean=0 and variance=1, while normalization scales the data to a range typically between 0 and 1. In the given context, feature scaling refers to preprocessing the input features so that they each have a range of [0,1], retaining the original distribution shape. This is achieved through min-max scaling.

3. Encoding Categorical Variables: Deep learning models cannot handle categorical variables directly; hence, they need to be converted into numerical form. Techniques like label encoding, one-hot encoding, or ordinal encoding might be employed based on the variable's nature.

4. Splitting Dataset: Before training, itâ€™s common practice to split the dataset into training, validation, and test datasets. Training set is used to train the model, validation set helps tune hyperparameters, and test set evaluates the final performance of the trained model.

5. Data Augmentation: This technique is mainly applied to image data where new images are created from existing ones by applying various transformations such as rotation, zooming, flipping, etc. It increases the amount of training data and helps prevent overfitting.
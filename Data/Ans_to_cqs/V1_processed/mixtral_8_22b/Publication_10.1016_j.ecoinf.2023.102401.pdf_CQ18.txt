To monitor the model performance during training, the authors implement a strategy involving several steps. Firstly, they train the proposed model using the PyTorch framework and the Windows operating system. They use a GeForce RTX 3060 12G GPU and an Intel(R) Xeon(R) E5â€“1620 v3 CPU @ 3.50 GHz for this purpose. The Stochastic Gradient Descent (SGD) optimizer is employed to optimize the model, with a batch size of 8. The momentum is set to 0.937, and the training epoch is 300.

After training, the authors evaluate the performance of the trained model by validating it against test images. They compute various metrics such as accuracy, recall, F1 score, and others to assess the model's effectiveness. The primary measure used for model evaluation is the mean Average Precision (mAp). It provides a comprehensive analysis of the model's detection accuracy, recall, and F1 scores.

Based on these evaluations, the authors adjust the parameters and structure of the model to improve its detection performance. They iterate the training and evaluation process until achieving a satisfactory level of performance. Therefore, the strategy to monitor the model performance during training includes initializing the model, setting up the environment, training the model, evaluating its performance using multiple metrics, and fine-tuning the model based on the evaluation results.
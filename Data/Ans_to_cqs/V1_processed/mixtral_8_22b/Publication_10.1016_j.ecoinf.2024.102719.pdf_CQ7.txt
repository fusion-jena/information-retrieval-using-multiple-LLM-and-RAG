The entire dataset was randomly divided into three parts: a training set (60%), a validation set (20%), and a testing set (20%). Each Machine Learning (ML) model was then trained on the training set, optimized on the validation set, and evaluated on the testing set. This approach allows for better tuning of the model's hyperparameters during the training phase and provides a more accurate assessment of its performance when exposed to previously unseen data.

The training set is used to fit the model, i.e., adjust the weights and biases of the neural network based on the input features and corresponding target values. The validation set serves two purposes: firstly, it helps prevent overfitting by allowing early stopping when the model starts performing poorly on this set; secondly, it enables hyperparameter tuning through cross-validation or grid search methods. Lastly, the testing set remains completely separate from both the training and validation processes, providing an independent evaluation of the final model's performance.
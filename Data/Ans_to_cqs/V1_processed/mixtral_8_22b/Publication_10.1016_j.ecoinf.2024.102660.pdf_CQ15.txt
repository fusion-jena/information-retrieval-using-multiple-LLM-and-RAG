Based on the given information, it appears that the optimization technique used in the deep learning pipeline is not explicitly mentioned. However, there are some details about the learning rate adjustment strategy used during the network training process. The initial learning rate was set to 0.001, and the learning rate was adjusted using the equal-interval adjustment strategy (StepLR), halving the original rate every 10 epochs of iteration. While this does not directly specify the use of specific optimization algorithms like Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam), it provides insight into how the learning rate was managed during the training process. Unfortunately, without more explicit information regarding the optimization algorithm employed, I cannot provide a definitive answer to which optimization techniques were applied in the deep learning pipeline.
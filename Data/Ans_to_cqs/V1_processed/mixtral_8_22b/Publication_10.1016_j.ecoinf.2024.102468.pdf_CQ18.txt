The strategy implemented to monitor the model performance during training is not explicitly stated in the given context. However, there are some indications about how the performance was evaluated. It appears that the F1-score was used as a metric to assess the model's performance, where a score closer to 1 indicated better performance due to a good balance between precision and recall. Additionally, multiple runs were conducted to account for any variability introduced by random factors, such as random initialization of weights in artificial neural networks (ANNs) or random sampling during training. Therefore, while the specific monitoring strategy is unclear, we can infer that the overall approach involved evaluating the F1-score across multiple runs to ensure robustness and reliability in the results.
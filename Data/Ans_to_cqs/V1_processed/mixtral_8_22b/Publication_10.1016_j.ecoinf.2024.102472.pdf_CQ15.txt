In the given context, there is no explicit mention of specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being applied in the deep learning pipeline. However, some optimization processes can be inferred.

For instance, during the training phase of the Multi-Layer Perceptron Neural Network (MLPNN), iterative error back-propagation adjusts connection weights until reaching a target minimal error. This process could be likened to gradient descent optimization methods, where the goal is to minimize the cost function by adjusting parameters based on the computed gradients.

Additionally, the text mentions that all hyperparameters were tuned with a grid search method. Grid search is a common technique used for hyperparameter optimization, which involves defining a set of possible parameter values and exhaustively trying all combinations to find the best one.

While these points do not directly address the use of specific optimization techniques like SGD or Adam, they suggest that optimization processes are indeed part of the deep learning pipeline discussed in the context.
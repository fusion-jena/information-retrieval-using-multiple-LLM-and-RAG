The papers mentioned do not directly address the issue of handling randomness in the deep learning pipeline. However, some general strategies can be used to manage randomness in machine learning algorithms, including deep learning pipelines. These include setting a fixed random seed value, using cross-validation techniques, and employing ensemble methods such as bagging or boosting. Setting a fixed random seed value ensures reproducibility by initializing the pseudorandom number generator with the same seed each time the algorithm is run. Cross-validation involves splitting the data into multiple subsets and training the model on different combinations of these subsets to reduce overfitting and improve generalization performance. Ensemble methods combine multiple models to produce more accurate predictions than any single model alone. Bagging involves creating multiple bootstrap samples from the original dataset and training a separate model on each sample, while boosting involves iteratively adjusting the weights assigned to individual observations based on their prediction errors.
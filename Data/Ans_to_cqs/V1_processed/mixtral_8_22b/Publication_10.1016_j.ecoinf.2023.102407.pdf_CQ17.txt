The papers mentioned do not directly discuss regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization techniques include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust to small changes in input features. L2 regularization adds a penalty term to the loss function based on the sum of squared weights, encouraging smaller weight values and reducing overfitting. Other regularization methods include early stopping, data augmentation, and batch normalization.
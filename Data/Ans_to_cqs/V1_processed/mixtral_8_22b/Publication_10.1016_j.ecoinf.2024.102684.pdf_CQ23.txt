To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the dataset was divided into three subsets: training (60%), validation (20%), and testing (20%). This division aimed to provide an unbiased estimation of error and prevent overfitting. Data augmentation was applied solely to the training data to avoid duplication across different partitions.

Secondly, the models were evaluated using the coefficients of determination (R2) and the root mean squared error (RMSE) metrics. These metrics helped measure the percentage of variation in the response variable explained by the model and quantify the difference between the actual and predicted values, respectively.

Thirdly, the issue of overfitting was addressed by comparing the performance of the models on the training, validation, and testing datasets. Overfitting occurs when a model performs well on the training data but poorly on the validation and testing data. By monitoring the performance on all three datasets, the researchers ensured that the model did not become too specialized to the training data.

Lastly, while conducting large cross-validation experiments with deep learning-based models can be computationally expensive, the researchers acknowledged the importance of evaluating the model's generalization to new sites. They mentioned that experimenting with different sites or spatially distinct areas in training, validation, and testing does not guarantee generalization to new sites, especially if those sites exhibit unique spatial variability and represent a different domain.

However, the researchers did not explicitly state whether they employed techniques like stratified sampling or cross-validation to further improve the model's generalizability. Instead, they focused on controlling every aspect of the hyperparameter search process to avoid potential problems.
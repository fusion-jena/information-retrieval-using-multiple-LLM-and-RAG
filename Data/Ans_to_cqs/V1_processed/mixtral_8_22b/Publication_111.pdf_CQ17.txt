The text mentions two regularization strategies to address overfitting in the deep learning pipeline. First, it suggests using the early stopping strategy, which stops training when the generalization performance starts to degrade after a certain number of epochs. This strategy helps avoid excessive training epochs that could lead to overfitting. Second, the text discusses the dropout approach, which trains neural networks with alternate topologies in parallel by randomly dropping out a proportion of layer neurons. This technique prevents the network from relying too heavily on any single feature or set of features, thus reducing overfitting. Although not explicitly mentioned, L2 regularization is also commonly used to prevent overfitting in deep learning pipelines. It adds a penalty term to the loss function, discouraging large weights and promoting simpler models.
The given context does not provide explicit information about the specific criteria used to determine when training is complete. However, based on general machine learning practices, some commonly used criteria include:

1. Validation Loss Plateau: Training can be considered complete if the validation loss stops decreasing or remains constant over several epochs. This indicates that the model has learned all it can from the training data and further training may lead to overfitting.

2. Maximum Number of Epochs: Training can also be stopped after a certain number of epochs have been completed. This is often used when time or computational resources are limited.

3. Early Stopping: In this approach, training is halted when the performance on a validation set stops improving. This helps prevent overfitting and saves time.

4. Learning Rate Schedule: Some models use a learning rate schedule where the learning rate is reduced after each epoch or after a certain number of epochs. Training can be considered complete once the learning rate reaches a minimum threshold.
The deep learning model used in this study is DeepLabV3+, and its training parameters are listed in Table 1. Some of the key hyperparameters used in this model are:

* Learning Policy: Poly
* Base Learning Rate: 0.0001
* Learning Rate Decay Factor: 0.1
* Learning Rate Decay Step: 2000
* Learning Power: 0.9
* Training Number of Steps: >=100,000
* Momentum: 0.9
* Train Batch Size: 2
* Weight Decay: 0.00004
* Train Crop Size: '513,513'
* Last Layer Gradient Multiplier: 1
* Upsample Logits: True
* Drop Path Keep Prob: 1
* TF Initial Checkpoint: deeplabv3\_pascal\_train\_aug
* Initialize Last Layer: False
* Last Layers Contain Logits Only: True
* Slow Start Step: 0
* Slow Start Learning Rate: 1e-4
* Fine Tune Batch Norm: False
* Min Scale Factor: 0.5
* Max Scale Factor: 2
* Scale Factor Step Size: 0.25
* Atrous Rates: [6,12,18]
* Output Stride: 16

These hyperparameters were chosen based on the recommendations for efficient model training in DeepLabV3+. For example, larger batch sizes are recommended for more efficient training, so a train batch size of 2 was used. Additionally, the base learning rate was set to 0.0001, and the learning rate decay factor was set to 0.1 to help prevent overfitting during training.
The text does not provide explicit information about how the hyperparameters of the models were optimized. However, it mentions that the learnable frontends had a slight advantage because the spreads of the Gaussian pooling filters for downsampling were learnable. This suggests that some form of optimization or learning process was applied to these parameters. Additionally, the authors mention that they did not experiment with fft and hop sizes to keep the number of experiments manageable, implying that they might have considered tuning those hyperparameters but decided against it. Unfortunately, without more specific details, it cannot be determined whether techniques such as grid search or random search were employed for hyperparameter optimization.
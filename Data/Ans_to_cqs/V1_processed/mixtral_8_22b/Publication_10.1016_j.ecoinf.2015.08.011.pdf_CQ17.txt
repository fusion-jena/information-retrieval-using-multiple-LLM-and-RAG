According to the given context, the most frequently used regularization technique to prevent overfitting in the deep learning pipeline is early stopping. This technique involves monitoring the model's performance on a separate validation dataset during training. When the model starts to overfit, the training process is halted before the model can memorize the training data. However, other common regularization techniques like dropout or L2 regularization are not explicitly mentioned in the provided text. Dropout is a technique where some random subset of activations are ignored during training, while L2 regularization adds a penalty term to the loss function to discourage large weight values. These methods could also potentially help prevent overfitting but are not discussed in detail here.
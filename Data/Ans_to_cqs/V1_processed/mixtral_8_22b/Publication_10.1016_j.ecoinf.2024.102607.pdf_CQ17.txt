The provided context does not directly discuss specific regularization methods such as dropout or L2 regularization used to prevent overfitting in the deep learning pipeline. However, it mentions that an incorrect sampling strategy may increase training complexity and lead to overfitting phenomena (Gu et al., 2022). Additionally, Zhang et al. (2020) introduced a hexagonal random-sampling strategy along with a tile model to improve the accuracy of LULC mapping and potentially mitigate overfitting issues. While these approaches do not explicitly involve regularization methods like dropout or L2 regularization, they demonstrate efforts to manage overfitting in machine learning models. Regularization methods like dropout and L2 regularization work differently; dropout randomly drops out neurons during training to reduce co-dependencies between them, while L2 regularization adds a penalty term to the loss function to discourage large weights. Both methods help prevent overfitting by reducing model complexity and improving generalization.
The given context does not explicitly mention specific techniques like stratified splitting, oversampling, undersampling, or diverse data collection to address data bias during preprocessing of the deep learning pipeline. However, there are some relevant techniques mentioned that help improve the model's performance and robustness despite having a relatively small dataset.

Firstly, the authors employed a two-stage training process for their Convolutional Neural Network (CNN) model. During the first stage, they utilized a pretrained convolutional part of the network to extract features from the images. Then, only the top classifier (fully connected layer) was trained based on these extracted features. This method helps leverage existing knowledge from other tasks while adapting the model to the new task.

Secondly, the authors implemented data augmentation by randomly deforming input images in each training cycle. Data augmentation is a technique used to increase the amount of training data by applying various transformations to the original data. It helps prevent overfitting and improves the generalization capabilities of the model.

Thirdly, the authors applied dropout techniques to further enhance the model's robustness. Dropout is a regularization technique that involves temporarily removing random neurons along with their connections during training. By doing so, the model becomes more resilient to noise and variations in the input data.

Lastly, the authors chose a suitable network architecture (ResNet-50) that balances representation power and complexity, enabling faster training without compromising accuracy. They also ensured a controlled environment for image acquisition, minimizing potential biases introduced by uncontrolled factors.

In summary, although the given context does not directly discuss techniques like stratified splitting, oversampling, undersampling, or diverse data collection, it highlights several methods used to mitigate data bias and improve model performance. These include a two-stage training process, data augmentation, dropout techniques, selecting an appropriate network architecture, and controlling the image acquisition protocol.
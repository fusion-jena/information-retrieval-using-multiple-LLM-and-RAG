The text does not explicitly mention how the hyperparameters of the models were optimized. However, it provides some details on the specific values chosen for certain hyperparameters. For instance, Random Forests were trained using 500 trees, with a minimum size of terminal nodes set to five, and all descriptors were sampled for splitting at each node. Gradient Boosting Machines were trained by ensembling a maximum of 10,000 trees, with shrinkage and interaction parameters set to 0.01 and 1, respectively. It appears that these hyperparameter choices were made based on the authors' experience and knowledge rather than through a systematic optimization process like grid search or random search.
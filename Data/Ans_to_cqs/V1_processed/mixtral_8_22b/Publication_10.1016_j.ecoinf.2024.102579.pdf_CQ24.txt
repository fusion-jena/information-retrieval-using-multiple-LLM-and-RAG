The paper does not provide any direct information about handling randomness in the deep learning pipeline. However, some common strategies used to manage randomness in machine learning models, including deep learning pipelines, are:

1. Fixed Random Seed: Setting a fixed random seed ensures reproducibility of results across different runs of the model. By setting a specific seed value, the same sequence of pseudo-random numbers will be generated each time the code is executed, leading to consistent results.

2. Cross-Validation: Using cross-validation techniques like k-fold cross-validation helps to mitigate the impact of random initialization of parameters in neural networks. It involves splitting the dataset into multiple folds and training/testing the model on different combinations of these folds. Averaging the performance metrics from these iterations provides a more robust estimate of the model's performance.

3. Ensemble Methods: Combining multiple models trained with different initializations or architectures can improve the stability and accuracy of predictions. Techniques such as bagging, boosting, and stacking can be applied to create ensembles of models, reducing the variability caused by randomness.

4. Regularization: Applying regularization techniques like L1, L2, or dropout can help prevent overfitting and improve the generalizability of the model. These methods introduce controlled noise during training, making the model more resilient to variations in input data and parameter initialization.
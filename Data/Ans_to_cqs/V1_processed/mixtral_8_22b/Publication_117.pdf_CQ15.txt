The text mentions that the authors used the Adam optimizer during their deep learning pipeline. Specifically, they used a learning rate of 10^-4 while training the last seven layers of the network for 25 epochs. Afterward, they unfreeze all layers, reduce the learning rate to 10^-6, and train for another 20 epochs. They also mention that they chose the Adam optimizer and learning rates based on minimizing the difference between the automatic and expert counts. However, the text does not provide details about how they determined the optimal hyperparameters or if they tried other optimization techniques like Stochastic Gradient Descent (SGD).
The given context does not provide explicit information about the specific regularization methods used in the described deep learning pipeline. However, based on general knowledge of deep learning practices, some commonly used regularization techniques to prevent overfitting include Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overreliance on any single feature. On the other hand, L2 Regularization adds a penalty term to the loss function, encouraging smaller weights and thus reducing complexity and preventing overfitting. Other popular regularization methods include early stopping, weight decay, and batch normalization.
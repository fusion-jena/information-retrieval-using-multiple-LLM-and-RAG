The given references do not provide explicit information about the optimization techniques used in the deep learning pipelines. However, it is common practice in deep learning to use various optimization techniques such as Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), Root Mean Square Propagation (RMSProp), etc. These methods help minimize the loss function during training, thereby improving model performance. Without specific details mentioned in the references, one cannot definitively state which optimization techniques were employed in these studies.
The criteria used to split the data for deep learning model training typically include a combination of random sampling and stratified sampling methods. Random sampling is often used when the dataset is large enough to ensure that each subset (train, test, validation) contains representative samples from the entire population. Stratified sampling can be employed if there are specific classes or categories within the data that need to be equally represented across all subsets. In this case, the data is divided into strata based on these class labels, and then samples are drawn randomly from each stratum. This ensures that the distribution of classes remains consistent across the different subsets. Additionally, it is common practice to use a larger portion of the available data for training purposes, while reserving smaller portions for testing and validation. A popular ratio for splitting the data is 80/10/10, where 80% of the data is allocated for training, 10% for testing, and another 10% for validation. However, depending on the size and complexity of the dataset, other ratios may also be appropriate.
The given context does not provide explicit details about the postprocessing steps involved after the model training for either the Convolutional Neural Network (CNN) or Extreme Gradient Boosting (XGB) models. However, based on common practices in machine learning, some possible postprocessing steps could include:

1. Evaluation metrics: Calculating various evaluation metrics such as accuracy, precision, recall, F1 score, ROC curves, etc., to assess the performance of the models. These metrics help understand how well the models have learned from the training data and their ability to generalize to unseen data.

2. Confusion Matrix: Creating a confusion matrix to visualize the performance of the classification models. The confusion matrix displays the number of correct and incorrect predictions made by the classifier compared to the actual values. This helps identify any biases or imbalances in the dataset.

3. Saliency maps: Generating saliency maps for CNN models to highlight important features contributing to the final output. Saliency maps help interpret the decisions made by the model and improve transparency in deep learning applications.

4. Hyperparameter tuning: Optimizing hyperparameters using techniques like grid search, random search, or Bayesian optimization to find the best combination of parameters that maximizes the model's performance.

5. Ensemble methods: Combining multiple models to create an ensemble that improves overall performance. Common ensemble methods include bagging, boosting, stacking, and voting.

Without specific information regarding the postprocessing steps used in this case, these suggestions serve as potential options based on standard practices in machine learning.
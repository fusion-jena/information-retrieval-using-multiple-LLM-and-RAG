Unfortunately, the given context does not provide information about any specific preprocessing steps taken before training the deep learning models. However, it can be assumed that some form of preprocessing would have been necessary due to the nature of the dataset, which includes datacubes with multiple channels (RGBI). Common preprocessing techniques for such datasets include normalization, scaling, or cleaning to ensure consistent input formats and reduce noise. Normalization typically involves adjusting pixel values to a common scale (e.g., between 0 and 1), while scaling may involve resizing images or adjusting their dimensions. Cleaning could entail removing or correcting corrupted or missing data points. Nevertheless, without explicit details from the provided context, one cannot definitively state what preprocessing steps were employed in this case.
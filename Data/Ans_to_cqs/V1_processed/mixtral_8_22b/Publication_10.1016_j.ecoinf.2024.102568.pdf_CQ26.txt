Based on the given context, there isn't explicit information about the use of specific techniques to address data bias during preprocessing of the deep learning pipeline. However, it can be inferred that the authors might have employed certain strategies to handle potential biases or imbalances in their datasets.

One common technique mentioned indirectly is the use of a MinMaxScaler for preprocessing the input features. While this doesn't directly address data bias, it does help normalize the data, ensuring that no single feature dominates the others due to its scale. Normalizing the data can also improve the overall performance of machine learning models.

Additionally, the mention of stratified sampling in the context of selecting the training and testing sets suggests that the authors may have considered class balance when partitioning the data. Stratified sampling ensures that the distribution of classes in the training and testing sets reflects the original dataset, thus reducing any potential bias introduced by skewed class distributions.

Although not explicitly stated, other techniques like oversampling, undersampling, or diverse data collection could potentially be applied depending on the nature of the dataset and the problem being addressed. These methods aim to mitigate issues related to class imbalance or limited representation of certain subgroups within the data.

Overall, based on the provided context, we cannot definitively state which specific techniques were used to address data bias during preprocessing of the deep learning pipeline. However, the use of MinMaxScaler and possible application of stratified sampling suggest that the authors took steps to ensure balanced and normalized data for optimal model performance.
In this context, there isn't explicit mention of specific strategies employed to handle randomness in the deep learning pipeline, such as managing the random seed value. However, we can infer some strategies based on the described process.

Firstly, the use of a validation set allows for monitoring the performance of the network during training and helps prevent overfitting. This indirectly addresses the issue of randomness because even if the initial weights of the network are randomly initialized, the validation set provides a way to evaluate the network's generalization ability.

Secondly, the application of data augmentation techniques introduces controlled variability into the dataset, which can help mitigate the effects of randomness. In this case, horizontal flipping, scaling contrast, and cropping are used. These transformations increase the diversity of the training data without altering its semantic content, making the model more robust to variations in the input data.

Lastly, the choice of the best performing model based on validation set accuracy after each epoch also contributes to handling randomness. By selecting the model with the highest validation accuracy, the impact of any potential negative effects caused by random initialization or other sources of randomness is minimized.

However, it should be noted that these strategies do not directly address the management of random seeds. Typically, setting a fixed random seed value ensures reproducibility in experiments involving random processes like weight initialization, shuffling datasets, etc. But since this aspect is not explicitly mentioned in the provided context, I cannot provide a direct answer regarding how random seed values are handled in this particular scenario.
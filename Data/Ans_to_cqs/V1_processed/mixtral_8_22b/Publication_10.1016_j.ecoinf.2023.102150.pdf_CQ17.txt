The paper by Minar and Naher (2018) provides an overview of recent advances in deep learning, including techniques for preventing overfitting. One common technique is dropout, which randomly drops out neurons from the network during training to reduce co-dependencies among them. Another technique mentioned is L2 regularization, also known as weight decay, which adds a penalty term to the loss function proportional to the square of the weights, encouraging smaller weights and thus simpler models. Other regularization methods include early stopping, which stops training when performance on a validation set starts to degrade, and batch normalization, which standardizes inputs to each layer to improve stability and convergence. However, it should be noted that these papers focus primarily on computer vision tasks rather than ecology or conservation biology.
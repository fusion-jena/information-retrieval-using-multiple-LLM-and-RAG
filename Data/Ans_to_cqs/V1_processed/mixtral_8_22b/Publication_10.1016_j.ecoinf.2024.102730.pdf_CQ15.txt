The study uses Multilayer Perceptron (MLP) models for their deep learning pipeline. To optimize these models, they employ several optimization techniques. One such technique mentioned is Stochastic Gradient Descent (SGD), specifically Barhate et al.'s (2023) hyperparameter-tuned batch-updated stochastic gradient descent approach. Additionally, another optimization technique utilized is Adam, which stands for Adaptive Moment Estimation. Both SGD and Adam are popular optimization algorithms used in deep learning to update network weights iteratively based on the training data. They help minimize the loss function during the training process.

In Table 2, it is shown that 'adam' was chosen as the optimal value for the'solver for weight optimization' parameter across all three regression tasks. This indicates that the Adam optimization algorithm was preferred over other methods like 'lbfgs'. Furthermore, 'adam' was also selected as the optimal choice for the 'learning rate schedule for weight updates', reinforcing the preference for the Adam optimization technique in this particular deep learning pipeline.
Unfortunately, there is no explicit mention or discussion about the specific criteria used to split the data for deep learning model training (e.g., train, test, validation) in the given context. Therefore, I cannot provide a direct answer based solely on the provided text snippets.

However, generally speaking, when working with machine learning models like deep learning, data is often divided into three subsets: training set, validation set, and testing set. The training set is used to fit the model, while the validation set helps tune hyperparameters and prevent overfitting. Lastly, the testing set evaluates the performance of the trained model. Common practices include splitting the dataset randomly or using techniques such as k-fold cross-validation.
In the given context, there isn't explicit information about the criteria used to determine when training is complete. However, it does mention that stochastic gradient descent is typically used as the learning algorithm, which is common in deep learning models.

Generally, in machine learning and deep learning, several criteria can be used to decide when training should stop. One commonly used criterion is the validation loss plateau. This means that if the validation loss stops decreasing significantly after a certain point during training, it might indicate that the model has reached its optimal performance and any further training may lead to overfitting.

Another criterion could be early stopping, where training is stopped once the validation error starts increasing again. Additionally, one could use a predefined maximum number of epochs or iterations as a simple stopping rule.

However, without additional context, I cannot provide a definitive answer regarding the exact criteria used to determine when training is complete in this particular scenario.
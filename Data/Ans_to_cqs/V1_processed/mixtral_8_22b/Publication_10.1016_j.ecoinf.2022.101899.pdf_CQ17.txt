The papers by Halgamuge et al. (2020) and Dowdy (2020) do not explicitly mention any specific regularization methods used to prevent overfitting in their deep learning pipelines. However, some common regularization techniques used in deep learning include dropout, L1/L2 regularization, early stopping, and batch normalization. Dropout randomly sets a fraction rate of input units to 0 during training time, helping to avoid overfitting. L1/L2 regularization adds a penalty term to the loss function, encouraging smaller weights and preventing complex co-adaptations on training data. Early stopping stops training when validation performance starts degrading, while batch normalization scales inputs to have zero mean and unit variance, reducing internal covariate shift and improving generalization. These methods may or may not have been employed in these studies but are commonly used in deep learning applications.
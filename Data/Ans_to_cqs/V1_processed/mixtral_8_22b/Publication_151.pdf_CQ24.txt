The text does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, there are some implicit indications regarding optimization techniques used during the training phase. One strategy mentioned is the use of a running average of the recent magnitude of the gradient to speed up training, known as momentum (Tieleman and Hinton 2012). This technique helps smooth out the fluctuations caused by individual gradients and can improve convergence rates. Additionally, the authors mention checking the training loss every 50 epochs and stopping the training when the value of the loss function fails to decrease in two consecutive checks. They also select the model with the optimal objective function value. These practices contribute to ensuring stable and efficient training, although they do not directly address managing randomness from sources like random seed values or other stochastic elements in the deep learning pipeline.
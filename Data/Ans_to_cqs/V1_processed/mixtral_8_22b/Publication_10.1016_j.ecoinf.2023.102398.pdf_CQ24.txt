Based on the given context, it appears that the focus is primarily on the architecture and performance of the deep learning model rather than addressing specific strategies for handling randomness in the deep learning pipeline. However, we can infer some potential strategies from the information provided.

Firstly, the use of a uniform training environment and the same dataset across different epochs suggests consistency in the data used for training. This could help mitigate the impact of randomness introduced by varying datasets or inconsistent training environments.

Secondly, the mention of optimized hyperparameters and avoiding overfitting and underfitting indicates careful tuning of the model. While this does not directly address randomness due to factors like random seed values, it shows an attempt to control other sources of variability in the model's performance.

Lastly, the adoption of a gradually increasing training cycle and accuracy calculation approach based on mean square error (MSE) implies a systematic and controlled methodology for training the model. This could potentially reduce the influence of randomness in the training process.

However, without explicit mention of strategies such as fixing the random seed value, using deterministic algorithms, or employing techniques like k-fold cross-validation, it is difficult to definitively state what strategies are used to handle randomness in the deep learning pipeline. Therefore, based on the provided context, I cannot provide a conclusive answer to this query.
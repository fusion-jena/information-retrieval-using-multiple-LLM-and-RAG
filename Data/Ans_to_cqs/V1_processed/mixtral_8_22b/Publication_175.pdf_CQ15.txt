The text does not provide explicit information about specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) used within the deep learning pipeline. However, it mentions that CNN architectures like ResNet utilize a process called back-propagation to adjust internal parameters and minimize classification errors. This implies that some form of gradient descent optimization technique was employed during the training phase. Additionally, the reference to optimization-based methods suggests that these methods might involve using optimization techniques to fine-tune models for better performance on new tasks.
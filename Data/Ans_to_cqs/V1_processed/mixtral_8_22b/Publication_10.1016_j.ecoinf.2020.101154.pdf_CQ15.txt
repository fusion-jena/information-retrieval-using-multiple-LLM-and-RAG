In this study, the authors use Gradient Boosting Regression Trees (GBRT), a type of ensemble machine learning algorithm, rather than deep learning methods. Therefore, traditional optimization techniques such as Stochastic Gradient Descent (SGD) or Adam are not directly applicable here. However, there are still some optimization techniques employed within the GBRT framework.

One key aspect of optimizing GBRT models involves tuning their hyperparameters. In this case, manual tuning and grid search were used to find optimal values for the learning rate, maximum tree depth, and number of iterations. The learning rate, also known as the shrinkage parameter, controls how much each tree contributes to the overall model. A lower learning rate means smaller contributions per tree but requires more iterations to converge. Maximum tree depth affects the degree of interaction between features in the model; shallower trees tend to perform better in GBRT due to reduced complexity and risk of overfitting.

To prevent overfitting, early stopping was utilized to determine when to halt model training. This method monitors performance on a validation dataset during training and stops once improvements plateau or begin to decline. Additionally, random subsets of training data were taken at each iteration for stochastic gradient boosting, adding another layer of regularization and improving overall model robustness.

While these techniques do not correspond exactly to those typically associated with deep learning pipelines like SGD or Adam, they serve similar purposes in terms of optimizing model performance and preventing overfitting.
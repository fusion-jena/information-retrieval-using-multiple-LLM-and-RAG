Based on the given context, there is no explicit mention of specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it is common practice in deep learning models to use techniques like dropout and L2 regularization to mitigate overfitting. Dropout involves randomly setting a fraction of input units to 0 during training time, which helps prevent complex co-adaptations on training data. On the other hand, L2 regularization adds a penalty term to the loss function, encouraging smaller weights and thus simpler models. Other popular regularization methods include early stopping, weight decay, and batch normalization. These techniques help generalize the model better to unseen data and avoid memorizing the training set.
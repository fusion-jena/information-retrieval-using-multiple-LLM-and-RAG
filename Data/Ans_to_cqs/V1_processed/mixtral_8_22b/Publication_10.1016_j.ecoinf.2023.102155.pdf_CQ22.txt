The performance of the deep learning models was evaluated using several metrics such as Mean Residual Sum of Squares (MeanRSS), Normalized Integrated Mean Square Error (NIMSLE), and median performance of the individual method. These metrics were used to compare the performance of various methods including kNN, LoopT, kmeans, random, CoFi, LoopA, LoopE, and Mixture methods under different conditions such as varying percentages of hidden observations and the implementation of lasso penalty and bias correction. The results showed that some methods performed better than others depending on these factors. For instance, LoopT and kNN were found to be competitive with the individual method even without the use of lasso penalty or bias correction for a low percentage of hidden observations. However, implementing lasso regularization resulted in improved performance, especially for poorly performing methods like kmeans, random, CoFi, LoopA, and LoopE. On the other hand, adding bias correction did not significantly affect the performance of the Mixture methods when lasso was applied.
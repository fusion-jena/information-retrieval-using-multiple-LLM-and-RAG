The references provided do not directly discuss the regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization techniques used in deep learning include Dropout, L1 Regularization, and L2 Regularization.

Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps to reduce overfitting. L1 Regularization adds an absolute value of the magnitude of coefficients as penalty term to the loss function, while L2 Regularization adds a squared magnitude of the coefficient as penalty term to the loss function. Both L1 and L2 Regularizations help to prevent overfitting by adding constraints to the model parameters. Other regularization techniques such as early stopping, weight decay, and batch normalization can also be used to prevent overfitting in deep learning pipelines.
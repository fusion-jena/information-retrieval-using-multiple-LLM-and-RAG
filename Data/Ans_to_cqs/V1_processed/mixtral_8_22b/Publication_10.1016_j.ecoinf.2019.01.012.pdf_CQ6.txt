The given context does not provide explicit information about any specific preprocessing steps taken before training the deep learning model. However, it can be inferred that some form of labeling was done on the dataset since the authors mention having nearly 30,000 labeled samples per class. Labeled datasets are crucial for supervised learning tasks like classification.

Preprocessing steps often depend on the nature of the dataset and the task at hand. Common preprocessing techniques include normalization or standardization, where the data is scaled to a certain range or made to follow a particular distribution. Data cleaning might also be necessary if there are missing values, outliers, or noise in the dataset. Image data may require additional preprocessing steps such as resizing, cropping, rotation, flipping, or augmentation to improve the robustness of the model.

In this case, the authors did not explicitly state whether they applied these preprocessing techniques. They directly mentioned training the Convolutional Neural Network (CNN) using ReLU activation for the convolutional layers and ELU activation on the fully connected layer. Therefore, without further context, one cannot definitively determine what preprocessing steps were involved before training the deep learning model.
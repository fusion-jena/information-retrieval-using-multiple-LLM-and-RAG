The authors mention that they stopped the network training after 70 epochs to prevent overfitting. An epoch refers to a single pass through the entire training dataset. Therefore, it appears that the criterion used to determine when training is complete is based on the number of epochs rather than other metrics such as validation loss plateau.
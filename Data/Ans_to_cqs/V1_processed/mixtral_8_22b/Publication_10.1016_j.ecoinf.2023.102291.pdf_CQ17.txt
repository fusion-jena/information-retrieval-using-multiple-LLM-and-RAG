The text does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions that the smallest dataset consisted of 3900 images, which suggests that some form of regularization might have been employed to avoid overfitting due to limited data. Common regularization techniques include dropout, L2 regularization, early stopping, and data augmentation. Dropout is a technique where randomly selected neurons are ignored during training, while L2 regularization adds a penalty term to the loss function to discourage large weights. Early stopping involves monitoring the validation error and halting training when it starts increasing. Data augmentation generates new samples from existing ones through transformations like rotation or scaling. Without further details, we cannot confirm which method(s) were utilized in this case.
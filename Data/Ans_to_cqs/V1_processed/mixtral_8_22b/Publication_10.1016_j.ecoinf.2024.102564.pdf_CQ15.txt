In the given context, two optimization techniques are mentioned for training Artificial Neural Networks (ANNs): Resilient Propagation (Rprop) and Scaled Conjugate Gradient (SCG). These methods are not explicitly stated as being part of a deep learning pipeline but can be considered optimization techniques within machine learning algorithms.

Resilient Propagation (Rprop) is a first-order optimization algorithm designed specifically for supervised learning in feedforward neural networks. It updates the weights based on local gradient information, making it efficient and adaptive to different problems.

Scaled Conjugate Gradient (SCG) is another optimization technique used for training ANNs. SCG belongs to the family of second-order optimization methods, which use both the gradient and Hessian matrix of the loss function to find the optimal solution. This approach often leads to faster convergence compared to first-order methods like Rprop.

Although not directly mentioned in the context, other popular optimization techniques commonly used in deep learning pipelines include Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam). SGD is a simple yet effective optimization algorithm widely used for training various types of neural networks. On the other hand, Adam combines the advantages of both AdaGrad and RMSProp, providing an efficient and adaptive learning rate for each parameter during training.
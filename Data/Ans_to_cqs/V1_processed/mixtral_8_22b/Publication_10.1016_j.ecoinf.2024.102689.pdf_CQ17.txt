The text does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions techniques such as pruning and knowledge distillation, which can be considered forms of regularization. Pruning is a process where unnecessary connections or neurons are removed from the network, reducing its complexity and preventing overfitting. Knowledge distillation involves transferring knowledge from a larger model to a smaller one, allowing the smaller model to maintain performance while being more efficient and less likely to overfit. Other common regularization methods include dropout, which randomly drops out nodes during training, and L2 regularization, also known as weight decay, which adds a penalty term to the loss function to discourage large weights. These methods were not explicitly mentioned but could potentially be part of the deep learning pipeline described in the text.
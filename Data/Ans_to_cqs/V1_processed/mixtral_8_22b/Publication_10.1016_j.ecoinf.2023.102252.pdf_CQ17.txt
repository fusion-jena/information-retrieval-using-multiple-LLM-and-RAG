The given context does not provide information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization techniques used in deep learning include Dropout, L1 Regularization, and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting. L1 Regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function, while L2 Regularization adds a penalty equal to the square of the magnitude of coefficients to the loss function. Both L1 and L2 Regularization help prevent overfitting by reducing the complexity of the model. Other regularization techniques include Early Stopping, Data Augmentation, and Batch Normalization.
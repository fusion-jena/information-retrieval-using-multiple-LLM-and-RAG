The text does not provide specific information on the regularization methods used in the deep learning pipeline to prevent overfitting. However, it mentions using TensorFlow, which is a popular deep learning framework that provides several built-in regularization techniques like dropout, L1 and L2 regularization, among others. Dropout is a technique where randomly selected neurons are ignored during training, which helps reduce interdependent learning amongst them. L1 and L2 regularizations add penalty terms to the loss function, discouraging complex models and thus preventing overfitting. It can be assumed that similar regularization methods were likely employed in this study, but without explicit mention, one cannot confirm the exact method(s) used.
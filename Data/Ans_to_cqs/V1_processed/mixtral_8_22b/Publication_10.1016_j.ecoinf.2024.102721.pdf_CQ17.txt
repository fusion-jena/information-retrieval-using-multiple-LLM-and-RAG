In the context of deep learning pipelines, there are several regularization methods used to prevent overfitting. Two commonly used techniques are Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training. They are "dropped-out" randomly, which helps to avoid overfitting. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. Essentially, each update to a neural network potentially represents a whole new network due to the dropped neurons.

On the other hand, L2 Regularization, also known as Ridge Regression or Weight Decay, is another method used to prevent overfitting. It involves adding a penalty equivalent to the sum of the squares of all the weights in the network to the loss function. This encourages smaller weights, leading to simpler models that generalize better.

These regularization methods help to improve the performance of deep learning models by reducing overfitting and improving their ability to generalize from the training data to unseen data.
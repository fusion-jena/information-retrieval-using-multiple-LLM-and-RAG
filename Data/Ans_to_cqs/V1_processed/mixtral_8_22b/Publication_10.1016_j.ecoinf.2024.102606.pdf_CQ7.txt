In the given context, the criterion used to split the data for deep learning model training is not explicitly mentioned. However, it can be inferred that the data was divided into training and evaluation sets using a method called K-fold cross-validation. In K-fold cross-validation, the original sample is randomly partitioned into K equal sized subsamples or folds. Of the K subsamples, a single subsample is retained as the validation data for testing the model, and the remaining K âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated K times, with each of the K subsamples used exactly once as the validation data. The K results can then be averaged to produce a single estimation. In this case, each model used a cross-validation of 5 crosses, meaning that the data was divided into five equal parts, and each part was used as the validation data once.
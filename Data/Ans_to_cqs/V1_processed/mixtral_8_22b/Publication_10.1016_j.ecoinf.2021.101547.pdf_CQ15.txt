Based on the given context, there isn't any explicit mention of specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being used in the deep learning pipeline. However, considering the typical use of these methods in deep learning models, it would be reasonable to assume that one of them was employed.

In the context of deep learning, optimization techniques like SGD and Adam are commonly used to update the weights of the neural networks during backpropagation. These algorithms help minimize the loss function, enabling the model to improve its predictions over time.

Since the text mentions that the network updates its internal parameters after each mini-batch of images, this suggests that some form of gradient descent optimization technique is likely being utilized. Without further information, however, it cannot be definitively stated whether SGD, Adam, or another method was chosen for this particular application.
To prevent overfitting in the deep learning pipeline, several regularization methods can be employed. In the given context, two such techniques are mentioned: reducing the number of convolutional kernels and incorporating residual structures.

Firstly, the study reduces the number of 3x3 convolutional kernels in the convolutional block from 64 to 32. This reduction helps decrease the model complexity, making it more generalized and less prone to overfitting. By having fewer parameters to learn, the model becomes less likely to memorize the training data and instead focuses on capturing the underlying patterns.

Secondly, the study integrates residual structures into the ResConv block based on the ResNet network. Residual structures help mitigate overfitting by enabling the creation of deeper networks without increasing computational cost or risking vanishing gradients. They do so by introducing shortcut connections between layers, allowing the gradient to flow directly through these connections during backpropagation. As a result, the network can maintain high accuracy even when adding more layers, thus preventing overfitting.

While not explicitly mentioned in the context, other common regularization methods include dropout and L2 regularization. Dropout randomly sets a fraction of input units to zero during training, which helps prevent complex co-adaptations among neurons and encourages the network to learn more robust features. On the other hand, L2 regularization adds a penalty term to the loss function proportional to the square of the magnitude of all weights in the network. This discourages large weight values and promotes simpler models, ultimately helping reduce overfitting.
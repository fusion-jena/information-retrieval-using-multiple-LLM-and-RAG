Based on the given context, there is no explicit mention of any specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization techniques include dropout, L1 and L2 regularization, early stopping, and data augmentation. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust by preventing complex co-adaptations on training data. L1 and L2 regularization add penalties to the loss function based on the weights of the neurons, encouraging simpler models and reducing overfitting. Early stopping involves monitoring the performance of the model on a validation set during training and halting the process when the performance stops improving. Data augmentation generates new training samples by applying random transformations to existing ones, increasing the diversity of the training data and helping to reduce overfitting. It is not clear if these techniques were employed in this particular case without further information.

Unhelpful 
The context does not provide enough information to determine what regularization methods were used to prevent overfitting in the deep learning pipeline. Therefore, I cannot give a definitive answer to this query.
AdamW optimizer is used for Hugging Face models, such as ViT and ResNet, which is the default optimizer in the transformers library by Hugging-Face. For Inception-V3, Adam optimizer is used, as it was the optimizer used in previous work and AdamW is not available in Keras. The learning rate is set to 10^-4 for batch size of 16 examples for Vanilla Inception-V3 and ViT, while for Deep Otolith ResNet, a learning rate of 10^-4 is used as suggested by Politikos et al. (2021). For every fine-tuning experiment, the model with the smallest validation loss is used for evaluation on the test set.
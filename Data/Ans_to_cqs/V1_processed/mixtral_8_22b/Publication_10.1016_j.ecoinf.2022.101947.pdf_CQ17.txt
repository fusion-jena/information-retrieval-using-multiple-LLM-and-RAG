The text does not provide information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. Regularization methods such as dropout and L2 regularization can be used to prevent overfitting in deep learning models. Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training, which helps to reduce interdependent learning amongst neurons. L2 regularization adds a penalty term to the loss function, which discourages large weights and encourages smaller ones, thus preventing overfitting. Other regularization techniques include early stopping, weight decay, and batch normalization.
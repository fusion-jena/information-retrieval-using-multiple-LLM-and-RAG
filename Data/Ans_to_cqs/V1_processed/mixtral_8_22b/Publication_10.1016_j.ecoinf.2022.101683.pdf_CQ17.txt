The text does not provide information about the regularization methods used in the deep learning pipeline. However, some common regularization methods used to prevent overfitting include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust by preventing it from relying too heavily on any one feature. L2 regularization adds a penalty term to the loss function, encouraging smaller weights and reducing overfitting. Other techniques such as early stopping, weight decay, and batch normalization can also be used to improve generalization performance.
During the training process, several strategies are employed to monitor the performance of the model. Firstly, the Adam optimizer is utilized, which adapts the learning rate based on the estimated moments of the gradients. This helps in achieving faster convergence and better generalization. Secondly, a scheduler is incorporated to adjust the learning rate when the network encounters a plateau, preventing overfitting and improving the overall performance. Thirdly, both Cross-Entropy and Focal Loss functions are considered for optimization. Cross-Entropy loss is commonly used for multi-class classification tasks, while Focal Loss addresses the issue of class imbalance by assigning more weights to misclassified examples. Lastly, various hyperparameters such as learning rates, batch sizes, weight decays, and dropouts are investigated to find the optimal combination for the best performance.
To address data bias during preprocessing of the deep learning pipeline, several techniques can be employed. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into training and testing sets while maintaining the same distribution of classes or labels in both sets. This ensures that the classifier learns from a representative sample of all classes, reducing any potential bias towards certain classes.

Oversampling is a technique where samples from underrepresented classes are duplicated to balance the dataset. This helps prevent the classifier from being biased towards overrepresented classes. However, it may lead to overfitting if not handled carefully.

Undersampling is another method to handle imbalanced datasets, where samples from overrepresented classes are removed to achieve balanced representation. While this reduces the risk of overfitting, it may result in loss of valuable information.

Lastly, diverse data collection aims to gather data from various sources, ensuring that the dataset represents different scenarios and conditions. This helps improve the generalization ability of the classifier and reduce bias towards specific patterns present only in limited data.

These techniques have been applied in various studies such as plant leaf classification using Deep Attention Dense CNN models (Pandey and Jain, 2022b), pine tree classification using DDYOLOv5 and ResNet-50 (Hu et al., 2022), and plant disease classification using dense convolutional neural networks (Ganguly et al., 2022b).
The study does not explicitly mention any specific regularization methods used to prevent overfitting in their deep learning pipeline. However, they do use several techniques that can help mitigate overfitting. For instance, they employ data augmentation, which is known to improve the performance of convolutional neural network (CNN)-based models and reduce the requirement of a large dataset for model training. Data augmentation involves creating new training samples by applying transformations such as rotation, scaling, or flipping to existing samples, thereby artificially expanding the dataset and making it more diverse. This helps prevent the model from memorizing the training set and encourages it to learn generalizable features. Additionally, the authors use transfer learning by fine-tuning pretrained Inception v.3 models on their custom datasets. Transfer learning allows them to leverage the knowledge gained by these models during their initial training on large-scale image classification tasks, reducing the risk of overfitting when adapting them to smaller, specialized datasets. Lastly, the authors configure their model parameters using a decay factor, which gradually reduces the learning rate during training, helping the model converge towards a better solution without overshooting. While these techniques contribute to preventing overfitting indirectly, the study does not discuss direct regularization methods like dropout or L2 regularization.
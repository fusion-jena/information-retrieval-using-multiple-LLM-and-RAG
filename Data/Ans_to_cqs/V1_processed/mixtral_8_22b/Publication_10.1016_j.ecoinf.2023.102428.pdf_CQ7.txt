Based on the given context, there isn't any explicit mention of how the data was split for training, testing, or validation purposes in the deep learning model. However, it can be assumed that the dataset was divided into training and testing sets based on common practices in machine learning and deep learning models. Typically, the dataset is randomly partitioned into two subsets: one for training the model and another for evaluating its performance. A commonly used ratio is 80% for training and 20% for testing. In some cases, a third subset called the validation set may also be created to fine-tune hyperparameters during the training process. Without further details about the specific methodology employed in this study, we cannot provide more precise information regarding the splitting criteria used for their deep learning model.
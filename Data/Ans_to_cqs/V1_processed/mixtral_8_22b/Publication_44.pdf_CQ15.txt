The text mentions the use of two optimization techniques in the deep learning pipeline: Stochastic Gradient Descent (SGD) and Adam.

In the case of YOLOv5s, the network was trained for 300 epochs using an Adam optimizer. Adam is an optimization algorithm that can be thought of as a combination of RMSprop and Stochastic Gradient Descent with momentum. It calculates individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.

On the other hand, the Faster R-CNN model was trained using Stochastic Gradient Descent (SGD) with a learning rate of 0.001. SGD is an iterative method for optimizing a differentiable objective function, often used for machine learning applications. It updates the weights of the model after computing the gradient of the cost function with respect to the weights for a small subset of the training data.

Both these optimization techniques help in improving the efficiency and accuracy of the models by minimizing the error or loss function.
Unfortunately, the provided context does not explicitly mention any specific dataset such as MNIST, CIFAR, or ImageNet. However, we do know that the study involves a dataset that was recorded at a different location and gradually expanded until satisfactory reconstructions were achieved. This dataset was then used to train a Variational Autoencoder (VAE) model, specifically a dynamic beta-VAE, for unsupervised and semi-supervised learning tasks. The goal was to obtain latent representations, which were subsequently clustered using K-means. These results were compared against several baseline methods, including Principal Component Analysis (PCA), Kernel-PCA, Hierarchical Cluster Analysis (HCA), and a conventional VAE. Therefore, while I cannot provide the exact name of the dataset used, I can confirm that it was employed in training a VAE model for representation learning purposes.
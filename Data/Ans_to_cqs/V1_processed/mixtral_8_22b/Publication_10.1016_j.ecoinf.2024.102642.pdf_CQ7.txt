Based on the provided context, it appears that the data was split into training and testing sets for deep learning model training. For the SEIO PBW vocalization training, a larger batch size of 5 was used compared to the blue whale Dcall training which had a batch size of 4. This difference in batch sizes was likely due to the fact that there were more samples available for SEIO PBW vocalization training. Additionally, the learning rate for SEIO PBW vocalization training was set to 1e-4 while the learning rate for blue whale Dcall training was set to 1e-3. These differences in hyperparameters suggest that the data may have been split based on the specific characteristics of each dataset.

Furthermore, the context mentions that the models were trained for 40 and 25 epochs respectively, but an early stopping method was used to prevent overfitting. This suggests that the data may have been split such that the training set contained enough examples to allow the model to learn effectively without becoming overfit.

However, the context does not provide explicit information about how the data was specifically split into train, test, and validation sets. It is possible that the authors used a common approach such as splitting the data into 80% for training, 10% for validation, and 10% for testing. Alternatively, they may have used a more sophisticated approach such as k-fold cross-validation or stratified sampling. Without additional context, it is difficult to determine exactly how the data was split for deep learning model training.
Before training a deep learning model, several preprocessing steps are typically involved to ensure the quality and consistency of the input data. These steps often include:

1. Data Cleaning: This involves removing or correcting any inconsistent, incorrect, or irrelevant data points within the dataset. This might also involve handling missing values through techniques such as imputation or deletion.

2. Normalization or Standardization: Deep learning models work best when all features have similar scales. Therefore, it's common to apply normalization or standardization techniques to rescale the feature values. Normalization usually rescales the values between 0 and 1, while standardization transforms them to have a mean of 0 and a standard deviation of 1.

3. Feature Scaling: Similar to normalization and standardization, feature scaling aims to bring all features to a comparable scale. This could be done using various techniques like Min-Max scaling, which scales the values between a given range, or Robust Scaling, which uses the interquartile range instead of the standard deviation.

4. Encoding Categorical Variables: Since neural networks only accept numerical inputs, categorical variables need to be converted into numerical form. Techniques like one-hot encoding or label encoding can be used for this purpose.

5. Data Augmentation: To increase the amount of training data and prevent overfitting, synthetic examples can be created from existing data using techniques like rotation, zooming, flipping, etc. This step is particularly useful in image classification tasks.
The text classification pipeline described here utilizes a transformer model called BERT (Bidirectional Encoder Representations from Transformers). This model is characterized by its self-attention mechanism, which allows it to weigh the importance of different parts of a text sequence and capture dependencies between words. BERT generates word representations considering both the left and right contexts around words. It has been pretrained on a large Wikipedia text corpus using masked-language modeling and next-sentence prediction tasks. In this pipeline, BERT is employed to process smaller subsections of longer web articles due to its limitation of accepting a maximum of 512 tokens as input. Texts are tokenized using the BERT tokenizer before being fed into the model.
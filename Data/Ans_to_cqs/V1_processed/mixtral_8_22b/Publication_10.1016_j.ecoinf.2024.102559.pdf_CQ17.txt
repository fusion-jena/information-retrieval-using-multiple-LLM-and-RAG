The papers cited do not directly discuss the specific regularization methods used to prevent overfitting in their deep learning pipelines. However, some common regularization techniques used in deep learning include dropout and L2 regularization. Dropout involves randomly setting a fraction of input units to 0 during training, which helps prevent overfitting by making the network more robust to small changes in the inputs. L2 regularization adds a penalty term to the loss function, encouraging smaller weights and thus reducing the complexity of the model. Other regularization techniques include early stopping, weight decay, and batch normalization. It is important to note that the choice of regularization method depends on the specific problem and architecture being used.
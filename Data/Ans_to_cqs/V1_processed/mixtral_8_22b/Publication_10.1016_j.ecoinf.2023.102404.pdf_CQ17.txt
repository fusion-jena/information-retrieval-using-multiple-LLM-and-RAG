The papers mentioned do not explicitly discuss the use of regularization methods such as dropout or L2 regularization in their deep learning pipelines. However, they do mention techniques aimed at preventing overfitting. For instance, Tamiminia et al. (2021a) employed decision tree-based models and discussed tuning parameters like'minLeafPopulation', which controls the minimum number of samples required to create a leaf node in each tree. This parameter helps prevent overfitting by ensuring that only sufficiently large subsets of the data contribute to the final prediction. Similarly, Adeli and Brisco (2020) also worked with decision tree ensembles and emphasized finding the right balance between model complexity and predictive performance when determining the number of trees ('numberOfTrees') in the ensemble. These approaches aim to improve generalization and reduce overfitting but do not directly correspond to traditional regularization methods found in neural networks.

In summary, although the papers do not specifically address the use of common regularization methods like dropout or L2 regularization, they employ other strategies to mitigate overfitting, such as optimizing hyperparameters related to model complexity and sample size requirements.
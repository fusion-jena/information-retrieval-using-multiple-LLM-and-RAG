To ensure the generalizability of the deep learning models, several measures were taken. Firstly, both datasets (SSLs and elephants) were divided into training and testing images with a ratio of 80:20, respectively. This approach allows the model to learn from a diverse range of examples during training and then be evaluated on unseen data, ensuring that the model can generalize well to new data. Secondly, the use of pre-trained weights based on the ImageNet dataset helped initialize Model-2, providing some prior relevant information. This initialization method has been shown to improve the convergence speed and overall performance of deep learning models. Lastly, the early stopping technique was applied based on the model's performance on the validation set to prevent overfitting. Overfitting occurs when a model performs well on the training data but poorly on new, unseen data due to memorizing the training data instead of learning general patterns. By applying early stopping, the model stops training once the validation error starts increasing, preventing further training that could lead to overfitting. These measures collectively contribute to improving the generalizability of the deep learning models.
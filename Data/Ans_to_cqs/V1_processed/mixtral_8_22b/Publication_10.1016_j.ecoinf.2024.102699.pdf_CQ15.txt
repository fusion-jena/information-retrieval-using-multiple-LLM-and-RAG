In the given context, the authors utilized the Adam optimizer (Kingma and Ba, 2015) in their deep learning pipeline. They mention using the Adam optimizer along with a concatenating cosine-annealing linear scheduler with an initial learning rate of 0.0001, decaying by a factor of 0.001*learning rate, and a batch size of 32. Additionally, early stopping was implemented to prevent overfitting when no further improvement was observed during training.
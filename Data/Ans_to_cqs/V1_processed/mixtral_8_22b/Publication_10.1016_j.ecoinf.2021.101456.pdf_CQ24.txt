Based on the provided context, there isn't any explicit mention of strategies employed to handle randomness in the deep learning pipeline, such as managing random seed values. However, I can provide some general insights into this topic.

Randomness in deep learning pipelines often arises due to various factors like weight initialization, shuffling training examples, dropout layers, etc. To manage these sources of randomness, several strategies can be adopted:

1. **Fixed Random Seeds**: One common approach is setting fixed random seeds for all operations involving randomness. This ensures consistency across multiple runs of the same experiment, making results more comparable and reproducible.

2. **Deterministic Algorithms**: Some algorithms used within deep learning models, like certain types of normalization or optimization techniques, may introduce non-determinism. Opting for deterministic alternatives can help reduce variability caused by randomness.

3. **Repeat Experiments**: Running experiments multiple times with different random seeds and averaging the results can give a better understanding of the overall performance and robustness of the model.

4. **Hyperparameter Tuning**: Adjusting hyperparameters related to random processes, such as learning rate schedules or dropout rates, might also impact how much randomness affects the final outcome.
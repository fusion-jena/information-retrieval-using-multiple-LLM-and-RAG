In this study, there are no explicit mentions of using regularization methods such as dropout or L2 regularization to prevent overfitting in the deep learning pipeline. However, the authors do employ several strategies to ensure their model does not overfit during training. One such method is the use of a validation set, which consists of 13% of the total data (607 images). During training, the validation set is utilized to fine-tune hyperparameters and assess the model's performance on unseen data. This helps prevent overfitting by monitoring the model's performance on new data and adjusting the training process accordingly. Additionally, the authors use a separate testing set (5%, including 260 images) to independently evaluate the model's performance on completely unseen data. This provides a reliable measure of the model's real-world effectiveness and ability to generalize. While these methods do not directly address overfitting through regularization techniques, they contribute to building a well-performing and robust model capable of handling new data.
The paper uses the Adam optimizer for fine-tuning the pretrained CNN backbone. The Adam optimizer is initialized with a default learning rate of 0.0001 and a weight decay of 0. The experiments were conducted in PyTorch on 8 x NVIDIA GeForce RTX 2080 Ti GPUs. Fine-tuning was performed on the entire model for 100 epochs, with one epoch corresponding to 500 episodic tasks.

Unhelpful 
The paper does not mention any specific optimization technique used in the deep learning pipeline.
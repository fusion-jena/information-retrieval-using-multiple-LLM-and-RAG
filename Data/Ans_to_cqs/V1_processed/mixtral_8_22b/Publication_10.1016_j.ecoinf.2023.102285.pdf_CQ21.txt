After the model training phase, there are several postprocessing steps involved. These include evaluating the performance of the trained models through metrics calculation, generating saliency maps, and creating confusion matrices.

Metrics calculation involves computing various statistical measures such as accuracy, precision, recall, F1 score, etc., to assess the performance of the trained models. This helps in understanding how well the models have learned from the given data and how accurately they can predict or classify new instances.

Saliency maps are visualizations that highlight the regions in the input image that contribute most significantly towards the final prediction made by the model. They help in interpreting the decision-making process of the model and identifying any biases or errors in the predictions.

Confusion matrices provide a more detailed analysis of the performance of the trained models by comparing the actual labels against the predicted ones. They show the number of true positives, false positives, true negatives, and false negatives, allowing us to calculate additional metrics like sensitivity, specificity, positive predictive value, negative predictive value, and overall accuracy.

Based on the provided context, we can see that the authors used confusion matrices to evaluate the performance of their proposed methodology. For instance, in Table 10, they present a detailed confusion matrix of the classifiers outlined in Table 11. Similarly, in Table 11, they summarize the test, evaluation, and validation results detailed in Table 10. From these tables, we can infer that the authors calculated various metrics such as sensitivity, specificity, positive predictive value, negative predictive value, and overall accuracy to evaluate the performance of their proposed methodology.
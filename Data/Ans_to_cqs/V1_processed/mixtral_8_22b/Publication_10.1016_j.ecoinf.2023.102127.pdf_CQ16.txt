The text does not provide explicit information on the specific criteria used to determine when training is complete. However, it mentions that appropriate validation metrics are important but less straightforward than for single datasets due to potential differences in biases and sampling methods. It also notes that withholding parts of the training data for validation may result in both training and testing data suffering from the same spatial and environmental biases. Additionally, the text discusses the use of various metrics for evaluating models, including Area Under Curve (AUC), Kappa statistics, Cohen’s Kappa, Boyce Index, Sørensen’s Similarity Index, Maximum Kappa, True Skill Statistic (TSS), and thresholding techniques like minimum presence values, fixed values, or percentile values. These metrics could potentially be used to evaluate the performance of a model during training and help determine when training should be stopped. However, without further context, it cannot be definitively stated which specific criteria are used to determine when training is complete.
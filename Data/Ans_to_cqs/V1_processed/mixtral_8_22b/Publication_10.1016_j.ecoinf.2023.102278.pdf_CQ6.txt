Before training a deep learning model, several preprocessing steps can be applied to improve its performance. These include normalization, scaling, and cleaning. Normalization involves rescaling the values of features into a specific range or distribution, such as between 0 and 1 or with a mean of 0 and a standard deviation of 1. Scaling adjusts the scale of feature values to prevent any single feature from dominating the others due to differences in their scales. Cleaning refers to removing or handling missing or incorrect data points, which could negatively impact the model's performance if left unaddressed. In this case, the authors used data augmentation techniques like image scaling, flipping, distortions, and color jitter to enhance the diversity of the training data and reduce overfitting. They also chose a batch size of 20 for training their models, which allowed for faster updates and reduced noise compared to smaller batch sizes.
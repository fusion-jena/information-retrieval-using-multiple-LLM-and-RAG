Based on the given context, there isn't any explicit mention of specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) used in the deep learning pipeline. However, it can be inferred that some form of optimization technique was employed due to the presence of a learning rate parameter, denoted as 'Î±'. Learning rates are typically associated with optimization algorithms like SGD or Adam, which adjust the weights and biases of neural networks based on the computed gradients. Therefore, although not explicitly stated, it is likely that one of these common optimization techniques was utilized in the deep learning pipeline described in the context.
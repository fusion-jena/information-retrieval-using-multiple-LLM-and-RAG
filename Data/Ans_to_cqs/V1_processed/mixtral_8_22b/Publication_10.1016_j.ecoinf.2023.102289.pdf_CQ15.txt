Based on the given information, there isn't any explicit mention of optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being applied in the deep learning pipeline. These techniques are commonly used during the training phase of deep learning models to update their parameters and minimize the loss function. However, since the focus of the provided text is primarily on the application of semantic segmentation using pre-trained models rather than the training process itself, details about optimization techniques might not have been included. Therefore, without further context or information, I cannot definitively state what optimization techniques are applied in the deep learning pipeline.
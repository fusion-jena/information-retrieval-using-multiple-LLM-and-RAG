Based on the given information, it appears that the authors have utilized Stochastic Gradient Descent (SGD) as their optimization technique. They started with a learning rate of 0.0005, a momentum of 0.9, and a weight decay of 0.0005. During the training process, they decreased the learning rate every 10 epochs by multiplying it with Î³ = 0.1. Additionally, they employed warmup iterations for the learning rate in the first epoch with a warmup factor of 1/1000. However, there is no explicit mention of any specific strategy or metric being used to monitor the model performance during training. Therefore, based solely on the provided context, I cannot provide a definitive answer regarding the strategy implemented to monitor the model performance during training.
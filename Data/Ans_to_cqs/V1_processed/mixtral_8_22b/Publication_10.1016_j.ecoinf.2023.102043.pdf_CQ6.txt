Before training a deep learning model such as YAMNet, several preprocessing steps are typically involved. In the case of YAMNet, these include converting the audio to a monophonic channel and resampling it at 16 kHz. Then, the spectrogram is extracted and created using Short Time Fourier Transform (STFT) magnitudes of 25 ms overlapped by 15 ms. Afterwards, the Mel spectrogram is calculated by mapping to 64 bins of the Mel scale within the range of 125â€“7500 Hz. These preprocessing steps help to transform the raw audio data into a format suitable for feeding into the deep learning model.
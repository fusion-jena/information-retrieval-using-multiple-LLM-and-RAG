To monitor the model performance during training, the authors employed the early stopping technique. Early stopping is a form of regularization used to prevent overfitting by halting the training process before it has the chance to converge. In this case, the number of iterations required until convergence was determined using early stopping. For the CS dataset, the number of iterations was 120, while for the NW dataset, it was 106. By implementing early stopping, the authors aimed to ensure that their models did not become too complex or tailored specifically to the training data, thereby improving their ability to generalize well to new, unseen data.
The text does not provide explicit information about how the hyperparameters of the model are optimized. However, it mentions that these models are typically trained using stochastic gradient descent, which is a common optimization method used to minimize the cost function of machine learning algorithms. It also states that these models are often trained in a supervised manner, meaning they use labeled data to adjust the weights and biases of the network during training. While there is no direct mention of methods like grid search or random search for hyperparameter tuning, these techniques could potentially be employed alongside stochastic gradient descent to find optimal values for parameters such as learning rate, batch size, or regularization strength.
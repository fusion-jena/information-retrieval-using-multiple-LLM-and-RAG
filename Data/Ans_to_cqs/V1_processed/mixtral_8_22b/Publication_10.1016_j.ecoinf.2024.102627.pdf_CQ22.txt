Two main metrics are used to evaluate the performance of the deep learning model: micro-average (Ak) and macro-average (MSAk). These metrics measure how well the model performs in predicting the correct species assemblages based on presence-only observations.

The micro-average metric (denoted Ak) calculates the success rate for all test set occurrences, combining all classes together. This metric gives more weight to common species since they have more representation in the dataset. For example, in the final test set performance at epoch 69, the correct label was returned among the first 30 species for 87% of the test observations using the micro-average metric.

On the other hand, the macro-average metric (denoted MSAk) first calculates the success rate for each class individually and then averages them together. This metric is heavily dependent on rare species performances and is suitable for highly imbalanced datasets. Using the macro-average metric, the correct label was within the first 30 classes returned almost half the time in the final test set performance at epoch 69. However, this seemingly low value should be considered in light of the fact that the test set contained 4166 different species, making the task particularly challenging.

In summary, the two main evaluation metrics used for the deep learning model are micro-average (Ak) and macro-average (MSAk), which provide insights into the model's ability to accurately predict species assemblages based on presence-only observations.
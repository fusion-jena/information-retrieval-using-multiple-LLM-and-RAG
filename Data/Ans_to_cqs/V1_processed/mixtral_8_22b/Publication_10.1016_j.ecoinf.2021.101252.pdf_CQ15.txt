The text does not provide explicit information about the specific optimization techniques used in the deep learning pipeline. However, it mentions that each candidate model is trained using a small subset of the data during a small number of epochs. After training, the performance of the candidate models is compared using a left-out validation dataset. The selected candidate model is then trained on the full training data. It is also mentioned that it is required to identify an optimal number of training epochs to avoid under- or overfitting. These steps suggest that some form of optimization technique is being employed, but without further details, we cannot specify which one (e.g., Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam)).
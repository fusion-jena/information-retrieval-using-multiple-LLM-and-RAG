The papers cited do not explicitly mention specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adam. However, it is common practice in deep learning pipelines to use these optimization algorithms during the training phase of Convolutional Neural Networks (CNNs). These optimization techniques help minimize the loss function and update the weights of the model accordingly. Both SGD and Adam are popular choices due to their efficiency and effectiveness in handling large-scale datasets. Other optimization techniques like RMSprop and Adagrad may also be employed depending on the specific requirements of the task at hand.
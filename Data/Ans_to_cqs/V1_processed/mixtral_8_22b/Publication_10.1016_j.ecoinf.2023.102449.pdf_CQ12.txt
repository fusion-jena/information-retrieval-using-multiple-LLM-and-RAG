In this pipeline, the deep learning models used are deep Convolutional Neural Networks (CNNs), specifically designed as wide residual networks. These networks belong to the class of functions known as Residual Networks (ResNets), which enable deeper architectures and increased model complexity. This is achieved by learning a perturbation of the input signal at each layer instead of a transformation, as seen in conventional neural architectures. The perturbation is implemented using skip connections within residual blocks, allowing the input signal to persist throughout the network.

The encoder employs a pre-processing block consisting of a convolution with a wide receptive field (5x5 kernel) and signal coarsening along the time axis (2x1 kernel). Following this, feature representations are divided into independent frames, each of which is then flattened and passed through a final linear layer to produce mean and log variance vectors as parameters for the Gaussian variational posterior.

On the other hand, the decoder mirrors the encoder's structure but operates in reverse order. It starts with a post-processing block, followed by several upsampling blocks interspersed with residual blocks. The output is then reshaped and subjected to a final convolution operation.

To address internal covariate shift and stabilize training, these residual networks necessitate some form of layer normalization, such as batch normalization. Although increasing the number of model parameters, batch normalization enables the utilization of higher learning rates, thereby accelerating training and decreasing power consumption. Furthermore, wide ResNets enhance network width while diminishing depth, facilitating quicker training due to shallower architectures.
The provided context does not explicitly mention any specific optimization techniques applied in the deep learning pipeline, such as Stochastic Gradient Descent (SGD), Adam, or others. However, we can infer that some form of optimization technique must be employed during the training process of a Convolutional Neural Network (CNN) mentioned in the text.

In general, CNNs use backpropagation to train their weights, which involves computing gradients of the loss function concerning each weight in the network. This gradient information is then used to update the weights via an optimization algorithm like SGD or Adam. These algorithms iteratively adjust the weights to minimize the overall error or loss of the network.

While the given context does not provide explicit details about the optimization techniques used, it mentions that the final layer passes its parameters through the SoftMax function. This suggests that the network might employ a cross-entropy loss function, commonly used in multi-class classification problems. Cross-entropy loss is often optimized using variants of stochastic gradient descent, including popular choices like Adam, RMSProp, or Adagrad.

Therefore, while the exact optimization technique used in this particular case remains unspecified, one can reasonably assume that a variant of stochastic gradient descent, possibly Adam or another similar method, was likely utilized based on common practices in deep learning pipelines involving CNNs.
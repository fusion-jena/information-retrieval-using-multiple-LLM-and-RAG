The text does not provide explicit information about handling randomness in the deep learning pipeline or setting specific values for the random seed. However, it mentions that Random Forests were used with 500 trees, which suggests some form of control over the randomness inherent in this type of ensemble learning method. Additionally, Gradient Boosting Machines (GBMs) were also used, but no details regarding controlling randomness within this framework are mentioned. In general, one common strategy to manage randomness is to set a fixed seed value before running any code involving random processes, ensuring reproducibility. Another approach could be cross-validation, where multiple runs with different seeds are performed to evaluate the stability and robustness of the results. Nevertheless, based solely on the given text, I cannot provide more detailed insights into the specific strategies employed to handle randomness in the deep learning pipeline.
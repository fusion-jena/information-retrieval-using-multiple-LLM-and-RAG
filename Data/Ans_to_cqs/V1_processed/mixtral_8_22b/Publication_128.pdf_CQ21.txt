The given context does not provide explicit details about any specific postprocessing steps involved after the model training. However, based on common practices in machine learning and deep learning, some possible postprocessing steps could include:

1. Evaluation metrics: After training the models, various evaluation metrics can be calculated to assess their performance. These may include accuracy, precision, recall, F1 score, ROC curves, etc. Depending on the task and dataset, different metrics might be more relevant.

2. Confusion matrix: A confusion matrix is often used to evaluate the performance of a classification model. It provides a summary of prediction results on a classification problem. Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class.

3. Visualizations: Postprocessing may also involve creating visual representations of the trained models or their outputs. Examples include saliency maps, which highlight areas of an input image that most influence the model's predictions, or t-SNE plots, which help visualize high-dimensional data in two dimensions.

4. Ensemble methods: Sometimes, multiple models are combined using ensemble methods like bagging, boosting, or stacking to improve overall performance. This would require additional postprocessing steps to combine the individual models' outputs.

However, without further context or information regarding the specific application or goals of these models, it is difficult to determine exactly what postprocessing steps would be necessary or appropriate.
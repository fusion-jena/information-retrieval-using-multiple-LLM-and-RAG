The given text does not provide explicit information about the specific optimization techniques used in the deep learning pipeline, such as Stochastic Gradient Descent (SGD), Adam, or others. However, we can infer that some form of optimization technique must have been utilized during the training process of the Deep Neural Network (DNN) since the goal was to achieve optimal performance based on the highest Mean Intersection over Union (MIoU) value. Additionally, the fact that the network was trained over 1000 epochs suggests that there might have been a learning rate scheduler involved, which could be part of an optimization algorithm like SGD or Adam. Nevertheless, without further details regarding the implementation of the DNN, it remains uncertain what exact optimization techniques were applied in the deep learning pipeline.
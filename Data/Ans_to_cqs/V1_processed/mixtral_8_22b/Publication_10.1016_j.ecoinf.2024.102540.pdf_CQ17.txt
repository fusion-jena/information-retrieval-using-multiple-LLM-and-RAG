Unfortunately, the given context does not provide specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. Regularization methods like dropout or L2 regularization are often employed to avoid overfitting during the training process. Dropout involves randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting by providing a way of approximately combining exponentially many different neural network architectures efficiently. On the other hand, L2 regularization adds a penalty term to the loss function, which discourages the weights from becoming too large, thus preventing overfitting. Other common regularization methods include early stopping, weight decay, and data augmentation. Without explicit mention in the provided context, it cannot be confirmed whether any of these methods were utilized in the deep learning pipeline.
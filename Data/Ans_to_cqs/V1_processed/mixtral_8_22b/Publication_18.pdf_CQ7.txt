The process of splitting data into different subsets such as training set, testing set, and validation set is crucial for building robust and accurate deep learning models. The primary goal of this division is to evaluate the performance of the model during its development phase and prevent overfitting or underfitting issues. Here are some common criteria used to split the data:

1. Random sampling: This method involves randomly selecting samples from the entire dataset to create separate sets. It ensures that all classes have equal representation in each subset. However, it may not always guarantee that the distribution of the original dataset is maintained.

2. Stratified sampling: To address the limitations of random sampling, stratified sampling is often employed. This technique maintains the class proportions present in the original dataset while creating the subsets. By doing so, it helps ensure that the model generalizes well across various classes.

3. Time-based splitting: When dealing with time series data, it is essential to maintain temporal dependencies between observations. Therefore, instead of shuffling the data before splitting, one should use time-based splitting. This approach divides the data based on timestamps, ensuring that the chronological order is preserved.

4. Cross-validation: Another popular method for evaluating the performance of deep learning models is cross-validation. It involves dividing the dataset into k folds, where each fold serves as a testing set once, while the remaining folds form the training set. This procedure is repeated k times until every sample has been part of both the training and testing sets. Cross-validation provides a more reliable estimate of the model's performance compared to single splits.

In summary, depending on the nature of the problem and the available resources, different methods can be applied to divide the data into training, testing, and validation sets. These include random sampling, stratified sampling, time-based splitting, and cross-validation. Each method has its advantages and disadvantages; thus, choosing the appropriate strategy depends on factors like data size, class imbalance, and computational constraints.
Based on the given context, there isn't any direct mention of regularization methods used to prevent overfitting in the deep learning pipeline. However, I can provide some general insights into common regularization techniques used in deep learning models.

Regularization methods help prevent overfitting by adding a penalty term to the loss function during training. This encourages smaller weights, leading to simpler models that generalize better. Two commonly used regularization techniques are Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent complex co-adaptations between neurons. By ignoring certain neurons, the model becomes more robust and less likely to overfit. The rate of dropout is a hyperparameter that needs to be tuned based on the specific problem and dataset.

L2 Regularization, also known as weight decay, adds a penalty proportional to the square of the magnitude of the weights to the loss function. This encourages smaller weights and discourages large ones, making the model less sensitive to individual features and thus less prone to overfitting. The strength of the regularization is controlled by a hyperparameter called the regularization parameter, which must be chosen carefully through cross-validation or other means.

In summary, while the given context does not directly address the use of regularization methods in the deep learning pipeline, common techniques include Dropout and L2 Regularization. These methods add penalties to the loss function during training, encouraging smaller weights and simpler models that generalize better.
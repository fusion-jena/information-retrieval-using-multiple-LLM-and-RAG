To evaluate the performance of a deep learning model, various metrics such as accuracy, precision, recall, and F-measure are commonly used.

Accuracy is defined as the ratio of correctly predicted observations to the total observations. It measures the overall effectiveness of the model. However, when dealing with imbalanced datasets, accuracy might not provide a complete picture of the model's performance.

Precision is calculated as the ratio of truly positive observations to the total predicted positives. It indicates the proportion of actual positives out of the total predicted positives. High precision means fewer false positives.

Recall, also known as sensitivity or True Positive Rate (TPR), is computed as the ratio of truly positive observations to the total actual positives. It shows the ability of the model to find all the positive samples. A high recall value means fewer false negatives.

The F-measure combines both precision and recall by taking their harmonic mean. This metric provides a single score that balances both concerns. The F-measure ranges from 0 to 1, where 1 represents perfect precision and recall.

In summary, accuracy, precision, recall, and F-measure are essential metrics used to assess the performance of deep learning models. Each metric has its significance and should be considered depending on the problem statement and data distribution.
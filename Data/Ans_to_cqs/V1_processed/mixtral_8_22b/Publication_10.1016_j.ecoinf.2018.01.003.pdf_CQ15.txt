The given context does not provide specific details about the optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, it mentions that researchers have improved algorithms based on classical neural network models, including the use of large-scale data to train deeper networks with more layers. This suggests that optimization techniques may be employed during the training process to minimize errors and improve the overall performance of the deep learning models. Additionally, the text refers to an adaptive strategy being used to modulate the learning rate, which could indicate the use of optimization techniques like gradient descent variants or other adaptive learning rate methods.
The context does not provide specific details about the regularization methods used to prevent overfitting in the deep learning pipeline. However, based on general knowledge, some common regularization methods used in deep learning include Dropout, L1 Regularization, and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overfitting.

L1 Regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function, while L2 Regularization adds a penalty equal to the square of the magnitude of coefficients to the loss function. Both methods help to reduce overfitting by discouraging large weights and encouraging simpler models.
In this study, two main regularization techniques were employed to prevent overfitting in their deep learning pipeline. Firstly, they utilized dropout as a Bayesian approximation method, which represents model uncertainty in deep learning. This technique involves randomly dropping out neurons during training, effectively preventing complex co-adaptations between them. Secondly, they applied L2 regularization, also known as weight decay, to control the strength of the unit Gaussian prior. By adding a penalty term to the loss function, L2 regularization discourages large weights, thus reducing overfitting. These regularization methods contribute to improving the generalization performance of the models.
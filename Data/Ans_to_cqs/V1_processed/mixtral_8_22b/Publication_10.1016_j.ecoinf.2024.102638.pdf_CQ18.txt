The strategy implemented to monitor the model performance during training involves dividing the dataset into training sets, which comprise 80% of the data, and a test set representing the remaining 20%. This division is done by random sampling to ensure that both subsets are representative of the overall dataset and minimize the risk of bias. Randomization helps to preserve the distribution of key features and target variables across both training and test sets, enhancing the generalizability of the model. The performance metrics detailed in Section 2.5 are meticulously evaluated using the designated test set. Subsequently, based on the performance metrics obtained, the most effective models for each target variable are discerned and selected. The scikit-learn function GridSearchCV is leveraged to fine-tune the hyperparameters of each model within the confines of the training set, aiming to enhance their predictive performance. A five-fold cross-validation is used and the negative mean square error is set as the target measure.
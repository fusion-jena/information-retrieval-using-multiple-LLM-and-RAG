In the given context, there is no explicit mention of specific regularization methods used to prevent overfitting in the deep learning pipeline. However, based on general practices in deep learning, several techniques can be employed to mitigate overfitting. These include:

1. Dropout: This technique involves temporarily removing a random selection of neurons during training, forcing the network to rely more heavily on other neurons and preventing any single neuron from becoming too influential.

2. L2 Regularization: Also known as weight decay, this method adds a penalty term to the loss function proportional to the sum of squared weights. This encourages smaller weights, making the model simpler and reducing its tendency to overfit.

3. Early Stopping: In this approach, the training process is halted once performance on a validation dataset stops improving, even if the training loss continues to decrease. This prevents the model from fitting noise in the training data.

4. Data Augmentation: By artificially increasing the amount of available training data through transformations like rotation, scaling, or flipping, models become less sensitive to individual data points and thus less prone to overfitting.

While these methods are commonly used to address overfitting in deep learning pipelines, the exact choice depends on factors such as the nature of the problem, the complexity of the model, and the characteristics of the dataset.
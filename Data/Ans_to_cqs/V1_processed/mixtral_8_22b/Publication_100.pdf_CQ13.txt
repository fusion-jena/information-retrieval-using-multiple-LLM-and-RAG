The deep learning model uses the Adam optimizer with hyperparameters 𝛽1 = 0.9, 𝛽2 = 0.999, and 𝜖 = 10^-8. The initial base learning rate is set to 𝛼 = 10^-4, which is then automatically reduced by a factor of 0.1 when the validation loss does not improve for 15 consecutive epochs. Weight decay is applied to control the strength of the unit Gaussian prior, with a magnitude of 10^-3 that is inversely proportional to the hyperparameter 𝜆 from Equation (1). The model is trained using stochastic gradient descent with a batch size of 𝐵 = 64.
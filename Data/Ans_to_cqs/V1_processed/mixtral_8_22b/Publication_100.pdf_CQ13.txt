The deep learning model uses the Adam optimizer with hyperparameters ğ›½1 = 0.9, ğ›½2 = 0.999, and ğœ– = 10^-8. The initial base learning rate is set to ğ›¼ = 10^-4, which is then automatically reduced by a factor of 0.1 when the validation loss does not improve for 15 consecutive epochs. Weight decay is applied to control the strength of the unit Gaussian prior, with a magnitude of 10^-3 that is inversely proportional to the hyperparameter ğœ† from Equation (1). The model is trained using stochastic gradient descent with a batch size of ğµ = 64.
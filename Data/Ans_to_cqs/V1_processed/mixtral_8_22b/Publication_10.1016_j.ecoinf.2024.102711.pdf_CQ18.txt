To monitor the model performance during training, the authors have implemented a strategy involving k-fold cross-validation and out-of-fold (OOF) predictions. They divide the development dataset evenly into k groups and use each group as a tuning dataset while developing the model on the remaining datasets. This process is repeated k times, ensuring that every subset serves once as the tuning dataset. The average performance of these k models represents the overall performance of the machine learning technique for the selected hyperparameters.

Grid search is utilized to evaluate the data-driven model's performance using different combinations of predefined hyperparameters and determine the optimal set. In this study, the maximum depth of the tree, the number of trees in the ensemble model, and the learning rate are considered as the crucial hyperparameters.

Furthermore, the authors employ XGBoost, a gradient boosting framework, to develop accurate models by minimizing the loss function. Out-of-fold (OOF) predictions are generated for each round of the k-fold cross-validation, and these predictions are averaged across all rounds to reduce variance. These averaged OOF predictions are then stacked and used as features to train a second layer of models. This layered approach helps improve the overall predictive power of the final model.
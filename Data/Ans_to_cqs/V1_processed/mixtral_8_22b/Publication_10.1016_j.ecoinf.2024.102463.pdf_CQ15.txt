Hyperband does not explicitly mention the use of specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) in its process. However, it is common practice in deep learning pipelines to use these optimization techniques during the training phase of neural networks.

Hyperband focuses on efficiently searching through the space of hyperparameters for model architectures. It achieves this by combining an initial random search with a'successive halving' strategy. This involves partial training of models on a small fraction of the data to quickly eliminate underperforming configurations. The remaining configurations are then grouped into sets of different sizes, and their corresponding models are trained further. The best performing configurations in each set are promoted to the next stage, while underperforming ones are eliminated. This process is repeated until only one configuration remains, which is then considered the best configuration found.

While Hyperband itself does not specify the use of optimization techniques like SGD or Adam, they would typically be employed during the actual training of the neural networks within this framework. These optimization techniques help minimize the loss function during backpropagation, thereby improving the accuracy of the model.
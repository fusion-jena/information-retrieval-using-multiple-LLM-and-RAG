In the provided context, there is no explicit mention of specific regularization techniques such as dropout or L2 regularization. However, several strategies are employed to improve generalization and avoid overfitting. These include:

1. Data Augmentation: This technique involves applying transformations like horizontal flipping, scaling contrast, and cropping to increase the diversity of the dataset. By creating new variations of existing examples, this method helps reduce overfitting and improves the model's ability to generalize.

2. Balancing Training Set: Upsampling the images from underrepresented classes ensures that all classes contribute equally to the learning process. This prevents the model from becoming biased towards the majority class, which could lead to poor performance on minority classes.

3. Validation Set Monitoring: During training, the network is evaluated on the validation set after every epoch. The best performing model based on validation set accuracy is saved for further analysis. This strategy helps identify the optimal point where the model has learned enough but hasn't started to overfit yet.

While these approaches do not directly correspond to traditional regularization methods like dropout or L2 regularization, they serve a similar purpose â€“ preventing overfitting and improving the model's capacity to generalize well to unseen data.
In this study, the training dataset consisting of 268 sample plots with 39 features was used for training the deep learning models. The dataset was divided into two parts, where 80% was allocated for training and validation purposes, while the remaining 20% was reserved as unseen data for testing. This division aims to prevent overfitting and evaluate the model's ability to generalize well to new, unseen data.

To further mitigate any potential biases, the researchers employed 10-fold cross-validation during the training process. Cross-validation involves dividing the available data into k subsets, also known as folds, and then iteratively training the model on k - 1 folds while validating its performance on the remaining fold. After completing one iteration, another fold becomes the validation set, and the process repeats until each fold has served as the validation set once. By averaging the results across all iterations, cross-validation provides a more reliable estimate of the model's performance compared to a single train-test split.

In summary, the criteria used to split the data for deep learning model training in this study include allocating 80% of the data for training and validation and reserving 20% for testing. Additionally, 10-fold cross-validation was applied during the training process to minimize bias and obtain a more accurate assessment of the model's performance.
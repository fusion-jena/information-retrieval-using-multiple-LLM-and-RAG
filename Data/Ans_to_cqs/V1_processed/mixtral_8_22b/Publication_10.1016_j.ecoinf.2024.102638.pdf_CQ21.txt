After the model training, several postprocessing steps can be taken depending on the specific requirements of the project or study. Some common postprocessing steps include:

1. Evaluation Metrics Calculation: After training the models, their performance needs to be assessed using various evaluation metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE), and Mean Absolute Scaled Error (MASE). These metrics help quantify the accuracy and reliability of the predictions made by the models.

2. Hyperparameter Optimization: The optimal values of hyperparameters for each model are determined through a rigorous optimization process. In this case, GridSearchCV from scikit-learn library was employed with a five-fold cross-validation and negative mean squared error as the target measure.

3. Model Selection: Based on the performance metrics obtained, the most effective models for each target variable are chosen. Figure 4 provides a visual representation of this model selection process.

4. Data Division: To maximize model training and maintain enough data for robust validation, the dataset is split into training and testing sets. Here, 80% of the data forms the training set, while the remaining 20% constitutes the test set. This division is carried out randomly to ensure representativeness and minimize bias risk.

5. Fine-Tuning Hyperparameters: Using GridSearchCV, the hyperparameters of each model are carefully tuned within the constraints of the training set to improve their predictive capabilities.

6. Table Generation: A table summarizing the results of the prediction models, including their names, hyperparameters, parameters selected during the 5-fold cross-validation, and adjusted hyperparameters, is created.

7. Performance Evaluation: Finally, the performance of the prediction models for H, LE, and CO2 is evaluated. However, no explicit information about saliency maps or confusion matrices has been mentioned in the given context.
The given context does not explicitly mention any specific regularization methods used to prevent overfitting in the deep learning pipeline. However, common regularization techniques include dropout, L1 regularization, and L2 regularization. Dropout randomly sets a fraction of input units to 0 during training, which helps prevent overfitting. L1 regularization adds a penalty equivalent to the absolute value of the magnitude of coefficients to the loss function, shrinking some coefficients to zero. L2 regularization adds a penalty equivalent to the square of the magnitude of coefficients to the loss function, shrinking coefficients but rarely reducing them to zero. These methods were not mentioned in the provided context, so it cannot be determined if they were used in this case.
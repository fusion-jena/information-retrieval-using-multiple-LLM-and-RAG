Based on the given context, there is no explicit mention of any specific preprocessing steps involved before training the deep learning model, SeedNet. However, it can be assumed that some common preprocessing techniques such as data normalization or standardization might have been employed to ensure that the input features have zero mean and unit variance. This helps in faster convergence during optimization process. Additionally, data augmentation techniques like rotation, flipping, zooming etc. could have been used to increase the diversity of the training set and prevent overfitting. But without more information about the exact preprocessing steps taken, these assumptions remain speculative.
The deep learning models were trained with several hyperparameters. These include an initial learning rate of 0.01, a weight decay of 0.005, and the optimizer Adam with a beta1 value of 0.937. The models were trained for 500 epochs. Additionally, data augmentation transformations were applied during training to help improve the model's performance and reduce the risk of overfitting.
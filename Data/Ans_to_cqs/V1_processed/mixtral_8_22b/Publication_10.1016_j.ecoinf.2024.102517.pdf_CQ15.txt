The experimental setting mentioned in the given context uses the Adam optimizer in the deep learning pipeline. The Adam optimizer is used to train the deep feature loss bird noise reduction network model for 50 epochs on an RTX 2080 Ti GPU. The initial learning rate of the Adam optimizer is set to 10^-4. During each epoch, the entire training set is presented in a random order, with one noisy bird sound signal per iteration.

Adam (short for Adaptive Moment Estimation) is a popular optimization technique used in deep learning pipelines. It combines the advantages of two other optimization methods - AdaGrad and RMSProp. Adam computes adaptive learning rates for different parameters from estimates of first and second moments of the gradients. It has been shown to perform better than other optimization techniques like Stochastic Gradient Descent (SGD) or Root Mean Square Propagation (RMSprop).

Therefore, in this case, the Adam optimizer is chosen over other optimization techniques due to its superior performance in handling large datasets and complex models.
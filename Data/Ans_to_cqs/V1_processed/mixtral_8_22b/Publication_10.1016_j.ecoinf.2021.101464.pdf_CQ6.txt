Before training a deep learning model, several preprocessing steps can be taken to improve its performance. These may include:

1. Data Cleaning: This involves removing or correcting any inconsistencies, errors, or missing values in the dataset. It also includes handling outliers, which could skew the model if not properly addressed.

2. Normalization/Standardization: This step scales numerical input features to have zero mean and unit variance. This is important because many machine learning algorithms perform better when numerical inputs are normalized.

3. Feature Scaling: Similar to normalization, feature scaling rescales the values into ranges such as [0,1] or [-1,1]. This is done to prevent attributes with larger numeric ranges from dominating those with smaller numeric ranges during the learning process.

4. Encoding Categorical Variables: Deep learning models cannot handle categorical data directly. Therefore, these types of data need to be converted into numerical form using techniques like label encoding or one-hot encoding.

5. Splitting Dataset: The dataset is usually divided into three parts - training set, validation set, and test set. The training set is used to train the model, the validation set is used to tune hyperparameters, and the test set is used to evaluate the final model.

6. Data Augmentation: This technique is often used in image processing tasks where new images are created from existing ones by applying transformations like rotation, zooming, flipping etc. This helps increase the size of the dataset and makes the model more robust.
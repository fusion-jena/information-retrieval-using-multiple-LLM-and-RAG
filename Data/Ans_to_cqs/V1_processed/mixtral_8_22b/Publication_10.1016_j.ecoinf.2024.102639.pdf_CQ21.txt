Based on the given context, it is not explicitly mentioned what the postprocessing steps are after the model training. However, some insights can be derived from the information available.

After training the Support Vector Machine (SVM) model using dimensionality reduction techniques like Principal Component Analysis (PCA), the authors used a combination of grid search and cross-validation to optimize the regularization parameter ('C') and the kernel type (linear). This approach aims to strike a balance between model complexity and its ability to generalize well.

Although not directly stated, one could infer that common postprocessing steps might include evaluating the performance of the trained model using metrics such as accuracy, precision, recall, F1 score, or Area Under Curve (AUC). These metrics help assess how well the model performs on unseen data and provide insights into potential areas of improvement.

Additionally, visualizing the results using tools like saliency maps or confusion matrices may also be part of the postprocessing steps. Saliency maps highlight important features contributing to the model's predictions, while confusion matrices display the relationship between predicted and actual class labels, helping identify misclassifications and patterns in errors.

However, without explicit mention of these postprocessing steps in the provided context, it remains speculative whether they were employed in this particular study.
The strategy implemented to monitor the model performance during training involved using 80% of the data as the training set and the remaining 20% as the evaluation set. The evaluation set was used to assess the performance of the models. Two metrics were employed to evaluate the accuracy of the models: the area under the curve (AUC) of the receiver operating characteristics (ROC) and the true skill statistics (TSS). Additionally, committee averaging and weighted mean approaches were applied to create separate ensemble models for each climate scenario and period. Special attention was given to selecting the final ensemble models with a TSS score of â‰¥0.7 to ensure the robustness of the results.
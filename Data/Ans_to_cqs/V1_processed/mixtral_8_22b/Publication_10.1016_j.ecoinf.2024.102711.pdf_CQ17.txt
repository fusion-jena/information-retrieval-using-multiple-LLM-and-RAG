Unfortunately, the given context does not provide specific information regarding the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, I can explain what those methods are and how they work.

Dropout is a regularization method used during the training process of neural networks to prevent overfitting. It works by randomly dropping out neurons along with their incoming and outgoing connections during training. This helps reduce co-dependency among neurons and encourages them to learn more robust features independently. Dropout rates typically range between 0.2 and 0.5.

L2 regularization, also known as weight decay, is another popular regularization method used to combat overfitting. It adds a penalty term to the loss function proportional to the square of the magnitude of the weights. By doing so, it discourages large weights and promotes smaller, more distributed representations. The strength of the regularization is controlled by a hyperparameter lambda, which determines the tradeoff between minimizing the loss and keeping the weights small.

These regularization methods help improve the generalization capabilities of deep learning models by reducing overfitting and promoting better feature learning.
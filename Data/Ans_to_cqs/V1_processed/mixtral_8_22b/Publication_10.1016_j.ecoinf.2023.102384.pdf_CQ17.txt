The text does not provide explicit information regarding the use of specific regularization methods like dropout or L2 regularization to prevent overfitting in the deep learning pipeline. However, there are some indications of strategies that could potentially help mitigate overfitting. During the training stage, the authors mention applying simple data augmentation techniques, including random cropping of square patches and horizontal/vertical flipping with a 50% probability. These techniques can act as implicit forms of regularization, helping to reduce overfitting by increasing the diversity of the training dataset. Additionally, the authors discuss dividing the images of the validation split into regularly spaced, overlapping patches during the validation phase, which might contribute to better generalization performance. Nevertheless, without direct evidence of employing explicit regularization methods like dropout or L2 regularization, one cannot definitively state their usage in this particular study.
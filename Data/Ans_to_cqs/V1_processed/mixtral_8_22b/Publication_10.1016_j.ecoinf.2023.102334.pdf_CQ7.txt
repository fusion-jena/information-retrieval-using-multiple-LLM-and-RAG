Based on the given context, there isn't explicit information about how the data was split for training, testing, or validation purposes. However, typically in machine learning and deep learning models, including those mentioned in the context such as Convolutional Neural Networks (CNN) and Vision Transformers (ViT), the data is often divided into three subsets: training, validation, and testing sets.

The training set is used to fit the parameters of the model, i.e., to learn the relationship between input features and output labels. The validation set is used during the model development phase to fine-tune the model's hyperparameters and prevent overfitting. Lastly, the testing set is used after the model has been fully developed to assess its performance on unseen data.

The proportion of each subset depends on various factors, such as the total amount of available data and the specific requirements of the project. Common practices include splitting the data into 60-80% for training, 10-20% for validation, and 10-20% for testing. Alternatively, techniques like k-fold cross-validation may be employed to ensure all samples are used for both training and validation.

In conclusion, based on common practices in machine learning and deep learning, we can infer that the data might have been split into training, validation, and testing sets. However, without further details, we cannot provide exact percentages or methods used for splitting the data in this particular case.
In the given context, two optimization techniques are mentioned for use in the deep learning pipeline - Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam). These techniques are used to update the weights of the neurons in the neural network during the training phase.

The authors compare these two optimization techniques and find that using Adam in conjunction with ReduceLROnPlateau can speed up the convergence rate of model training and effectively mitigate the risk of overfitting. This means that Adam is more effective than SGD in updating the weights of the neurons in the neural network.

ReduceLROnPlateau is a technique used to dynamically adjust the learning rate based on the performance of the model. It reduces the learning rate when the validation loss stops decreasing for a certain number of epochs. In this case, if the validation loss does not decrease after five consecutive epochs, the learning rate is halved. If there is still no decrease in the validation loss after ten consecutive epochs, the training process is stopped.

Therefore, the optimization techniques applied in the deep learning pipeline in this context are Adam and ReduceLROnPlateau.
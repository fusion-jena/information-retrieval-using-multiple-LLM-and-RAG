The given text does not provide information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, common regularization techniques include Dropout, which randomly sets a fraction rate of input units to 0 at each update during training time, helping reduce overfitting; and L2 Regularization, also known as weight decay, which adds a penalty term to the loss function proportional to the squared magnitude of all parameters. Other methods include early stopping, data augmentation, and batch normalization.
To monitor the model performance during training, the authors employed an early stopping criterion. This means that they halted the training process when certain conditions were met, preventing overfitting or underfitting. To determine whether to stop training, they relied on the Mean Squared Error (MSE) as their primary metric. They ensured a reliable estimate of the MSE by performing a 10-fold cross validation. Additionally, considering the probabilistic nature of their models, they ran each evaluation 1000 times for every test point. This approach allowed them to accurately reconstruct the predicted output distribution, providing a robust estimation process with a 95% confidence interval and a precision level of 5%.
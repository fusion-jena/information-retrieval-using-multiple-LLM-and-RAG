The criteria used to split the data for deep learning model training typically involve dividing the dataset into three subsets: training, testing, and validation sets. This division allows for evaluating the performance of the model during training and fine-tuning its parameters.

The training set is used to fit the model by adjusting its weights and biases through backpropagation. It consists of input features and their corresponding output labels.

The testing set is utilized to assess the model's ability to generalize to unseen data. By applying the trained model to the testing set, one can measure its accuracy, precision, recall, F1 score, or any other relevant metric.

Lastly, the validation set serves as a means to tune hyperparameters and prevent overfitting. During the training phase, the model's performance is evaluated on the validation set after each epoch or batch. Based on these results, decisions can be made regarding when to stop training, which architecture to use, or what learning rate to apply.

The proportion of data allocated to each subset depends on the size of the dataset and the complexity of the problem. A common practice is to allocate around 60-80% of the data for training, 10-20% for testing, and another 10-20% for validation. However, these percentages might vary depending on the specific requirements and characteristics of the project.
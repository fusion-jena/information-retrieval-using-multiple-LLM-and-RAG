The context does not contain information about regularization methods used to prevent overfitting in the deep learning pipeline. However, I can still provide an answer to your query. Regularization methods are techniques used to prevent overfitting by adding a penalty term to the loss function during training. Two common regularization methods used in deep learning pipelines are Dropout and L2 regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overfitting. During each iteration of training, a certain percentage of neurons are temporarily removed along with all its incoming and outgoing connections. This forces the network to learn redundant representations of the input features, making it less likely to rely too heavily on any one feature.

L2 regularization, also known as weight decay, adds a penalty term to the loss function equal to the sum of squares of all the weights in the network. This encourages smaller weights, which makes the model less complex and less prone to overfitting. It also helps to reduce the variance of the model, improving its generalization performance.

Other regularization methods used in deep learning pipelines include L1 regularization, early stopping, and batch normalization. Each of these methods works differently, but they all aim to improve the model's ability to generalize to new data by preventing overfitting.
Based on the given context, there is no explicit mention of any regularization methods used to prevent overfitting in the deep learning pipeline such as dropout or L2 regularization. However, some techniques mentioned in the text could potentially help mitigate overfitting. For example, using validation data for tuning key hyperparameters for each model helps ensure that the models do not become too specialized to the training data. Additionally, the use of ensemble methods like Random Forest (RF) and Gradient Tree Boosting (GTB), which combine differently weighted trees, also helps prevent overfitting. While these methods are not explicitly referred to as regularization techniques, they contribute to reducing overfitting in the models.
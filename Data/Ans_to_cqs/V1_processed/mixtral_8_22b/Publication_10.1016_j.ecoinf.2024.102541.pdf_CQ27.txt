The given context does not provide explicit information about the specific process used to deploy the trained deep learning model. However, it can be inferred that the authors focused on optimizing their model for efficient use in resource-constrained environments, such as remote areas. They achieved this by reducing the number of model parameters and floating point operations (FLOPs). Additionally, they recorded improvements in frames per second (FPS) when comparing their approach to other state-of-the-art deep learning models. These optimizations suggest that the authors may have serialized their model into a compact format suitable for deployment on devices with limited computational resources. Furthermore, the mention of a faster model loading time indicates that platform selection might have been considered during the deployment process. Nevertheless, without more detailed information regarding the exact steps taken to deploy the model, it remains uncertain what specific methods were employed.
Based on the given context, there is no explicit mention of any specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it does state that the'stochastic gradient descent with momentum optimizer' was used for training the AN (AlexNet) model, but without specifying whether any additional regularization techniques were applied or not. Therefore, I cannot provide a definitive answer regarding the regularization methods used in this particular case.

However, some common regularization methods used in deep learning pipelines include dropout, L1/L2 regularization, early stopping, data augmentation, and batch normalization. Dropout randomly drops out neurons during training to reduce co-dependency between them, while L1/L2 regularization adds penalty terms to the loss function to discourage complex models. Early stopping stops training when validation error starts increasing, indicating overfitting. Data augmentation generates new training examples by applying transformations like rotation, scaling, etc., to existing ones. Batch normalization standardizes activations across batches to improve convergence speed and stability.
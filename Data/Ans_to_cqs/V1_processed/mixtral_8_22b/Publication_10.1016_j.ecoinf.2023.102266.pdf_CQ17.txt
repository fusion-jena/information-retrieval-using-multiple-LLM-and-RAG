The query asks about regularization methods used to prevent overfitting in deep learning pipelines, specifically mentioning dropout and L2 regularization. However, the provided context does not directly discuss these specific topics. Nevertheless, I can provide some information on these regularization methods based on general knowledge.

Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by reducing the dependence of the network on any single neuron. It effectively creates an ensemble of models within the neural network, improving overall performance and robustness.

L2 regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the magnitude of the weights. By penalizing large weights, this encourages smaller, simpler models that are less likely to overfit the data. Regularization terms like L2 can be seen as adding prior beliefs about the distribution of parameters, favoring small values unless there is strong evidence otherwise.

While the provided context doesn't explicitly cover these regularization methods, they are commonly employed in deep learning pipelines to combat overfitting and improve model generalization.
In the given context, there is no explicit mention of specific regularization methods such as dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. However, it can be inferred that some form of regularization was applied to prevent overfitting in the multi-layer perceptron (MLP) model, which is a type of artificial neural network (ANN).

The text mentions that early stopping was enabled to prevent overfitting in the MLP model. Early stopping is a form of regularization where training is stopped before the model starts to overfit the training data. This helps to avoid memorizing noise in the training set and improves generalization performance on unseen data.

Additionally, the text states that the penalty method was employed within the framework of ANN models to address the tendency of neural networks to overfit data. While the exact nature of this penalty method is not specified, it could refer to various forms of regularization such as weight decay (L2 regularization) or sparsity constraints (L1 regularization).

Therefore, while the specific regularization methods used may not be explicitly mentioned, it can be deduced that some form of regularization was implemented to prevent overfitting in the deep learning pipeline.
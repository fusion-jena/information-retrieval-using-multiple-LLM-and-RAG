Before training a deep learning model, several preprocessing steps can be involved depending on the specific requirements of the dataset and the chosen architecture. Based on the given context, some of these preprocessing steps include:

1. Image Resizing: The resolution of input data is adjusted to 224x224 pixels with RGB format. This step ensures uniformity in the dimensions of all input images, making them compatible with the deep learning model.

2. Normalization: Inputs are batch normalized before training. Data normalization scales the values of features to a standard range, typically between 0 and 1 or -1 and 1. It helps improve the performance and convergence of the model during training.

3. Activation Function: After convolution or concatenate layers, an optimized rectified linear activation called Leaky ReLU is introduced into the models. This activation function allows small negative values to pass through, helping to mitigate the issue of dying neurons caused by traditional ReLU activations.

These preprocessing steps help prepare the data for efficient training and accurate predictions using deep learning models like AlexNet and ResNet mentioned in the context. However, other common preprocessing techniques such as data augmentation, noise removal, missing value imputation, encoding categorical variables, etc., may also be required based on the nature of the dataset and the specific application.
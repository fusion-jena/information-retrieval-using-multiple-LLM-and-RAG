In the given context, there isn't explicit mention of the specific data annotation techniques used in their deep learning pipeline. However, we can infer from the text that they likely employed some form of object detection or semantic segmentation, considering their focus on detecting and identifying fish in images.

Object detection involves drawing bounding boxes around objects of interest in an image, which could be what was done here since they discuss recognizing the head and mouth of fish in specific positions within underwater stereo images. This process often uses techniques like bounding box annotation during the labeling phase.

Semantic segmentation, on the other hand, involves assigning each pixel in an image to a particular class, essentially creating a mask over the entire image. This might have been used in their approach, especially when discussing the use of fish body area as a parameter to estimate body weight, suggesting they may have needed to accurately delineate the boundaries of individual fish. Instance segmentation is a related concept, where not only do you identify the pixels belonging to each object but also distinguish between different instances of the same class.

Without further details about their exact methods, it's hard to definitively state which specific data annotation techniques were used. But based on the described tasks, it seems plausible that either bounding box annotation for object detection or semantic/instance segmentation for accurate fish boundary identification would have been necessary.
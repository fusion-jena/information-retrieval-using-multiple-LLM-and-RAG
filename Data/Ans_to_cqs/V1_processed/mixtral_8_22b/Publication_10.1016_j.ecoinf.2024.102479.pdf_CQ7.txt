In the given context, the authors have used a 10-fold cross-validation design to evaluate the performance of their models. They divided their dataset, which consisted of 138 ground-based AGB estimates, into two distinct sets: 70% of the dataset was allocated for model training, while the remaining 30% was used as a testing dataset. This approach ensures that the model is trained on a sufficient amount of data while also providing an independent dataset for evaluating its performance. However, it should be noted that the context does not explicitly mention any specific criteria used to split the data for deep learning model training, such as train, test, or validation datasets. Nonetheless, based on the information provided, we can infer that the authors followed a common practice in machine learning where the dataset is randomly partitioned into training and testing datasets. Additionally, they employed a grid search approach using cross-validation to fine-tune the hyperparameters of their algorithms, aiming to minimize errors and achieve the best possible performance.
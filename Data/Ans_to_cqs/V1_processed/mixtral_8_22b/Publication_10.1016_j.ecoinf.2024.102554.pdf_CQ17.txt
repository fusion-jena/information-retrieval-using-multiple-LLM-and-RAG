The provided context does not directly mention any specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it can be inferred that some form of regularization might be applied due to the use of neural networks in the mentioned algorithms. Regularization techniques like dropout or L2 regularization could potentially be employed to avoid overfitting. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust. On the other hand, L2 regularization adds a penalty term to the loss function, discouraging large weights and promoting simpler models. Nevertheless, without explicit information about the specific regularization methods used in these algorithms, we cannot provide a definitive answer.
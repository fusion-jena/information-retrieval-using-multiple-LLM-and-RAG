To address data bias during preprocessing of the deep learning pipeline, several techniques can be employed. One common technique is stratified splitting, which ensures that each subset of the data maintains the same percentage of samples for each target class as the complete set. This helps prevent any one class from dominating a particular subset of the data. Another technique is oversampling, which involves duplicating samples from the minority classes to balance out the dataset. On the other hand, undersampling entails randomly removing samples from the majority classes to achieve a balanced dataset. Both oversampling and undersampling aim to mitigate the impact of imbalanced datasets on machine learning algorithms. Additionally, diverse data collection can also help reduce data bias by ensuring that the dataset includes examples from all relevant subgroups or categories. Lastly, the Synthetic Minority Over-sampling Technique (SMOTE) is another commonly used method that generates synthetic samples based on the feature space and combines features from neighboring samples to augment the dataset.
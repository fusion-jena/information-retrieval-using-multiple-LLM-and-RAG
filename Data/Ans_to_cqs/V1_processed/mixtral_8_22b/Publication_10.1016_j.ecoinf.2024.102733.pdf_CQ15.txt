The deep learning pipeline employs the Adam optimizer (Kingma and Ba, 2015) throughout its training process. This optimizer is utilized in both the training of the Multi-Layer Perceptron (MLP) and the Autoencoder model. The Adam optimizer dynamically adapts the learning rate for each parameter during the training phase, making it suitable for problems involving large amounts of data or parameters. Furthermore, a scheduler is implemented to adjust the learning rate when the network reaches a plateau, i.e., when the loss function stops decreasing after a certain number of epochs. This strategy helps prevent overfitting and improves the overall performance of the models.
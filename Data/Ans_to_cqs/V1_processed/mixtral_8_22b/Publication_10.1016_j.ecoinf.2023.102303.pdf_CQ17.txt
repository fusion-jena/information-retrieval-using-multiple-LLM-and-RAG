Based on the given context, there is no explicit mention of specific regularization methods such as dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. However, the text does discuss two approaches to address the issue of limited labeled data availability, which can lead to overfitting. These approaches include combining weak supervision and transfer learning, as well as developing a semi-supervised contrastive learning model and a weakly-supervised model.

Transfer learning involves using pre-trained models on large datasets like ImageNet to improve performance on smaller, task-specific datasets. In this case, two well-known DNNs, EfficientNet and ViT, were pre-trained on ImageNet before being fine-tuned on the custom FishInTurbidWater dataset. This process helps reduce overfitting by leveraging the knowledge gained from training on a larger dataset.

Additionally, the semi-supervised contrastive learning model was developed to further mitigate the risk of overfitting. Contrastive learning is a type of self-supervised learning that encourages representations of similar inputs to be close together in the embedding space, while dissimilar inputs should be far apart. By applying this technique, the authors aimed to create a more robust and generalizable model that could perform well even when trained on a limited amount of weakly labeled data.

While not explicitly mentioned, it is possible that some form of regularization, such as dropout or weight decay (L2 regularization), may have been employed during the training process to further combat overfitting. However, without additional information, it cannot be confirmed whether these specific techniques were utilized.
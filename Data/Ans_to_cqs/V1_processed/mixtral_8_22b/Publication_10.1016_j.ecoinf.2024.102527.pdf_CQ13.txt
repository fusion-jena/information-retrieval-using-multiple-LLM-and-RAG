The authors used the Stochastic Gradient Descent (SGD) optimizer with a momentum value of Î¼ = 0.9 to train their deep learning model. They did not explicitly mention other hyperparameters such as learning rate, batch size, or weight decay in this section. However, they noted that other hyperparameter settings for model training can be found in Table S.4 of the Supporting Information.
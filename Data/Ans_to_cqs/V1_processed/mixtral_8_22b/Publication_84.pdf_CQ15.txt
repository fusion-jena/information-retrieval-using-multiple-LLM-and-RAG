Unfortunately, the given context does not provide specific details about the optimization techniques used in the deep learning pipeline, such as Stochastic Gradient Descent (SGD) or Adam. Instead, the text focuses on comparing the performance of Stacked AutoEncoders (SAE) and Support Vector Machines (SVM) for detecting invasive Spartina alterniflora using Landsat satellite imagery.

In the context, the authors mention tuning parameters for the SVM model by testing different combinations of 'gamma' and 'cost' values. They use a sensitivity analysis method described in another paper (Heydari and Mountrakis, 2018) and select the highest overall accuracy achieved from different parameter combinations. In this case, they set 'gamma' and 'cost' to 2 and 10, respectively.

While the context does not explicitly discuss optimization techniques like SGD or Adam, these techniques are commonly used in training deep learning models like SAEs. Optimization algorithms help minimize the loss function during the training process, enabling the model to learn effectively from the input data. Without explicit mentions in the provided context, however, we cannot definitively state which optimization technique was employed in their deep learning pipeline.
The paper does not provide information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions using an optimized random trees approach where each new tree learns from the errors of those previous. This could potentially include techniques such as dropout or L2 regularization, but without explicit mention, we cannot confirm which methods were employed.

Based on general practices in machine learning, common regularization methods used to prevent overfitting include dropout, L1 regularization, and L2 regularization. Dropout randomly drops out nodes during training to reduce co-dependencies between neurons, while L1 and L2 regularizations add penalties to the loss function to discourage large weights and encourage simpler models.
The given context discusses several techniques used to address data bias during preprocessing of the deep learning pipeline. These include stratified splitting, oversampling, and diverse data collection.

Stratified splitting involves dividing the dataset into subsets based on certain characteristics, such as class labels, ensuring that each subset maintains the same proportions of these characteristics as the original dataset. This technique helps prevent overrepresentation or underrepresentation of any particular characteristic in the training or test set. As mentioned in the context, stratifying the training data for classification tasks 2 and 3 improved the ability of classifiers to identify relevant texts in testing data.

Oversampling is another method used to balance classes by increasing the number of instances in the minority class. Although not explicitly stated in the context, it is implied that oversampling could potentially improve classification performance when dealing with imbalanced datasets.

Diverse data collection refers to gathering data from various sources or employing different relevance criteria to ensure that the dataset represents a wide range of perspectives and scenarios. This approach is particularly useful when addressing issues like the exploitation or sale of a particular taxa, where no existing proxy datasets might exist.

In summary, the techniques used to address data bias during preprocessing of the deep learning pipeline include stratified splitting, oversampling, and diverse data collection. Each technique serves its unique purpose in mitigating biases and improving overall model performance.
The given context does not provide specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, some commonly used regularization methods include dropout, L1 regularization, and L2 regularization. Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training, which helps to reduce interdependent learning amongst neurons. L1 regularization adds an absolute value of the magnitude of coefficients as penalty term to the loss function, while L2 regularization adds a squared magnitude of the coefficient as penalty term to the loss function. These methods help to prevent overfitting by adding constraints to the optimization problem and reducing the complexity of the learned models.
The text does not explicitly state how the hyperparameters of the model were optimized. However, it mentions that the model was trained for 150 epochs with a batch size of 128, dropout rate of 0.3, learning rate of 0.0001, and ADAM optimization was used. It also states that these parameters were chosen to minimize the cross entropy loss between the network's predictions and the pre-labeled data. Therefore, it can be inferred that some form of hyperparameter tuning was likely employed, but the specific method (grid search, random search, etc.) is not mentioned.
The authors have addressed data bias through various methods during the preprocessing stage of their deep learning pipeline. They utilized stratified splitting to divide their dataset into training and testing sets, ensuring that both sets maintain the same proportion of samples belonging to each class. This approach helps prevent any imbalance between classes in the training and testing datasets.

To further tackle potential issues related to data imbalance, they employed cross-validation by shifting the validation set five times. Cross-validation is a technique used to estimate the skill of machine learning models and reduce overfitting. By repeating this process multiple times, it allows for more robust evaluation of model performance.

Additionally, the authors mentioned using different subsets of features and varying the number of hidden layers in their neural networks. These strategies can help improve model generalization and mitigate the impact of any inherent biases within the dataset. However, there is no explicit mention of oversampling, undersampling, or diverse data collection techniques being applied in this study.
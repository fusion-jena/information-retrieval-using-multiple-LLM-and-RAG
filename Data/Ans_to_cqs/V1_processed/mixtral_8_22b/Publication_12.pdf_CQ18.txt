The strategy implemented to monitor the model performance during training involves several steps. Firstly, the images were reduced in size from 256*256 pixels to 64*64 pixels to save training time and reduce high memory graphics use. These size-reduced images were then packaged into two datasets: a training dataset with 85% of the images and a validation dataset with 15%. This was done using a Python script Making_dataset.py in Waveman.

To balance the image number for all the species, an upper limit was provided for when a species had too much data. Additionally, a method was developed to increase the number of images for the rare species, where signals were rescaled exponentially, and the window was shifted slightly either side of the call to change background noise.

Waveman was further improved by modifying BatNet and optimizing parameter settings of batch size. New kinds of BNorm layers were added behind the 22 convolutional layers to prevent overfitting when training models using BatNet. The model was trained using the training dataset for 50â€“60 times using GPU for two hours and 10 minutes. After training, the model contained refined parameters and a graph which were saved for downstream analysis.

The testing and further refinement of Waveman involved using filtered data (validation dataset) to test different models and taxon bias. Audio files from a corridor for the 15 species were collected to lower misidentification and false positives. The model was optimized, and audio from different human and natural conditions was recorded to test the generalization of the model.

However, the specific strategy to monitor the model performance during training is not explicitly mentioned in the given context. It could involve techniques such as tracking loss and accuracy metrics during training, visualizing the model's predictions, or using tools like TensorBoard for real-time monitoring. But without explicit mention in the context, it remains uncertain what exact methods were employed.
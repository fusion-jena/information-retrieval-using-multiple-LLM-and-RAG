In this study, the authors use the L1 penalty, also known as LASSO (Least Absolute Shrinkage and Selection Operator) regularization, to perform simultaneous shrinkage estimation and variable selection. This regularization technique adds a penalty term to the loss function, encouraging smaller weights and thus preventing overfitting. Unlike L2 regularization or Ridge Regression, which uses the sum of the squares of the weights as the penalty term, L1 regularization uses the absolute value of the weights. This often results in sparse solutions where many weights become zero, effectively performing feature selection. However, it should be noted that other regularization techniques such as Dropout, Batch Normalization, and Early Stopping could also be employed in deep learning pipelines to prevent overfitting. These methods work differently but share the common goal of reducing complexity and improving generalization performance.
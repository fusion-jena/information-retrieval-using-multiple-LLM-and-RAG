Unfortunately, there isn't any explicit information given regarding the specific hardware resources used for training the deep learning model. Therefore, it cannot be determined whether GPUs, TPUs, or other types of hardware resources were utilized during the training process.

However, based on common practices in deep learning, it can be assumed that either GPU or TPU was likely employed due to their superior computational capabilities compared to CPUs. This assumption is further supported by the fact that the model required 14 hours to complete its training, which suggests the use of high-performance computing resources such as GPUs or TPUs. Nevertheless, without concrete evidence, it remains uncertain what exact type of hardware resource was used for training the deep learning model.
After the model training process, several postprocessing steps can be performed depending on the specific requirements of the study or project. Based on the given context, it appears that the researchers focused on evaluating their model's performance using two main metrics: Area Under the receiver-operator Curve (AUC) and True Skill Statistics (TSS). These metrics help determine the accuracy and reliability of the model predictions.

To calculate these metrics, one must first gather the necessary information from the trained model. For instance, when calculating the AUC, the true positive rate (TPR) and false positive rate (FPR) need to be determined based on the model's output. Similarly, computing the TSS requires knowing the true positives, true negatives, false positives, and false negatives obtained from the model's results.

Additionally, visualizing the model's performance through tools like saliency maps could provide valuable insights into which input features contribute significantly to the final outcome. However, this is not explicitly mentioned in the provided context.

Lastly, creating a confusion matrix might also be helpful during the postprocessing stage. This tool allows researchers to compare actual versus predicted outcomes, providing a clearer understanding of the model's strengths and weaknesses. Again, while not directly stated in the context, generating a confusion matrix is often part of the postprocessing steps in many machine learning projects.
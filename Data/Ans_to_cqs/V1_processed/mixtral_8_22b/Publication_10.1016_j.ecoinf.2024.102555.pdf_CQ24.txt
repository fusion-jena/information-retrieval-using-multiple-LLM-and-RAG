The papers provided do not directly address the issue of handling randomness in the deep learning pipeline, such as managing random seed values. However, some common strategies used to manage this type of randomness include setting fixed seeds for reproducible results, using ensemble methods or cross-validation techniques to reduce variability caused by different initializations, and employing deterministic algorithms where possible. Additionally, researchers may use techniques like dropout regularization or batch normalization to introduce controlled noise during training, which can help improve model generalization and robustness.
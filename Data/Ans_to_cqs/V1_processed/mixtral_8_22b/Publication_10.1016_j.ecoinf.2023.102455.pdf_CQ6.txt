The preprocessing steps involved before training a deep learning model can vary depending on the specific requirements and nature of the data. However, some common preprocessing techniques include:

1. Data Cleaning: This involves removing or correcting any inconsistencies, errors, or missing values in the dataset. It may also involve handling outliers or anomalies.

2. Normalization/Standardization: These techniques are used to rescale the features so they have zero mean and unit variance. This is important because many machine learning algorithms perform better when numerical input variables are scaled to a standard range.

3. Feature Scaling: Similar to normalization, feature scaling rescales the values into a certain range, often between 0 and 1. This is done to ensure all features contribute equally to the final output.

4. Encoding Categorical Variables: Deep learning models cannot handle categorical data directly. Therefore, it needs to be converted into numerical form using encoding techniques like label encoding or one-hot encoding.

5. Splitting Dataset: Before training the model, the dataset is usually split into two parts - training set and testing set. The model learns from the training set and its performance is evaluated on the test set.

6. Data Augmentation: In case of image data, this technique is used to increase the amount of training data by creating modified versions of images in the dataset. This helps prevent overfitting and improves the modelâ€™s ability to generalize.
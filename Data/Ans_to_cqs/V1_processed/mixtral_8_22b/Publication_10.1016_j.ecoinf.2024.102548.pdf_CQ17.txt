The text does not provide explicit details about the regularization methods used in the deep learning pipeline to prevent overfitting. However, based on common practices in deep learning, some potential regularization methods could include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust and reducing reliance on individual neurons. On the other hand, L2 regularization adds a penalty term to the loss function, encouraging smaller weight values and preventing any single weight from becoming too dominant, thus helping reduce overfitting. Other possible regularization methods might include early stopping, data augmentation, or batch normalization, although these are not explicitly mentioned in the given context.
The hyperparameters of the model are optimized by testing various combinations of their values using random generators. This approach ensures a more comprehensive evaluation of the model's performance. Specifically, the authors explore and evaluate different values for learning rate, batch size, and latent dimension. They also use the Adam optimizer during the training of the MLP and employ a scheduler that adjusts the learning rate when the network reaches a plateau. Furthermore, they test various regularization techniques, such as Cross-Entropy or Focal Loss, and experiment with additional parameters like α and γ introduced by Focal Loss.
Before training a deep learning model, several preprocessing steps can be applied to the data. These may include:

1. Data Cleaning: This involves removing or correcting erroneous data points, such as missing values, outliers, or inconsistent entries.

2. Normalization/Standardization: This step scales numerical data to a similar range, which helps prevent variables with larger ranges from dominating those with smaller ranges during model training. Common methods include Min-Max Scaling, StandardScaler, and RobustScaler.

3. Encoding Categorical Variables: Deep learning models typically work better with numeric inputs rather than categorical ones. Therefore, it might be necessary to convert categorical variables into numerical representations using techniques like one-hot encoding, label encoding, or ordinal encoding.

4. Feature Selection: Selecting relevant features can help improve model performance and efficiency. Techniques such as Recursive Feature Elimination with Cross Validation (RFECV) can be employed to identify important features.

5. Data Augmentation: To increase the size of the dataset and introduce more variability, various transformations can be applied to existing images, including rotation, flipping, zooming, cropping, etc.

In this specific case, the authors have mentioned using Recursive Feature Elimination with Cross Validation (RFECV) for selecting optimal features. However, they did not explicitly mention other preprocessing steps like data cleaning, normalization, or encoding categorical variables.
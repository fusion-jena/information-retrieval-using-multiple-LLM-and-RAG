To address randomness in the deep learning pipeline, several strategies can be employed. One common approach is to use stochastic methods like stochastic gradient boosting (Friedman, 2002). In this method, randomness is introduced into the process through the selection of samples or features used during training. By incorporating randomness, the model becomes more robust and generalizes better to unseen data.

Another strategy is to employ extremely randomized trees (Geurts et al., 2006). These decision trees introduce randomness by selecting cut points randomly instead of searching for the best split point. This results in faster computation times and reduces overfitting.

Random forests (Hengl et al., 2018) also incorporate randomness by building multiple decision trees based on different subsets of the original dataset. Each tree makes predictions independently, and the final prediction is determined by aggregating the individual outputs. This ensemble approach helps reduce variance and improve overall accuracy.

Dropout layers (Ozg√ºr & Nar, 2020) are another technique used to manage randomness in neural networks. Dropout works by randomly deactivating neurons during training, which prevents overfitting and encourages the network to learn more robust representations.

Additionally, setting a fixed random seed value before running experiments ensures consistency across runs and enables reproducible research. However, it should be noted that fixing the random seed may limit the exploration of different solutions and could potentially lead to missing out on better models.

Finally, when dealing with large datasets, stratified sampling techniques can help ensure that the distribution of classes or labels remains consistent between training and validation sets. This maintains balance within the dataset and improves the reliability of evaluation metrics.
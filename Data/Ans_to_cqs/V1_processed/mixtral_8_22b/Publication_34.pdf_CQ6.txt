Before training a deep learning model such as a Convolutional Neural Network (CNN), several preprocessing steps may be involved. These steps aim to prepare the data for optimal performance during the training process. Some common preprocessing techniques include:

1. Normalization: In the given context, Batch-Normalization is applied to every pre-activation (before applying the ReLU) for all classes of neural network models, including CNNs. Batch-Normalization helps optimize the learning rate initialization and speeds up the optimization process. It involves standardizing the inputs or activations of each layer to have zero mean and unit variance across the mini-batch.

2. Scaling: Although not explicitly mentioned in the provided context, scaling is another common preprocessing step. Data scaling ensures that features with different scales do not dominate others during the learning process. Common scaling methods include Min-Max scaling and Standardization.

3. Cleaning: While not directly discussed in the context, data cleaning is often necessary before feeding data into a deep learning model. Data cleaning involves handling missing values, removing outliers, and correcting inconsistent entries. Properly cleaned data improves the overall quality of the dataset and enhances the model's ability to learn meaningful patterns.

In summary, the main preprocessing steps involved before training a deep learning model like a CNN include normalization, scaling, and cleaning. Normalization, specifically Batch-Normalization, is employed in the given context to improve optimization speed and simplify the learning rate initialization. Additionally, although not mentioned in the context, scaling and cleaning are also essential preprocessing steps to ensure optimal performance during the training process.
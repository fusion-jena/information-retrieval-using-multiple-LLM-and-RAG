Unfortunately, the provided context does not explicitly mention any specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adam that are commonly used in the deep learning pipeline. However, we do see that the study utilizes a training-and-test approach with stratification splitting, which involves tuning hyperparameters on the training set for each split. This process could potentially involve optimization techniques like SGD or Adam, although this information isn't directly stated in the context.

Additionally, four methods were employed to model forest AGB in the study: AdaBoost, Random Decision Forest (RF), Multilayer Neural Network (NET), and Bayesian Ridge Regression (Bayes). Among these, only the Multilayer Neural Network (NET) would typically use optimization techniques like SGD or Adam during its training phase. However, without further details about the implementation of the NET method, we cannot definitively state whether SGD, Adam, or another optimization technique was used.
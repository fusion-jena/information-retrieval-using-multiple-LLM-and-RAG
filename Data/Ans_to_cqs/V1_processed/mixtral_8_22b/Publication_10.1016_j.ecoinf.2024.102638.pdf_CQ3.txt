Based on the given context, there is no information about the specific data annotation techniques used in the deep learning pipeline. Therefore, it cannot be determined what type of data annotation techniques were employed, such as bounding box annotation or instance segmentation.

However, the context does provide details about the machine learning models used, including KNN, GP, TREE, RF, XGB, SVR, and MLP. It also mentions various parameters associated with these models, such as alpha, l1\_ratio, neighbours, weights, p, max\_depth, min\_samples\_split, min\_samples\_leaf, max\_features, n\_estimators, learning\_rate, subsample, colsample\_bytree, reg\_alpha, gamma, min\_child\_weight, C, epsilon, hidden\_layer\_sizes, activation, and max\_iter. These parameters are likely optimized using grid search and cross-validation methods to improve the accuracy of the models.

Additionally, the context describes how the dataset was split into training and testing sets, with 80% of the data allocated for training and 20% for testing. The division was performed randomly to maintain representativeness and minimize bias. Furthermore, the negative mean squared error was chosen as the target metric for evaluating the performance of the models.

In summary, based on the available context, it is not possible to determine the specific data annotation techniques used in the deep learning pipeline. However, the context provides valuable insights into the machine learning models utilized, their optimization strategies, and the evaluation metrics employed.
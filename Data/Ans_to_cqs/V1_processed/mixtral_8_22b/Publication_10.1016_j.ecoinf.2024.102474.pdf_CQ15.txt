The deep learning pipeline uses the AdamW optimizer. Unlike the standard Adam optimizer, AdamW decouples the weight decay, which has been shown to improve generalization. The learning rate is set to 0.0003. Additionally, hyperparameters were tuned using an Optuna-based hyperparameter optimization procedure.
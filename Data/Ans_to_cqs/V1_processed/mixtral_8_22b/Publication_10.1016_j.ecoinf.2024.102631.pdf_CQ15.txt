Based on the given context, it does not explicitly mention any specific optimization technique such as Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), etc., that were applied in the deep learning pipeline. However, it can be inferred from the context that some form of optimization was employed during the training process of the networks mentioned. Specifically, the use of batch normalization and leaky ReLU activation functions suggests that these networks were trained using gradient descent based optimizers. Additionally, the presence of residual blocks within the encoder indicates that skip connections might have been utilized to mitigate the vanishing gradients problem, further suggesting the application of advanced optimization techniques like Adam or RMSprop. Nevertheless, without explicit information about the optimization techniques used, one cannot definitively state what specific optimization techniques were applied in the deep learning pipeline.
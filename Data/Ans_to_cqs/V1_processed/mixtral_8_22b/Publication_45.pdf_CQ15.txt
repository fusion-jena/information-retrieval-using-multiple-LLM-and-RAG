In the given context, it is mentioned that one of the main tools of deep learning is the Deep Neural Network (DNN), which builds upon Artificial Neural Networks (ANNs). The advancements in deep learning have been made possible due to improved algorithms for optimizing connection weights, such as stochastic gradient descent (SGD). This indicates that SGD is one of the optimization techniques applied in the deep learning pipeline. Additionally, the context mentions that the hyper-parameters evaluated iteratively included the applied regularization techniques, as well as the used loss function and optimizer. However, the specific optimizers or optimization techniques used are not explicitly stated in the provided context. Therefore, while we can confirm that SGD is an optimization technique used in deep learning pipelines, further information would be required to provide a comprehensive list of all optimization techniques applied in this particular case.
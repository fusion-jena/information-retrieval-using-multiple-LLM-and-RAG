The text does not provide explicit information about how the hyperparameters of the model were optimized. However, it mentions that the Adam optimizer algorithm was used for model optimization, which suggests that some form of gradient descent was employed. Additionally, it states that the learning rates were chosen from empirical trials over 100 epochs, indicating that a trial-and-error or iterative approach may have been used to find optimal values for these parameters. Unfortunately, without more specific details, it cannot be definitively stated whether techniques such as grid search or random search were utilized for hyperparameter optimization.
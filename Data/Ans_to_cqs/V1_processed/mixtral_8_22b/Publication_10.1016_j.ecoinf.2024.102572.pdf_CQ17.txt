The provided context does not directly discuss specific regularization methods used to prevent overfitting in deep learning pipelines such as dropout or L2 regularization. However, I can provide some information about these methods. Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function during training. Dropout is a popular regularization method where randomly selected neurons are ignored during training, which helps reduce interdependent learning amongst them. On the other hand, L2 regularization adds a squared magnitude of coefficient as penalty term to the loss function, encouraging smaller weights and preventing any single feature from dominating the model. Other common regularization methods include L1 regularization, early stopping, and data augmentation.
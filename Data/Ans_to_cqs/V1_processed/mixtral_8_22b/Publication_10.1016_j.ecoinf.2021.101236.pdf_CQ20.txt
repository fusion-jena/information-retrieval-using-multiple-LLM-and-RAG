Unfortunately, there isn't enough information available to determine what specific hardware resources were used for training the deep learning model. The given context only discusses the overall process, the use of a residual network (ResNet), and the structure of the model, including its components such as the pre-processing block, residual stacks, downsampling blocks, and the classification block. It also mentions the use of convolutions, batch normalization, and ReLU activation functions. However, no details about the hardware resources used during training, like GPUs or TPUs, have been mentioned.
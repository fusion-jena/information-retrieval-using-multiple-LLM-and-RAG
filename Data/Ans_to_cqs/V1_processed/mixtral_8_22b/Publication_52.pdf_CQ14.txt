According to the given information, Extreme Learning Machines (ELMs) are a type of machine learning algorithm that uses Single-hidden Layer Feedforward Neural Networks (SLFFNNs) and random representation of hidden neurons. Unlike traditional neural networks, ELMs do not require fine manipulation of operating parameters or determination of optimal output weights through iterative processes like backpropagation. Instead, all hidden layer parameters are determined independently from the activation functions and training data. This means that ELMs can generate hidden nodes or parameters randomly before even seeing the training data.

However, there is no explicit mention of how the hyperparameters of the model are optimized in the provided text. Hyperparameter optimization involves selecting the best values for various parameters that affect the performance of the model, such as the number of hidden nodes, regularization coefficients, etc. Common approaches include grid search, random search, Bayesian optimization, and others. Since the text does not provide specific details about this aspect, we cannot definitively state how hyperparameters are optimized in ELMs. However, it is worth noting that unlike some other machine learning models, ELMs do not suffer from issues related to stopping criteria, learning rates, and learning epochs, suggesting that they may be more robust to variations in these types of hyperparameters.
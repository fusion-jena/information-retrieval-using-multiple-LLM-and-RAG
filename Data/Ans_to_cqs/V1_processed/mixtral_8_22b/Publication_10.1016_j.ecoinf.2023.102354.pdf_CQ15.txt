The given context does not provide specific information about the optimization techniques used in the deep learning pipeline. However, common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), Root Mean Square Propagation (RMSProp), and others. These techniques help minimize the loss function during training by adjusting the model parameters iteratively based on the gradient of the loss with respect to those parameters. Without explicit mention in the provided context, it is not possible to determine which specific optimization technique was employed in this case.
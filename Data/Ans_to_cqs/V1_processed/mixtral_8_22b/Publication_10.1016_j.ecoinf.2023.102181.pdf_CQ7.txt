The text does not provide explicit criteria for splitting data for deep learning model training. However, it mentions cross-validation as a method for validating map predictions. Cross-validation involves partitioning the data into 'k' different subsets or folds, each of which is held out in turn as test data while the model is trained using the other 'k - 1' folds. This process is repeated 'k' times, with each fold serving as the test set once. The results are averaged across all runs to give an overall measure of model performance.

Additionally, the text discusses bagging, a technique used to reduce variance and improve the stability of machine learning algorithms. Bagging works by creating multiple subsets of the original data, fitting a model to each subset, and combining their predictions. Each subset is created by randomly sampling the original data with replacement, meaning some observations may appear more than once in any given subset.

While these methods do involve splitting the data, they do not directly address the specific criteria for dividing data into training, testing, and validation sets. Typically, these splits are made based on the size and nature of the dataset, as well as the requirements of the specific problem being addressed. For instance, a common practice is to use 70% of the data for training, 20% for testing, and 10% for validation. However, these proportions can vary depending on factors such as the total amount of available data and the desired balance between bias and variance in the final model.
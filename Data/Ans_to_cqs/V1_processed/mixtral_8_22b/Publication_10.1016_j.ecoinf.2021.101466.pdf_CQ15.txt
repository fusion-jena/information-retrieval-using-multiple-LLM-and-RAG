In the given context, the optimization technique used to train the EfficientNet is Adagrad. Adagrad is an algorithm for gradient-based optimization that adapts the learning rate to the parameters, performing smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequent features. It does so by dividing the learning rate for a weight by the square root of the sum of the squares of its previous gradients. This adaptive learning rate helps to prevent overfitting and improves convergence speed. Other popular optimization techniques include Stochastic Gradient Descent (SGD), Adam, and RMSprop, among others. However, the context only mentions the use of Adagrad.
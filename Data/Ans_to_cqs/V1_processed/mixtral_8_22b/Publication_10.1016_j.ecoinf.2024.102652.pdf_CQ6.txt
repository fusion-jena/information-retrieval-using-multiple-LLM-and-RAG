Before training a deep learning model, several preprocessing steps may be required depending on the nature of the dataset. These steps help improve the performance and efficiency of the model during training. Some common preprocessing techniques include:

1. Normalization: This involves rescaling the input features so they fall within a specific range, typically between 0 and 1 or -1 and 1. Normalizing the data helps prevent any single feature from dominating the others due to differences in scales. In the given context, it is mentioned that the training set needs to be normalized and standardized before fitting into the network.

2. Standardization: Also known as Z-score normalization, this technique transforms the data by subtracting the mean and dividing by the standard deviation. It ensures that the transformed data has a zero mean and unit variance. Standardization is useful when dealing with datasets where different features have varying units or scales.

3. Data Cleaning: This step involves identifying and handling missing values, outliers, or inconsistent data points in the dataset. Removing or imputing these problematic instances can significantly impact the model's performance and generalizability.

4. Feature Scaling: Similar to normalization, feature scaling aims to rescale the input features to a smaller range. However, unlike normalization, feature scaling does not necessarily bound the data between fixed limits. Common methods for feature scaling include Min-Max scaling and Max Absolute Scaling.

5. Data Augmentation: This technique involves creating new samples by applying various transformations to existing ones. Data augmentation helps increase the diversity of the training dataset and improves the model's ability to generalize to unseen examples.

In summary, some common preprocessing steps involved before training a deep learning model include normalization, standardization, data cleaning, feature scaling, and data augmentation. Depending on the characteristics of the dataset, one or more of these techniques might be employed to enhance the model's performance and efficiency.
The authors do not explicitly mention any specific strategies employed to handle randomness in their deep learning pipeline. However, they state that they used purely deterministic variants of architectures, implying that they might have avoided or minimized the use of elements that introduce randomness into the model training process. This could include setting fixed seeds for reproducibility purposes or avoiding techniques like dropout, which introduces randomness by deactivating neurons during training. Additionally, they utilized hyperparameter optimization procedures, such as those based on Optuna, to fine-tune their models. These methods can help reduce uncertainty and improve overall performance by finding optimal configurations for various parameters within the model architecture.
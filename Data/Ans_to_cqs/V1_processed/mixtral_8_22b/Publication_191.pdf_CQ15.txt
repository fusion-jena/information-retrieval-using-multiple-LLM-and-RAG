The given text does not provide explicit information about the specific optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD), Adam, etc. However, based on the common practices in deep learning, these optimization techniques are typically employed during the training phase of Convolutional Neural Networks (CNN). These methods help minimize the loss function by adjusting the model parameters iteratively. While the exact technique used in this case isn't mentioned, popular choices include SGD, Adam, RMSprop, and others. Therefore, without additional context, it cannot be definitively stated which optimization technique was utilized in this study.
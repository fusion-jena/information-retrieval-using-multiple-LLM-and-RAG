The text mentions that the optimization is carried out using the Adam optimizer with a momentum of 0.9 and a batch size of 16. This means that the Adam optimization algorithm is being used in the deep learning pipeline. Adam is a popular optimization method used in machine learning and deep learning models due to its ability to adaptively adjust the learning rate based on the first and second moments of gradients. It combines the advantages of two other optimization methods, AdaGrad and RMSProp, making it more suitable for problems with noisy or sparse gradients.
In the given context, there is no explicit mention of the regularization methods used to prevent overfitting in the deep learning pipeline. However, based on common practices in deep learning, some popular regularization techniques include Dropout and L2 Regularization.

Dropout is a regularization method where randomly selected neurons are ignored during training, which helps prevent overfitting. It can be thought of as a way to combine many different neural network architectures efficiently.

On the other hand, L2 Regularization, also known as weight decay, adds a penalty term to the loss function proportional to the sum of squares of all the weights in the network. This encourages smaller weights, leading to simpler models and reduced chances of overfitting.

These regularization methods can be applied independently or together depending on the requirements of the deep learning model. Other regularization techniques not mentioned here may also exist but are beyond the scope of this response.
The given context does not provide explicit information regarding the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, there is mention of using Batch Normalization (BN), which can help accelerate network convergence and prevent overfitting during training. Additionally, merging the BN parameters into a convolution layer improves the forward inference speed of the model. While these techniques contribute to better model performance, they do not directly address the issue of preventing overfitting through regularization methods like dropout or L2 regularization.
The given context does not provide information on the optimization techniques used in the deep learning pipeline. However, some commonly used optimization techniques in deep learning include Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), Root Mean Square Propagation (RMSProp), and AdaGrad. These techniques help minimize the loss function during training by adjusting the model weights iteratively based on the gradient of the loss function concerning the weights. The choice of optimization technique depends on factors such as convergence speed, memory requirements, and computational efficiency.

Based on the given context, it is not possible to determine which optimization techniques were applied in the deep learning pipeline. Therefore, I do not have enough information to provide a specific answer to this query.
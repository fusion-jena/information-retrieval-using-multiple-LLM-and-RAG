The authors utilized a Windows 11 Pro (64-bit)-based computational system with an Intel Core i7-8565U CPU @ 1.80GHz x 16 GB memory. They were also remotely connected to Ubuntu 18.04.6 LTS (GNU/Linux 4.15.0-169-generic x86 64), NVIDIA-SMI 470.103.01, GeForce GTX 1080 Ti, using CUDA 10.2 for GPU parallelization. Therefore, it can be concluded that they used a combination of CPUs and GPUs for training their deep learning model.
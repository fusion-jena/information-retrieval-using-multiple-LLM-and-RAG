In the given context, there is no explicit mention of specific regularization methods such as dropout or L2 regularization being employed to prevent overfitting in the deep learning pipeline. However, some strategies to mitigate overfitting can still be identified from the text.

One approach mentioned is early stopping, where the model is monitored during training, and once the validation loss stops improving, the training is halted. This prevents the model from memorizing the training data instead of learning a generalized strategy.

Additionally, data augmentation techniques like left-right mirroring were utilized to reduce overfitting. Data augmentation involves creating modified versions of the original images through transformations such as rotation, scaling, flipping, etc. These transformed images provide the model with more varied examples, helping it learn better representations and reducing the risk of overfitting.

While not explicitly stated, another possible method could involve using a smaller network architecture to avoid overparameterization, which might lead to overfitting. A simpler model would have fewer parameters to tune, making it less likely to fit the noise in the training data.

However, without further information, it cannot be confirmed whether any of these methods were specifically chosen as regularization techniques to combat overfitting.
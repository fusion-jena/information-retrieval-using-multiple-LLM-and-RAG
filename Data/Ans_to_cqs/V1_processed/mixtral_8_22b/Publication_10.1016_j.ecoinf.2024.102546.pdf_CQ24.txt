The papers mentioned do not directly address strategies for handling randomness in the deep learning pipeline, such as managing random seed values. However, some common practices used by researchers include setting fixed seeds for reproducibility or using techniques like ensemble methods, cross-validation, and bootstrapping to account for variability caused by random initialization or data shuffling. These approaches help improve model performance and robustness when dealing with randomness in the training process.
The papers mentioned do not directly discuss the specific regularization methods used to prevent overfitting in a deep learning pipeline. However, some common regularization techniques used in deep learning include Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training, which helps to prevent overfitting by making the model more robust and less sensitive to the specific weights of individual neurons.

L2 Regularization, also known as weight decay, adds a penalty term to the loss function based on the sum of squares of all the weights in the model. This encourages smaller weights and prevents any single weight from becoming too large, reducing the risk of overfitting.

Other regularization techniques include early stopping, data augmentation, and batch normalization. Early stopping involves monitoring the validation error during training and stopping when it starts to increase, indicating that the model is beginning to overfit. Data augmentation generates additional training examples by applying random transformations to existing data, increasing the diversity of the training set and helping to reduce overfitting. Batch normalization standardizes the inputs to each layer, improving the stability of the learning process and reducing the risk of overfitting.
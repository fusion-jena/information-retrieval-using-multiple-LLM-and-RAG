The hyperparameters used in the deep learning model include the number of neural network layers, the number of neurons per layer, dropout percentages, learning rate, and batch size. These hyperparameters have specific tuning ranges, such as 1-5 for the number of neural network layers, 64, 128, 256, 512, 1024, or 2048 for the number of neurons per layer, 0.1, 0.25, 0.35, or 0.5 for dropout, 0.001, 0.0001, or 0.00001 for learning rate, and 32, 64, 128, or 256 for batch size. Each set of parameters was trained based on training data (80%) and validated using validation data (10%). The results of each parameter set represent the average of ten experiments, with the data being shuffled each time. The goal of the optimization process was to maximize the R value of the validation set using the grid search method.
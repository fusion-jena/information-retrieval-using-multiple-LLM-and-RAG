The deep learning model uses ADAM optimization with a learning rate of 0.0001. Dropout with a rate of 0.3 is used during training to stabilize the model. The model is trained for 150 epochs with a batch size of 128. Cross entropy loss is used to minimize the difference between the network's predictions and the labeled data.
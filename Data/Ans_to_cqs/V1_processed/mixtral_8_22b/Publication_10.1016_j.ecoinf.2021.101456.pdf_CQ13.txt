The deep learning model used in this study is a dynamic β-VAE, which has several tunable model parameters. However, these are not considered hyperparameters in the traditional sense. The actual hyperparameters mentioned in the given context include the learning rate and the optimizer.

The learning rate used for training the models is 10^-5. The optimizer used is Adam. Other details about the implementation include the use of PyTorch as the framework, training for 5000 epochs, and utilizing a Nvidia GTX 1050 graphics processing unit with 4 GB memory and a batch size of 256.

A decision to adapt β, presumably a parameter related to the VAE's objective function, was made every fifth epoch to prevent random fluctuations. Additionally, the scaling of the clustering loss, denoted by γ, was cycled between 0.01 and 0.2 every 100 epochs during semi-supervised mode.
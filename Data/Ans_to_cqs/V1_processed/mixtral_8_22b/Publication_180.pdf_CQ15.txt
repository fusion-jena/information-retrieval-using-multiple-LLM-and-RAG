The text does not provide explicit information about specific optimization techniques like Stochastic Gradient Descent (SGD) or Adam being used in the deep learning pipeline. However, it mentions adjusting certain parameters, namely 'validation percentage' and 'number of training steps', which can be considered part of model tuning and optimization. The authors also mention exploring different numbers of training steps to find a balance between computation time and classification efficiency, indicating they likely experimented with hyperparameter tuning to optimize their model. Nevertheless, without further details, it cannot be confirmed if more advanced optimization techniques were employed.
The text does not provide explicit information on the specific strategy used to monitor the model performance during training. However, based on the given context, one could infer that the authors might have employed built-in cross-validation support available in XGBoost, which aids in model evaluation and hyperparameter tuning. Cross-validation involves splitting the dataset into several folds, where each fold serves as a validation set once while the remaining folds form the training set. This process helps evaluate the model's performance and prevent overfitting. Furthermore, the use of regularization techniques, such as loss function, mentioned in the context, effectively prevents overfitting and promotes simpler models. Therefore, these strategies may be utilized to monitor the model performance during training.
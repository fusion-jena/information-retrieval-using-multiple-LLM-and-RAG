The authors used several regularization methods to prevent overfitting in their deep learning pipeline. They employed an early stopping strategy by halting the network training after 70 epochs. Additionally, they utilized an exponential learning rate decay with a gamma value of 0.95. Another method implemented was dropout regularization, setting the dropout rate to 50%. These techniques are considered classic hyperparameters for achieving fast convergence and avoiding overfitting in neural networks (Srivastava et al., 2014). Furthermore, the authors used a Gaussian random initialization for weights, which is another common practice to help prevent overfitting.
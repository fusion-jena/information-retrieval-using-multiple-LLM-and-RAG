The deep learning pipeline described in the given context uses audio data. Specifically, it converts the waveforms of the sounds into Mel-spectrograms, which is a signal representation derived from the spectrogram that substitutes the normal frequency scale with the nonlinear Mel scale. This conversion allows the audio data to be processed by the VGGish model, a convolutional neural network (CNN) based on Configuration A of the VGG image classification model. Additionally, the pipeline also mentions using a CNN from Googleâ€™s AudioSet project to generate embeddings per second of audio. Therefore, the data format used in this deep learning pipeline is primarily audio data.
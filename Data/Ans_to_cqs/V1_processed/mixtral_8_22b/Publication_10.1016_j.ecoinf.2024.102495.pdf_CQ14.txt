Based on the information provided, it appears that the authors used a combination of various hyperparameter settings to find the best performing model. They experimented with three different optimizers (RMSProp, Adam, SGD), two learning rates (0.001, 0.01), and both with and without dropout layers. This suggests that they may have employed a form of manual search or grid search to explore these combinations. However, the exact methodology used is not explicitly stated in the provided text. Therefore, while we can infer that some form of systematic exploration of hyperparameters was conducted, the specific technique remains unclear.
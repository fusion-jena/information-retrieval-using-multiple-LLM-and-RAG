The authors used an Adam Solver type as their learning parameter. They also employed an exponential learning decay with a gamma of 0.95 and a dropout of 50%. These are considered classic hyperparameters for achieving fast convergence of the network without overfitting. Additionally, they mention using a random Gaussian initialization for weights. However, no specific information about the optimizer used within the Adam solver is mentioned.
Based on the given context, there isn't explicit information about specific preprocessing steps such as normalization, scaling, or cleaning before training the deep learning models like SegFormer or EPDNet. However, it is common practice in machine learning and deep learning to perform these preprocessing tasks. For instance, image datasets often undergo resizing, normalization, augmentation, etc., while text data might be tokenized, padded, or vectorized using techniques like Word2Vec or TF-IDF. Similarly, numerical data could be scaled or normalized to ensure consistent ranges across different features. Therefore, although not explicitly mentioned in the context, it is highly likely that some form of preprocessing was performed before training the deep learning models.
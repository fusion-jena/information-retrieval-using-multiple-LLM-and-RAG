The papers referenced do not directly discuss specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) used within the deep learning pipeline. However, these optimization techniques are commonly employed in deep learning frameworks to minimize loss functions during the training process.

For instance, Ruder (2016) provides an overview of various gradient descent optimization algorithms, including SGD and Adam. These methods help update the weights of the neural network based on the calculated gradients, aiming to find the optimal solution that minimizes the error between predicted and actual values.

While the mentioned papers primarily focus on architectures like MobileNetV2, Bottleneck Transformers, EfficientNet, and YOLOv7, it should be noted that these architectures would still rely on optimization techniques like SGD or Adam during their respective training processes.
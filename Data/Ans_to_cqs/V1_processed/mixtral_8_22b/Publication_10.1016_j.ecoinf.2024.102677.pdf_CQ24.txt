To handle randomness in the deep learning pipeline, including the use of random seed values, several strategies can be employed. One common approach is to fix the random seed value for reproducibility purposes. By setting a fixed seed value, researchers can ensure that their results will remain consistent even when running multiple experiments or simulations. Another strategy is to perform cross-validation, which involves splitting the data into different subsets for training, validation, and testing. This helps to reduce overfitting and improve model generalization by ensuring that the model has been exposed to a diverse range of examples during training. Additionally, techniques such as ensemble learning and bootstrap aggregating (bagging) can also help to mitigate the effects of randomness by combining the predictions of multiple models or resampling the data with replacement, respectively. In the given context, the authors have used stratified sampling to divide the dataset into three parts - training, validation, and testing sets - while maintaining class proportions across all parts. They have also mentioned using an NVIDIA 3090 GPU with a specific image input size and setting parameters like learning rate, batch size, and number of epochs after parameter tuning. However, they do not explicitly mention any strategies related to handling randomness in the deep learning pipeline. Therefore, based on the provided information, it cannot be determined what specific strategies are being employed to handle randomness in this case.
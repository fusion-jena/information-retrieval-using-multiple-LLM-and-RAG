The methodological workflow of the Joint Energy-Based Semantic Segmentation (JESS) includes a deep learning part implemented with the Python library PyTorch. Although not explicitly mentioned, it can be assumed that some form of optimization technique would be used during the training process of the deep learning model. One common optimization technique used in deep learning is Stochastic Gradient Descent (SGD). However, the given text specifically mentions the use of the Adam optimizer (Kingma and Ba, 2017) for training. Therefore, we can conclude that the Adam optimizer is one of the optimization techniques applied in the deep learning pipeline.
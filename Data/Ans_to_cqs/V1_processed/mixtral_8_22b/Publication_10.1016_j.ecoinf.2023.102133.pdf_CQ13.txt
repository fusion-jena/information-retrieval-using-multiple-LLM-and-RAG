The deep learning model used in this study is a Multi-Layer Perceptron (MLP) model. The hyperparameters used in this model are as follows:

1. Solvers: Limited-Broyden–Fletcher–Goldfarb–Shanno (L-BFGS), Adam, Stochastic Gradient Descent (SGD) with constant learning rate, SGD with adaptive learning rate. The optimal solver chosen for this model is Adam.

2. Number of neurons in the hidden layers: This ranges from 1 to 100. However, the optimal number of neurons per hidden layer for this model is 10.

3. Regularization factor "alpha" (L2 penalty): This is set to 10.

4. Activation function: The options tested are identity, logistic, tanh, and ReLU. The optimal activation function chosen for this model is ReLU.

5. Learning Rate: The options tested are constant, invscaling, and adaptive. However, the specific learning rate used in this model is not explicitly mentioned.

These hyperparameters were selected through a process of grid search, where all possible combinations of hyperparameters were tested and the combination that resulted in the best performance score was chosen.
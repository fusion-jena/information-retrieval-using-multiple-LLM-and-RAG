Unfortunately, there isn't enough information available to provide a detailed explanation about how the trained deep learning model was deployed. However, based on the given context, we can infer that the authors have developed a highly competitive model suitable for real-time processing applications due to its high-performance speed.

To deploy such a model, one common approach would involve saving or serializing the trained model into a format that can be loaded later for making predictions. This could include formats like.h5 (HDF5),.pkl (Pickle), or.onnx (Open Neural Network Exchange). Once serialized, the model can be integrated into different platforms depending on the specific use case requirements. For instance, it might be deployed on cloud services like AWS, Google Cloud, or Azure, or on edge devices like Raspberry Pi or Jetson Nano.

However, without further details regarding the platform selection, hardware requirements, or any other specifics related to the deployment process, it is impossible to provide a comprehensive answer.
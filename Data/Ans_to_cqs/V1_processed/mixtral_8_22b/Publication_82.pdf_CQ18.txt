The authors implement a strategy to monitor the model performance during training by running their experiments across various train-test set splits. These include 80-20, 60-40, 50-50, 40-60, and 20-80 percentages of the entire dataset allocated for training and testing respectively. This approach allows them to evaluate how their models might perform on unseen data and helps detect potential overfitting issues. Each experiment is conducted for a total of 30 epochs, with one epoch defined as the number of training iterations required for the specific neural network to complete a full pass through the entire training set. The convergence of learning within this period is confirmed by observing the aggregated plots across all experiments. To ensure a fair comparison among different configurations, they maintain consistent hyperparameters throughout all experiments. These include stochastic gradient descent solver type, base learning rate of 0.005, step learning rate policy decreasing by a factor of 10 every certain number of steps, and momentum of 0.9. In summary, they employ a comprehensive strategy involving varying train-test set distributions, maintaining consistent hyperparameters, and monitoring learning convergence to effectively assess and manage model performance during training.
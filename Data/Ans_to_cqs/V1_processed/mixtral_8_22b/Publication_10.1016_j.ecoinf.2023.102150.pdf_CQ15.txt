The text mentions that an adam optimizer was used in the deep learning pipeline. Adam is a popular optimization algorithm used in deep learning that combines the advantages of two other extensions of stochastic gradient descent, namely adaptive gradient algorithm (AdaGrad) and root mean square propagation (RMSProp). It uses estimates of first and second moments of gradients to adaptively control the learning rates for each weight in the neural network. This allows it to converge faster and perform better than traditional stochastic gradient descent methods.
The primary method used for collecting raw data in the deep learning pipeline for camera trapping in biodiversity monitoring is through the deployment of camera traps in various regions. These devices automatically capture images when they detect motion or heat signatures using Passive InfraRed (PIR) technology. However, many of these captured images often do not contain any animals, leading to significant time spent on removing empty images. To address this issue, recent advancements have introduced Machine Learning (ML) techniques into this field.

One notable development is the integration of Artificial Intelligence (AI) into commercial camera traps like the Trail-Guard by Resolve. This allows for AI processing "at the edge," which can help mitigate some challenges associated with traditional camera traps, such as storing and analyzing numerous empty images. Researchers are also exploring deep learning techniques to minimize the collection of unnecessary data. For instance, Schindler and Steinhage (2021) employed a combination of Mask R-CNN with Flow-Guided Feature Aggregation (Zhu et al., 2017) to enhance instance segmentation of animal species in video clips captured by PIR-equipped camera traps during dusk and nighttime hours in Bavaria.

Additionally, there are tools available to assist researchers in identifying empty images within their dataset, such as Zilong (Wei et al., 2020) and Image Filtering and Labelling Assistant (IFLA) (Xi et al., 2021). These resources enable users to quickly remove irrelevant images, streamlining the data preprocessing phase before training deep learning models.

In summary, the main method for gathering raw data in the deep learning pipeline for camera trapping involves deploying camera traps equipped with PIR technology. Recent developments focus on integrating ML and AI techniques to improve image quality and reduce the volume of empty images stored and analyzed. Tools like Zilong and IFLA further aid researchers in efficiently managing their datasets prior to model training.
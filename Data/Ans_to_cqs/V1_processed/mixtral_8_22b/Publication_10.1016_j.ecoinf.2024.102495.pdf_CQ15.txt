Three optimization techniques are applied in the deep learning pipeline: RMSProp, Adam, and Stochastic Gradient Descent (SGD). These techniques are used to update the weights of the neurons in the neural network during training. Each technique has its own strengths and weaknesses, and they can be chosen based on the specific requirements of the problem being solved.

RMSProp is an adaptive learning rate method that uses the root mean square of recent gradients to normalize the gradient updates. This helps prevent oscillations and improves convergence speed. It is particularly useful when dealing with sparse data or noisy gradients.

Adam is another adaptive learning rate method that combines the advantages of both RMSProp and momentum methods. It maintains a moving average of past squared gradients and past gradients, which allows it to adaptively adjust the learning rate for each weight. Adam is often considered a good default choice for many problems due to its robustness and efficiency.

SGD is a simple yet effective optimization technique that updates the weights by taking small steps proportional to the negative gradient of the loss function. Although it may converge slowly, it is computationally efficient and works well for large datasets.

In this study, the authors experimented with these three optimization techniques to find the most optimal deep learning model. They found that Model 1, which did not use dropout, used the RMSprop optimizer, and had a learning rate of 0.001, performed the best among all the tested configurations.
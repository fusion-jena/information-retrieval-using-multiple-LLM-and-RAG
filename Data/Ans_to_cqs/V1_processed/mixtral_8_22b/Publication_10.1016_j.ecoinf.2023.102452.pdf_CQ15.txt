The given context does not provide explicit information about specific optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, it can be inferred from the references that various optimization methods have been employed in different studies related to machine learning and deep learning. For instance, Ang et al. (2022) discuss a modified Particle Swarm Optimization (PSO) algorithm for optimizing Artificial Neural Networks (ANNs) in classification tasks. Bas et al. (2022) explore training Simple Recurrent Deep Artificial Neural Networks (SRDANNs) for forecasting using PSO. Bowden et al. (2002) focus on optimal division of data for neural network models in water resources applications. These examples suggest that optimization techniques play a crucial role in improving the performance of deep learning models. Nevertheless, without more specific details regarding the use of optimization algorithms like SGD or Adam within these studies, we cannot definitively state which optimization techniques were applied in their respective deep learning pipelines.
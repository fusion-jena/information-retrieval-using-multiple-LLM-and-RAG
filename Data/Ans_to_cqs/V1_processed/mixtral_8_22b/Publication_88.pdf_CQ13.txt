The hyperparameters used in the deep learning model include a batch size of 32, 100 number of epochs, early stopping after 10 epochs with no improvement over the validation set, and a dropout rate of 0.2 in the final fully connected layer. The Adam optimizer was chosen with a weight decay of 0.9 and a learning rate of 10^-3. The binary cross-entropy was used as the loss function.
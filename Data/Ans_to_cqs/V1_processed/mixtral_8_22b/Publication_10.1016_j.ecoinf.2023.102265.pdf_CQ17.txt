The text does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, based on common practices in deep learning, some possible regularization methods could include dropout, L2 regularization, early stopping, data augmentation, and batch normalization. Dropout randomly sets a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. L2 regularization adds a penalty term to the loss function, encouraging smaller weights and preventing any single weight from dominating the others. Early stopping stops the training process before convergence if the performance on a validation set starts to degrade, thus avoiding overfitting. Data augmentation generates additional training samples by applying random transformations to existing ones, increasing the diversity of the dataset and reducing overfitting. Batch normalization normalizes the activations of each layer, making the network more stable and faster to converge while also acting as a form of regularization. It should be noted that these methods are speculative and not explicitly mentioned in the given context.
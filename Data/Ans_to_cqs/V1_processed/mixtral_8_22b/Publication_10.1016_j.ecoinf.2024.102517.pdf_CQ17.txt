Based on the given context, there is no explicit mention of specific regularization methods such as dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. However, some techniques mentioned could potentially help mitigate overfitting indirectly. For instance, batch normalization is employed using the adaptive normalization operator proposed by Chen et al. (2017). This technique helps reduce internal covariate shift, making the model more stable during training and possibly reducing overfitting. Additionally, the use of the leaky rectified linear unit (LReLU) as a non-linear operation might also contribute to preventing overfitting due to its ability to introduce sparsity into the activations. Nonetheless, without further information, it cannot be definitively stated whether these techniques were explicitly utilized as regularization methods to combat overfitting.
The provided context does not explicitly mention any specific strategies employed to handle randomness in the deep learning pipeline, such as managing random seed values. However, some general practices can be inferred from the information given.

Firstly, the use of a consistent batch size of 16 examples across experiments suggests that maintaining consistency in data processing steps could help manage randomness. Secondly, using the model with the smallest validation loss for evaluation on the test set implies that selecting the best performing model during training might mitigate the effects of randomness. Lastly, employing techniques like cross-validation or repeating experiments multiple times with different seeds could also aid in handling randomness, although this is not mentioned in the provided context.
The given context does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, based on common practices in machine learning and deep learning, there are several strategies that could potentially be applied here:

1. Fixing the Random Seed Value: One way to manage randomness is to fix the random seed value before running each experiment. This ensures that the same sequence of pseudo-random numbers is generated every time the code runs, making the results more consistent and reproducible.

2. Multiple Runs and Average Results: Another strategy is to run multiple iterations of the model with different initializations or seeds and then average the results. This helps reduce the impact of individual random variations and provides a more robust estimate of performance.

3. Cross-Validation: Using cross-validation techniques like k-fold cross-validation can also help mitigate the effects of randomness. In this approach, the data is divided into 'k' subsets, and the model is trained and tested on different combinations of these subsets. This allows for a more comprehensive evaluation of the model's performance and reduces the influence of specific random factors.

While these strategies are commonly used to address randomness in deep learning pipelines, without further contextual information, it cannot be confirmed if they were specifically implemented in Mohanty et al.'s study.
The text does not provide information about specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adam that are typically used in deep learning pipelines. However, it mentions that different numbers of background points were used to train each model, and the number of background points was optimized for each particular modeling algorithm. This suggests that some form of hyperparameter tuning or optimization was employed to find the optimal number of background points for each algorithm. Additionally, the use of various modeling algorithms like GLMs, GAMs, MARS, RF, ANN, and MaxEnt indicates that multiple methods were explored to build the most accurate models possible.
The paper does not provide information on the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization techniques used in deep learning include dropout, L1 regularization, and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to reduce overfitting. L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients, while L2 regularization adds a penalty equal to the square of the magnitude of coefficients. These penalties help to prevent overfitting by discouraging large weights in the model.
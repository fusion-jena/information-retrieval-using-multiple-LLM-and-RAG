The text does not provide explicit information on specific regularization methods such as dropout or L2 regularization used to prevent overfitting in the deep learning pipeline. However, it mentions the use of data augmentation strategies like horizontal flipping, Gaussian noise addition, and contrast reduction to increase the training set size fourfold. These techniques help mitigate overfitting by introducing variations into the existing dataset, thereby improving the model's ability to generalize better to unseen data. While these aren't traditional regularization methods, they serve a similar purpose in preventing overfitting.
Based on the provided context, it does not explicitly mention any specific preprocessing steps taken before training the deep learning models. However, there are some clues about possible preprocessing techniques used. For instance, the text mentions that the moths dataset was used with the same data augmentation while comparing the customized CNN architectures with selected state-of-the-art CNN optimized architectures. Data augmentation is a technique used to artificially expand the size of a dataset by applying various transformations like rotation, zooming, flipping, etc., to the existing images. Additionally, the use of dropout probability of 0.3 after the second to last hidden layer suggests that regularization techniques might have been employed during the training process to prevent overfitting. Dropout is a commonly used regularization method where randomly selected neurons are ignored or 'dropped out' during training to reduce interdependent learning amongst them. Lastly, the fact that the average F1-score was used as a measure for a given architecture's performance implies that the target variable could be imbalanced, requiring appropriate handling during preprocessing. Therefore, although the exact preprocessing steps are not mentioned, we can infer that data augmentation, regularization methods, and possibly handling imbalance in the target variable might have been part of the preprocessing pipeline.
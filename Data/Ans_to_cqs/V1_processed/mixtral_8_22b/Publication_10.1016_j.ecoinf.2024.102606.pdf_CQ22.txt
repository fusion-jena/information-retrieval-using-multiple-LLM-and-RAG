Two primary metrics are used to evaluate the performance of the deep learning model: Intersection over Union (IoU) and Mean Average Precision (mAP).

Intersection over Union (IoU), defined by Equation 1, quantifies the overlap between the annotation and the prediction box. A threshold of 0.5 IoU is set to consider a prediction-annotation pair as a match. Any unmatched predictions are regarded as background elements.

Mean Average Precision (mAP), given by Equation 4, provides a single score for the entire model rather than individual scores for each class, like Average Precision (AP). The goal is to achieve a minimum of 95% precision for each species of interest.

Additionally, precision and recall are mentioned but not explicitly stated as evaluation metrics. However, they contribute to the calculation of another advanced metric called the precision-recall curve. This curve represents the tradeoff between recall and precision, depending on the confidence threshold applied to the model's output probabilities.
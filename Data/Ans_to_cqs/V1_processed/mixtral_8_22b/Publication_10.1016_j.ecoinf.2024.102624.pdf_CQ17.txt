Based on the given context, there is no direct mention of specific regularization methods such as dropout or L2 regularization used to prevent overfitting in a deep learning pipeline. However, some strategies mentioned in the text could be considered as forms of regularization. For instance, training data augmentation can help prevent overfitting by increasing the amount of available training data and reducing the risk of memorizing the training set. Additionally, post-classification techniques like segmentation and filtering can reduce noise and improve the quality of predictions, indirectly preventing overfitting. Hyperparameter tuning is another method that can help optimize the performance of machine learning algorithms and avoid overfitting. Although these methods do not directly correspond to traditional regularization techniques like dropout or L2 regularization, they still contribute to mitigating overfitting in the modeling process.
The authors use dropout regularization in the forward pass during training to prevent overfitting. Dropout is a technique where randomly selected neurons are ignored during training, which helps to reduce interdependent learning amongst them. Additionally, they also mention using weight decay, which is another term for L2 regularization. Weight decay adds a penalty equivalent to the square of the magnitude of coefficients to the loss function, discouraging large weights and thus preventing overfitting. They do not explicitly state whether other forms of regularization were employed or considered.

Reference(s):
[1] An, G., Akiba, M., Omodaka, K., Nakazawa, T., Yokota, H., 2021. Hierarchical deep learning models using transfer learning for disease detection and classification based on small number of medical images. Scient. Rep. 11 (1) <https://doi.org/10.1038/> s41598-021-83503-7.
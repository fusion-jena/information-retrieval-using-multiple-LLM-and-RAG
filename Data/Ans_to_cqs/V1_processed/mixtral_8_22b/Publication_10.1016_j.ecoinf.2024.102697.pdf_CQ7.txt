A repeated nested k-fold cross-validation approach was employed for all machine learning models developed here. An outer 10 k-fold cross-validation provided an initial division of the data into holdout data and data for model development. An inner 5 k-fold cross-validation was then used to minimize the bias caused by tuning of hyperparameters on training results. The training performance was calculated based on the inner cross-validated ensemble, while the testing performance was determined by selecting the optimal set of hyperparameters from the entire training set to predict on the holdout data. This process was repeated 15 times for each unique model to reduce the potential bias and variance related to random sampling. Therefore, the criteria used to split the data for deep learning model training were a combination of outer 10 k-fold cross-validation and inner 5 k-fold cross-validation.
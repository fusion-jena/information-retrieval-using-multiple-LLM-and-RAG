The given context does not provide specific information on the techniques used to address data bias during preprocessing of the deep learning pipeline. However, some commonly used techniques include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into subsets or strata based on certain criteria such as class labels, and then randomly selecting samples from each stratum to create balanced training and testing sets. This technique helps maintain the original distribution of classes in both sets.

Oversampling is a technique where synthetic examples are generated from existing minority class instances to balance the dataset. Synthetic Minority Over-sampling Technique (SMOTE) is one popular approach for generating new instances.

Undersampling involves removing instances from the majority class to achieve a more balanced dataset. Random under-sampling is a simple way to do this, but it may lead to loss of valuable information. More sophisticated methods like NearMiss and Condensed Nearest Neighbor (CNN) can help retain useful information while reducing the size of the majority class.

Diverse data collection aims to collect data from various sources and perspectives to reduce bias and improve model performance. This includes collecting data from different demographics, regions, time periods, etc.

Again, these techniques are not mentioned explicitly in the given context, but they are common approaches used to address data bias during preprocessing of the deep learning pipeline.
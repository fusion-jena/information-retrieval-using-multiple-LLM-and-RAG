The deep learning model used in this study employs several hyperparameters for efficient optimization. The exponential decay rates of first-moment and second-moment estimations are set to 0.9 and 0.999, respectively. A positive scalar value for epsilon is also defined as 1e-08. Furthermore, fivefold cross-validation and early stopping techniques are utilized to prevent overfitting during hyperparameter optimization. The early stopping criterion is based on the model's performance when it ceases to show improvement.

In terms of specific hyperparameters related to the training process, the model uses the mini-batch gradient descent algorithm with a batch size of 32. The learning rate is kept constant at 0.0001 throughout the training phase. Additionally, the Adam optimizer is applied to fine-tune the model, incorporating the previously mentioned exponential decay rates and epsilon values.
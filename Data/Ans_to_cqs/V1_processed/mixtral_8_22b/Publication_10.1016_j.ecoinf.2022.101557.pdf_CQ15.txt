ADAM optimisation with a learning rate of 0.0001 is used to find optimal weights in the neural network in order to minimize the cross entropy loss between the networkâ€™s predictions and the pre-labelled data. Additionally, dropout with a rate of 0.3 is used during the training to make it more stable.
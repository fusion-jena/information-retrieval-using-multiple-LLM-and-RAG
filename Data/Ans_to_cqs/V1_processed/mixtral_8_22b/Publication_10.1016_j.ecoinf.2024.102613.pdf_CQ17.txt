The query asks about regularization methods used to prevent overfitting in the deep learning pipeline. Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function during training. Two commonly used regularization methods in deep learning pipelines are Dropout and L2 regularization.

Dropout is a regularization technique where randomly selected neurons are ignored during training. This helps to reduce overfitting by preventing complex co-adaptations between neurons. During each iteration of training, some percentage of nodes are dropped out, forcing the network to learn redundant representations of the input data.

L2 regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the magnitude of all trainable parameters. This encourages smaller weights, reducing the complexity of the learned model and helping to prevent overfitting.

Other regularization methods include L1 regularization, which adds a penalty term proportional to the absolute value of the weights, and early stopping, which stops training before convergence based on validation set performance. These methods can be combined to achieve better results.

In summary, regularization methods such as Dropout and L2 regularization are used to prevent overfitting in deep learning pipelines. Dropout ignores randomly selected neurons during training, while L2 regularization adds a penalty term to the loss function proportional to the square of the magnitude of all trainable parameters. Other regularization methods include L1 regularization and early stopping.

Based on the given context, there is no direct information regarding the regularization methods used to prevent overfitting in the deep learning pipeline. Therefore, this answer provides general knowledge about regularization methods used in deep learning pipelines.
Before training a deep learning model, several preprocessing steps can be taken to improve its performance. These may include:

1. Data Cleaning: This involves removing or correcting any inconsistencies, errors, or missing values in the dataset. It also includes handling outliers which might affect the model's performance.

2. Normalization/Standardization: This step scales numerical input features to have zero mean and unit variance. This is important because many machine learning algorithms perform better when numerical input variables are scaled to a standard range.

3. Feature Selection: This process involves selecting a subset of relevant features for use in model construction. It helps reduce overfitting, improves accuracy by reducing noise and redundancy, and reduces training time.

4. Data Augmentation: In this step, new data is created from existing data using techniques such as cropping, padding, zooming, etc. This technique is particularly useful in image classification tasks where there is limited labeled data available.

5. Splitting Dataset into Training and Test Sets: Before training the model, the dataset is usually split into a training set and a test set. The training set is used to train the model while the test set is used to validate the trained model.

However, please note that not all these steps are always required. The choice of preprocessing steps depends on the nature of the problem, the type of data, and the requirements of the specific deep learning algorithm being used.
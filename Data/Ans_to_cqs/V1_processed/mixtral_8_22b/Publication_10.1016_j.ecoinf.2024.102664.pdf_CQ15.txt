Three optimization techniques are applied in the deep learning pipeline: adam, sgdm, and RMSProp. These techniques are used as solvers for training networks in the deep learning models. The table shows the Root Mean Square Error (RMSE) values for each optimization technique when used with different input time series lengths and deep learning models such as Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), and CNN-LSTM. Based on the analysis, it was found that Adam had better performance compared to other optimizers, particularly when the input time series length was set to 5 days. Therefore, Adam was adopted as the solver for training the network.
To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the data set was randomly split into two sets, with 80% allocated for model training and 20% for model testing. This allowed for the computation of the gradient and updating of the network parameters during training, while also enabling the testing of the model error validation. Secondly, an early stopping technique was employed to prevent overfitting, which can hinder the model's ability to generalize to new situations. Overfitting occurs when the model becomes too complex and starts fitting the noise in the training data instead of the underlying pattern. By monitoring the error on the validation set, the training process could be stopped before overfitting occurred. Lastly, a trial-and-error method was used to determine the appropriate number of neurons and learning rate. This involved varying the number of nodes in the hidden layer and the learning rate and calculating the mean square error between the model output and the measured data for each configuration. The optimal number of neurons in the hidden layer and the optimal learning rate were then selected based on the minimum mean square error. These measures helped to ensure that the deep learning model was able to generalize well to new data.
During training, the model performance was monitored by minimizing the cost function using the Adam optimizer. The initial learning rate was set to 0.0001, and momentum was set to 0.9. To improve convergence during training, a learning rate decay procedure was implemented. This procedure followed the equation μ0 / (1 + αp)^β, where μ0 is the initial learning rate, p is the current training epoch divided by the total number of epochs, and α and β were set to 10 and 0.75, respectively. Additionally, early stopping was employed to prevent overfitting, with a patience parameter of 10 controlling the number of epochs without improvement in the validation loss.
Regularization is a technique used in machine learning and deep learning to prevent overfitting by adding a penalty term to the loss function during optimization. Two common regularization methods used in deep learning pipelines are Dropout and L2 Regularization.

Dropout is a regularization method where randomly selected neurons are ignored or "dropped out" during training, which helps to prevent complex co-adaptations on training data. This results in simpler networks with better generalization performance. The rate of dropout can be adjusted as a hyperparameter.

L2 Regularization, also known as weight decay, adds a penalty term proportional to the square of the magnitude of the weights to the loss function. This encourages smaller weights and prevents any single weight from dominating the output. It can be implemented using the Ridge Regression algorithm.

Other regularization techniques include L1 Regularization (also known as Lasso Regression), Early Stopping, and Data Augmentation. These methods help improve the model's ability to generalize well to unseen data and reduce overfitting.

Based on the given context, there is no specific mention of regularization methods used in the study. Therefore, I cannot provide information about the exact regularization methods employed in their deep learning pipeline.
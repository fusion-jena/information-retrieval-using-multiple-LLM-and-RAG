In the given context, it is mentioned that the model was trained using the Adam optimizer. The Adam optimizer is a popular choice for stochastic gradient descent methods used in machine learning models. It combines the advantages of two other extensions of stochastic gradient descent, Adagrad and RMSProp, to provide an optimization algorithm that can handle sparse gradients on noisy problems. The learning rate was initially set at 0.001 and was subsequently reduced by a factor of 0.1 in a step-wise manner whenever the validation loss remained constant for five epochs. The minimum learning rate was established at 1 Ã— 10^-5. These settings indicate that the learning rate scheduler was implemented to adjust the learning rate during training dynamically. Additionally, weight decay was also employed as a regularization technique to prevent overfitting.
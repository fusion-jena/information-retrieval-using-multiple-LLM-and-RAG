The experimental setup and training strategy mentioned in the given context involve conducting experiments on a Linux machine equipped with Ubuntu 20.04, utilizing an Intel(R) Xeon(R) CPU E5â€“2650 v4 @ 2.20GHz processor. The codes were implemented in Python 3.8, and all models were constructed using PyTorch. Each basic model was compactly trained on a single TITAN Xp 12G GPU for evaluation. Standard evaluation metrics such as accuracy, macro-precision, macro-recall, and macro-F1 score were employed to assess the performance of the models. Following the guidelines provided by Wu et al. (2019), the IP102 dataset was partitioned into training, validation, and test sets with sample sizes of 45,095, 7508, and 22,619, respectively. Importantly, the parameters of the basic models were exclusively updated using the training set, while the weights of the ensemble methods were calculated on the validation set. This indicates that the validation set was used to monitor the model performance during training.
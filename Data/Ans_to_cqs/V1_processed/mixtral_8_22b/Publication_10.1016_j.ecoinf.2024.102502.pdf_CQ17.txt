Unfortunately, there is no mention of any regularization methods used to prevent overfitting in the deep learning pipeline in the given context. Regularization methods like dropout or L2 regularization are techniques used to prevent overfitting in machine learning models, especially neural networks. Dropout is a technique where randomly selected neurons are ignored during training, while L2 regularization adds a penalty term to the loss function to discourage large weights. However, without further context, I cannot provide more details about the use of these methods in the deep learning pipeline.
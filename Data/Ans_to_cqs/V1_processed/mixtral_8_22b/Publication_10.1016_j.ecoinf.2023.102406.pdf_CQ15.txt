The given context does not provide information about specific optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, it mentions that for the selected classifier, modeling package options enable automated tuning for parameters through resampling and standard default values to reach the most accurate final model configuration. This could imply the use of optimization techniques like grid search or randomized search for hyperparameter tuning, but no explicit mention of SGD or Adam was found in the provided context.
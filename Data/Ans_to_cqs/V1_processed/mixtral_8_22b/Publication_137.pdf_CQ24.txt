Based on the given context, there is no explicit mention of strategies employed to handle randomness in the deep learning pipeline, such as managing random seed values. However, some implicit strategies can be inferred. For instance, the authors use a fixed voxel size and batch size during their experiments, which could help reduce variability caused by random initialization or sampling. Additionally, they experiment with different regularization strengths, finding that a regularization strength of 0.2 yields the best results. This suggests that tuning hyperparameters can mitigate the effects of randomness in the model. Lastly, using a larger training batch size of 8 seems to improve efficiency and segmentation accuracy, possibly due to the network obtaining a sharper minimum instead of a flatter one. Overall, while not explicitly stated, the authors seem to manage randomness through careful selection and tuning of various parameters and settings in their deep learning pipeline.
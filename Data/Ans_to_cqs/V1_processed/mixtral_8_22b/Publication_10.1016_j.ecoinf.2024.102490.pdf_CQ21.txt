After the model training process, several postprocessing steps can be taken to further analyze and understand the performance of the trained models. Based on the provided context, two main postprocessing steps were performed: evaluating the accuracy of the models using specific metrics and applying ensemble methods to improve the robustness of the results.

To evaluate the accuracy of their models, the authors used two metrics: the Area Under the Curve (AUC) of the Receiver Operating Characteristics (ROC) and True Skill Statistics (TSS). These metrics help quantify the predictive power of the models and provide insights into how well they perform in classifying or predicting outcomes. The ROC curve is a graphical representation of the contrast between sensitivity (true positive rate) and specificity (false positive rate) across various decision thresholds, while the AUC summarizes this information into a single value representing overall model performance. The TSS metric combines both sensitivity and specificity to give a more balanced measure of model performance.

Additionally, the authors applied committee averaging and weighted mean approaches to create separate ensemble models for each climate scenario and period. Ensemble methods combine multiple base models to produce improved results compared to individual models. Committee averaging involves taking the average prediction of all base models, while the weighted mean approach assigns different weights to each base model based on its performance. By creating these ensembles, the authors aimed to increase the robustness of their results and reduce overfitting.

When selecting the final ensemble models, special attention was given to those with a TSS score of â‰¥0.7. This indicates that the chosen models had high predictive capabilities and could accurately capture the patterns within the dataset. Overall, these postprocessing steps helped the authors better understand the performance of their models and select the most accurate ones for further analysis.
The completion of training is determined by monitoring the performance of the model on a separate validation set. Specifically, the authors use the validation set accuracy as an indicator of whether the model has learned useful representations. When the validation set accuracy stops improving, they consider the training process to be complete. Additionally, they also adjust the learning rate during training. Initially, the learning rate is set to 10^-3, and then it is decreased by a factor of 10 when the validation set accuracy stops improving. This approach allows them to fine-tune the model and prevent overfitting. Furthermore, they apply regularization techniques such as weight decay and dropout to further improve generalization. Overall, these strategies help ensure that the model learns robust and meaningful representations without overfitting to the training data.
The deep learning pipeline uses both audio and image data formats. Initially, sound data is represented as amplitude-time waveforms, which can be trained using 1-dimensional neural network architectures like Recurrent Neural Networks (RNNs) to identify distinct patterns. Additionally, Convolutional Neural Networks (CNNs) are employed to classify 2D image spectrograms, specifically mel-spectrograms derived from the sound data. These mel-spectrograms serve as input for various 2D CNN architectures, such as ResNet, VGGish, and MobileNet. In this process, well-labeled training and validation data from the study domain are utilized to optimize weights in existing image-based architectures through transfer learning. However, acquiring large amounts of labeled data within the study domain can be challenging, leading to the incorporation of larger labeled data from external sources. For instance, recent BirdCLEF competitions employ weakly-labeled audio data from outside the study domain for training purposes.
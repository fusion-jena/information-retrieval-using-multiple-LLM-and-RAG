The information provided does not contain details about a specific machine learning project or monitoring strategies for model performance during training. Therefore, I cannot provide a direct answer to this query based on the given context. However, common strategies used to monitor model performance during training include tracking metrics such as accuracy, loss, precision, recall, F1 score, ROC curves, confusion matrices, etc. Cross-validation techniques like k-fold cross validation can also be employed to evaluate the model's performance across different subsets of the dataset. Additionally, visualization tools like TensorBoard or matplotlib can help plot these metrics over time to observe trends and identify potential issues like underfitting or overfitting.
Based on the given context, it appears that the model's performance is evaluated using two main criteria: the Area Under the receiver-operator Curve (AUC) and the True Skill Statistics (TSS). The AUC values range from 0 to 1, where values closer to 1 indicate a higher accuracy of model prediction. On the other hand, TSS values range between -1 and +1, where a TSS value of +1 means complete agreement between observed and predicted distributions, while values of â‰¤0 denote no better than random performance. Additionally, the Akaike Information Criterion corrected for small sample sizes (AICc) is also mentioned as a criterion for selecting the best model among a set of candidate models. However, none of these criteria directly address the completion of training or validation loss plateaus. Therefore, based solely on the provided context, I cannot provide a specific answer regarding the criteria used to determine when training is complete.
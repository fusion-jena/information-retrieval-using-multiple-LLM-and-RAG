The authors do not explicitly mention any specific techniques such as diverse datasets, cross-validation, or stratified splitting to ensure the generalizability of their deep learning model. However, they did use different architectures (resnet18 vs. resnet50), trained the model for varying numbers of epochs (2 vs. 20), and tested the model on both smaller and larger datasets. Additionally, they saved the model after each epoch for later use, which could potentially help improve the model's performance over time. The authors also cited relevant literature regarding transferable features in deep neural networks, suggesting that they considered the importance of generalization when developing their model.
Unfortunately, the given context does not explicitly mention any specific optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD), Adam, etc. However, it can be inferred from the context that some sort of optimization technique was employed during the training process of the fine-tuned neural network.

In the context, it is mentioned that the fine-tuned neural network was trained with a batch size of 100 for 20,000 steps. This suggests that some form of gradient descent optimization algorithm was likely used, as these algorithms typically involve iteratively updating the weights of the neural network based on the gradients calculated from batches of data. Additionally, dropout regularization was also utilized to prevent overfitting, which further indicates that some form of optimization was being performed.

However, without explicit information regarding the specific optimization technique used, we cannot definitively state whether SGD, Adam, or another optimization method was implemented in the deep learning pipeline.
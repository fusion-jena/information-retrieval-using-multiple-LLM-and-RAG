The optimization technique used in the deep learning pipeline mentioned in the given context is Adam. Adam is a popular optimization algorithm used in training neural networks due to its ability to adaptively adjust the learning rate based on the first and second moments of gradients. This makes it more efficient and effective compared to other optimization algorithms such as Stochastic Gradient Descent (SGD), which uses a fixed learning rate throughout the training process. In this case, Adam was chosen as the optimizer with different learning rates depending on the specific model being trained. For example, one model had a learning rate of 0.000455 while another had a learning rate of 0.0015. Overall, using Adam as the optimization technique helped improve the performance of the deep learning pipeline in predicting streamflow in Bear and Winter Rivers of PEI.
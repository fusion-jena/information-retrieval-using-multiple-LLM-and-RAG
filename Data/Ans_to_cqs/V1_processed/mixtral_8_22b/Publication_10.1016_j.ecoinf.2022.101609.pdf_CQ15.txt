In the given context, several optimization techniques were explored for use in the deep learning pipeline. These include Stochastic Gradient Descent (SGD), Root Mean Square Propagation (RMSProp), Lamb Optimizer (LAMB), Large Batch Optimizer (LARS), and Adaptive Moment Estimation (Adam). After conducting experiments with these popular optimization functions, Adam optimizer was selected for model training. Additionally, Bayesian optimization was employed to fine-tune the number of architecture layers, the size of each layer, and dropout rates for the network.
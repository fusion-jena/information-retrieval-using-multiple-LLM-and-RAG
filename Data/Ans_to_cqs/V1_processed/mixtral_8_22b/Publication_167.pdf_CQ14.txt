Based on the given information, the hyperparameters of the model are optimized using the validation set through the process of 10-fold cross-validation. In this method, the dataset is split into ten equal subsets or folds. Nine of these folds are used for training the model, while the remaining fold is used for validating the model. This process is repeated ten times, each time with a different fold serving as the validation set. The average performance across all ten trials is then calculated to produce a single estimation.

However, the specific methods for optimizing the hyperparameters, such as grid search or random search, are not explicitly mentioned in the provided context. Grid search involves an exhaustive search through a manually specified subset of a hyperparameter space of a learning algorithm, whereas random search tries random combinations within the defined ranges. Without further details, we cannot definitively state whether either of these methods was employed.
Based on the given context, it appears that the authors employ several strategies to handle randomness in their deep learning pipeline. One such strategy is using a search space of hyperparameters, which includes parameters like filters, filter size, number of neurons, batch size, epochs, and CNN model optimizers. By exploring this search space, they aim to find the optimal set of hyperparameters that yield the highest accuracy.

Additionally, the authors use data augmentation techniques to ensure that they cover all possible cases that may occur in the real world. These techniques include random rotation, random cropping, random flipping, random color jittering, and random noise addition. By applying these techniques during the learning phase of the CNN, they increase the diversity of the training data and reduce overfitting.

However, there is no explicit mention of how the authors handle randomness related to the initial seed value or other sources of randomness in the deep learning pipeline. It is common practice in deep learning to fix the seed value to ensure reproducibility of experiments, but this aspect is not discussed in the provided context.
The text provides information about handling randomness in the deep learning pipeline through the use of Random Year (RY) data splitting. In this approach, individual observations are grouped according to the year of capture and then randomly selected for inclusion in either the training or testing dataset. This strategy helps prevent systematic differences between the test and train datasets, ensuring that the model is tested with previously unseen combinations of explanatory variables. By doing so, it becomes a more robust test of model performance in this context.

Additionally, hyperparameter tuning is performed manually to further optimize the model. The learning rate, also known as the shrinkage parameter, is set as low as possible, while the number of iterations is set as high as computationally feasible. These settings help determine the contribution of each tree to the model and maintain a low degree of interaction within the model. Gradient Boosting Regression Tree (GBRT), which works well with shallow trees, is chosen due to its flexible properties and ability to identify potential drivers of changes in growth across populations without specifying the relationship between the response and explanatory variables.
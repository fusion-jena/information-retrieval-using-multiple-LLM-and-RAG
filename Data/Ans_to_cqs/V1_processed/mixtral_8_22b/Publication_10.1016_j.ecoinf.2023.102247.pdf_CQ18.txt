The authors employed cross-validation by using 70% of the dataset as training data to build the models and the remaining 30% as testing data to calculate the models' accuracy. They used the Area Under the Receiving Operator Characteristics Curve (AUC) as the index to measure the accuracy of their models. A higher AUC value indicates that the model is more accurate. This means that they monitored the model performance during training by evaluating its ability to accurately classify or predict the target variable based on the input features. By splitting the dataset into training and testing sets, they could assess how well the model generalizes to new, unseen data, which helps prevent overfitting and ensures that the model can perform well when applied to real-world scenarios.
Based on the given information, it appears that the strategy implemented to monitor the model performance during training involves conducting a series of ablation experiments using different network models. Specifically, ten ablation experiments were carried out, each utilizing a unique network model. The first experiment used the original YOLOv8 model, while the subsequent nine experiments employed models with varying structures based on YOLOv8.

To ensure consistency in evaluating these models, several parameters were standardized across all experiments. For instance, a batch size of 16 was chosen, and the input image resolution was fixed at 640 x 640 pixels. Furthermore, the total number of epochs was set to 150, and the learning rate was maintained at 0.01 throughout the training process. The optimization algorithm used was Adam, and two additional parameters - depth_scale and width_scale - were assigned values of 0.33 and 0.25 respectively.

However, the exact methodology used to track and compare the performance of these models during training remains unclear without further details. Typically, such strategies might involve monitoring metrics like loss or accuracy over time, visualizing model predictions, or analyzing confusion matrices. Given the available information, we can infer that the researchers likely utilized similar techniques to assess their models' progress during training. However, the precise approach adopted here cannot be definitively determined based solely on the provided context.
Based on the given context, it does not explicitly state the specific criteria used to determine when training is complete. However, there are some indications that suggest possible methods for determining completion.

Firstly, the text mentions that loss and accuracy development during training of models are reported in Figures S2 and S3. This suggests that monitoring the loss and accuracy metrics could be used to assess the progress of training. Typically, if these values reach a stable level or plateau, it may indicate that further training would not improve performance significantly, suggesting that training could be considered complete.

Secondly, the context discusses the use of a scaled confusion matrix to visualize classification errors. This implies that evaluating the performance of the trained model on a separate validation dataset could also serve as a criterion for determining completion. Once the model achieves satisfactory performance on the validation dataset, training could be deemed complete.

Lastly, the mention of optimization of model parameters in the shared part of the network to minimize the loss of the label classifier and maximize the loss of the domain classifier suggests that convergence of these losses might also be used as a stopping criterion. When the losses no longer decrease significantly, it could signal that further training would not yield substantial improvements, indicating that training should conclude.
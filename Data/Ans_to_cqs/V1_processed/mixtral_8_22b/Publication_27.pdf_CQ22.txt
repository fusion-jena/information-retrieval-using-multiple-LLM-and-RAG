The performance of a deep learning model can be evaluated using various metrics depending on the specific task and requirements. Some commonly used evaluation metrics include:

1. Accuracy: This is the ratio of correct predictions made by the model to the total number of input samples. It measures how often the classifier makes the right prediction. However, it may not be suitable for imbalanced datasets where one class has significantly more instances than others.

2. Precision: Also known as Positive Predictive Value, this metric calculates the proportion of true positive predictions out of all predicted positives. High precision indicates low false-positive rate.

3. Recall or Sensitivity: This metric computes the fraction of relevant instances that were retrieved out of all relevant instances present in the dataset. A high recall value means fewer false negatives.

4. F1 Score: The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both properties. It ranges between 0 and 1, with higher values indicating better performance.

5. Area Under Curve (AUC): This metric represents the area under the Receiver Operating Characteristic (ROC) curve, which plots True Positive Rate against False Positive Rate. AUC provides an aggregate measure of performance across all possible classification thresholds.

6. Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and R^2 Score: These metrics are typically used for regression tasks to quantify the difference between actual and predicted values. Lower MAE/RMSE and higher R^2 scores indicate better model performance.
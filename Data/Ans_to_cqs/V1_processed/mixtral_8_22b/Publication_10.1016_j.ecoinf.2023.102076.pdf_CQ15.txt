The deep learning pipeline uses Adam optimization during training. This optimization technique is applied to fine-tune the entire architecture, including the base model and the linear classification layer. Adam optimization is chosen due to its superior performance compared to other methods like stochastic gradient descent (SGD). Additionally, classification dropout of 0.3 is introduced to increase model generalizability.
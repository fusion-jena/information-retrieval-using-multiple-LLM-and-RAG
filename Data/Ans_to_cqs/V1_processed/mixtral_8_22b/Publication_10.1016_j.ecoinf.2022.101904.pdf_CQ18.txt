Based on the given context, it appears there isn't explicit information regarding the specific strategy used to monitor the model performance during training. However, some insights can still be derived from the available text.

In Section 3.4, the authors discuss how they evaluated their proposed model with varying amounts of training data to determine the optimal quantity needed to achieve satisfactory wetland mapping accuracy. They started with a 10% training dataset and incrementally increased it to 90%, observing improvements in classification accuracy as more data were added. Notably, the most significant increase occurred between the 10% and 30% ratios, suggesting that a larger dataset may not always yield proportionate benefits.

While this does not directly address the monitoring strategy, it implies that the researchers might have been tracking the model's performance throughout the training process by assessing its accuracy with various training datasets.

Additionally, in Section 3.6, the authors provide details about the computational resources used for their experiments and the time taken for training different models. Although this doesn't explicitly mention any monitoring strategies, it suggests that the researchers could have monitored the progress of their models based on the time taken for training or other relevant metrics.

Unfortunately, without further context or direct statements from the authors, it remains unclear what precise methodology was adopted to track the model performance during training.
The given context does not provide specific information on the optimization techniques used in the deep learning pipeline. However, it mentions that hyper-parameter tuning was performed via GridSearchCV with cross-validation (cv = 10). This suggests that the authors may have used grid search for hyperparameter optimization. Grid search involves defining a range of values for each hyperparameter and then exhaustively searching through all possible combinations of those parameters to find the best one. Cross-validation is a technique used to evaluate the performance of a model by dividing the dataset into several subsets and iteratively fitting the model on one subset while evaluating it on another. In this case, cv = 10 means that the dataset was divided into ten folds, and the model was trained and evaluated on each fold separately.

Regarding optimization techniques specifically used in deep learning pipelines, Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam) are commonly used optimization algorithms. SGD updates the weights of the network based on the gradient of the loss function with respect to the current weight values, while Adam combines the advantages of two other optimization methods, namely Root Mean Square Propagation (RMSProp) and momentum. These optimization techniques help minimize the error between predicted and actual outputs, thereby improving the accuracy of the model.
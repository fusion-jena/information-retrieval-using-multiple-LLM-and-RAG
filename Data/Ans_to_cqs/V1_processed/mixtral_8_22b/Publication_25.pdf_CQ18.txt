Based on the provided information, there isn't explicit mention of a specific strategy implemented to monitor the model performance during training. However, we can infer some strategies based on the context.

Firstly, the use of both self-supervised learning and hand-annotated data suggests that the model's performance was monitored during training through the comparison of these two methods. By comparing the results obtained solely from the hand-annotated data and those from the self-supervised model, researchers could gauge the effectiveness of each method and their combined impact on the model's performance.

Secondly, the text mentions the improvement of the model with a combination of better validation data and more hand-annotated training samples. This implies that the model's performance was also monitored through the use of validation data, which is a common practice in machine learning to evaluate how well a model is performing during training.

Lastly, the discussion about the full model's performance on different types of canopy complexity indicates that the model's performance was evaluated post-training on various scenarios. This evaluation would have allowed researchers to understand the strengths and weaknesses of the model, thereby monitoring its overall performance.
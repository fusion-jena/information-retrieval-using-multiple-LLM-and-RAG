The study employs various strategies to monitor the model performance during training across different machine learning algorithms. For instance, in the case of Artificial Neural Networks (ANN) classification, the authors use the 'nnet' package, which allows them to adjust weighted decay and size to counteract the effects of model overfitting. They also employ a Multilayer Perceptron (MLP) ANN architecture with one hidden layer and six neurons defined for their model inputs. The number of neurons in the input layer equals the number of used bands (six), while the output layer has eight neurons representing eight land use/land cover (LULC) classes. During the training phase, they utilize a backpropagation learning algorithm. The primary model tuning parameters are defined using size and decay, and the control() function is employed to manage model runs. Similarly, for Random Forest (RF) classification, the authors use the 'randomForest' package. They allow the model to set the number of trees (ntree) and the number of features in each split (mtry) by default to ensure satisfactory model performance. This results in approximately 500 decision trees created by the model under default settings, with more than 3000 training samples randomly selected for training purposes under default settings.
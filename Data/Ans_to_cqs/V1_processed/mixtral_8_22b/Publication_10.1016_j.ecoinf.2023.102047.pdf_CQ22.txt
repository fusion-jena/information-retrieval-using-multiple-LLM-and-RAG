In the field of image classification or object detection, the most common performance evaluation metrics are average precision and recall. Specifically, in this study, the authors use mean average precision (mAP) as the primary metric. This metric comprises three components: precision, recall, and mean.

Precision refers to the ratio of correctly classified positive samples (True Positives) to the total number of classified positive samples (either incorrectly or correctly). It can be calculated using the formula:

Precision = TP / (TP + FP)

where TP stands for True Positives and FP represents False Positives.

On the other hand, the recall rate refers to the ratio between the number of positive samples (True Positives) correctly classified as Positive to the total number of Positive samples. Recall can be computed using the following equation:

Recall = TP / (TP + FN)

Here, TP denotes True Positives, while FN signifies False Negatives.

Additionally, the authors mention another metric called mean Intersection over Union (mIoU), which was employed by Weidmann et al. (2019) to compare different models. However, it appears that mAP remains the main metric used throughout the study.
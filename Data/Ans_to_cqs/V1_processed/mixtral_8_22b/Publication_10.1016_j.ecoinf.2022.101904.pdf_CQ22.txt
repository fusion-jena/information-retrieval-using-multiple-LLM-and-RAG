The paper uses three main evaluation metrics to assess the performance of the deep learning models: Kappa Index (KI), Overall Accuracy (OA), and Average Accuracy (AA). These metrics are presented in Table 5 for each class and model. Additionally, the table includes Time (h) as another metric, which measures the computational cost of each model.

The Kappa Index is a statistical measure used to determine inter-rater reliability or agreement between two raters who categorize items into mutually exclusive categories. In this case, it is used to compare the agreement between the predicted classes and the actual classes. A higher value indicates better agreement.

Overall Accuracy represents the proportion of correctly classified instances out of all instances. It provides a general idea of how well the model performs across all classes.

Average Accuracy calculates the mean accuracy for each class separately and then averages them together. This metric helps identify if there are any biases towards certain classes in the model's predictions.

Finally, Time (h) measures the computational cost of running each model. Lower values indicate faster processing times and more efficient models.
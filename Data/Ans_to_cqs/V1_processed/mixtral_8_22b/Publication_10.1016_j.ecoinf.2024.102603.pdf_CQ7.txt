The given context primarily discusses the differences between XGBoost and deep learning models but does not explicitly mention the criteria used to split the data for deep learning model training. However, it is common practice in machine learning to split the dataset into three parts: training set, validation set, and testing set.

For deep learning models, the criteria used to split the data usually depend on factors like the size of the dataset, class balance, and the nature of the problem being solved. Commonly, the dataset is randomly split into these subsets, with the most popular ratios being 80:10:10 or 70:15:15 for training, validation, and testing respectively. This ensures that the model has enough examples to learn from during training, while also providing separate sets for tuning hyperparameters (validation set) and evaluating final performance (testing set).

However, if the dataset is small or imbalanced, other techniques like stratified sampling or cross-validation may be employed to ensure that all classes are adequately represented in each subset. It should be noted that the context does not provide information about the specific criteria used for splitting the data for deep learning models.
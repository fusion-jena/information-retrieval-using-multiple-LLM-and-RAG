During training, the model monitors its performance by optimizing cross-entropy loss via mini-batch stochastic gradient descent. A constant learning rate of 0.01 is used, and dropout is applied to the input of the last two layers with a probability of 0.5. L2-regularization is applied to the weights of the last two layers with a penalty factor of 0.001. The model is trained for 100 epochs and is checkpointed after each epoch. A validation set is used to identify the parameter setting (epoch) achieving the highest classification accuracy.
Based on the given information, there isn't a specific criterion mentioned for determining when the training is complete. However, several indicators suggest that the model might have reached its optimal state or close to it. For instance, the model improved swiftly in terms of precision, recall, and mean average precision before plateauing after around 1000 iterations. Additionally, the loss showed a rapid decline around the same point. These observations indicate that the model may have learned most of what it can from the training data. Furthermore, the model was trained for a total of 4000 iterations, which could imply that this was considered sufficient for achieving satisfactory results. Lastly, it's worth noting that when the number of epochs exceeded 200, the network seemed to become overfitted to the training data. This suggests that training beyond this point might lead to diminishing returns or even detrimental effects on the model's ability to generalize to new data. Therefore, while no explicit stopping criterion is stated, these factors collectively hint at potential points where one might decide to conclude the training process.
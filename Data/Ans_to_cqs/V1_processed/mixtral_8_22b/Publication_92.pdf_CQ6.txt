Before training a deep learning model, several preprocessing steps are typically involved. These steps aim to prepare the raw data into a suitable format for the model to learn effectively. While the specific preprocessing techniques depend on the nature of the dataset and the problem being solved, common methods include:

1. Normalization: This involves rescaling the values of the input features so they fall within a certain range, often between 0 and 1 or -1 and 1. Normalizing the data helps prevent large-scale features from dominating the learning process and improves convergence during optimization.

2. Scaling: Similar to normalization, scaling adjusts the range of input features, usually by standardizing them to have zero mean and unit variance. Scaling ensures that all features contribute equally to the learning process and prevents those with larger scales from having undue influence.

3. Cleaning: This step involves identifying and handling missing, invalid, or outlier data points. Cleaning might involve removing such instances, imputing missing values using statistical methods, or transforming extreme values to bring them closer to the rest of the distribution.

In the case of the provided context, the authors mention working with biology experts to label many videos and creating a training database composed of fish thumbnails. They also discuss keeping only the species with more than 450 thumbnails and widening the database by applying rotations and symmetries. Although these actions do not directly correspond to traditional preprocessing steps like normalization, scaling, or cleaning, they still represent essential data preparation activities aimed at improving the quality and diversity of the training set.
The given context does not provide information about the specific hardware resources used for training the deep learning models. It only mentions that CNN models were trained using each annotation set to study the impact of inter-annotator variability on the performance of deep learning models. The models were then evaluated on different datasets in various areas with different recording devices and multiple expert annotations. Consistent hyperparameters were maintained across all training instances for each task, such as setting the batch sizes and learning rates. However, there is no mention of the type of hardware resources used for training these models, like GPUs or TPUs.
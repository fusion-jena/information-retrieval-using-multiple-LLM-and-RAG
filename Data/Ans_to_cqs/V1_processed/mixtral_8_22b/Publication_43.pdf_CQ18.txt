Based on the given information, there is no explicit mention of a specific strategy implemented to monitor the model performance during training. However, it is common practice in machine learning to use techniques such as cross-validation or validation sets to evaluate the model's performance during training. These methods help prevent overfitting and ensure that the model generalizes well to unseen data. In addition, monitoring metrics like accuracy, precision, recall, F1 score, etc., can provide insights into the model's performance. It is important to note that the choice of evaluation metric depends on the problem statement and the dataset being used.
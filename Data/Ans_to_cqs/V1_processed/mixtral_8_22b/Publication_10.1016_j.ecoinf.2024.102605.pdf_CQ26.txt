There are several techniques used to address data bias during preprocessing of the deep learning pipeline. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting is a technique where the dataset is divided into training and testing sets while maintaining the same distribution of classes in both sets. This ensures that the model does not overfit to one class and can generalize well to unseen data.

Oversampling is a technique where synthetic samples are generated from the minority class to balance the dataset. This helps prevent the model from being biased towards the majority class. Synthetic Minority Over-sampling Technique (SMOTE) is a popular algorithm used for this purpose.

Undersampling is another technique used to balance datasets. It involves removing samples from the majority class until it matches the size of the minority class. Random under-sampling and NearMiss algorithms are commonly used for this purpose.

Diverse data collection is also important to reduce data bias. Collecting data from different sources, environments, and populations can help ensure that the model is robust and can perform well in various scenarios.

These techniques can be combined to achieve better results. For example, SMOTE can be used along with random under-sampling to create a balanced dataset. Similarly, collecting diverse data can complement other techniques such as stratified splitting and oversampling.
The given context does not directly provide information about the criteria used to split the data for deep learning model training (e.g., train, test, validation). However, I can explain some common practices used in splitting data for machine learning and deep learning models.

Data splitting is typically done based on the following sets:

1. Training Set: This is the main dataset used to train the model. It consists of input features and their corresponding target variables. The model learns patterns and relationships between these inputs and outputs during this phase.

2. Validation Set: Also known as the development or holdout set, this subset is used to fine-tune hyperparameters and prevent overfitting. During the training process, the performance of the model is evaluated on the validation set after each epoch or iteration. Based on the results, adjustments are made to improve the model's performance.

3. Test Set: After the model has been trained and optimized, its final evaluation is performed using the test set. This set contains unseen data that was not part of the training or validation processes. Evaluating the model on the test set provides insights into how well it will perform when deployed in real-world scenarios.

Common ratios for splitting data include 70/15/15 (training/validation/test) or 80/10/10. These ratios may vary depending on the specific requirements and constraints of the problem being addressed.
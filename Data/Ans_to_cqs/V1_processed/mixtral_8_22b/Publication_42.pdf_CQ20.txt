Unfortunately, there isn't any information available regarding the specific hardware resources used for training the deep learning models mentioned in the provided text. Therefore, I cannot provide a definitive answer about whether GPUs or TPUs were utilized during the training process. However, considering the complexity of the models discussed, including those with millions of learnable parameters like ResNetV50 and InceptionNetV3, it can be assumed that some form of accelerated computing hardware, such as GPUs or TPUs, might have been employed to expedite the training process.
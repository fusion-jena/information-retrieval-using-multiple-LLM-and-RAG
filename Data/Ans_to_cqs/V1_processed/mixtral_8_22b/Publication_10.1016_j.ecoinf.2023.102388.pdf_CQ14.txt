The text does not provide explicit details on how the hyperparameters were optimized, such as through methods like grid search or random search. However, it does mention two scenarios for training the model: one where measures are taken to prevent overfitting, and another where conditions are created to encourage overfitting.

In the scenario designed to avoid overfitting, the Stochastic Gradient Descent (SGD) optimization algorithm is employed with a learning rate of 0.0001 and a weight decay of 0.5. Other parameters include a batch size of 32, 200 training epochs, and the application of batch normalization and dropout techniques. These settings seem to be fixed rather than being the result of a systematic hyperparameter tuning process.

Similarly, in the scenario intended to induce overfitting, the same hyperparameters are used except for the removal of batch normalization, weight decay, and dropout techniques. Again, there's no indication of any specific method used to select these values; they appear to be predetermined.

Therefore, based on the given information, it cannot be definitively stated how the hyperparameters were optimized. It appears that the hyperparameters were chosen beforehand without specifying whether methods like grid search or random search were utilized.
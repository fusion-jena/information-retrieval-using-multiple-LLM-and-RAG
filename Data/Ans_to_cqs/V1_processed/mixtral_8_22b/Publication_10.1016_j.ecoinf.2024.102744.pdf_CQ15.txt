The given context does not provide explicit information about the specific optimization techniques used in the deep learning pipeline, such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, we can infer that the models mentioned in the text, like YOLOv5 and Faster RCNN, use some form of optimization technique during their training phase. These optimization algorithms help minimize the loss function and update the weights of the neural network accordingly. Without knowing the exact implementation details, it cannot be confirmed whether SGD, Adam, or any other optimization algorithm was employed.
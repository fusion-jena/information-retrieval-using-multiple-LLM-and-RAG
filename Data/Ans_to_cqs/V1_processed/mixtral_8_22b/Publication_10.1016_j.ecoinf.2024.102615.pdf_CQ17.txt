In the given context, the authors have mentioned the use of dropout layers as a regularization technique to prevent overfitting in their deep learning pipeline. Dropout is a commonly used regularization method in deep learning models, which randomly drops out or ignores some neurons during training to reduce co-dependencies between them. By doing so, it helps in preventing overfitting and improving the generalization capability of the model. Apart from dropout, other popular regularization techniques include L1 and L2 regularizations, early stopping, and weight decay. However, the context does not explicitly mention the use of any other regularization methods apart from dropout. Therefore, we can only confirm the use of dropout as a regularization method in the given context.
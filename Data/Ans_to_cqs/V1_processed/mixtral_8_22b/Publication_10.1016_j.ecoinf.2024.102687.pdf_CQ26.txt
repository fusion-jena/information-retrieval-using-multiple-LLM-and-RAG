Based on the given context, there isn't any direct mention of techniques used to address data bias during preprocessing of the deep learning pipeline. However, some relevant information can still be derived.

In section 2.2, it discusses the fine-tuning of a pretrained Convolutional Neural Network (CNN) backbone using classical meta-metric learning architectures adapted from EasyFSL. While not directly addressing data bias, this technique helps adapt the network to better suit the target dataset, potentially reducing biases introduced by training on a different dataset like ImageNet.

Section 3.3 mentions comparative analysis of CNN backbones to determine the optimal architecture. Four pretrained CNNs - ResNet18, VGG16, DenseNet121, and AlexNet - were tested, with DenseNet121 performing best overall. Using pretrained networks can help reduce potential biases caused by insufficient training data.

Although not explicitly mentioned, stratified splitting, oversampling, undersampling, and diverse data collection are common techniques used to address data bias during preprocessing. Stratified splitting ensures each subset maintains the original class distribution, while oversampling and undersampling balance classes by duplicating minority samples or removing majority ones, respectively. Collecting diverse data reduces biases due to limited representation.

To summarize, although the given context does not provide explicit details about techniques used to address data bias during preprocessing, methods like fine-tuning pretrained networks and selecting appropriate architectures can indirectly mitigate these issues. Common techniques include stratified splitting, oversampling, undersampling, and diverse data collection.
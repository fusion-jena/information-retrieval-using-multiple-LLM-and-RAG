The provided context does not directly address the preprocessing steps involved before training a deep learning model such as normalization, scaling, or cleaning. However, it does mention several relevant aspects related to preparing data for modeling.

In section 2.1, it discusses splitting the dataset into training and testing sets, which is a common preprocessing step in machine learning. In this case, 75% of the samples were used for training, and 25% were reserved for testing. This process was performed using the `createDataPartition` function in the `caret` package.

Section 4.5 briefly touches upon sample thinning, which can be considered a form of data preprocessing. Although it doesn't have a direct impact on normalization, scaling, or cleaning, sample thinning can help reduce redundancy in the dataset by removing similar instances.

Lastly, section 2.4 mentions variable reduction for correlation, which could involve techniques like Principal Component Analysis (PCA) or feature selection. These methods aim to remove highly correlated features or reduce their dimensionality, improving the overall performance of the model.

While these points do not cover all possible preprocessing steps for deep learning models, they highlight essential practices for preparing datasets for machine learning tasks.
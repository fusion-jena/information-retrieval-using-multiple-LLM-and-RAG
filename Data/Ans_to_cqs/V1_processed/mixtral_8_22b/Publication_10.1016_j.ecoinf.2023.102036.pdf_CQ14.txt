Hyperparameters of the model are optimized using Bayesian Optimization. Unlike grid search or random search, Bayesian Optimization doesn't require studying every possible combination of the parameters. Instead, it uses the information from previous evaluations to select the most promising set of parameters for the next evaluation. It achieves this by calculating the posterior predictive distribution for the function's value at each point. In this case, the parameters optimized include the learning rate, momentum, SD threshold, and input size. The specific values for these parameters after optimization can be found in Table 1.
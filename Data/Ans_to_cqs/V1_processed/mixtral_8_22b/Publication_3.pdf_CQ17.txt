The text does not provide information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization techniques include dropout, L1 regularization, and L2 regularization. Dropout randomly sets a fraction of input units to 0 during training, which helps prevent overfitting by reducing co-dependencies between neurons. L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients, while L2 regularization adds a penalty equal to the square of the magnitude of coefficients. These penalties discourage complex models and help prevent overfitting.
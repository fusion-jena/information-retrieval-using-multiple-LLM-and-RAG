Based on the given references, there isn't any explicit mention of specific criteria used to split the data for deep learning model training into train, test, and validation sets. However, generally, the common practice is to randomly divide the available dataset into three subsets: training, testing, and validation. Typically, around 60-80% of the data is allocated for training, while the remaining 20-40% is divided between testing and validation. This division helps ensure that the model does not overfit or underfit during the training process. In some cases, stratified sampling may also be employed to maintain class distribution across all subsets. Additionally, cross-validation techniques like k-fold cross-validation might be utilized to further improve the robustness of the model evaluation.
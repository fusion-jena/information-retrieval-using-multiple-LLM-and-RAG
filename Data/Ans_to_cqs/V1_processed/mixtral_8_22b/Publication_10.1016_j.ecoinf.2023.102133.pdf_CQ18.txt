Based on the given context, the strategy implemented to monitor the model performance during training involves the use of grid search technique for both Multilayer Perceptron (MLP) and Random Forest (RF) predictors. This technique is used to compute the optimum values of hyperparameters. For the MLP, the network is implemented with an input layer, three hidden layers, and an output layer. During the model selection phase, the performance obtained using different hyperparameters is compared. Specifically, the following hyperparameters are tested: solvers (Limited-Broyden–Fletcher–Goldfarb–Shanno, Adam, Stochastic Gradient Descent with constant learning rate, and Stochastic Gradient Descent with adaptive learning rate), number of neurons in the hidden layers (from 1 to 100), regularization factor "alpha" (L2 penalty), activation function (identity, logistic, tanh, ReLU), and learning rate (constant, invscaling, adaptive). Similarly, for the RF predictor, various hyperparameters such as the maximum depth of the tree, minimum number of samples required to split a node, minimum number of samples required at each leaf node, method of selecting samples for training each tree, and parameter for minimal cost-complexity pruning are considered. The best model is then selected by comparing the performance scores of all possible combinations of these hyperparameters.
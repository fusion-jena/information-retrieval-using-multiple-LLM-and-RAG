Before training a deep learning model, several preprocessing steps are typically involved to ensure the quality and usability of the input data. These steps may include:

1. Data Cleaning: This involves removing any irrelevant, incorrect, or duplicate data from the dataset. It also includes handling missing values, which could be done through various techniques such as imputation or deletion.

2. Normalization: This is a technique often applied as part of data preparation for machine learning. Normalization ensures that all features have the same scale, which is important because most machine learning algorithms measure the distance between two points. If one feature has a large range compared to another, its effect will dominate the others. Common normalization techniques include Min-Max scaling and Z-score standardization.

3. Scaling: Similar to normalization, scaling is used to bring all features to a comparable level. However, while normalization rescales the values into a range of 0 to 1, scaling usually transforms the data without restricting it to a predefined range. For example, StandardScaler in sklearn library scales the data to unit variance.

4. Feature Selection: Not all features contribute equally to predicting the target variable. Some might even add noise and decrease performance. Therefore, selecting relevant features is crucial. Techniques like correlation matrix, mutual information, or using models themselves (like tree-based estimators) can help identify significant features.

5. Encoding Categorical Variables: Most machine learning algorithms require numerical inputs. Thus, categorical variables need to be converted into numbers. One popular method is one-hot encoding, where each category becomes a new binary column.

6. Splitting Dataset: The dataset should ideally be split into three parts: training set, validation set, and test set. The training set trains the model, the validation set tunes hyperparameters, and the test set evaluates the final model.
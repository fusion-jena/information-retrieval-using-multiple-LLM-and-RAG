The hyperparameters of the model are optimized using Bayesian optimization. This technique employs a Gaussian process model of the objective function. In this study, the network was optimized for batch size (between 1 and 32) and initial learning rate (between 1x10^-4 and 1x10^-2). The optimization was performed by minimizing the loss on the validation dataset to perform objective function evaluations. The models were trained for a maximum of 10 epochs using the stochastic gradient descent with a momentum ('sgdm') optimizer.
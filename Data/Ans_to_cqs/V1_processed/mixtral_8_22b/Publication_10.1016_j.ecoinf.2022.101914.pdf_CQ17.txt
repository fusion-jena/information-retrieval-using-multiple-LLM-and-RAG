The text does not provide specific information about regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions two common regularization techniques: dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the network more robust to small changes in input features. L2 regularization, also known as weight decay, adds a penalty term to the loss function based on the sum of squared weights, encouraging smaller weights and reducing overfitting. Other regularization methods include early stopping, data augmentation, and batch normalization. These techniques help improve generalization and reduce overfitting in deep learning models.
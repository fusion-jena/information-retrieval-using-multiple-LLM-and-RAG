Unfortunately, the provided context does not explicitly mention the criteria used to split the data for deep learning model training, such as splitting the data into training, testing, and validation sets. However, I can provide some general information about this topic.

In machine learning and deep learning, it is common practice to divide the available dataset into three subsets: training set, validation set, and test set. This division allows us to evaluate the performance of our model more accurately and prevent overfitting. Here are some commonly used criteria for splitting the data:

1. Random sampling: In this method, we randomly select samples from the entire dataset to create the training, validation, and test sets. This ensures that each subset has a similar distribution of classes or labels.

2. Stratified sampling: When dealing with imbalanced datasets, where certain classes have significantly fewer examples compared to others, stratified sampling can help maintain class proportions in each subset. This involves dividing the data based on the proportion of each class present in the original dataset.

3. Time series split: For time-dependent data, such as stock prices or weather forecasting, it is essential to preserve temporal dependencies when creating the subsets. One approach is to use a sliding window technique, where the initial window contains only training data, followed by windows containing both training and validation data, and finally, a window containing only test data.

4. Cross-validation: To ensure robustness and reduce bias, cross-validation techniques like K-fold cross-validation can be employed. In this method, the dataset is divided into 'K' folds, and the model is trained and validated 'K' times, each time using a different fold as the validation set while keeping the rest as the training set. The average performance across all iterations provides a reliable estimate of the model's true performance.
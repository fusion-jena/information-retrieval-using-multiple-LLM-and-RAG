The given context does not provide specific information about the regularization methods used in the deep learning pipeline to prevent overfitting. However, based on general knowledge, there are several common regularization methods used in deep learning models to prevent overfitting. These include:

1. Dropout: This method randomly drops out neurons during training, effectively making the layer thinner. It helps to reduce overfitting by preventing complex co-adaptations on training data.

2. L2 Regularization: Also known as weight decay, this method adds a penalty term to the loss function proportional to the square of the magnitude of all weights in the model. This encourages smaller weights, leading to simpler models that generalize better.

3. Early Stopping: This method involves monitoring the validation error while training the model. When the validation error starts increasing, it indicates that the model is starting to overfit, so we stop training.

4. Data Augmentation: This technique increases the amount of training data available by applying random transformations to existing examples, such as rotation, scaling, or flipping. This can help improve the model's ability to generalize.

5. Batch Normalization: This method normalizes the activations of each layer, reducing internal covariate shift and improving the speed and stability of the training process. It also has a slight regularizing effect, similar to dropout.
The given context does not provide information on the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization techniques include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to reduce overfitting by preventing complex co-adaptations on training data. On the other hand, L2 regularization adds a penalty term to the loss function, encouraging smaller weights and reducing model complexity. Other regularization methods include early stopping, weight decay, and batch normalization. It is important to note that the choice of regularization method depends on various factors such as the size and nature of the dataset, the architecture of the neural network, and the desired trade-off between bias and variance.
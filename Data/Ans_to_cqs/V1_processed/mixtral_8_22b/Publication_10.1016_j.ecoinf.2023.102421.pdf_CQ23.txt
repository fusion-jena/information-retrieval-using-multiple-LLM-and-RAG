To ensure the generalizability of their deep learning model, several measures were implemented. Firstly, the authors utilized a diverse dataset consisting of 268 sample plots with 39 features in a tabulated format. This diversity in the dataset helped capture a wide range of variations within the data, thereby improving the model's ability to generalize well to new, unseen data.

Secondly, the authors employed a technique called stratified splitting to divide the training dataset into two parts. Eighty percent of the data was allocated for training and validation purposes, while the remaining twenty percent was reserved exclusively for testing. By employing this strategy, the authors aimed to maintain a balance between the classes present in both the training and testing datasets, ensuring that the model could learn from a representative subset of the entire dataset.

Lastly, the authors incorporated a technique known as 10-fold cross-validation during the training process. Cross-validation is a powerful tool that helps prevent overfitting by evaluating the model's performance on multiple subsets of the training data. In this case, the authors used 10-fold cross-validation, where the original dataset was randomly partitioned into ten equal-sized subsamples. Of these ten subsamples, nine were used for training the model, while the remaining one was retained for validating its performance. This process was repeated ten times, each time with a different subset held out for validation. The average performance across all ten trials was then calculated to provide a more robust estimation of the model's true error rate.

In summary, the authors took several steps to ensure the generalizability of their deep learning model. These included utilizing a diverse dataset, implementing stratified splitting for dividing the training dataset, and incorporating 10-fold cross-validation during the training process.
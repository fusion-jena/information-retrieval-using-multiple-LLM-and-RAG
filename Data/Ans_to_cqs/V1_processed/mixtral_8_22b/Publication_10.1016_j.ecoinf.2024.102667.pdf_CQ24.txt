The papers mentioned do not directly address the topic of handling randomness in the deep learning pipeline or using random seed values. However, some common strategies used to manage randomness in deep learning include:

1. Fixing the Random Seed Value: By setting a fixed random seed value, one can ensure reproducibility in their experiments by generating the same sequence of pseudo-random numbers each time the code is executed. This helps in comparing different models or hyperparameters more accurately.

2. Cross-Validation: Using techniques like k-fold cross-validation can help reduce the impact of randomness by training and testing the model multiple times with different subsets of data. The final performance metric is then calculated as the average across all folds.

3. Ensemble Methods: Combining multiple models trained with different initializations or architectures can also mitigate the effect of randomness. Techniques such as bagging, boosting, or stacking can be applied for this purpose.

4. Bayesian Optimization: Instead of manually tuning hyperparameters, Bayesian optimization algorithms can automatically search for optimal configurations while taking into account the inherent uncertainty due to randomness.

5. Regularization Techniques: Applying regularization methods like dropout, weight decay, or early stopping can help prevent overfitting caused by random fluctuations in the dataset.
The text does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions that adaptive moment estimation (ADAM) was initially used as an optimization algorithm but resulted in convergence on a sharp minimum leading to negligible accuracy. This suggests that ADAM might have caused overfitting, and alternative methods were sought. The use of dropout or L2 regularization techniques is not explicitly mentioned in the given context.

Unhelpful 
The text discusses various aspects of deep learning architectures such as ResNet50, InceptionV3, global average pooling (GAP), and skip connections. It also mentions the use of spectral, texture, and geometric features selection using PCA-NCA and sequence layers of GRU-LSTM-BiLSTM in BivalveNet. However, there is no direct mention of regularization methods like dropout or L2 regularization used to prevent overfitting in the deep learning pipeline.
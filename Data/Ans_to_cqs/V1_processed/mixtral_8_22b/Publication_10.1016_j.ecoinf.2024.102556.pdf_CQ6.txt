Based on the given context, there isn't any direct mention of specific preprocessing steps involved before training a deep learning model such as normalization, scaling, or cleaning. However, I can provide some general insights about common preprocessing techniques used in deep learning models.

Preprocessing data before feeding it into a deep learning model is crucial to ensure better performance and accuracy. Some commonly used preprocessing techniques include:

1. Normalization: This technique scales numeric columns to a range between 0 and 1. Min-max normalization and z-score normalization are popular methods used for normalizing input data.

2. Scaling: Scaling adjusts the values of numeric features to have similar magnitudes without distorting differences in ranges or units. Techniques like standardization and min-max scaling help bring all features to a comparable scale.

3. Cleaning: Data cleaning involves handling missing values, removing outliers, and correcting inconsistent entries. Properly cleaned datasets improve the overall quality of the trained model.

4. Encoding categorical variables: Categorical variables need to be converted into numerical form before being fed into a deep learning model. One-hot encoding and label encoding are widely used techniques for converting categorical variables into numbers.

5. Image augmentation: In case of image data, preprocessing may involve resizing images, applying transformations like rotation, flipping, zooming, etc., to create additional training samples and prevent overfitting.
The text does not provide specific details about the regularization methods used in the deep learning pipeline to prevent overfitting. However, common regularization techniques include dropout and L2 regularization. Dropout involves randomly setting a fraction of input units to 0 during training time, which helps reduce interdependent learning amongst neurons. On the other hand, L2 regularization adds a penalty term to the loss function, encouraging smaller weights and preventing any single weight from dominating the output. These techniques help generalize models better by reducing their complexity and avoiding overfitting.
The text does not provide explicit information about the strategy used to monitor the model performance during training. However, it mentions the use of batch normalization, which can be considered a method for monitoring and adjusting the model's performance during training. Batch normalization helps to stabilize the learning process by normalizing the input features of each mini-batch, effectively reducing internal covariate shift. This allows for the use of higher learning rates, leading to faster training times and reduced power consumption. Additionally, the wide residual network architecture mentioned in the text may also contribute to improved model performance and stability during training.
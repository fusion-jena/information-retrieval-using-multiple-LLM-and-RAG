To handle randomness in the deep learning pipeline, particularly related to the random seed value, several strategies have been employed in this study. Firstly, each network is trained and executed three times, each time with a different (random) initialization of the trainable parameters and with a different data fold. This approach helps mitigate the impact of any one specific random initialization or data split on the overall performance of the model. Secondly, the use of early stopping procedure with a patience parameter set to 10 also contributes to handling randomness. Early stopping is a form of regularization used to avoid overfitting by halting the training process before it has a chance to converge to a minimum that may provide poor generalization. Lastly, data augmentation techniques like rotations and flips are applied to all extracted patches. These techniques help increase the diversity of the training data and reduce the risk of overfitting due to limited data variability.
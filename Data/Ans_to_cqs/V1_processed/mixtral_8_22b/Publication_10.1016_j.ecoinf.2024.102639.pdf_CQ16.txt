In the given context, the criteria used to determine when training is complete involve the use of cross-validation error and the explained variance ratio. Specifically, the selection of the distance metric, such as Euclidean, is predicated on the data's characteristics and the specific analytical prerequisites. Furthermore, the number of components selected for dimensionality reduction using Principal Component Analysis (PCA) is dictated by the explained variance ratio. This ensures that the distilled dataset retains the majority of the original data's variance.

For the Support Vector Machine (SVM) model, the regularization parameter ('C') and the kernel type (linear) are calibrated through a synergistic application of grid search and cross-validation. The goal here is to mediate the balance between model complexity and its generalization prowess. In other words, the optimal values for 'C' and the kernel type are determined based on their impact on the model's performance during cross-validation.

Therefore, it can be inferred that the completion of training is determined by achieving a satisfactory level of cross-validation error and explained variance ratio. However, the exact threshold or criteria for determining this satisfactory level are not explicitly mentioned in the provided context.
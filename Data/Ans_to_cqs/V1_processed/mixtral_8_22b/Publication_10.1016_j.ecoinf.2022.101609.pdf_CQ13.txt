Based on the given context, it can be determined that the deep learning model employed in this study utilized several hyperparameters. Specifically, the learning rate was set at 10^-4 after evaluating its performance using different values. Additionally, various optimization algorithms were tested, including Stochastic Gradient Descent (SGD), Root Mean Square Propagation (RMSProp), Lamb Optimization Algorithm (LAMB), Large Batch Size Method (LARS), and Adaptive Moment Estimation (Adam). However, the specific optimizer chosen for the final model is not explicitly mentioned in the provided information.
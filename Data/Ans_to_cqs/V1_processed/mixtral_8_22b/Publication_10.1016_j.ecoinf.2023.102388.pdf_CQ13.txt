The paper uses Stochastic Gradient Descent (SGD) as the optimization algorithm for Convolutional Neural Networks (CNNs). For the no-overfitting scenario, the learning rate is set to 0.0001, while for the overfitting scenario, the learning rate is set to 0.001. Both scenarios have a weight decay of 0.5. The batch size is consistently set to 32, and the number of training epochs is set to 200. To prevent overfitting, batch normalization and dropout (with a probability of 0.5) are employed in the no-overfitting scenario. However, these techniques are not utilized in the overfitting scenario. Furthermore, the paper mentions using a momentum of 0.9 and applying a decay of 0.5 every 512 iterations, resulting in a total of 2048 iterations.
Based on the given context, it does not explicitly mention how the data was split for deep learning model training (i.e., into train, test, or validation sets). However, it can be inferred that the authors have followed standard practices for splitting the data. Typically, the dataset is divided into three parts: training set, validation set, and testing set. The training set is used to fit the model, while the validation set is used to tune the parameters of the model. Finally, the testing set is used to evaluate the performance of the fully specified model. In this case, the authors have used a sufficiently large dataset for custom training and have taken measures to prevent overfitting and underfitting situations. They have also used a uniform training environment and the same dataset for all epoch values. Therefore, it can be assumed that they have followed best practices for splitting the data.
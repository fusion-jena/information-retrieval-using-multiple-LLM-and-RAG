The proposed framework uses a Transformer-based deep learning model. This model is chosen due to its effectiveness in handling complex dependencies in the input data and mitigating the risk of gradient issues during training. The Transformer model consists of two main parts: an encoder and a decoder. Each encoder layer contains a multi-head self-attention mechanism and a feedforward neural network. The multi-head attention mechanism allows the model to capture context from the entire input sequence, while the positional encoding scheme gives the model a sense of word order.
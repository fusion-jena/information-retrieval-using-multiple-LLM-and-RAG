The strategy implemented to monitor the model performance during training involves generating 1000 random pseudo-absence points and splitting the collected data into training (80%) and testing (20%) datasets. To avoid sampling bias, pseudo-absence points are generated three times for each model and cross-validated per model using bootstrapping. The performance and accuracy of each model are then evaluated and compared using AUC (area under the receiver operating curve) and TSS (true skill statistic). Positive true skill statistics values indicate a perfect fit, while higher AUC values indicate improved accuracy and predictive power.
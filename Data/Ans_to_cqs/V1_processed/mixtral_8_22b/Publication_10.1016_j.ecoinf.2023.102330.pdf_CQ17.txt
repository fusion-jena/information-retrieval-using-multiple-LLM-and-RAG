The text does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, based on general practices in deep learning, some commonly used regularization techniques include dropout and L2 regularization. Dropout involves randomly setting a fraction of input units to 0 at each update during training time, helping to prevent overfitting. On the other hand, L2 regularization adds a penalty term to the loss function, encouraging smaller weights and reducing complexity in the model. Other possible regularization methods could include early stopping, weight decay, and batch normalization. Without further details, we cannot confirm which specific method(s) were employed in this case.
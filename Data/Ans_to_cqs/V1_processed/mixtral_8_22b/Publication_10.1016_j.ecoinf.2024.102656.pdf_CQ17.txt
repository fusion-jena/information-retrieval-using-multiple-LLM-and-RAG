The provided context does not directly address the specific regularization methods used to prevent overfitting in the deep learning pipeline such as dropout or L2 regularization. However, it mentions the use of five statistical models in an ensemble framework to enhance predictive accuracy and mitigate uncertainties and biases inherent in relying on a single modeling method. While these techniques do not specifically refer to regularization methods in deep learning pipelines, they can still contribute to reducing overfitting by incorporating diverse perspectives and minimizing reliance on any one particular approach.

The mentioned statistical models include Generalized Linear Models (GLM), Maximum Entropy (Max-Ent), Random Forest (RF), Generalized Boosting Model (GBM), and Multivariate Adaptive Regression Splines (MARS). Each of these models has its own strengths and weaknesses, but when combined in an ensemble framework, they can help improve overall performance and reduce overfitting. Additionally, the context discusses using a 3:1 train-test split for occurrence points and generating 1000 random pseudo-absence points to facilitate model training and provide a reference for unlikely inhabitation areas. These practices also aid in preventing overfitting by ensuring a balanced dataset and providing a more comprehensive representation of potential habitats.
The context does not directly address the specific strategies used to handle randomness in the deep learning pipeline, such as managing random seed values. However, it is common practice in machine learning and deep learning to set a fixed random seed value before training or testing a model to ensure reproducibility of results. This allows researchers to compare different algorithms or hyperparameters under identical conditions. Additionally, techniques like cross-validation, where data is split into multiple subsets for training and validation, help mitigate the effects of randomness by averaging over multiple runs with different splits. The context mentions using 10% and 30% of the remaining dataset as validation and test sets, which could be part of a cross-validation strategy.
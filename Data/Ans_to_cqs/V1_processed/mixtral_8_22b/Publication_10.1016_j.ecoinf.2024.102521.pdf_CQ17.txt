The papers cited do not provide explicit information about the use of regularization methods to prevent overfitting in the deep learning pipelines they describe. Regularization methods like dropout and L2 regularization are commonly employed in deep learning to avoid overfitting and improve model generalization. Dropout randomly sets a fraction rate of input units to 0 at each update during training time, which helps reduce interdependent learning amongst neurons. On the other hand, L2 regularization adds a penalty term to the loss function, encouraging smaller weights and preventing any single weight from dominating the others. While these techniques are widely adopted, without direct reference to them in the given sources, I cannot definitively confirm their usage in those particular studies.
The deep learning models described in the given context use several hyperparameters during their training process. Some of these hyperparameters include:

1. Learning Rate: Various learning rates were tested, ranging from 0.00001 to 0.01. This includes specific values like 0.0001, 0.001, 0.01, and 0.1.

2. Batch Size: Different batch sizes were evaluated, including 32, 64, 128, 256, 512, 8, 16, 32, 64, and 128.

3. Latent Dimension: The latent dimension was also investigated, with values such as 128, 256, 512, and 2048 being mentioned.

4. Weight Decay: Weight decay values ranged from 0 to 0.59, with specific values like 0.0001, 0.001, and 0.00001 being used.

5. Optimizer: The Adam optimizer was employed during the training of the MLP.

6. Scheduler: A scheduler was utilized to adjust the learning rate when the network reached a plateau.

7. Regularization Techniques: Different regularization techniques were explored, including Cross-Entropy and Focal Loss.

8. Dropout: Dropout values of 0.1, 0.05, 0.2, and 0.25 were applied.

9. Additional Parameters: When using Focal Loss, additional parameters α and γ were introduced. The former served as a balancing factor, either as a fixed value for all classes or as the inverse of each class frequency. The latter regulated the impact of the scaling factor. Specifically, when set to zero, Focal Loss was equivalent to Cross-Entropy Loss. Experiments included setting α as the inverse of class frequency, 0.25, or 0.5, while γ was set to 0.25, 0.5, 0.75, 1, 1.5, and 2.
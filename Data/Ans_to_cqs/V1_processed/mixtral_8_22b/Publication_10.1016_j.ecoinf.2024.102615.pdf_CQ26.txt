Based on the given context, there is no direct mention of specific techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, it does discuss some aspects related to data partitioning and model evaluation.

For instance, in section 2.1.3, it mentions that the raw dataset was randomly shuffled before being divided into training, validation, and test sets. This could potentially help mitigate any inherent biases present in the original order of the data. Furthermore, the data partitioning ratio was set as 8:1:1, and this random partition was repeated ten times to reduce the influence of random splitting and ensure converged performance.

While these methods do not directly address data imbalance or bias, they contribute towards creating a more robust and reliable model by reducing the impact of chance factors in the data split. For addressing issues like class imbalance or bias specifically, one might consider techniques such as oversampling the minority class, undersampling the majority class, or generating synthetic samples through methods like SMOTE (Synthetic Minority Over-sampling Technique), among others. However, these techniques are not mentioned in the provided context.
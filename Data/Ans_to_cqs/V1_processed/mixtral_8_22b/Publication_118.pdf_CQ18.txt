Unfortunately, based on the given context, there isn't enough information to determine what specific strategy was implemented to monitor the model performance during training. It only mentions that the dataset was extensively annotated, which allowed the feature representation produced by the auto-encoder to be easily evaluated and trained on a higher proportion of non-silent audio clips. However, no details are provided about how the monitoring or evaluation process took place.
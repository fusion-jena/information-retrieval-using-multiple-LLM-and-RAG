To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the dataset was standardized to reduce the influence of outliers. This involved normalizing and scaling all features so that they had a similar range. A standard score was used, which ensured that all features had a zero mean and a unit variance.

Secondly, the model's performance was evaluated based on three statistical indicators: coefficient of determination (R2), root mean square error (RMSE), and mean bias error (MBE). These indicators helped assess the accuracy and reliability of the model.

Lastly, the best model was selected by comparing the performance scores of all possible combinations of hyperparameters. Hyperparameters are parameters whose values are set before the learning process begins. They can significantly impact the model's performance. For instance, in the case of Random Forest (RF), the hyperparameters included the number of decision trees in the forest, the maximum depth of the decision tree, the number of features considered by each tree when splitting a node, etc. These hyperparameters were tested using a grid configuration.

However, it should be noted that while these measures help improve the model's generalizability, they do not guarantee it. Other factors such as the diversity of the dataset, use of cross-validation, stratified splitting, etc., also play a crucial role in ensuring the model's ability to perform well on unseen data. Unfortunately, there is no explicit mention of these techniques being employed in the given context.
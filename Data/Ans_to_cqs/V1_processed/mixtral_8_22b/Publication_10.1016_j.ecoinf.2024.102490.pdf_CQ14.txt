The sources provided do not explicitly mention how the hyperparameters of the model were optimized. Therefore, it is not possible to provide a definitive answer based solely on these references. However, common techniques used for hyperparameter optimization include grid search, random search, Bayesian optimization, and gradient-based optimization. Grid search involves testing all combinations of hyperparameters within a manually specified range, while random search randomly samples from this space. Bayesian optimization uses a probabilistic approach to iteratively update the probability distributions over hyperparameters, focusing on promising regions of the parameter space. Gradient-based optimization methods use gradients or derivatives of the loss function with respect to the hyperparameters to find optimal values. Without further information, it cannot be determined which method was employed in the studies referenced here.
The optimization technique applied in the deep learning pipeline is RMSprop with a learning rate of 0.001 and a learning rate decay of 0.0001. RMSprop is an optimization algorithm developed by Geoff Hinton that adapts the learning rate for each weight individually based on the historical gradient information. It uses a moving average of squared gradients to normalize the gradient, allowing it to adjust the step size dynamically during training. This helps prevent oscillations and improves convergence speed.
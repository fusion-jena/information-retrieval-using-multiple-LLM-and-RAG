Unfortunately, the given context does not provide explicit information regarding the hyperparameters used in the deep learning models, such as the learning rate or the type of optimizer employed. However, it does offer some insights into the structure of the neural network architecture used in both the MCD and VBI approaches.

In Figure 4, we see that the neural network architecture consists of an input layer with two nodes (one for Day Of Year - DOY, and another for Growing Degree Days - GDD), followed by two hidden layers containing 32 nodes each. These hidden layers employ either Monte Carlo Dropout (MCD) or DenseVariational layers (VBI). Lastly, there is an output layer comprised of a dense layer with two nodes and a DistributionLambda layer, which maps the values of those two nodes into the parameters of a Gaussian distribution used to predict the BBCH value.

While the context does not explicitly state the hyperparameters used, it does mention that the study focuses on scenarios with limited datasets. This suggests that the authors may have chosen hyperparameters suitable for working with smaller datasets, though without further details, it remains speculative.
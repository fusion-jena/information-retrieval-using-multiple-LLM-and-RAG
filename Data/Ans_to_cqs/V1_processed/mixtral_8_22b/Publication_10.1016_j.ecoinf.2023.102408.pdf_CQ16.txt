Based on the given context, the criteria used to determine when training is complete are not explicitly mentioned. However, some common practices in machine learning can be inferred. One possible criterion could be monitoring the performance metrics like R^2, MAE, and RMSE on both the training and validation datasets during the training process. When these metrics stop improving significantly or start deteriorating on the validation set while still improving on the training set, it might indicate that the model is starting to overfit the training data. At this point, the training could be considered complete. Another potential criterion could be observing the validation loss plateau, where the loss function stops decreasing substantially on the validation set. This would suggest that further training may not improve the model's generalization ability. In this study, the authors used a stratified sampling method to allocate 70% of the samples to the training dataset and the remaining 30% to the validation dataset. They likely monitored the performance metrics on both sets to decide when to stop training.
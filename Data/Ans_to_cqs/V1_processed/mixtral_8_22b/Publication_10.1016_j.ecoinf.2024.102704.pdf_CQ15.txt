The given context does not provide specific information about optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, it discusses methods to improve the efficiency of Convolutional Neural Networks (CNNs) by reducing memory access and increasing computational efficiency. This includes reconfiguring the computation sequence of 'convolutional layer + batch normalization + activation layer', which can lead to significant reductions in memory access and improvements in computational efficiency. Additionally, lowering the memory access in models can accelerate operations across the three main layers of CNNs, thus fully utilizing the model's strengths and more effectively using the computational resources provided by embedded devices. Reducing the number of layers in the network model can also help to reduce memory access and increase computational efficiency.
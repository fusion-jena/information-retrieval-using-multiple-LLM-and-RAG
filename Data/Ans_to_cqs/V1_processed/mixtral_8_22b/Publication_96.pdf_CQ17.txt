In the given context, no explicit mention of regularization methods used to prevent overfitting in the deep learning pipeline is mentioned. However, some common regularization methods used in deep learning pipelines include Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training, which helps to avoid overfitting by preventing complex co-adaptations on training data. This can be thought of as a way to automatically and adaptively regularize neural networks by adding noise to its inputs (Srivastava et al., 2014).

On the other hand, L2 Regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the magnitude of the weights. This encourages smaller weights, effectively limiting the complexity of the model and reducing overfitting (Ng, 2017).

These regularization methods could have potentially been applied in the development of the Deep Neural Network (DNN) models mentioned in the context, but without further information, their usage cannot be confirmed.
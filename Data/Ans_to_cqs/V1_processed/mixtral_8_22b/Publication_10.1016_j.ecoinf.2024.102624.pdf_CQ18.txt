In order to monitor the model performance during training, the authors employed a 10-fold cross-validation technique with three repetitions. They randomly divided the reference dataset into a 70:30 ratio for training and testing purposes. During the model building phase, they utilized the training dataset and applied the 10-fold cross-validation method. In this process, the training dataset was further partitioned into ten equal subsets or 'folds'. For each iteration, nine folds were used for model training while one fold served as the testing set. This cycle continued until every fold had been designated as the testing data once. To ensure robustness, the entire sequence was reiterated twice using different random splits.

This approach to accuracy assessment, coupled with hyperparameter tuning, facilitated the identification of the most accurate models based on their overall accuracy (OA). Moreover, it offered insights into the distribution of potential model outcomes by providing information about minimums, maximums, quartiles, and medians. A narrower range of possible model outcomes indicated higher reliability in predictions.

Therefore, the strategy adopted to track the model performance during training involved implementing a rigorous 10-fold cross-validation technique with multiple iterations and random splits. This methodology not only helped pinpoint the optimal models but also gauged the stability and dependability of these models through an analysis of their outcome distributions.
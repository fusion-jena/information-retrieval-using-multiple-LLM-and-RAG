Before training a deep learning model, several preprocessing steps can be taken to ensure optimal performance. These may include:

1. Data Cleaning: This involves removing or correcting any inconsistencies, errors, or missing values within the dataset. It also includes handling outliers, which could potentially skew the model's predictions if not properly addressed.

2. Normalization/Standardization: Deep learning algorithms often work better when numerical input variables are scaled to a standard range, typically between 0 and 1 (normalization) or with a mean of 0 and a standard deviation of 1 (standardization). This helps prevent attributes with larger magnitudes from dominating those with smaller ones during the learning process.

3. Feature Selection/Extraction: Not all features contribute equally to the output variable. Some might even add noise rather than useful information. Therefore, selecting relevant features or extracting new ones through techniques like Principal Component Analysis (PCA) can significantly improve model accuracy.

4. Data Augmentation: In cases where there isn't enough data available, synthetic samples can be created by applying transformations such as rotation, zooming, flipping, etc., especially common in image datasets.

5. Encoding Categorical Variables: If the dataset contains categorical variables, they need to be converted into numerical form since deep learning models only understand numbers. Techniques such as one-hot encoding or label encoding can be used depending upon the nature of the problem.

6. Splitting Dataset: Finally, the cleaned and processed dataset should be split into three parts - training set, validation set, and test set. The training set is used to train the model, while the validation set is used to tune hyperparameters and avoid overfitting. Once the final model has been built, it is evaluated against the test set to measure its generalization capability.
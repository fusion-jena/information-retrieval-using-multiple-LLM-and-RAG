In the given context, the optimization technique applied in the deep learning pipeline is mentioned as the RMSprop stochastic descent optimizer. This optimizer is used with the default parameters. Additionally, dropout layers with a rate of 0.2 are added to each hidden layer to prevent model overfitting. However, specific details about the implementation of these techniques or their comparison with other optimization techniques like Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) are not provided in the given context.
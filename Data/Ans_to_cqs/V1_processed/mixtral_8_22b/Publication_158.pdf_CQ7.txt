A systematic approach was employed to collect samples for training, verification, and final independent testing of the deep learning classifier. Samples were gathered following a grid of sample blocks measuring 9 by 9 km with 15 km spacing between blocks. These blocks were then designated as either training, validation, or test blocks.

To create the test sample, a column of the sample grid was chosen at random, and every fourth column starting from this point was added. However, only every second row within the selected columns was incorporated into the test sample. This process resulted in a total of 39,520 (60%) training samples, 23,934 (30%) verification samples, and 6509 (10%) test samples.

The focus was primarily placed on gathering more samples for training and verification efforts in order to maximize the overall performance of the model. During the training phase, the Adam optimizer was utilized along with 500 epochs and a categorical-cross entropy loss function. A batch size of 64 was also established.

Data augmentation techniques such as rotation, reflectance bias of ±10%, and random noise of ±10% reflectance for 15% of the input image were implemented to increase sample diversity. Training convergence was monitored through observation of the validation data over the course of multiple training epochs. The network weights exhibiting the best performance were retained. An early stopping criterion was enforced if no improvements were observed after 50 consecutive epochs.

In summary, the criteria used to split the data for deep learning model training involved creating a grid of sample blocks, designating them as training, validation, or test blocks, and employing various optimization strategies during the training phase to ensure robustness and accuracy of the model.
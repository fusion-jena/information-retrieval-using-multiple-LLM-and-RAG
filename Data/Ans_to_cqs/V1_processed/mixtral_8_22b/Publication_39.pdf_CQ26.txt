Based on the given context, there isn't any explicit mention of specific techniques such as stratified splitting, oversampling, undersampling, or diverse data collection being employed to address data bias during preprocessing of the deep learning pipeline. However, some relevant information regarding data handling can be found.

For instance, the authors have mentioned using different data augmentation techniques during the training process to prevent overfitting and improve model performance. These techniques include horizontal flipping, adding/subtracting intensity, applying Gaussian blur, adding additive Gaussian noise, and simulating fog in the video. They also state that these augmentations are applied simultaneously, except for the fog simulation which has a separate application probability.

Additionally, they describe how the dataset was divided into training, validation, and testing sets. Specifically, the dataset was randomly but consistently split across all experiments and networks, with 20% of the video clips forming the test set, another 20% of the remaining clips creating the validation set, and the rest comprising the training set.

However, without additional context, it cannot be determined if these methods were specifically implemented to mitigate data bias or if they served other purposes within their deep learning pipeline.
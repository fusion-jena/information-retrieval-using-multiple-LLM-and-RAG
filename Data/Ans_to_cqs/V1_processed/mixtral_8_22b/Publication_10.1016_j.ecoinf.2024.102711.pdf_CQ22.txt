In the given context, the performance of the deep learning models is primarily evaluated using two key metrics: Accuracy and F1-score.

Accuracy is a common metric used to measure the overall effectiveness of a classifier. It is calculated by dividing the total number of correct predictions by the total number of instances. However, while accuracy can be useful when dealing with balanced datasets, it may not provide a complete picture of the model's performance when working with imbalanced classes.

To address this issue, another metric called the F1-score is often used alongside accuracy. The F1-score is the harmonic mean of precision and recall, where precision measures the proportion of true positive predictions out of all positive predictions made, and recall measures the proportion of true positives out of all actual positives. By combining these two metrics, the F1-score provides a more comprehensive view of the model's performance, taking into account both false positives and false negatives.

In the context provided, the TabPFN model achieved the highest performance among all models, with an Accuracy of 91% and an F1-score of 93%. Similarly, the AutoGluon model also performed well, with an Accuracy of 87% and an F1-score of 93%. On the other hand, the ensemble ML models such as XGBoost and Random Forest (RF) produced lower fit metrics, with the RF model producing the lowest fit metrics (Accuracy = 73%, F1-score = 79%).
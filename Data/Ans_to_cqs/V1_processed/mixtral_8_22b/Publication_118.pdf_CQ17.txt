In the given context, there is no mention of specific regularization methods such as dropout or L2 regularization used to prevent overfitting in the deep learning pipeline. However, some techniques have been employed to ensure the model does not overfit the data. These include:

1. Batch Normalization: This technique is mentioned in the context where it states that 'the squared difference between a pixel...was calculated, with the mean across all pixels giving the loss, with 2d batch normalization'. Batch normalization helps reduce internal covariance shift, making the model more stable during training and reducing overfitting.

2. Adam Optimizer: The use of the Adam optimization algorithm is also noted. While not strictly a regularization method, Adam optimizer can help prevent overfitting due to its adaptive learning rates. It combines the advantages of two other extensions of stochastic gradient descent: Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp), providing an optimization algorithm that can handle sparse gradients on noisy problems.

3. Early Stopping: Although not explicitly stated, the fact that the networks were only trained for a few epochs could suggest early stopping as a form of regularization. Early stopping involves monitoring the validation error while training the model, and once the validation error starts increasing, training is stopped to avoid overfitting.

However, without further information about the implementation details, it cannot be definitively confirmed whether these techniques were specifically used as regularization methods to prevent overfitting.
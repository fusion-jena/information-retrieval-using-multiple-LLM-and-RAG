The given context does not explicitly mention any specific optimization techniques such as Stochastic Gradient Descent (SGD), Adam, or others that are commonly used in deep learning pipelines. However, it is mentioned that the authors use a Dropout Layer which is a regularization technique used to prevent overfitting during training. Additionally, the use of Batch Normalization Layers suggests that some form of normalization was employed during training, although this doesn't directly indicate the use of a particular optimization algorithm. Therefore, based on the information available, we cannot definitively state what optimization techniques were applied in the deep learning pipeline.
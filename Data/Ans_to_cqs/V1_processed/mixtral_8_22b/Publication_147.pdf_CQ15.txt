The text mentions that the model was trained using the Keras Nadam optimizer, which is a variant of the Adam optimizer. This optimizer is based on stochastic gradient descent (SGD) but incorporates momentum and adaptive learning rates to improve convergence. Additionally, the use of a combination of binary crossentropy and dice coefficient loss functions suggests that the model may have been optimized for both classification accuracy and segmentation quality. However, the text does not provide specific details about how these optimization techniques were implemented or tuned during the training process.
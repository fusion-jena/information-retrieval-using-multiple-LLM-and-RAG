Unfortunately, there isn't enough information available in the given context to determine what specific optimization techniques have been applied in the deep learning pipeline. However, some commonly used optimization techniques in deep learning include Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), and Root Mean Square Propagation (RMSProp). These methods help minimize the loss function during the training process, allowing the model to learn more effectively from the input data. Without additional details about the deep learning pipeline mentioned in the paper, I cannot provide a definitive answer regarding the optimization techniques used.
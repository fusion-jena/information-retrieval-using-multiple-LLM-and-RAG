The deep learning model was trained using the Adam optimizer over 200 epochs. The batch size was fixed at 16. The initial learning rate was set at 0.001 and was subsequently reduced by a factor of 0.1 in a step-wise manner whenever the validation loss remained constant for five epochs. The minimum learning rate was established at 1 × 10^-5. The hyperparameter λ in Eq. (4) was set to 0.8, Mk in Eq. (5) was defined as 2k+1(k = 1, 2,...K), and β in Eq. (6) was set to 0.4.
The strategy implemented to monitor the model performance during training is the use of the early stopping strategy. This strategy involves separating the training data into training and validation groups. Training is then stopped when the generalization performance starts to degrade for a number of epochs. Additionally, the dropout approach was also employed to address the issue of overfitting. Dropout is a regularization technique that trains neural networks with alternate topologies in parallel by randomly dropping out a certain proportion of layer neurons.
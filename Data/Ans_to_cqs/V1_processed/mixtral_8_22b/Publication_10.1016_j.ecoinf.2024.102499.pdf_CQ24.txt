The authors do not explicitly discuss strategies to handle randomness in the deep learning pipeline, such as managing random seed values. However, they mention using a k-fold cross-validation approach, which is a technique often used to reduce variability due to random sampling effects. This method involves splitting the dataset into 'k' subsets or folds, then training and testing the model 'k' times, each time using a different subset for validation while the remaining subsets are used for training. By averaging the results across these runs, one can obtain more reliable estimates of the model's performance. Additionally, the authors use a consistent set of hyperparameters, including a fixed learning rate and a specific number of iterations, which may help mitigate some sources of randomness.
The authors use several techniques to address data bias during preprocessing of their deep learning pipeline. They first remove outliers from the dataset before conducting any subsequent analyses. This helps ensure that extreme values do not skew the results.

Next, they use a predictor screening analysis that employs bootstrap forest partitioning to evaluate the contribution of predictors on the response object. This technique allows them to select the most important features for classification tasks such as species or fisher sex discrimination. By focusing on these key features, they can reduce noise and improve model performance.

Additionally, the authors take steps to mitigate foot position bias and observer bias. Foot position bias occurs because some tracks were created using both left and right front feet, while others only used one foot. The authors record the foot position of each track during the feature extraction process and account for this variable in their models. Observer bias arises due to differences in how individuals manually mark landmark points in the FIT interface. To minimize this source of error, multiple observers independently marked the same set of tracks, allowing the authors to compare and validate their results.

Finally, the authors follow best practices for stratified sampling when dividing their datasets into training, testing, and validation sets. As shown in Table 1, they maintain roughly equal proportions of male and female tracks across all three subsets. This ensures that their models generalize well to new data and reduces overfitting.

Overall, the authors demonstrate a comprehensive approach to addressing data bias during preprocessing of their deep learning pipeline. Their methods include removing outliers, selecting relevant features through predictor screening, accounting for biases related to foot position and observer variability, and implementing stratified sampling strategies. These techniques help enhance the robustness and reliability of their machine learning models.
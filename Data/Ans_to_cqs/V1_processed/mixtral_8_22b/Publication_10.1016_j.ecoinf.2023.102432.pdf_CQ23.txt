To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the data was split into training and testing data using 5 K-fold cross-validations. This technique involves dividing the data into k subsets, where each subset serves as the test set exactly once while the remaining kâˆ’1 subsets are used for training. This process is repeated k times, ensuring that every observation has the chance of being validated against.

Secondly, before running the algorithm, MTMI-SMF required a few parameters to be set by the user. These parameters included the number of initialized targets, initialization method, background mean and inverse covariance, and alpha. After experimentation with training-validation approaches, specific parameters were chosen. This process helped to fine-tune the model and improve its performance.

Lastly, the model was tested on the 20% of the data not used for training the algorithm. A confusion matrix was calculated, and the overall, producer, and user accuracy were reported. This step allowed for the evaluation of the model's performance and ensured that it could accurately classify new, unseen data.
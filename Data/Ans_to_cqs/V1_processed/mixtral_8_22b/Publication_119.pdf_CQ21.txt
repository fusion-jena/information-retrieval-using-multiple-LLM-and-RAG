After the model training process, several postprocessing steps can be performed depending on the specific requirements of the project. Some common postprocessing steps include:

1. Model Evaluation: Once the model has been trained, its performance needs to be evaluated using metrics like accuracy, precision, recall, F1 score, etc. A confusion matrix can also be generated to visualize the performance of the model. In the given context, the authors have mentioned that they used independent testing data for model evaluation and calculated metrics like testing accuracy and kappa coefficient. They also mention generating a confusion/error matrix for comparing the accuracy of various models.

2. Visualizing Results: Another important step in postprocessing involves visualizing the results obtained from the model. For example, saliency maps can be generated to highlight the areas in the image that contributed most towards the final prediction made by the model. However, there is no explicit mention of any such visualizations being carried out in the given context.

3. Fine-Tuning Parameters: Postprocessing may also involve fine-tuning certain parameters based on the initial results obtained from the model. For instance, if the model performs poorly on certain classes, one might consider adjusting the weights assigned to those classes during training. Similarly, if the model tends to overfit or underfit the data, adjustments can be made to regularization techniques like dropout or batch normalization. In the given context, the authors mention having added a dropout layer and a batch normalization layer after hidden layer 2 to reduce overfitting and improve generalization on testing data. They also state that they further tuned hyperparameters like learning rate, number of epochs, and batch size to achieve optimal performance.

In summary, some common postprocessing steps after model training include evaluating the model's performance using metrics and confusion matrices, visualizing results using tools like saliency maps, and fine-tuning parameters based on initial results.
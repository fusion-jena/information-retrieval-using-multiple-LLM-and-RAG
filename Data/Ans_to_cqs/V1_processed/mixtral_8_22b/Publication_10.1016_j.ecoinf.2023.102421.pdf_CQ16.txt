In the given context, it is not explicitly mentioned what specific criteria are used to determine when the training is complete. However, based on common practices in machine learning, there are several possible ways to decide when the training should stop. These include reaching a certain number of epochs or iterations, achieving a desired level of accuracy or loss on the validation set, observing no further improvement in validation metrics for a certain number of epochs (also known as early stopping), or meeting some other predefined criterion.

From the information available, we do know that the training dataset was divided into two parts, where 80% was used for training/validation and the remaining 20% was kept as unseen data for testing. This suggests that the authors might have been monitoring the performance of their models on the validation set during training to decide when to stop. Additionally, they mention using 10-fold cross-validation during the training process, which could also help prevent overfitting and provide a more robust estimate of the model's generalization error.

However, without explicit details about the exact stopping criteria used by the authors, one cannot definitively state how they determined when training was complete. It would be necessary to refer back to the original research paper or consult with the authors directly to obtain this information.
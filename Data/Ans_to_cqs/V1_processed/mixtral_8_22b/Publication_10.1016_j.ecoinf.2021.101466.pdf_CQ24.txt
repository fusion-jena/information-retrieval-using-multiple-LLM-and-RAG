The text does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, it mentions a technique called "random erasing" applied during the training phase of the EfficientNet model. Random erasing is a data augmentation strategy where parts of input images are randomly eliminated, helping improve the robustness and generalization capabilities of the model. This approach introduces controlled randomness into the training process, potentially mitigating the impact of other sources of randomness like initial weights or order of examples. Nevertheless, specific strategies for managing random seed values or addressing other aspects of randomness in the deep learning pipeline are not discussed in the given context.
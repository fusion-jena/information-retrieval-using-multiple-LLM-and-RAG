The provided context does not contain explicit information about the hyperparameters used in the deep learning model, such as the learning rate or the type of optimizer employed. However, based on general practices in deep learning, some common hyperparameters include the learning rate, which controls the step size during gradient descent; the batch size, which determines the number of training examples used in one iteration; the number of epochs, which specifies the number of times the entire training dataset will be passed through the network; and the choice of optimizer, such as stochastic gradient descent (SGD), Adam, or RMSprop. Additionally, depending on the specific application and architecture, other hyperparameters may also be relevant, like weight decay, momentum, dropout rate, etc. To find out the exact hyperparameters used in the mentioned studies, please refer to the original papers or contact the authors directly.
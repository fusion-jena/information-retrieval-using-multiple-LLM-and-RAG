Unfortunately, none of the provided sources explicitly mention the specific hardware resources used for training their respective deep learning models. Therefore, it is not possible to provide a definitive answer based solely on these references. However, generally speaking, deep learning models often require powerful computational resources due to their complexity and large datasets. Commonly used hardware resources for training deep learning models include Graphics Processing Units (GPUs), Tensor Processing Units (TPUs), and Field Programmable Gate Arrays (FPGAs). These specialized processors enable faster computation and efficient parallel processing, making them suitable for handling the high computational demands of deep learning algorithms.
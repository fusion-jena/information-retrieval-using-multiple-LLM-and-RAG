The authors of this study used dropout as a regularization method to prevent overfitting in their deep learning pipeline. Dropout is a technique where randomly selected neurons are ignored during training, which helps to reduce interdependent learning amongst them. This can help improve the generalizability of the model. However, it should be noted that the authors mention that they only applied dropout to the training data, which resulted in a gap between training and validation accuracy. They also mentioned applying data augmentation, but it is unclear if this was done solely on the training data or both the training and validation sets. There is no explicit mention of other regularization techniques like L2 regularization being employed in this study.
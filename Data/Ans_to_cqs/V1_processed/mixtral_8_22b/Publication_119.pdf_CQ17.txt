In the given context, two regularization methods have been mentioned to prevent overfitting in the deep learning pipeline. These methods are Dropout and Batch Normalization.

Dropout is a technique where randomly selected neurons are ignored during training. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. This has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs. It reduces overfitting by providing a way of automatically and adaptively regularizing neural networks.

Batch Normalization is another method used to prevent overfitting. It involves standardizing the inputs in every mini-batch to have zero mean and unit variance. This helps to stabilize the learning process and dramatically reduces the number of training epochs required to train deep networks. It also acts as a form of regularization, implicitly encouraging the network to spread out the learned weights over all the inputs, rather than depending too much on individual features.

These techniques help to ensure that the model does not become too complex and learn the noise in the training data, which would lead to poor performance when tested on unseen data. They contribute to improving the model's ability to generalize well to new, unseen data.
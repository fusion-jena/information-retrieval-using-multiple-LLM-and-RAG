The text does not provide explicit information about how the hyperparameters of the model were optimized, such as through methods like grid search or random search. However, it does mention that the Stochastic Gradient Descent (SGD) optimizer was used with an initial learning rate of 0.01 for most models, except for MnasNet-A1, which started with a learning rate of 0.1. Additionally, the learning rate was reduced by a factor of 10 after every 30 epochs. This suggests that some form of manual tuning or trial-and-error approach might have been employed to determine these values. Nevertheless, without more specific details regarding the optimization process, we cannot definitively state whether techniques like grid search or random search were utilized.
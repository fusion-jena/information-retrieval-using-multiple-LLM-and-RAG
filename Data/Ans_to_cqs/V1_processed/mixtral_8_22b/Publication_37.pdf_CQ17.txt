In the given context, there are two regularization methods mentioned to prevent overfitting in the deep learning pipeline. These methods are Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training. This helps to avoid overfitting by making sure that the model does not rely too heavily on any single neuron. In this case, dropout was applied to the input of the last two layers with a probability of 0.5.

L2 Regularization, also known as weight decay, is another common technique used to prevent overfitting. It adds a penalty term to the loss function, which discourages large weights and encourages smaller ones. This results in simpler models that generalize better. In this context, L2 Regularization was applied to the weights of the last two layers with a penalty factor of 0.001.
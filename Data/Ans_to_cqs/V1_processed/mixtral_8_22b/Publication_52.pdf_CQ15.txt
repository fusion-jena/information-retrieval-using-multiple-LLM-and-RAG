The provided text does not explicitly mention any specific optimization techniques applied in the deep learning pipeline, such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, it does provide information about Extreme Learning Machines (ELMs), which are a type of Single-Layer Feedforward Neural Network (SLFN) that can learn much faster than traditional backpropagation networks.

In the context of ELMs, the optimization technique involves setting the input weights and biases randomly, then determining the output weights through a simple matrix calculation. This process eliminates the need for iterative tuning of parameters, making ELMs significantly faster than other types of neural networks.

While this information does not directly answer the query regarding optimization techniques like SGD or Adam, it provides insight into how ELMs optimize their performance without relying on these commonly used techniques.
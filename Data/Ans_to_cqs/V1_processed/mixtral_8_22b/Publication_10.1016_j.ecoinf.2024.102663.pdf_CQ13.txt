Based on the given context, the hyperparameters used in the deep learning model include the number of epochs, learning rate, use of cosine learning rate scheduler, and choice of optimizer. Specifically, the figures mentioned in the context compare the results obtained by varying these parameters. Figure 13 compares the performance between 10 and 50 epochs, showing that increasing the number of epochs improves the performance. Figure 14 examines the impact of using or not using the cosine learning rate scheduler, indicating mixed results depending on the specific metric being evaluated. Figures 15 and 16 explore the influence of changing the initial and final learning rates, suggesting that finding the optimal value is important. Lastly, Figure 17 contrasts three different optimizers - RMSProp, SGD, and AdamW - implying that the choice of optimizer also affects the outcome.
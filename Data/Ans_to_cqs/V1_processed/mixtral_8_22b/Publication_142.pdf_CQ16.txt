The determination of when training is complete can vary depending on the specific machine learning model and task. However, some common criteria for determining when training should stop include:

* Validation Loss Plateau: This occurs when the validation loss stops decreasing or starts increasing after each epoch. At this point, further training may not improve the performance of the model and could lead to overfitting.
* Early Stopping: This technique involves monitoring the validation loss during training and stopping the training process once the validation loss stops improving for a certain number of consecutive epochs. The idea behind early stopping is to prevent overfitting by stopping the training before it becomes too complex.
* Learning Rate Schedule: Another way to determine when training should stop is by using a learning rate schedule. In this case, the learning rate is gradually reduced after each epoch until it reaches a minimum value. Once the learning rate reaches its minimum value, training is stopped.
* Maximum Number of Epochs: Sometimes, training is stopped after a predetermined maximum number of epochs have been completed. This can help prevent overfitting and ensure that the model does not become too complex.

In summary, there are several criteria that can be used to determine when training is complete, including validation loss plateau, early stopping, learning rate schedule, and maximum number of epochs. The choice of criterion depends on the specific machine learning model and task.
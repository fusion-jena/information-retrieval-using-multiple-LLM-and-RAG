The table and text provided do not contain information about regularization methods used to prevent overfitting in the deep learning pipeline such as dropout or L2 regularization. Regularization methods are techniques used to prevent overfitting by adding a penalty term to the loss function during training. Dropout is a technique where randomly selected neurons are ignored during training which helps to make the model more robust and prevents overfitting. L2 regularization adds a penalty equal to the sum of squares of all the weights in the network to the loss function, this encourages smaller weight values and makes the model simpler and less likely to overfit. Other popular regularization methods include L1 regularization, early stopping, and data augmentation.
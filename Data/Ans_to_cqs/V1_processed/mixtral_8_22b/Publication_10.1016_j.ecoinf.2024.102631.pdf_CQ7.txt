The criteria used to split the data for deep learning model training typically include the size of the dataset, the complexity of the model, and the desired level of generalization. Commonly, the dataset is divided into three subsets: training, validation, and testing sets.

The training set is used to fit the parameters of the model, while the validation set is employed during the training process to tune hyperparameters and prevent overfitting. Lastly, the testing set is utilized to evaluate the final performance of the trained model on unseen data.

Regarding the proportions of these splits, there isn't a one-size-fits-all solution. A common practice is to use around 60-80% of the data for training, 10-20% for validation, and the remaining 10-20% for testing. However, these percentages may vary depending on the specific requirements and constraints of the problem at hand. For instance, when dealing with small datasets, it might be more appropriate to adopt techniques such as cross-validation or bootstrapping to maximize the utilization of available data.
Based on the given context, it appears that the researchers applied transfer learning and fine-tuning techniques using the ResNet50 deep learning model, which had been previously trained on ImageNet data. They utilized RGB images of size 224x224x3 as inputs, specifically color spectrograms of two-second durations. The implemented architecture included only the feature extraction layers from ResNet50, discarding the higher classification layers. Two fully connected layers were added to allow the model to learn new features. Dropout was employed to prevent overfitting and simulate the training of multiple distinct models.

However, the context does not provide explicit information about specific preprocessing steps like normalization, scaling, or cleaning. These steps typically occur prior to feeding input data into a deep learning model. Normalization involves rescaling numeric values to a common range, often between 0 and 1, while scaling may involve standardizing data to have zero mean and unit variance. Cleaning refers to removing noise, outliers, or irrelevant data points from the dataset. Since these details are not mentioned in the provided context, I cannot definitively state what preprocessing steps were taken before training the deep learning model.
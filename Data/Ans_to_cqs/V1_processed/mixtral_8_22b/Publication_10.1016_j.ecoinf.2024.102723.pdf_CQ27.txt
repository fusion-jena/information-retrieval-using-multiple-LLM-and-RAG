Unfortunately, there isn't enough information available to provide a detailed explanation about how the trained deep learning model was deployed. However, I can share some common practices for deploying such models.

Model Serialization: After training the deep learning model, it needs to be saved or serialized so that it can be loaded later for making predictions. This involves saving the weights and architecture of the model into a file format like HDF5, JSON, or Pickle.

Platform Selection: Once the model is serialized, it needs to be integrated into a platform where users can interact with it. Depending on the use case, this could involve integrating the model into a web application, mobile app, or cloud service. Some popular platforms include Flask, Django, AWS Lambda, Google Cloud Functions, Azure Functions, etc.

Deployment Process: Typically, the deployment process would involve setting up the necessary infrastructure, configuring the environment variables, loading the serialized model, and exposing an endpoint through which users can send requests and receive responses. It may also involve implementing error handling mechanisms, logging, monitoring, and scaling capabilities depending on the requirements.

Since the specific details regarding the deployment process are not mentioned in the given context, it cannot be determined exactly what steps were taken to deploy the trained deep learning model.
The text mentions two regularization methods used to prevent overfitting in machine learning models: regularization and ridge regression. Regularization involves adding a penalty term to the log likelihood that quantifies model complexity. Ridge regression is a specific type of regularization where the model coefficients are shrunk towards smaller values by altering the maximum likelihood criterion to include a penalty term based on the weighted L2 norm of the parameter vector. While not explicitly mentioned in the given context, other common regularization techniques used in deep learning pipelines include dropout and L2 regularization. Dropout randomly drops out nodes during training to prevent overfitting, while L2 regularization adds a penalty term to the loss function to discourage large weights. These techniques help improve generalization and reduce overfitting in complex models.
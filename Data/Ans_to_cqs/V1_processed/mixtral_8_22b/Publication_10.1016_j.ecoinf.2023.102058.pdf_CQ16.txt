The context does not provide explicit information on the specific criteria used to determine when training is completed for the mentioned models (PCR, PLSR, or RF). However, generally speaking, there are several common criteria used to decide when training should stop:

1. Validation Loss Plateau: This criterion involves monitoring the validation loss during training. When the validation loss stops decreasing significantly and remains nearly constant over several epochs, it can indicate that the model has learned all it can from the training data, and further training may lead to overfitting.

2. Early Stopping: Similar to the validation loss plateau, early stopping monitors the validation loss but automatically halts training once the validation loss fails to improve after a certain number of epochs. This helps prevent overfitting and saves computational resources.

3. Maximum Number of Iterations/Epochs: Training can also be stopped based on reaching a predefined maximum number of iterations or epochs. This ensures that the training process doesn't continue indefinitely if other criteria aren't met.

4. Learning Rate Schedule: Some models use a learning rate schedule where the learning rate gradually decreases over time. Once the learning rate reaches a minimum threshold, training can be considered complete.

Without more specific details about the training process for these models, it isn't possible to pinpoint exactly which criteria were used. Still, one or more of these general criteria might have been applied to determine when training was complete.
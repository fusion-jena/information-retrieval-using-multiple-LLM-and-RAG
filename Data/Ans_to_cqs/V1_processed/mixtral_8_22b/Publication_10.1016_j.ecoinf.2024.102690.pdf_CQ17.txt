Based on the given context, there is no information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization methods used in deep learning include dropout, L1 regularization, and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to reduce overfitting. L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function, whereas L2 regularization adds a penalty equal to the square of the magnitude of coefficients to the loss function. Both L1 and L2 regularizations help to prevent overfitting by reducing the complexity of the model.
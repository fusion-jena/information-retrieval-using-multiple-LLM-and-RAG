The text does not provide explicit details about the specific hyperparameters used in the deep learning model, such as the learning rate or optimizer. However, it mentions that the model uses stochastic gradient descent for optimization and that the Adam algorithm was used to optimize the network parameters during training. Additionally, the text states that the model has a batch size of 64 samples and was trained for 100 epochs. These are examples of hyperparameters used in the deep learning model.
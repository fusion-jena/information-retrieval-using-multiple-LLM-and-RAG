The strategy implemented to monitor the model performance during training involves using a separate validation dataset. This dataset is used to calculate various metrics such as model accuracy, precision, recall, and F1 score for each class. These metrics are calculated by comparing the predicted labels generated by the model against the actual labels present in the validation dataset. By monitoring these metrics throughout the training process, researchers can assess how well their model is performing and make necessary adjustments to improve its performance. Additionally, they can also use this information to determine when the model has reached optimal performance and stop further training to prevent overfitting.
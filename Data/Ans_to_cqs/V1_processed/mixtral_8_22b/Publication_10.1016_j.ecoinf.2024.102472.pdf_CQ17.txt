In the given context, there is no explicit mention of specific regularization methods such as dropout or L2 regularization being used to prevent overfitting in the deep learning pipeline. However, some techniques mentioned can be considered as forms of regularization. For instance, the use of Extreme Gradient Boosting (XGBoost) can help prevent overfitting due to its ability to minimize bias error within and between spectral predictors. Additionally, the iterative error back-propagation adjustment in the MLPNN training phase can also contribute to preventing overfitting by fine-tuning the connection weights until reaching a target minimal error. Lastly, the use of a grid search method for hyperparameter tuning can also aid in avoiding overfitting by finding the best combination of parameters that generalizes well to unseen data.
The text provides information about the process of determining the optimal size for algorithm training and test sets for a deep learning model applied to classify footprints of captive Amur tigers. The authors conducted holdback partitioning trials using different combinations of test and training set sizes to find the minimum number of trails required for testing. They found that the optimal classification accuracy was achieved when the test set size was smallest relative to the training set. Specifically, they observed that the predicted value was close to the expected value even when the test to training set ratio was 32:12, which demonstrated the robustness of the model.

To perform this analysis, the authors used the algorithm generated in FIT and iterated it ten times for each combination of test/training size, with randomly selected trails. This allowed them to compare the predicted outcome with the known test set size. The results showed that when the test set size comprised trails from four tigers, and the test/training set size comprised trails from 04/40 tigers, the predicted test set sizes were very similar across a range of partitioning trials. However, when the test set size was 36 tigers, and the test/training ratio was 36/08 tigers, there was a wide range of predicted test set sizes.

Therefore, based on the given context, the criteria used to split the data for deep learning model training were not explicitly stated. However, it can be inferred that the authors used random selection of trails to create different combinations of test and training sets and evaluated their performance by comparing the predicted outcomes with the known test set sizes. Additionally, they aimed to achieve optimal classification accuracy while minimizing the number of trails required for testing.
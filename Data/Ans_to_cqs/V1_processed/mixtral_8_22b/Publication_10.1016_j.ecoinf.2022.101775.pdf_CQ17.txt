The text does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions the use of a regularized cost function, which is often employed to avoid overfitting. Regularization techniques like L1 or L2 regularization, also known as weight decay, add a penalty term to the loss function to discourage large weights and encourage simpler models. Dropout is another popular method where randomly selected neurons are ignored during training to prevent complex co-adaptations. Although these specific methods aren't mentioned in the given context, they are commonly used in deep learning pipelines to combat overfitting.
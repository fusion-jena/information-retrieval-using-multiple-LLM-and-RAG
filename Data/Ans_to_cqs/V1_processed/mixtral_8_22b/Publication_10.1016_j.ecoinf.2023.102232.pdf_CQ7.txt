Unfortunately, there isn't any information about splitting the data for deep learning model training in the given context. However, typically, when working with machine learning or deep learning models, the data is often divided into three parts: training, testing, and validation datasets.

The training dataset is used to train the model, which means it helps the model learn patterns and relationships within the data. The testing dataset is used to assess how well the trained model performs on unseen data. Lastly, the validation dataset is used to fine-tune the model's hyperparameters and prevent overfitting.

In general, the common practice is to use around 70% of the available data for training, 20% for testing, and 10% for validation. However, these percentages may vary depending on the size of the dataset and the complexity of the problem being addressed.
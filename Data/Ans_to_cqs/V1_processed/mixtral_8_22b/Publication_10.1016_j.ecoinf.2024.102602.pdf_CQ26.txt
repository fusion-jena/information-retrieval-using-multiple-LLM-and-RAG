The paper does not explicitly mention any techniques used to address data bias during preprocessing of the deep learning pipeline. However, it can be inferred that some form of stratification or balanced sampling might have been employed due to the high performance metrics achieved in the classification task. Additionally, the use of transfer learning with popular convolutional neural networks (CNNs) such as VGG16, ResNet50, ResNet101, Inception-v3, DenseNet201, and EfficientNetB0 could help mitigate potential biases in the dataset since these architectures have shown good performance on various computer vision tasks. Furthermore, the authors mentioned using the Adam optimizer algorithm, a batch size of 10, and 100 epochs for model optimization, but they did not discuss any specific strategies to handle imbalanced classes or data bias. Lastly, the paper mentions implementing an early stop approach with a patience value of 16 to prevent overfitting and conserve computational resources, which could indirectly contribute to addressing data bias.
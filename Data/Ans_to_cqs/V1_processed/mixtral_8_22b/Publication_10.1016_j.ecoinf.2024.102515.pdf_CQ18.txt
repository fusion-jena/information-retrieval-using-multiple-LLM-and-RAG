The given context does not provide explicit details on the specific strategy used for monitoring the model performance during training. However, based on the available information, we can infer some aspects related to the model development and evaluation processes.

The study focuses on gathering tweets containing biodiversity observations and uses them to create a training dataset. It mentions that using exclusively textual information from tweets may result in a deficient model, suggesting that additional features might have been incorporated to improve its performance.

Regarding the monitoring aspect, the context describes a manual verification process where a taxonomist reviews the collected tweets and annotates categories of interest. This implies that the model's performance could potentially be monitored by comparing its predictions against these manually verified labels.

Additionally, the context discusses the need for location information alongside species observations to ensure meaningful analysis and application. While this doesn't directly address the query, it suggests that spatial data might play a role in the overall modeling approach.

In summary, while the exact strategy for monitoring the model performance during training isn't explicitly stated, the context indicates that manual verification and possibly incorporating additional features like spatial data are part of the broader methodology.
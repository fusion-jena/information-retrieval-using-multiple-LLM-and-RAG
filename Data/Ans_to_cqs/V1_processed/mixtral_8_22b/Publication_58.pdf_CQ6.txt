Based on the given context, there isn't explicit information about specific preprocessing steps such as normalization, scaling, or cleaning applied to the dataset before feeding it into the various Deep Learning architectures and the XGBoost model. However, we can infer some general preprocessing steps from the context.

Firstly, the context mentions that the SPOT6 satellite data was preprocessed to be fed into the models. This implies that some form of preprocessing took place, although the exact nature of these preprocessing steps is not specified.

Secondly, the context discusses how neural networks do not perform well when trained with unbalanced datasets. To address this issue, they mention creating balanced datasets with the initial pixels selection used for learning in the case of 'central-pixel labeling' architectures. They also state that they tried several methods for semantic labeling but found negligible differences in performance. Therefore, one could argue that ensuring a balanced dataset might have been part of the preprocessing step.

Lastly, the context states that the input of 64x64 windows was used for labeling the central pixel in the XGBoost model. This suggests that the raw data may have been segmented or partitioned into smaller units during preprocessing.

In conclusion, while the context does not provide detailed information regarding specific preprocessing techniques like normalization, scaling, or cleaning, it does suggest that some form of preprocessing occurred, including potentially addressing imbalance issues and segmentation of the raw data.
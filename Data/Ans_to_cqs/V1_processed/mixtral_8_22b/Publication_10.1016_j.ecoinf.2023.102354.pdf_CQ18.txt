Unfortunately, there isn't enough context provided to accurately determine what strategy was used to monitor the model performance during training. However, some common methods include tracking metrics like accuracy, precision, recall, F1 score, ROC AUC, etc. during training epochs. Cross-validation techniques can also be employed to ensure the model generalizes well to unseen data. Regularization techniques might be applied to prevent overfitting. Learning curves could be plotted to visualize how the loss function changes with respect to the number of iterations. Early stopping criteria could be set based on validation loss to avoid unnecessary computational costs. These are all potential strategies but without further context, I cannot provide a definitive answer.
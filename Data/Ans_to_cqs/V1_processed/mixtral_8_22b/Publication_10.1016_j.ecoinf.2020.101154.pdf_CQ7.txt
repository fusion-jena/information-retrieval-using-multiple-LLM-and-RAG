In the given context, two types of data splitting approaches have been discussed - Random Year (RY) data splitting and Three-Part Split. For the RY data splitting approach, individual observations were grouped based on the year of capture, and years were randomly chosen for inclusion in either the train or test dataset. However, this method could potentially lead to systematic differences between the test and train datasets. On the other hand, the Three-Part Split involves dividing the data into training, validation, and testing sets. Here, the Validation Set Index (VI) score is used instead of the test set score during the process of fine-tuning the model's hyperparameters. Once confident that the validation score cannot be improved further through parameter adjustments or feature engineering, the best model is evaluated on the final test set only once.

Hyperparameters play a crucial role in optimizing the performance of deep learning models. In the given context, hyperparameters were tuned using Grid Search, which conducts an exhaustive search across predefined parameter values for an estimator. Grid Search generates validation sets internally, eliminating the need for manual creation. For the RI models, the same initial parameters were applied to analyze the CS and NW datasets, followed by a Grid Search on both datasets with a Two-Part Split to identify hyperparameters yielding the highest accuracy levels. Conversely, Grid Search was unnecessary for Three-Part Splits, and additional parameter tuning was carried out manually.

In summary, the criteria used to split the data for deep learning model training include random selection of years for RY data splitting and division into training, validation, and testing sets for Three-Part Split. Additionally, hyperparameters are essential for improving model performance, and they can be optimized using techniques like Grid Search.
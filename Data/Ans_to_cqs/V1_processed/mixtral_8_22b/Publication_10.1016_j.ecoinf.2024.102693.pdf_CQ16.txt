The table titled "Table 1 General transfer training hyper-parameters for each basic model" provides information about various hyperparameters used during the training process for several models such as ResNet-50, ViT-S/16, Volo-d1, and ViP-Small/7. However, it does not explicitly mention any specific criteria or metrics used to determine when the training should be stopped.

Typically, there are multiple ways to decide when a machine learning model has been trained sufficiently. Some common criteria include observing a plateau in the validation loss, reaching a predefined maximum number of epochs, or achieving a target accuracy level on the validation data. In this case, since we do not have explicit information regarding these aspects from Table 1, we cannot definitively state what criteria were used to determine when training was complete.

However, based on general practices in deep learning, one possible criterion could be the use of a cosine scheduler mentioned under the 'Scheduler' column for all models. This type of scheduler gradually reduces the learning rate over time, which can help prevent overfitting and improve convergence. Additionally, the 'Epochs' column indicates that all models were trained for 210 epochs, suggesting that a fixed number of iterations might also have been employed as a stopping criterion.
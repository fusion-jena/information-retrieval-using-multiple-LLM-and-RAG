Based on the given information, it appears that two types of regularization techniques have been employed to prevent overfitting in the deep learning pipeline: L2 regularization and weight decay.

L2 regularization, also known as Ridge Regression or Weight Decay, adds a penalty term to the loss function during training. This penalty term is proportional to the square of the magnitude of the weights in the network. By adding this penalty, we encourage smaller weights, effectively reducing the complexity of the model and preventing overfitting. In the provided context, the parameter'regular\_para' likely represents the coefficient for the L2 regularization term.

Weight decay is another form of regularization that gradually reduces the values of the weights in the network during training. It can be thought of as a continuous process of shrinking the weights towards zero. In the given context, the 'weight\_random\_range' parameter might indicate the range within which the initial random weights are generated before applying weight decay.

However, there is no explicit mention of using dropout as a regularization technique in the provided context. Dropout is a method where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust to small changes in the input data.

In summary, based on the available information, L2 regularization and weight decay appear to be the regularization methods used to prevent overfitting in the deep learning pipeline.
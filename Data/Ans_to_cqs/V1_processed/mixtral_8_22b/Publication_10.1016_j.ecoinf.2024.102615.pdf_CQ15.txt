The text discusses the use of the grid search method to optimize the performance of a Multilayer Perceptron (MLP) model. Grid search is a traditional hyperparameter optimization technique that systematically explores a manually specified subset of a hyperparameter space of a learning algorithm. It is not explicitly mentioned whether stochastic gradient descent (SGD) or adaptive moment estimation (Adam) optimization algorithms are used in the deep learning pipeline. However, it can be inferred that some form of optimization algorithm would have been employed since they are essential components of training deep learning models.

The grid search method is used to find the best combination of hyperparameters for the MLP model. These hyperparameters include the number of neural network layers, the number of neurons per layer, the percentage values of the dropout layers, the learning rate, and the batch size. The specific tuning ranges for these hyperparameters are listed in Table 1. Each set of parameters is then trained based on a set of training data (80%) and validated using validation data (10%). The results of each group of parameters represent the average of ten experiments, and the data are shuffled each time.

While the text does not provide explicit information about the use of SGD or Adam optimization algorithms, it does mention the use of a learning rate hyperparameter. Learning rates are critical parameters in both SGD and Adam optimization algorithms. Therefore, it can be assumed that either SGD or Adam, or another similar optimization algorithm, has been utilized in conjunction with the grid search method to train and optimize the MLP model.
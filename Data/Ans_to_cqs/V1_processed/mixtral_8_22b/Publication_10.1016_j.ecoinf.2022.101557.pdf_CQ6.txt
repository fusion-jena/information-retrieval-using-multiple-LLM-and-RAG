Before training a deep learning model like a Convolutional Neural Network (CNN), several preprocessing steps are typically applied to the input data. While the specific context does not provide explicit details about these steps, some common practices include:

1. Normalization: This involves rescaling the values of features so they fall within a certain range, often between 0 and 1. This helps prevent any single feature from dominating the others and improves convergence during optimization.

2. Scaling: Similar to normalization, scaling adjusts the range of feature values but maintains their distribution shape. Common methods include min-max scaling and standardization (z-score normalization).

3. Cleaning: This refers to handling missing or inconsistent data points in the dataset. Techniques might involve removing outliers, imputing missing values, or dropping entire rows/columns containing missing data.

4. Augmentation: For image datasets, augmentation techniques like rotation, flipping, zooming, cropping, etc., help increase the diversity of training examples and improve the model's ability to generalize.

5. Splitting: As mentioned in the given context, splitting the data into larger blocks for cross-validation is essential to evaluate the performance of the model accurately. Additionally, separating the dataset into training, validation, and testing sets ensures unbiased evaluation.

While the provided context doesn't explicitly mention these preprocessing steps, they are commonly employed when preparing data for deep learning models.
The given context does not provide explicit information about the specific optimization techniques used in the deep learning pipeline. However, there is a mention of Stochastic Gradient Descent (SGD) being used in conjunction with Backpropagation (BP). Additionally, the concept of reparameterization trick is mentioned, which is often used in Variational Autoencoders (VAEs) to optimize the loss function during training. Other common optimization techniques used in deep learning include Adam, RMSProp, and Momentum. Without more specific details, it cannot be definitively stated which optimization techniques were used in this particular case.
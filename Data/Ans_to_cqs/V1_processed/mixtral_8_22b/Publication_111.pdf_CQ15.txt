The text mentions two specific optimization techniques used in the deep learning pipeline: Dropout [45] and Adam [46]. Dropout is a technique used to prevent neural networks from overfitting by randomly dropping out neurons during training. This helps improve the generalization of the model. On the other hand, Adam is a method for stochastic optimization that uses adaptive estimates of lower-order moments to update the weights of the network. These techniques help improve the efficiency and effectiveness of the deep learning pipeline.
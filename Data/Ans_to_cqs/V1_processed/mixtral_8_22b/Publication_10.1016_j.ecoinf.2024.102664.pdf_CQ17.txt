Based on the given context, there is no explicit mention of any regularization methods used to prevent overfitting in the deep learning pipeline. However, we can infer from the table and text that some form of regularization might have been applied during the training phase. For instance, the use of different optimizers like adam, sgdm, and RMSProp suggests that some form of weight decay or L2 regularization was likely employed. Dropout, another popular regularization technique, may also have been utilized but isn't explicitly mentioned. It's important to note that while not directly stated, regularization techniques are often implicitly included in deep learning pipelines to avoid overfitting and improve generalization performance.

Unhelpful 
I am sorry, but based on the provided context, I do not see any information about the regularization methods used to prevent overfitting in the deep learning pipeline.
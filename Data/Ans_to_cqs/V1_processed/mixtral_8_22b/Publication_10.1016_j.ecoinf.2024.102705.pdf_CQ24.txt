In the context of deep learning, handling randomness is crucial to ensure consistent and reliable results. Randomness can be introduced into the deep learning pipeline in several ways, such as through the initialization of weights, shuffling of input data, or selection of hyperparameters. One common strategy to manage randomness is by setting a fixed random seed value. This ensures that any operations dependent on random number generation will produce the same output every time the code is executed.

Moreover, when working with large datasets, it may not always be feasible to use all available data due to computational constraints. In such cases, researchers often employ sampling techniques like stratified sampling or bootstrapping to create smaller yet representative datasets. By applying these techniques consistently across different runs, one can maintain some level of control over the randomness introduced by selecting specific samples.

Additionally, cross-validation is another technique used to reduce variability caused by randomness. It involves splitting the dataset into 'k' folds, where 'k-1' folds are used for training and the remaining fold for validation. This process is repeated 'k' times, ensuring each fold gets its turn as the validation set. Cross-validation helps in obtaining more robust estimates of model performance and reduces the impact of randomness on evaluation metrics.

Lastly, ensemble methods also help deal with randomness in deep learning pipelines. Ensemble models combine predictions from multiple individual models, thereby reducing variance and improving overall accuracy. Techniques like bagging and boosting fall under this category. They work by creating diverse sets of base learners, either by resampling the original dataset or assigning varying weights to instances, thus introducing controlled randomness into the modeling process.

Overall, managing randomness in deep learning pipelines involves implementing strategies such as fixing random seeds, utilizing appropriate sampling techniques, performing cross-validation, and leveraging ensemble methods. Each approach contributes uniquely towards achieving stable and accurate outcomes despite the presence of inherent randomness within the system.
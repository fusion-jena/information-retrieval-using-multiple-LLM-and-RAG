The text does not provide explicit information about the regularization methods used in the deep learning pipeline to prevent overfitting. However, common regularization techniques include dropout and L2 regularization. Dropout randomly sets a fraction rate of input units to 0 during training, which helps reduce overfitting by providing a way to automatically and adaptively regularize neural networks. On the other hand, L2 regularization adds a penalty equivalent to the sum of all squared weights to the loss function, encouraging smaller weight values and thus helping avoid overfitting. Other possible regularization methods could be used but are not mentioned in the given context.
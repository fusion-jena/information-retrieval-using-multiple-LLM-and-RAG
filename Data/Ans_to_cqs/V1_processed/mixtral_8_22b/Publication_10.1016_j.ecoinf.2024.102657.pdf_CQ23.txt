To ensure the generalizability of the deep learning models, several measures were taken during the training process. Firstly, the dataset used for training consisted of diverse dialect types from different years, ensuring that the model could learn patterns applicable across various conditions. Secondly, the dataset was divided into three distinct subsets - training, validation, and testing sets. This division allowed for tuning the hyperparameters of the Convolutional Neural Network (CNN) using the validation set while preventing overfitting or bias towards specific examples. Lastly, stratified splitting was employed to maintain a consistent distribution of dialect classes within each subset. By preserving the original proportions of dialect classes, the risk of imbalanced datasets affecting the model's performance was minimized. These steps collectively contributed to enhancing the robustness and reliability of the deep learning models.
In the given context, there are several regularization techniques mentioned to prevent overfitting in the deep learning pipeline. One such technique is Batch Normalization, which accelerates deep network training by reducing internal covariate shift. Batch Normalization consists of a parameterized centering and reduction of pre-activations, which facilitates optimization and enables raising the learning rate, leading to quicker convergence. Additionally, it has a regularization effect because the centering and reduction of a neuron activation are linked to the mini-batch statistics. Since the mini-batch selection is stochastic at every iteration, a neuron activation becomes stochastic itself, and the model will not rely on it when it has no good effect on prediction. Another regularization method mentioned indirectly is the use of a shallow neural network architecture instead of a deeper one. In this case, considering the restricted number of samples, a deep CNN would be very prone to overfitting. Therefore, using a simpler architecture with fewer parameters helps reduce the risk of overfitting. Other common regularization techniques like Dropout or L2 Regularization are not explicitly mentioned in the provided context but could also be employed to further mitigate overfitting risks.
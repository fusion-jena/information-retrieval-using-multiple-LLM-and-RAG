The hyperparameters of the XGBoost model used in this study were optimized through grid search functions. Grid search is a methodical way to find the best combination of parameters for a machine learning algorithm. It involves defining a set of values for each parameter and exhaustively trying all possible combinations of these values. In this case, the optimized XGBoost parameters found through grid search were: nrounds = 500, max_depth = 4, eta = 0.03, gamma = 1, min_child_weight = 1, subsample = 0.5, colsample_bytree = 0.8.
Based on the given context, there isn't any explicit mention of specific measures taken to ensure the generalizability of the deep learning model such as using diverse datasets, cross-validation, or stratified splitting. However, some information can be inferred about the methods employed during the training process.

For instance, it was mentioned that the image shapes were fixed, indicating that the input data had consistent dimensions. Additionally, the model was trained using both ResNet-50 and ResNet-101 backbones, suggesting that different architectures were explored to improve the robustness of the model. Furthermore, various anchor box ratios were utilized, implying that multiple scales were considered during object detection.

Moreover, the model underwent training for 20 epochs with 500 steps each for training images and 100 steps for validation images. This indicates that the model was exposed to numerous iterations of the dataset, potentially enhancing its ability to learn complex patterns and improving its overall performance.

However, without further details regarding the nature of the dataset, the exact methodology used for training, and the evaluation metrics employed, it remains unclear whether these factors directly contributed to ensuring the generalizability of the deep learning model.
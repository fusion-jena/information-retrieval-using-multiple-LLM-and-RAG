The given context does not provide specific details about the optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, it mentions that an improved SoftMax function was used to speed up training and improve robustness. This suggests that some form of optimization technique was employed during the training phase of the deep learning pipeline. Unfortunately, without further context, we cannot determine the exact optimization techniques used.
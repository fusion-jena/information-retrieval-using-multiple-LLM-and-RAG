Based on the given context, it appears that several measures were taken to ensure the generalizability of the deep learning model. However, the specific details regarding diverse datasets, cross-validation, or stratified splitting are not explicitly mentioned. Here are some relevant points from the context:

1. The model was trained using a batch size of 100 for 20,000 steps.
2. Feature vectors for the frozen layers were extracted in parallel using Google Cloud DataFlow.
3. Dropout was employed as a form of regularization to reduce overfitting.
4. The model was designed to prioritize minimizing false negatives, potentially at the cost of increasing false positives.

While these points suggest efforts towards ensuring the robustness and generalizability of the model, there is no explicit mention of using a diverse dataset, implementing cross-validation, or employing stratified splitting techniques. Therefore, based solely on the provided context, we cannot definitively state which specific methods were used to ensure the generalizability of the deep learning model.
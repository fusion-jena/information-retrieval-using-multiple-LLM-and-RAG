To evaluate the performance of the deep learning model, several metrics are used. These include Precision, Recall, Average Precision (AP), and Mean Average Precision (mAP).

Precision measures the proportion of true positive predictions out of all positive predictions made by the model. It is calculated as the ratio of True Positives (TP) to the sum of TP and False Positives (FP).

Recall measures the proportion of true positive predictions out of all actual positives. It is calculated as the ratio of TP to the sum of TP and False Negatives (FN).

Average Precision (AP) is a summary metric that calculates the average precision value over all unique thresholds. It is calculated by taking the sum of the precision at each threshold multiplied by the change in recall from the previous threshold, divided by the total number of relevant instances.

Mean Average Precision (mAP) is another summary metric that averages the AP across multiple classes. It is calculated by dividing the sum of AP for each class by the total number of classes.

In addition to these metrics, the authors also use the F1 score, which is the harmonic mean of precision and recall. They decrease the beta coefficient down to 0.5 to give more weightage to precision than recall.

Overall, these metrics help evaluate the performance of the deep learning model and determine how well it can distinguish between different bird species based on their vocalizations.
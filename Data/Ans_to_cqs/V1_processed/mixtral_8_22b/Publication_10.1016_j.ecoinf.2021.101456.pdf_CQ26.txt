Based on the provided context, there isn't any direct mention of specific techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, we can infer some possible methods based on the information available.

In the context, it mentions that the dataset was gradually expanded until reconstructions were sufficiently good. This implies that the dataset might have been imbalanced initially, and expanding the dataset could help mitigate this issue. Expansion of the dataset could involve collecting more diverse data or using augmentation techniques like oversampling minority classes or undersampling majority classes.

Additionally, the use of Variational Autoencoders (VAEs) suggests another way to potentially handle data bias. By limiting the size of the bottleneck of the model (latent dimension) to two, they aim to visualize the latent representation. A well-structured and smooth latent space can yield useful representations of the input data. This process may inherently reduce biases present in the original data by transforming them into a more balanced and representative latent space.

However, without explicit details about the preprocessing steps taken to address data bias, it is not possible to provide a definitive list of techniques used for handling data bias in this particular case.
In the given context, there isn't explicit mention of specific criteria used to determine when training is complete. However, it does provide some insight into how the dataset was divided for training and validation purposes. It states that 75% of the data was devoted to model training and 25% to testing. This implies that the training process might have been completed once the model had learned from the 75% of the available data. Additionally, since the study mentions evaluating the performance of the RF models against the testing datasets, it suggests that the completion of training could also depend on achieving satisfactory performance metrics such as adj\_R2, RMSE, MAE, or KGE. These metrics are mentioned in relation to Fig. 4, which shows the performance of the random forest model with both training and testing/validation datasets for different time lags. Therefore, although not explicitly stated, it can be inferred that the training process may be considered complete when the model has achieved acceptable performance levels on the validation set.
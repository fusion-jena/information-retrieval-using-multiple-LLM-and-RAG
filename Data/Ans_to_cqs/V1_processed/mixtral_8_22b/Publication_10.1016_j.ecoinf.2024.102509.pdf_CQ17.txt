The text does not provide specific information about the regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. However, it does mention that the accuracy of the Artificial Neural Network (ANN)'s training process depends on the quality and quantity of the sample dataset, and that the optimal number of hidden layers of an ANN model depends on the problem itself. Additionally, it notes that the over-fitting problem can occur at any instance, which implies that measures should be taken to avoid this issue. Regularization techniques like dropout and L2 regularization are commonly employed to address overfitting in deep learning pipelines. Dropout involves randomly dropping out nodes during training to reduce over-dependence on certain features, while L2 regularization adds a penalty term to the loss function to discourage large weight values. These methods help generalize the model better and prevent overfitting.
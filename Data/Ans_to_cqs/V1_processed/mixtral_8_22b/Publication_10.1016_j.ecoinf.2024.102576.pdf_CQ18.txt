Based on the given context, there isn't explicit information about the specific strategy implemented to monitor the model performance during training. However, we can infer some aspects related to this topic.

Firstly, the text mentions that 8330 cells (half of the total) were randomly extracted as a mask for training data, while 3,332 cells (20% of the total) were randomly extracted as a mask for validation data. This suggests that the authors might use the validation dataset to evaluate the model's performance during training. By comparing the predicted output of the model against the actual values in the validation set, one could assess the accuracy and efficiency of the model.

Secondly, the context discusses the possibility of improving multi-year classification performance by learning classification results and incorporating them into the model. This implies that the authors may consider monitoring the long-term performance of the model and adjusting its parameters accordingly.

Lastly, the mention of generating a land-cover matrix that can spatially identify changes between land-cover types indicates that the authors aim to track the evolution of land-cover patterns over time. Monitoring such changes would require evaluating the model's performance consistently and updating it when necessary.

In conclusion, although the exact strategy to monitor the model performance during training is not explicitly stated, the context provides clues suggesting the use of a validation dataset, potential improvements through learning classification results, and tracking land-cover pattern changes over time.
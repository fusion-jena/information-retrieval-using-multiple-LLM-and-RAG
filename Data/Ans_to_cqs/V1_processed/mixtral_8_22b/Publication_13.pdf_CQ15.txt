Based on the given context, there isn't any explicit mention of specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being used in the deep learning pipeline. However, it can be inferred that some form of optimization technique would have been utilized during the training process of the Convolutional Neural Network (CNN) models mentioned in the text.

Deep learning pipelines typically involve optimizing parameters like weights and biases through backpropagation algorithms, which use gradient descent methods to minimize the cost function. Common optimization techniques include SGD, Adam, Root Mean Square Propagation (RMSProp), and others. Without further information about the exact implementation details of the deep learning pipeline, it cannot be definitively stated which optimization techniques were specifically applied.
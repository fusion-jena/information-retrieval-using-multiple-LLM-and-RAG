Based on the provided context, there isn't a direct mention of how the data was split into training, testing, and validation sets for the deep learning model. However, it can be inferred from common practices in machine learning that the data would typically be divided randomly into these subsets. A common approach is to allocate around 70-80% of the data for training, 10-15% for validation, and the remaining 10-15% for testing. This division allows the model to learn patterns from the majority of the data during training, while also providing separate datasets for tuning hyperparameters (validation set) and evaluating final performance (testing set). It should be noted that the exact percentages may vary depending on factors such as the total amount of available data or specific requirements of the project.
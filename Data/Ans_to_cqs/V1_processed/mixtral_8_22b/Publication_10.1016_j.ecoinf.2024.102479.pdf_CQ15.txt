The text does not provide specific information about optimization techniques used in the deep learning pipeline such as stochastic gradient descent (SGD) or Adaptive Moment Estimation (Adam). However, it mentions that the parameters of the algorithms were optimized through a grid search approach involving cross-validation. This suggests that some form of optimization was performed during the training phase of the models. Additionally, the table lists several hyperparameters that were tuned for each algorithm, including'shrinkage', which could refer to a learning rate parameter often adjusted in optimization methods like SGD or Adam. Without more specific details, it cannot be confirmed exactly which optimization techniques were employed in the deep learning pipeline.
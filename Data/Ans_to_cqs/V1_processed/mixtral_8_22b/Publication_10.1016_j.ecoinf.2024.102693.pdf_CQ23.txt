To ensure the generalizability of the deep learning model, the authors used a deep transfer-learning strategy based on fine-tuning. They initialized the model parameters using a pre-trained model and replaced the final classification layer with a fully connected layer, aligning its output dimension with the number of classes in the target dataset. All parameters across the layers were then fine-tuned on the target dataset. Additionally, the use of diverse datasets such as ImageNet and JFT-300M for pre-training could also contribute to the generalizability of the model. However, the paper does not explicitly mention any specific techniques like cross-validation or stratified splitting for ensuring the generalizability of the model.
During the training phase, a strategy was implemented to prevent overfitting and ensure optimal performance of the model. This involved setting aside 10% of the total dataset as a validation set. By doing so, the model could be evaluated on unseen data while it was still being trained, allowing for adjustments to be made if necessary. Additionally, a min-max scaler was applied before computing the performance metrics, which was then reversed once the evaluation was complete. This scaling technique helped normalize the data and improve the accuracy of the results. Furthermore, the training was conducted using a batch size of 64 samples and continued for 100 epochs, utilizing the Adam optimization algorithm to fine-tune the network parameters.
To handle randomness in the deep learning pipeline, such as the random seed value, the approach described in the given context employs several strategies. Firstly, the data is divided into several large blocks based on their coordinates for cross validation purposes. This helps ensure that the model generalizes well across different parts of the dataset. Secondly, dropout regularization is used during training with a rate of 0.3. Dropout works by randomly dropping out neurons during training, which helps prevent overfitting and makes the model more stable. Lastly, the Adam optimization algorithm is used with a learning rate of 0.0001 to minimize the cross entropy loss between the network's predictions and the labeled data. By using these techniques, the researchers aim to reduce the impact of randomness and improve the overall performance of the deep learning pipeline.
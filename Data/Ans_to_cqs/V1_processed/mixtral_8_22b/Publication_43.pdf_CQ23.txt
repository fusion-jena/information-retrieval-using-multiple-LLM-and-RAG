Two primary measures were taken to ensure the generalizability of the deep learning model: fine-tuning and data augmentation.

Firstly, fine-tuning was employed by initializing the evaluated models with the pre-trained weights of ImageNet. This technique allows the model to leverage the knowledge gained from a larger and more diverse dataset, thereby improving its ability to generalize to new datasets. However, it should be noted that fine-tuning the entire network may lead to overfitting if the new dataset is small. Therefore, only the last learnable layers of the CNN were gradually adjusted to the specific task while keeping the first layers intact since they extract low-level features that remain consistent across various visual recognition tasks.

Secondly, data augmentation techniques were applied to increase the size of the dataset from 100 to 6000 images. These techniques included random rotations, horizontal flips, vertical flips, and zoom ranges. By artificially expanding the dataset, the model becomes exposed to a wider range of variations, which enhances its capacity to generalize well to unseen data.

These strategies collectively contribute to ensuring the robustness and adaptability of the deep learning model, enabling it to perform effectively even when presented with previously unobserved examples.
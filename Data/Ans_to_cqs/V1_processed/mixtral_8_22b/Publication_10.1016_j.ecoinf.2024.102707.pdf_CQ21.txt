After the model training process, several postprocessing steps can be performed depending on the specific requirements of the project or research. Some common postprocessing steps include:

1. Evaluation metrics: Calculating evaluation metrics such as accuracy, precision, recall, F1 score, etc., is essential to assess the performance of the machine learning model. These metrics help determine how well the model has learned from the training data and its ability to predict new instances accurately.

2. Confusion Matrix: A confusion matrix is a table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class. This helps identify where misclassifications occur and provides insights into improving the model.

3. Saliency Maps: Saliency maps are visualizations that highlight the most important features contributing to the output of a neural network. They provide insight into which parts of the input data have the greatest impact on the final prediction.

4. Hyperparameter tuning: After evaluating the initial results, hyperparameters may need adjustment to improve the model's performance further. This involves fine-tuning parameters like learning rate, batch size, epochs, etc.

5. Model interpretation: Understanding why the model made certain predictions is crucial for building trust in the model and identifying potential biases. Techniques like SHAP values, LIME, and feature importance plots can aid in interpreting complex models.

Based on the given context, it appears that the authors calculated the F1 scores for their model using both training and validation datasets. Additionally, they reported the number of true positives, false positives, and false negatives for each individual orthomosaic. However, there is no explicit mention of other postprocessing techniques like saliency maps or confusion matrices being employed.
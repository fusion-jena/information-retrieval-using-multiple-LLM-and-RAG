Based on the given information, it appears that the optimization of hyperparameters for the models is not explicitly mentioned to use methods such as grid search or random search. However, there are indications of how the models were selected and trained.

In the model selection procedure, five models were generated for each of the four available deep-ANN architecture types, resulting in a total of 20 models. Each model was trained with a small subset of the training data for four epochs. The accuracy of these candidate models was evaluated using a left-out validation dataset, and the model with the highest performance was chosen for further training on the complete training data for up to 30 epochs.

Additionally, Table 3 provides specific parameter values for each classical machine learning model used in different test years. These parameters include 'nrounds','max\_depth', 'eta', 'gamma', 'colsample\_bytree','min\_child\_weight','subsample','mtry','size', 'decay', 'layer1', 'layer2', 'layer3', 'hidden\_dropout', and 'visible\_dropout'. Although the table does not show any systematic variation in these parameters, suggesting that no explicit hyperparameter tuning method like grid search or random search was applied, it still indicates that some form of manual adjustment might have been done based on initial results.

Therefore, while the exact methodology for hyperparameter optimization is not specified, it can be inferred from the given context that the authors may have manually adjusted the hyperparameters based on preliminary results rather than employing a formal optimization technique like grid search or random search.
The text mentions several strategies to handle randomness in the deep learning pipeline. One such strategy is the use of a deep ensemble, which involves training multiple independent models on the same data but with different initial sets of random weights. This approach leverages the randomness inherent in weight initialization and sampling of training batches to cause each model to converge to a different local minimum in the solution space. As a result, the final weights can be interpreted as samples from an approximate posterior distribution. Additionally, the text suggests that among various methods performing approximate Bayesian inference in deep learning, ensembles have been shown to yield superior results in terms of predictive performance and reliability of the generated outputs. Another strategy mentioned is the use of stochastic gradient descent for iterative learning of model parameters, which introduces some level of randomness due to its reliance on randomly sampled mini-batches of data during optimization. However, the specific impact of these strategies on handling randomness related to the choice of random seed values is not explicitly discussed in the given context.
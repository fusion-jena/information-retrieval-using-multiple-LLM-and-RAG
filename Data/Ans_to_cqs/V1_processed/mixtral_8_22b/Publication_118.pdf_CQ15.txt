In the given context, the optimization technique applied in the deep learning pipeline is mentioned as the "Adam" optimization algorithm. Adam is a popular optimization algorithm used in deep learning models due to its computational efficiency and low memory requirements. It combines the advantages of two other extensions of stochastic gradient descent, namely Root Mean Square Propagation (RMSProp) and Adaptive Gradient Algorithm (AdaGrad), resulting in an efficient adaptive learning rate method for gradient descent optimization.
The authors do not explicitly mention the specific evaluation metrics they used to measure the performance of their deep learning models. However, they do discuss running experiments with different train-test split ratios to assess how well their models generalize to unseen data and prevent overfitting. This suggests that they likely used common classification metrics like accuracy, precision, or recall to compare the performance of their various experimental configurations. Additionally, since they mention tracking whether any of their approaches are overfitting, it implies that they might have been monitoring metrics related to overfitting, such as validation loss or F1 score. Nevertheless, without explicit confirmation from the authors regarding the exact evaluation metrics employed, this remains speculative.
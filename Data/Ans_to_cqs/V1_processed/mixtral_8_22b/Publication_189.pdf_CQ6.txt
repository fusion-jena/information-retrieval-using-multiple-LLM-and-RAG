Before training a deep learning model such as Mask R-CNN, several preprocessing steps may be involved depending on the specific requirements of the task and the nature of the input data. Some common preprocessing techniques include:

1. Data Cleaning: This involves removing or correcting any inconsistencies, errors, or missing values in the dataset. It can also involve handling outliers or anomalous data points.

2. Normalization: This technique scales the input features so they have zero mean and unit variance. It helps improve the convergence speed and stability of the optimization algorithm during training. Common methods include min-max normalization, z-score normalization, and decimal scaling.

3. Scaling: Similar to normalization, scaling adjusts the range of feature values to prevent certain features from dominating others due to their larger numerical values. Methods like min-max scaling and logarithmic scaling can be applied.

4. Augmentation: Data augmentation generates new samples by applying transformations such as rotation, flipping, zooming, cropping, or adding noise to existing ones. This increases the diversity of the training set and improves the generalizability of the model.

5. Encoding: Categorical variables need to be converted into numerical form using encoding schemes like one-hot encoding, label encoding, or ordinal encoding.

In the given context, it appears that the authors have performed some level of image preprocessing, including fixing the width and height of images and setting the image and mask shapes based on the number of training images and true masked images. However, details regarding other potential preprocessing steps, such as normalization, scaling, or augmentation, are not explicitly mentioned.
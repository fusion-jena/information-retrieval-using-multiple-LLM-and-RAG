The papers mentioned do not provide explicit details regarding the specific criteria used to split the data for deep learning model training into train, test, and validation sets. However, it can be inferred that common practices have been followed. Typically, the dataset is divided into three subsets: training, validation, and testing. The training set is used to fit the parameters of the model, while the validation set is used to tune hyperparameters and prevent overfitting. Lastly, the testing set is used to evaluate the final performance of the trained model. In the given context, the paper by Probst et al. discusses hyperparameters and tuning strategies for Random Forest models but does not explicitly mention how they split their data. Similarly, other papers use various machine learning techniques like Support Vector Machines (SVM) and Random Forest (RF) without specifying the exact splitting methodology. Therefore, it cannot be definitively stated what criteria were used to split the data for deep learning model training in these studies.
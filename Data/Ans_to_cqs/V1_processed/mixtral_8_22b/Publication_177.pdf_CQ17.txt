In the given context, there isn't any explicit mention of specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it does state that one of the hyperparameters optimized during training was'regularization strength'. This suggests that some form of regularization was indeed applied. Regularization techniques help to reduce overfitting by adding a penalty term to the loss function, which discourages complex models. Common types of regularization include L1 and L2 regularization, also known as Lasso and Ridge regression respectively, and Dropout. L1 regularization adds an absolute value of the magnitude of coefficients as penalty term, while L2 regularization adds the squared magnitude of coefficients. Dropout is another popular regularization technique where randomly selected neurons are ignored or 'dropped out' during training, which helps to make the model more robust and prevents overfitting. Without further information, it cannot be determined exactly which type of regularization was used in this case.
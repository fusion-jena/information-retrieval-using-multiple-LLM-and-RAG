After the model training process, several postprocessing steps can be performed depending on the type of analysis being conducted. In this particular study, it appears that the authors focused on evaluating the performance of their models using various metrics such as Area Under Curve (AUC), True Positive Rate (TPR), Cohen's kappa coefficient (Kappa), and True Skill Statistic (TSS). These metrics help assess the accuracy and reliability of the models by comparing predicted values against actual observations. Additionally, they also calculated omission rates, which indicate how well the models correctly exclude absences.

The authors used two separate test datasets to evaluate their models, applying the same threshold (the 10th percentile training presence) to ensure comparability between them. This approach allowed them to compare model evaluation metrics directly. They also mentioned subjecting each of the Integrated SDMs (iSDMs) to five treatments to test the impact of potential biases and errors, including random backgrounds and spatial filtering input data. However, there is no explicit mention of saliency maps or confusion matrices in the given context. Therefore, these techniques do not appear to have been employed in this study.
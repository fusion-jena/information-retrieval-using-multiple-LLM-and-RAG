The authors did not explicitly mention any specific techniques such as diverse datasets, cross-validation, or stratified splitting to ensure the generalizability of their deep learning model. However, they employed several strategies to optimize the hyperparameters of their machine learning models, including Random Forest, K-Nearest Neighbors, AdaBoost, and Artificial Neural Network. They utilized a randomized grid search method (Bergstra & Bengio, 2012), which helped them strike a balance between model complexity and the dataset while minimizing the risk of overfitting. By randomly selecting subsets of the parameter grid instead of exhaustively searching through all possible combinations, they reduced computational costs and found equally or more optimal solutions compared to an exhaustive grid search. The hyperparameters obtained after tuning each machine learning model for CHL-a, SD, and TSS are presented in Table 1. Although these optimization techniques do not directly address the issue of generalizability, they contribute to building robust and accurate models that could potentially perform better on unseen data.
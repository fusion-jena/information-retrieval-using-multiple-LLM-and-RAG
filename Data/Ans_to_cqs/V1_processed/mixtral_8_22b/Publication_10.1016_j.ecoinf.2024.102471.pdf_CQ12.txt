The pipeline uses a hybrid model that combines Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), known as Hybrid CNN-LSTM. This architecture leverages the strengths of both types of networks. CNNs are excellent at extracting spatial information, such as edges, textures, and shapes, from inputs. On the other hand, LSTMs excel at handling sequential data, making them ideal for tasks involving time series or sequences, such as language translation or speech recognition. Combining these two architectures enables the model to capture both spatial and temporal patterns in the data. Additionally, the pipeline also employs a transformer-based model called Wav2Vec, which is specifically designed for automatic speech recognition and other audio-related tasks. It utilizes a self-supervised learning approach, allowing it to pre-train on unlabeled data before being fine-tuned on a specific task.
The text does not provide explicit information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, based on common practices in deep learning, popular regularization techniques include Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust by preventing complex co-adaptations on training data. On the other hand, L2 Regularization adds a penalty term to the loss function, encouraging smaller weights and reducing overfitting. Other regularization methods may include early stopping, weight decay, and data augmentation.
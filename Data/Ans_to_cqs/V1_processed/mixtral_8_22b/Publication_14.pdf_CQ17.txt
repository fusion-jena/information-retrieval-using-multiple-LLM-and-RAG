One of the regularization methods used to prevent overfitting in the deep learning pipeline mentioned in the given context is dropout. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents it from relying too heavily on any single input feature. In the paper by Srivastava et al. (2014), dropout is described as a method that randomly removes a proportion of nodes to reduce overfitting. Other common regularization techniques include L2 regularization, also known as weight decay, which adds a penalty term to the loss function based on the magnitude of the weights, encouraging smaller weights and reducing overfitting. However, the use of L2 regularization is not explicitly mentioned in the provided context.
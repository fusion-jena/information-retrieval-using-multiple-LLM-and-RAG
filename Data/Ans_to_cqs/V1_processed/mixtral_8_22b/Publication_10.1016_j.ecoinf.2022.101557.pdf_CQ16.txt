The text does not explicitly state the specific criteria used to determine when training is complete. However, it mentions that the model is trained for 150 epochs. An epoch refers to one cycle through the full training dataset, so this suggests that the number of epochs could be used as a criterion to stop training. Additionally, the text mentions the use of dropout regularization and ADAM optimization with a learning rate of 0.0001, which are techniques often employed to prevent overfitting and improve convergence respectively. Therefore, while not directly stated, these factors might also influence the decision on when to stop training.
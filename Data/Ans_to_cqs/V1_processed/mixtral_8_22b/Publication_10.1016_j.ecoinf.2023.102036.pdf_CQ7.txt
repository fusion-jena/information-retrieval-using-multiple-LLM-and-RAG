Based on the given context, there isn't explicit information about how the data was split for deep learning model training (e.g., train, test, validation sets). However, some insights can be derived indirectly.

In the context, it mentions that each optimization run consisted of 3000 train iterations with a batch size of 25 individuals to pick a pair of similar and a pair of dissimilar photos (i.e., a batch size of 100 images), and validating each 300 iterations. From this statement, one could infer that the data might have been divided into a training set and a validation set. The training set would consist of the 3000 train iterations, while the validation set would be used every 300 iterations to evaluate the model's performance during training.

Additionally, TensorBoard, a tool for visualizing and analyzing machine learning experiments, was used to study the performance of the runs. This further supports the idea that the data was likely divided into a training set and a validation set, as TensorBoard allows users to monitor various metrics such as loss and accuracy over time, which are typically calculated using separate training and validation datasets.

However, without more specific details regarding the splitting criteria or any mention of a testing set, it cannot be definitively stated what exact criteria were used to split the data for deep learning model training.
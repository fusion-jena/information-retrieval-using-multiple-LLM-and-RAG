In this case, the text does not explicitly mention any optimization techniques typically associated with deep learning pipelines like Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). Instead, the focus is primarily on hyperparameter optimization for machine learning models, specifically XGBoost and LightGBM.

Two types of optimization techniques are mentioned in the given context: Bayesian Optimization (BO) and Tree Parzen Estimator based Optimizer (TDO). These techniques are employed to find the best set of hyperparameters for the machine learning models, aiming to minimize Root Mean Square Error (RMSE) as the objective function.

Bayesian Optimization uses probabilistic models to guide the search for the optimal hyperparameters by iteratively updating its belief about the underlying function being optimized. On the other hand, TDO employs a kernel density estimator called Tree Parzen Estimator to approximate the distribution of high-performing hyperparameters.

Additionally, BOHB (Bayesian Optimization with HyperBand) is briefly mentioned but not elaborated upon in detail within the provided context. It combines Bayesian Optimization with Hyperband, a multi-armed bandit strategy designed to efficiently allocate resources when searching for the best configuration.

To summarize, while the given context does not discuss traditional deep learning optimization techniques like SGD or Adam, it focuses on hyperparameter optimization techniques for machine learning models, namely Bayesian Optimization and Tree Parzen Estimator based Optimizer.
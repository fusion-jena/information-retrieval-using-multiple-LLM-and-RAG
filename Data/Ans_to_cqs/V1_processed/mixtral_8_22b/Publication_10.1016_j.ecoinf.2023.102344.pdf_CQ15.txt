The text mentions two optimization algorithms used in the deep learning pipeline: Stochastic Gradient Descent with Momentum (SGDM) and Adaptive Moment Estimation (ADAM). In Approach 2, SGDM was used as the optimization algorithm for all pre-trained models, allowing them to converge after 1000 epochs. ADAM was initially considered but appeared to converge on a sharp minimum, leading to negligible accuracy, and thus was not further considered in the study. These optimization techniques help improve the efficiency and effectiveness of the deep learning process by adjusting the model's weights during training based on the calculated gradient of the loss function.
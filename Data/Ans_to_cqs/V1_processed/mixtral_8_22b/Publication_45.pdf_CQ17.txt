In the given context, it is mentioned that one of the hyper-parameters evaluated during the training process of the deep learning pipeline was the applied regularization techniques. However, the specific regularization methods used to prevent overfitting are not explicitly stated. Nevertheless, based on common practices in deep learning, some potential regularization methods that could have been employed include dropout and L2 regularization.

Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training, which helps to prevent complex co-adaptations between neurons and reduce overfitting (Srivastava et al., 2014). On the other hand, L2 regularization involves adding a penalty term to the loss function, which discourages large weight values and encourages smaller ones, thereby reducing complexity and preventing overfitting. Other regularization methods such as early stopping, batch normalization, and data augmentation could also be considered depending on the specific requirements of the problem being addressed.
The context does not provide explicit details about specific preprocessing steps such as normalization, scaling, or cleaning applied before training the deep learning models mentioned (EfficientNet-B0, ResNet101, MobileNetV2, and InceptionV3). However, it can be inferred that some form of preprocessing might have been performed on the raw bivalve images used as inputs for these optimized pre-trained networks.

For instance, the use of global average pooling (GAP) in the architectures suggests that the spatial dimensions of the feature maps are reduced to a single value, effectively summarizing the presence of each feature across the entire image. This could imply that the input images were rescaled or normalized to ensure consistent size and intensity levels before feeding them into the networks.

Additionally, the mention of label smoothing enhancement in InceptionV3 indicates that the ground truth labels may have undergone some form of processing to prevent overconfidence in predictions. Label smoothing involves modifying one-hot encoded labels to introduce uncertainty, encouraging the model to produce more conservative probability estimates.

While the context does not explicitly state any preprocessing steps, it is common practice in deep learning applications to perform tasks like normalization, scaling, and cleaning to improve model performance and convergence speed. Therefore, it can be assumed that similar preprocessing techniques would likely have been employed here as well.
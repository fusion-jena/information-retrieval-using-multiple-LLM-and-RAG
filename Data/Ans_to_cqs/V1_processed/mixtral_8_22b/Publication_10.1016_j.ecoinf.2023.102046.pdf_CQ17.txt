Regularization methods are employed in deep learning pipelines to prevent overfitting and improve generalization. Two common regularization techniques mentioned in the given context are dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps reduce interdependent learning among them. This method is not explicitly mentioned in the provided text, but it is commonly used in deep learning architectures like those discussed here. On the other hand, L2 regularization is directly referenced in the context. Also known as weight decay, this method adds a penalty term to the loss function based on the square of the magnitude of coefficients. By doing so, it encourages smaller weights and prevents any single feature from dominating the decision boundary. In the given context, L2 regularization is applied in the Ridge Regression model, where the regularization parameter alpha controls the strength of the L2 term.
In order to address data bias during preprocessing of the deep learning pipeline, various techniques have been employed. One common approach is stratified splitting, which involves dividing the dataset into strata or groups based on certain criteria, such as class labels, before splitting it into training and testing sets. This ensures that both sets contain a balanced representation of all classes. Another technique is oversampling, which involves duplicating samples from minority classes to balance out the distribution of classes in the dataset. Conversely, undersampling involves removing samples from majority classes to achieve the same goal. Additionally, collecting diverse data can help mitigate biases introduced by limited or skewed data sources. In the given context, the authors addressed data bias by creating a separate geometry called 'add trees', where well-defined examples of woody cover elements were added until the two classes had the same number of training points. This new geometry was then merged with the randomly allocated woody points to create a more balanced training set. Furthermore, the authors used a combination of random and stratified sampling approaches to generate training/validation points, ensuring that their dataset contained a representative sample of both non-woody and woody classes.
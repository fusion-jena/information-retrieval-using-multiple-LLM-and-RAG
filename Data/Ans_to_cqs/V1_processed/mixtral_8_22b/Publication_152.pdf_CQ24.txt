To handle randomness in the deep learning pipeline, several strategies can be employed. One common approach is to use a fixed random seed value for reproducibility purposes. This ensures that the same results can be obtained when running the code multiple times. Another strategy is to perform multiple runs with different random seeds and then average the results or select the best performing model based on validation metrics. Additionally, techniques such as cross-validation and bootstrapping can also help mitigate the effects of randomness by providing more robust estimates of performance. In terms of specific methods used in this work, it appears that data augmentation was employed to increase the F1 score, which may have helped reduce overfitting caused by randomness in the training data. However, no explicit mention of handling randomness in the deep learning pipeline is made in the given context.
In this study, the authors use the stochastic gradient descent with momentum optimizer for training their deep learning model, specifically AlexNet. They mention setting the initial learning rate to 0.001 and the maximum number of epochs to 15 after some preliminary tests. However, it is important to note that other optimization techniques such as Adam, RMSprop, or AdaDelta could also have been used depending on the requirements and constraints of the problem being solved. Each optimization technique has its own strengths and weaknesses, and choosing the right one often involves experimentation and fine-tuning.

Unhelpful 
I am not sure what optimization techniques are applied in the deep learning pipeline. I do not have enough information to provide an accurate response.
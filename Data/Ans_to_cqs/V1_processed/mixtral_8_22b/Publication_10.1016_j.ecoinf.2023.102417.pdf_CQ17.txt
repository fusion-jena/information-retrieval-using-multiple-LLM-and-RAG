Regularization methods are techniques used in machine learning and deep learning pipelines to prevent overfitting. Overfitting occurs when a model learns the noise in the training data instead of generalizing from the underlying patterns. Two common regularization methods used in deep learning are Dropout and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training. They are "dropped out" randomly, which helps to avoid over-dependence on any particular set of neurons. By dropping out these neurons, the network becomes more robust and less likely to overfit. During testing, all neurons are included but their outputs are scaled down by the dropout rate to maintain consistency.

L2 Regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the magnitude of the weights. This encourages smaller weights, effectively limiting the complexity of the learned model and reducing overfitting. It does so by adding a cost to the loss function for large weights, making the optimization process prefer smaller weights unless they significantly improve performance.

In summary, both Dropout and L2 Regularization are effective ways to combat overfitting in deep learning models. Dropout achieves this by preventing the model from becoming too dependent on individual neurons, while L2 Regularization limits the size of the weights, thereby controlling the model's complexity.
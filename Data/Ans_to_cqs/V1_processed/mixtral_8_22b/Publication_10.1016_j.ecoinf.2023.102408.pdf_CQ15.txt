Based on the given context, there isn't any explicit mention of specific optimization techniques like Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being used in the deep learning pipeline. However, the text does indicate that the learning rate and weight are used as calibration parameters to improve the results in the artificial neural network (ANN) model. Additionally, the Nguyen-Widrow method is mentioned for initializing the weights randomly. Furthermore, the regularization parameter 'decay' is used to prevent overfitting. In the case of support vector machines (SVM), the parameter 'c' is utilized to optimize the line fit and penalize the number of samples within the margin. Therefore, while the exact optimization techniques aren't specified, the use of learning rates, weight initialization methods, and regularization parameters suggest that some form of optimization is indeed taking place in the deep learning pipeline.
To ensure the generalizability of the deep learning models, several measures were taken during the research process. Firstly, the datasets were randomly divided into three parts for training, validation, and testing. This division allowed the models to learn from a diverse set of data, preventing overfitting and improving their ability to generalize to unseen data. Specifically, 60% of the data was used for training, while 20% was allocated for validation and another 20% for testing. This distribution ensured that the models could effectively learn patterns from the training data and then be validated and tested on separate sets of data.

Secondly, the use of various deep learning models such as MobileNetV2, VGG16, VGG19, ResNet-50, Inception-V3, Inception-ResNetV2, Xception, DenseNet121, DenseNet169, and DenseNet201 contributed to the generalizability of the results. Each of these models has unique architectures and strengths, allowing them to capture different features and patterns within the data. By employing multiple models, the researchers aimed to increase the robustness and reliability of their findings.

Lastly, the experimental setup included evaluating the performance of the models using precision, recall, and F1 score metrics. These metrics provided a comprehensive assessment of the models' abilities to accurately classify images. Precision measured the proportion of true positive predictions among all positive predictions, recall calculated the proportion of true positive predictions among actual positive instances, and the F1 score represented the harmonic mean of precision and recall. By considering these metrics, the researchers could better understand the strengths and weaknesses of each model and select the most suitable one(s) for their application.

Overall, the combination of diverse datasets, multiple deep learning models, and rigorous evaluation methods helped ensure the generalizability of the deep learning models in this research.
The authors use F1 scores to evaluate the performance of their deep learning model. F1 score is a measure of a test's accuracy and is calculated from the precision and recall of the test. Precision measures the proportion of true positive results against all positive results returned by the classifier, while recall measures the proportion of true positive results against all actual positive samples available during classification. The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both properties. It ranges from 0 to 1, where 1 represents perfect precision and recall. Additionally, the authors mention using mean average precision (mAP) as another standard metric for evaluating object detection models. However, they argue that mAP may not be informative for their specific study since it focuses on the exact overlap between the ground truth and predicted bounding boxes. Instead, they rely primarily on F1 scores to assess their model's performance.
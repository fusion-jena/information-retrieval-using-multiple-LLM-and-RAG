The text does not provide specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, common regularization techniques include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overfitting. L2 regularization adds a penalty term to the loss function, encouraging smaller weights and preventing any single weight from dominating the others. Other regularization methods include early stopping, data augmentation, and batch normalization. Early stopping involves monitoring the validation error while training and stopping when it starts to increase, indicating that the model may be beginning to overfit. Data augmentation generates new training examples by applying transformations such as rotation or scaling to existing images, increasing the size of the training set and making the model more generalizable. Batch normalization standardizes the inputs to each layer, improving convergence and reducing overfitting.
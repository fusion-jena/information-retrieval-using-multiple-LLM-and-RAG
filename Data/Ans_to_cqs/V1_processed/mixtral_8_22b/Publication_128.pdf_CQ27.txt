The given context does not provide explicit details about the deployment process of the trained deep learning models. However, based on common practices in machine learning projects, we can infer some possible steps taken during the deployment phase.

After training the six neural network models using the specified parameters and functions, the next step would typically involve saving or serializing these models. Serialization allows us to store the entire structure of the trained model along with its learned weights and biases. This serialized version of the model can later be loaded back into memory for making predictions on new data.

Once the models have been saved, they need to be deployed onto a suitable platform where they can receive real-world data and generate meaningful outputs. Depending upon the use case, different platforms may be chosen for deployment. Some popular choices include cloud services like AWS, Google Cloud, Azure, etc., or edge devices such as mobile phones, IoT sensors, etc.

Given that the computational resources mentioned in the context include a CentOS Linux operating system with 32 CPUs and each CPU having 64 processors, it suggests that the models might have been deployed on a powerful server capable of handling large volumes of data efficiently. Additionally, since the development environment was built using Python and packages like TensorFlow and NumPy, it indicates that the deployment could also leverage similar tools and libraries for seamless integration.

However, without specific information regarding the actual deployment process, these assumptions remain speculative. Therefore, further clarification would be required to accurately describe how the trained deep learning models were deployed.
The text does not provide specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, common regularization techniques include dropout, L1 regularization, and L2 regularization. Dropout randomly sets a fraction rate of input units to 0 at each update during training time, helping reduce overfitting. L1 regularization adds an absolute value of the weights to the loss function, while L2 regularization adds the squared magnitude of the weights to the loss function. Both L1 and L2 regularizations help prevent overfitting by adding constraints to the optimization problem.
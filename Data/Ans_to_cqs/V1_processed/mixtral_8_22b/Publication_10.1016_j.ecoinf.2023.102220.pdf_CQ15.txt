The given text does not explicitly mention any specific optimization technique used in the deep learning pipeline, such as Stochastic Gradient Descent (SGD), Adam, or others. However, since the authors mentioned using TensorFlow, which is a popular library for building and training neural networks, it can be assumed that they might have employed one of these common optimization algorithms available within TensorFlow. These include but are not limited to SGD, Adam, RMSprop, Adagrad, and Adadelta. Without further information from the original source, it cannot be definitively stated which optimization technique was utilized in their deep learning pipeline.
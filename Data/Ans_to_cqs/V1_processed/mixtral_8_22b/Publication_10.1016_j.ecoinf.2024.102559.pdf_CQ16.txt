The context does not provide explicit information on the specific criteria used to determine when training is completed. However, common practices in machine learning include monitoring the model's performance on a separate validation dataset during training. When the validation loss or error rate stops improving significantly over several epochs, it can be considered a sign that the model has converged and further training may lead to overfitting. This phenomenon is often referred to as a 'validation loss plateau.' Other possible criteria could involve setting a predefined number of epochs or iterations, or stopping when the maximum allowed time for training has been reached.
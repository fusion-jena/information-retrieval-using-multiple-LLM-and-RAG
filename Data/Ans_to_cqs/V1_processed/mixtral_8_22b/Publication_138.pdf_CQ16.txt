In machine learning, determining when training is complete depends on various factors such as the type of problem being solved, the dataset, and the specific algorithm used. However, some common criteria used to decide when training should be stopped include:

1. Validation Loss Plateau: One popular criterion is monitoring the validation loss during training. Training is often considered complete when the validation loss stops decreasing or starts increasing, indicating that the model has reached its optimal performance on unseen data.

2. Maximum Number of Epochs: Another commonly used stopping criterion is setting a predefined maximum number of epochs. An epoch refers to one pass through the entire training dataset. By limiting the number of epochs, training can be stopped even if the model hasn't fully converged but still provides acceptable performance.

3. Early Stopping: Early stopping involves tracking the validation error during training and halting the process once it begins to increase. This technique helps prevent overfitting by stopping the training before the model becomes too complex and starts memorizing the training data instead of generalizing patterns.

4. Learning Rate Scheduling: Some models use adaptive learning rates that decrease over time. Once the learning rate reaches a sufficiently low threshold, training can be terminated since further improvements would likely require extensive computational resources without significant gains in performance.

Based on the given context, it appears that the authors have used the concept of epochs to monitor their model's progress. They observed that as the mini-batch size increases, so does the number of epochs required for convergence. Additionally, they noted that a higher number of epochs corresponds to longer training times. Therefore, finding an appropriate balance between mini-batch size and the number of epochs is essential for efficient training.
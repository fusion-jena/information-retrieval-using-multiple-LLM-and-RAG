Regularization methods are crucial in preventing overfitting in deep learning pipelines. Two commonly used regularization methods mentioned in the given context are Dropout and Weight Decay (L2 Regularization).

Dropout is a technique where randomly selected neurons are ignored or "dropped out" during training, which helps to make the model more robust by reducing its dependence on any single feature. It can be seen in the 'No-Overfitting' scenario where a dropout rate of 0.5 is employed.

Weight Decay, also known as L2 Regularization, adds a penalty term to the loss function proportional to the square of the magnitude of all trainable parameters. This encourages smaller weights, leading to simpler models that generalize better. In the given context, a weight decay of 0.5 is utilized in the 'No-Overfitting' setup.

Additionally, another method not explicitly mentioned but implied through the use of Stochastic Gradient Descent (SGD) is Early Stopping. Although not directly listed as a regularization method, early stopping involves monitoring the validation error while training and halting the process once the error stops decreasing. This prevents the model from fitting too closely to the training data.

Lastly, Batch Normalization is another technique used to combat overfitting. By normalizing the activations of each layer, it reduces internal covariate shift, making the network easier to train and improving its ability to generalize. In the provided context, Batch Normalization is used in the 'No-Overfitting' case but removed in the 'Overfitting' scenario to intentionally induce overfitting.
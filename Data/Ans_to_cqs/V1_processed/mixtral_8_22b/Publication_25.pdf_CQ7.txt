The text does not provide explicit information about the specific criteria used to split the data for deep learning model training into sets such as train, test, and validation. However, it does mention several aspects related to data usage in deep learning models.

For instance, the text discusses the use of both high-quality annotations and moderate to low-quality annotations for training deep learning models. High-quality annotations refer to manually annotated data, while lower-quality annotations might come from unsupervised methods. The authors also suggest that a combination of these two types of data could potentially improve deep learning performance on limited training data.

Additionally, the text mentions the importance of having sufficient training data to prevent overfitting, especially in cases where there is high variability in the target object's appearance. Overfitting occurs when a model performs well on its training data but poorly on new, unseen data because it has essentially memorized the training set rather than learned generalizable patterns. To mitigate this issue, the authors propose using self-supervised learning, which generates additional training data through unsupervised methods.

Lastly, the text suggests that improving deep learning models for tasks like tree detection could involve using a combination of better validation data and more hand-annotated training samples. Validation data is typically used during the model development process to fine-tune hyperparameters and assess how well the model generalizes to unseen data.

In summary, although the text does not explicitly state the criteria used to split the data for deep learning model training, it highlights the importance of having diverse and abundant training data, including both high-quality manual annotations and lower-quality automated labels generated via self-supervised learning. Furthermore, the text emphasizes the role of validation data in refining and evaluating deep learning models.
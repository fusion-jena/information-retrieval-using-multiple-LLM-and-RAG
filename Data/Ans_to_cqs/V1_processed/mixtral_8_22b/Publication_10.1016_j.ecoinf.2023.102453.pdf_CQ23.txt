Based on the given context, there isn't explicit mention of specific measures taken to ensure the generalizability of the deep learning model such as diverse dataset, cross-validation, or stratified splitting. However, some relevant points can still be highlighted.

Firstly, the study mentioned evaluating the model on two different datasets, although they were smaller compared to the ImageNet dataset used for pre-training. This suggests that the authors aimed to test the model's adaptability across various datasets, contributing to its potential generalizability.

Secondly, while not explicitly stated, the use of pre-trained models implies a form of transfer learning, where knowledge gained from one task is leveraged to improve performance on another related task. Transfer learning often enhances the model's ability to generalize well to new tasks or datasets.

Lastly, the discussion about hyperparameter optimization indicates efforts towards improving the model's performance, which may indirectly contribute to its generalizability. Optimal hyperparameters can help prevent overfitting, thereby making the model more robust and applicable to varied scenarios.

In conclusion, based on the available context, we cannot definitively state what specific measures were taken to ensure the generalizability of the deep learning model. However, the use of multiple datasets, pre-trained models, and hyperparameter optimization suggest attempts towards achieving this goal.
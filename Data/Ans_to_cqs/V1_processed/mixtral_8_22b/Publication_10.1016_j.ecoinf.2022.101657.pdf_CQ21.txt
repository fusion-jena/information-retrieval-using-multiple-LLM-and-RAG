After the model training process, several postprocessing steps can be performed depending on the specific requirements of the project. Here are some common postprocessing techniques:

1. Model evaluation metrics: Calculating various performance metrics such as accuracy, precision, recall, F1-score, ROC curves, etc., helps assess the effectiveness of the trained model.

2. Confusion matrices: Creating confusion matrices provides insights into the types of errors made by the model during classification tasks. This visual representation shows the number of correctly classified instances versus misclassified ones.

3. Saliency maps: Generating saliency maps highlights the regions within an image that contribute most significantly to the model's decision-making process. This technique can help improve interpretability and understanding of deep learning models.

4. Hyperparameter tuning: Adjusting hyperparameters like learning rate, batch size, regularization strength, etc., based on the results of initial training runs can lead to improved model performance.

5. Ensemble methods: Combining multiple models using ensemble methods like bagging, boosting, stacking, etc., can often result in better overall performance compared to individual models.

In the given context, there isn't any explicit mention of postprocessing steps taken after model training. However, it mentions that the official, MS-COCO pre-trained version of the architecture was further trained using the WCS dataset and achieved a credible mean average precision (mAP50) score of 0.75. This indicates that the authors evaluated the model's performance using the mAP50 metric, which is one of the possible postprocessing steps mentioned above.
The given context does not directly discuss techniques used to address data bias during preprocessing of the deep learning pipeline such as stratified splitting, oversampling, undersampling, or diverse data collection. However, it does mention two relevant aspects related to handling data: sample thinning and selecting a classifier.

Sample thinning was discussed as having minimal impact on model accuracy or variable importance. This technique could potentially help reduce data bias by removing redundant or unnecessary samples, though it might not specifically target imbalanced datasets.

Regarding classifier selection, the text mentions several popular classifiers like General Linear Models, MaxEnt, and Random Forests. While these classifiers do not inherently mitigate data bias, they can be combined with various sampling strategies to handle imbalance issues. For instance, oversampling or undersampling techniques can be employed before training a classifier to balance the dataset. Additionally, certain algorithms like Random Forests have built-in mechanisms to deal with imbalanced data, making them suitable for addressing data bias.

In conclusion, based on the provided context, we cannot definitively state which techniques are used to address data bias during preprocessing of the deep learning pipeline. Nevertheless, sample thinning and appropriate classifier selection can contribute to managing data bias indirectly.
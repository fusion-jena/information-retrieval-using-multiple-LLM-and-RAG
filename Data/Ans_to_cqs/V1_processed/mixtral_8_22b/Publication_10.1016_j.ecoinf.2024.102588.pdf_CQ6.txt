The text does not provide explicit details about the preprocessing steps taken before training the deep learning models mentioned. However, it can be inferred that some form of preprocessing was carried out based on the use of certain techniques within the models themselves. For instance, batch normalization was used to perform feature generalization of the input mini-batch by calculating the mean and standard deviation. This process helps in avoiding covariate shifts, suggesting that some sort of normalization might have been applied during preprocessing. Additionally, dropout layers were employed to reduce overfitting by randomly inactivating nodes containing learnable weights. While these techniques do not directly indicate specific preprocessing steps like scaling or cleaning, they imply that measures were taken to ensure the quality and appropriateness of the input data for training the deep learning models.
The paper does not explicitly mention any specific regularization methods used to prevent overfitting in their deep learning pipeline. However, it does discuss some aspects related to preventing overfitting indirectly.

Firstly, the authors employ active learning techniques, where instead of training the model on all available labeled data, they strategically choose a subset of unlabeled samples to label and add them iteratively to the training set. This approach helps avoid overfitting by ensuring that the model focuses more on the challenging examples rather than memorizing the entire dataset.

Secondly, the authors use pretrained models, specifically the ResNet-50 architecture, for feature extraction. Pretrained models have been shown to generalize better due to being exposed to large amounts of diverse data during pretraining. Using these features as inputs to the classifier reduces the risk of overfitting since the model doesn't need to learn complex representations from scratch.

Lastly, while not directly mentioned as a regularization method, the authors do acknowledge the importance of tuning hyperparameters to improve results. Hyperparameter optimization can also help mitigate overfitting by finding optimal configurations that balance between underfitting and overfitting.

In conclusion, although the paper does not provide explicit details about regularization methods used, it employs several practices known to aid in avoiding overfitting, including active learning, transfer learning, and hyperparameter tuning.
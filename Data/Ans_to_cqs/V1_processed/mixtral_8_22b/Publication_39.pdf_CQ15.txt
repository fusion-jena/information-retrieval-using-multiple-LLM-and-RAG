The deep learning pipelines described in the given context primarily apply Stochastic Gradient Descent (SGD) as their optimization technique. Specifically, SGD is utilized for optimizing the SlowFast architecture, where the learning rate is initially set to 0.1 and then decreased using a cosine strategy. Additionally, SGD is employed for training the three ResNet variants, with an initial learning rate of 0.001, which is also reduced every 10 epochs by multiplying it with a gamma value of 0.1. Furthermore, another variant of SGD is used for training the models, starting with a learning rate of 0.0005, a momentum of 0.9, and a weight decay of 0.0005. Here, the learning rate is also decreased every 10 epochs by multiplying it with a gamma value of 0.1.
In addressing data bias during preprocessing of the deep learning pipeline, various techniques were employed. One technique mentioned is the use of transfer learning, which involves adapting networks trained on one task to perform similar tasks. This method proved particularly beneficial when dealing with sparse and narrowly distributed datasets. In this case, the researchers utilized EfficientNet, a pre-trained network that had been trained on the large benchmark dataset, ImageNet. By leveraging these pre-existing models, the researchers were able to take advantage of the network's ability to abstract lower-level features, which had already been honed through extensive training on a vast and varied dataset.

Additionally, the researchers also addressed potential reader bias in the labeling process. They did so by employing a group average approach, wherein multiple readers labeled each sample independently. This strategy aimed to mitigate individual reader biases and ensure that the resulting labels reflected a more balanced perspective. Consequently, the trained convolutional neural network (CNN) tended to predict the group average rather than being influenced by any particular reader's interpretation.

However, it appears that specific techniques like stratified splitting, oversampling, undersampling, or diverse data collection were not explicitly discussed in the given context. These methods are commonly used to manage class imbalance and improve model generalization, but there is no direct mention of them in relation to this research. Therefore, while we cannot definitively state whether these techniques were applied, we can confirm that transfer learning and a group averaging approach were implemented to tackle data bias during preprocessing.
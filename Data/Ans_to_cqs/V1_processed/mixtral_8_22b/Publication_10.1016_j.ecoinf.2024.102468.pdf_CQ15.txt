Unfortunately, none of the provided sources directly discuss specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) used in the deep learning pipeline. These methods are commonly employed to update neural network weights during training, aiming to minimize loss functions and improve model performance.

However, some insights can still be drawn from the given information. For instance, the Support Vector Machine (SVM) detector mentioned in one source uses a 'kernel trick,' which allows it to handle nonlinear relationships in high-dimensional datasets by transforming them into linear ones. Although not explicitly stated, this process might involve optimization techniques similar to those found in deep learning pipelines.

Additionally, another study mentions using Hidden Markov Models (HMM) and Artificial Neural Networks (ANN). Both these models require parameter tuning during their respective training processes, often involving optimization algorithms like gradient descent variants.

Lastly, while discussing wavelet transformation methods, one reference cites Kaiser & Hudgins (1994)'s work titled "A Friendly Guide to Wavelets." Though unrelated to deep learning per se, understanding signal processing techniques could indirectly help optimize input features before feeding them into any machine learning algorithm, including deep learning models.

In conclusion, based on the available resources, we cannot pinpoint exact optimization techniques used in the deep learning pipeline. Nevertheless, there are hints suggesting possible applications of optimization strategies in handling complex datasets and improving model performances.
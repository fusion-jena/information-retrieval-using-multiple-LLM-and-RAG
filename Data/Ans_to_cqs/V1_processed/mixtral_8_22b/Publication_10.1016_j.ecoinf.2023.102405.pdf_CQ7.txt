When splitting data for deep learning model training, there are several common approaches based on different criteria. The most popular methods include random sampling, stratified sampling, and temporal or sequential splitting.

1. Random sampling: This method involves randomly dividing the dataset into three subsets - training set, validation set, and testing set. Typically, the ratio is 60:20:20 or 70:15:15, but it can vary depending on the specific use case. Random sampling ensures that all classes have equal representation across datasets. However, this approach may not be suitable when dealing with imbalanced datasets.

2. Stratified sampling: Similar to random sampling, stratified sampling also divides the dataset into three subsets. However, instead of purely random selection, stratified sampling maintains the same distribution of classes within each subset. This means that if a class has fewer samples compared to others, they will still maintain their proportion in each subset. Stratified sampling is particularly useful when working with imbalanced datasets.

3. Temporal or sequential splitting: When dealing with time series data, such as stock prices or weather forecasts, it is essential to preserve the chronology of events during data partitioning. Therefore, one should avoid random or stratified sampling techniques. Instead, the dataset should be divided by selecting consecutive observations from the beginning until the desired number of instances is reached for each subset. For example, the first 60% of the data could be allocated for training, followed by 20% for validation, and the remaining 20% for testing.

In summary, the choice of data splitting criterion depends on the nature of the problem being addressed and the characteristics of the available dataset. It is crucial to ensure that the chosen method accurately reflects the underlying patterns and relationships present in the data while avoiding any potential biases or misrepresentations.
To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the model was constructed using the EfficientNet B0 network, which had been pretrained for generic image classification. This allowed the model to leverage the knowledge gained from this prior training, improving its ability to classify images accurately. Secondly, transfer learning was applied by freezing the EfficientNet feature extraction layers while only updating the weights of the final dense classification layers during training. This approach enabled the model to learn more specific features relevant to the task at hand without losing the valuable information obtained from the initial training. Thirdly, the model was fine-tuned using Gulf of Mexico data, with the feature extractor remaining frozen. During this process, a cyclical learning rate was used, and early stopping was deployed if the validation loss did not improve within 10 epochs. Additionally, a dropout rate of 0.2 was implemented along with DropConnect, further enhancing the model's robustness and preventing overfitting. Lastly, the model was trained within the Google Collaboratory 'Colab' platform using the Tesla K80 GPU, providing access to powerful computational resources through cloud computing. These combined strategies helped ensure the model's adaptability and applicability across various datasets and scenarios.
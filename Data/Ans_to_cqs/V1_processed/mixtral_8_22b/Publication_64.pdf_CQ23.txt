To ensure the generalizability of their deep learning model, several measures were taken. Firstly, they selected a diverse dataset consisting of 3680 labels, which represents approximately 60% of all labels in their training dataset. These labels were chosen based on having more than 12 images per label. To further increase diversity, they utilized the Google Image Search engine to automatically retrieve the first 10 images for each label. Although there may be minor mislabeled or corrupted examples in the dataset, selecting popular labels and top search results minimizes such occurrences.

Secondly, they employed data augmentation techniques during both training and testing phases. Standard data augmentation methods, including sheering, translating, and mirroring, were applied during training to prevent the network from seeing identical images repeatedly. However, rotational and upside-down mirroring transformations were avoided for 'habitat' labeled images, as these modifications would not make logical sense for trees or landscapes. Additionally, smaller data augmentation parameters were used during testing compared to those implemented during training.

Lastly, the authors initialized the weights of their model using pretrained weights from the ImageNet dataset, which was provided through the Lasagne Model Zoo. They then trained the model for 100 epochs on a GTX 1080 GPU utilizing the Adam optimization algorithm.

While the text does not explicitly mention cross-validation or stratified splitting, the combination of employing a diverse dataset, implementing data augmentation techniques, and initializing the model with pretrained weights contributes significantly towards ensuring the generalizability of the deep learning model.
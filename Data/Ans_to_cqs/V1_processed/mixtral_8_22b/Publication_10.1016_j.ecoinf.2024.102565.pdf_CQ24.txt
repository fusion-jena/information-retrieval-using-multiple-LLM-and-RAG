The text does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, it mentions that during the validation phase of the machine learning algorithms, a K-fold cross-validation technique was used. Cross-validation helps reduce overfitting and provides a more accurate estimate of how well the model will generalize to new data. Furthermore, a four-fold cross-validation was performed on the training dataset, where each fold served as the validation set, while the remaining three folds formed the training set for model training. This approach can help manage some aspects of randomness in the deep learning pipeline.

Additionally, the text discusses addressing imbalances in the dataset using random under-sampling (RUS) applied to the training set. Random undersampling involves removing instances from the majority class to balance the classes. While this method doesn't directly address randomness related to the deep learning pipeline, it contributes to improving the overall performance of the model by reducing bias towards the majority class.
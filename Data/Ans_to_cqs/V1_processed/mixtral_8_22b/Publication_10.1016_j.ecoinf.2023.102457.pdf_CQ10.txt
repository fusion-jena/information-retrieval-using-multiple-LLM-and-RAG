The code repository link of the deep learning pipeline is <https://github.com/kahst/BirdNET-Analyzer>. It is mentioned in the given context that the model and code were run in Python version 3.10 based on the code and methods presented in this repository.

Based on the provided context, it appears that the researchers used several approaches to extract features from audio recordings of gibbons, including Mel Frequency Cepstral Coefficients (MFCCs), BirdNET, VGGish, Wav2Vec2, and acoustic indices. They then used these features to train a supervised classifier, specifically a random forest algorithm, to classify female gibbon calls. To evaluate the performance of these different feature extraction methods, they performed 20 iterations of cross-validation and reported the classification accuracy, mean, and standard deviation for each method.

One interesting finding from their study was that the BirdNET model, which had been trained on audio data of bird vocalizations, outperformed both VGGish and Wav2Vec2, which had been trained on YouTube audio data and human speech audio data, respectively. This suggests that training a model specifically on gibbon vocalization data could potentially improve its performance for discriminating between individual gibbons. However, the authors note that collecting and processing such a large dataset would be computationally expensive and time-consuming.
The provided context does not directly address how the hyperparameters of the model are optimized using techniques like grid search or random search. However, it discusses some optimization strategies related to the implementation of the particle filter algorithm.

In section 5.4, the text mentions that most of the computation time is spent on projecting particles forward to the next time step. It also suggests that the distribution of computational resources may differ based on factors like state dimensionality and particle interaction. For applications with high state dimensionality or substantial particle interaction, alternative resampling methods could help mitigate the computational costs associated with particle interactions. Additionally, implementing the algorithm in different programming languages might yield slight variations in resource allocation among tasks.

Section 3.5 touches upon tuning thread and block sizes when working with GPU architectures like NVIDIA's CUDA. By minimizing the number of idle threads, one can improve overall performance. While this information doesn't explicitly cover hyperparameter optimization techniques, it highlights the importance of fine-tuning certain parameters to achieve optimal results.

Based on the available context, we cannot definitively determine which specific method—grid search, random search, or another technique—is used to optimize the hyperparameters of the model. Further context would be required to provide a more precise response.
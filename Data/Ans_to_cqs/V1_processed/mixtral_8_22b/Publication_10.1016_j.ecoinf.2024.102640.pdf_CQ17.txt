The context does not provide specific information about the regularization methods used in the deep learning pipeline mentioned in the text. However, some common regularization techniques used to prevent overfitting in deep learning models include dropout, L1/L2 regularization, early stopping, and data augmentation. Dropout randomly sets a fraction of input units to 0 during training time, helping to avoid overfitting. L1/L2 regularization adds a penalty term to the loss function, encouraging smaller weights and reducing complexity. Early stopping stops training when validation performance starts degrading, preventing overfitting due to excessive training. Data augmentation generates new training samples from existing ones through transformations like rotation or scaling, increasing dataset size and diversity.
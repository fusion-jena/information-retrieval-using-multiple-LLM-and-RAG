To ensure the generalizability of their deep learning models, several measures were implemented during the development process. Firstly, the authors utilized diverse datasets, which included raw trajectory data. This allowed them to extract meaningful features from the data, contributing to the robustness of the models. Secondly, they performed extensive hyperparameter tuning, exploring various combinations of learning rates, batch sizes, and latent dimensions. By evaluating these configurations, they aimed to find optimal settings that would improve the overall performance and applicability of their models across different scenarios. Thirdly, the researchers investigated the use of both Cross-Entropy and Focal Loss functions, along with different regularization techniques such as weight decay and dropout. These strategies helped prevent overfitting and enhanced the ability of the models to generalize better on unseen data. Lastly, although not explicitly mentioned in the given context, cross-validation or stratified splitting could potentially have been employed to further validate the models' performance and assess their generalizability. However, without specific information regarding these methods, we cannot confirm if they were indeed part of the study.
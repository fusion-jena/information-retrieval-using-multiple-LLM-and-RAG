Based on the given information, it appears that the strategy used to monitor the model performance during training involved testing the model on a separate un-annotated dataset after retraining it on all the previously annotated scans. Visual inspection was initially used to determine if the model performed well or not. To further evaluate the model's performance using an average precision metric, approximately 200 of these scans were manually annotated based on the predicted bounding boxes. These annotations were then compared with the original predictions to assess the precision of the organ detection model on this new dataset. Additionally, Tables 4 and 5 provide specific metrics such as AP50, AP75, and AP for evaluating the model's performance. However, there is no explicit mention of any real-time monitoring strategies like early stopping or learning rate adjustment during the training process.
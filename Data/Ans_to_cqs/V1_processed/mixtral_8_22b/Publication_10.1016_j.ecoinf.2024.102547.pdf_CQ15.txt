The given context does not provide information on the specific optimization techniques used in the deep learning pipeline. However, common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), Root Mean Square Propagation (RMSProp), and AdaGrad. These techniques help minimize the loss function during training, enabling the model to learn effectively from the input data. Without explicit mention in the provided context, it cannot be determined which of these optimization techniques were employed in the deep learning pipeline.
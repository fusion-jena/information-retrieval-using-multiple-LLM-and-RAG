The authors utilize Stochastic Gradient Descent (SGD) as their optimization technique in the deep learning pipeline. They specifically use the SGD optimizer with a momentum value of Î¼ = 0.9 to train the model. No other specific optimization techniques such as Adam, RMSprop, or Adagrad are mentioned in the given context.
Based on the given context, there isn't any explicit mention of strategies employed to handle randomness in the deep learning pipeline, specifically related to random seed values. However, some commonly used techniques to manage randomness in deep learning pipelines include:

1. Seed Fixing: One popular method for handling randomness is fixing the seed value before running experiments. By setting a fixed seed, one ensures reproducibility across different runs of the same experiment. This technique helps in debugging and comparing results between multiple runs.

2. Ensemble Methods: Another strategy to deal with randomness involves using ensemble methods. These methods combine predictions from multiple models, each initialized with different seeds, to improve overall performance and reduce variance caused by random initialization.

3. Data Augmentation: Randomness can also be introduced through various forms of data augmentation. Techniques like rotation, scaling, cropping, etc., help create new training examples and increase the robustness of the model against variations in the input data.

4. Dropout Regularization: Dropout regularization is another way to introduce controlled randomness during training. It randomly drops out neurons during training, which helps prevent overfitting and improves generalization capabilities.
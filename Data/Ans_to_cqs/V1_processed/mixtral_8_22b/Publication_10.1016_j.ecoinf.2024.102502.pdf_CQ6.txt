Before training a deep learning model, several preprocessing steps can be taken to ensure optimal performance. These include data cleaning, normalization, and scaling.

Data cleaning involves removing any irrelevant or incorrect data points from the dataset. This could involve handling missing values, outliers, or duplicates. Normalization is used to rescale the input features so they have similar magnitudes, typically between 0 and 1. This helps prevent certain features from dominating others due to their larger numerical ranges. Scaling is another technique used to transform the feature values into a specific range, such as [-1, 1] or [0, 1]. It ensures that all features contribute equally to the learning process.

In this particular case, the authors discuss data scaling as part of the preprocessing steps. They mention different scaling domains proposed by Fritsch et al. (2020) and follow a "pre-modelling scaling" protocol. The goal is to aggregate data across scales before modeling. Spatial and temporal scaling are considered, where spatial scaling applies to each metric of the time series using a parametrizable function like addTexture_3 Ã— 3_Mean. Temporal scaling uses formulas such as mean, median, or standard deviation to temporally aggregate data.

These preprocessing techniques help improve the accuracy and efficiency of the deep learning models by ensuring that the input data is clean, normalized, and scaled appropriately.
In the given context, there is no explicit mention of regularization methods used to prevent overfitting in the deep learning pipeline. However, it can be inferred that the authors did consider overfitting when designing their artificial neural network (ANN) model. They mentioned that adding any hidden layers resulted in less accuracy of the prediction on the test data due to overfitting. This suggests that they might have experimented with different architectures or hyperparameters to find the optimal balance between underfitting and overfitting.

Regularization methods such as dropout and L2 regularization are commonly used techniques to prevent overfitting in deep learning pipelines. Dropout involves randomly dropping out neurons during training, which helps reduce co-dependencies among them and encourages the network to learn more robust features. On the other hand, L2 regularization adds a penalty term to the loss function, discouraging large weights and promoting smaller ones, thus preventing overfitting. Other popular regularization methods include early stopping, weight decay, and batch normalization. These techniques help improve generalization performance and prevent overfitting by introducing some form of constraint or noise into the learning process.
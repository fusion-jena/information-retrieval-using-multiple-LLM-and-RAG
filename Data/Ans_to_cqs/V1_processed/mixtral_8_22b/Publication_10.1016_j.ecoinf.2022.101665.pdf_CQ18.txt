In machine learning, when data is abundant, a common approach is to randomly divide the full dataset used for modeling into three parts: a training set, a validation set, and a test set. The training set is used for fitting the models, the validation set is used to estimate prediction error for model selection and hyperparameter tuning, while the test set is used for assessing the accuracy of the final model. This process is known as partitioning the dataset.

During the training phase, the model's performance is monitored using the validation set. This is done by estimating the prediction error, which helps in selecting the best model and fine-tuning its parameters. The validation set provides an unbiased evaluation of the model's performance because it has not been seen by the model during training.

However, if data availability is limited, setting aside a separate test set might not be feasible. In such cases, resampling methods like k-fold cross-validation are used. In k-fold cross-validation, the full dataset is split into 'k' approximately equal-sized disjoint subsets or folds. The model is then trained on 'k-1' folds, while the remaining fold is used for assessing prediction accuracy. This process is repeated 'k' times, with each fold serving as the validation set once. The overall cross-validation accuracy is estimated by aggregating the residuals over the 'k' folds.
The text does not provide explicit information about which specific optimization technique such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) was used during the deep learning pipeline. However, it mentions that the Convolutional Neural Network (CNN) went through cycles of training on the training set, evaluating performance on the validation set, and adjusting CNN node weights. This suggests that some form of gradient descent optimization method was employed to update the network weights based on the error computed from the loss function. Additionally, the concept of early stopping is mentioned, which is a regularization technique used to prevent overfitting by halting the training process before the model starts to lose its ability to generalize well to unseen data.
In the experimental settings described, the Inception-ResNet-v2 model was trained using the mini-batch gradient descent algorithm with the batch size set to 32. The Adam optimizer was used to fine-tune the model. Specifically, the exponential decay rate of first-moment estimation of 0.9 was employed.

Mini-batch gradient descent is a variant of stochastic gradient descent (SGD). It splits the training dataset into small batches and performs updates after each batch rather than every single example like traditional SGD or after all examples like full-batch gradient descent. This approach provides a balance between speed and accuracy.

Adam (short for Adaptive Moment Estimation) is another optimization technique used in this case. It combines the advantages of two popular methods, RMSprop and Stochastic Gradient Descent with momentum. Adam calculates adaptive learning rates for different parameters based on estimates of first and second moments of the gradients. This makes it well suited for handling problems involving large datasets and/or high dimensionality.
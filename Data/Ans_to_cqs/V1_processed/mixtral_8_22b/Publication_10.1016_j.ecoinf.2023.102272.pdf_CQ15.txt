Based on the given information, it is not explicitly stated what specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) are applied in the deep learning pipeline. However, there are some indications about the use of certain hyperparameters related to optimization. For instance, the table shows parameters like 'Eta', 'Gamma', 'Decay', and 'hidden\_dropout'. These can be associated with optimization techniques.

In general, Eta is often referred to as the learning rate, Gamma is a regularization parameter, Decay is used to reduce the learning rate over time, and hidden\_dropout is a technique used to prevent overfitting during training. While these terms suggest the application of optimization techniques, they do not directly indicate whether SGD, Adam, or any other specific optimizer is employed. Therefore, based solely on the provided context, I cannot definitively state which optimization techniques are applied in the deep learning pipeline.
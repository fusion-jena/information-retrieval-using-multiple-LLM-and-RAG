Based on the given context, it appears that the authors have employed several strategies to prevent overfitting and underfitting in their deep learning model. However, the specific regularization techniques such as dropout or L2 regularization are not explicitly mentioned. Instead, they focus on avoiding overtraining the model and adopting a gradually increasing training cycle along with a graded Mean Square Error (MSE) convergence approach. This method allows them to fine-tune the model without causing overfitting issues. Additionally, they use optimized hyperparameters to further prevent overfitting and underfitting scenarios. Therefore, based on the information available, we cannot provide explicit details about the usage of dropout or L2 regularization in this particular case.
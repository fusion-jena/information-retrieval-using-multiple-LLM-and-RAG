Regularization methods are techniques used in machine learning models, including deep learning pipelines, to prevent overfitting by adding a penalty term to the loss function during training. Two common regularization methods mentioned in the given context are dropout and L2 regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust and less sensitive to specific weights in the network. By ignoring certain neurons, the model becomes less dependent on any single feature and learns to generalize better. Dropout is often applied to fully connected layers in neural networks.

L2 regularization, also known as weight decay, adds a penalty term proportional to the square of the magnitude of the weights to the loss function. This encourages smaller weights, effectively reducing the complexity of the model and preventing it from fitting too closely to the training data. L2 regularization is commonly used in linear regression, logistic regression, and neural networks.

Other regularization methods include L1 regularization, which adds a penalty term proportional to the absolute value of the weights, encouraging sparse solutions; and early stopping, which stops the training process before the model starts to overfit the training data. These methods are not explicitly mentioned in the given context but are relevant to the topic.
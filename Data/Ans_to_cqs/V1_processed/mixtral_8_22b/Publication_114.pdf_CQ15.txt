In the given context, the optimization technique used in the deep learning pipeline is mentioned as the Adam optimizer. The Adam optimizer is a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement. It combines the advantages of AdaGrad and RMSProp methods to provide an optimization algorithm that can handle sparse gradients on noisy problems. This makes it well suited for large-scale machine learning applications such as neural networks.

In the context of this study, the Adam optimizer was used to optimize the learning rate for all three CNNs - VGG 16, ResNet50, and AlexNet. The initial learning rate was determined through trial and error, and finally set to 0.00001 for all three networks. The max iteration was set as 200,000 for all three networks in the training phase, and the training models were saved every 10,000 iterations to identify the best models for testing.
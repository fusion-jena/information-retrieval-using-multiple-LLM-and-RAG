The given context does not provide information on the optimization techniques applied in the deep learning pipeline, such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). Therefore, I cannot provide a helpful answer based on the available context.

Based on general knowledge, however, it is common to use optimization algorithms like SGD, Adam, RMSprop, or others in deep learning pipelines to minimize the loss function during training. These methods help adjust the model parameters iteratively to improve its performance.
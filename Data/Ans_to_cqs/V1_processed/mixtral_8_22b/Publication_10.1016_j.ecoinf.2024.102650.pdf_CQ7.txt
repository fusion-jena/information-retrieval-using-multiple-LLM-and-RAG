The context does not provide explicit information about how the data is split for deep learning model training into sets like train, test, or validation. However, it mentions that machine learning methods require text preprocessing, which includes selecting a portion of the text to manually annotate emotions for accuracy verification. This suggests that some form of splitting might have been done for this purpose. Additionally, the context discusses using the ERNIE model, which has shown good performance in previous studies. It also mentions using Python's pyLDAvis library to visualize topics in tweets, but there is no direct mention of specific criteria used to split the data for deep learning model training. Therefore, I cannot provide a definitive answer regarding the criteria used to split the data for deep learning model training based solely on the given context.
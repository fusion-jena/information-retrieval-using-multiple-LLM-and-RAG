The papers cited do not provide specific information about the optimization techniques used in their deep learning pipelines. However, common optimization techniques used in deep learning include Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam). SGD is a simple yet effective method for optimizing neural networks by updating model parameters based on the gradient of the loss function with respect to those parameters. On the other hand, Adam combines the advantages of both momentum-based methods and adaptive learning rate methods, resulting in faster convergence and better performance. Other popular optimization techniques include RMSprop, AdaGrad, and Nesterov Accelerated Gradient (NAG). Without explicit mention of the optimization technique used in these papers, we cannot definitively state which one was employed.

Based on the given context, there is no direct reference to any specific optimization techniques used in the deep learning pipeline. Therefore, I am unable to provide a precise answer to the query.
Before training a deep learning model, several preprocessing steps can be taken to improve its performance. These may include:

1. Data Cleaning: This involves removing or correcting any errors, inconsistencies, or missing values in the dataset. It also includes handling outliers which might affect the model's performance.

2. Normalization: This is a technique often applied as part of data preparation for machine learning. In this process, numeric fields are scaled to a standard range to prevent attributes with larger ranges from dominating those with smaller ranges. Common techniques include Min-Max Normalization and Z-Score Normalization.

3. Feature Scaling: Similar to normalization, feature scaling rescales the features so they have the same order of magnitude. This is important when using optimization algorithms such as gradient descent where having features on different scales could slow down convergence.

4. Encoding Categorical Variables: Deep learning models cannot directly handle categorical variables. Therefore, these need to be converted into numerical form. Techniques like one-hot encoding or label encoding can be used.

5. Data Augmentation: This is a strategy that enables practitioners to significantly increase the diversity of data available for training models without actually collecting new data. For example, in image classification tasks, augmentation can involve flipping, rotating, zooming, etc.

6. Train-Test Split: Before training the model, it is common practice to split the dataset into two parts - one for training the model and another for testing it. This helps ensure that the model generalizes well to unseen data.
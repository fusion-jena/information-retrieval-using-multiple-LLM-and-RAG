The performance of the deep learning model is evaluated using several metrics. Precision (P(τ)) is one such metric, which measures the fraction of predictions of presence that are correct. It is calculated as the ratio of true positive (TP) predictions to the total number of predicted presences (NP'). Another metric used is recall (R(τ)), also known as sensitivity or true-positive rate, which measures the fraction of presences that are correctly identified. It is calculated as the ratio of true positive (TP) predictions to the total number of true presences (NP). Additionally, based on the precision-recall curve, the average-precision (AP) of predictions is measured. This is defined as the weighted sum of precisions at each threshold, where the weights are the increases in recall from the previous threshold. The AP is independent of the chosen threshold, making it useful for model comparison. In cases of multi-label prediction evaluation, the mean-average-precision (mAP) across classes is often used.
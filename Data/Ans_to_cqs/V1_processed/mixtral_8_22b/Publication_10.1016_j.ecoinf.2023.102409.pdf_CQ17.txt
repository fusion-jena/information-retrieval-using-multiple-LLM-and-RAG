The given context does not provide explicit information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions the use of Stochastic Gradient Descent (SGD) optimizer, which can be combined with various regularization techniques such as dropout or L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting. On the other hand, L2 regularization adds a penalty term to the loss function, encouraging smaller weight values and thus preventing overfitting. Without specific details from the context, we cannot confirm if these methods were employed.
The text does not explicitly mention any specific optimization technique such as Stochastic Gradient Descent (SGD), Adam, or others. However, it does discuss the use of batch normalization, which is a technique used during training to improve the speed, stability, and performance of neural networks. This technique involves normalizing the activations of each layer in a mini-batch of inputs, allowing for faster convergence and better generalization. Additionally, the text mentions the use of a multinomial logistic regression on top of the softmax layer and the linear classification layer, but this is more related to the output activation function rather than an optimization technique.
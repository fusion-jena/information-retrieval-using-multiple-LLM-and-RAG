The given context does not provide explicit information about the specific regularization methods used in the deep learning pipeline to prevent overfitting. However, common techniques include dropout, which randomly sets a fraction rate of input units to 0 during training time, and L2 regularization, also known as weight decay, which adds a penalty term to the loss function proportional to the squared magnitude of the weights. These methods help reduce overfitting by making the network more robust and preventing complex co-adaptations on training data. Other popular regularization techniques include early stopping, data augmentation, and batch normalization.
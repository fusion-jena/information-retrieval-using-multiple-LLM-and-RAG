The text does not provide explicit details about how the hyperparameters of the model were optimized. However, it mentions the use of the Adam optimizer, which is an algorithm used for stochastic gradient descent based optimization of machine learning models. This suggests that some form of optimization technique was employed during the training process. Unfortunately, without further specifics regarding the methodology or results, it cannot be definitively stated whether grid search, random search, or another approach was utilized for hyperparameter tuning.

Reference(s):
[Kingma, Diederik P., and Jimmy Ba. "Adam: A method for stochastic optimization." International Conference on Learning Representations. 2015.]
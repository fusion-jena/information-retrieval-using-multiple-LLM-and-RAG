The deep learning pipeline uses two main datasets: a training dataset and a validation dataset. The training dataset contains 111,244 images, while the validation dataset includes 19,614 images. These datasets are created using a Python script named Making\_dataset.py within the Waveman framework. The images are initially sized at 256x256 pixels but are then reduced to 64x64 pixels to save training time and minimize high memory graphics usage. The images are balanced across various species by providing an upper limit for species with excessive data and increasing the number of images for rarer species through exponential scaling and slight shifts in the window around the calls. This ensures that rare species do not become under-represented compared to more common ones.
The authors of the study conducted a series of experiments with varying configurations to evaluate their approach to plant disease detection using deep learning models. They considered six different factors in their experiments: choice of deep learning architecture, choice of training mechanism, choice of dataset type, choice of training-testing set distribution, measurement of performance, and hyperparameter tuning.

Regarding the criterion used to split the data for deep learning model training, the authors employed five different train-test set distributions: Train: 80%, Test: 20%; Train: 60%, Test: 40%; Train: 50%, Test: 50%; Train: 40%, Test: 60%; and Train: 20%, Test: 80%. These distributions were chosen to assess the performance of their models on new, unseen data and to monitor overfitting. Each experiment ran for a total of 30 epochs, where one epoch was defined as the number of training iterations required for the specific neural network to complete a full pass through the entire training set. To ensure a fair comparison among the various experimental configurations, the authors standardized the hyperparameters across all experiments.

In summary, the authors used five distinct train-test set distributions to divide the data for deep learning model training: 80-20, 60-40, 50-50, 40-60, and 20-80. This allowed them to gauge the effectiveness of their models on previously unobserved data and detect potential overfitting issues.
The text does not provide specific details about the data annotation techniques used in the deep learning pipeline, such as bounding box annotation or instance segmentation. However, it mentions several aspects related to data annotation. During the campaign, methods like majority voting and soft labeling are proposed to assess inter-annotator variability and aggregate annotation sets. These methods help determine the minimum number of annotators required. Additionally, increasing the number of annotators per sample may be beneficial if the variability is too high. The goal of these grouping methods is to create relevant annotation sets for deep learning.

Moreover, the study emphasizes the need for a user-friendly annotation platform to engage annotators and maintain their interest in annotating large datasets. Online access to data is suggested, along with automation of the process from initial data observation to collection of all annotations. Unifying data representation for all users is essential to minimize variability. Furthermore, preserving the independence of all annotations helps prevent over-representation of early annotations.

Lastly, the text discusses the use of Convolutional Neural Networks (CNNs) trained on annotations from both novices and experts. Results indicate variations in model performance based on different annotation strategies. Annotation guidelines promoting a more conservative approach are encouraged to enhance overall annotation quality. Two annotation aggregation methods—majority voting and soft labeling—are proposed and tested to optimize multi-annotation and decrease the impact of noisy labels. Both methods, especially when enough annotators participate, significantly improve model performance and lower variability.
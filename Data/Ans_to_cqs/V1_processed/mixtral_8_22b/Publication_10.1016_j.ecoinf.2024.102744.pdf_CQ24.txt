The text does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, it mentions using dropout layers to prevent overfitting, which can introduce randomness into the training process. Dropout randomly sets a fraction of input units to 0 during each update, helping to avoid co-adaptation among neurons. While this doesn't directly address the issue of controlling randomness due to factors like random seed values, it demonstrates one way to manage variability within the deep learning pipeline.
Regularization methods are techniques used in deep learning pipelines to prevent overfitting, where a model learns the noise or outliers in the training data instead of generalizing well to new, unseen data. Two common regularization methods are Dropout and L2 Regularization.

Dropout is a technique introduced by Srivastava et al. (2014) that randomly drops out, or sets to zero, a proportion of the units in a layer during training. This prevents complex co-adaptations among the feature detectors in the layer, reducing overfitting. During testing, all units are active but their outputs are scaled down by the same proportion used during training, ensuring consistent predictions.

L2 Regularization, also known as Ridge Regression or Weight Decay, adds a penalty term to the loss function proportional to the square of the magnitude of the weights. This encourages smaller weights, making the model simpler and more likely to generalize better. It can be implemented by adding a factor multiplied by the sum of squares of the weights to the cost function.

Other regularization methods include L1 Regularization (also called Lasso Regression), Early Stopping, and Max Norm Constraints. These methods aim to reduce complexity and increase generalizability, improving the overall performance of deep learning models.

Reference(s):
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15(1), 1929-1958.
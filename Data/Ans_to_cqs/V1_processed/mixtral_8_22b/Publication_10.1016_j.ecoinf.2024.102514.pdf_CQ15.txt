The given context does not explicitly mention any optimization techniques applied in the deep learning pipeline such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, it discusses various Machine Learning (ML) algorithms including Recurrent Neural Network (RNN) models, which often use optimization techniques like SGD or Adam for updating weights during backpropagation. Additionally, the context mentions the use of nested cross-validation, which is a technique used for hyperparameter tuning and model selection, but this doesn't directly relate to optimization techniques within the deep learning pipeline itself. Therefore, while we cannot definitively state what optimization techniques are being used from the provided information, common practices suggest that techniques like SGD or Adam could potentially be employed.
Based on the given context, it appears that the paper uses Bayesian Optimization as part of its deep learning pipeline. However, there is no mention of other optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). Therefore, we cannot definitively state whether these techniques were used in this study.

Bayesian Optimization is a sequential design strategy for global optimization of black-box functions that doesn't require derivatives. It works by constructing a posterior distribution of functions (Gaussian Process) that best describes the observed data, which is then used to select the next query point. This technique has been shown to perform well in practice and is particularly useful when the function evaluations are expensive.

In summary, while we do not have explicit confirmation that SGD or Adam were used in this study, we do know that Bayesian Optimization was employed as part of their deep learning pipeline.
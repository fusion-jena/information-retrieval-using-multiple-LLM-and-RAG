The text does not provide specific details on handling randomness in the deep learning pipeline or setting random seed values. However, some common strategies include fixing the random seed value before running experiments to ensure reproducibility, using techniques like cross-validation to reduce variance due to random sampling, and employing ensemble methods such as bagging or boosting to average out the effects of randomness. These strategies help improve the robustness and reliability of deep learning models.

Based on the given context, it appears that the authors used the 'randomForest' package for their study. This package allows the user to specify the number of trees (ntree) and the number of features in each split (mtry), which can impact the performance of the model. The authors mention that they let the model set these parameters by default, allowing it to create around 500 decision trees and select over 3000 training samples randomly for training purposes. While this doesn't directly address the issue of handling randomness in the deep learning pipeline, it suggests that the authors relied on the built-in mechanisms of the 'randomForest' package to manage randomness within the model itself.
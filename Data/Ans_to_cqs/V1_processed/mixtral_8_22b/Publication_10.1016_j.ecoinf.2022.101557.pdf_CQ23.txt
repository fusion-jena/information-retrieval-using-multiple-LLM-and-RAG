To ensure the generalizability of the deep learning model, several measures were implemented. Firstly, a diverse dataset was utilized, consisting of samples selected randomly from various areas used in the training set. These samples were never employed to fit the model, thereby preventing any potential bias towards specific regions.

Secondly, a 10-fold cross-validation technique was adopted to further enhance the model's ability to generalize across unseen maps. To achieve this, the study area was divided into ten distinct subsets using a 3x3 grid. Given that the studied region was not perfectly square, one of the nine cells contained more area than the others. This cell was subsequently partitioned into two separate cells, resulting in a total of ten sets. Throughout the training phase, nine of these sets were allocated for training purposes, while the tenth set served as the evaluation subset. This approach allowed for a comprehensive assessment of the model's capacity to handle varying terrains and mapping styles.

Lastly, dropout regularization was incorporated during the training stage with a rate of 0.3. This strategy helped stabilize the model and mitigate overfitting concerns. Additionally, the ADAM optimization algorithm was employed with a learning rate of 0.0001 to determine optimal weights within the neural network. The objective was to minimize the cross-entropy loss between the network's predictions and the labeled data.

These combined strategies aimed to guarantee the robustness and adaptability of the deep learning model when confronted with new or previously unseen datasets.
The given context does not provide explicit details about any specific preprocessing steps taken before training the deep learning models mentioned. However, some common preprocessing techniques often applied when working with image datasets include:

1. Normalization: Scaling pixel values between 0 and 1 or -1 and 1 to ensure consistent input ranges across images.
2. Data augmentation: Applying transformations like rotation, flipping, zooming, etc., to increase the size of the training set and prevent overfitting.
3. Cleaning: Removing noisy data points or outliers that may negatively impact the model's performance.
4. Splitting the dataset: Dividing the dataset into training, validation, and test sets to evaluate the model's generalizability.

In this case, the authors mention adapting a pre-trained VGG16 network for transfer learning on their dataset. Transfer learning involves leveraging knowledge gained from solving one problem to solve another related problem. Here, the authors use a pre-trained network that has already learned features from the ImageNet dataset and fine-tune it on their own dataset. They freeze early layers of the network to keep these learned features intact and add extra layers to adapt the network to their task. Additionally, they modify the architecture by reducing the dimensionality of the fully connected layer to decrease computational complexity. These modifications could be considered part of the overall preparation process for training the deep learning model.
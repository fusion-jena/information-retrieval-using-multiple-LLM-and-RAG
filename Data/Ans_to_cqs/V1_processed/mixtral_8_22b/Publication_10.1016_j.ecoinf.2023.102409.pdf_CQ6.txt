The given context does not provide explicit information about any specific preprocessing steps involved before training the mentioned deep learning models (Unet, Pspnet, and Deeplabv3+). However, typically, some common preprocessing steps used in deep learning include:

1. Normalization: This step scales the input features to have zero mean and unit variance or between 0 and 1. Common methods include Min-Max scaling, z-score normalization, and log transformation.

2. Data Cleaning: Removing missing values, outliers, or irrelevant data from the dataset can improve the performance of the model.

3. Image Augmentation: For computer vision tasks, techniques like rotation, flipping, zooming, cropping, and shifting can be applied to increase the diversity of the training dataset.

4. Encoding Categorical Variables: Converting categorical variables into numerical form using one-hot encoding, label encoding, or binary encoding.

5. Feature Scaling: Standardizing the range of independent variables so that they contribute proportionately during training.

6. Dimensionality Reduction: Techniques such as Principal Component Analysis (PCA) can reduce the dimensionality of the dataset while retaining most of the relevant information.
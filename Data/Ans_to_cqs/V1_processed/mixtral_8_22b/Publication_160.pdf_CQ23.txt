To ensure the generalizability of the deep learning models, several measures were taken. Firstly, the use of diverse datasets played a crucial role. Different ResNet architectures were employed, including ResNet152, ResNet50, and ResNet18, each with varying input sizes. These networks were then evaluated on both validation and test sets, allowing for a more comprehensive assessment of their performance.

Secondly, the concept of patch size was introduced to manage the balance between capturing sufficient individual information and avoiding excessive contextual noise. A larger patch size of 224 x 224 pixels was found to perform well, as it included enough contextual noise to regularize overfitting and improve generalization. This was evidenced by the fact that this patch size achieved the highest micro-F1 score on the test set.

Moreover, an ensemble network was used, combining the features extracted from the four tested patch sizes. This approach improved the overall classification performance, further enhancing the model's ability to generalize.

Lastly, the use of cross-validation techniques can also be inferred, although not explicitly mentioned. Given the consistent increase in validation performances with increasing patch size, and the decreasing gap between patch size performances in training curves, it suggests that some form of cross-validation or iterative testing was likely employed.

In conclusion, the combination of using diverse datasets, managing patch size, employing an ensemble network, and potentially utilizing cross-validation techniques contributed significantly to ensuring the generalizability of the deep learning models.
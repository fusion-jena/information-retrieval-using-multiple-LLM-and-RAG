The text does not provide explicit information about how the data is divided into training, testing, and validation sets. However, it mentions that the authors did not fine-tune their hyperparameters or adjust their stopping criterion based on any specific dataset. Instead, they chose preselected values for these parameters. This suggests that they may have used a single dataset for both training and validation purposes without explicitly separating them.

In general, when developing deep learning models, it is common practice to divide the available data into three distinct subsets: training, validation, and testing. The training set is used to fit the model, while the validation set is employed to optimize the model's hyperparameters and prevent overfitting. Finally, the testing set serves as an independent evaluation of the final model's performance.

Although the text does not specify the exact methodology used for splitting the data, it does mention that the authors considered fine-tuning their hyperparameters using a validation set when working with real datasets. This implies that they might follow standard practices for data partitioning when dealing with actual data.
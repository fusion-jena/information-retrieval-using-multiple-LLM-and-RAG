In this study, the researchers utilized the Adam learning rate optimizer for their Convolutional Neural Network (CNN) model. The Adam optimizer is a popular choice due to its efficiency and ease of use. It dynamically adjusts the learning rate during training, improving overall performance. By using the Adam optimizer, the researchers could avoid manually tuning the learning rate, making it more convenient and effective. Additionally, they employed early stopping as another optimization technique. Early stopping involves halting the training process once the validation error does not decrease for a certain number of epochs. In this case, the researchers stopped training if there was no improvement in validation error for 10 consecutive epochs. This strategy helped identify the appropriate number of epochs needed for each dataset, resulting in 14, 14, and 23 epochs for datasets 1, 2, and 3, respectively.
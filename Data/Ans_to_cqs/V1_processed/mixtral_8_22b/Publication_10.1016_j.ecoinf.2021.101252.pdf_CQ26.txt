To address data bias during preprocessing of the deep learning pipeline, various techniques can be employed. These include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into subsets while maintaining the same proportions of target variables across each split. This ensures that each subset has a representative sample of all possible outcomes, reducing the risk of biased results due to uneven distribution of target variables.

Oversampling is a technique used to increase the representation of minority classes within a dataset. It involves creating copies of existing instances of the minority class or synthesizing new instances based on existing ones. By increasing the size of the minority class, oversampling aims to reduce the impact of class imbalance on model performance.

Undersampling, on the other hand, involves removing instances from the majority class to achieve a more balanced dataset. While this approach may result in loss of information, it can help mitigate the effects of class imbalance and improve model performance.

Diverse data collection refers to the practice of collecting data from multiple sources or populations to ensure that the resulting dataset is representative of the broader population. This can involve sampling from different geographic regions, demographics, or time periods to capture variations in patterns and trends.

In the given context, the authors addressed data bias by balancing their data through random duplication of presence records and deletion of absence records until a balance of approximately 50:50 was achieved. This was executed using the ROSE package for R. This approach falls under the category of oversampling, where the goal is to create a more balanced dataset by increasing the representation of the minority class.
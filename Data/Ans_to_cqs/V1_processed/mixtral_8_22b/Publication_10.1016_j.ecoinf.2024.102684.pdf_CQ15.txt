The text mentions that the researchers considered the activation function, the optimizer, and the learning rate as part of their standard machine learning hyperparameter search. Specifically, they looked at the hidden activation function, choosing from options like ReLU, sigmoid, or elu. As for the optimizer, they selected either RMSprop or adam. Lastly, they experimented with various learning rates, including 0.1, 0.001, 0.0001, and 0.00001. Therefore, the optimization techniques applied in this deep learning pipeline include RMSprop and Adam as optimizers, along with adjusting the learning rate to find the best value.
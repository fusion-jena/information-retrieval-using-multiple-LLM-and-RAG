Based on the given information, there isn't any explicit mention of a specific strategy used to monitor the model performance during training. However, some insights can be derived from the described methodology.

Firstly, the authors mentioned using a classic DL algorithm architecture with their largest dataset, AT0, as a baseline for DL accuracy. This suggests they might have monitored the performance of the model by comparing its results against this established benchmark.

Secondly, the authors also conducted experiments where they trained the same DL architecture with smaller datasets, such as AT1, and subsets of AT0 containing 250 or 500 images per class. By comparing the performance of these models with the baseline, they could assess how well the model was performing under different conditions.

Lastly, the authors reported the mean accuracy and standard deviation of their models on the T2 test-dataset. This indicates that they evaluated the performance of their models post-training, which could serve as a form of monitoring during the development phase.

However, without further details about the actual implementation, it cannot be definitively stated what specific strategy was employed to monitor the model performance during training.
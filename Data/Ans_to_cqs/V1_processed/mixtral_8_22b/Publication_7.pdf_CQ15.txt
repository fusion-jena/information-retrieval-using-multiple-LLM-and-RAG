The given context does not provide information about specific optimization techniques used in the deep learning pipeline, such as Stochastic Gradient Descent (SGD) or Adam. However, it can be assumed that some form of optimization technique was used during training of the deep neural networks mentioned, like VGGnet, since these models require parameter updates through backpropagation. Typically, popular choices include SGD with momentum, Adaptive Moment Estimation (Adam), Root Mean Square Propagation (RMSProp), and others. Without explicit mention in the text, it cannot be confirmed which optimization method was utilized in this case.
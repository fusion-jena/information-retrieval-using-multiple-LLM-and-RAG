The study mentioned in the given context uses the AlexNet architecture, which incorporates practical strategies like ReLU nonlinear layers and dropout regularization techniques. Dropout is a regularization method used to prevent overfitting in neural networks, including deep learning pipelines. It works by randomly dropping out neurons during training, effectively creating a sparse network. This helps to reduce co-dependencies between nodes and makes the model more robust to noise in the input data. Additionally, the use of ReLU (Rectified Linear Unit) activation functions also contributes to reducing overfitting, as they help to introduce nonlinearity into the model while avoiding the vanishing gradient problem often encountered with sigmoidal activation functions. However, the text does not explicitly mention any other specific regularization methods, such as L2 regularization, being employed in this particular study.
Based on the given context, there isn't any explicit mention of strategies employed to handle randomness in the deep learning pipeline, such as managing random seed values. However, some relevant information can still be extracted regarding how randomness was handled during the data collection and processing stages.

In the context provided, the authors discuss their approach to creating a balanced training set by adding well-defined examples of woody cover elements until both classes had the same number of training points. They also describe how they generated a set of 400 random points within the subplot and assigned them to one of the two land cover classes. These methods suggest that the researchers took deliberate actions to manage randomness during the data collection stage.

Additionally, the text mentions the use of the Random Forest classifier, a supervised learning algorithm known for producing multiple decision trees using a randomly selected subset of training data and independent variables. While this does involve some level of randomness, it is inherent to the chosen method rather than being explicitly managed through strategies like setting random seed values.

Therefore, based on the available context, it appears that the focus was more on addressing issues related to imbalance in the training set and ensuring representativeness of the sampled data, rather than directly tackling randomness in the deep learning pipeline itself.
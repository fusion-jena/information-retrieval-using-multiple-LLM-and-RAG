The context does not explicitly mention the specific criteria used to split the data for deep learning model training into sets such as train, test, or validation. However, generally, in machine learning and deep learning practices, there are several commonly used strategies for splitting datasets. These include:

1. Random Split: Data is randomly divided into training, testing, and sometimes validation sets. A typical ratio might be 70% for training, 20% for testing, and 10% for validation.

2. Stratified Sampling: When dealing with imbalanced classes, stratified sampling ensures that each subset maintains the same distribution of classes as the original dataset.

3. Time Series Split: For time-series data, where order matters, a chronological split is often employed. For example, earlier data could be used for training, while later data serves as the test set.

4. Cross-Validation: This involves dividing the dataset into k subsets or 'folds'. Each fold is then used once as a validation set, while the remaining k - 1 folds form the training set. This process is repeated k times, ensuring all observations serve as both training and validation instances.

Without explicit information from the given context, one cannot definitively state which strategy was used for this particular study.
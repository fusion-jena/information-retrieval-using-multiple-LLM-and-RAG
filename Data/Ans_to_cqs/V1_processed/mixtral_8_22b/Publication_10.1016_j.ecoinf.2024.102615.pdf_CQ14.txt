The hyperparameters of the model are optimized using the grid search approach. Grid search is a traditional hyperparameter optimization method that compares the values of evaluation metrics for different parameters and selects the optimal parameters from a large search space. In this study, the grid search method was used to optimize five of the MLP model's hyperparameters: the number of neural network layers, the number of neurons per layer, dropout, learning rate, and batch size. The hyperparameter tuning ranges for these parameters are listed in Table 1. The optimal MLP architecture and parameters are presented in Table 2. The model performance optimization process is divided into two stages. During the first stage, the neural network structure and the number of neurons in the hidden layers are adjusted. A total of 60 sets of combinations, including one to five hidden layers and neurons with a range of 64--2048 (nth power of 2), are considered. The second stage involves adjusting the percentage of the dropout layers, the learning rate, and the batch size, to achieve a total of 192 combinations (4\*4\*3\*4).
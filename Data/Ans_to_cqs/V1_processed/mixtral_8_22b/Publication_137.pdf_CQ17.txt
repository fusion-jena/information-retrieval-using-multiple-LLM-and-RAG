In this study, the authors use regularization methods to prevent overfitting in their deep learning pipeline. They mention using a learning decay rate of 0.7 to gradually decrease the learning rate during training. However, they do not explicitly state whether they use specific regularization techniques like dropout or L2 regularization. Instead, they focus on adjusting the regularization strength to improve the performance of their model. By experimenting with different values for the regularization strength, they found that a value of 0.2 yielded the best results. This suggests that the authors may have implemented some form of regularization, but they do not provide details about the exact technique used.
Based on the given information, there isn't a direct statement about the exact criteria used to determine when training is complete. However, some clues can be derived from the text.

Firstly, the dataset was split into three parts: training (60%), validation (20%), and testing (20%). This division suggests that the validation set might play a role in determining when training should stop.

Secondly, the paper mentions that the models were evaluated using the coefficients of determination (R2) and the root mean squared error (RMSE) metrics. These metrics were also used to compare the performance of the models on the training, validation, and testing datasets. This implies that the point where the model performs well consistently across all three sets, without significant improvement upon further training, could indicate completion of training.

Lastly, the authors conducted a hyperparameter search involving the hidden activation function, optimizer, and learning rate. They compared the training, validation, and testing performance to avoid overfitting. This indicates that monitoring the performance on the validation set during training could help prevent overfitting and signal when training should cease.

Therefore, while not explicitly stated, one could infer that the criteria used to determine when training is complete might involve observing the model's performance on the validation set, specifically looking for a plateau in validation loss or no significant improvement in validation accuracy. Additionally, achieving consistent performance across the training, validation, and testing sets could also serve as an indicator.
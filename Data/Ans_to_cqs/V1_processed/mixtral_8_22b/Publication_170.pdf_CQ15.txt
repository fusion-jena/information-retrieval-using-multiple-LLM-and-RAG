The paper does not explicitly mention any specific optimization technique such as Stochastic Gradient Descent (SGD) or Adam being used in the deep learning pipeline. However, it does state that the iteration termination condition of the model was the same in each round of training, where the loss rate of the model was less than a preset threshold. This suggests that some form of gradient descent optimization might have been employed, but without explicit confirmation, it cannot be definitively stated which one was used.
The paper does not provide explicit information about how the hyperparameters were optimized. However, it mentions specific values for certain parameters such as learning rate, dropout rate, and bias value, all set to 0.001. It also specifies the use of Leaky ReLU as an activation function and the application of batch normalization before training. Additionally, the number of epochs varies depending on the depth of the neural network, with fewer epochs for deeper networks to avoid overfitting. Despite these details, there is no mention of using techniques like grid search or random search for hyperparameter optimization. Therefore, based on the available information, we cannot determine the method used for hyperparameter optimization in this study.
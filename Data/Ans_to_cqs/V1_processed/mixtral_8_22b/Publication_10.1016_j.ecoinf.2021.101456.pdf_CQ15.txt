In the given context, the models were trained using the Adam optimizer (Kingma et al., 2015), which is a popular optimization technique used in deep learning pipelines. Adam combines the advantages of two other extensions of stochastic gradient descent (SGD): Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). It computes adaptive learning rates for different parameters by performing smaller updates for parameters associated with frequently occurring features and larger updates for infrequently occurring ones. This makes Adam well suited for handling sparse gradients and noisy data, often encountered in deep learning tasks.
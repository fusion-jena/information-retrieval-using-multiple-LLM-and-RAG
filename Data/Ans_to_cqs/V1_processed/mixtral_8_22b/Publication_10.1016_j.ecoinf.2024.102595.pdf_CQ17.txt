The text does not provide explicit information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it mentions that sophisticated feature selection and cross-validation methods were employed to address overfitting. These methods could include techniques like dropout and L2 regularization, but without further details, we cannot confirm if they were specifically utilized in this case. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the network more robust. On the other hand, L2 regularization adds a penalty term to the loss function, discouraging large weights and thus reducing complexity and preventing overfitting. Other common regularization methods include early stopping, weight decay, and batch normalization.
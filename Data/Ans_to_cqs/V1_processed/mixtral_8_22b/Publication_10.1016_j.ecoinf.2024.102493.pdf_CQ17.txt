The text does not provide specific information on the regularization methods used to prevent overfitting in the deep learning pipeline. Regularization methods like dropout and L2 regularization are commonly used techniques to avoid overfitting in machine learning models, including deep learning pipelines. Dropout involves randomly setting a fraction rate of input units to 0 during training time, which helps reduce interdependent learning amongst neurons. On the other hand, L2 regularization adds a penalty term to the loss function, encouraging smaller weights and preventing any single weight from dominating the output. These methods help generalize the model better by avoiding overreliance on any particular feature or pattern in the training dataset.
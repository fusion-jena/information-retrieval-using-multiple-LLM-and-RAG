Unfortunately, there isn't enough context provided to give a specific answer about how the trained deep learning model was deployed. However, I can provide some general steps typically taken when deploying a trained deep learning model:

1. Model Serialization: After training, the model weights and architecture are saved or serialized into a file format such as.h5 (HDF5) for Keras/TensorFlow models or.pth for PyTorch models. This allows us to use the same model later without having to retrain it.

2. Platform Selection: Depending on the application requirements, one might choose different platforms for deployment. Some popular choices include cloud services like AWS, Google Cloud, Azure, etc., or edge devices like Nvidia Jetson boards, Raspberry Pi, etc.

3. Inference Scripts: Once the platform has been selected, scripts need to be written to load the serialized model and perform predictions on new data. These scripts should also handle any necessary preprocessing and postprocessing steps required before feeding data into the model and interpreting its output.

4. Testing & Validation: Before fully deploying the model, thorough testing and validation must be performed to ensure that the model performs well under various conditions and scenarios. This may involve creating test cases, benchmarking performance metrics, and comparing against other existing solutions if applicable.

Without more details regarding the specific project mentioned in the given context, these are the general steps usually followed during the deployment of a trained deep learning model.
The paper uses the Adam optimizer algorithm for model optimization. This choice was made due to its effectiveness in handling sparse gradients and large datasets, making it suitable for deep learning models like Convolutional Neural Networks (CNNs). The authors employed a batch size of 10 and trained the models for 100 epochs. The learning rates were determined through empirical trials over 100 epochs.

Reference(s):
[1] Kingma, Diederik P., and Jimmy Ba. "Adam: A Method for Stochastic Optimization." arXiv preprint arXiv:1412.6980 (2014).
The optimization technique applied in the deep learning pipeline mentioned in the given context is Adam processing. Adam is a popular optimization algorithm used in deep learning models due to its ability to adaptively adjust the learning rate during training. It combines the advantages of two other optimization algorithms - Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). Adam computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. This makes it well suited for problems with large datasets and high dimensionality.
Regularization is a technique used in machine learning and deep learning models to prevent overfitting by adding a penalty term to the loss function during training. Two common types of regularization techniques used in deep learning pipelines are Dropout and L2 Regularization.

Dropout is a regularization technique introduced by Geoffrey Hinton and his team in 2012. It works by randomly dropping out neurons from the neural network during training with a certain probability. This helps to reduce co-dependency between nodes and prevents overfitting. The dropped-out neurons do not contribute to the forward pass or backpropagation, but their weights are still updated. During testing, all neurons are included without any dropouts.

L2 Regularization, also known as weight decay, adds a penalty term to the loss function proportional to the square of the magnitude of the weights. This encourages smaller weights and reduces overfitting. The strength of this regularization can be controlled using a hyperparameter called lambda.

Other regularization techniques include L1 Regularization, which uses the absolute value instead of the square of the weights, and early stopping, which stops training before convergence if validation error starts increasing. However, these are less commonly used compared to Dropout and L2 Regularization.
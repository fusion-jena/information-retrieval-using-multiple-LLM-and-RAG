The text does not provide specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. Regularization methods such as dropout and L2 regularization are commonly used to prevent overfitting in deep learning models. Dropout randomly drops out neurons during training, preventing complex co-adaptations between neurons and reducing overfitting. L2 regularization adds a penalty term to the loss function, discouraging large weights and promoting simpler models. Other regularization methods include early stopping, weight decay, and batch normalization. These methods help improve model generalization and reduce overfitting by adding constraints or penalties to the model parameters.
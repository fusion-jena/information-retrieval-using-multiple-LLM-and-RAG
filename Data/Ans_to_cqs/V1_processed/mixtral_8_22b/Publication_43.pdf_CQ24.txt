Based on the given context, it appears that the strategies employed to handle randomness in the deep learning pipeline include data augmentation and transfer learning. Data augmentation involves techniques such as random scaling, cropping, flipping, and adjusting brightness to increase the size and diversity of the training set. Transfer learning, on the other hand, leverages pre-trained models to improve the performance of new models, especially when limited labeled training data is available. These strategies help overcome limitations caused by random initialization of network weights and contribute to better generalization and robustness of the models. However, the context does not explicitly mention handling randomness related to specific aspects like random seed values.
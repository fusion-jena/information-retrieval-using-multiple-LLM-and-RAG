The query asks about regularization methods used to prevent overfitting in the deep learning pipeline. However, none of the given pieces of context discuss this topic directly. Therefore, I cannot provide a direct answer based solely on the information provided.

However, I can share some general knowledge about regularization methods used in deep learning pipelines to prevent overfitting. Regularization techniques aim to reduce model complexity and avoid memorizing training data, thus improving its ability to generalize to unseen data. Two common regularization methods mentioned in the query are Dropout and L2 regularization.

Dropout is a technique where randomly selected neurons are ignored during training, effectively creating a thinned network. This helps prevent complex co-adaptations between neurons and reduces overfitting.

L2 regularization, also known as weight decay, involves adding a penalty term to the loss function proportional to the square of the magnitude of all weights in the network. This encourages smaller weights, reducing overall model complexity and helping prevent overfitting.

Other popular regularization methods include L1 regularization, early stopping, and data augmentation. L1 regularization uses the absolute value instead of the squared value of the weights in the penalty term, promoting sparse solutions. Early stopping stops training when validation performance starts degrading, preventing further overfitting. Data augmentation generates additional training examples by applying transformations like rotations, translations, or flips to existing data, increasing dataset size and diversity.
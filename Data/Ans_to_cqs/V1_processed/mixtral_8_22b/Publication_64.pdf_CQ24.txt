Based on the given context, there isn't explicit mention of specific strategies employed to handle randomness in the deep learning pipeline, such as managing random seed values. However, there are hints about how randomness was handled in certain aspects of the process.

For instance, during the training phase, the authors applied standard data augmentation techniques, including shearing, translating, and mirroring images. These transformations ensured that the network would never see the exact same image twice, thereby introducing controlled variability into the training set.

Additionally, when evaluating the ResNet50 model, they utilized random ten crop testing with smaller data augmentation parameters compared to those used during training. This approach suggests that randomness was incorporated into the evaluation process to ensure robust performance across different crops of the input images.

While these methods address handling randomness indirectly, they do not directly tackle the issue of managing random seeds within the deep learning pipeline. To manage randomness related to random seed values, common practices include setting fixed seed values for reproducibility purposes or employing multiple runs with varying seed values to assess the stability and generalizability of the models.
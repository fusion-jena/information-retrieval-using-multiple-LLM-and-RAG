The given context does not provide explicit information about the specific data annotation techniques used in the deep learning pipelines mentioned in some studies. However, based on common practices in machine learning and computer vision, we can infer that various annotation techniques might have been employed depending on the nature of the task.

For example, in the study by Do and Tran (2023), which uses a deep learning model to detect the dispersal of the early stages of spotted butterfish, it is likely that object detection or image segmentation techniques were utilized. Object detection involves identifying the location of objects within an image and classifying them accordingly, often done through bounding boxes. Image segmentation, on the other hand, entails partitioning an image into segments or regions, which could be useful for distinguishing between different marine organisms or features.

Similarly, in the work by Villon et al. (2021) focusing on automatic underwater fish species classification, it is probable that semantic segmentation was applied to distinguish individual fish instances and their corresponding classes. This technique allows for pixel-level labeling of each object present in an image, enabling more precise identification and categorization.

Additionally, in the research conducted by Edmunds et al. (2016) involving differential distribution of octocorals and scleractinians, it is possible that point annotations were used to mark the locations of these organisms within the studied areas. Point annotations involve placing a single dot or marker at a specific point in an image, making them suitable for tasks requiring spatial analysis or tracking changes over time.

Overall, while the exact data annotation techniques used in the referenced studies remain unspecified, it is reasonable to assume that they may include bounding box annotation, instance segmentation, semantic segmentation, and point annotations, among others, depending on the objectives and requirements of each investigation.
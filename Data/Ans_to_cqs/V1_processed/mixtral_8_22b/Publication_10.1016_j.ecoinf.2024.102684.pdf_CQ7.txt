The dataset was divided into three subsets for model evaluation and hyperparameter search: training (60%), validation (20%), and testing (20%). This division aimed to achieve an unbiased estimation of error. Data augmentation was applied solely to the training data to prevent duplication across different partitions.

To evaluate the models, the coefficients of determination (R^2) and the root mean squared error (RMSE) were employed. R^2 measures the percentage of variation in the response variable explained by the model, while RMSE quantifies the difference between the actual and predicted values. Lower RMSE values indicate superior model predictions. Overfitting was checked by contrasting the performance of the models on the training, validation, and testing datasets.

A standard machine learning hyperparameter search was conducted over two sites due to computational constraints. The considered parameters included the activation function, the optimizer, and the learning rate. An exhaustive search was carried out for the fully connected architecture for all mono-site and multi-site models. The hyperparameter search encompassed a small grid search involving the hidden activation function (ReLU, sigmoid, or elu), the optimizer (RMSprop or adam), and the learning rate (0.1, 0.001, 0.0001, or 0.00001).

Deep learning-based models commonly use straightforward data partitioning methods like training, validation, and testing. While this strategy facilitates the incorporation of diverse information into the training process, it might not adequately capture the variety of heterogeneous spatial information present in remote sensing data associated with distinct learning domains.

In summary, the dataset was segmented into training, validation, and testing subsets to ensure an impartial assessment of error. The primary criterion for splitting the data was to maintain a balance between the subsets, allocating 60%, 20%, and 20% of the data to training, validation, and testing, respectively. Additionally, data augmentation was exclusively implemented on the training set to evade redundancy across various partitions.
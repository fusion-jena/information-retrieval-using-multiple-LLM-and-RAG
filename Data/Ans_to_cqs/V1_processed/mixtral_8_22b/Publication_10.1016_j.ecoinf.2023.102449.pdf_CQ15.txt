While the given text does not explicitly mention any specific optimization technique used in the deep learning pipeline, it can be inferred that some form of gradient descent optimization algorithm was employed during the training process. This is because the authors discuss the use of higher learning rates enabled by batch normalization, which is a common practice when using optimization algorithms like Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), or Root Mean Square Propagation (RMSProp). These optimization methods help minimize the loss function during the training phase of deep learning models. However, without explicit information about the exact optimization method used, we cannot definitively state whether SGD, Adam, or another method was utilized in this particular case.
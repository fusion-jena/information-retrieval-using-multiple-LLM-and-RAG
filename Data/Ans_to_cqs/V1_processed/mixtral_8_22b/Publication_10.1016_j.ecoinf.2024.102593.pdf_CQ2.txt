In this case, the deep learning pipeline primarily uses audio data. Specifically, the models mentioned - VGGish, BirdNET, and the custom models - all process sound inputs. These sounds are converted into mel spectrograms using fast Fourier transform, indicating that the raw format of the data is indeed audio. While the models themselves are based on architectures initially designed for image classification (ResNet152), they are adapted for audio analysis in this context. Therefore, the primary data format used in this deep learning pipeline is audio.
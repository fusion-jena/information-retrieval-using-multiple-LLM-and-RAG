The given context does not provide specific details about the data annotation techniques used in the deep learning pipeline for tasks like object detection (bounding box annotation), semantic segmentation, or instance segmentation. However, based on the general practices in machine learning and computer vision, we can infer some possible methods.

In this case, since the research focuses on audio signals, particularly bird vocalizations, the annotations might involve labeling the start and end times of different bird calls within the audio recordings. This process is similar to creating bounding boxes around objects in images but applies to temporal data instead.

Additionally, the researchers may use spectral analysis techniques like Short-Time Fourier Transform (STFT) or Constant Q Transform (CQT) to convert the raw audio signals into visual representations, such as spectrograms or mel-spectrograms. In this scenario, they might also perform manual annotation on these visual representations to highlight regions containing relevant bird sounds.

Moreover, the text mentions using deep neural networks for automatic and task-relevant feature extraction. This implies that the researchers likely employed supervised learning approaches, where labeled examples guide the model during training. Consequently, accurate and consistent data annotation becomes crucial for achieving high performance in the final models.
Adam optimizer is applied in the deep learning pipeline. This is mentioned in the first piece of context where it states, "Additionally, Adam optimizer was selected empirically...". Adam is a popular optimization algorithm used in deep learning due to its ability to adaptively adjust the learning rate based on the historical gradient information. It combines the advantages of both AdaGrad and RMSProp algorithms, making it suitable for problems with large datasets and high dimensionality. Other optimization techniques such as Stochastic Gradient Descent (SGD) or other variants could also be used depending on the specific requirements and characteristics of the problem being addressed. However, the given context only mentions the use of Adam optimizer.
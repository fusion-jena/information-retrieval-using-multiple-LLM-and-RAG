Before training a deep learning model, several preprocessing steps are usually involved to ensure the quality and suitability of the input data. These steps may include:

1. Data Cleaning: This involves removing or correcting any inconsistencies, errors, or missing values in the dataset. It might also include handling outliers or irrelevant data points.

2. Normalization or Scaling: Deep learning algorithms work best when all features have similar scales. Therefore, it's common to normalize or scale the data so that each feature contributes proportionately to the final output. Common techniques include Min-Max scaling, which rescales features to a fixed range like 0-1, and standardization, where features are transformed to have a mean of 0 and variance of 1.

3. Feature Selection or Extraction: Depending on the nature of the problem, certain features might be more relevant than others. Feature selection helps identify these important features, reducing dimensionality and improving model performance. Alternatively, feature extraction transforms raw data into a set of new features that better represent the underlying patterns.

4. Data Augmentation: To improve the robustness and generalizability of the model, synthetic data can be generated from existing samples through various transformations like rotation, zooming, flipping, etc. This technique is especially useful when dealing with image data.

5. Encoding Categorical Variables: Since deep learning models only accept numerical inputs, categorical variables need to be converted into numerical form. Techniques like one-hot encoding or ordinal encoding can be used depending on whether categories have inherent order or not.
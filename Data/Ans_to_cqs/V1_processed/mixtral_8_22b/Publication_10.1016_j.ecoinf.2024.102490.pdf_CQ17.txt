The query asks about regularization methods used to prevent overfitting in a deep learning pipeline. However, none of the provided sources directly discuss regularization methods in deep learning pipelines. Nevertheless, based on the given information, some relevant insights can be drawn.

In the paper by Friedman, Hastie, and Tibshirani (2010), they discuss regularization paths for generalized linear models using coordinate descent. Coordinate descent is an optimization algorithm often employed in machine learning applications, including deep learning. Although not explicitly mentioned, this could imply the use of L1 or L2 regularization techniques, which are commonly applied in conjunction with coordinate descent algorithms.

Additionally, the source mentions multivariate adaptive regression splines (MARS) developed by Jerome H. Friedman (1991). While MARS itself does not involve regularization, its application may require regularization techniques like pruning to avoid overfitting.

However, there is no direct mention of dropout, another popular regularization technique used in deep learning. Dropout involves randomly dropping out neurons during training to reduce co-dependencies among them, thus preventing overfitting.

To summarize, while the provided sources do not provide explicit details regarding regularization methods used in deep learning pipelines, they suggest potential connections to L1 and L2 regularizations through their discussion of coordinate descent and MARS. Nonetheless, without additional context, it cannot be definitively stated whether these regularization methods were specifically utilized within a deep learning framework.
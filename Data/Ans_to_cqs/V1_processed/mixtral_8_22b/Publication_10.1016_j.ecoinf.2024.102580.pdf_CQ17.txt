The text does not provide explicit information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, based on common practices in deep learning, some possible regularization methods include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the network more robust. L2 regularization adds a penalty term to the loss function, encouraging smaller weights and reducing the complexity of the model. Other potential regularization methods could include early stopping, weight decay, or batch normalization. Without further details, it is impossible to determine which specific methods were employed in this case.

Based on the given context, the authors did not explicitly mention any regularization methods used to prevent overfitting in their deep learning pipeline. Therefore, I cannot provide a definitive answer regarding the use of dropout, L2 regularization, or any other specific method. However, it is worth noting that regularization methods like dropout and L2 regularization are commonly used in deep learning pipelines to address overfitting issues. These methods help reduce the model's complexity and improve its generalizability. In addition to dropout and L2 regularization, other popular regularization techniques include early stopping, weight decay, and batch normalization. Nevertheless, without additional information, it remains uncertain whether any of these methods were utilized in this particular study.
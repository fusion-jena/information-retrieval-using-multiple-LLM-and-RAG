Training completion is determined based on several factors, including signs of overfitting and the convergence of the model to its most accurate state. Overfitting occurs when the model focuses too much on data fitting, leading it to mimic insignificant noise instead of the broader properties of interest. To avoid overfitting, models are trained and tested by adjusting the number of required epochs and examining the model outputs and error outcomes. In this study, the model converged to its most accurate state after 10--30 training epochs, while the typical range is between 70 and 250 epochs. Further training did not improve or even worsened the algorithm's performance on held-out test data, suggesting that its long-term behavior was more like memorizing the training set rather than learning a generalized matching strategy. This phenomenon is referred to as overfitting in machine learning. Therefore, the criteria used to determine when training is complete include monitoring the model's performance on both training and testing datasets, looking for signs of overfitting, and ensuring that the model has reached its most accurate state without excessive data fitting.
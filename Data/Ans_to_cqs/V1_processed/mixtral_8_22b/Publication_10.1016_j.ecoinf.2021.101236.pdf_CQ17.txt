The deep learning pipeline employed several regularization methods to prevent overfitting. One such method was dropout, where the dropout probabilities were initially set to 0.5 and gradually reduced step-wise by 0.1 whenever the validation loss stalled. Additionally, early stopping with a cooldown period of 3 epochs was implemented to further mitigate overfitting. Furthermore, the use of batch normalization after most convolutions contributed to regularization by reducing internal covariate shift and improving the stability of the neural network. Lastly, the choice of a wide residual network architecture inherently provided better regularization due to improved regularization in the residual blocks and scaling of the network in width and depth.
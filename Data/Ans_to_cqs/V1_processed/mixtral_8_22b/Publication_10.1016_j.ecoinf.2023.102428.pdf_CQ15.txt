Based on the given context, it appears that the authors use stochastic gradient descent (SGD) as an optimization technique for training their Multilayer Perceptron (MLP) neural network model. This method is employed during the feedforward propagation (FP) algorithm to improve the efficiency of the MLP neural networks. However, there is no mention of other optimization techniques like Adam being utilized in this particular study.
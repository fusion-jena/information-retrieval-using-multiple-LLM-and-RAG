Unfortunately, the given context does not provide information about the specific hardware resources used for training the deep learning models. However, it is common practice to use Graphics Processing Units (GPUs), Tensor Processing Units (TPUs), or other specialized hardware accelerators when training deep learning models due to their computational requirements.

Based on the context, we can infer that the authors likely used some form of GPU or TPU for training their models since they mention employing Bayesian optimization to find optimal hyperparameter values. This process typically involves multiple iterations of training and evaluation, which would require significant computational power. Additionally, the authors reference recent studies that utilize popular deep learning architectures like GoogleNet and Inception-ResNet-v2, both of which are known to benefit greatly from GPU or TPU acceleration during training.

However, without explicit mention of the hardware resources used, we cannot definitively state what type of hardware was employed for training these models.
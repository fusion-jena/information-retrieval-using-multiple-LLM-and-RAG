The context does not provide specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization methods include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent overfitting by reducing the co-dependency between neurons. L2 regularization adds a penalty term to the loss function, which discourages large weights and encourages smaller ones, thus preventing overfitting. Other regularization methods include early stopping, weight decay, and batch normalization. Early stopping involves monitoring the validation error during training and stopping when it starts to increase, indicating overfitting. Weight decay is similar to L2 regularization but uses a different formulation. Batch normalization standardizes the inputs to each layer, which can help reduce overfitting by making the network more robust to changes in input distribution.
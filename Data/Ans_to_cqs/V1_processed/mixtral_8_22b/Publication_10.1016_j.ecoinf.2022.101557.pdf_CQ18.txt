The strategy implemented to monitor the model performance during training involves the use of validation samples. These samples are selected randomly from all areas used in the training set but are never used to fit the model. This approach allows for the evaluation of the model's performance without biasing its learning process. Additionally, dropout with a rate of 0.3 is used during training to enhance stability. The model is trained for 150 epochs with a batch size of 128. ADAM optimization with a learning rate of 0.0001 is employed to minimize the cross-entropy loss between the network's predictions and the pre-labeled data.
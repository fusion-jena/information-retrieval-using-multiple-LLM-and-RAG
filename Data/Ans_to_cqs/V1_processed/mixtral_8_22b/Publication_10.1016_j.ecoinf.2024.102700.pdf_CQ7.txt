The context provided does not explicitly mention the specific criteria used to split the data for deep learning model training. However, it discusses various strategies employed for internal and external validation of models. For internal validation, cross-validation is the most commonly used strategy, specifically k-fold cross-validation. This technique involves dividing the dataset into 'k' subsets or folds, where each fold serves as the testing set once, while the remaining 'k-1' folds form the training set. Other internal validation techniques mentioned include bootstrap, leave-one-out cross-validation, repeated split sampling, simple sample splitting, and generalized cross-validation. External validation strategies involve using a fully independent dataset, but this was only done in a minority of studies. Therefore, while the exact criteria for splitting the data for deep learning model training are not specified, these validation strategies suggest that datasets are typically divided into training and testing sets, often using cross-validation techniques like k-fold cross-validation.
In the given context, there is only one explicit mention of a regularization method used to prevent overfitting in the deep learning pipeline - Dropout. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust and less sensitive to the specific weights of individual neurons. In Table 3, it can be seen that Dropout is applied after MaxPooling in Layers 10 and 11, with a rate of 0.4. However, other regularization techniques such as L2 regularization or early stopping are not explicitly mentioned in the provided context. Therefore, while Dropout is confirmed as a regularization method used in this case, we cannot definitively state whether any other methods were employed without further information.
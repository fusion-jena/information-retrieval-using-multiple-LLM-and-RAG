The completion of machine learning model training can be determined by several criteria, including:

1. Training Loss Plateau: When the training loss stops decreasing significantly or reaches a minimum value, it may indicate that the model has learned all it can from the data and further training will not improve its performance.

2. Validation Loss Plateau: Similarly, if the validation loss stops decreasing or reaches a minimum value, it suggests that the model has achieved optimal performance on unseen data. This criterion is often more reliable than the training loss plateau because it measures how well the model generalizes to new data rather than simply memorizing the training set.

3. Maximum Number of Epochs: In some cases, training may be stopped after a predetermined number of epochs have been completed, regardless of whether the losses have converged. This approach is useful when computational resources are limited or when there is a risk of overfitting due to excessive training.

4. Early Stopping: This technique involves monitoring the validation loss during training and stopping the process early if the loss starts increasing instead of decreasing. Early stopping helps prevent overfitting by terminating training before the model begins to fit noise in the data.

5. Learning Rate Schedule: Some models use a dynamic learning rate that changes throughout training according to a predefined schedule. For example, the learning rate might decrease exponentially every few epochs until it reaches a minimum value. Once the learning rate becomes too small, further training will have little effect on the model's performance, so training can be considered complete.
Based on the given context, there isn't any explicit mention of specific preprocessing steps taken before training the deep learning models. However, some common preprocessing techniques often employed in such scenarios include data normalization, scaling, and cleaning. Data normalization involves rescaling the input features so they fall within a certain range or distribution, typically between 0 and 1 or with zero mean and unit variance. Scaling refers to adjusting the values of numeric columns in the dataset to a standard scale without distorting their differences or losing information. Cleaning entails handling missing values, outliers, and inconsistent data points to improve the overall quality of the dataset. In this case, the authors mentioned applying batch normalization, which is a technique used during training to stabilize the learning process and reduce the risk of overfitting. They also utilized dropout regularization to further mitigate overfitting. These methods contribute to improving the performance and generalizability of the trained models.
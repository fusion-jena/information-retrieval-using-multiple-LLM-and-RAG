The given context does not provide information about the specific regularization methods used to prevent overfitting in the deep learning pipeline mentioned in the paper by Lassalle et al. (2022). However, some common regularization techniques used in deep learning include dropout, L1 regularization, and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust to small variations in input features. L1 regularization adds a penalty term to the loss function based on the absolute value of the weights, while L2 regularization uses the squared values of the weights instead. Both L1 and L2 regularization help reduce overfitting by encouraging smaller weight values, but they differ in their effect on sparsity; L1 regularization tends to produce sparse models with many zero-valued weights, while L2 regularization produces denser models with smaller nonzero weights.
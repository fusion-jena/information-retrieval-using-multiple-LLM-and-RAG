The hyperparameters of the model were optimized using Bayesian Optimization. This technique was applied to minimize a 5 k-fold cross-validation loss during the model development phase. The optimization process was part of a larger machine learning training methodology which included a repeated nested k-fold cross-validation approach. This approach was used to ensure the accuracy and reliability of the results. It involved an outer 10 k-fold cross-validation that divided the data into holdout data and data for model development. Then, an inner 5 k-fold cross-validation was used to minimize the bias caused by tuning of hyperparameters on training results. The training performance was calculated based on the inner cross-validated ensemble, while the testing performance was determined by selecting the optimal set of hyperparameters from the entire training set to predict on the holdout data. This process was repeated 15 times for each unique model to reduce the potential bias and variance related to random sampling.
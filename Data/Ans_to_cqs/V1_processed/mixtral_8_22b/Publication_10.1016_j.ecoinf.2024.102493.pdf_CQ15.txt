Based on the given context, there is no direct mention of any specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being applied in the deep learning pipeline. However, it can be inferred that some form of optimization technique might have been employed during the machine learning processes mentioned in the text. For instance, when discussing the use of the "GD" package in RStudio 4.2.2, it could imply the usage of gradient descent algorithms, which are commonly used optimization techniques in machine learning models. Additionally, the reference to selecting optimal parameters for discretizing continuous data suggests the application of hyperparameter tuning, another common optimization technique in machine learning pipelines. Nevertheless, without explicit details about the specific optimization techniques used, one cannot definitively state whether SGD, Adam, or other optimization techniques were utilized in the deep learning pipeline.
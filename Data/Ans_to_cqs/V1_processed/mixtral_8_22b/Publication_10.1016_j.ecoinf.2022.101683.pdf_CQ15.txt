The text does not provide information about the optimization techniques applied in the deep learning pipeline. However, some commonly used optimization techniques in deep learning include Stochastic Gradient Descent (SGD), Adam, RMSprop, and Adagrad. These techniques help minimize the loss function during the training process by adjusting the weights and biases of the neural network. Each technique has its own strengths and weaknesses, and the choice of optimization technique depends on factors such as the size of the dataset, the complexity of the model, and the desired speed of convergence.
The authors of this study employed several strategies to mitigate overfitting in their deep learning models. One approach they utilized was varying the test set to train set ratio. By observing the performance of the models under different ratios, they found that even when training on only 20% of the data and testing on the remaining 80%, the models still achieved high overall accuracy. This indicates that the models did not suffer significantly from overfitting.

In terms of specific regularization techniques, the authors mentioned the use of a dropout layer associated with the first two fully connected layers (fc{6, 7}) in the AlexNet architecture. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent complex co-adaptations on training data. In this case, the dropout ratio was set to 0.5, meaning half of the neurons in these layers were dropped out during each iteration.

Additionally, weight decay, also known as L2 regularization, was applied with a value of 0.0005. Weight decay adds a penalty term to the loss function proportional to the square of the magnitude of the weights, encouraging smaller weights and thus simpler models.

These regularization methods, along with other factors like the choice of deep learning architecture and training mechanisms, contributed to preventing overfitting in the models used in this study.
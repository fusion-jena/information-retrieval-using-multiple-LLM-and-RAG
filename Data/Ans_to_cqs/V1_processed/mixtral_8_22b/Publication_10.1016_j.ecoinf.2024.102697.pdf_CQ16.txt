Based on the provided context, it appears that the criteria used to determine when training is complete involves a repeated nested k-fold cross-validation approach. Specifically, an outer 10 k-fold cross-validation is used to initially divide the data into holdout data and data for model development. Then, an inner 5 k-fold cross-validation is applied to minimize bias caused by tuning of hyperparameters on training results. The training performance is calculated based on the inner cross-validated ensemble, while the testing performance is determined by selecting the optimal set of hyperparameters from the entire training set to predict on the holdout data. This process is repeated 15 times for each unique model to reduce potential bias and variance related to random sampling. Additionally, Bayesian Optimization is used to minimize a 5 k-fold cross-validation loss during model development. However, the exact criteria for determining when training is complete, such as reaching a certain threshold or plateau in validation loss, is not explicitly stated in the provided context.
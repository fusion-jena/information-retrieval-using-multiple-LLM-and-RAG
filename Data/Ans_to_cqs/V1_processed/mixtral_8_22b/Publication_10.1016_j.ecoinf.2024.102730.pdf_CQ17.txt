The given context does not provide explicit information on all the regularization methods used in the deep learning pipeline. However, it mentions the strength of the L2 regularization term as one of the hyperparameters used in the Optuna grid search. Therefore, it can be inferred that L2 regularization was employed to prevent overfitting in the deep learning pipeline. Unfortunately, the context does not mention other regularization techniques such as dropout.

In general, regularization methods are used to prevent overfitting in machine learning models, including deep learning pipelines. Overfitting occurs when a model learns the noise in the training data instead of the underlying pattern, resulting in poor performance on unseen data. Regularization methods introduce a penalty term to the loss function during optimization, discouraging complex models and promoting simpler ones. Common regularization techniques include L1 regularization, L2 regularization, and dropout.

L1 regularization adds the absolute value of the weights as a penalty term to the loss function, encouraging sparse solutions where many weights become zero. On the other hand, L2 regularization adds the squared magnitude of the weights as a penalty term, favoring smaller weights overall. Dropout is another technique that randomly drops out some neurons during training, preventing the network from relying too heavily on specific neurons and improving its ability to generalize.
In the given context, the optimization technique applied in the deep learning pipeline is the Stochastic Gradient Descent (SGD) algorithm. This algorithm is used to optimize the model weights during the training process. Specifically, the cross-entropy cost function is chosen as the objective function to be minimized. The SGD algorithm iteratively updates the model parameters in the direction that reduces the value of the cost function. It does so by computing the gradient of the cost function with respect to the model parameters and then updating the parameters in the opposite direction of the gradient. This process continues until convergence or a maximum number of iterations is reached. Other popular optimization algorithms used in deep learning include Adam, RMSprop, and Adagrad, among others. However, these are not mentioned in the provided context.
Unfortunately, there isn't any specific mention of preprocessing steps involved before training the deep learning models discussed in the given text. However, generally speaking, common preprocessing steps include data normalization, scaling, cleaning, augmentation, and splitting datasets into training and testing sets. Data normalization involves transforming the data so it fits within a specific range, often between 0 and 1. Scaling adjusts the values measured on different scales to a standard scale. Cleaning removes noise from the data, while augmentation increases the amount of training data through techniques like rotation, zooming, flipping, etc. Lastly, splitting datasets ensures that the model doesn't overfit or underfit during training.
The provided context does not directly address the query regarding the criteria used to determine when training is complete. However, based on general machine learning principles, some common criteria used to determine when training is complete include:

1. Validation Loss Plateau: This occurs when the model's performance on the validation set stops improving or starts degrading after each epoch. At this point, further training may lead to overfitting, so it can be beneficial to stop training.

2. Early Stopping: This technique involves monitoring the model's performance on a validation set during training and stopping the process once the performance stops improving within a certain number of epochs. This helps prevent overfitting and saves computational resources.

3. Maximum Number of Epochs: Training can also be stopped after reaching a predefined maximum number of epochs. This ensures that the training process doesn't continue indefinitely if no other stopping criterion is met.

4. Learning Rate Schedule: Some models use a dynamic learning rate schedule where the learning rate decreases after every few epochs. In such cases, training might be considered complete once the learning rate reaches its minimum value.
To ensure the generalizability of the deep learning models, several measures were taken. Firstly, two diverse datasets, namely UBD_45 and VP_200, were utilized. These datasets likely contained different types of images or features, which helped improve the robustness and applicability of the models across various scenarios. Secondly, the models were initialized with pre-trained ImageNet weights before being fine-tuned to these specific datasets. This transfer learning approach allowed the models to leverage previously learned features and adapt them to new tasks more efficiently. Thirdly, instead of manual selection, Bayesian optimization was employed to find the optimal hyperparameter values for each of the six models. This automated process reduced human bias and improved the overall performance of the models. Lastly, the models were trained using stochastic gradient descent with a momentum ('sgdm') optimizer for a maximum of 10 epochs. Although no explicit mention of cross-validation or stratified splitting was made, these methods could have been implicitly applied during the training phase to further enhance the generalizability of the models.
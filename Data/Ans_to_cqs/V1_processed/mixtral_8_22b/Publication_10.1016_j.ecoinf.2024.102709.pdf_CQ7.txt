In order to find the optimal training-test split for their deep learning model, the authors employed k-fold cross-validation with k=5, epochs=500, and imagesize=640. K-fold cross-validation is a technique where the original sample is randomly partitioned into k equal sized subsamples or folds. Of these k subsamples, one subsample is retained as the validation data for testing the model, while the remaining kâˆ’1 subsamples are used as training data. This process is repeated k times, with each of the k subsamples being used exactly once as the validation data. The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop.

In this case, the data was divided into five different training-test splits. Each time, the model was trained on one of the four training splits, leaving out one fold for validation. The function of the Python library scikit-learn was utilized to perform the splitting and evaluate the outcomes of every trained model. Additionally, YOLO training losses and mean Average Precision (mAP50) values of each model were examined. The split with the highest mAP50 value signified the greatest number of accurately predicted labels for the model trained on a particular split. Ultimately, the split with the highest mAP50 (0.987) was chosen for the final model's predictions.
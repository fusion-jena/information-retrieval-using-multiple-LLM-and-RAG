The text mentions that the deep learning models were trained using stochastic gradient descent with a momentum of 0.9 and an initial learning rate of 10^-2. Additionally, every 20 epochs, the learning rate was divided by 10 until reaching 10^-6. However, there is no explicit mention of the specific optimizer used. Therefore, we can only confirm that the learning rate schedule and momentum value were part of the hyperparameters used during training.
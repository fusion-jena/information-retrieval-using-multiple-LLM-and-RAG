The hyperparameters of the model are optimized using the grid search technique. This technique involves testing various combinations of hyperparameters and selecting the one that yields the best performance score. For both Multilayer Perceptron (MLP) and Random Forest (RF) predictors, this method was applied to determine the optimal values of their respective hyperparameters.

In the case of MLP, the hyperparameters tested included solvers such as Limited-Broyden–Fletcher–Goldfarb–Shanno (L-BFGS), Adam, Stochastic Gradient Descent (SGD) with constant learning rate, and SGD with adaptive learning rate. Other parameters like the number of neurons in the hidden layers (ranging from 1 to 100), regularization factor ("alpha" or L2 penalty), activation functions (identity, logistic, tanh, ReLU), and learning rates (constant, invscaling, adaptive) were also evaluated.

Similarly, for the RF, the hyperparameters tested using the grid search technique included the number of decision trees in the forest (from 100 to 1000 in steps of 100), the maximum depth of the decision tree, the number of features considered by each tree when splitting a node, among others.

After evaluating all these combinations, the configuration that optimized the performance of the model was chosen for further analysis. Specifically, for the MLP, the Adam solver with a constant learning rate, 10 neurons per hidden layer, alpha equal to 10, and the rectifier activation function (also known as the Rectified Linear Unit or ReLU) were used.
Based on the given context, there is no explicit mention of specific regularization methods such as dropout or L2 regularization being employed in the deep learning pipeline. However, some measures have been taken to address potential overfitting issues. These include using early stopping during training, which halts the training process once the model's performance on the validation set stops improving. Additionally, the context mentions utilizing the binary cross entropy based focal loss and scaled focal loss, which could potentially help mitigate the impact of class imbalance and improve the model's ability to generalize better. Furthermore, the context discusses employing a data set sampler by upsampling and duplicating randomly selected samples and augmenting them, which might contribute to reducing overfitting by providing more diverse training examples.
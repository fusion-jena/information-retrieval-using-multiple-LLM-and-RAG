The given context does not provide specific information regarding the regularization methods used to prevent overfitting in the deep learning pipeline. However, some commonly used regularization techniques include Dropout, L1 Regularization, and L2 Regularization.

Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust and reducing its dependency on any single input feature or neuron.

L1 Regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function, encouraging sparse solutions with fewer nonzero weights. This can help reduce overfitting by simplifying the model.

L2 Regularization adds a penalty equal to the square of the magnitude of coefficients to the loss function, encouraging smaller weight values. This can also help reduce overfitting by preventing large weight values that may cause the model to fit too closely to the training data.

These regularization techniques can be applied individually or combined to improve the performance of deep learning models and prevent overfitting.
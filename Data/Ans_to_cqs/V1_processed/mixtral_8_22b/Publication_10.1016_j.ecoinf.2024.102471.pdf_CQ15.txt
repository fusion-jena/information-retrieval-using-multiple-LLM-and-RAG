Adam is used as an optimizer in the deep learning pipeline described in the context. It is mentioned that Adam is used as an optimizer with an initial learning rate of order 10e-5, which is appropriate for small weight updates throughout the network while fine-tuning. Other hyperparameters such as epochs of 30 with batch size 1 are also chosen as optimal values based on the manual fine-tuning process. However, no other optimization techniques are explicitly mentioned in the provided context.
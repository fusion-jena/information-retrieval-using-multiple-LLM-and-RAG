Based on the given context, there isn't explicit information about how the hyperparameters of the XGBoost model were optimized. However, common techniques for hyperparameter tuning include Grid Search, Random Search, Bayesian Optimization, and Gradient-based optimization. These methods involve iteratively trying different combinations of hyperparameters and evaluating their performance on a validation set. Without further details from the authors or the papers cited, we cannot definitively state which method was used for this particular study.
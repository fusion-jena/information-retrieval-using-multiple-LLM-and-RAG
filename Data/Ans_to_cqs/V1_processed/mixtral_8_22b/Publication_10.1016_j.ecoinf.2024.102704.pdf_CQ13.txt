Unfortunately, the given context does not provide information about the hyperparameters used in the deep learning model, such as the learning rate or optimizer. Instead, it focuses on techniques to reduce memory access and improve computational efficiency in Convolutional Neural Networks (CNNs) using different architectures like ResNet-50, Inception V3, and DenseNet. It discusses how reconfiguring the computation sequence of 'convolutional layer + batch normalization + activation layer' can lead to reductions in memory access and improvements in computational efficiency. Additionally, it mentions the use of different scales ('n','s','m', 'l', and 'x') to meet varying scenario and task requirements, but there is no mention of specific hyperparameters used in these models.
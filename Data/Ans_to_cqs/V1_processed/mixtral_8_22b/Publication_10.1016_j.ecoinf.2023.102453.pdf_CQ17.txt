In the given context, there is no explicit mention of specific regularization methods used to prevent overfitting in the deep learning pipeline. However, we do see that the EfficientNetV2B0 model uses progressive learning to adapt regularization to the image size (Tan and Le, 2021). Regularization techniques help reduce overfitting by adding constraints to the learning process, making the model more generalized and improving its ability to perform well on unseen data.

While the context does not provide details about the exact regularization method employed, popular techniques include dropout, L1/L2 regularization, early stopping, and data augmentation. Dropout involves randomly deactivating neurons during training to prevent co-adaptations between them. L1/L2 regularization adds penalty terms to the loss function, discouraging complex models and promoting simpler solutions. Early stopping halts the training process when validation error starts to increase, preventing overtraining. Data augmentation generates new samples by applying transformations like rotation, scaling, and flipping, expanding the dataset and reducing overfitting risk.

Without further information, it cannot be confirmed if any of these methods were specifically applied in the mentioned studies. Nonetheless, they remain relevant examples of regularization techniques commonly utilized in deep learning pipelines to mitigate overfitting.
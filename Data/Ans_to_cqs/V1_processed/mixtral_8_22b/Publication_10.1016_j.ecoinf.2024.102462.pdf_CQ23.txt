To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the data was divided into four non-overlapping subsites to maximize the independence between the training and testing sets. This division aimed to prevent overfitting and improve the model's ability to handle unseen data. Secondly, instead of randomly dividing a single data set into training and testing sets, which may lead to overestimation of practical performance, the study focused on creating more realistic conditions. These conditions included variations in class distribution between training and testing sets, which could potentially affect the model's performance. However, these unfavorable conditions did not significantly impact the model's performance, as it still achieved a weighted accuracy of 71.76%, surpassing human observers. Lastly, while data augmentation or building datasets that better reflect real percentages were suggested as potential improvements, they were not explicitly mentioned as part of the measures taken to ensure the generalizability of the deep learning model.
The deep learning model uses several hyperparameters, including those that define its architecture and those that control the training process. For the architecture, the number of layers and the number of neurons are considered. Regarding the training process, the main hyperparameters include the learning rate, the optimizer, the batch size, and the number of epochs. Specifically, the learning rate was not mentioned explicitly but was likely tuned during the optimization process. The optimizer used was the Adam algorithm, which is a type of stochastic gradient descent method. The batch size was also not specified, suggesting that it may have been kept at a default value or determined through experimentation. Lastly, the number of epochs was initially set to 500 but later adjusted to the minimum number of epochs that guaranteed the stabilization of the loss curve during the training process.
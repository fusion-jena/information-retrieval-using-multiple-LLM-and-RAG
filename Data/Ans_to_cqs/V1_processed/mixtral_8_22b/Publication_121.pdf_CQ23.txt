To ensure the generalizability of their deep learning model, several measures were implemented during its development and training. One key measure was the selection of diverse datasets for training and testing purposes. The model was trained on ImageNet dataset, which consists of 1.4 million images across 1000 different classes. This diversity helped prevent overfitting and ensured that the model could perform well on unseen data.

Additionally, the authors employed transfer learning, a technique that leverages knowledge gained from solving one problem to solve another related problem. In this case, they utilized pre-trained models from MobileNetV2, MobileNetV3-Large, and MobileNetV3-Small architectures. By doing so, they took advantage of the already existing knowledge embedded within these models, thereby improving the performance of their own model without requiring extensive additional training data or computational resources.

Furthermore, the dataset was split into a training set and a test set in a 75:25 ratio. While it isn't explicitly mentioned whether stratified sampling was used, the large size of the dataset suggests that both sets would still contain sufficient examples from each class to maintain representativeness.

Lastly, the authors also incorporated a dropout layer with a rate of 0.2 to further mitigate potential overfitting issues. Dropout is a regularization method that randomly drops out neurons during training, effectively preventing complex co-adaptations between them. As a result, the model becomes more robust and better able to generalize to new data.

In summary, the authors ensured the generalizability of their deep learning model through various means including utilizing a diverse dataset, employing transfer learning, implementing a reasonable train/test split, and incorporating a dropout layer for regularization.
Based on the given context, there isn't explicit information about the specific strategy implemented to monitor the model performance during training. However, there are indications of how the model's performance was evaluated after training.

From the context, we can infer that the model's performance was assessed using metrics such as Average Precision (AP), precision, and recall. These metrics were calculated based on the number of true positives, false positives, and false negatives produced by the model when making predictions.

Additionally, the context mentions that the model's performance was compared across different species, and it was found that there wasn't a clear association between training sample size and species-specific performance. Some species with smaller training sample sizes still achieved strong performance.

Furthermore, the context suggests that the model's overall performance was influenced by the frequency of species presence, which might be related to the correlation between species frequency and training sample size. Rarer species required more robustness to noise to achieve low False Positive Rate (FPR) and high precision.

In summary, while the context does not provide direct information about the monitoring strategy during training, it indicates that the model's performance was evaluated post-training using various metrics and comparing results across different species.
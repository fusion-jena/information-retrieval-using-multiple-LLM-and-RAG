To handle randomness in the deep learning pipeline, several strategies can be employed. One common strategy is to use fixed random seeds during model training and evaluation. By setting a specific seed value, researchers can ensure reproducibility of their experiments and eliminate any potential bias introduced by random initialization. Another strategy is to perform multiple runs with different random seeds and report average performance metrics. This helps to reduce the impact of outliers and provides a more robust estimate of the model's performance. Additionally, some deep learning frameworks offer deterministic algorithms that do not rely on random numbers, providing another way to mitigate the effects of randomness. However, it should be noted that these strategies may not completely eliminate all sources of randomness, and careful consideration must still be given to factors such as data sampling and hyperparameter tuning.

Based on the provided context, there is no information regarding handling randomness in the deep learning pipeline. Therefore, I cannot provide a helpful answer to the query.
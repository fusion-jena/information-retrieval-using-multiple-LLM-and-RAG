The given context does not provide information about specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) used in the deep learning pipeline. Instead, it discusses the application of ensemble methods like bagging and boosting in machine learning models, specifically focusing on Random Forests (RF) and Extreme Gradient Boosting (XGBoost).

In the context of optimizing the iteration number for XGBoost, a nested k-fold cross-validation scheme is employed. This involves splitting datasets into three subsets - tuning, validation, and test sets. For each fold, the model is trained on the tuning set, and its performance is monitored on the validation set by calculating the out-of-sample prediction accuracy. The median of k-1 folds is taken to give k medians for each number of iterations.

However, there is no mention of any optimization techniques typically associated with deep learning pipelines, such as SGD or Adam. These techniques are commonly used to update neural network weights during training to minimize loss functions. Therefore, based on the provided context, it cannot be determined what optimization techniques are applied in the deep learning pipeline.
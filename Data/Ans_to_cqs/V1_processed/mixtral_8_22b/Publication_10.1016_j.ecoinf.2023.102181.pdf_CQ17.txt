The text does not provide explicit information about the specific regularization methods used in the described machine learning pipeline. However, it mentions that the study uses random forest models, which inherently incorporate a type of regularization through their construction. Random forests work by creating multiple decision trees and averaging their predictions, which helps reduce overfitting. This process is known as ensemble learning. Additionally, the study mentions using covariance-weighted bagging, which might imply some form of weight decay or L2 regularization, although this is not explicitly stated. Dropout, another common regularization technique often used in deep learning, is not mentioned in the given context.
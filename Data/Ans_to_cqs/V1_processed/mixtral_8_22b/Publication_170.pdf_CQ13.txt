Unfortunately, there isn't enough information available in the given context to determine the specific hyperparameters used in the deep learning model such as the learning rate or the type of optimizer employed. However, it does mention that the iteration termination condition of the model was the same in each round training, where the loss rate of the model was less than a preset threshold ɛ −4. This suggests that some form of early stopping mechanism was implemented during the training phase to prevent overfitting. Additionally, the main configuration parameters of the two platforms used are listed in Table 2, but these do not provide any insight into the specific hyperparameters used within the deep learning model itself.
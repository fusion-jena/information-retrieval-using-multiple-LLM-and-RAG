The context does not provide specific information about the techniques used to address data bias during preprocessing of a deep learning pipeline. However, based on general knowledge, some common techniques used to address data bias include stratified splitting, oversampling, undersampling, and diverse data collection.

Stratified splitting involves dividing the dataset into training and testing sets while maintaining the same distribution of classes or labels in both sets. This ensures that the model is trained and tested on representative samples of each class.

Oversampling and undersampling involve adjusting the number of instances in each class to balance the dataset. Oversampling increases the number of instances in the minority class by duplicating existing instances or generating new synthetic instances using techniques such as SMOTE (Synthetic Minority Over-sampling Technique). Undersampling reduces the number of instances in the majority class by randomly removing instances or selecting only those instances that contribute most to the classification task.

Diverse data collection involves collecting data from different sources, demographics, or environments to ensure that the dataset is representative of the population being studied. This can help reduce bias caused by overrepresentation of certain groups or underrepresentation of others.

Other techniques used to address data bias during preprocessing include normalization, standardization, feature scaling, and dimensionality reduction. Normalization scales numerical features to a range between 0 and 1, while standardization transforms features to have zero mean and unit variance. Feature scaling rescales features to have similar magnitudes, which can improve the performance of distance-based machine learning algorithms. Dimensionality reduction techniques such as PCA (Principal Component Analysis) and t-SNE (t-distributed Stochastic Neighbor Embedding) can be used to reduce the number of features in high-dimensional datasets, which can also help reduce bias caused by irrelevant or redundant features.
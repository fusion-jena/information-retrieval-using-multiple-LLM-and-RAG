Based on the given context, there is no explicit mention of any data augmentation techniques like flipping, rotation, or scaling being used in the deep learning pipeline. However, one technique mentioned is the use of a dropout layer, which is a form of regularization rather than traditional data augmentation. This technique helps prevent overfitting during the training process by randomly excluding certain inputs from activations and weight updates. Additionally, the context mentions the use of a MinMaxScaler, which scales each feature to a range between 0 and 1, but this is more of a normalization technique rather than data augmentation. Therefore, based solely on the provided information, I cannot identify any specific data augmentation techniques applied in the deep learning pipeline.
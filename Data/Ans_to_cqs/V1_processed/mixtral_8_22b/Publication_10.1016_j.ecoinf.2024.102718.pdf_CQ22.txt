To evaluate the performance of the deep learning models, several metrics are employed. These include Accuracy, Loss, Area Under Curve (AUC), Cohen’s Kappa (CK), Precision, Recall (also known as Sensitivity), F1 Score, Model Size, Inference Time, Floating Point Operations Per Second (FLOPs) count, and Analysis Time.

Accuracy measures the proportion of correctly classified instances out of the total instances in the dataset. This gives an overall indication of how well the model is performing. The formula for calculating accuracy is:

Accuracy = Number of Correct Predictions / Total Number of Predictions

Loss represents the error between predicted and actual values in a model. Lower loss indicates better performance.

Area Under Curve (AUC) represents the area under the Receiver Operating Characteristic (ROC) curve. Higher AUC means better performance.

Cohen’s Kappa (CK) measures the agreement between two raters classifying items into mutually exclusive categories. It takes into account the possibility of the agreement occurring by chance. CK is calculated as:

CK = po - pe / 1 - pe

where po is the observed agreement and pe is the expected agreement.

Precision measures the proportion of true positive predictions among all positive predictions made by the model. High precision relates to low false positive rate. The formula for calculating precision is:

Precision = True Positives / (True Positives + False Positives)

Recall (or sensitivity) measures the proportion of true positive predictions among all actual positive instances in the dataset. High recall relates to low false negative rate. The formula for calculating recall is:

Recall = True Positives / (True Positives + False Negatives)

F1 score is the harmonic mean of precision and recall. It tries to balance both precision and recall. The formula for calculating F1 score is:

F1 = 2 * (Precision * Recall) / (Precision + Recall)

Model Size refers to the size of the trained model, usually measured in terms of parameters or memory footprint. Smaller model sizes are generally preferred due to lower computational requirements.

Inference Time measures the time taken by the model to process a single input and generate an output prediction. Shorter inference times indicate faster processing speed.

Floating Point Operations Per Second (FLOPs) count represents the number of arithmetic operations performed by the model during inference or training. Lower FLOPs count implies more efficient computation.

Analysis Time measures the time taken by the model to process a given dataset or perform a specific task. Shorter analysis times suggest quicker data processing capabilities.
To ensure the generalizability of the deep learning model, several measures were implemented during the development process. Firstly, the dataset used for training and validating the model consisted of diverse images collected from various sources, including Google Images, Flickr, and iNaturalist. This diversity helped capture different aspects of the target species, such as variations in lighting conditions, angles, and background elements. Secondly, the images were preprocessed by resizing them to a common resolution (227x197 pixels) and normalizing them to the [0,1] range. This step ensured consistency across the input data and facilitated efficient processing by the deep learning models. Thirdly, six popular convolutional neural networks (CNNs) were employed for the classification task, namely VGG16, ResNet50, ResNet101, Inception-v3, DenseNet201, and EfficientNetB0. These architectures have demonstrated excellent performance on similar classification tasks and are well-suited for transfer learning. Fourthly, the optimization of these models was carried out using the Adam optimizer algorithm, with a batch size of 10 and 100 epochs. Learning rates were determined through empirical trials over 100 epochs. Lastly, the performance of the models was evaluated using 5-fold cross-validation over the entire dataset. This technique involves dividing the dataset into five equal parts or 'folds,' where four folds are used for training and the remaining fold is utilized for testing. This procedure is repeated five times, ensuring that every fold serves as the test set once. Cross-validation helps prevent overfitting and provides a more accurate estimate of the model's performance on unseen data. Overall, these measures aimed to enhance the robustness and reliability of the deep learning model when applied to new datasets.
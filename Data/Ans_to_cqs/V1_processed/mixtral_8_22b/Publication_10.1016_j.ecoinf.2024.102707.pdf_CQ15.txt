The given context does not provide specific information on the optimization techniques applied in the deep learning pipelines mentioned. However, it is common practice in deep learning to use optimization algorithms such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) to minimize the loss function during training. These techniques adjust the weights and biases of the neural network based on the gradient of the error with respect to those parameters. Without explicit mention in the provided context, one cannot definitively state which optimization technique was used in each case.
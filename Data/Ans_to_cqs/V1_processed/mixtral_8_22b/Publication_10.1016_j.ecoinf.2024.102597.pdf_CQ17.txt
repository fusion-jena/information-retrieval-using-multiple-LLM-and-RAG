The text does not provide specific information about the regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization techniques used in deep learning models include dropout, L1 regularization, and L2 regularization. Dropout randomly sets a fraction rate of input units to 0 during training time, helping to avoid overfitting. L1 regularization adds an absolute value of the magnitude of coefficients as penalty term to the loss function, while L2 regularization adds the squared magnitude of coefficient as penalty term to the loss function. Both L1 and L2 regularizations help to reduce overfitting by adding constraints to the weights of the model. Other regularization techniques include early stopping, weight decay, and batch normalization.
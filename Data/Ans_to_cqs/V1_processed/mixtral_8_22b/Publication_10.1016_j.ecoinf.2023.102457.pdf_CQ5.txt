In this case, the authors used three different deep-learning architectures to extract embeddings from the female calls. The first one mentioned is VGGish, which was trained by Google to perform general-purpose audio classification using a preliminary version of the YouTube-8M dataset. Another model used is Wav2Vec 2.0, although it's not explicitly stated what dataset it was trained on; however, it can be assumed that it was trained on a human speech audio dataset since it's designed for automatic speech recognition tasks. Lastly, they also used the BirdNET model, which was specifically trained on audio data of bird vocalizations. Therefore, the datasets used in their deep learning pipeline include a preliminary version of the YouTube-8M dataset, a human speech audio dataset, and a dataset containing bird vocalization sounds.
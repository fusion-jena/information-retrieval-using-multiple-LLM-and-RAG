The text does not provide explicit information about the specific criteria used to determine when training is completed. However, it mentions that the dataset was increased by 50% through data augmentation, which is important for small training datasets like the one being used. This suggests that the authors might have been looking for ways to improve the performance of their models despite having limited training data.

In general, common criteria used to determine when training is complete include reaching a certain number of epochs or iterations, achieving a desired level of performance on a validation set, observing a plateau in the validation loss, or meeting some other stopping condition based on the learning dynamics of the model. Given that the authors mention using a held-out test set for accuracy assessment and a separate validation set during fine-tuning, it can be inferred that they likely monitored the performance of their models on these sets to decide when to stop training. However, without more specific details, it is difficult to definitively state what exact criteria were used to determine when training was complete.
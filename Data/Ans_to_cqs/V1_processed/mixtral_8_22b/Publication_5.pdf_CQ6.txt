Based on the given context, it does not provide explicit information about the specific preprocessing steps taken before training the deep learning model. However, generally speaking, some common preprocessing steps often applied before training a deep learning model include:

1. Data Cleaning: This involves removing any irrelevant or redundant data from the dataset. It also includes handling missing values by either filling them with appropriate values or dropping those records altogether.

2. Normalization/Standardization: These techniques are used to scale the input features so they fall within a similar range. Normalization scales the data to lie between 0 and 1, while standardization transforms the data to have a mean of 0 and a standard deviation of 1.

3. Feature Selection/Extraction: This step involves selecting relevant features or extracting new ones from existing data. This can help improve the model's performance and reduce computational complexity.

4. Data Augmentation: This technique is commonly used when dealing with images. It involves creating new samples by applying different transformations like rotation, zooming, flipping etc.

Again, these are general practices and may vary depending upon the nature of the problem and the type of data being dealt with. Without more specific details about the particular deep learning model mentioned in the context, it isn't possible to state exactly what preprocessing steps were undertaken.
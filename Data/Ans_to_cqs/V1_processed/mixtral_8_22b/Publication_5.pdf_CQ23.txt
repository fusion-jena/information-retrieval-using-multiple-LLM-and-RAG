The text does not provide explicit information about specific measures taken to ensure the generalizability of the deep learning model. However, some clues can be found regarding the use of diverse datasets and parameter tuning techniques.

Firstly, the authors mention using a dataset from Korea for model training and testing, suggesting they employed a geographically diverse dataset. This could help improve the model's ability to generalize across different regions.

Secondly, the authors discuss using TensorFlow for developing their species richness estimation model. They also mention performing iterative grid search parameter tuning using a small subset of their data. These steps indicate that the researchers took care in selecting appropriate parameters for their model, which may contribute to its overall robustness and generalizability.

Lastly, while not explicitly mentioned, the authors might have utilized cross-validation or stratified splitting during the development of their deep learning model. Cross-validation involves dividing the dataset into subsets and repeatedly fitting the model to different combinations of these subsets. Stratified splitting ensures that each subset maintains the same class distribution as the original dataset. Both techniques can help assess the model's performance and generalizability more accurately.

In conclusion, although the text does not directly state the specific measures taken to ensure the generalizability of the deep learning model, it suggests the use of diverse datasets and careful parameter tuning. Additionally, the possibility exists that the authors implemented cross-validation or stratified splitting techniques during model development.
The paper does not explicitly mention any specific techniques such as diverse datasets, cross-validation, or stratified splitting to ensure the generalizability of their deep learning models. However, they do discuss using stochastic gradient descent (SGD) optimization with an initial learning rate of 0.01 for most models and 0.1 for MnasNet. They also employed batch normalization and trained the models for 150 epochs using Nvidia Tesla K80 or P100 GPUs. Additionally, they mentioned using data augmentation, which can help improve the generalizability of deep learning models by increasing the diversity of training samples.
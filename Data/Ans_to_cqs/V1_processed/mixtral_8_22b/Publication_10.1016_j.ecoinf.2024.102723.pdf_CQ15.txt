The optimization technique applied in the deep learning pipeline mentioned in the given context is Adam. This can be found in the description of the MCD network where it states, "The optimizer is Adam." Adam is an optimization algorithm that can be used instead of stochastic gradient descent (SGD) for training deep learning models. It calculates adaptive learning rates for different parameters, providing an efficient way to converge towards the optimal solution.
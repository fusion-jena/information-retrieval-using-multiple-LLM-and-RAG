The text does not provide information about how the hyperparameters of the model were optimized. It only mentions that consistent hyperparameters were maintained across all training instances for each task, specifically setting the batch sizes to 5 and 4, and the learning rates to 1e-4 and 1e-3 for SEIO PBW vocalization training and blue whale Dcall training, respectively. These decisions were based on the fact that more samples were used for SEIO PBW vocalization training. However, it does not mention any specific optimization technique such as grid search or random search.
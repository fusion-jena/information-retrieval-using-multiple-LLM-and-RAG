After the model training process, several postprocessing steps can be taken to evaluate the model's performance and interpret its results. Some common postprocessing steps include:

1. Calculating metrics: Various metrics such as accuracy, precision, recall, F1 score, and AUC-ROC can be calculated to quantify the model's performance. These metrics help determine how well the model is performing and whether it needs further tuning or optimization.

2. Generating confusion matrices: A confusion matrix provides a visual representation of the model's predictions compared to actual outcomes. It helps identify where the model is making mistakes and which classes are being confused with others.

3. Creating saliency maps: Saliency maps highlight areas of input data that contribute most significantly to the model's output. They can be useful for understanding what features the model is focusing on when making predictions.

In the given context, the authors have employed various techniques to validate their model's performance. They utilized 30% of each taxon's data records as testing data and the remaining 70% as training data. The models were then evaluated based on multiple criteria, including AUC, TSS, Cohen's Kappa, point-biserial correlation, and ROC. Additionally, they applied 10-fold cross-validation and 10-fold bootstrapping to ensure robustness and reduce variance.

The authors also mentioned that the sensitivity-specificity sum maximization technique was used for determining the model threshold. This method aims to find the optimal balance between false positives and false negatives by maximizing both sensitivity and specificity simultaneously.

However, there is no explicit mention of generating confusion matrices or creating saliency maps in the provided context. Nevertheless, these techniques could still be applicable depending on the specific requirements and objectives of the study.
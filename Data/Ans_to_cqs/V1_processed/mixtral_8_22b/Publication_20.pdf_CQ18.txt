The authors monitored the model performance during training by stopping the network training after 70 epochs to prevent overfitting. They used a learning rate of 10^-5 with an exponential learning decay, a gamma of 0.95, a dropout of 50%, and an Adam solver type as learning parameters. These are classic hyperparameters for fast convergence of the network without overfitting. The training lasted 8 days on their configuration, which included a computer with 64GB of RAM, an i7 3.50GHz CPU, and a Titan X GPU card for processing 900,000 images. Additionally, they used at least 2200 thumbnails per fish species class and batches of 240 images for training.
The given context does not provide information about the specific regularization methods used to prevent overfitting in a deep learning pipeline. However, some common regularization techniques include Dropout and L2 Regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust by preventing complex co-adaptations on training data. On the other hand, L2 Regularization adds a penalty term to the loss function, discouraging large weights and thus reducing overfitting. Other popular regularization methods include L1 Regularization, Early Stopping, and Batch Normalization. These techniques can be applied individually or combined to improve the performance of deep learning models.
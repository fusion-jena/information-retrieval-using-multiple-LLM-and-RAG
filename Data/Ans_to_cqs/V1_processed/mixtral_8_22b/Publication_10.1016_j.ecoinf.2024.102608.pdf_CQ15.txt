The given context does not provide explicit information about specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam) being applied in the deep learning pipeline. However, it mentions the use of Randomized Grid Search method for exploring the parameter space, which is a technique often used in conjunction with various optimization techniques during the process of tuning hyperparameters in machine learning and deep learning models. It should be noted that while this method is useful for finding optimal hyperparameters, it doesn't directly refer to optimization techniques like SGD or Adam, which are typically used for updating weights within the neural network architecture itself.
The text mentions two regularization methods used to prevent overfitting in the deep learning pipeline. These methods are batch normalization and dropout layers.

Batch normalization assists in feature extraction of convolutional layers by performing feature generalization of the input mini-batch. This is done by calculating the mean and standard deviation of the batch normalization layer to avoid covariate shifts.

Dropout layers help reduce overfitting of the CNN model by inactivating random nodes containing learnable weights. This method randomly sets a fraction rate of input units to 0 at each update during training time, effectively preventing complex co-adaptations on training data.

These techniques contribute to improving the overall performance and accuracy of the deep learning models by reducing overfitting and enhancing their ability to generalize well to new, unseen data.
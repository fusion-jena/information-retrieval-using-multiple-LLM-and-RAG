The deep learning models mentioned in the given context include Wide-ResNet101, InceptionV3, ResNet101, and MnasNet-A1. All these models, except MnasNet-A1, use the Stochastic Gradient Descent (SGD) optimizer with an initial learning rate of 0.01. For MnasNet-A1, the initial learning rate was set at 0.1. The learning rate was reduced by a factor of 10 after every 30 epochs. Batch normalization was also employed during training. These models were trained for 150 epochs using either Nvidia Tesla K80 or P100 GPUs.
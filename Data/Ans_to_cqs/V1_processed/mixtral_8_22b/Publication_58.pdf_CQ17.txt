Based on the given context, there is no explicit mention of any specific regularization methods such as dropout or L2 regularization being used in the deep learning pipeline. However, the context does provide information about other aspects of the training process. For instance, stochastic gradient descent was used with a momentum of 0.9 and an initial learning rate of 10^-2, which was then reduced every 20 epochs until reaching 10^-6. Additionally, neural networks were trained with balanced datasets to improve performance, specifically using the median frequency balancing method. While these details contribute to understanding the overall training procedure, they do not directly address the query regarding regularization methods used to prevent overfitting. Therefore, based solely on the provided context, I cannot determine what regularization methods were employed to prevent overfitting in the deep learning pipeline.
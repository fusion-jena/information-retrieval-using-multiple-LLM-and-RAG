The authors do not explicitly discuss strategies for handling randomness in the deep learning pipeline, such as managing random seed values. However, it is common practice in deep learning to use a fixed random seed value when training models to ensure reproducibility of results. This allows researchers to compare different model architectures or hyperparameters while controlling for the effects of random initialization. Additionally, techniques like cross-validation and ensemble methods can help mitigate the impact of randomness on model performance. In this study, the authors likely employed some combination of these strategies, although specific details are not mentioned.

Based on the given context, I cannot provide a detailed explanation of the exact strategies used in this particular study regarding handling randomness in the deep learning pipeline. However, generally speaking, using a fixed random seed value, employing cross-validation, and utilizing ensemble methods are common practices to manage randomness in deep learning pipelines.
The paper uses the AdamW optimizer during all three phases of their investigation. This optimizer is a variant of the popular Adam optimizer, which combines the advantages of both adaptive gradient methods like Adagrad and momentum-based methods like Momentum. The AdamW optimizer further improves upon Adam by adding weight decay regularization, which helps prevent overfitting and improve generalization performance. Additionally, the authors mention using a learning rate of 10^-4 during the first two phases of their study. However, they do not specify whether this learning rate is constant throughout training or if it decays over time. Overall, the choice of optimization technique and hyperparameters can greatly impact the performance of deep learning models, so it is important to carefully consider these factors when designing and implementing such pipelines.
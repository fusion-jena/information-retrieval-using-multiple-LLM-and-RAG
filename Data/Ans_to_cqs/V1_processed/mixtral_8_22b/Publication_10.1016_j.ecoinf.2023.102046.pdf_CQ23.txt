To ensure the generalizability of the deep learning model, several measures were taken. Firstly, the dataset used for training and testing consisted of diverse samples from various locations and seasons, ensuring that the model could learn patterns applicable across multiple scenarios. Secondly, class weights were included in the cross-entropy loss function to address any potential dataset imbalances. This approach helps prevent the model from being biased towards classes with more data points. Lastly, the model with the smallest validation loss was chosen for evaluation on the test set after conducting fine-tuning experiments. This strategy ensures that the final model selected for evaluation is the one that performs best on unseen data, thus promoting its ability to generalize.
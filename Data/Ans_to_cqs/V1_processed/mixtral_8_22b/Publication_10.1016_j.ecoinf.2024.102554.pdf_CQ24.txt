The context does not directly address the specific strategies used to handle randomness in the deep learning pipeline, such as managing random seed values. However, it can be inferred from general practices in machine learning and deep learning that there are several common approaches to manage randomness:

1. Fixed Random Seed: One way to ensure reproducibility in experiments involving random processes like shuffling datasets or initializing weights in neural networks is by setting a fixed random seed value. This ensures that every time the code runs, it will produce the same sequence of random numbers, making results consistent across different runs.

2. Cross-Validation: To reduce the impact of randomness on model performance evaluation, techniques like k-fold cross-validation can be employed. In this method, the dataset is divided into 'k' subsets, and the model is trained and tested 'k' times, using a different subset for testing each time while keeping the rest for training. This helps in obtaining a more robust estimate of the model's performance.

3. Ensemble Methods: Another strategy to mitigate the effects of randomness is through ensemble methods, where multiple models are created and their predictions combined to obtain better overall performance. Techniques like bagging and boosting fall under this category.

4. Hyperparameter Tuning: By systematically searching over possible hyperparameters and selecting the best combination based on validation set performance, one can minimize the effect of random initialization on final model performance. Grid search and random search are popular methods for hyperparameter tuning.

While the given context does not explicitly discuss these strategies, they are commonly used in handling randomness within deep learning pipelines.
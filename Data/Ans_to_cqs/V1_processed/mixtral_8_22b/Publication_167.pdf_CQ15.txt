The deep learning pipeline uses the Adam optimization algorithm to update the weights of the neural network. This information is mentioned in both pieces of context provided. However, it does not explicitly state whether other optimization techniques such as Stochastic Gradient Descent (SGD) were also considered or tested during the development of the pipeline. Therefore, based on the given context, we can only confirm that the Adam optimization technique was applied in the deep learning pipeline.
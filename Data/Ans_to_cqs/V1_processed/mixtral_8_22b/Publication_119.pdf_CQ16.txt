In the given context, there isn't explicit information about the specific criteria used to determine when training is complete. However, we can infer from the text that the authors might have used some common machine learning practices to decide when to stop training their models.

One possible criterion could be monitoring the validation loss during training. When the validation loss stops decreasing significantly or starts increasing, it may indicate that the model has started to overfit the training data, and further training would not improve its performance on unseen data. This point where the validation loss plateaus or increases is often considered as the optimal stopping point for training.

Additionally, the authors mention that they used techniques like dropout and batch normalization to reduce overfitting and improve generalization on testing data. They also state that they tuned hyperparameters like learning rate, number of epochs, and batch size to achieve the best testing accuracy and Kappa score. This suggests that they likely monitored these metrics during training and stopped training when they observed satisfactory results on the test set.

However, without more specific details about the training process, it is impossible to provide a definitive answer regarding the exact criteria used by the authors to determine when training was complete.
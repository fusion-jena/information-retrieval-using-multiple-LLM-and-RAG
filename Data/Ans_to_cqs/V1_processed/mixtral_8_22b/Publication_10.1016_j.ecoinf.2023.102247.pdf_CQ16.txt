The context does not provide specific information about the criteria used to determine when training is complete. However, based on general machine learning practices, some common criteria include reaching a minimum threshold for validation loss or accuracy, achieving a stable validation loss over several epochs, or observing no further improvement in validation metrics after a certain number of iterations. These criteria can vary depending on the specific algorithm and problem being addressed.

Based on the given context, it appears that the authors used cross-validation with 70% of their dataset as training data and the remaining 30% as testing data to evaluate the accuracy of their models. They also mentioned using the Area Under the Receiving Operator Characteristics Curve (AUC) as an index to measure the accuracy of their models. While these details do not directly address the query, they suggest that the authors may have used similar criteria to determine when training was complete. For example, they might have stopped training once the validation loss reached a satisfactory level or if there were no significant improvements in the AUC score after multiple iterations.
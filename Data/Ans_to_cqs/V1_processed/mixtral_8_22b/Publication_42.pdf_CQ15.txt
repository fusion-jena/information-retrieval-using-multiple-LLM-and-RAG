Two optimization techniques are mentioned in the text - Adam and Stochastic Gradient Descent (SGD). These techniques are used during the training process of Convolutional Neural Networks (CNNs) to minimize the loss function and improve the model's performance. According to the study, a total of 64 architectures were trained using these two optimizers. It was found that Adam turned out to be the superior optimizer for training of all models, resulting in higher F1-scores compared to SGD. Therefore, the final chosen architecture was trained using the Adam optimizer.

Based on the information provided, we can conclude that both Adam and SGD optimization techniques are applied in the deep learning pipeline. However, in this particular case, Adam proved to be more effective in achieving better results.
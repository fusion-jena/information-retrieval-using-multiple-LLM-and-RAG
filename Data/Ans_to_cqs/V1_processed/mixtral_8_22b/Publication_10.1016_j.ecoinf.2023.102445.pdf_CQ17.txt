Based on the given context, there is no explicit mention of any specific regularization methods used to prevent overfitting in the deep learning pipeline. However, it does provide information about the experimental setup and some modifications made to the YOLOv8 model.

Regularization methods like dropout or L2 regularization are not explicitly mentioned in the provided text. Dropout is a technique where randomly selected neurons are ignored during training, which helps to avoid overfitting. On the other hand, L2 regularization adds a penalty term to the loss function, discouraging large weights and thus helping to reduce overfitting.

While these techniques are common in deep learning pipelines, without further details about the specific implementation of the models used in the experiments, it cannot be confirmed whether they were employed or not. Therefore, based solely on the provided context, I am unable to definitively state what regularization methods were used to prevent overfitting in the deep learning pipeline.
In the given context, one specific optimization technique is mentioned for use in a deep learning pipeline. The authors [27] discuss the use of Adam, which stands for Adaptive Moment Estimation. It is a method for stochastic optimization and can be used instead of classical stochastic gradient descent procedure to update network weights iterative based on training data. Adam computes adaptive learning rates for different parameters from estimates of first and second moments of the gradients. This allows it to combine the advantages of two other extensions of stochastic gradient descent: AdaGrad, which maintains a per-parameter learning rate, and RMSProp, which divides the learning rate by an exponentially decaying average of squared gradients. Adam has been shown to work well in practice and is easy to implement.
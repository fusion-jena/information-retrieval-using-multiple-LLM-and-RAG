The text does not provide explicit information about the specific regularization methods used to prevent overfitting in their deep learning pipeline. However, some common regularization methods used in deep learning include dropout, L1 regularization, and L2 regularization. Dropout randomly drops out neurons during training to prevent overfitting by making the network more robust to changes in input data. L1 regularization adds a penalty term to the cost function that encourages sparsity in the weights, while L2 regularization adds a penalty term that discourages large weight values. These methods help prevent overfitting by reducing the complexity of the model and improving generalization to new data. It is possible that one or more of these methods may have been used in this study, but without additional information, it cannot be confirmed.
The text does not provide explicit information about handling randomness in the deep learning pipeline, such as managing random seed values. However, it mentions some strategies related to the training process of the deep learning models. These include setting specific parameters, such as the resolution of input data (224x224), applying batch normalization, using a batch size of 30 during training, employing an optimized rectified linear activation (Leaky ReLU), adjusting width and resolution multipliers for certain models (like MobileNet), and setting consistent learning rates, dropout rates, and bias values across all models. Additionally, the number of training epochs is adjusted based on the depth of the neural networks to prevent overfitting. While these strategies do not directly address randomness due to factors like random seed values, they contribute to creating a more controlled and standardized environment for training deep learning models.
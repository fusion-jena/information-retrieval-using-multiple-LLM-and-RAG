Based on the given information, there isn't any explicit mention of specific optimization techniques such as Stochastic Gradient Descent (SGD), Adam, or others being used in the deep learning pipeline. However, some insights can be derived from the context provided.

For instance, the text mentions that the model was trained for a certain number of epochs, which suggests that some form of gradient descent optimization technique might have been employed during the training process. Epochs refer to one complete pass through the entire training dataset, and they are typically associated with iterative optimization methods like gradient descent.

Moreover, the use of a cross-entropy loss function for classification loss computation implies that the model could potentially be optimized using backpropagation, which is often paired with stochastic gradient descent or other similar optimization algorithms.

However, without direct evidence stating the exact optimization techniques used, it would be speculative to definitively claim that techniques like SGD or Adam were utilized in this case. Therefore, based on the provided context, it cannot be confirmed what optimization techniques were applied in the deep learning pipeline.
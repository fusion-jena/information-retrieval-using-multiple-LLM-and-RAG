The authors of the paper mention that they added a new kind of Batch Normalization (BNorm) layers behind the 22 convolutional layers to prevent overfitting when training models using BatNet. Batch normalization is a technique used to stabilize the learning process and improve the speed of convergence during training. It does so by normalizing the activations of each layer, which helps reduce internal covariance shift and makes the optimization landscape smoother. This allows the use of higher learning rates and reduces the need for careful initialization of parameters. Additionally, it acts as a form of regularization, reducing overfitting by providing some noise to the inputs of each layer. The authors do not explicitly mention any other regularization techniques such as dropout or L2 regularization.
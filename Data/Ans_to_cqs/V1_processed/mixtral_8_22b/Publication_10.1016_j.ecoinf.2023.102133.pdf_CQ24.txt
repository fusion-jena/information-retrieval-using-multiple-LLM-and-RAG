The strategy employed to handle randomness in the deep learning pipeline, specifically in the context of Random Forests, involves introducing randomness into the construction of the forest itself. This is achieved through two main methods: bagging and feature randomness.

Bagging, short for Bootstrap Aggregating, is a technique where multiple subsets of the original dataset are created by selecting samples randomly with replacement. Each subset is used to train a separate decision tree. The final prediction is made by aggregating the results from all these trees, often done through voting or averaging. This approach helps reduce overfitting and increases the robustness of the model.

Feature randomness is another strategy used in Random Forests. At each node of every decision tree, only a random subset of features is considered for splitting. This introduces more diversity among the trees and further reduces correlation between them.

These strategies help manage the inherent randomness in the deep learning pipeline, such as the random initialization of weights in neural networks or the random shuffling of datasets before training. By incorporating randomness in a controlled manner, we can improve the generalization ability of our models and prevent overfitting.
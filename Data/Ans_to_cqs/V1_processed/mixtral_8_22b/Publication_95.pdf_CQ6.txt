Before training a deep learning model such as a deep convolutional neural network, several preprocessing steps are typically performed on the dataset. These steps aim to improve the quality of the data and enhance the model's ability to learn meaningful features. In this case, the following preprocessing steps were taken:

1. Data splitting: The dataset was divided into three subsets - training, validation, and testing sets. This division ensures that the model does not overfit to the training data and allows for better generalization. It also enables the comparison of different networks and their performances.

2. Data balancing: The training set was balanced by upsampling the images from underrepresented classes through repetition. This step helps prevent the model from becoming biased towards the majority class and improves its overall performance.

3. Data augmentation: A three-step data augmentation process was applied to the images before they were fed into the network. This included horizontal flipping, scaling the contrast of each image channel, and cropping the image to a square. Data augmentation techniques help increase the diversity of the training data and reduce overfitting.

4. Input image size adjustment: The input image size was adjusted based on five different resolutions. This step impacts the processing time and classification accuracy of the model. By experimenting with different image sizes, the authors aimed to find the optimal balance between computational efficiency and model performance.

5. Adapting final classification layer: The final classification layer of the network structure was modified to match the seven classes present in the dataset. This adaptation ensures that the output of the model aligns with the desired number of classes.

These preprocessing steps contribute to improving the overall performance of the deep learning model and ensuring that it can effectively learn from the given dataset.
The text does not provide information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. Regularization methods such as dropout and L2 regularization are commonly used techniques to prevent overfitting in deep learning models. Dropout randomly drops out neurons during training to reduce co-dependencies between them, while L2 regularization adds a penalty term to the loss function to discourage large weights. Other regularization methods include early stopping, weight decay, and batch normalization. Without more context or information, I cannot determine which specific regularization methods were used in this particular case.
Based on the given context, there isn't explicit mention of specific measures such as diverse datasets, cross-validation, or stratified splitting to ensure the generalizability of the deep learning model. However, some aspects related to the training process might contribute to improving the model's performance and potentially enhance its generalization capabilities.

Firstly, the normalization and standardization of the training set help prevent any single feature from dominating the others during the learning process, which could lead to better generalization. Secondly, using a batch size of 32 ensures efficient utilization of computational resources without exceeding memory limits, allowing the model to learn from different subsets of the data. Lastly, employing an early stopping mechanism halts the learning process when there is no further improvement in accuracy after 20 epochs, preventing overfitting and promoting better generalization.

Although these practices may indirectly improve the model's generalizability, they do not directly address the query regarding specific methods like diverse datasets, cross-validation, or stratified splitting. Therefore, based solely on the provided context, we cannot definitively state what measures were explicitly taken to ensure the generalizability of the deep learning model concerning those particular methods.
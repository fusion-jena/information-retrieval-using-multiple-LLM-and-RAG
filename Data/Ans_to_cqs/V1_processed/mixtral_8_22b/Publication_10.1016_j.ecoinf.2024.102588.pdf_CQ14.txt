The text does not provide explicit information about how the hyperparameters of the Random Forest (RF) or Extreme Gradient Boosting (XGBoost) models were optimized. However, it mentions that the hyperparameters of the RF were set to the number of estimators at 100 and the random state at 0, while the XGBoost model had specific settings such as the subsample ratio of the columns being 0.3, the learning rate being 0.1, the maximum depth of a tree being 30, the alpha being 10, and the number of estimators being 100. These values could have been determined using various methods like grid search, random search, Bayesian optimization, or manual tuning based on domain knowledge and experience. Grid search involves trying out different combinations of hyperparameter values within a manually specified range, while random search selects random combinations instead. Bayesian optimization uses probabilistic models to guide the selection of hyperparameters. Without further details, we cannot definitively determine which method was used to optimize these hyperparameters.
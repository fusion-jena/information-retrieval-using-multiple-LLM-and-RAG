The provided context does not directly address the specific strategies employed to handle randomness in the deep learning pipeline, such as managing random seed values. However, based on general practices in machine learning, some common strategies to manage randomness include:

1. Setting a fixed seed value: By setting a fixed seed value, one can ensure that the results obtained from running the code multiple times remain consistent. This helps in reproducing the results and debugging the code.

2. Using techniques like k-fold cross-validation: K-fold cross-validation involves splitting the dataset into 'k' subsets or folds. Each fold is used as a validation set, while the remaining folds serve as the training set. This process is repeated 'k' times, ensuring that every data point gets to be in the validation set exactly once. This technique reduces the variability in the results caused by the randomness in selecting the train and validation sets.

3. Ensemble methods: Ensemble methods involve combining multiple models to improve prediction accuracy and reduce overfitting. Techniques like bagging and boosting help in reducing the variance caused by randomness in individual models.

While these strategies are commonly employed to handle randomness in the deep learning pipeline, the provided context does not explicitly mention their use in the study. Therefore, further information would be required to provide a more accurate response regarding the specific strategies employed in this case.
Unfortunately, based on the given context, there isn't explicit information about how the hyperparameters of the model were optimized, such as through methods like grid search or random search. However, it does mention that the model was trained for a specific number of epochs (200 and 50) during different stages of the fine-tuning process. Additionally, it mentions that the patch size was set to 1024 x 1024 pixels during inference to accelerate the process. These details suggest that certain parameters were manually chosen rather than being determined by an optimization method. Nonetheless, without more specific information regarding hyperparameter tuning, I cannot provide a definitive answer.
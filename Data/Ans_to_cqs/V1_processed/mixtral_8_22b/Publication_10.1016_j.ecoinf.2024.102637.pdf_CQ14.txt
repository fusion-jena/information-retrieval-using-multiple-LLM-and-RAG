Based on the given information, it appears that the optimization of the hyperparameters of the model is not explicitly mentioned or described in detail. However, there are some clues about how certain hyperparameters might be adjusted during the training process.

For instance, the value of the hyperparameter lambda (Î»), which is used to control the balance between the nonlinear factors of the activation function and the identity mapping of A'(x), is determined by the ratio of the current number of training rounds (e) to the total number of training rounds (E). This suggests that the value of lambda changes dynamically throughout the training process, rather than being fixed beforehand through methods such as grid search or random search.

Additionally, the learning rate of the model is set to 0.00005, but it is unclear whether this value was chosen arbitrarily or through some sort of optimization technique. Similarly, the batch size is set to 16 without any explanation of why this particular value was selected.

Overall, while the text does mention some aspects of the model's design and implementation, it does not provide enough information to determine exactly how the hyperparameters were optimized.
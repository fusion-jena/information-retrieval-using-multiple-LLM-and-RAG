The paper uses several metrics to evaluate the performance of their deep learning model. These include precision, average precision (AP), mean Average Precision (mAP), and Intersection over Union (IoU).

Precision measures the proportion of true positive predictions made by the model relative to the total number of positive predictions made. A higher precision value indicates fewer false positives, meaning the model is better at correctly identifying objects within its class.

Average precision (AP) is a metric that summarizes the precision-recall curve into a single value. It calculates the area under the precision-recall curve, where a larger area corresponds to a more accurate model. In this study, two types of AP are reported: AP0.5 and AP0.5:0.95. AP0.5 represents the AP calculated at a 50% IoU threshold, while AP0.5:0.95 represents the AP calculated over an IoU range from 0.5 to 0.95 in increments of 0.05.

Mean Average Precision (mAP) is another metric used to evaluate the performance of the model. It is defined as the mean of all AP values, as shown in Equation (13).

Intersection over Union (IoU) is a metric used to determine how well the predicted bounding box matches the ground truth bounding box. It is calculated as the ratio of the intersection area between the predicted and ground truth bounding boxes to the union area between them. A higher IoU value indicates a better match between the predicted and ground truth bounding boxes.

Overall, these metrics provide a comprehensive evaluation of the model's performance in terms of both accuracy and efficiency.
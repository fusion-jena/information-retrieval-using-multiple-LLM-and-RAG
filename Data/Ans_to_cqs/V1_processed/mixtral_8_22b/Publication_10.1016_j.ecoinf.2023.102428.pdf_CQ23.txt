Based on the given information, it appears that there are no specific details about the measures taken to ensure the generalizability of the deep learning model used in this study. However, we can infer some possible methods based on common practices in machine learning and deep learning models.

One way to ensure the generalizability of a deep learning model is by using a diverse dataset. A diverse dataset includes various examples from different sources or domains, which helps the model learn more robust features and patterns. In this case, the authors have not explicitly mentioned whether they used a diverse dataset or not.

Cross-validation is another technique commonly employed to evaluate the performance of a model and its ability to generalize well to unseen data. Cross-validation involves dividing the available data into multiple subsets, training the model on one subset while validating it on another, and repeating this process with different combinations of subsets. This method provides a better estimate of the model's performance compared to simply splitting the data once into training and testing sets. Unfortunately, there is no mention of cross-validation being applied in this study.

Stratified splitting is yet another approach to improve the generalization of a model. It involves partitioning the data such that each subset maintains the same distribution of target variables as the original dataset. By doing so, the model can be trained and evaluated on representative samples, reducing bias and improving overall performance. Again, there is no explicit reference to stratified splitting in the provided context.

Therefore, without further clarification from the authors regarding their methodology, it remains uncertain what specific measures were taken to ensure the generalizability of the deep learning model used in this study.
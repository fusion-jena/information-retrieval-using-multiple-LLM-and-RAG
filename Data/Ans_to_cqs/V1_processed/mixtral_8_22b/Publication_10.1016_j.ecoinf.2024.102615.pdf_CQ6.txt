Before training a deep learning model such as the Multilayer Perceptron (MLP) model mentioned in the given context, several preprocessing steps may be involved depending on the nature of the dataset. However, based on the provided information, two main preprocessing techniques have been utilized:

1) Normalization: The text mentions the use of the 'Batch Normalization' technique, which is a type of normalization method used in deep learning models. This technique helps to stabilize the learning process and reduce the training time by normalizing the inputs in every mini-batch. It does this by making the mean activation close to zero and the activation variance close to one.

2) Dropout: Dropout is another preprocessing step used in deep learning models to prevent overfitting. Overfitting occurs when a model performs well on the training data but poorly on unseen data due to memorizing the training data instead of generalizing from it. By randomly dropping out or ignoring some neurons during training, dropout encourages the network to distribute the learned features across all neurons, thereby reducing overfitting.

While not explicitly stated in the provided context, other common preprocessing steps might include data cleaning (removing missing or inconsistent data), feature scaling (adjusting the scale of independent variables so they fall within a similar range), and encoding categorical variables (converting qualitative data into quantitative data). These steps help improve the accuracy and efficiency of the deep learning model.
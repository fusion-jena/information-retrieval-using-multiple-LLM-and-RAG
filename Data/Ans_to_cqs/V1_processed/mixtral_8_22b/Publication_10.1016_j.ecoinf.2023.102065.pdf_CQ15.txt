The papers mentioned do not explicitly discuss specific optimization techniques used in their deep learning pipelines, such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, it can be assumed that some form of optimization technique would have been employed during training, given that they are essential components of any deep learning process. The choice of optimization algorithm depends on various factors like computational resources available, size of dataset, complexity of the problem etc. Without explicit mention of which optimization techniques were used, one cannot definitively state what was implemented in each case.
Hyperband, a method used for optimizing hyperparameters in machine learning algorithms, employs several strategies to handle randomness in the deep learning pipeline. One such strategy is partial training, where models are initially trained on a small fraction of the data to quickly eliminate underperforming configurations. This approach helps manage the variability introduced by random initialization of weights and biases in neural networks. Another strategy used by Hyperband is successive halving, which involves grouping the remaining configurations into sets of different sizes and training them further. Only the best-performing configurations from each set are promoted to the next stage, while underperforming ones are eliminated. By iteratively repeating these steps, Hyperband efficiently explores various configurations in parallel while allocating more training resources to promising ones. This process allows for a more effective search of the hyperparameter space, especially when computational resources are limited. Additionally, Hyperband combines an initial random search with a'successive halving' strategy to rapidly identify promising configurations while eliminating those with inferior performance. These techniques help mitigate the impact of randomness in the deep learning pipeline, ultimately leading to improved model performance and stability.
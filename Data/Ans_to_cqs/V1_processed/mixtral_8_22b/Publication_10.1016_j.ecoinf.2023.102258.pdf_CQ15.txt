The given text does not provide explicit details about specific optimization techniques such as Stochastic Gradient Descent (SGD) or Adaptive Moment Estimation (Adam). However, it mentions that during the training process, the neural network is optimized to minimize the distances between prototypes and corresponding query points while maximizing the distances with non-matching points. This suggests that some form of gradient descent optimization technique might be employed to update the weights of the neural network during backpropagation. Additionally, the use of a softmax function indicates that the output layer of the neural network could be using a form of categorical cross-entropy loss function, which is commonly minimized using optimization techniques like SGD or Adam.
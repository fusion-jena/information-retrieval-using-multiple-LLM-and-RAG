In the given context, there isn't any explicit mention or discussion on regularization methods such as dropout or L2 regularization used to prevent overfitting in the deep learning pipeline. Instead, the text focuses on the use of residual neural networks like ResNet152, ResNet18, and others, along with Feature Pyramid Network (FPN) for object detection tasks using UAV thermal images.

However, I can explain how regularization techniques help prevent overfitting in general terms. Regularization methods aim to reduce the complexity of the learned model, thus preventing overfitting. Dropout is one popular method where randomly selected neurons are ignored during training, which helps avoid relying too much on individual neurons and encourages better overall network performance. Another common technique is L2 regularization, also known as weight decay, which adds a penalty term to the loss function based on the magnitude of weights, discouraging large weights and promoting simpler models. Other regularization methods include early stopping, data augmentation, and batch normalization.
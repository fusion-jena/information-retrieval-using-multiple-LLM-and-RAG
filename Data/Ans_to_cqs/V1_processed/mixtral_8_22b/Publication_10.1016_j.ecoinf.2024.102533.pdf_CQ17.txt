The text does not explicitly mention any specific regularization methods used to prevent overfitting in the deep learning pipeline. Regularization methods such as dropout and L2 regularization can be applied during training to reduce model complexity and improve generalization performance. Dropout randomly sets a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. On the other hand, L2 regularization adds a penalty term to the loss function, encouraging smaller weights and reducing overfitting. Other techniques like early stopping or weight decay can also help prevent overfitting. However, without further information from the original source, it cannot be determined if these methods were specifically utilized in this study.
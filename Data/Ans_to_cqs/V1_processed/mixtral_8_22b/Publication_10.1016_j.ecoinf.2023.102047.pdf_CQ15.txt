The text does not explicitly mention any specific optimization technique like Stochastic Gradient Descent (SGD), Adam, etc. However, it does indicate that a momentum optimizer was used with a value of 0.5. This suggests that some form of gradient descent optimization was employed, possibly a variant of SGD with momentum. Additionally, the use of early stopping and monitoring validation loss/accuracy implies strategies to prevent overfitting and improve generalization. But without explicit mention, we cannot definitively say what exact optimization techniques were used beyond the mentioned momentum optimizer.
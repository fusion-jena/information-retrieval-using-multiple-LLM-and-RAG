Based on the given context, there isn't any direct information about specific optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD), Adam, etc. However, it can be inferred from the context that node-based programming is employed for various tasks including complex customized functions requiring considerable computational resources. In this regard, optimization techniques like SGD, Adam, or others could potentially be utilized within the node-based programming paradigm for training neural networks or other models. Nonetheless, without explicit mention in the provided context, we cannot definitively state which optimization techniques are being used.
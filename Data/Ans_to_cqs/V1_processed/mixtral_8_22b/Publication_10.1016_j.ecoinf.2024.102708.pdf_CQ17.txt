Based on the given context, there is no explicit mention of any regularization methods used to prevent overfitting in the deep learning pipeline. However, considering the common practices in deep learning, one might infer that techniques like dropout or L2 regularization could have been employed. Dropout is a technique where randomly selected neurons are ignored during training, which helps prevent overfitting by making the model more robust. On the other hand, L2 regularization adds a penalty term to the loss function, discouraging large weights and thus helping reduce overfitting. Nevertheless, without concrete information about the specific regularization methods used in this case, these assumptions remain speculative.
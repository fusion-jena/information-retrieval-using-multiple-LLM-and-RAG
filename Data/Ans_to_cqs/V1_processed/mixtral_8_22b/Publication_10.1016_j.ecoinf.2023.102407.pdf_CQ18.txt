Based on the given information, the authors use two main strategies to evaluate the performance of their models during training: Area Under Curve (AUC) and True Skill Statistic (TSS). These metrics are commonly used in machine learning to assess the quality of binary classifiers.

To compare the performances of various Species Distribution Models (SDMs), they employ these evaluation criteria objectively. They mention using threshold values ranging from 0 to 1, but do not provide specific details about how these thresholds were applied or selected.

Furthermore, the authors investigate the impact of incremental Pseudo Labeling (PL) on the effectiveness of training the student model. They observe that the improvement in model performance depends on the value of 'K', which represents the number of data points generated by the initial teacher model. Their findings suggest that setting 'K' between 50 and 400 generally leads to better improvements in model performance compared to higher ranges like 600 to 800.

However, the authors note that allowing the initial teacher model to produce excessive amounts of pseudo-labeled data negatively impacts the subsequent training of the teacher model, resulting in inaccurate student models. Consequently, they recommend careful consideration when determining the number of data points added in each PL iteration to ensure the accuracy of SDP-EPL. In their study, they found that 'K = 200' yielded optimal results for most cases.

Overall, while the authors discuss monitoring model performance through AUC and TSS metrics, they primarily focus on evaluating the effect of incremental PL on improving the student model's training process.
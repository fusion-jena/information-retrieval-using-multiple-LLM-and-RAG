Regularization methods were not explicitly mentioned in the given context. However, it can be inferred that measures were taken to avoid overfitting during the development of the deep learning model. This is evident from Figure 12, which shows a gradual decrease in both training and validation losses as the number of epochs increases, indicating that the model was learning without overfitting. Furthermore, the text mentions that additional training beyond 20,000 epochs could result in overfitting or no further reduction in losses. Although specific regularization techniques are not discussed, common methods to prevent overfitting include dropout, L1 and L2 regularization, early stopping, and data augmentation. Dropout randomly sets a fraction of input units to 0 at each update during training time, helping to prevent overfitting. L1 and L2 regularization add penalties to the loss function based on the weights of the neurons, encouraging smaller weights and simpler models. Early stopping terminates the training process once the performance on a validation set stops improving, while data augmentation expands the original dataset by applying random transformations to existing samples.
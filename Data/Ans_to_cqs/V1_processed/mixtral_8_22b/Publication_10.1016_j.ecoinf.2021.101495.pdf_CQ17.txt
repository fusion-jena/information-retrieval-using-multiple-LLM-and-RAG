The text does not provide information about the specific regularization methods used to prevent overfitting in the deep learning pipeline. However, common techniques include dropout, which randomly drops out nodes during training to reduce co-dependencies between neurons, and L2 regularization, which adds a penalty term to the loss function to discourage large weights. These methods help prevent overfitting by reducing complexity and improving generalization.

Based on the given context, we cannot determine if these specific methods were employed in their deep learning pipeline. It is important to note that there are various ways to address overfitting, including early stopping, data augmentation, and using simpler models. Without further details, we cannot confirm whether any of these strategies were utilized.
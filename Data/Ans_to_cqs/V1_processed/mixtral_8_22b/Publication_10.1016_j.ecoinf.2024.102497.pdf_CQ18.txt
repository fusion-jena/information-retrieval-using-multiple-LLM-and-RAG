Based on the given context, there isn't explicit information about the strategy used to monitor the model performance during training. However, some common strategies can be inferred based on general practices in implementing Random Forest Regression (RFR) models.

One possible approach could be using Cross-Validation, which involves splitting the dataset into k subsets or folds. Each fold is then used once as a test set while the remaining k-1 folds form the training set. This allows for evaluating the model's performance across different subsets of the data, providing insights into its ability to generalize well to unseen data.

Additionally, monitoring metrics like Mean Absolute Error (MAE), Root Mean Square Error (RMSE), or coefficient of determination (R^2 score) during training can help assess the model's performance. By tracking these metrics, one can observe how well the model fits the training data and identify potential issues like underfitting or overfitting.

Furthermore, tuning hyperparameters through Grid Search or Randomized Search can also aid in optimizing the model's performance. Adjusting parameters such as 'n_estimators','max_depth','min_samples_split','min_samples_leaf', and'max_features' can significantly impact the model's accuracy and prevent overfitting.

In summary, although the exact strategy for monitoring the model performance during training is not specified in the provided context, common approaches include employing Cross-Validation, tracking evaluation metrics, and fine-tuning hyperparameters.
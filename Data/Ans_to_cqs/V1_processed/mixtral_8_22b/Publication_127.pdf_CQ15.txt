Based on the given context, there isn't any direct information about specific optimization techniques used in the deep learning pipeline such as Stochastic Gradient Descent (SGD), Adam, or others. However, some related concepts can be found. For instance, the paper [51] discusses identifying and attacking the saddle point problem in high-dimensional non-convex optimization, which could imply the use of optimization techniques like gradient descent or its variants. Additionally, the context mentions the use of 1×1 convolutions in NiN architecture to increase feature combinations, and the use of 3×3 convolutional filters in another architecture to improve performance over AlexNet. These architectural choices might involve optimization techniques during the training process, but no explicit mention of these techniques is made in the provided context. Therefore, based solely on the given context, it cannot be determined what optimization techniques are applied in the deep learning pipeline.